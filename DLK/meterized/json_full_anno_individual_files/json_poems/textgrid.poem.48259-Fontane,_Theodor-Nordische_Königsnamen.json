{"textgrid.poem.48259": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Nordische K\u00f6nigsnamen", "genre": "verse", "period": "N.A.", "pub_year": 1889, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bb ... Da (so hei\u00dft es in hochgelahrten Schriften)", "tokens": ["\u00bb", "...", "Da", "(", "so", "hei\u00dft", "es", "in", "hoch\u00b7ge\u00b7lahr\u00b7ten", "Schrif\u00b7ten", ")"], "token_info": ["punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "KOUS", "$(", "ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Begann das heillose ", "tokens": ["Be\u00b7gann", "das", "heil\u00b7lo\u00b7se"], "token_info": ["word", "word", "word"], "pos": ["NN", "ART", "ADJA"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Statt Zahlen, die klarer doch und reeller,", "tokens": ["Statt", "Zah\u00b7len", ",", "die", "kla\u00b7rer", "doch", "und", "re\u00b7el\u00b7ler", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "ADJD", "ADV", "KON", "ADJA", "$,"], "meter": "-+--+-++-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Kam Auceps, Finkler, Vogelsteller,", "tokens": ["Kam", "Au\u00b7ceps", ",", "Fink\u00b7ler", ",", "Vo\u00b7gel\u00b7stel\u00b7ler", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Kam L\u00f6we, Rotbart. Und gar nicht lange,", "tokens": ["Kam", "L\u00f6\u00b7we", ",", "Rot\u00b7bart", ".", "Und", "gar", "nicht", "lan\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$.", "KON", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Gab's einen \u00bbmit der gebissenen Wange\u00ab,", "tokens": ["Gab's", "ei\u00b7nen", "\u00bb", "mit", "der", "ge\u00b7bis\u00b7se\u00b7nen", "Wan\u00b7ge", "\u00ab", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "ART", "$(", "APPR", "ART", "ADJA", "NN", "$(", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Dazwischen, blasphemisch und wie zum Spott,", "tokens": ["Da\u00b7zwi\u00b7schen", ",", "blas\u00b7phe\u00b7misch", "und", "wie", "zum", "Spott", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "ADJD", "KON", "PWAV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Sogar einen Heinrich Jasomirgott.", "tokens": ["So\u00b7gar", "ei\u00b7nen", "Hein\u00b7rich", "Ja\u00b7so\u00b7mir\u00b7gott", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NE", "NE", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.9": {"text": "So ging es in Deutschland. Anderswo", "tokens": ["So", "ging", "es", "in", "Deutschland", ".", "An\u00b7ders\u00b7wo"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "$.", "XY"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "War's, wenn nicht schlimmer, ebenso;", "tokens": ["Wa\u00b7r's", ",", "wenn", "nicht", "schlim\u00b7mer", ",", "e\u00b7ben\u00b7so", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PTKNEG", "ADJD", "$,", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Geschmacklos war die ganze Zeit,", "tokens": ["Ge\u00b7schmack\u00b7los", "war", "die", "gan\u00b7ze", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Und die ", "tokens": ["Und", "die"], "token_info": ["word", "word"], "pos": ["KON", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "Thyra D\u00e4nentrost oder \u00bbDanebod\u00ab,", "tokens": ["Thy\u00b7ra", "D\u00e4\u00b7nen\u00b7trost", "o\u00b7der", "\u00bb", "Da\u00b7ne\u00b7bod", "\u00ab", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "NE", "KON", "$(", "NE", "$(", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.14": {"text": "Erik Seelensgut oder \u00bbEiegod\u00ab,", "tokens": ["E\u00b7rik", "See\u00b7lens\u00b7gut", "o\u00b7der", "\u00bb", "Ei\u00b7e\u00b7god", "\u00ab", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "NN", "KON", "$(", "NN", "$(", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Erik Hasenfu\u00df oder Laufgeschwind,", "tokens": ["E\u00b7rik", "Ha\u00b7sen\u00b7fu\u00df", "o\u00b7der", "Lauf\u00b7ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.16": {"text": "Erik Lamm, Erik Pommer, Erik Kind,", "tokens": ["E\u00b7rik", "Lamm", ",", "E\u00b7rik", "Pom\u00b7mer", ",", "E\u00b7rik", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$,", "NN", "NN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.17": {"text": "Erik Pflugpfennig, Erik Pfaffentort,", "tokens": ["E\u00b7rik", "Pflug\u00b7pfen\u00b7nig", ",", "E\u00b7rik", "Pfaf\u00b7fen\u00b7tort", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "NN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.18": {"text": "Erik Mendved oder Manneswort,", "tokens": ["E\u00b7rik", "Mend\u00b7ved", "o\u00b7der", "Man\u00b7nes\u00b7wort", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.19": {"text": "Erik Glipping, der mit den Wimpern glippt,", "tokens": ["E\u00b7rik", "Glip\u00b7ping", ",", "der", "mit", "den", "Wim\u00b7pern", "glippt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "Erik Kipping, der die M\u00fcnzen kippt,", "tokens": ["E\u00b7rik", "Kip\u00b7ping", ",", "der", "die", "M\u00fcn\u00b7zen", "kippt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.21": {"text": "Ein Gl\u00fcck, da\u00df der Eriks nicht mehr gewesen,", "tokens": ["Ein", "Gl\u00fcck", ",", "da\u00df", "der", "E\u00b7riks", "nicht", "mehr", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "ART", "NN", "PTKNEG", "ADV", "VAPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.22": {"text": "Wir w\u00fcrden sonst noch viel Schlimmeres lesen.\u00ab", "tokens": ["Wir", "w\u00fcr\u00b7den", "sonst", "noch", "viel", "Schlim\u00b7me\u00b7res", "le\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "VVINF", "$.", "$("], "meter": "-+-+--++-+-", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "So die Hochgelahrten, die Weisen und Alten.", "tokens": ["So", "die", "Hoch\u00b7ge\u00b7lahr\u00b7ten", ",", "die", "Wei\u00b7sen", "und", "Al\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ART", "NN", "KON", "NN", "$."], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Geschicht' und Dichtkunst sind zweierlei Z\u00fcnfte,", "tokens": ["Ge\u00b7schicht'", "und", "Dicht\u00b7kunst", "sind", "zwei\u00b7er\u00b7lei", "Z\u00fcnf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "PIAT", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mir gef\u00e4llt nicht der \u00bbErste\u00ab, der \u00bbDritte\u00ab, der \u00bbF\u00fcnfte\u00ab,", "tokens": ["Mir", "ge\u00b7f\u00e4llt", "nicht", "der", "\u00bb", "Ers\u00b7te", "\u00ab", ",", "der", "\u00bb", "Drit\u00b7te", "\u00ab", ",", "der", "\u00bb", "F\u00fcnf\u00b7te", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "punct", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "$(", "ADJA", "$(", "$,", "ART", "$(", "ADJA", "$(", "$,", "ART", "$(", "NN", "$(", "$,"], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Zahlen und wieder Zahlen blo\u00df", "tokens": ["Zah\u00b7len", "und", "wie\u00b7der", "Zah\u00b7len", "blo\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ADV", "NN", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Scheinen mir tot und charakterlos.", "tokens": ["Schei\u00b7nen", "mir", "tot", "und", "cha\u00b7rak\u00b7ter\u00b7los", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "Ragnar Pechhos' und Iwar Klaftergriff", "tokens": ["Rag\u00b7nar", "Pech\u00b7hos'", "und", "I\u00b7war", "Klaf\u00b7ter\u00b7griff"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "KON", "NE", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Haben schon andern Schneid und Schliff,", "tokens": ["Ha\u00b7ben", "schon", "an\u00b7dern", "Schneid", "und", "Schliff", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Harald Blauzahn und Rolf Krake der Zwerg", "tokens": ["Ha\u00b7rald", "Blau\u00b7zahn", "und", "Rolf", "Kra\u00b7ke", "der", "Zwerg"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "KON", "NE", "NE", "ART", "NN"], "meter": "+-+--++--+", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "Helfen schon anders \u00fcber den Berg,", "tokens": ["Hel\u00b7fen", "schon", "an\u00b7ders", "\u00fc\u00b7ber", "den", "Berg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Swend Gabel- und Hakon Borkenbart,", "tokens": ["Swend", "Ga\u00b7bel", "und", "Ha\u00b7kon", "Bor\u00b7ken\u00b7bart", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "TRUNC", "KON", "NE", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Fleckauge, Sch\u00f6nhaar, Sigurd Ring,", "tokens": ["Fleck\u00b7au\u00b7ge", ",", "Sch\u00f6n\u00b7haar", ",", "Si\u00b7gurd", "Ring", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NE", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.12": {"text": "Alles schon ein ander Ding,", "tokens": ["Al\u00b7les", "schon", "ein", "an\u00b7der", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.13": {"text": "Gorm Grymme, Frede Harde-Schnut,", "tokens": ["Gorm", "Grym\u00b7me", ",", "Fre\u00b7de", "Har\u00b7de\u00b7Sch\u00b7nut", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "NN", "NN", "$,"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.14": {"text": "Olaf Hunger vor allem gef\u00e4llt mir gut,", "tokens": ["O\u00b7laf", "Hun\u00b7ger", "vor", "al\u00b7lem", "ge\u00b7f\u00e4llt", "mir", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "PIS", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.15": {"text": "Und zum letzten: Olaf Kragebeen \u2013", "tokens": ["Und", "zum", "letz\u00b7ten", ":", "O\u00b7laf", "Kra\u00b7ge\u00b7been", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "$.", "NE", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.16": {"text": "Tretet vor und verneigt euch und la\u00dft euch sehn.", "tokens": ["Tre\u00b7tet", "vor", "und", "ver\u00b7neigt", "euch", "und", "la\u00dft", "euch", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "KON", "VVFIN", "PPER", "KON", "VVIMP", "PPER", "VVINF", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bb ... Da (so hei\u00dft es in hochgelahrten Schriften)", "tokens": ["\u00bb", "...", "Da", "(", "so", "hei\u00dft", "es", "in", "hoch\u00b7ge\u00b7lahr\u00b7ten", "Schrif\u00b7ten", ")"], "token_info": ["punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "KOUS", "$(", "ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Begann das heillose ", "tokens": ["Be\u00b7gann", "das", "heil\u00b7lo\u00b7se"], "token_info": ["word", "word", "word"], "pos": ["NN", "ART", "ADJA"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Statt Zahlen, die klarer doch und reeller,", "tokens": ["Statt", "Zah\u00b7len", ",", "die", "kla\u00b7rer", "doch", "und", "re\u00b7el\u00b7ler", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "ADJD", "ADV", "KON", "ADJA", "$,"], "meter": "-+--+-++-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Kam Auceps, Finkler, Vogelsteller,", "tokens": ["Kam", "Au\u00b7ceps", ",", "Fink\u00b7ler", ",", "Vo\u00b7gel\u00b7stel\u00b7ler", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Kam L\u00f6we, Rotbart. Und gar nicht lange,", "tokens": ["Kam", "L\u00f6\u00b7we", ",", "Rot\u00b7bart", ".", "Und", "gar", "nicht", "lan\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$.", "KON", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Gab's einen \u00bbmit der gebissenen Wange\u00ab,", "tokens": ["Gab's", "ei\u00b7nen", "\u00bb", "mit", "der", "ge\u00b7bis\u00b7se\u00b7nen", "Wan\u00b7ge", "\u00ab", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "ART", "$(", "APPR", "ART", "ADJA", "NN", "$(", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Dazwischen, blasphemisch und wie zum Spott,", "tokens": ["Da\u00b7zwi\u00b7schen", ",", "blas\u00b7phe\u00b7misch", "und", "wie", "zum", "Spott", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "ADJD", "KON", "PWAV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Sogar einen Heinrich Jasomirgott.", "tokens": ["So\u00b7gar", "ei\u00b7nen", "Hein\u00b7rich", "Ja\u00b7so\u00b7mir\u00b7gott", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NE", "NE", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.9": {"text": "So ging es in Deutschland. Anderswo", "tokens": ["So", "ging", "es", "in", "Deutschland", ".", "An\u00b7ders\u00b7wo"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "$.", "XY"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "War's, wenn nicht schlimmer, ebenso;", "tokens": ["Wa\u00b7r's", ",", "wenn", "nicht", "schlim\u00b7mer", ",", "e\u00b7ben\u00b7so", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PTKNEG", "ADJD", "$,", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Geschmacklos war die ganze Zeit,", "tokens": ["Ge\u00b7schmack\u00b7los", "war", "die", "gan\u00b7ze", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Und die ", "tokens": ["Und", "die"], "token_info": ["word", "word"], "pos": ["KON", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "Thyra D\u00e4nentrost oder \u00bbDanebod\u00ab,", "tokens": ["Thy\u00b7ra", "D\u00e4\u00b7nen\u00b7trost", "o\u00b7der", "\u00bb", "Da\u00b7ne\u00b7bod", "\u00ab", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "NE", "KON", "$(", "NE", "$(", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.14": {"text": "Erik Seelensgut oder \u00bbEiegod\u00ab,", "tokens": ["E\u00b7rik", "See\u00b7lens\u00b7gut", "o\u00b7der", "\u00bb", "Ei\u00b7e\u00b7god", "\u00ab", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "NN", "KON", "$(", "NN", "$(", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Erik Hasenfu\u00df oder Laufgeschwind,", "tokens": ["E\u00b7rik", "Ha\u00b7sen\u00b7fu\u00df", "o\u00b7der", "Lauf\u00b7ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.16": {"text": "Erik Lamm, Erik Pommer, Erik Kind,", "tokens": ["E\u00b7rik", "Lamm", ",", "E\u00b7rik", "Pom\u00b7mer", ",", "E\u00b7rik", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$,", "NN", "NN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.17": {"text": "Erik Pflugpfennig, Erik Pfaffentort,", "tokens": ["E\u00b7rik", "Pflug\u00b7pfen\u00b7nig", ",", "E\u00b7rik", "Pfaf\u00b7fen\u00b7tort", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "NN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.18": {"text": "Erik Mendved oder Manneswort,", "tokens": ["E\u00b7rik", "Mend\u00b7ved", "o\u00b7der", "Man\u00b7nes\u00b7wort", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.19": {"text": "Erik Glipping, der mit den Wimpern glippt,", "tokens": ["E\u00b7rik", "Glip\u00b7ping", ",", "der", "mit", "den", "Wim\u00b7pern", "glippt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "Erik Kipping, der die M\u00fcnzen kippt,", "tokens": ["E\u00b7rik", "Kip\u00b7ping", ",", "der", "die", "M\u00fcn\u00b7zen", "kippt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.21": {"text": "Ein Gl\u00fcck, da\u00df der Eriks nicht mehr gewesen,", "tokens": ["Ein", "Gl\u00fcck", ",", "da\u00df", "der", "E\u00b7riks", "nicht", "mehr", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "ART", "NN", "PTKNEG", "ADV", "VAPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.22": {"text": "Wir w\u00fcrden sonst noch viel Schlimmeres lesen.\u00ab", "tokens": ["Wir", "w\u00fcr\u00b7den", "sonst", "noch", "viel", "Schlim\u00b7me\u00b7res", "le\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "VVINF", "$.", "$("], "meter": "-+-+--++-+-", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "So die Hochgelahrten, die Weisen und Alten.", "tokens": ["So", "die", "Hoch\u00b7ge\u00b7lahr\u00b7ten", ",", "die", "Wei\u00b7sen", "und", "Al\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ART", "NN", "KON", "NN", "$."], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Geschicht' und Dichtkunst sind zweierlei Z\u00fcnfte,", "tokens": ["Ge\u00b7schicht'", "und", "Dicht\u00b7kunst", "sind", "zwei\u00b7er\u00b7lei", "Z\u00fcnf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "PIAT", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mir gef\u00e4llt nicht der \u00bbErste\u00ab, der \u00bbDritte\u00ab, der \u00bbF\u00fcnfte\u00ab,", "tokens": ["Mir", "ge\u00b7f\u00e4llt", "nicht", "der", "\u00bb", "Ers\u00b7te", "\u00ab", ",", "der", "\u00bb", "Drit\u00b7te", "\u00ab", ",", "der", "\u00bb", "F\u00fcnf\u00b7te", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "punct", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "$(", "ADJA", "$(", "$,", "ART", "$(", "ADJA", "$(", "$,", "ART", "$(", "NN", "$(", "$,"], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Zahlen und wieder Zahlen blo\u00df", "tokens": ["Zah\u00b7len", "und", "wie\u00b7der", "Zah\u00b7len", "blo\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ADV", "NN", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Scheinen mir tot und charakterlos.", "tokens": ["Schei\u00b7nen", "mir", "tot", "und", "cha\u00b7rak\u00b7ter\u00b7los", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "Ragnar Pechhos' und Iwar Klaftergriff", "tokens": ["Rag\u00b7nar", "Pech\u00b7hos'", "und", "I\u00b7war", "Klaf\u00b7ter\u00b7griff"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "KON", "NE", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Haben schon andern Schneid und Schliff,", "tokens": ["Ha\u00b7ben", "schon", "an\u00b7dern", "Schneid", "und", "Schliff", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Harald Blauzahn und Rolf Krake der Zwerg", "tokens": ["Ha\u00b7rald", "Blau\u00b7zahn", "und", "Rolf", "Kra\u00b7ke", "der", "Zwerg"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "KON", "NE", "NE", "ART", "NN"], "meter": "+-+--++--+", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "Helfen schon anders \u00fcber den Berg,", "tokens": ["Hel\u00b7fen", "schon", "an\u00b7ders", "\u00fc\u00b7ber", "den", "Berg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Swend Gabel- und Hakon Borkenbart,", "tokens": ["Swend", "Ga\u00b7bel", "und", "Ha\u00b7kon", "Bor\u00b7ken\u00b7bart", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "TRUNC", "KON", "NE", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Fleckauge, Sch\u00f6nhaar, Sigurd Ring,", "tokens": ["Fleck\u00b7au\u00b7ge", ",", "Sch\u00f6n\u00b7haar", ",", "Si\u00b7gurd", "Ring", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NE", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.12": {"text": "Alles schon ein ander Ding,", "tokens": ["Al\u00b7les", "schon", "ein", "an\u00b7der", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.13": {"text": "Gorm Grymme, Frede Harde-Schnut,", "tokens": ["Gorm", "Grym\u00b7me", ",", "Fre\u00b7de", "Har\u00b7de\u00b7Sch\u00b7nut", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "NN", "NN", "$,"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.14": {"text": "Olaf Hunger vor allem gef\u00e4llt mir gut,", "tokens": ["O\u00b7laf", "Hun\u00b7ger", "vor", "al\u00b7lem", "ge\u00b7f\u00e4llt", "mir", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "PIS", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.15": {"text": "Und zum letzten: Olaf Kragebeen \u2013", "tokens": ["Und", "zum", "letz\u00b7ten", ":", "O\u00b7laf", "Kra\u00b7ge\u00b7been", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "$.", "NE", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.16": {"text": "Tretet vor und verneigt euch und la\u00dft euch sehn.", "tokens": ["Tre\u00b7tet", "vor", "und", "ver\u00b7neigt", "euch", "und", "la\u00dft", "euch", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "KON", "VVFIN", "PPER", "KON", "VVIMP", "PPER", "VVINF", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}}}}