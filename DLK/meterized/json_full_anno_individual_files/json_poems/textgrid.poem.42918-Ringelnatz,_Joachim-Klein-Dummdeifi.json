{"textgrid.poem.42918": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Klein-Dummdeifi", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Klein-Dummdeifi ging vor\u00fcber,", "tokens": ["Klein\u00b7Dumm\u00b7dei\u00b7fi", "ging", "vor\u00b7\u00fc\u00b7ber", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Witzig wie ein Nasenst\u00fcber.", "tokens": ["Wit\u00b7zig", "wie", "ein", "Na\u00b7sen\u00b7st\u00fc\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch ihr schnippisches Geschau", "tokens": ["Doch", "ihr", "schnip\u00b7pi\u00b7sches", "Ge\u00b7schau"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Spielte Hochmut und verneinte,", "tokens": ["Spiel\u00b7te", "Hoch\u00b7mut", "und", "ver\u00b7nein\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Ungefragt, was ich nicht meinte,", "tokens": ["Un\u00b7ge\u00b7fragt", ",", "was", "ich", "nicht", "mein\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "PWS", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sah in mir nur \u00bbKerl zur Frau\u00ab.", "tokens": ["Sah", "in", "mir", "nur", "\u00bb", "Kerl", "zur", "Frau", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "APPR", "PPER", "ADV", "$(", "NN", "APPRART", "NN", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Da\u00df ich beinah um sie weinte,", "tokens": ["Da\u00df", "ich", "bei\u00b7nah", "um", "sie", "wein\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ahnt sie nicht. Ihr eignes, scheues", "tokens": ["Ahnt", "sie", "nicht", ".", "Ihr", "eig\u00b7nes", ",", "scheu\u00b7es"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "PPOSAT", "ADJA", "$,", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Proletarisch, tierisch treues", "tokens": ["Pro\u00b7le\u00b7ta\u00b7risch", ",", "tie\u00b7risch", "treu\u00b7es"], "token_info": ["word", "punct", "word", "word"], "pos": ["NN", "$,", "ADJD", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Abwehr-Notgesicht", "tokens": ["Ab\u00b7wehr\u00b7Not\u00b7ge\u00b7sicht"], "token_info": ["word"], "pos": ["NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Kennt sie nicht.", "tokens": ["Kennt", "sie", "nicht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Hab mit ihr nicht angebandelt,", "tokens": ["Hab", "mit", "ihr", "nicht", "an\u00b7ge\u00b7ban\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPER", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Liebte, schwieg und ging.", "tokens": ["Lieb\u00b7te", ",", "schwieg", "und", "ging", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Klein-Dummdeifi, junges Ding!", "tokens": ["Klein\u00b7Dumm\u00b7dei\u00b7fi", ",", "jun\u00b7ges", "Ding", "!"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Du und ich! \u2013 Die Zeit verwandelt.", "tokens": ["Du", "und", "ich", "!", "\u2013", "Die", "Zeit", "ver\u00b7wan\u00b7delt", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "PPER", "$.", "$(", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Ob auch mir jemals jemand begegnete,", "tokens": ["Ob", "auch", "mir", "je\u00b7mals", "je\u00b7mand", "be\u00b7ge\u00b7gne\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der mich dumm fand und doch segnete? \u2013", "tokens": ["Der", "mich", "dumm", "fand", "und", "doch", "seg\u00b7ne\u00b7te", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPER", "ADJD", "VVFIN", "KON", "ADV", "VVFIN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Klein-Dummdeifi ging vor\u00fcber,", "tokens": ["Klein\u00b7Dumm\u00b7dei\u00b7fi", "ging", "vor\u00b7\u00fc\u00b7ber", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Witzig wie ein Nasenst\u00fcber.", "tokens": ["Wit\u00b7zig", "wie", "ein", "Na\u00b7sen\u00b7st\u00fc\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch ihr schnippisches Geschau", "tokens": ["Doch", "ihr", "schnip\u00b7pi\u00b7sches", "Ge\u00b7schau"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Spielte Hochmut und verneinte,", "tokens": ["Spiel\u00b7te", "Hoch\u00b7mut", "und", "ver\u00b7nein\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Ungefragt, was ich nicht meinte,", "tokens": ["Un\u00b7ge\u00b7fragt", ",", "was", "ich", "nicht", "mein\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "PWS", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sah in mir nur \u00bbKerl zur Frau\u00ab.", "tokens": ["Sah", "in", "mir", "nur", "\u00bb", "Kerl", "zur", "Frau", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "APPR", "PPER", "ADV", "$(", "NN", "APPRART", "NN", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Da\u00df ich beinah um sie weinte,", "tokens": ["Da\u00df", "ich", "bei\u00b7nah", "um", "sie", "wein\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ahnt sie nicht. Ihr eignes, scheues", "tokens": ["Ahnt", "sie", "nicht", ".", "Ihr", "eig\u00b7nes", ",", "scheu\u00b7es"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "PPOSAT", "ADJA", "$,", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Proletarisch, tierisch treues", "tokens": ["Pro\u00b7le\u00b7ta\u00b7risch", ",", "tie\u00b7risch", "treu\u00b7es"], "token_info": ["word", "punct", "word", "word"], "pos": ["NN", "$,", "ADJD", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Abwehr-Notgesicht", "tokens": ["Ab\u00b7wehr\u00b7Not\u00b7ge\u00b7sicht"], "token_info": ["word"], "pos": ["NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Kennt sie nicht.", "tokens": ["Kennt", "sie", "nicht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.8": {"line.1": {"text": "Hab mit ihr nicht angebandelt,", "tokens": ["Hab", "mit", "ihr", "nicht", "an\u00b7ge\u00b7ban\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPER", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Liebte, schwieg und ging.", "tokens": ["Lieb\u00b7te", ",", "schwieg", "und", "ging", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Klein-Dummdeifi, junges Ding!", "tokens": ["Klein\u00b7Dumm\u00b7dei\u00b7fi", ",", "jun\u00b7ges", "Ding", "!"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Du und ich! \u2013 Die Zeit verwandelt.", "tokens": ["Du", "und", "ich", "!", "\u2013", "Die", "Zeit", "ver\u00b7wan\u00b7delt", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "PPER", "$.", "$(", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Ob auch mir jemals jemand begegnete,", "tokens": ["Ob", "auch", "mir", "je\u00b7mals", "je\u00b7mand", "be\u00b7ge\u00b7gne\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der mich dumm fand und doch segnete? \u2013", "tokens": ["Der", "mich", "dumm", "fand", "und", "doch", "seg\u00b7ne\u00b7te", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPER", "ADJD", "VVFIN", "KON", "ADV", "VVFIN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}}}}