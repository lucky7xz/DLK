{"textgrid.poem.40333": {"metadata": {"author": {"name": "Dehmel, Richard Fedor Leopold", "birth": "N.A.", "death": "N.A."}, "title": "Venus Maculata", "genre": "verse", "period": "N.A.", "pub_year": 1891, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Drum komm, o komm, noch einmal schweigt", "tokens": ["Drum", "komm", ",", "o", "komm", ",", "noch", "ein\u00b7mal", "schweigt"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PAV", "VVFIN", "$,", "FM", "VVFIN", "$,", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so voll ins Feld, so voll bereit", "tokens": ["so", "voll", "ins", "Feld", ",", "so", "voll", "be\u00b7reit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPRART", "NN", "$,", "ADV", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "der Mond ins Feld; noch einmal zeigt", "tokens": ["der", "Mond", "ins", "Feld", ";", "noch", "ein\u00b7mal", "zeigt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN", "$.", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die weite Nacht,", "tokens": ["die", "wei\u00b7te", "Nacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "die zweite Nacht,", "tokens": ["die", "zwei\u00b7te", "Nacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "mir deine nackte Seligkeit.", "tokens": ["mir", "dei\u00b7ne", "nack\u00b7te", "Se\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "O komm, o komm, ich will dich sehn!", "tokens": ["O", "komm", ",", "o", "komm", ",", "ich", "will", "dich", "sehn", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "FM", "VVFIN", "$,", "PPER", "VMFIN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "rings rauscht der alte Eichenhain;", "tokens": ["rings", "rauscht", "der", "al\u00b7te", "Ei\u00b7chen\u00b7hain", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die langen Wiesenhalme stehn", "tokens": ["die", "lan\u00b7gen", "Wie\u00b7sen\u00b7hal\u00b7me", "stehn"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "so still, so weich", "tokens": ["so", "still", ",", "so", "weich"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "am kleinen Teich,", "tokens": ["am", "klei\u00b7nen", "Teich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "und schimmernd tauchen wir hinein.", "tokens": ["und", "schim\u00b7mernd", "tau\u00b7chen", "wir", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und schimmernd, schimmernd heb'ich dich", "tokens": ["Und", "schim\u00b7mernd", ",", "schim\u00b7mernd", "he\u00b7b'\u00b7ich", "dich"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "ADJD", "ADJD", "PPER"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "heraus ins dunkelgr\u00fcne Kraut;", "tokens": ["he\u00b7raus", "ins", "dun\u00b7kel\u00b7gr\u00fc\u00b7ne", "Kraut", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "dein schwarzes Haar umrieselt mich,", "tokens": ["dein", "schwar\u00b7zes", "Haar", "um\u00b7rie\u00b7selt", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der Tau wird warm,", "tokens": ["der", "Tau", "wird", "warm", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "und Arm um Arm", "tokens": ["und", "Arm", "um", "Arm"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NN", "APPR", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "erkennt den Br\u00e4utigam die Braut.", "tokens": ["er\u00b7kennt", "den", "Br\u00e4u\u00b7ti\u00b7gam", "die", "Braut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und dann \u2013 o dann \u2013 o flieh! \u2013 denn dann:", "tokens": ["Und", "dann", "\u2013", "o", "dann", "\u2013", "o", "flieh", "!", "\u2013", "denn", "dann", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "FM", "ADV", "$(", "FM", "FM", "$.", "$(", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wir hatten Schoo\u00df in Schoo\u00df geruht:", "tokens": ["wir", "hat\u00b7ten", "Schoo\u00df", "in", "Schoo\u00df", "ge\u00b7ruht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "von einer wei\u00dfen Bl\u00fcte rann,", "tokens": ["von", "ei\u00b7ner", "wei\u00b7\u00dfen", "Bl\u00fc\u00b7te", "rann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "du sahst es nicht,", "tokens": ["du", "sahst", "es", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "im bleichen Licht", "tokens": ["im", "blei\u00b7chen", "Licht"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "ein Tropfen Blut \u2013 Dein Tropfen Blut \u2013", "tokens": ["ein", "Trop\u00b7fen", "Blut", "\u2013", "Dein", "Trop\u00b7fen", "Blut", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "PPOSAT", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}}, "stanza.6": {"line.1": {"text": "Eitle R\u00fchrung, frech Bedauern,", "tokens": ["Eit\u00b7le", "R\u00fch\u00b7rung", ",", "frech", "Be\u00b7dau\u00b7ern", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "R\u00e4ubermitleid nach dem Raube.", "tokens": ["R\u00e4u\u00b7ber\u00b7mit\u00b7leid", "nach", "dem", "Rau\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Oder war's ein echt Erschauern?", "tokens": ["O\u00b7der", "wa\u00b7r's", "ein", "echt", "Er\u00b7schau\u00b7ern", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Narr, was fragst du \u2013 glaube! glaube!", "tokens": ["Narr", ",", "was", "fragst", "du", "\u2013", "glau\u00b7be", "!", "glau\u00b7be", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "PWS", "VVFIN", "PPER", "$(", "VVFIN", "$.", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Und da lass ich mich von schalen", "tokens": ["Und", "da", "lass", "ich", "mich", "von", "scha\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "APPR", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Skrupeln bis aufs Blut zerqu\u00e4len?", "tokens": ["Skru\u00b7peln", "bis", "aufs", "Blut", "zer\u00b7qu\u00e4\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "APPRART", "NN", "VVINF", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "hier, wo hochher Sterne strahlen,", "tokens": ["hier", ",", "wo", "hoch\u00b7her", "Ster\u00b7ne", "strah\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "die zu frischem Mut mich st\u00e4hlen!", "tokens": ["die", "zu", "fri\u00b7schem", "Mut", "mich", "st\u00e4h\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKZU", "ADJA", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Nein, ich will mir's k\u00fchn bekennen:", "tokens": ["Nein", ",", "ich", "will", "mir's", "k\u00fchn", "be\u00b7ken\u00b7nen", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VMFIN", "NE", "ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "auch die L\u00fcste, die wir schuldbewu\u00dft", "tokens": ["auch", "die", "L\u00fcs\u00b7te", ",", "die", "wir", "schuld\u00b7be\u00b7wu\u00dft"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Unnatur und Unzucht nennen,", "tokens": ["Un\u00b7na\u00b7tur", "und", "Un\u00b7zucht", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVINF", "$,"], "meter": "+-+-+---", "measure": "unknown.measure.tri"}, "line.4": {"text": "sind Natur und neue Z\u00fcchtungslust \u2013", "tokens": ["sind", "Na\u00b7tur", "und", "neu\u00b7e", "Z\u00fcch\u00b7tungs\u00b7lust", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "ADJA", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "ich, der selber einst tiefinnen", "tokens": ["ich", ",", "der", "sel\u00b7ber", "einst", "tie\u00b7fin\u00b7nen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "ADV", "ADV", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "nur empor nach freierer Menschheit \u00e4chzte,", "tokens": ["nur", "em\u00b7por", "nach", "frei\u00b7e\u00b7rer", "Menschheit", "\u00e4chz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+---+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "w\u00e4hrend meine tierischen Sinne", "tokens": ["w\u00e4h\u00b7rend", "mei\u00b7ne", "tie\u00b7ri\u00b7schen", "Sin\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "doch nach Dir tyrannisch lechzten,", "tokens": ["doch", "nach", "Dir", "ty\u00b7ran\u00b7nisch", "lechz\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Drum komm, o komm, noch einmal schweigt", "tokens": ["Drum", "komm", ",", "o", "komm", ",", "noch", "ein\u00b7mal", "schweigt"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PAV", "VVFIN", "$,", "FM", "VVFIN", "$,", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so voll ins Feld, so voll bereit", "tokens": ["so", "voll", "ins", "Feld", ",", "so", "voll", "be\u00b7reit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPRART", "NN", "$,", "ADV", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "der Mond ins Feld; noch einmal zeigt", "tokens": ["der", "Mond", "ins", "Feld", ";", "noch", "ein\u00b7mal", "zeigt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN", "$.", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die weite Nacht,", "tokens": ["die", "wei\u00b7te", "Nacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "die zweite Nacht,", "tokens": ["die", "zwei\u00b7te", "Nacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "mir deine nackte Seligkeit.", "tokens": ["mir", "dei\u00b7ne", "nack\u00b7te", "Se\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "O komm, o komm, ich will dich sehn!", "tokens": ["O", "komm", ",", "o", "komm", ",", "ich", "will", "dich", "sehn", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "FM", "VVFIN", "$,", "PPER", "VMFIN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "rings rauscht der alte Eichenhain;", "tokens": ["rings", "rauscht", "der", "al\u00b7te", "Ei\u00b7chen\u00b7hain", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die langen Wiesenhalme stehn", "tokens": ["die", "lan\u00b7gen", "Wie\u00b7sen\u00b7hal\u00b7me", "stehn"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "so still, so weich", "tokens": ["so", "still", ",", "so", "weich"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "am kleinen Teich,", "tokens": ["am", "klei\u00b7nen", "Teich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "und schimmernd tauchen wir hinein.", "tokens": ["und", "schim\u00b7mernd", "tau\u00b7chen", "wir", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Und schimmernd, schimmernd heb'ich dich", "tokens": ["Und", "schim\u00b7mernd", ",", "schim\u00b7mernd", "he\u00b7b'\u00b7ich", "dich"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "ADJD", "ADJD", "PPER"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "heraus ins dunkelgr\u00fcne Kraut;", "tokens": ["he\u00b7raus", "ins", "dun\u00b7kel\u00b7gr\u00fc\u00b7ne", "Kraut", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "dein schwarzes Haar umrieselt mich,", "tokens": ["dein", "schwar\u00b7zes", "Haar", "um\u00b7rie\u00b7selt", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der Tau wird warm,", "tokens": ["der", "Tau", "wird", "warm", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "und Arm um Arm", "tokens": ["und", "Arm", "um", "Arm"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NN", "APPR", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "erkennt den Br\u00e4utigam die Braut.", "tokens": ["er\u00b7kennt", "den", "Br\u00e4u\u00b7ti\u00b7gam", "die", "Braut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und dann \u2013 o dann \u2013 o flieh! \u2013 denn dann:", "tokens": ["Und", "dann", "\u2013", "o", "dann", "\u2013", "o", "flieh", "!", "\u2013", "denn", "dann", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "FM", "ADV", "$(", "FM", "FM", "$.", "$(", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wir hatten Schoo\u00df in Schoo\u00df geruht:", "tokens": ["wir", "hat\u00b7ten", "Schoo\u00df", "in", "Schoo\u00df", "ge\u00b7ruht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "von einer wei\u00dfen Bl\u00fcte rann,", "tokens": ["von", "ei\u00b7ner", "wei\u00b7\u00dfen", "Bl\u00fc\u00b7te", "rann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "du sahst es nicht,", "tokens": ["du", "sahst", "es", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "im bleichen Licht", "tokens": ["im", "blei\u00b7chen", "Licht"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "ein Tropfen Blut \u2013 Dein Tropfen Blut \u2013", "tokens": ["ein", "Trop\u00b7fen", "Blut", "\u2013", "Dein", "Trop\u00b7fen", "Blut", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "PPOSAT", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}}, "stanza.15": {"line.1": {"text": "Eitle R\u00fchrung, frech Bedauern,", "tokens": ["Eit\u00b7le", "R\u00fch\u00b7rung", ",", "frech", "Be\u00b7dau\u00b7ern", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "R\u00e4ubermitleid nach dem Raube.", "tokens": ["R\u00e4u\u00b7ber\u00b7mit\u00b7leid", "nach", "dem", "Rau\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Oder war's ein echt Erschauern?", "tokens": ["O\u00b7der", "wa\u00b7r's", "ein", "echt", "Er\u00b7schau\u00b7ern", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Narr, was fragst du \u2013 glaube! glaube!", "tokens": ["Narr", ",", "was", "fragst", "du", "\u2013", "glau\u00b7be", "!", "glau\u00b7be", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "PWS", "VVFIN", "PPER", "$(", "VVFIN", "$.", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Und da lass ich mich von schalen", "tokens": ["Und", "da", "lass", "ich", "mich", "von", "scha\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "APPR", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Skrupeln bis aufs Blut zerqu\u00e4len?", "tokens": ["Skru\u00b7peln", "bis", "aufs", "Blut", "zer\u00b7qu\u00e4\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "APPRART", "NN", "VVINF", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "hier, wo hochher Sterne strahlen,", "tokens": ["hier", ",", "wo", "hoch\u00b7her", "Ster\u00b7ne", "strah\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "die zu frischem Mut mich st\u00e4hlen!", "tokens": ["die", "zu", "fri\u00b7schem", "Mut", "mich", "st\u00e4h\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKZU", "ADJA", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Nein, ich will mir's k\u00fchn bekennen:", "tokens": ["Nein", ",", "ich", "will", "mir's", "k\u00fchn", "be\u00b7ken\u00b7nen", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VMFIN", "NE", "ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "auch die L\u00fcste, die wir schuldbewu\u00dft", "tokens": ["auch", "die", "L\u00fcs\u00b7te", ",", "die", "wir", "schuld\u00b7be\u00b7wu\u00dft"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Unnatur und Unzucht nennen,", "tokens": ["Un\u00b7na\u00b7tur", "und", "Un\u00b7zucht", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVINF", "$,"], "meter": "+-+-+---", "measure": "unknown.measure.tri"}, "line.4": {"text": "sind Natur und neue Z\u00fcchtungslust \u2013", "tokens": ["sind", "Na\u00b7tur", "und", "neu\u00b7e", "Z\u00fcch\u00b7tungs\u00b7lust", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "ADJA", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.18": {"line.1": {"text": "ich, der selber einst tiefinnen", "tokens": ["ich", ",", "der", "sel\u00b7ber", "einst", "tie\u00b7fin\u00b7nen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "ADV", "ADV", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "nur empor nach freierer Menschheit \u00e4chzte,", "tokens": ["nur", "em\u00b7por", "nach", "frei\u00b7e\u00b7rer", "Menschheit", "\u00e4chz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+---+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "w\u00e4hrend meine tierischen Sinne", "tokens": ["w\u00e4h\u00b7rend", "mei\u00b7ne", "tie\u00b7ri\u00b7schen", "Sin\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "doch nach Dir tyrannisch lechzten,", "tokens": ["doch", "nach", "Dir", "ty\u00b7ran\u00b7nisch", "lechz\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}