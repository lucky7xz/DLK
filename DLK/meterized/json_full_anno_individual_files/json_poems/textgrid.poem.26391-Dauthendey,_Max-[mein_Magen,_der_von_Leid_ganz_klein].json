{"textgrid.poem.26391": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[mein Magen, der von Leid ganz klein]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein Magen, der von Leid ganz klein,", "tokens": ["Mein", "Ma\u00b7gen", ",", "der", "von", "Leid", "ganz", "klein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "APPR", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Lie\u00df kaum den Durst und Hunger ein;", "tokens": ["Lie\u00df", "kaum", "den", "Durst", "und", "Hun\u00b7ger", "ein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Mein Geld war l\u00e4ngst in fremden H\u00e4nden,", "tokens": ["Mein", "Geld", "war", "l\u00e4ngst", "in", "frem\u00b7den", "H\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich lebte schon mit lahmen Lenden;", "tokens": ["Ich", "leb\u00b7te", "schon", "mit", "lah\u00b7men", "Len\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Lehrreich ist zwar Philosophie,", "tokens": ["Lehr\u00b7reich", "ist", "zwar", "Phi\u00b7lo\u00b7so\u00b7phie", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sehr s\u00e4ttigend ist sie doch nie,", "tokens": ["Sehr", "s\u00e4t\u00b7ti\u00b7gend", "ist", "sie", "doch", "nie", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "ADV", "ADV", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.4": {"line.1": {"text": "Und niemand einem etwas schenkt,", "tokens": ["Und", "nie\u00b7mand", "ei\u00b7nem", "et\u00b7was", "schenkt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ART", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr all das Geld, das man sich denkt.", "tokens": ["F\u00fcr", "all", "das", "Geld", ",", "das", "man", "sich", "denkt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ART", "NN", "$,", "PRELS", "PIS", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Wollt' mich vom Leben nicht entfernen,", "tokens": ["Wollt'", "mich", "vom", "Le\u00b7ben", "nicht", "ent\u00b7fer\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fabrizierte Nachtlichtlaternen,", "tokens": ["Fab\u00b7ri\u00b7zier\u00b7te", "Nacht\u00b7licht\u00b7la\u00b7ter\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Laternen, sch\u00f6n aus buntem Glas,", "tokens": ["La\u00b7ter\u00b7nen", ",", "sch\u00f6n", "aus", "bun\u00b7tem", "Glas", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Leuchten zu Haus und auf der Stra\u00df',", "tokens": ["Leuch\u00b7ten", "zu", "Haus", "und", "auf", "der", "Stra\u00df'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.7": {"line.1": {"text": "Darauf male ich manchen Reim,", "tokens": ["Da\u00b7rauf", "ma\u00b7le", "ich", "man\u00b7chen", "Reim", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Leute leuchten sich damit heim.", "tokens": ["Leu\u00b7te", "leuch\u00b7ten", "sich", "da\u00b7mit", "heim", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "PAV", "PTKVZ", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.8": {"line.1": {"text": "Doch \u00f6fters leg' ich mir die Karten,", "tokens": ["Doch", "\u00f6f\u00b7ters", "leg'", "ich", "mir", "die", "Kar\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn w\u00fcnscht man was, tut man's erwarten.", "tokens": ["Denn", "w\u00fcnscht", "man", "was", ",", "tut", "man's", "er\u00b7war\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PWS", "$,", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.9": {"line.1": {"text": "Noch einmal w\u00fcnsch' Frau K\u00f6nigin", "tokens": ["Noch", "ein\u00b7mal", "w\u00fcn\u00b7sch'", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "NN", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich mir an meinen Busen hin.", "tokens": ["Ich", "mir", "an", "mei\u00b7nen", "Bu\u00b7sen", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Nur eine Nacht, voll von Vergessen,", "tokens": ["Nur", "ei\u00b7ne", "Nacht", ",", "voll", "von", "Ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll sie sich liebend mit mir messen,", "tokens": ["Soll", "sie", "sich", "lie\u00b7bend", "mit", "mir", "mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "ADJD", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Und dann soll kommen was da will,", "tokens": ["Und", "dann", "soll", "kom\u00b7men", "was", "da", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "VVINF", "PWS", "ADV", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben bringt ja stets so viel. \u2013", "tokens": ["Das", "Le\u00b7ben", "bringt", "ja", "stets", "so", "viel", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Wie man von Loreley es wei\u00df,", "tokens": ["Wie", "man", "von", "Lo\u00b7re\u00b7ley", "es", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Haar t\u00f6tete gern mit Flei\u00df.", "tokens": ["Ihr", "Haar", "t\u00f6\u00b7te\u00b7te", "gern", "mit", "Flei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Tat sie beim K\u00e4mmen auch noch singen,", "tokens": ["Tat", "sie", "beim", "K\u00e4m\u00b7men", "auch", "noch", "sin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gleich ganze Schiffe untergingen.", "tokens": ["Gleich", "gan\u00b7ze", "Schif\u00b7fe", "un\u00b7ter\u00b7gin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Jetzt wie ein Spuk es \u00f6fters war,", "tokens": ["Jetzt", "wie", "ein", "Spuk", "es", "\u00f6f\u00b7ters", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "NN", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Braun bin ich und fand blondes Haar,", "tokens": ["Braun", "bin", "ich", "und", "fand", "blon\u00b7des", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "KON", "VVFIN", "ADJA", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.15": {"line.1": {"text": "Fand's noch im \u00c4rmelfutter h\u00e4ngen", "tokens": ["Fan\u00b7d's", "noch", "im", "\u00c4r\u00b7mel\u00b7fut\u00b7ter", "h\u00e4n\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "APPRART", "NN", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Und weiblich waren seine L\u00e4ngen.", "tokens": ["Und", "weib\u00b7lich", "wa\u00b7ren", "sei\u00b7ne", "L\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Die Uhr blieb mir vor Schreck dann stehn,", "tokens": ["Die", "Uhr", "blieb", "mir", "vor", "Schreck", "dann", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sehns\u00fcchtig tat ich um mich sehn", "tokens": ["Sehn\u00b7s\u00fcch\u00b7tig", "tat", "ich", "um", "mich", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Das Goldh\u00e4rlein flog zitternd hoch,", "tokens": ["Das", "Gold\u00b7h\u00e4r\u00b7lein", "flog", "zit\u00b7ternd", "hoch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Kopf dazu fehlte jedoch.", "tokens": ["Der", "Kopf", "da\u00b7zu", "fehl\u00b7te", "je\u00b7doch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VVFIN", "ADV", "$."], "meter": "-+--+---", "measure": "iambic.di.relaxed"}}, "stanza.18": {"line.1": {"text": "Tief seufzend fiel ich jedenfalls", "tokens": ["Tief", "seuf\u00b7zend", "fiel", "ich", "je\u00b7den\u00b7falls"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVPP", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem goldnen H\u00e4rlein um den Hals.", "tokens": ["Dem", "gold\u00b7nen", "H\u00e4r\u00b7lein", "um", "den", "Hals", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "So kommt sie stellenweis nur an", "tokens": ["So", "kommt", "sie", "stel\u00b7len\u00b7weis", "nur", "an"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Schattenbild zum Schattenmann.", "tokens": ["Als", "Schat\u00b7ten\u00b7bild", "zum", "Schat\u00b7ten\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Guckt mir der Abend in die Fenster,", "tokens": ["Guckt", "mir", "der", "A\u00b7bend", "in", "die", "Fens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nahn glucksend die Liebesgespenster,", "tokens": ["Nahn", "gluck\u00b7send", "die", "Lie\u00b7bes\u00b7ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.21": {"line.1": {"text": "Schneuzend brauch' ich dann Taschent\u00fccher,", "tokens": ["Schneu\u00b7zend", "brauch'", "ich", "dann", "Ta\u00b7schen\u00b7t\u00fc\u00b7cher", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "St\u00f6hnend wie \u00fcber sch\u00f6ne B\u00fccher.", "tokens": ["St\u00f6h\u00b7nend", "wie", "\u00fc\u00b7ber", "sch\u00f6\u00b7ne", "B\u00fc\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "APPR", "ADJA", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.22": {"line.1": {"text": "Seh' sacht Frau K\u00f6nigin entstehn,", "tokens": ["Seh'", "sacht", "Frau", "K\u00f6\u00b7ni\u00b7gin", "ent\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "NN", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Aus der Tapet ins Zimmer gehn,", "tokens": ["Aus", "der", "Ta\u00b7pet", "ins", "Zim\u00b7mer", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Verr\u00fcckt wird dann das ganz Haus,", "tokens": ["Ver\u00b7r\u00fcckt", "wird", "dann", "das", "ganz", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ADV", "ART", "ADV", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Schwimmt als ein Schlo\u00df ins Meer hinaus,", "tokens": ["Schwimmt", "als", "ein", "Schlo\u00df", "ins", "Meer", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "APPRART", "NN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Die Pf\u00fctzent\u00fcmpel auf der Stra\u00dfe,", "tokens": ["Die", "Pf\u00fct\u00b7zen\u00b7t\u00fcm\u00b7pel", "auf", "der", "Stra\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die werden Austerb\u00e4nke, blasse,", "tokens": ["Die", "wer\u00b7den", "Aus\u00b7ter\u00b7b\u00e4n\u00b7ke", ",", "blas\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Mein Herz schl\u00e4gt schwerer als ein Gong", "tokens": ["Mein", "Herz", "schl\u00e4gt", "schwe\u00b7rer", "als", "ein", "Gong"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6nigin tritt zum Schlo\u00dfbalkon,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "tritt", "zum", "Schlo\u00df\u00b7bal\u00b7kon", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.26": {"line.1": {"text": "Wo sie dem Meer sich zeigen l\u00e4\u00dft,", "tokens": ["Wo", "sie", "dem", "Meer", "sich", "zei\u00b7gen", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "PRF", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alle Fische halten Fest.", "tokens": ["Und", "al\u00b7le", "Fi\u00b7sche", "hal\u00b7ten", "Fest", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Fische schnellen zum Speisesaal", "tokens": ["Fi\u00b7sche", "schnel\u00b7len", "zum", "Spei\u00b7se\u00b7saal"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPRART", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Auf Silberplatten ohne Zahl,", "tokens": ["Auf", "Sil\u00b7ber\u00b7plat\u00b7ten", "oh\u00b7ne", "Zahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Hirsche vom Walde springen hin", "tokens": ["Hir\u00b7sche", "vom", "Wal\u00b7de", "sprin\u00b7gen", "hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "VVFIN", "PTKVZ"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Auf Monsterplatten, schwer aus Zinn,", "tokens": ["Auf", "Mons\u00b7ter\u00b7plat\u00b7ten", ",", "schwer", "aus", "Zinn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Und Schafe tuen lieblich bl\u00f6ken", "tokens": ["Und", "Scha\u00b7fe", "tu\u00b7en", "lieb\u00b7lich", "bl\u00f6\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVFIN", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und nicht wider den Bratspie\u00df l\u00f6ken,", "tokens": ["Und", "nicht", "wi\u00b7der", "den", "Brat\u00b7spie\u00df", "l\u00f6\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.30": {"line.1": {"text": "Alle sind Frau K\u00f6nigin gut", "tokens": ["Al\u00b7le", "sind", "Frau", "K\u00f6\u00b7ni\u00b7gin", "gut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "NN", "NN", "ADJD"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und braten sich aus Liebesglut.", "tokens": ["Und", "bra\u00b7ten", "sich", "aus", "Lie\u00b7bes\u00b7glut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Und jedes Bein vom Speisetisch,", "tokens": ["Und", "je\u00b7des", "Bein", "vom", "Spei\u00b7se\u00b7tisch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bl\u00fcht flott als Weinstock gr\u00fcn und frisch", "tokens": ["Bl\u00fcht", "flott", "als", "Wein\u00b7stock", "gr\u00fcn", "und", "frisch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "KOKOM", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Und tr\u00e4gt schon zum Dessert die Trauben;", "tokens": ["Und", "tr\u00e4gt", "schon", "zum", "Des\u00b7sert", "die", "Trau\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man wird das wahrscheinlich kaum glauben.", "tokens": ["Man", "wird", "das", "wahr\u00b7schein\u00b7lich", "kaum", "glau\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "ADJD", "ADV", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.33": {"line.1": {"text": "K\u00f6nigin spricht. \u00bbGl\u00fcck war Geruch,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "spricht", ".", "\u00bb", "Gl\u00fcck", "war", "Ge\u00b7ruch", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "NN", "VAFIN", "NN", "$,"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "War wie etwas im Taschentuch,", "tokens": ["War", "wie", "et\u00b7was", "im", "Ta\u00b7schen\u00b7tuch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Gl\u00fcck lag tief vor uns auf dem Bauch", "tokens": ["Gl\u00fcck", "lag", "tief", "vor", "uns", "auf", "dem", "Bauch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ADJD", "APPR", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und r\u00e4ucherte wie Weiherauch,", "tokens": ["Und", "r\u00e4u\u00b7cher\u00b7te", "wie", "Wei\u00b7her\u00b7auch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Gl\u00fccksgeruch badete mein Blut,", "tokens": ["Gl\u00fccks\u00b7ge\u00b7ruch", "ba\u00b7de\u00b7te", "mein", "Blut", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Ich roch einst selber mir so gut,", "tokens": ["Ich", "roch", "einst", "sel\u00b7ber", "mir", "so", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Lie\u00df Sonne nach Belieben scheinen,", "tokens": ["Lie\u00df", "Son\u00b7ne", "nach", "Be\u00b7lie\u00b7ben", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Selbst Meerrettich machte nicht weinen.\u00ab", "tokens": ["Selbst", "Meer\u00b7ret\u00b7tich", "mach\u00b7te", "nicht", "wei\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.37": {"line.1": {"text": "Sie tut die Sonn' vom Nagel nehmen,", "tokens": ["Sie", "tut", "die", "Sonn'", "vom", "Na\u00b7gel", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPRART", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Damit die Stern' als Lampen k\u00e4men.", "tokens": ["Da\u00b7mit", "die", "Stern'", "als", "Lam\u00b7pen", "k\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Wir tuen dann den Mond aufh\u00e4ngen", "tokens": ["Wir", "tu\u00b7en", "dann", "den", "Mond", "auf\u00b7h\u00e4n\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und drunter Lipp' an Lippe dr\u00e4ngen.", "tokens": ["Und", "drun\u00b7ter", "Lipp'", "an", "Lip\u00b7pe", "dr\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "NN", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Die Uhr schl\u00e4gt wie die Nachtigall", "tokens": ["Die", "Uhr", "schl\u00e4gt", "wie", "die", "Nach\u00b7ti\u00b7gall"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sagt nicht mehr der Stunden Zahl ...", "tokens": ["Und", "sagt", "nicht", "mehr", "der", "Stun\u00b7den", "Zahl", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "So tr\u00e4um' ich n\u00e4chtlich ins Nachtlicht;", "tokens": ["So", "tr\u00e4um'", "ich", "n\u00e4cht\u00b7lich", "ins", "Nacht\u00b7licht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Seufzend geht's aus, das Zimmer riecht", "tokens": ["Seuf\u00b7zend", "geht's", "aus", ",", "das", "Zim\u00b7mer", "riecht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PTKVZ", "$,", "ART", "NN", "VVFIN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.41": {"line.1": {"text": "Nach weichem \u00d6l und warmem Rauch,", "tokens": ["Nach", "wei\u00b7chem", "\u00d6l", "und", "war\u00b7mem", "Rauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mond lehnt mir leer und k\u00fchl am Bauch,", "tokens": ["Mond", "lehnt", "mir", "leer", "und", "k\u00fchl", "am", "Bauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.42": {"line.1": {"text": "Aus Zeitungspapier scheint der Mond,", "tokens": ["Aus", "Zei\u00b7tungs\u00b7pa\u00b7pier", "scheint", "der", "Mond", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Alt, da\u00df sich nichts zu lesen lohnt.", "tokens": ["Alt", ",", "da\u00df", "sich", "nichts", "zu", "le\u00b7sen", "lohnt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "KOUS", "PRF", "PIS", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Die H\u00e4user, Droschken, Ladenfenster", "tokens": ["Die", "H\u00e4u\u00b7ser", ",", "Droschken", ",", "La\u00b7den\u00b7fens\u00b7ter"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "NN", "$,", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sind nur Pappendeckelgespenster,", "tokens": ["Sind", "nur", "Pap\u00b7pen\u00b7de\u00b7ckel\u00b7ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.44": {"line.1": {"text": "Gleich Papierp\u00fcppchen anzuschaun,", "tokens": ["Gleich", "Pa\u00b7pier\u00b7p\u00fcpp\u00b7chen", "an\u00b7zu\u00b7schaun", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "T\u00e4nzeln vorbei Herren und Fraun.", "tokens": ["T\u00e4n\u00b7zeln", "vor\u00b7bei", "Her\u00b7ren", "und", "Fraun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.45": {"line.1": {"text": "Und geh' ich fr\u00fch zur Stadt hinaus,", "tokens": ["Und", "geh'", "ich", "fr\u00fch", "zur", "Stadt", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "APPRART", "NN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sehn Wolken wie Nachtm\u00fctzen aus.", "tokens": ["Sehn", "Wol\u00b7ken", "wie", "Nacht\u00b7m\u00fct\u00b7zen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KOKOM", "NN", "PTKVZ", "$."], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}}, "stanza.46": {"line.1": {"text": "Es g\u00e4hnen B\u00e4um', Wolken, Erdscholl',", "tokens": ["Es", "g\u00e4h\u00b7nen", "B\u00e4um'", ",", "Wol\u00b7ken", ",", "Erd\u00b7scholl'", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Schafherden g\u00e4hnen wei\u00df aus Woll',", "tokens": ["Schaf\u00b7her\u00b7den", "g\u00e4h\u00b7nen", "wei\u00df", "aus", "Woll'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "VVFIN", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Es g\u00e4hnt das Feuer in der Schmied',", "tokens": ["Es", "g\u00e4hnt", "das", "Feu\u00b7er", "in", "der", "Schmied'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Riesenschlaf aus allem zieht,", "tokens": ["Ein", "Rie\u00b7sen\u00b7schlaf", "aus", "al\u00b7lem", "zieht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Der Pflug im Acker f\u00e4llt um still,", "tokens": ["Der", "Pflug", "im", "A\u00b7cker", "f\u00e4llt", "um", "still", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VVFIN", "APPR", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil Gaul und Bauer g\u00e4hnen will,", "tokens": ["Weil", "Gaul", "und", "Bau\u00b7er", "g\u00e4h\u00b7nen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Der Bach sich dicht ans Ufer lehnt,", "tokens": ["Der", "Bach", "sich", "dicht", "ans", "U\u00b7fer", "lehnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADJD", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wasser, Luft, Erde, Feuer g\u00e4hnt,", "tokens": ["Was\u00b7ser", ",", "Luft", ",", "Er\u00b7de", ",", "Feu\u00b7er", "g\u00e4hnt", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "VVFIN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.50": {"line.1": {"text": "T\u00fcrme kein Gleichgewicht mehr haben,", "tokens": ["T\u00fcr\u00b7me", "kein", "Gleich\u00b7ge\u00b7wicht", "mehr", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "ADV", "VAFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "G\u00e4hnend fallen auf mich die Raben,", "tokens": ["G\u00e4h\u00b7nend", "fal\u00b7len", "auf", "mich", "die", "Ra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "PPER", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.51": {"line.1": {"text": "Seh' alle Ding' im Schlaf fortschweben,", "tokens": ["Seh'", "al\u00b7le", "Ding'", "im", "Schlaf", "fort\u00b7schwe\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Frage mich: \u00bbBin ich noch am Leben?\u00ab", "tokens": ["Fra\u00b7ge", "mich", ":", "\u00bb", "Bin", "ich", "noch", "am", "Le\u00b7ben", "?", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "$.", "$(", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$.", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.52": {"line.1": {"text": "Vielleicht sind's tausend Jahre bald,", "tokens": ["Viel\u00b7leicht", "sin\u00b7d's", "tau\u00b7send", "Jah\u00b7re", "bald", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "CARD", "NN", "ADV", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Seit ich einschlief und schlief mich alt.", "tokens": ["Seit", "ich", "ein\u00b7schlief", "und", "schlief", "mich", "alt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "KON", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "M\u00f6chts gern noch allen Leuten sagen,", "tokens": ["M\u00f6chts", "gern", "noch", "al\u00b7len", "Leu\u00b7ten", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sch\u00f6n's war, Liebe zu ertragen.", "tokens": ["Wie", "sch\u00f6n's", "war", ",", "Lie\u00b7be", "zu", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "$,", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Die Liebe ich allm\u00e4chtig fand,", "tokens": ["Die", "Lie\u00b7be", "ich", "all\u00b7m\u00e4ch\u00b7tig", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tod ist nur interessant.", "tokens": ["Der", "Tod", "ist", "nur", "in\u00b7ter\u00b7es\u00b7sant", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.55": {"line.1": {"text": "Werden mir dunkel jetzt die Fenster,", "tokens": ["Wer\u00b7den", "mir", "dun\u00b7kel", "jetzt", "die", "Fens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Seh' ich im Tode nicht Gespenster.", "tokens": ["Seh'", "ich", "im", "To\u00b7de", "nicht", "Ge\u00b7spens\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "PTKNEG", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Mache nur still die Augen zu,", "tokens": ["Ma\u00b7che", "nur", "still", "die", "Au\u00b7gen", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADJD", "ART", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Weh tat noch keinem Mensch die Ruh'.", "tokens": ["Weh", "tat", "noch", "kei\u00b7nem", "Mensch", "die", "Ruh'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "PIAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Das Essen uns nur teilweis z\u00fcndet,", "tokens": ["Das", "Es\u00b7sen", "uns", "nur", "teil\u00b7weis", "z\u00fcn\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn es uns so behaglich r\u00fcndet.", "tokens": ["Wenn", "es", "uns", "so", "be\u00b7hag\u00b7lich", "r\u00fcn\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Weisheit erquickt, wenn sie uns pa\u00dft,", "tokens": ["Weis\u00b7heit", "er\u00b7quickt", ",", "wenn", "sie", "uns", "pa\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Man f\u00fchlt sich blendend angefa\u00dft.", "tokens": ["Man", "f\u00fchlt", "sich", "blen\u00b7dend", "an\u00b7ge\u00b7fa\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Doch Liebe uns ganz voll entz\u00fcckt,", "tokens": ["Doch", "Lie\u00b7be", "uns", "ganz", "voll", "ent\u00b7z\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verliebt f\u00fchlt sich der Floh entr\u00fcckt.", "tokens": ["Ver\u00b7liebt", "f\u00fchlt", "sich", "der", "Floh", "ent\u00b7r\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PRF", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Die Liebe ist im Weltall Trumpf,", "tokens": ["Die", "Lie\u00b7be", "ist", "im", "Wel\u00b7tall", "Trumpf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch unten bei dem Frosch im Sumpf.", "tokens": ["Auch", "un\u00b7ten", "bei", "dem", "Frosch", "im", "Sumpf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Verliebtsein ist das Himmelreich,", "tokens": ["Ver\u00b7liebt\u00b7sein", "ist", "das", "Him\u00b7mel\u00b7reich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da sind sich Mensch, Tier, Pflanze gleich.", "tokens": ["Da", "sind", "sich", "Mensch", ",", "Tier", ",", "Pflan\u00b7ze", "gleich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "NN", "$,", "NN", "$,", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.62": {"line.1": {"text": "Verliebt geht man aus sich heraus,", "tokens": ["Ver\u00b7liebt", "geht", "man", "aus", "sich", "he\u00b7raus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PIS", "APPR", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Pflanze, Tier, Mensch sehn prachtvoll aus.", "tokens": ["Pflan\u00b7ze", ",", "Tier", ",", "Mensch", "sehn", "pracht\u00b7voll", "aus", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.63": {"line.1": {"text": "Liebe im Mittelpunkt dasteht,", "tokens": ["Lie\u00b7be", "im", "Mit\u00b7tel\u00b7punkt", "das\u00b7teht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Die ganze Welt sich darum dreht.", "tokens": ["Die", "gan\u00b7ze", "Welt", "sich", "da\u00b7rum", "dreht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Und tut ein altes Herz verderben,", "tokens": ["Und", "tut", "ein", "al\u00b7tes", "Herz", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Um neu zu lieben, kann es sterben.", "tokens": ["Um", "neu", "zu", "lie\u00b7ben", ",", "kann", "es", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "ADJD", "PTKZU", "VVINF", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Doch mach' ich aus dem Tod kein Fest,", "tokens": ["Doch", "mach'", "ich", "aus", "dem", "Tod", "kein", "Fest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da man sich gern beweinen l\u00e4\u00dft.", "tokens": ["Da", "man", "sich", "gern", "be\u00b7wei\u00b7nen", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PRF", "ADV", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Und nicht wie sterbend ein C\u00e4sar,", "tokens": ["Und", "nicht", "wie", "ster\u00b7bend", "ein", "C\u00e4\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "KOKOM", "ADJD", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Befiehlt Applaus der Balthasar.", "tokens": ["Be\u00b7fiehlt", "Ap\u00b7plaus", "der", "Balt\u00b7ha\u00b7sar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "Ich ruf', wenn ich den Leib fortschiebe:", "tokens": ["Ich", "ruf'", ",", "wenn", "ich", "den", "Leib", "fort\u00b7schie\u00b7be", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbdie Lieb' ist tot! Es leb' die Liebe!\u00ab", "tokens": ["\u00bb", "die", "Lieb'", "ist", "tot", "!", "Es", "leb'", "die", "Lie\u00b7be", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADJD", "$.", "PPER", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Mein Magen, der von Leid ganz klein,", "tokens": ["Mein", "Ma\u00b7gen", ",", "der", "von", "Leid", "ganz", "klein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "APPR", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Lie\u00df kaum den Durst und Hunger ein;", "tokens": ["Lie\u00df", "kaum", "den", "Durst", "und", "Hun\u00b7ger", "ein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "Mein Geld war l\u00e4ngst in fremden H\u00e4nden,", "tokens": ["Mein", "Geld", "war", "l\u00e4ngst", "in", "frem\u00b7den", "H\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich lebte schon mit lahmen Lenden;", "tokens": ["Ich", "leb\u00b7te", "schon", "mit", "lah\u00b7men", "Len\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.70": {"line.1": {"text": "Lehrreich ist zwar Philosophie,", "tokens": ["Lehr\u00b7reich", "ist", "zwar", "Phi\u00b7lo\u00b7so\u00b7phie", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sehr s\u00e4ttigend ist sie doch nie,", "tokens": ["Sehr", "s\u00e4t\u00b7ti\u00b7gend", "ist", "sie", "doch", "nie", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "ADV", "ADV", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.71": {"line.1": {"text": "Und niemand einem etwas schenkt,", "tokens": ["Und", "nie\u00b7mand", "ei\u00b7nem", "et\u00b7was", "schenkt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ART", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr all das Geld, das man sich denkt.", "tokens": ["F\u00fcr", "all", "das", "Geld", ",", "das", "man", "sich", "denkt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ART", "NN", "$,", "PRELS", "PIS", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Wollt' mich vom Leben nicht entfernen,", "tokens": ["Wollt'", "mich", "vom", "Le\u00b7ben", "nicht", "ent\u00b7fer\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fabrizierte Nachtlichtlaternen,", "tokens": ["Fab\u00b7ri\u00b7zier\u00b7te", "Nacht\u00b7licht\u00b7la\u00b7ter\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.73": {"line.1": {"text": "Laternen, sch\u00f6n aus buntem Glas,", "tokens": ["La\u00b7ter\u00b7nen", ",", "sch\u00f6n", "aus", "bun\u00b7tem", "Glas", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Leuchten zu Haus und auf der Stra\u00df',", "tokens": ["Leuch\u00b7ten", "zu", "Haus", "und", "auf", "der", "Stra\u00df'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.74": {"line.1": {"text": "Darauf male ich manchen Reim,", "tokens": ["Da\u00b7rauf", "ma\u00b7le", "ich", "man\u00b7chen", "Reim", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Leute leuchten sich damit heim.", "tokens": ["Leu\u00b7te", "leuch\u00b7ten", "sich", "da\u00b7mit", "heim", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "PAV", "PTKVZ", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.75": {"line.1": {"text": "Doch \u00f6fters leg' ich mir die Karten,", "tokens": ["Doch", "\u00f6f\u00b7ters", "leg'", "ich", "mir", "die", "Kar\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn w\u00fcnscht man was, tut man's erwarten.", "tokens": ["Denn", "w\u00fcnscht", "man", "was", ",", "tut", "man's", "er\u00b7war\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PWS", "$,", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.76": {"line.1": {"text": "Noch einmal w\u00fcnsch' Frau K\u00f6nigin", "tokens": ["Noch", "ein\u00b7mal", "w\u00fcn\u00b7sch'", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "NN", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich mir an meinen Busen hin.", "tokens": ["Ich", "mir", "an", "mei\u00b7nen", "Bu\u00b7sen", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.77": {"line.1": {"text": "Nur eine Nacht, voll von Vergessen,", "tokens": ["Nur", "ei\u00b7ne", "Nacht", ",", "voll", "von", "Ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll sie sich liebend mit mir messen,", "tokens": ["Soll", "sie", "sich", "lie\u00b7bend", "mit", "mir", "mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "ADJD", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.78": {"line.1": {"text": "Und dann soll kommen was da will,", "tokens": ["Und", "dann", "soll", "kom\u00b7men", "was", "da", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "VVINF", "PWS", "ADV", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben bringt ja stets so viel. \u2013", "tokens": ["Das", "Le\u00b7ben", "bringt", "ja", "stets", "so", "viel", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Wie man von Loreley es wei\u00df,", "tokens": ["Wie", "man", "von", "Lo\u00b7re\u00b7ley", "es", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Haar t\u00f6tete gern mit Flei\u00df.", "tokens": ["Ihr", "Haar", "t\u00f6\u00b7te\u00b7te", "gern", "mit", "Flei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Tat sie beim K\u00e4mmen auch noch singen,", "tokens": ["Tat", "sie", "beim", "K\u00e4m\u00b7men", "auch", "noch", "sin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gleich ganze Schiffe untergingen.", "tokens": ["Gleich", "gan\u00b7ze", "Schif\u00b7fe", "un\u00b7ter\u00b7gin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "Jetzt wie ein Spuk es \u00f6fters war,", "tokens": ["Jetzt", "wie", "ein", "Spuk", "es", "\u00f6f\u00b7ters", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "NN", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Braun bin ich und fand blondes Haar,", "tokens": ["Braun", "bin", "ich", "und", "fand", "blon\u00b7des", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "KON", "VVFIN", "ADJA", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.82": {"line.1": {"text": "Fand's noch im \u00c4rmelfutter h\u00e4ngen", "tokens": ["Fan\u00b7d's", "noch", "im", "\u00c4r\u00b7mel\u00b7fut\u00b7ter", "h\u00e4n\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "APPRART", "NN", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Und weiblich waren seine L\u00e4ngen.", "tokens": ["Und", "weib\u00b7lich", "wa\u00b7ren", "sei\u00b7ne", "L\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.83": {"line.1": {"text": "Die Uhr blieb mir vor Schreck dann stehn,", "tokens": ["Die", "Uhr", "blieb", "mir", "vor", "Schreck", "dann", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sehns\u00fcchtig tat ich um mich sehn", "tokens": ["Sehn\u00b7s\u00fcch\u00b7tig", "tat", "ich", "um", "mich", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Das Goldh\u00e4rlein flog zitternd hoch,", "tokens": ["Das", "Gold\u00b7h\u00e4r\u00b7lein", "flog", "zit\u00b7ternd", "hoch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Kopf dazu fehlte jedoch.", "tokens": ["Der", "Kopf", "da\u00b7zu", "fehl\u00b7te", "je\u00b7doch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VVFIN", "ADV", "$."], "meter": "-+--+---", "measure": "iambic.di.relaxed"}}, "stanza.85": {"line.1": {"text": "Tief seufzend fiel ich jedenfalls", "tokens": ["Tief", "seuf\u00b7zend", "fiel", "ich", "je\u00b7den\u00b7falls"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVPP", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem goldnen H\u00e4rlein um den Hals.", "tokens": ["Dem", "gold\u00b7nen", "H\u00e4r\u00b7lein", "um", "den", "Hals", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "So kommt sie stellenweis nur an", "tokens": ["So", "kommt", "sie", "stel\u00b7len\u00b7weis", "nur", "an"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Schattenbild zum Schattenmann.", "tokens": ["Als", "Schat\u00b7ten\u00b7bild", "zum", "Schat\u00b7ten\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.87": {"line.1": {"text": "Guckt mir der Abend in die Fenster,", "tokens": ["Guckt", "mir", "der", "A\u00b7bend", "in", "die", "Fens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nahn glucksend die Liebesgespenster,", "tokens": ["Nahn", "gluck\u00b7send", "die", "Lie\u00b7bes\u00b7ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.88": {"line.1": {"text": "Schneuzend brauch' ich dann Taschent\u00fccher,", "tokens": ["Schneu\u00b7zend", "brauch'", "ich", "dann", "Ta\u00b7schen\u00b7t\u00fc\u00b7cher", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "St\u00f6hnend wie \u00fcber sch\u00f6ne B\u00fccher.", "tokens": ["St\u00f6h\u00b7nend", "wie", "\u00fc\u00b7ber", "sch\u00f6\u00b7ne", "B\u00fc\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "APPR", "ADJA", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.89": {"line.1": {"text": "Seh' sacht Frau K\u00f6nigin entstehn,", "tokens": ["Seh'", "sacht", "Frau", "K\u00f6\u00b7ni\u00b7gin", "ent\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "NN", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Aus der Tapet ins Zimmer gehn,", "tokens": ["Aus", "der", "Ta\u00b7pet", "ins", "Zim\u00b7mer", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.90": {"line.1": {"text": "Verr\u00fcckt wird dann das ganz Haus,", "tokens": ["Ver\u00b7r\u00fcckt", "wird", "dann", "das", "ganz", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ADV", "ART", "ADV", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Schwimmt als ein Schlo\u00df ins Meer hinaus,", "tokens": ["Schwimmt", "als", "ein", "Schlo\u00df", "ins", "Meer", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "APPRART", "NN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.91": {"line.1": {"text": "Die Pf\u00fctzent\u00fcmpel auf der Stra\u00dfe,", "tokens": ["Die", "Pf\u00fct\u00b7zen\u00b7t\u00fcm\u00b7pel", "auf", "der", "Stra\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die werden Austerb\u00e4nke, blasse,", "tokens": ["Die", "wer\u00b7den", "Aus\u00b7ter\u00b7b\u00e4n\u00b7ke", ",", "blas\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.92": {"line.1": {"text": "Mein Herz schl\u00e4gt schwerer als ein Gong", "tokens": ["Mein", "Herz", "schl\u00e4gt", "schwe\u00b7rer", "als", "ein", "Gong"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6nigin tritt zum Schlo\u00dfbalkon,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "tritt", "zum", "Schlo\u00df\u00b7bal\u00b7kon", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.93": {"line.1": {"text": "Wo sie dem Meer sich zeigen l\u00e4\u00dft,", "tokens": ["Wo", "sie", "dem", "Meer", "sich", "zei\u00b7gen", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "PRF", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alle Fische halten Fest.", "tokens": ["Und", "al\u00b7le", "Fi\u00b7sche", "hal\u00b7ten", "Fest", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.94": {"line.1": {"text": "Fische schnellen zum Speisesaal", "tokens": ["Fi\u00b7sche", "schnel\u00b7len", "zum", "Spei\u00b7se\u00b7saal"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPRART", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Auf Silberplatten ohne Zahl,", "tokens": ["Auf", "Sil\u00b7ber\u00b7plat\u00b7ten", "oh\u00b7ne", "Zahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.95": {"line.1": {"text": "Hirsche vom Walde springen hin", "tokens": ["Hir\u00b7sche", "vom", "Wal\u00b7de", "sprin\u00b7gen", "hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "VVFIN", "PTKVZ"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Auf Monsterplatten, schwer aus Zinn,", "tokens": ["Auf", "Mons\u00b7ter\u00b7plat\u00b7ten", ",", "schwer", "aus", "Zinn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.96": {"line.1": {"text": "Und Schafe tuen lieblich bl\u00f6ken", "tokens": ["Und", "Scha\u00b7fe", "tu\u00b7en", "lieb\u00b7lich", "bl\u00f6\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVFIN", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und nicht wider den Bratspie\u00df l\u00f6ken,", "tokens": ["Und", "nicht", "wi\u00b7der", "den", "Brat\u00b7spie\u00df", "l\u00f6\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.97": {"line.1": {"text": "Alle sind Frau K\u00f6nigin gut", "tokens": ["Al\u00b7le", "sind", "Frau", "K\u00f6\u00b7ni\u00b7gin", "gut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "NN", "NN", "ADJD"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und braten sich aus Liebesglut.", "tokens": ["Und", "bra\u00b7ten", "sich", "aus", "Lie\u00b7bes\u00b7glut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.98": {"line.1": {"text": "Und jedes Bein vom Speisetisch,", "tokens": ["Und", "je\u00b7des", "Bein", "vom", "Spei\u00b7se\u00b7tisch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bl\u00fcht flott als Weinstock gr\u00fcn und frisch", "tokens": ["Bl\u00fcht", "flott", "als", "Wein\u00b7stock", "gr\u00fcn", "und", "frisch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "KOKOM", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.99": {"line.1": {"text": "Und tr\u00e4gt schon zum Dessert die Trauben;", "tokens": ["Und", "tr\u00e4gt", "schon", "zum", "Des\u00b7sert", "die", "Trau\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man wird das wahrscheinlich kaum glauben.", "tokens": ["Man", "wird", "das", "wahr\u00b7schein\u00b7lich", "kaum", "glau\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "ADJD", "ADV", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.100": {"line.1": {"text": "K\u00f6nigin spricht. \u00bbGl\u00fcck war Geruch,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "spricht", ".", "\u00bb", "Gl\u00fcck", "war", "Ge\u00b7ruch", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "NN", "VAFIN", "NN", "$,"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "War wie etwas im Taschentuch,", "tokens": ["War", "wie", "et\u00b7was", "im", "Ta\u00b7schen\u00b7tuch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.101": {"line.1": {"text": "Gl\u00fcck lag tief vor uns auf dem Bauch", "tokens": ["Gl\u00fcck", "lag", "tief", "vor", "uns", "auf", "dem", "Bauch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ADJD", "APPR", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und r\u00e4ucherte wie Weiherauch,", "tokens": ["Und", "r\u00e4u\u00b7cher\u00b7te", "wie", "Wei\u00b7her\u00b7auch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.102": {"line.1": {"text": "Gl\u00fccksgeruch badete mein Blut,", "tokens": ["Gl\u00fccks\u00b7ge\u00b7ruch", "ba\u00b7de\u00b7te", "mein", "Blut", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Ich roch einst selber mir so gut,", "tokens": ["Ich", "roch", "einst", "sel\u00b7ber", "mir", "so", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.103": {"line.1": {"text": "Lie\u00df Sonne nach Belieben scheinen,", "tokens": ["Lie\u00df", "Son\u00b7ne", "nach", "Be\u00b7lie\u00b7ben", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Selbst Meerrettich machte nicht weinen.\u00ab", "tokens": ["Selbst", "Meer\u00b7ret\u00b7tich", "mach\u00b7te", "nicht", "wei\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.104": {"line.1": {"text": "Sie tut die Sonn' vom Nagel nehmen,", "tokens": ["Sie", "tut", "die", "Sonn'", "vom", "Na\u00b7gel", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPRART", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Damit die Stern' als Lampen k\u00e4men.", "tokens": ["Da\u00b7mit", "die", "Stern'", "als", "Lam\u00b7pen", "k\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.105": {"line.1": {"text": "Wir tuen dann den Mond aufh\u00e4ngen", "tokens": ["Wir", "tu\u00b7en", "dann", "den", "Mond", "auf\u00b7h\u00e4n\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und drunter Lipp' an Lippe dr\u00e4ngen.", "tokens": ["Und", "drun\u00b7ter", "Lipp'", "an", "Lip\u00b7pe", "dr\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "NN", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.106": {"line.1": {"text": "Die Uhr schl\u00e4gt wie die Nachtigall", "tokens": ["Die", "Uhr", "schl\u00e4gt", "wie", "die", "Nach\u00b7ti\u00b7gall"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sagt nicht mehr der Stunden Zahl ...", "tokens": ["Und", "sagt", "nicht", "mehr", "der", "Stun\u00b7den", "Zahl", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.107": {"line.1": {"text": "So tr\u00e4um' ich n\u00e4chtlich ins Nachtlicht;", "tokens": ["So", "tr\u00e4um'", "ich", "n\u00e4cht\u00b7lich", "ins", "Nacht\u00b7licht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Seufzend geht's aus, das Zimmer riecht", "tokens": ["Seuf\u00b7zend", "geht's", "aus", ",", "das", "Zim\u00b7mer", "riecht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PTKVZ", "$,", "ART", "NN", "VVFIN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.108": {"line.1": {"text": "Nach weichem \u00d6l und warmem Rauch,", "tokens": ["Nach", "wei\u00b7chem", "\u00d6l", "und", "war\u00b7mem", "Rauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mond lehnt mir leer und k\u00fchl am Bauch,", "tokens": ["Mond", "lehnt", "mir", "leer", "und", "k\u00fchl", "am", "Bauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.109": {"line.1": {"text": "Aus Zeitungspapier scheint der Mond,", "tokens": ["Aus", "Zei\u00b7tungs\u00b7pa\u00b7pier", "scheint", "der", "Mond", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Alt, da\u00df sich nichts zu lesen lohnt.", "tokens": ["Alt", ",", "da\u00df", "sich", "nichts", "zu", "le\u00b7sen", "lohnt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "KOUS", "PRF", "PIS", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.110": {"line.1": {"text": "Die H\u00e4user, Droschken, Ladenfenster", "tokens": ["Die", "H\u00e4u\u00b7ser", ",", "Droschken", ",", "La\u00b7den\u00b7fens\u00b7ter"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "NN", "$,", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sind nur Pappendeckelgespenster,", "tokens": ["Sind", "nur", "Pap\u00b7pen\u00b7de\u00b7ckel\u00b7ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.111": {"line.1": {"text": "Gleich Papierp\u00fcppchen anzuschaun,", "tokens": ["Gleich", "Pa\u00b7pier\u00b7p\u00fcpp\u00b7chen", "an\u00b7zu\u00b7schaun", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "T\u00e4nzeln vorbei Herren und Fraun.", "tokens": ["T\u00e4n\u00b7zeln", "vor\u00b7bei", "Her\u00b7ren", "und", "Fraun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.112": {"line.1": {"text": "Und geh' ich fr\u00fch zur Stadt hinaus,", "tokens": ["Und", "geh'", "ich", "fr\u00fch", "zur", "Stadt", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "APPRART", "NN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sehn Wolken wie Nachtm\u00fctzen aus.", "tokens": ["Sehn", "Wol\u00b7ken", "wie", "Nacht\u00b7m\u00fct\u00b7zen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KOKOM", "NN", "PTKVZ", "$."], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}}, "stanza.113": {"line.1": {"text": "Es g\u00e4hnen B\u00e4um', Wolken, Erdscholl',", "tokens": ["Es", "g\u00e4h\u00b7nen", "B\u00e4um'", ",", "Wol\u00b7ken", ",", "Erd\u00b7scholl'", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Schafherden g\u00e4hnen wei\u00df aus Woll',", "tokens": ["Schaf\u00b7her\u00b7den", "g\u00e4h\u00b7nen", "wei\u00df", "aus", "Woll'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "VVFIN", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.114": {"line.1": {"text": "Es g\u00e4hnt das Feuer in der Schmied',", "tokens": ["Es", "g\u00e4hnt", "das", "Feu\u00b7er", "in", "der", "Schmied'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Riesenschlaf aus allem zieht,", "tokens": ["Ein", "Rie\u00b7sen\u00b7schlaf", "aus", "al\u00b7lem", "zieht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.115": {"line.1": {"text": "Der Pflug im Acker f\u00e4llt um still,", "tokens": ["Der", "Pflug", "im", "A\u00b7cker", "f\u00e4llt", "um", "still", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VVFIN", "APPR", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil Gaul und Bauer g\u00e4hnen will,", "tokens": ["Weil", "Gaul", "und", "Bau\u00b7er", "g\u00e4h\u00b7nen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.116": {"line.1": {"text": "Der Bach sich dicht ans Ufer lehnt,", "tokens": ["Der", "Bach", "sich", "dicht", "ans", "U\u00b7fer", "lehnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADJD", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wasser, Luft, Erde, Feuer g\u00e4hnt,", "tokens": ["Was\u00b7ser", ",", "Luft", ",", "Er\u00b7de", ",", "Feu\u00b7er", "g\u00e4hnt", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "VVFIN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.117": {"line.1": {"text": "T\u00fcrme kein Gleichgewicht mehr haben,", "tokens": ["T\u00fcr\u00b7me", "kein", "Gleich\u00b7ge\u00b7wicht", "mehr", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "ADV", "VAFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "G\u00e4hnend fallen auf mich die Raben,", "tokens": ["G\u00e4h\u00b7nend", "fal\u00b7len", "auf", "mich", "die", "Ra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "PPER", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.118": {"line.1": {"text": "Seh' alle Ding' im Schlaf fortschweben,", "tokens": ["Seh'", "al\u00b7le", "Ding'", "im", "Schlaf", "fort\u00b7schwe\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Frage mich: \u00bbBin ich noch am Leben?\u00ab", "tokens": ["Fra\u00b7ge", "mich", ":", "\u00bb", "Bin", "ich", "noch", "am", "Le\u00b7ben", "?", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "$.", "$(", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$.", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.119": {"line.1": {"text": "Vielleicht sind's tausend Jahre bald,", "tokens": ["Viel\u00b7leicht", "sin\u00b7d's", "tau\u00b7send", "Jah\u00b7re", "bald", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "CARD", "NN", "ADV", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Seit ich einschlief und schlief mich alt.", "tokens": ["Seit", "ich", "ein\u00b7schlief", "und", "schlief", "mich", "alt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "KON", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.120": {"line.1": {"text": "M\u00f6chts gern noch allen Leuten sagen,", "tokens": ["M\u00f6chts", "gern", "noch", "al\u00b7len", "Leu\u00b7ten", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sch\u00f6n's war, Liebe zu ertragen.", "tokens": ["Wie", "sch\u00f6n's", "war", ",", "Lie\u00b7be", "zu", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "$,", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.121": {"line.1": {"text": "Die Liebe ich allm\u00e4chtig fand,", "tokens": ["Die", "Lie\u00b7be", "ich", "all\u00b7m\u00e4ch\u00b7tig", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tod ist nur interessant.", "tokens": ["Der", "Tod", "ist", "nur", "in\u00b7ter\u00b7es\u00b7sant", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.122": {"line.1": {"text": "Werden mir dunkel jetzt die Fenster,", "tokens": ["Wer\u00b7den", "mir", "dun\u00b7kel", "jetzt", "die", "Fens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Seh' ich im Tode nicht Gespenster.", "tokens": ["Seh'", "ich", "im", "To\u00b7de", "nicht", "Ge\u00b7spens\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "PTKNEG", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.123": {"line.1": {"text": "Mache nur still die Augen zu,", "tokens": ["Ma\u00b7che", "nur", "still", "die", "Au\u00b7gen", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADJD", "ART", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Weh tat noch keinem Mensch die Ruh'.", "tokens": ["Weh", "tat", "noch", "kei\u00b7nem", "Mensch", "die", "Ruh'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "PIAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.124": {"line.1": {"text": "Das Essen uns nur teilweis z\u00fcndet,", "tokens": ["Das", "Es\u00b7sen", "uns", "nur", "teil\u00b7weis", "z\u00fcn\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn es uns so behaglich r\u00fcndet.", "tokens": ["Wenn", "es", "uns", "so", "be\u00b7hag\u00b7lich", "r\u00fcn\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.125": {"line.1": {"text": "Weisheit erquickt, wenn sie uns pa\u00dft,", "tokens": ["Weis\u00b7heit", "er\u00b7quickt", ",", "wenn", "sie", "uns", "pa\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Man f\u00fchlt sich blendend angefa\u00dft.", "tokens": ["Man", "f\u00fchlt", "sich", "blen\u00b7dend", "an\u00b7ge\u00b7fa\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.126": {"line.1": {"text": "Doch Liebe uns ganz voll entz\u00fcckt,", "tokens": ["Doch", "Lie\u00b7be", "uns", "ganz", "voll", "ent\u00b7z\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verliebt f\u00fchlt sich der Floh entr\u00fcckt.", "tokens": ["Ver\u00b7liebt", "f\u00fchlt", "sich", "der", "Floh", "ent\u00b7r\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PRF", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.127": {"line.1": {"text": "Die Liebe ist im Weltall Trumpf,", "tokens": ["Die", "Lie\u00b7be", "ist", "im", "Wel\u00b7tall", "Trumpf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch unten bei dem Frosch im Sumpf.", "tokens": ["Auch", "un\u00b7ten", "bei", "dem", "Frosch", "im", "Sumpf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.128": {"line.1": {"text": "Verliebtsein ist das Himmelreich,", "tokens": ["Ver\u00b7liebt\u00b7sein", "ist", "das", "Him\u00b7mel\u00b7reich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da sind sich Mensch, Tier, Pflanze gleich.", "tokens": ["Da", "sind", "sich", "Mensch", ",", "Tier", ",", "Pflan\u00b7ze", "gleich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "NN", "$,", "NN", "$,", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.129": {"line.1": {"text": "Verliebt geht man aus sich heraus,", "tokens": ["Ver\u00b7liebt", "geht", "man", "aus", "sich", "he\u00b7raus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PIS", "APPR", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Pflanze, Tier, Mensch sehn prachtvoll aus.", "tokens": ["Pflan\u00b7ze", ",", "Tier", ",", "Mensch", "sehn", "pracht\u00b7voll", "aus", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.130": {"line.1": {"text": "Liebe im Mittelpunkt dasteht,", "tokens": ["Lie\u00b7be", "im", "Mit\u00b7tel\u00b7punkt", "das\u00b7teht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Die ganze Welt sich darum dreht.", "tokens": ["Die", "gan\u00b7ze", "Welt", "sich", "da\u00b7rum", "dreht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.131": {"line.1": {"text": "Und tut ein altes Herz verderben,", "tokens": ["Und", "tut", "ein", "al\u00b7tes", "Herz", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Um neu zu lieben, kann es sterben.", "tokens": ["Um", "neu", "zu", "lie\u00b7ben", ",", "kann", "es", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "ADJD", "PTKZU", "VVINF", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.132": {"line.1": {"text": "Doch mach' ich aus dem Tod kein Fest,", "tokens": ["Doch", "mach'", "ich", "aus", "dem", "Tod", "kein", "Fest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da man sich gern beweinen l\u00e4\u00dft.", "tokens": ["Da", "man", "sich", "gern", "be\u00b7wei\u00b7nen", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PRF", "ADV", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.133": {"line.1": {"text": "Und nicht wie sterbend ein C\u00e4sar,", "tokens": ["Und", "nicht", "wie", "ster\u00b7bend", "ein", "C\u00e4\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "KOKOM", "ADJD", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Befiehlt Applaus der Balthasar.", "tokens": ["Be\u00b7fiehlt", "Ap\u00b7plaus", "der", "Balt\u00b7ha\u00b7sar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.134": {"line.1": {"text": "Ich ruf', wenn ich den Leib fortschiebe:", "tokens": ["Ich", "ruf'", ",", "wenn", "ich", "den", "Leib", "fort\u00b7schie\u00b7be", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbdie Lieb' ist tot! Es leb' die Liebe!\u00ab", "tokens": ["\u00bb", "die", "Lieb'", "ist", "tot", "!", "Es", "leb'", "die", "Lie\u00b7be", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADJD", "$.", "PPER", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}