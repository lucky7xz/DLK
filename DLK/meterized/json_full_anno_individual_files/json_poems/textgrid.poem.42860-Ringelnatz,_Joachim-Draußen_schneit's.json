{"textgrid.poem.42860": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Drau\u00dfen schneit's", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir hatten ein Schaukelpferd vorher gekauft.", "tokens": ["Wir", "hat\u00b7ten", "ein", "Schau\u00b7kel\u00b7pferd", "vor\u00b7her", "ge\u00b7kauft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Aber nachher kam gar kein Kind.", "tokens": ["A\u00b7ber", "nach\u00b7her", "kam", "gar", "kein", "Kind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Darum hatten wir damals das Pferd dann Bubi getauft. \u2013", "tokens": ["Da\u00b7rum", "hat\u00b7ten", "wir", "da\u00b7mals", "das", "Pferd", "dann", "Bu\u00b7bi", "ge\u00b7tauft", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ART", "NN", "ADV", "NN", "VVPP", "$.", "$("], "meter": "--+--+--+-+--+", "measure": "anapaest.tri.plus"}}, "stanza.2": {"line.1": {"text": "Weil nun die Holzpreise so unerschwinglich sind;", "tokens": ["Weil", "nun", "die", "Holz\u00b7prei\u00b7se", "so", "un\u00b7er\u00b7schwing\u00b7lich", "sind", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und ich nun doch schon seit Donnerstag", "tokens": ["Und", "ich", "nun", "doch", "schon", "seit", "Don\u00b7ners\u00b7tag"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADV", "ADV", "ADV", "APPR", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Nicht mehr angestellt bin, weil ich nicht mehr mag;", "tokens": ["Nicht", "mehr", "an\u00b7ge\u00b7stellt", "bin", ",", "weil", "ich", "nicht", "mehr", "mag", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "VVPP", "VAFIN", "$,", "KOUS", "PPER", "PTKNEG", "ADV", "VMFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Haben wir's eingeteilt. Und zwar:", "tokens": ["Ha\u00b7ben", "wir's", "ein\u00b7ge\u00b7teilt", ".", "Und", "zwar", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "$.", "KON", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Die Schaukel selbst f\u00fcr November,", "tokens": ["Die", "Schau\u00b7kel", "selbst", "f\u00fcr", "No\u00b7vem\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Kopf und Beine Dezember,", "tokens": ["Kopf", "und", "Bei\u00b7ne", "De\u00b7zem\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.7": {"text": "Rumpf mit Sattel f\u00fcr Januar.", "tokens": ["Rumpf", "mit", "Sat\u00b7tel", "f\u00fcr", "Ja\u00b7nu\u00b7ar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.3": {"line.1": {"text": "Ich gehe nie wieder in die Fabrik.", "tokens": ["Ich", "ge\u00b7he", "nie", "wie\u00b7der", "in", "die", "Fab\u00b7rik", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich habe das Regelm\u00e4\u00dfige dick.", "tokens": ["Ich", "ha\u00b7be", "das", "Re\u00b7gel\u00b7m\u00e4\u00b7\u00dfi\u00b7ge", "dick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da geht das K\u00fcnstlerische dar\u00fcber abhanden.", "tokens": ["Da", "geht", "das", "K\u00fcnst\u00b7le\u00b7ri\u00b7sche", "da\u00b7r\u00fc\u00b7ber", "ab\u00b7han\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PAV", "VVINF", "$."], "meter": "-+-+-+-++-++-", "measure": "unknown.measure.septa"}, "line.4": {"text": "Wenn die auch jede Woche bezahlen,", "tokens": ["Wenn", "die", "auch", "je\u00b7de", "Wo\u00b7che", "be\u00b7zah\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Aber nur immer Girlanden und wieder Girlanden", "tokens": ["A\u00b7ber", "nur", "im\u00b7mer", "Gir\u00b7lan\u00b7den", "und", "wie\u00b7der", "Gir\u00b7lan\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "NN", "KON", "ADV", "NN"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}, "line.6": {"text": "Auf Spuckn\u00e4pfe malen,", "tokens": ["Auf", "Spuck\u00b7n\u00e4p\u00b7fe", "ma\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "Die sich die Leute doch nie begucken,", "tokens": ["Die", "sich", "die", "Leu\u00b7te", "doch", "nie", "be\u00b7gu\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ART", "NN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Im Gegenteil noch drauf spucken, \u2013 \u2013", "tokens": ["Im", "Ge\u00b7gen\u00b7teil", "noch", "drauf", "spu\u00b7cken", ",", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPRART", "NN", "ADV", "PAV", "VVINF", "$,", "$(", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "Das bringt ja ein Pferd auf den Hund.", "tokens": ["Das", "bringt", "ja", "ein", "Pferd", "auf", "den", "Hund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.4": {"line.1": {"text": "Als freier K\u00fcnstler kann ich bis mittags liegen", "tokens": ["Als", "frei\u00b7er", "K\u00fcnst\u00b7ler", "kann", "ich", "bis", "mit\u00b7tags", "lie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADJA", "NN", "VMFIN", "PPER", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Bleiben. \u2013 Na und die Frau ist gesund.", "tokens": ["Blei\u00b7ben", ".", "\u2013", "Na", "und", "die", "Frau", "ist", "ge\u00b7sund", "."], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "NE", "KON", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Es wird sich schon was finden, um Geld beizukriegen.", "tokens": ["Es", "wird", "sich", "schon", "was", "fin\u00b7den", ",", "um", "Geld", "bei\u00b7zu\u00b7krie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "PIS", "VVINF", "$,", "KOUI", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Anna und ich haben vorl\u00e4ufig nun", "tokens": ["An\u00b7na", "und", "ich", "ha\u00b7ben", "vor\u00b7l\u00e4u\u00b7fig", "nun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "KON", "PPER", "VAFIN", "ADJD", "ADV"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Erst mal genug mit dem Bubi zu tun.", "tokens": ["Erst", "mal", "ge\u00b7nug", "mit", "dem", "Bu\u00b7bi", "zu", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.6": {"text": "Rumpf zers\u00e4gen, Beine rausdrehn,", "tokens": ["Rumpf", "zer\u00b7s\u00e4\u00b7gen", ",", "Bei\u00b7ne", "raus\u00b7drehn", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "NN", "VVINF", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "N\u00e4gel rausrei\u00dfen, Fell absch\u00e4len.", "tokens": ["N\u00e4\u00b7gel", "raus\u00b7rei\u00b7\u00dfen", ",", "Fell", "ab\u00b7sch\u00e4\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "NN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Dar\u00fcber k\u00f6nnen Wochen vergehn.", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "k\u00f6n\u00b7nen", "Wo\u00b7chen", "ver\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.9": {"text": "Das will auch gelernt und verstanden sein,", "tokens": ["Das", "will", "auch", "ge\u00b7lernt", "und", "ver\u00b7stan\u00b7den", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "VVPP", "KON", "VVPP", "VAINF", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "Sonst kann man sich daran zu Tode qu\u00e4len.", "tokens": ["Sonst", "kann", "man", "sich", "da\u00b7ran", "zu", "To\u00b7de", "qu\u00e4\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PRF", "PAV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Solches Holz ist h\u00e4rter als Stein.", "tokens": ["Sol\u00b7ches", "Holz", "ist", "h\u00e4r\u00b7ter", "als", "Stein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.12": {"text": "Dann spalten und Sp\u00e4ne zum Anz\u00fcnden schneiden", "tokens": ["Dann", "spal\u00b7ten", "und", "Sp\u00e4\u00b7ne", "zum", "An\u00b7z\u00fcn\u00b7den", "schnei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "KON", "NN", "APPRART", "NN", "VVINF"], "meter": "-+--+--++-+-", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "Und tausenderlei.", "tokens": ["Und", "tau\u00b7sen\u00b7der\u00b7lei", "."], "token_info": ["word", "word", "punct"], "pos": ["KON", "CARD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.14": {"text": "Aber das tut uns gut, uns beiden,", "tokens": ["A\u00b7ber", "das", "tut", "uns", "gut", ",", "uns", "bei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "ADJD", "$,", "PPER", "PIAT", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.15": {"text": "Sich mal so k\u00f6rperlich auszuschwitzen.", "tokens": ["Sich", "mal", "so", "k\u00f6r\u00b7per\u00b7lich", "aus\u00b7zu\u00b7schwit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Au\u00dferdem kann man ja dabei", "tokens": ["Au\u00b7\u00dfer\u00b7dem", "kann", "man", "ja", "da\u00b7bei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VMFIN", "PIS", "ADV", "PAV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Ganz bequem auf dem Sofa sitzen;", "tokens": ["Ganz", "be\u00b7quem", "auf", "dem", "So\u00b7fa", "sit\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Raucht seine Pfeife, trinkt seinen Tee,", "tokens": ["Raucht", "sei\u00b7ne", "Pfei\u00b7fe", ",", "trinkt", "sei\u00b7nen", "Tee", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$,", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und vor allem: Man ist eben frei!", "tokens": ["Und", "vor", "al\u00b7lem", ":", "Man", "ist", "e\u00b7ben", "frei", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIS", "$.", "PIS", "VAFIN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Man hat sein eigenes Atelier.", "tokens": ["Man", "hat", "sein", "ei\u00b7ge\u00b7nes", "A\u00b7te\u00b7lier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Man hat seinen eigenen Herd;", "tokens": ["Man", "hat", "sei\u00b7nen", "ei\u00b7ge\u00b7nen", "Herd", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Da wird ein Feuerchen angemacht \u2013", "tokens": ["Da", "wird", "ein", "Feu\u00b7er\u00b7chen", "an\u00b7ge\u00b7macht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Mit Bubipferd \u2013,", "tokens": ["Mit", "Bu\u00b7bip\u00b7ferd", "\u2013", ","], "token_info": ["word", "word", "punct", "punct"], "pos": ["APPR", "NN", "$(", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "Da\u00df die Esse kracht.", "tokens": ["Da\u00df", "die", "Es\u00b7se", "kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Und die Anna singt und die Anna lacht.", "tokens": ["Und", "die", "An\u00b7na", "singt", "und", "die", "An\u00b7na", "lacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NE", "VVFIN", "KON", "ART", "NE", "VVFIN", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Da k\u00f6nnen wir nach Belieben", "tokens": ["Da", "k\u00f6n\u00b7nen", "wir", "nach", "Be\u00b7lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die Arbeit auf sp\u00e4ter verschieben.", "tokens": ["Die", "Ar\u00b7beit", "auf", "sp\u00e4\u00b7ter", "ver\u00b7schie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Denn wenn man das Gas uns sperren l\u00e4\u00dft", "tokens": ["Denn", "wenn", "man", "das", "Gas", "uns", "sper\u00b7ren", "l\u00e4\u00dft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PIS", "ART", "NN", "PPER", "VVINF", "VVFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Oder kein Bier ohne Bargeld mehr gibt,", "tokens": ["O\u00b7der", "kein", "Bier", "oh\u00b7ne", "Bar\u00b7geld", "mehr", "gibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPR", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Dann kriechen wir gleich nach Mittag ins Nest", "tokens": ["Dann", "krie\u00b7chen", "wir", "gleich", "nach", "Mit\u00b7tag", "ins", "Nest"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "APPRART", "NN"], "meter": "-+-+-+---+", "measure": "zehnsilber"}, "line.6": {"text": "Und schlafen, solange es uns beliebt.", "tokens": ["Und", "schla\u00b7fen", ",", "so\u00b7lan\u00b7ge", "es", "uns", "be\u00b7liebt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PPER", "PRF", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.7": {"line.1": {"text": "Freilich: Der feste Lohn f\u00e4llt nun fort,", "tokens": ["Frei\u00b7lich", ":", "Der", "fes\u00b7te", "Lohn", "f\u00e4llt", "nun", "fort", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ART", "ADJA", "NN", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Aber die Freiheit ist auch was wert.", "tokens": ["A\u00b7ber", "die", "Frei\u00b7heit", "ist", "auch", "was", "wert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADV", "PWS", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Und das mit dem Schaukelpferd", "tokens": ["Und", "das", "mit", "dem", "Schau\u00b7kel\u00b7pferd"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist jetzt unser Wintersport.", "tokens": ["Ist", "jetzt", "un\u00b7ser", "Win\u00b7ter\u00b7sport", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Wir hatten ein Schaukelpferd vorher gekauft.", "tokens": ["Wir", "hat\u00b7ten", "ein", "Schau\u00b7kel\u00b7pferd", "vor\u00b7her", "ge\u00b7kauft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Aber nachher kam gar kein Kind.", "tokens": ["A\u00b7ber", "nach\u00b7her", "kam", "gar", "kein", "Kind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Darum hatten wir damals das Pferd dann Bubi getauft. \u2013", "tokens": ["Da\u00b7rum", "hat\u00b7ten", "wir", "da\u00b7mals", "das", "Pferd", "dann", "Bu\u00b7bi", "ge\u00b7tauft", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ART", "NN", "ADV", "NN", "VVPP", "$.", "$("], "meter": "--+--+--+-+--+", "measure": "anapaest.tri.plus"}}, "stanza.9": {"line.1": {"text": "Weil nun die Holzpreise so unerschwinglich sind;", "tokens": ["Weil", "nun", "die", "Holz\u00b7prei\u00b7se", "so", "un\u00b7er\u00b7schwing\u00b7lich", "sind", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und ich nun doch schon seit Donnerstag", "tokens": ["Und", "ich", "nun", "doch", "schon", "seit", "Don\u00b7ners\u00b7tag"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADV", "ADV", "ADV", "APPR", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Nicht mehr angestellt bin, weil ich nicht mehr mag;", "tokens": ["Nicht", "mehr", "an\u00b7ge\u00b7stellt", "bin", ",", "weil", "ich", "nicht", "mehr", "mag", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "VVPP", "VAFIN", "$,", "KOUS", "PPER", "PTKNEG", "ADV", "VMFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Haben wir's eingeteilt. Und zwar:", "tokens": ["Ha\u00b7ben", "wir's", "ein\u00b7ge\u00b7teilt", ".", "Und", "zwar", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "$.", "KON", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Die Schaukel selbst f\u00fcr November,", "tokens": ["Die", "Schau\u00b7kel", "selbst", "f\u00fcr", "No\u00b7vem\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Kopf und Beine Dezember,", "tokens": ["Kopf", "und", "Bei\u00b7ne", "De\u00b7zem\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.7": {"text": "Rumpf mit Sattel f\u00fcr Januar.", "tokens": ["Rumpf", "mit", "Sat\u00b7tel", "f\u00fcr", "Ja\u00b7nu\u00b7ar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.10": {"line.1": {"text": "Ich gehe nie wieder in die Fabrik.", "tokens": ["Ich", "ge\u00b7he", "nie", "wie\u00b7der", "in", "die", "Fab\u00b7rik", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich habe das Regelm\u00e4\u00dfige dick.", "tokens": ["Ich", "ha\u00b7be", "das", "Re\u00b7gel\u00b7m\u00e4\u00b7\u00dfi\u00b7ge", "dick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da geht das K\u00fcnstlerische dar\u00fcber abhanden.", "tokens": ["Da", "geht", "das", "K\u00fcnst\u00b7le\u00b7ri\u00b7sche", "da\u00b7r\u00fc\u00b7ber", "ab\u00b7han\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PAV", "VVINF", "$."], "meter": "-+-+-+-++-++-", "measure": "unknown.measure.septa"}, "line.4": {"text": "Wenn die auch jede Woche bezahlen,", "tokens": ["Wenn", "die", "auch", "je\u00b7de", "Wo\u00b7che", "be\u00b7zah\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Aber nur immer Girlanden und wieder Girlanden", "tokens": ["A\u00b7ber", "nur", "im\u00b7mer", "Gir\u00b7lan\u00b7den", "und", "wie\u00b7der", "Gir\u00b7lan\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "NN", "KON", "ADV", "NN"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}, "line.6": {"text": "Auf Spuckn\u00e4pfe malen,", "tokens": ["Auf", "Spuck\u00b7n\u00e4p\u00b7fe", "ma\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "Die sich die Leute doch nie begucken,", "tokens": ["Die", "sich", "die", "Leu\u00b7te", "doch", "nie", "be\u00b7gu\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ART", "NN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Im Gegenteil noch drauf spucken, \u2013 \u2013", "tokens": ["Im", "Ge\u00b7gen\u00b7teil", "noch", "drauf", "spu\u00b7cken", ",", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPRART", "NN", "ADV", "PAV", "VVINF", "$,", "$(", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "Das bringt ja ein Pferd auf den Hund.", "tokens": ["Das", "bringt", "ja", "ein", "Pferd", "auf", "den", "Hund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.11": {"line.1": {"text": "Als freier K\u00fcnstler kann ich bis mittags liegen", "tokens": ["Als", "frei\u00b7er", "K\u00fcnst\u00b7ler", "kann", "ich", "bis", "mit\u00b7tags", "lie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADJA", "NN", "VMFIN", "PPER", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Bleiben. \u2013 Na und die Frau ist gesund.", "tokens": ["Blei\u00b7ben", ".", "\u2013", "Na", "und", "die", "Frau", "ist", "ge\u00b7sund", "."], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "NE", "KON", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Es wird sich schon was finden, um Geld beizukriegen.", "tokens": ["Es", "wird", "sich", "schon", "was", "fin\u00b7den", ",", "um", "Geld", "bei\u00b7zu\u00b7krie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "PIS", "VVINF", "$,", "KOUI", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Anna und ich haben vorl\u00e4ufig nun", "tokens": ["An\u00b7na", "und", "ich", "ha\u00b7ben", "vor\u00b7l\u00e4u\u00b7fig", "nun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "KON", "PPER", "VAFIN", "ADJD", "ADV"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Erst mal genug mit dem Bubi zu tun.", "tokens": ["Erst", "mal", "ge\u00b7nug", "mit", "dem", "Bu\u00b7bi", "zu", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.6": {"text": "Rumpf zers\u00e4gen, Beine rausdrehn,", "tokens": ["Rumpf", "zer\u00b7s\u00e4\u00b7gen", ",", "Bei\u00b7ne", "raus\u00b7drehn", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "NN", "VVINF", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "N\u00e4gel rausrei\u00dfen, Fell absch\u00e4len.", "tokens": ["N\u00e4\u00b7gel", "raus\u00b7rei\u00b7\u00dfen", ",", "Fell", "ab\u00b7sch\u00e4\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "NN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Dar\u00fcber k\u00f6nnen Wochen vergehn.", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "k\u00f6n\u00b7nen", "Wo\u00b7chen", "ver\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.9": {"text": "Das will auch gelernt und verstanden sein,", "tokens": ["Das", "will", "auch", "ge\u00b7lernt", "und", "ver\u00b7stan\u00b7den", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "VVPP", "KON", "VVPP", "VAINF", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "Sonst kann man sich daran zu Tode qu\u00e4len.", "tokens": ["Sonst", "kann", "man", "sich", "da\u00b7ran", "zu", "To\u00b7de", "qu\u00e4\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PRF", "PAV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Solches Holz ist h\u00e4rter als Stein.", "tokens": ["Sol\u00b7ches", "Holz", "ist", "h\u00e4r\u00b7ter", "als", "Stein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.12": {"text": "Dann spalten und Sp\u00e4ne zum Anz\u00fcnden schneiden", "tokens": ["Dann", "spal\u00b7ten", "und", "Sp\u00e4\u00b7ne", "zum", "An\u00b7z\u00fcn\u00b7den", "schnei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "KON", "NN", "APPRART", "NN", "VVINF"], "meter": "-+--+--++-+-", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "Und tausenderlei.", "tokens": ["Und", "tau\u00b7sen\u00b7der\u00b7lei", "."], "token_info": ["word", "word", "punct"], "pos": ["KON", "CARD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.14": {"text": "Aber das tut uns gut, uns beiden,", "tokens": ["A\u00b7ber", "das", "tut", "uns", "gut", ",", "uns", "bei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "ADJD", "$,", "PPER", "PIAT", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.15": {"text": "Sich mal so k\u00f6rperlich auszuschwitzen.", "tokens": ["Sich", "mal", "so", "k\u00f6r\u00b7per\u00b7lich", "aus\u00b7zu\u00b7schwit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Au\u00dferdem kann man ja dabei", "tokens": ["Au\u00b7\u00dfer\u00b7dem", "kann", "man", "ja", "da\u00b7bei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VMFIN", "PIS", "ADV", "PAV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Ganz bequem auf dem Sofa sitzen;", "tokens": ["Ganz", "be\u00b7quem", "auf", "dem", "So\u00b7fa", "sit\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Raucht seine Pfeife, trinkt seinen Tee,", "tokens": ["Raucht", "sei\u00b7ne", "Pfei\u00b7fe", ",", "trinkt", "sei\u00b7nen", "Tee", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$,", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und vor allem: Man ist eben frei!", "tokens": ["Und", "vor", "al\u00b7lem", ":", "Man", "ist", "e\u00b7ben", "frei", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIS", "$.", "PIS", "VAFIN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Man hat sein eigenes Atelier.", "tokens": ["Man", "hat", "sein", "ei\u00b7ge\u00b7nes", "A\u00b7te\u00b7lier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Man hat seinen eigenen Herd;", "tokens": ["Man", "hat", "sei\u00b7nen", "ei\u00b7ge\u00b7nen", "Herd", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Da wird ein Feuerchen angemacht \u2013", "tokens": ["Da", "wird", "ein", "Feu\u00b7er\u00b7chen", "an\u00b7ge\u00b7macht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Mit Bubipferd \u2013,", "tokens": ["Mit", "Bu\u00b7bip\u00b7ferd", "\u2013", ","], "token_info": ["word", "word", "punct", "punct"], "pos": ["APPR", "NN", "$(", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "Da\u00df die Esse kracht.", "tokens": ["Da\u00df", "die", "Es\u00b7se", "kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Und die Anna singt und die Anna lacht.", "tokens": ["Und", "die", "An\u00b7na", "singt", "und", "die", "An\u00b7na", "lacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NE", "VVFIN", "KON", "ART", "NE", "VVFIN", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "Da k\u00f6nnen wir nach Belieben", "tokens": ["Da", "k\u00f6n\u00b7nen", "wir", "nach", "Be\u00b7lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die Arbeit auf sp\u00e4ter verschieben.", "tokens": ["Die", "Ar\u00b7beit", "auf", "sp\u00e4\u00b7ter", "ver\u00b7schie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Denn wenn man das Gas uns sperren l\u00e4\u00dft", "tokens": ["Denn", "wenn", "man", "das", "Gas", "uns", "sper\u00b7ren", "l\u00e4\u00dft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PIS", "ART", "NN", "PPER", "VVINF", "VVFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Oder kein Bier ohne Bargeld mehr gibt,", "tokens": ["O\u00b7der", "kein", "Bier", "oh\u00b7ne", "Bar\u00b7geld", "mehr", "gibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPR", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Dann kriechen wir gleich nach Mittag ins Nest", "tokens": ["Dann", "krie\u00b7chen", "wir", "gleich", "nach", "Mit\u00b7tag", "ins", "Nest"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "APPRART", "NN"], "meter": "-+-+-+---+", "measure": "zehnsilber"}, "line.6": {"text": "Und schlafen, solange es uns beliebt.", "tokens": ["Und", "schla\u00b7fen", ",", "so\u00b7lan\u00b7ge", "es", "uns", "be\u00b7liebt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PPER", "PRF", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.14": {"line.1": {"text": "Freilich: Der feste Lohn f\u00e4llt nun fort,", "tokens": ["Frei\u00b7lich", ":", "Der", "fes\u00b7te", "Lohn", "f\u00e4llt", "nun", "fort", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ART", "ADJA", "NN", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Aber die Freiheit ist auch was wert.", "tokens": ["A\u00b7ber", "die", "Frei\u00b7heit", "ist", "auch", "was", "wert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADV", "PWS", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Und das mit dem Schaukelpferd", "tokens": ["Und", "das", "mit", "dem", "Schau\u00b7kel\u00b7pferd"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist jetzt unser Wintersport.", "tokens": ["Ist", "jetzt", "un\u00b7ser", "Win\u00b7ter\u00b7sport", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}