{"textgrid.poem.49811": {"metadata": {"author": {"name": "Abschatz, Hans A\u00dfmann von", "birth": "N.A.", "death": "N.A."}, "title": "Eines Englischen Hundes", "genre": "verse", "period": "N.A.", "pub_year": 1672, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein Leser/ unter diesem Steine/", "tokens": ["Mein", "Le\u00b7ser", "/", "un\u00b7ter", "die\u00b7sem", "Stei\u00b7ne", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "APPR", "PDAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ruht Rodomonds Gebeine/", "tokens": ["Ruht", "Ro\u00b7do\u00b7monds", "Ge\u00b7bei\u00b7ne", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der in der Bl\u00fctte seiner Jahr", "tokens": ["Der", "in", "der", "Bl\u00fct\u00b7te", "sei\u00b7ner", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Ausbund sch\u00f6ner Hunde war/", "tokens": ["Ein", "Aus\u00b7bund", "sch\u00f6\u00b7ner", "Hun\u00b7de", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der sich als einen Held in Kampff und Streit erwiesen/", "tokens": ["Der", "sich", "als", "ei\u00b7nen", "Held", "in", "Kampff", "und", "Streit", "er\u00b7wie\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "KOUS", "ART", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und wegen seiner Treu vor andern wird gepriesen.", "tokens": ["Und", "we\u00b7gen", "sei\u00b7ner", "Treu", "vor", "an\u00b7dern", "wird", "ge\u00b7prie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "APPR", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Der unter tausend klugen Hunden", "tokens": ["Der", "un\u00b7ter", "tau\u00b7send", "klu\u00b7gen", "Hun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verst\u00e4ndig ward erfunden.", "tokens": ["Ver\u00b7st\u00e4n\u00b7dig", "ward", "er\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der ihm durch ungemeine Kunst/", "tokens": ["Der", "ihm", "durch", "un\u00b7ge\u00b7mei\u00b7ne", "Kunst", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verdient des Herren gutte Gunst/", "tokens": ["Ver\u00b7di\u00b7ent", "des", "Her\u00b7ren", "gut\u00b7te", "Gunst", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADJA", "NN", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Der Freunde Lieb und Hold/ der Feinde Furcht und Schrecken/", "tokens": ["Der", "Freun\u00b7de", "Lieb", "und", "Hold", "/", "der", "Fein\u00b7de", "Furcht", "und", "Schre\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NE", "$(", "ART", "NN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Das nach dem Tode noch sein Nahme kan erwecken.", "tokens": ["Das", "nach", "dem", "To\u00b7de", "noch", "sein", "Nah\u00b7me", "kan", "er\u00b7we\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "ADV", "PPOSAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Zwar hat das Ende seinem Leben", "tokens": ["Zwar", "hat", "das", "En\u00b7de", "sei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein schwerer Tod gegeben/", "tokens": ["Ein", "schwe\u00b7rer", "Tod", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Doch siel er als ein k\u00fchner Held/", "tokens": ["Doch", "siel", "er", "als", "ein", "k\u00fch\u00b7ner", "Held", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "KOUS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von keiner schn\u00f6den Hand gef\u00e4llt.", "tokens": ["Von", "kei\u00b7ner", "schn\u00f6\u00b7den", "Hand", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Er kan die Kranckheit nicht/ sie ihn nicht \u00fcberwinden/", "tokens": ["Er", "kan", "die", "Kran\u00b7ck\u00b7heit", "nicht", "/", "sie", "ihn", "nicht", "\u00fc\u00b7berw\u00b7in\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "$(", "PPER", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Ihn mu\u00df ein hei\u00dfes Bley auff gr\u00fcner Au entbinden.", "tokens": ["Ihn", "mu\u00df", "ein", "hei\u00b7\u00dfes", "Bley", "auff", "gr\u00fc\u00b7ner", "Au", "ent\u00b7bin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Der Schiffer f\u00fcrcht sich nicht in Wellen", "tokens": ["Der", "Schif\u00b7fer", "f\u00fcrcht", "sich", "nicht", "in", "Wel\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "PTKNEG", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Grabmahl zu bestellen.", "tokens": ["Sein", "Grab\u00b7mahl", "zu", "be\u00b7stel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Bergmann zieht dem Tode nach", "tokens": ["Der", "Berg\u00b7mann", "zieht", "dem", "To\u00b7de", "nach"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Pluto finstres Schlaff-Gemach:", "tokens": ["In", "Plu\u00b7to", "finst\u00b7res", "Schlaff\u00b7Ge\u00b7mach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Held wird ihm sein Grab mit Blutte lieber f\u00e4rben/", "tokens": ["Ein", "Held", "wird", "ihm", "sein", "Grab", "mit", "Blut\u00b7te", "lie\u00b7ber", "f\u00e4r\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "PPOSAT", "NN", "APPR", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Als auff gemeine Weis' in siechen Lager sterben.", "tokens": ["Als", "auff", "ge\u00b7mei\u00b7ne", "Weis'", "in", "sie\u00b7chen", "La\u00b7ger", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ADJA", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Mu\u00df gleich der Leib im Tode b\u00fcssen/", "tokens": ["Mu\u00df", "gleich", "der", "Leib", "im", "To\u00b7de", "b\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den Staub der Bayre k\u00fcssen/", "tokens": ["Den", "Staub", "der", "Bay\u00b7re", "k\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So bleibt doch der bekandte Ruhm", "tokens": ["So", "bleibt", "doch", "der", "be\u00b7kand\u00b7te", "Ruhm"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein unverg\u00e4nglich Eigenthum.", "tokens": ["Sein", "un\u00b7ver\u00b7g\u00e4ng\u00b7lich", "Ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Es schreibt sein hohes Lob bey Ph\u00f6bus Wagen-R\u00e4der", "tokens": ["Es", "schreibt", "sein", "ho\u00b7hes", "Lob", "bey", "Ph\u00f6\u00b7bus", "Wa\u00b7gen\u00b7R\u00e4\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "APPR", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit Diamanten an gelehrter Leute Feder.", "tokens": ["Mit", "Di\u00b7a\u00b7man\u00b7ten", "an", "ge\u00b7lehr\u00b7ter", "Leu\u00b7te", "Fe\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Du/ dem sich im f\u00fcr\u00fcber-reisen", "tokens": ["Du", "/", "dem", "sich", "im", "f\u00fcr\u00b7\u00fcber\u00b7rei\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$(", "PRELS", "PRF", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Di\u00df schlechte Grab wird weisen.", "tokens": ["Di\u00df", "schlech\u00b7te", "Grab", "wird", "wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Spreit \u00fcber dieses Todten-Hau\u00df", "tokens": ["Spreit", "\u00fc\u00b7ber", "die\u00b7ses", "Tod\u00b7ten\u00b7Hau\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit vollen H\u00e4nden Blumen aus/", "tokens": ["Mit", "vol\u00b7len", "H\u00e4n\u00b7den", "Blu\u00b7men", "aus", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und wo noch Rodomund so viel ist werh gewesen/", "tokens": ["Und", "wo", "noch", "Ro\u00b7do\u00b7mund", "so", "viel", "ist", "werh", "ge\u00b7we\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ADV", "NN", "ADV", "ADV", "VAFIN", "ADJD", "VAPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Bem\u00fch dich beygef\u00fcgt sein Testament zu lesen.", "tokens": ["Be\u00b7m\u00fch", "dich", "bey\u00b7ge\u00b7f\u00fcgt", "sein", "Tes\u00b7ta\u00b7ment", "zu", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Mein Leser/ unter diesem Steine/", "tokens": ["Mein", "Le\u00b7ser", "/", "un\u00b7ter", "die\u00b7sem", "Stei\u00b7ne", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "APPR", "PDAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ruht Rodomonds Gebeine/", "tokens": ["Ruht", "Ro\u00b7do\u00b7monds", "Ge\u00b7bei\u00b7ne", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der in der Bl\u00fctte seiner Jahr", "tokens": ["Der", "in", "der", "Bl\u00fct\u00b7te", "sei\u00b7ner", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Ausbund sch\u00f6ner Hunde war/", "tokens": ["Ein", "Aus\u00b7bund", "sch\u00f6\u00b7ner", "Hun\u00b7de", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der sich als einen Held in Kampff und Streit erwiesen/", "tokens": ["Der", "sich", "als", "ei\u00b7nen", "Held", "in", "Kampff", "und", "Streit", "er\u00b7wie\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "KOUS", "ART", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und wegen seiner Treu vor andern wird gepriesen.", "tokens": ["Und", "we\u00b7gen", "sei\u00b7ner", "Treu", "vor", "an\u00b7dern", "wird", "ge\u00b7prie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "APPR", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Der unter tausend klugen Hunden", "tokens": ["Der", "un\u00b7ter", "tau\u00b7send", "klu\u00b7gen", "Hun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verst\u00e4ndig ward erfunden.", "tokens": ["Ver\u00b7st\u00e4n\u00b7dig", "ward", "er\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der ihm durch ungemeine Kunst/", "tokens": ["Der", "ihm", "durch", "un\u00b7ge\u00b7mei\u00b7ne", "Kunst", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verdient des Herren gutte Gunst/", "tokens": ["Ver\u00b7di\u00b7ent", "des", "Her\u00b7ren", "gut\u00b7te", "Gunst", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADJA", "NN", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Der Freunde Lieb und Hold/ der Feinde Furcht und Schrecken/", "tokens": ["Der", "Freun\u00b7de", "Lieb", "und", "Hold", "/", "der", "Fein\u00b7de", "Furcht", "und", "Schre\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NE", "$(", "ART", "NN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Das nach dem Tode noch sein Nahme kan erwecken.", "tokens": ["Das", "nach", "dem", "To\u00b7de", "noch", "sein", "Nah\u00b7me", "kan", "er\u00b7we\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "ADV", "PPOSAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Zwar hat das Ende seinem Leben", "tokens": ["Zwar", "hat", "das", "En\u00b7de", "sei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein schwerer Tod gegeben/", "tokens": ["Ein", "schwe\u00b7rer", "Tod", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Doch siel er als ein k\u00fchner Held/", "tokens": ["Doch", "siel", "er", "als", "ein", "k\u00fch\u00b7ner", "Held", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "KOUS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von keiner schn\u00f6den Hand gef\u00e4llt.", "tokens": ["Von", "kei\u00b7ner", "schn\u00f6\u00b7den", "Hand", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Er kan die Kranckheit nicht/ sie ihn nicht \u00fcberwinden/", "tokens": ["Er", "kan", "die", "Kran\u00b7ck\u00b7heit", "nicht", "/", "sie", "ihn", "nicht", "\u00fc\u00b7berw\u00b7in\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "$(", "PPER", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Ihn mu\u00df ein hei\u00dfes Bley auff gr\u00fcner Au entbinden.", "tokens": ["Ihn", "mu\u00df", "ein", "hei\u00b7\u00dfes", "Bley", "auff", "gr\u00fc\u00b7ner", "Au", "ent\u00b7bin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Der Schiffer f\u00fcrcht sich nicht in Wellen", "tokens": ["Der", "Schif\u00b7fer", "f\u00fcrcht", "sich", "nicht", "in", "Wel\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "PTKNEG", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Grabmahl zu bestellen.", "tokens": ["Sein", "Grab\u00b7mahl", "zu", "be\u00b7stel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Bergmann zieht dem Tode nach", "tokens": ["Der", "Berg\u00b7mann", "zieht", "dem", "To\u00b7de", "nach"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Pluto finstres Schlaff-Gemach:", "tokens": ["In", "Plu\u00b7to", "finst\u00b7res", "Schlaff\u00b7Ge\u00b7mach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Held wird ihm sein Grab mit Blutte lieber f\u00e4rben/", "tokens": ["Ein", "Held", "wird", "ihm", "sein", "Grab", "mit", "Blut\u00b7te", "lie\u00b7ber", "f\u00e4r\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "PPOSAT", "NN", "APPR", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Als auff gemeine Weis' in siechen Lager sterben.", "tokens": ["Als", "auff", "ge\u00b7mei\u00b7ne", "Weis'", "in", "sie\u00b7chen", "La\u00b7ger", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ADJA", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Mu\u00df gleich der Leib im Tode b\u00fcssen/", "tokens": ["Mu\u00df", "gleich", "der", "Leib", "im", "To\u00b7de", "b\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den Staub der Bayre k\u00fcssen/", "tokens": ["Den", "Staub", "der", "Bay\u00b7re", "k\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So bleibt doch der bekandte Ruhm", "tokens": ["So", "bleibt", "doch", "der", "be\u00b7kand\u00b7te", "Ruhm"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein unverg\u00e4nglich Eigenthum.", "tokens": ["Sein", "un\u00b7ver\u00b7g\u00e4ng\u00b7lich", "Ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Es schreibt sein hohes Lob bey Ph\u00f6bus Wagen-R\u00e4der", "tokens": ["Es", "schreibt", "sein", "ho\u00b7hes", "Lob", "bey", "Ph\u00f6\u00b7bus", "Wa\u00b7gen\u00b7R\u00e4\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "APPR", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit Diamanten an gelehrter Leute Feder.", "tokens": ["Mit", "Di\u00b7a\u00b7man\u00b7ten", "an", "ge\u00b7lehr\u00b7ter", "Leu\u00b7te", "Fe\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Du/ dem sich im f\u00fcr\u00fcber-reisen", "tokens": ["Du", "/", "dem", "sich", "im", "f\u00fcr\u00b7\u00fcber\u00b7rei\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$(", "PRELS", "PRF", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Di\u00df schlechte Grab wird weisen.", "tokens": ["Di\u00df", "schlech\u00b7te", "Grab", "wird", "wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Spreit \u00fcber dieses Todten-Hau\u00df", "tokens": ["Spreit", "\u00fc\u00b7ber", "die\u00b7ses", "Tod\u00b7ten\u00b7Hau\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit vollen H\u00e4nden Blumen aus/", "tokens": ["Mit", "vol\u00b7len", "H\u00e4n\u00b7den", "Blu\u00b7men", "aus", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und wo noch Rodomund so viel ist werh gewesen/", "tokens": ["Und", "wo", "noch", "Ro\u00b7do\u00b7mund", "so", "viel", "ist", "werh", "ge\u00b7we\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ADV", "NN", "ADV", "ADV", "VAFIN", "ADJD", "VAPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Bem\u00fch dich beygef\u00fcgt sein Testament zu lesen.", "tokens": ["Be\u00b7m\u00fch", "dich", "bey\u00b7ge\u00b7f\u00fcgt", "sein", "Tes\u00b7ta\u00b7ment", "zu", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}