{"textgrid.poem.57931": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Circulus vitiosus", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Am \u00c4gidientor in Hannover", "tokens": ["Am", "\u00c4\u00b7gi\u00b7di\u00b7en\u00b7tor", "in", "Han\u00b7no\u00b7ver"], "token_info": ["word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ist der Stra\u00dfenbahnrendezvousplatz,", "tokens": ["Ist", "der", "Stra\u00b7\u00dfen\u00b7bahn\u00b7ren\u00b7dez\u00b7vou\u00b7splatz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da trifft die gr\u00fcne die rote,", "tokens": ["Da", "trifft", "die", "gr\u00fc\u00b7ne", "die", "ro\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "ART", "ADJA", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die St\u00f6ckener den Hildesheimer Schatz.", "tokens": ["Die", "St\u00f6\u00b7cke\u00b7ner", "den", "Hil\u00b7des\u00b7hei\u00b7mer", "Schatz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Es ist r\u00fchrend anzusehn,", "tokens": ["Es", "ist", "r\u00fch\u00b7rend", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVIZU", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Freude und die Z\u00e4rtlichkeit,", "tokens": ["Die", "Freu\u00b7de", "und", "die", "Z\u00e4rt\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie k\u00f6nnen sich gar nicht trennen,", "tokens": ["Sie", "k\u00f6n\u00b7nen", "sich", "gar", "nicht", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sie vergessen Ort und Zeit.", "tokens": ["Sie", "ver\u00b7ges\u00b7sen", "Ort", "und", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Sie kommen aus der Breiten-", "tokens": ["Sie", "kom\u00b7men", "aus", "der", "Brei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und aus der Georgenstra\u00df',", "tokens": ["Und", "aus", "der", "Ge\u00b7or\u00b7gen\u00b7stra\u00df'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Von der Prinzen- und Marien-,", "tokens": ["Von", "der", "Prin\u00b7zen", "und", "Ma\u00b7ri\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "TRUNC", "KON", "TRUNC", "$,"], "meter": "--+-++--", "measure": "anapaest.init"}, "line.4": {"text": "Und so weiter ohne Unterla\u00df.", "tokens": ["Und", "so", "wei\u00b7ter", "oh\u00b7ne", "Un\u00b7ter\u00b7la\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Jeden Augenblick kommt noch eine,", "tokens": ["Je\u00b7den", "Au\u00b7gen\u00b7blick", "kommt", "noch", "ei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ADV", "ART", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und f\u00e4hrt endlich eine weg,", "tokens": ["Und", "f\u00e4hrt", "end\u00b7lich", "ei\u00b7ne", "weg", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "PTKVZ", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "So kommt schon wieder eine andre", "tokens": ["So", "kommt", "schon", "wie\u00b7der", "ei\u00b7ne", "and\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von dieser oder jener Eck'.", "tokens": ["Von", "die\u00b7ser", "o\u00b7der", "je\u00b7ner", "Eck'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "KON", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Oft sind es blo\u00df sechs bis sieben,", "tokens": ["Oft", "sind", "es", "blo\u00df", "sechs", "bis", "sie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "CARD", "APPR", "CARD", "$,"], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Meist aber zwanzig oder mehr,", "tokens": ["Meist", "a\u00b7ber", "zwan\u00b7zig", "o\u00b7der", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "CARD", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Von denen dann diese oder jene", "tokens": ["Von", "de\u00b7nen", "dann", "die\u00b7se", "o\u00b7der", "je\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADV", "PDAT", "KON", "PDAT"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Vor Freude f\u00e4hrt hin und her.", "tokens": ["Vor", "Freu\u00b7de", "f\u00e4hrt", "hin", "und", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Oder eine bekommt den Rappel", "tokens": ["O\u00b7der", "ei\u00b7ne", "be\u00b7kommt", "den", "Rap\u00b7pel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VVFIN", "ART", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und f\u00e4hrt um den Platz herum", "tokens": ["Und", "f\u00e4hrt", "um", "den", "Platz", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "APZR"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zum allergr\u00f6\u00dften Erg\u00f6tzen", "tokens": ["Zum", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7ten", "Er\u00b7g\u00f6t\u00b7zen"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Von dem geehrten Publikum.", "tokens": ["Von", "dem", "ge\u00b7ehr\u00b7ten", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Aber auf einmal f\u00e4llt ihnen ein,", "tokens": ["A\u00b7ber", "auf", "ein\u00b7mal", "f\u00e4llt", "ih\u00b7nen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Da\u00df sie noch etwas hatten vor,", "tokens": ["Da\u00df", "sie", "noch", "et\u00b7was", "hat\u00b7ten", "vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VAFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und dann ist es pl\u00f6tzlich einsam", "tokens": ["Und", "dann", "ist", "es", "pl\u00f6tz\u00b7lich", "ein\u00b7sam"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu Hannover am \u00c4gidientor.", "tokens": ["Zu", "Han\u00b7no\u00b7ver", "am", "\u00c4\u00b7gi\u00b7di\u00b7en\u00b7tor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Das dauert aber meist nicht lange,", "tokens": ["Das", "dau\u00b7ert", "a\u00b7ber", "meist", "nicht", "lan\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sind sie wieder alle zusamm',", "tokens": ["Dann", "sind", "sie", "wie\u00b7der", "al\u00b7le", "zu\u00b7samm'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Und erz\u00e4hlen eine der andern,", "tokens": ["Und", "er\u00b7z\u00e4h\u00b7len", "ei\u00b7ne", "der", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ART", "ADJA", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Was sie unterwegs gesehen ham.", "tokens": ["Was", "sie", "un\u00b7ter\u00b7wegs", "ge\u00b7se\u00b7hen", "ham", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVPP", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "Das Publikum findet das \u00f6de,", "tokens": ["Das", "Pub\u00b7li\u00b7kum", "fin\u00b7det", "das", "\u00f6\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "ADJA", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Das Publikum, das ist so dumm,", "tokens": ["Das", "Pub\u00b7li\u00b7kum", ",", "das", "ist", "so", "dumm", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PDS", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es denkt, da\u00df die Stra\u00dfenbahn w\u00e4re", "tokens": ["Es", "denkt", ",", "da\u00df", "die", "Stra\u00b7\u00dfen\u00b7bahn", "w\u00e4\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "ART", "NN", "VAFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von wegen dem Publikum.", "tokens": ["Von", "we\u00b7gen", "dem", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.10": {"line.1": {"text": "Am \u00c4gidientor in Hannover", "tokens": ["Am", "\u00c4\u00b7gi\u00b7di\u00b7en\u00b7tor", "in", "Han\u00b7no\u00b7ver"], "token_info": ["word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ist der Stra\u00dfenbahnrendezvousplatz,", "tokens": ["Ist", "der", "Stra\u00b7\u00dfen\u00b7bahn\u00b7ren\u00b7dez\u00b7vou\u00b7splatz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da trifft die gr\u00fcne die rote,", "tokens": ["Da", "trifft", "die", "gr\u00fc\u00b7ne", "die", "ro\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "ART", "ADJA", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die St\u00f6ckener den Hildesheimer Schatz.", "tokens": ["Die", "St\u00f6\u00b7cke\u00b7ner", "den", "Hil\u00b7des\u00b7hei\u00b7mer", "Schatz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Es ist r\u00fchrend anzusehn,", "tokens": ["Es", "ist", "r\u00fch\u00b7rend", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVIZU", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Freude und die Z\u00e4rtlichkeit,", "tokens": ["Die", "Freu\u00b7de", "und", "die", "Z\u00e4rt\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie k\u00f6nnen sich gar nicht trennen,", "tokens": ["Sie", "k\u00f6n\u00b7nen", "sich", "gar", "nicht", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sie vergessen Ort und Zeit.", "tokens": ["Sie", "ver\u00b7ges\u00b7sen", "Ort", "und", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Sie kommen aus der Breiten-", "tokens": ["Sie", "kom\u00b7men", "aus", "der", "Brei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und aus der Georgenstra\u00df',", "tokens": ["Und", "aus", "der", "Ge\u00b7or\u00b7gen\u00b7stra\u00df'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Von der Prinzen- und Marien-,", "tokens": ["Von", "der", "Prin\u00b7zen", "und", "Ma\u00b7ri\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "TRUNC", "KON", "TRUNC", "$,"], "meter": "--+-++--", "measure": "anapaest.init"}, "line.4": {"text": "Und so weiter ohne Unterla\u00df.", "tokens": ["Und", "so", "wei\u00b7ter", "oh\u00b7ne", "Un\u00b7ter\u00b7la\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "Jeden Augenblick kommt noch eine,", "tokens": ["Je\u00b7den", "Au\u00b7gen\u00b7blick", "kommt", "noch", "ei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ADV", "ART", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und f\u00e4hrt endlich eine weg,", "tokens": ["Und", "f\u00e4hrt", "end\u00b7lich", "ei\u00b7ne", "weg", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "PTKVZ", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "So kommt schon wieder eine andre", "tokens": ["So", "kommt", "schon", "wie\u00b7der", "ei\u00b7ne", "and\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von dieser oder jener Eck'.", "tokens": ["Von", "die\u00b7ser", "o\u00b7der", "je\u00b7ner", "Eck'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "KON", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Oft sind es blo\u00df sechs bis sieben,", "tokens": ["Oft", "sind", "es", "blo\u00df", "sechs", "bis", "sie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "CARD", "APPR", "CARD", "$,"], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Meist aber zwanzig oder mehr,", "tokens": ["Meist", "a\u00b7ber", "zwan\u00b7zig", "o\u00b7der", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "CARD", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Von denen dann diese oder jene", "tokens": ["Von", "de\u00b7nen", "dann", "die\u00b7se", "o\u00b7der", "je\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADV", "PDAT", "KON", "PDAT"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Vor Freude f\u00e4hrt hin und her.", "tokens": ["Vor", "Freu\u00b7de", "f\u00e4hrt", "hin", "und", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Oder eine bekommt den Rappel", "tokens": ["O\u00b7der", "ei\u00b7ne", "be\u00b7kommt", "den", "Rap\u00b7pel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VVFIN", "ART", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und f\u00e4hrt um den Platz herum", "tokens": ["Und", "f\u00e4hrt", "um", "den", "Platz", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "APZR"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zum allergr\u00f6\u00dften Erg\u00f6tzen", "tokens": ["Zum", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7ten", "Er\u00b7g\u00f6t\u00b7zen"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Von dem geehrten Publikum.", "tokens": ["Von", "dem", "ge\u00b7ehr\u00b7ten", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Aber auf einmal f\u00e4llt ihnen ein,", "tokens": ["A\u00b7ber", "auf", "ein\u00b7mal", "f\u00e4llt", "ih\u00b7nen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Da\u00df sie noch etwas hatten vor,", "tokens": ["Da\u00df", "sie", "noch", "et\u00b7was", "hat\u00b7ten", "vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VAFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und dann ist es pl\u00f6tzlich einsam", "tokens": ["Und", "dann", "ist", "es", "pl\u00f6tz\u00b7lich", "ein\u00b7sam"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu Hannover am \u00c4gidientor.", "tokens": ["Zu", "Han\u00b7no\u00b7ver", "am", "\u00c4\u00b7gi\u00b7di\u00b7en\u00b7tor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Das dauert aber meist nicht lange,", "tokens": ["Das", "dau\u00b7ert", "a\u00b7ber", "meist", "nicht", "lan\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sind sie wieder alle zusamm',", "tokens": ["Dann", "sind", "sie", "wie\u00b7der", "al\u00b7le", "zu\u00b7samm'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Und erz\u00e4hlen eine der andern,", "tokens": ["Und", "er\u00b7z\u00e4h\u00b7len", "ei\u00b7ne", "der", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ART", "ADJA", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Was sie unterwegs gesehen ham.", "tokens": ["Was", "sie", "un\u00b7ter\u00b7wegs", "ge\u00b7se\u00b7hen", "ham", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVPP", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.18": {"line.1": {"text": "Das Publikum findet das \u00f6de,", "tokens": ["Das", "Pub\u00b7li\u00b7kum", "fin\u00b7det", "das", "\u00f6\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "ADJA", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Das Publikum, das ist so dumm,", "tokens": ["Das", "Pub\u00b7li\u00b7kum", ",", "das", "ist", "so", "dumm", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PDS", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es denkt, da\u00df die Stra\u00dfenbahn w\u00e4re", "tokens": ["Es", "denkt", ",", "da\u00df", "die", "Stra\u00b7\u00dfen\u00b7bahn", "w\u00e4\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "ART", "NN", "VAFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von wegen dem Publikum.", "tokens": ["Von", "we\u00b7gen", "dem", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}}}}