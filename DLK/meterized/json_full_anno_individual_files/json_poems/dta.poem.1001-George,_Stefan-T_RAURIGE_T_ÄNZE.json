{"dta.poem.1001": {"metadata": {"author": {"name": "George, Stefan", "birth": "N.A.", "death": "N.A."}, "title": "T RAURIGE   T \u00c4NZE", "genre": "Lyrik", "period": "N.A.", "pub_year": "1897", "urn": "urn:nbn:de:kobv:b4-200905191530", "language": ["de:0.99"], "booktitle": "George, Stefan: Das Jahr der Seele. Berlin, 1897."}, "poem": {"stanza.1": {"line.1": {"text": "Wir werden nicht mehr starr und bleich               ", "tokens": ["Wir", "wer\u00b7den", "nicht", "mehr", "starr", "und", "bleich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADV", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den fr&#252;heren liebeshelden gleich               ", "tokens": ["Den", "fr", "&#252;", "he\u00b7ren", "lie\u00b7bes\u00b7hel\u00b7den", "gleich"], "token_info": ["word", "word", "XML_entity", "word", "word", "word"], "pos": ["ART", "ADJA", "$(", "VVIZU", "VVFIN", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "An tr&#252;bsal waren wir zu reich               ", "tokens": ["An", "tr", "&#252;", "bsal", "wa\u00b7ren", "wir", "zu", "reich"], "token_info": ["word", "word", "XML_entity", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "ADJD", "VAFIN", "PPER", "PTKA", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir zucken leis und dulden weich             ", "tokens": ["Wir", "zu\u00b7cken", "leis", "und", "dul\u00b7den", "weich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "VVINF", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sie hiessen tapfer hiessen frei               ", "tokens": ["Sie", "hies\u00b7sen", "tap\u00b7fer", "hies\u00b7sen", "frei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "VVFIN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Trotz ihrer lippen manchem schrei               ", "tokens": ["Trotz", "ih\u00b7rer", "lip\u00b7pen", "man\u00b7chem", "schrei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir litten lang und vielerlei               ", "tokens": ["Wir", "lit\u00b7ten", "lang", "und", "vie\u00b7ler\u00b7lei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "PIAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Doch schweigen m&#252;ssen wir dabei             ", "tokens": ["Doch", "schwei\u00b7gen", "m", "&#252;", "ssen", "wir", "da\u00b7bei"], "token_info": ["word", "word", "word", "XML_entity", "word", "word", "word"], "pos": ["KON", "VVFIN", "NE", "$(", "VVFIN", "PPER", "PAV"], "meter": "-+---+-+", "measure": "dactylic.init"}}, "stanza.3": {"line.1": {"text": "Sie gingen um mit schwert und beil               ", "tokens": ["Sie", "gin\u00b7gen", "um", "mit", "schwert", "und", "beil"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "APPR", "VVFIN", "KON", "XY"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch streiten ist nicht unser teil               ", "tokens": ["Doch", "strei\u00b7ten", "ist", "nicht", "un\u00b7ser", "teil"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "VAFIN", "PTKNEG", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Uns ist der friede nicht mehr feil               ", "tokens": ["Uns", "ist", "der", "frie\u00b7de", "nicht", "mehr", "feil"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "PTKNEG", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um ihrer g&#252;ter weh und heil.             ", "tokens": ["Um", "ih\u00b7rer", "g", "&#252;", "ter", "weh", "und", "heil", "."], "token_info": ["word", "word", "word", "XML_entity", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "$(", "ADJD", "PTKVZ", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}