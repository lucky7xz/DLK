{"textgrid.poem.48664": {"metadata": {"author": {"name": "Fleming, Paul", "birth": "N.A.", "death": "N.A."}, "title": "27. An Basilenen, nachdem er von ihr gereiset war", "genre": "verse", "period": "N.A.", "pub_year": 1624, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ist mein Gl\u00fccke gleich gesonnen", "tokens": ["Ist", "mein", "Gl\u00fc\u00b7cke", "gleich", "ge\u00b7son\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "mich zu f\u00fchren weit von dir,", "tokens": ["mich", "zu", "f\u00fch\u00b7ren", "weit", "von", "dir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "ADJD", "APPR", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "o du Sonne meiner Wonnen,", "tokens": ["o", "du", "Son\u00b7ne", "mei\u00b7ner", "Won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "PPER", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "so verbleibst du doch in mir.", "tokens": ["so", "ver\u00b7bleibst", "du", "doch", "in", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Du in mir und ich in dir", "tokens": ["Du", "in", "mir", "und", "ich", "in", "dir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "PPER", "KON", "PPER", "APPR", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "sind beisammen f\u00fcr und f\u00fcr.", "tokens": ["sind", "bei\u00b7sam\u00b7men", "f\u00fcr", "und", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "APPR", "KON", "APPR", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "K\u00fcnftig werd ich ganz nicht scheuen,", "tokens": ["K\u00fcnf\u00b7tig", "werd", "ich", "ganz", "nicht", "scheu\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kaspis, deine fremde Flut", "tokens": ["Kas\u00b7pis", ",", "dei\u00b7ne", "frem\u00b7de", "Flut"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "und die \u00f6den W\u00fcsteneien,", "tokens": ["und", "die", "\u00f6\u00b7den", "W\u00fcs\u00b7te\u00b7nei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "da man nichts als f\u00fcrchten tut.", "tokens": ["da", "man", "nichts", "als", "f\u00fcrch\u00b7ten", "tut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIS", "KOKOM", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Auch das Wilde macht mir zahm,", "tokens": ["Auch", "das", "Wil\u00b7de", "macht", "mir", "zahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Liebste, dein gelobter Nam'.", "tokens": ["Liebs\u00b7te", ",", "dein", "ge\u00b7lob\u00b7ter", "Nam'", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "\u00dcberstehe diese Stunden,", "tokens": ["\u00dc\u00b7bers\u00b7te\u00b7he", "die\u00b7se", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "$,"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.2": {"text": "Schwester, und sei unverwant.", "tokens": ["Schwes\u00b7ter", ",", "und", "sei", "un\u00b7ver\u00b7want", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich verbleibe dir verbunden", "tokens": ["Ich", "ver\u00b7blei\u00b7be", "dir", "ver\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und du bist mein festes Band.", "tokens": ["und", "du", "bist", "mein", "fes\u00b7tes", "Band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Meines Herzens Trost bist du", "tokens": ["Mei\u00b7nes", "Her\u00b7zens", "Trost", "bist", "du"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "VAFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und mein Herze selbst darzu.", "tokens": ["und", "mein", "Her\u00b7ze", "selbst", "dar\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "VVFIN", "ADV", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Ihr, ihr Tr\u00e4ume, sollt indessen", "tokens": ["Ihr", ",", "ihr", "Tr\u00e4u\u00b7me", ",", "sollt", "in\u00b7des\u00b7sen"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "$,", "PPOSAT", "NN", "$,", "VMFIN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "unter uns das Beste tun.", "tokens": ["un\u00b7ter", "uns", "das", "Bes\u00b7te", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kein Schlaf, der soll ihr vergessen,", "tokens": ["Kein", "Schlaf", ",", "der", "soll", "ihr", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "VMFIN", "PPER", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "ohne mich soll sie nicht ruhn,", "tokens": ["oh\u00b7ne", "mich", "soll", "sie", "nicht", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "da\u00df die s\u00fc\u00dfe Nacht ersetzt,", "tokens": ["da\u00df", "die", "s\u00fc\u00b7\u00dfe", "Nacht", "er\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "was der tr\u00fcbe Tag verletzt.", "tokens": ["was", "der", "tr\u00fc\u00b7be", "Tag", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Lebe, meines Lebens Leben,", "tokens": ["Le\u00b7be", ",", "mei\u00b7nes", "Le\u00b7bens", "Le\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "stirb nicht, meines Todes Tod,", "tokens": ["stirb", "nicht", ",", "mei\u00b7nes", "To\u00b7des", "Tod", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "$,", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "da\u00df wir uns uns wiedergeben,", "tokens": ["da\u00df", "wir", "uns", "uns", "wie\u00b7der\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "abgetan von aller Not.", "tokens": ["ab\u00b7ge\u00b7tan", "von", "al\u00b7ler", "Not", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Sei gegr\u00fc\u00dft, bald Trost, itzt Qual,", "tokens": ["Sei", "ge\u00b7gr\u00fc\u00dft", ",", "bald", "Trost", ",", "itzt", "Qual", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "tausent, tausent, tausentmal!", "tokens": ["tau\u00b7sent", ",", "tau\u00b7sent", ",", "tau\u00b7sent\u00b7mal", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Ist mein Gl\u00fccke gleich gesonnen", "tokens": ["Ist", "mein", "Gl\u00fc\u00b7cke", "gleich", "ge\u00b7son\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "mich zu f\u00fchren weit von dir,", "tokens": ["mich", "zu", "f\u00fch\u00b7ren", "weit", "von", "dir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "ADJD", "APPR", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "o du Sonne meiner Wonnen,", "tokens": ["o", "du", "Son\u00b7ne", "mei\u00b7ner", "Won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "PPER", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "so verbleibst du doch in mir.", "tokens": ["so", "ver\u00b7bleibst", "du", "doch", "in", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Du in mir und ich in dir", "tokens": ["Du", "in", "mir", "und", "ich", "in", "dir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "PPER", "KON", "PPER", "APPR", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "sind beisammen f\u00fcr und f\u00fcr.", "tokens": ["sind", "bei\u00b7sam\u00b7men", "f\u00fcr", "und", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "APPR", "KON", "APPR", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "K\u00fcnftig werd ich ganz nicht scheuen,", "tokens": ["K\u00fcnf\u00b7tig", "werd", "ich", "ganz", "nicht", "scheu\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kaspis, deine fremde Flut", "tokens": ["Kas\u00b7pis", ",", "dei\u00b7ne", "frem\u00b7de", "Flut"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "und die \u00f6den W\u00fcsteneien,", "tokens": ["und", "die", "\u00f6\u00b7den", "W\u00fcs\u00b7te\u00b7nei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "da man nichts als f\u00fcrchten tut.", "tokens": ["da", "man", "nichts", "als", "f\u00fcrch\u00b7ten", "tut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIS", "KOKOM", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Auch das Wilde macht mir zahm,", "tokens": ["Auch", "das", "Wil\u00b7de", "macht", "mir", "zahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Liebste, dein gelobter Nam'.", "tokens": ["Liebs\u00b7te", ",", "dein", "ge\u00b7lob\u00b7ter", "Nam'", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "\u00dcberstehe diese Stunden,", "tokens": ["\u00dc\u00b7bers\u00b7te\u00b7he", "die\u00b7se", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "$,"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.2": {"text": "Schwester, und sei unverwant.", "tokens": ["Schwes\u00b7ter", ",", "und", "sei", "un\u00b7ver\u00b7want", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich verbleibe dir verbunden", "tokens": ["Ich", "ver\u00b7blei\u00b7be", "dir", "ver\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und du bist mein festes Band.", "tokens": ["und", "du", "bist", "mein", "fes\u00b7tes", "Band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Meines Herzens Trost bist du", "tokens": ["Mei\u00b7nes", "Her\u00b7zens", "Trost", "bist", "du"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "VAFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und mein Herze selbst darzu.", "tokens": ["und", "mein", "Her\u00b7ze", "selbst", "dar\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "VVFIN", "ADV", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Ihr, ihr Tr\u00e4ume, sollt indessen", "tokens": ["Ihr", ",", "ihr", "Tr\u00e4u\u00b7me", ",", "sollt", "in\u00b7des\u00b7sen"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "$,", "PPOSAT", "NN", "$,", "VMFIN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "unter uns das Beste tun.", "tokens": ["un\u00b7ter", "uns", "das", "Bes\u00b7te", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kein Schlaf, der soll ihr vergessen,", "tokens": ["Kein", "Schlaf", ",", "der", "soll", "ihr", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "VMFIN", "PPER", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "ohne mich soll sie nicht ruhn,", "tokens": ["oh\u00b7ne", "mich", "soll", "sie", "nicht", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "da\u00df die s\u00fc\u00dfe Nacht ersetzt,", "tokens": ["da\u00df", "die", "s\u00fc\u00b7\u00dfe", "Nacht", "er\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "was der tr\u00fcbe Tag verletzt.", "tokens": ["was", "der", "tr\u00fc\u00b7be", "Tag", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Lebe, meines Lebens Leben,", "tokens": ["Le\u00b7be", ",", "mei\u00b7nes", "Le\u00b7bens", "Le\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "stirb nicht, meines Todes Tod,", "tokens": ["stirb", "nicht", ",", "mei\u00b7nes", "To\u00b7des", "Tod", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "$,", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "da\u00df wir uns uns wiedergeben,", "tokens": ["da\u00df", "wir", "uns", "uns", "wie\u00b7der\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "abgetan von aller Not.", "tokens": ["ab\u00b7ge\u00b7tan", "von", "al\u00b7ler", "Not", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Sei gegr\u00fc\u00dft, bald Trost, itzt Qual,", "tokens": ["Sei", "ge\u00b7gr\u00fc\u00dft", ",", "bald", "Trost", ",", "itzt", "Qual", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "tausent, tausent, tausentmal!", "tokens": ["tau\u00b7sent", ",", "tau\u00b7sent", ",", "tau\u00b7sent\u00b7mal", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}