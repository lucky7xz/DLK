{"textgrid.poem.53874": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Week-End", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Erst sagt es einer.", "tokens": ["Erst", "sagt", "es", "ei\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Denn ists ne Weile still,", "tokens": ["Denn", "ists", "ne", "Wei\u00b7le", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "weil keiner will.", "tokens": ["weil", "kei\u00b7ner", "will", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Dann kommen aber zu Haufen", "tokens": ["Dann", "kom\u00b7men", "a\u00b7ber", "zu", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "APPR", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "die Organisationsorganisatoren gelaufen:", "tokens": ["die", "Or\u00b7ga\u00b7ni\u00b7sa\u00b7ti\u00b7ons\u00b7or\u00b7ga\u00b7ni\u00b7sa\u00b7to\u00b7ren", "ge\u00b7lau\u00b7fen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+----+-++-+--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Beamte und Journalisten", "tokens": ["Be\u00b7am\u00b7te", "und", "Jour\u00b7na\u00b7lis\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "KON", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "und andre Juden und sogar Christen \u2013", "tokens": ["und", "and\u00b7re", "Ju\u00b7den", "und", "so\u00b7gar", "Chris\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "KON", "ADV", "NN", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "und ein ganzes Komitee", "tokens": ["und", "ein", "gan\u00b7zes", "Ko\u00b7mi\u00b7tee"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und Offiziere a. D.", "tokens": ["und", "Of\u00b7fi\u00b7zie\u00b7re", "a.", "D."], "token_info": ["word", "word", "abbreviation", "abbreviation"], "pos": ["KON", "NN", "APPRART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Propaganda? Famos!", "tokens": ["Pro\u00b7pa\u00b7gan\u00b7da", "?", "Fa\u00b7mos", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Jetzt gehts los.", "tokens": ["Jetzt", "gehts", "los", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKVZ", "$."], "meter": "+--", "measure": "dactylic.init"}}, "stanza.4": {"line.1": {"text": "Sie kleben Plakate", "tokens": ["Sie", "kle\u00b7ben", "Pla\u00b7ka\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["PPER", "ADJA", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "und Bildinserate", "tokens": ["und", "Bil\u00b7din\u00b7se\u00b7ra\u00b7te"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "und sind nie alleine", "tokens": ["und", "sind", "nie", "al\u00b7lei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "ADV"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "und gr\u00fcnden Vereine;", "tokens": ["und", "gr\u00fcn\u00b7den", "Ver\u00b7ei\u00b7ne", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Deutschlands \u00e4ltester Soldat", "tokens": ["Deutschlands", "\u00e4l\u00b7tes\u00b7ter", "Sol\u00b7dat"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "hat das Ehrenprotektorat . . .", "tokens": ["hat", "das", "Eh\u00b7ren\u00b7pro\u00b7tek\u00b7to\u00b7rat", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "$.", "$.", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.7": {"text": "und es l\u00e4\u00dft sie nicht ruhn,", "tokens": ["und", "es", "l\u00e4\u00dft", "sie", "nicht", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.8": {"text": "und sie haben ze tun.", "tokens": ["und", "sie", "ha\u00b7ben", "ze", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "XY", "VVINF", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.5": {"line.1": {"text": "Wahrheit breitet sich nicht aus,", "tokens": ["Wahr\u00b7heit", "brei\u00b7tet", "sich", "nicht", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "PTKNEG", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "hast die Zeitung du im Haus.", "tokens": ["hast", "die", "Zei\u00b7tung", "du", "im", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Und bald sind die Gehirne bei allen", "tokens": ["Und", "bald", "sind", "die", "Ge\u00b7hir\u00b7ne", "bei", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "APPR", "PIAT"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "von einem linden Wahnsinn befallen:", "tokens": ["von", "ei\u00b7nem", "lin\u00b7den", "Wahn\u00b7sinn", "be\u00b7fal\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbweek-end!\u00ab nuckelt der Embryo;", "tokens": ["\u00bb", "week\u00b7end", "!", "\u00ab", "nu\u00b7ckelt", "der", "Emb\u00b7ryo", ";"], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$.", "$(", "VVFIN", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": "\u00bbweek-end!\u00ab fl\u00fcstert der Gro\u00dfpopo.", "tokens": ["\u00bb", "week\u00b7end", "!", "\u00ab", "fl\u00fcs\u00b7tert", "der", "Gro\u00df\u00b7po\u00b7po", "."], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$.", "$(", "VVFIN", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.5": {"text": "Vergessen die Wirren um Tschiang Kai-schek;", "tokens": ["Ver\u00b7ges\u00b7sen", "die", "Wir\u00b7ren", "um", "Tschi\u00b7ang", "Kai\u00b7schek", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "NE", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.6": {"text": "vergessen der ganze Stahlhelmdreck;", "tokens": ["ver\u00b7ges\u00b7sen", "der", "gan\u00b7ze", "Stahl\u00b7helm\u00b7dreck", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "vergessen der Volksb\u00fchne tiefer Fall . . .", "tokens": ["ver\u00b7ges\u00b7sen", "der", "Volks\u00b7b\u00fch\u00b7ne", "tie\u00b7fer", "Fall", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVPP", "ART", "NN", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "es braust ein Ruf wie Donnerhall:", "tokens": ["es", "braust", "ein", "Ruf", "wie", "Don\u00b7ner\u00b7hall", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KOKOM", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Week-end \u2013!", "tokens": ["Week\u00b7end", "\u2013", "!"], "token_info": ["word", "punct", "punct"], "pos": ["NE", "$(", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.7": {"line.1": {"text": "Wiek-ent-Gamaschen und Wik-end-Zigarren,", "tokens": ["Wiek\u00b7ent\u00b7Ga\u00b7ma\u00b7schen", "und", "Wi\u00b7k\u00b7en\u00b7dZi\u00b7gar\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Wiehk-end-Windeln und Wigent-Knarren;", "tokens": ["Wieh\u00b7k\u00b7en\u00b7dWin\u00b7deln", "und", "Wi\u00b7gent\u00b7Knar\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Wieghennd-Nachtt\u00f6pfe (mit drei Henkeln),", "tokens": ["Wieg\u00b7henn\u00b7dNacht\u00b7t\u00f6p\u00b7fe", "(", "mit", "drei", "Hen\u00b7keln", ")", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$(", "APPR", "CARD", "NN", "$(", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wieck\u00e4nt-Stiefel mit Wieg\u00e4nd-Senkeln . . .", "tokens": ["Wieck\u00e4nt\u00b7S\u00b7tie\u00b7fel", "mit", "Wie\u00b7g\u00e4n\u00b7dSen\u00b7keln", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "APPR", "NN", "$.", "$.", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Weegent-H\u00e4uschen und Wiekent-Bauch,", "tokens": ["Wee\u00b7gent\u00b7H\u00e4us\u00b7chen", "und", "Wie\u00b7kent\u00b7Bauch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.6": {"text": "und was London kann, das k\u00f6nnen wir auch.", "tokens": ["und", "was", "Lon\u00b7don", "kann", ",", "das", "k\u00f6n\u00b7nen", "wir", "auch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "NE", "VMFIN", "$,", "PDS", "VMFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.8": {"line.1": {"text": "Blo\u00df:", "tokens": ["Blo\u00df", ":"], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Die Geh\u00e4lter der kleinen Angestellten", "tokens": ["Die", "Ge\u00b7h\u00e4l\u00b7ter", "der", "klei\u00b7nen", "An\u00b7ge\u00b7stell\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "erh\u00f6hen sich in Deutschland selten . . .", "tokens": ["er\u00b7h\u00f6\u00b7hen", "sich", "in", "Deutschland", "sel\u00b7ten", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NE", "ADJD", "$.", "$.", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Mit 145 Mark", "tokens": ["Mit", "145", "Mark"], "token_info": ["word", "number", "word"], "pos": ["APPR", "CARD", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "f\u00fchlt sich nicht jeder week-end-stark.", "tokens": ["f\u00fchlt", "sich", "nicht", "je\u00b7der", "week\u00b7en\u00b7dstark", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKNEG", "PIAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Die Villa auf der einen Seite", "tokens": ["Die", "Vil\u00b7la", "auf", "der", "ei\u00b7nen", "Sei\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "mit dem Maybach in imposanter Breite . . .", "tokens": ["mit", "dem", "May\u00b7bach", "in", "im\u00b7po\u00b7san\u00b7ter", "Brei\u00b7te", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "auf der andern das Bild von dem Week-end-Haus \u2013:", "tokens": ["auf", "der", "an\u00b7dern", "das", "Bild", "von", "dem", "Week\u00b7en\u00b7dHaus", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "ADJA", "ART", "NN", "APPR", "ART", "NN", "$(", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "So sieht bei uns der Klassenkampf aus.", "tokens": ["So", "sieht", "bei", "uns", "der", "Klas\u00b7sen\u00b7kampf", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.9": {"line.1": {"text": "Erst sagt es einer.", "tokens": ["Erst", "sagt", "es", "ei\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.10": {"line.1": {"text": "Denn ists ne Weile still,", "tokens": ["Denn", "ists", "ne", "Wei\u00b7le", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "weil keiner will.", "tokens": ["weil", "kei\u00b7ner", "will", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Dann kommen aber zu Haufen", "tokens": ["Dann", "kom\u00b7men", "a\u00b7ber", "zu", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "APPR", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "die Organisationsorganisatoren gelaufen:", "tokens": ["die", "Or\u00b7ga\u00b7ni\u00b7sa\u00b7ti\u00b7ons\u00b7or\u00b7ga\u00b7ni\u00b7sa\u00b7to\u00b7ren", "ge\u00b7lau\u00b7fen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+----+-++-+--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Beamte und Journalisten", "tokens": ["Be\u00b7am\u00b7te", "und", "Jour\u00b7na\u00b7lis\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "KON", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "und andre Juden und sogar Christen \u2013", "tokens": ["und", "and\u00b7re", "Ju\u00b7den", "und", "so\u00b7gar", "Chris\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "KON", "ADV", "NN", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "und ein ganzes Komitee", "tokens": ["und", "ein", "gan\u00b7zes", "Ko\u00b7mi\u00b7tee"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und Offiziere a. D.", "tokens": ["und", "Of\u00b7fi\u00b7zie\u00b7re", "a.", "D."], "token_info": ["word", "word", "abbreviation", "abbreviation"], "pos": ["KON", "NN", "APPRART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Propaganda? Famos!", "tokens": ["Pro\u00b7pa\u00b7gan\u00b7da", "?", "Fa\u00b7mos", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Jetzt gehts los.", "tokens": ["Jetzt", "gehts", "los", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKVZ", "$."], "meter": "+--", "measure": "dactylic.init"}}, "stanza.12": {"line.1": {"text": "Sie kleben Plakate", "tokens": ["Sie", "kle\u00b7ben", "Pla\u00b7ka\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["PPER", "ADJA", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "und Bildinserate", "tokens": ["und", "Bil\u00b7din\u00b7se\u00b7ra\u00b7te"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "und sind nie alleine", "tokens": ["und", "sind", "nie", "al\u00b7lei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "ADV"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "und gr\u00fcnden Vereine;", "tokens": ["und", "gr\u00fcn\u00b7den", "Ver\u00b7ei\u00b7ne", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Deutschlands \u00e4ltester Soldat", "tokens": ["Deutschlands", "\u00e4l\u00b7tes\u00b7ter", "Sol\u00b7dat"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "hat das Ehrenprotektorat . . .", "tokens": ["hat", "das", "Eh\u00b7ren\u00b7pro\u00b7tek\u00b7to\u00b7rat", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "$.", "$.", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.7": {"text": "und es l\u00e4\u00dft sie nicht ruhn,", "tokens": ["und", "es", "l\u00e4\u00dft", "sie", "nicht", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.8": {"text": "und sie haben ze tun.", "tokens": ["und", "sie", "ha\u00b7ben", "ze", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "XY", "VVINF", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.13": {"line.1": {"text": "Wahrheit breitet sich nicht aus,", "tokens": ["Wahr\u00b7heit", "brei\u00b7tet", "sich", "nicht", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "PTKNEG", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "hast die Zeitung du im Haus.", "tokens": ["hast", "die", "Zei\u00b7tung", "du", "im", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Und bald sind die Gehirne bei allen", "tokens": ["Und", "bald", "sind", "die", "Ge\u00b7hir\u00b7ne", "bei", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "APPR", "PIAT"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "von einem linden Wahnsinn befallen:", "tokens": ["von", "ei\u00b7nem", "lin\u00b7den", "Wahn\u00b7sinn", "be\u00b7fal\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbweek-end!\u00ab nuckelt der Embryo;", "tokens": ["\u00bb", "week\u00b7end", "!", "\u00ab", "nu\u00b7ckelt", "der", "Emb\u00b7ryo", ";"], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$.", "$(", "VVFIN", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": "\u00bbweek-end!\u00ab fl\u00fcstert der Gro\u00dfpopo.", "tokens": ["\u00bb", "week\u00b7end", "!", "\u00ab", "fl\u00fcs\u00b7tert", "der", "Gro\u00df\u00b7po\u00b7po", "."], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$.", "$(", "VVFIN", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.5": {"text": "Vergessen die Wirren um Tschiang Kai-schek;", "tokens": ["Ver\u00b7ges\u00b7sen", "die", "Wir\u00b7ren", "um", "Tschi\u00b7ang", "Kai\u00b7schek", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "NE", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.6": {"text": "vergessen der ganze Stahlhelmdreck;", "tokens": ["ver\u00b7ges\u00b7sen", "der", "gan\u00b7ze", "Stahl\u00b7helm\u00b7dreck", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "vergessen der Volksb\u00fchne tiefer Fall . . .", "tokens": ["ver\u00b7ges\u00b7sen", "der", "Volks\u00b7b\u00fch\u00b7ne", "tie\u00b7fer", "Fall", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVPP", "ART", "NN", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "es braust ein Ruf wie Donnerhall:", "tokens": ["es", "braust", "ein", "Ruf", "wie", "Don\u00b7ner\u00b7hall", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KOKOM", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Week-end \u2013!", "tokens": ["Week\u00b7end", "\u2013", "!"], "token_info": ["word", "punct", "punct"], "pos": ["NE", "$(", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.15": {"line.1": {"text": "Wiek-ent-Gamaschen und Wik-end-Zigarren,", "tokens": ["Wiek\u00b7ent\u00b7Ga\u00b7ma\u00b7schen", "und", "Wi\u00b7k\u00b7en\u00b7dZi\u00b7gar\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Wiehk-end-Windeln und Wigent-Knarren;", "tokens": ["Wieh\u00b7k\u00b7en\u00b7dWin\u00b7deln", "und", "Wi\u00b7gent\u00b7Knar\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Wieghennd-Nachtt\u00f6pfe (mit drei Henkeln),", "tokens": ["Wieg\u00b7henn\u00b7dNacht\u00b7t\u00f6p\u00b7fe", "(", "mit", "drei", "Hen\u00b7keln", ")", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$(", "APPR", "CARD", "NN", "$(", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wieck\u00e4nt-Stiefel mit Wieg\u00e4nd-Senkeln . . .", "tokens": ["Wieck\u00e4nt\u00b7S\u00b7tie\u00b7fel", "mit", "Wie\u00b7g\u00e4n\u00b7dSen\u00b7keln", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "APPR", "NN", "$.", "$.", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Weegent-H\u00e4uschen und Wiekent-Bauch,", "tokens": ["Wee\u00b7gent\u00b7H\u00e4us\u00b7chen", "und", "Wie\u00b7kent\u00b7Bauch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.6": {"text": "und was London kann, das k\u00f6nnen wir auch.", "tokens": ["und", "was", "Lon\u00b7don", "kann", ",", "das", "k\u00f6n\u00b7nen", "wir", "auch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "NE", "VMFIN", "$,", "PDS", "VMFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.16": {"line.1": {"text": "Blo\u00df:", "tokens": ["Blo\u00df", ":"], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Die Geh\u00e4lter der kleinen Angestellten", "tokens": ["Die", "Ge\u00b7h\u00e4l\u00b7ter", "der", "klei\u00b7nen", "An\u00b7ge\u00b7stell\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "erh\u00f6hen sich in Deutschland selten . . .", "tokens": ["er\u00b7h\u00f6\u00b7hen", "sich", "in", "Deutschland", "sel\u00b7ten", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NE", "ADJD", "$.", "$.", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Mit 145 Mark", "tokens": ["Mit", "145", "Mark"], "token_info": ["word", "number", "word"], "pos": ["APPR", "CARD", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "f\u00fchlt sich nicht jeder week-end-stark.", "tokens": ["f\u00fchlt", "sich", "nicht", "je\u00b7der", "week\u00b7en\u00b7dstark", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKNEG", "PIAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Die Villa auf der einen Seite", "tokens": ["Die", "Vil\u00b7la", "auf", "der", "ei\u00b7nen", "Sei\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "mit dem Maybach in imposanter Breite . . .", "tokens": ["mit", "dem", "May\u00b7bach", "in", "im\u00b7po\u00b7san\u00b7ter", "Brei\u00b7te", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "auf der andern das Bild von dem Week-end-Haus \u2013:", "tokens": ["auf", "der", "an\u00b7dern", "das", "Bild", "von", "dem", "Week\u00b7en\u00b7dHaus", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "ADJA", "ART", "NN", "APPR", "ART", "NN", "$(", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "So sieht bei uns der Klassenkampf aus.", "tokens": ["So", "sieht", "bei", "uns", "der", "Klas\u00b7sen\u00b7kampf", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}}}}