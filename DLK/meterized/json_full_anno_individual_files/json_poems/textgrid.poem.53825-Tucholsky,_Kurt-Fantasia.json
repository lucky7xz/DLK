{"textgrid.poem.53825": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Fantasia", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bb . . . sattsam bekannte Ignaz Wrobel. Ja, glaubt denn dieser degenerierte W\u00fcstensohn . . . \u00ab", "tokens": ["\u00bb", ".", ".", ".", "satt\u00b7sam", "be\u00b7kann\u00b7te", "Ig\u00b7naz", "Wro\u00b7bel", ".", "Ja", ",", "glaubt", "denn", "die\u00b7ser", "de\u00b7ge\u00b7ne\u00b7rier\u00b7te", "W\u00fcs\u00b7ten\u00b7sohn", ".", ".", ".", "\u00ab"], "token_info": ["punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "$.", "$.", "$.", "ADJD", "VVFIN", "NE", "NE", "$.", "PTKANT", "$,", "VVFIN", "ADV", "PDAT", "ADJA", "NN", "$.", "$.", "$.", "$("], "meter": "+--+-+-+--+-+--+-+-+-+", "measure": "iambic.octa.plus.invert"}}, "stanza.2": {"line.1": {"text": "\u00bbdein d\u00e4mliches Gefrage ehrt den gemeinen Mann \u2013 der Majest\u00e4t des Todes kann niemand entgehn \u2013", "tokens": ["\u00bb", "dein", "d\u00e4m\u00b7li\u00b7ches", "Ge\u00b7fra\u00b7ge", "ehrt", "den", "ge\u00b7mei\u00b7nen", "Mann", "\u2013", "der", "Ma\u00b7jes\u00b7t\u00e4t", "des", "To\u00b7des", "kann", "nie\u00b7mand", "ent\u00b7gehn", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "ADJA", "NN", "VVFIN", "ART", "ADJA", "NN", "$(", "ART", "NN", "ART", "NN", "VMFIN", "PIS", "VVINF", "$("], "meter": "-+-+-+-+--+-+-+-+-+--+--+", "measure": "iambic.octa.plus.relaxed"}, "line.2": {"text": "Wenn Sie meinen, da\u00df Fantasia gut ist \u2013 mir soll sie nicht zu dick sein.\u00ab", "tokens": ["Wenn", "Sie", "mei\u00b7nen", ",", "da\u00df", "Fan\u00b7ta\u00b7sia", "gut", "ist", "\u2013", "mir", "soll", "sie", "nicht", "zu", "dick", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$,", "KOUS", "NE", "ADJD", "VAFIN", "$(", "PPER", "VMFIN", "PPER", "PTKNEG", "PTKA", "ADJD", "VAINF", "$.", "$("], "meter": "+-+--+-+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbwer ist der sch\u00f6ne Reiter dort, der keines unbeschnittenen Christenhundes Wut wich?", "tokens": ["\u00bb", "wer", "ist", "der", "sch\u00f6\u00b7ne", "Rei\u00b7ter", "dort", ",", "der", "kei\u00b7nes", "un\u00b7be\u00b7schnit\u00b7te\u00b7nen", "Chris\u00b7ten\u00b7hun\u00b7des", "Wut", "wich", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$,", "PRELS", "PIAT", "ADJA", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-+--+-+-+-", "measure": "iambic.octa.plus.relaxed"}, "line.2": {"text": "G\u00e4nnsde d\u00e4hn? Das ist wohl Aemil Ludwig!\u00ab", "tokens": ["G\u00e4nns\u00b7de", "d\u00e4hn", "?", "Das", "ist", "wohl", "A\u00b7e\u00b7mil", "Lud\u00b7wig", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVINF", "$.", "PDS", "VAFIN", "ADV", "NE", "NE", "$.", "$("], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "und ich antwortete mit den Versen:", "tokens": ["und", "ich", "ant\u00b7wor\u00b7te\u00b7te", "mit", "den", "Ver\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "\u00bbwer ist der edle Moslem dort \u2013 mit jenem rosa Pickele?", "tokens": ["\u00bb", "wer", "ist", "der", "ed\u00b7le", "Mos\u00b7lem", "dort", "\u2013", "mit", "je\u00b7nem", "ro\u00b7sa", "Pi\u00b7cke\u00b7le", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$(", "APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+--", "measure": "unknown.measure.septa"}, "line.5": {"text": "G\u00e4nnsde d\u00e4hn? Das ist wohl Ren\u00e9 Schickele!\u00ab", "tokens": ["G\u00e4nns\u00b7de", "d\u00e4hn", "?", "Das", "ist", "wohl", "Ren\u00e9", "Schi\u00b7cke\u00b7le", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVINF", "$.", "PDS", "VAFIN", "ADV", "NE", "NN", "$.", "$("], "meter": "+-+-+--+--", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "und so sprachen wir noch viele sch\u00f6ne Verse.", "tokens": ["und", "so", "spra\u00b7chen", "wir", "noch", "vie\u00b7le", "sch\u00f6\u00b7ne", "Ver\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}}, "stanza.4": {"line.1": {"text": "\u00bb . . . sattsam bekannte Ignaz Wrobel. Ja, glaubt denn dieser degenerierte W\u00fcstensohn . . . \u00ab", "tokens": ["\u00bb", ".", ".", ".", "satt\u00b7sam", "be\u00b7kann\u00b7te", "Ig\u00b7naz", "Wro\u00b7bel", ".", "Ja", ",", "glaubt", "denn", "die\u00b7ser", "de\u00b7ge\u00b7ne\u00b7rier\u00b7te", "W\u00fcs\u00b7ten\u00b7sohn", ".", ".", ".", "\u00ab"], "token_info": ["punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "$.", "$.", "$.", "ADJD", "VVFIN", "NE", "NE", "$.", "PTKANT", "$,", "VVFIN", "ADV", "PDAT", "ADJA", "NN", "$.", "$.", "$.", "$("], "meter": "+--+-+-+--+-+--+-+-+-+", "measure": "iambic.octa.plus.invert"}}, "stanza.5": {"line.1": {"text": "\u00bbdein d\u00e4mliches Gefrage ehrt den gemeinen Mann \u2013 der Majest\u00e4t des Todes kann niemand entgehn \u2013", "tokens": ["\u00bb", "dein", "d\u00e4m\u00b7li\u00b7ches", "Ge\u00b7fra\u00b7ge", "ehrt", "den", "ge\u00b7mei\u00b7nen", "Mann", "\u2013", "der", "Ma\u00b7jes\u00b7t\u00e4t", "des", "To\u00b7des", "kann", "nie\u00b7mand", "ent\u00b7gehn", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "ADJA", "NN", "VVFIN", "ART", "ADJA", "NN", "$(", "ART", "NN", "ART", "NN", "VMFIN", "PIS", "VVINF", "$("], "meter": "-+-+-+-+--+-+-+-+-+--+--+", "measure": "iambic.octa.plus.relaxed"}, "line.2": {"text": "Wenn Sie meinen, da\u00df Fantasia gut ist \u2013 mir soll sie nicht zu dick sein.\u00ab", "tokens": ["Wenn", "Sie", "mei\u00b7nen", ",", "da\u00df", "Fan\u00b7ta\u00b7sia", "gut", "ist", "\u2013", "mir", "soll", "sie", "nicht", "zu", "dick", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$,", "KOUS", "NE", "ADJD", "VAFIN", "$(", "PPER", "VMFIN", "PPER", "PTKNEG", "PTKA", "ADJD", "VAINF", "$.", "$("], "meter": "+-+--+-+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbwer ist der sch\u00f6ne Reiter dort, der keines unbeschnittenen Christenhundes Wut wich?", "tokens": ["\u00bb", "wer", "ist", "der", "sch\u00f6\u00b7ne", "Rei\u00b7ter", "dort", ",", "der", "kei\u00b7nes", "un\u00b7be\u00b7schnit\u00b7te\u00b7nen", "Chris\u00b7ten\u00b7hun\u00b7des", "Wut", "wich", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$,", "PRELS", "PIAT", "ADJA", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-+--+-+-+-", "measure": "iambic.octa.plus.relaxed"}, "line.2": {"text": "G\u00e4nnsde d\u00e4hn? Das ist wohl Aemil Ludwig!\u00ab", "tokens": ["G\u00e4nns\u00b7de", "d\u00e4hn", "?", "Das", "ist", "wohl", "A\u00b7e\u00b7mil", "Lud\u00b7wig", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVINF", "$.", "PDS", "VAFIN", "ADV", "NE", "NE", "$.", "$("], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "und ich antwortete mit den Versen:", "tokens": ["und", "ich", "ant\u00b7wor\u00b7te\u00b7te", "mit", "den", "Ver\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "\u00bbwer ist der edle Moslem dort \u2013 mit jenem rosa Pickele?", "tokens": ["\u00bb", "wer", "ist", "der", "ed\u00b7le", "Mos\u00b7lem", "dort", "\u2013", "mit", "je\u00b7nem", "ro\u00b7sa", "Pi\u00b7cke\u00b7le", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$(", "APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+--", "measure": "unknown.measure.septa"}, "line.5": {"text": "G\u00e4nnsde d\u00e4hn? Das ist wohl Ren\u00e9 Schickele!\u00ab", "tokens": ["G\u00e4nns\u00b7de", "d\u00e4hn", "?", "Das", "ist", "wohl", "Ren\u00e9", "Schi\u00b7cke\u00b7le", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVINF", "$.", "PDS", "VAFIN", "ADV", "NE", "NN", "$.", "$("], "meter": "+-+-+--+--", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "und so sprachen wir noch viele sch\u00f6ne Verse.", "tokens": ["und", "so", "spra\u00b7chen", "wir", "noch", "vie\u00b7le", "sch\u00f6\u00b7ne", "Ver\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}}}}}