{"dta.poem.13397": {"metadata": {"author": {"name": "Suppius, Christoph Eusebius", "birth": "N.A.", "death": "N.A."}, "title": "Die Pferde.  \n   \n  Da  \n  unser oberster Bereuter,  \n  Herr Martin Tamm,  \n der muntre Greis,  \n  nach 78 Jahren  \n weiter mit uns noch umzuspringen wei\u00df,  \n  so w\u00fcnscht  \n durch muthig wiehernd Schnellen  \n  Des frohen Tages Wiederkunft  \n  die  \n  in den sch\u00f6nen F\u00fcrsten-St\u00e4llen  \n  wohlzugerittne  \n  Pferde-Zunft.  \n   \n  Den 16 May 1741.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1749", "urn": "urn:nbn:de:kobv:b4-20594-7", "language": ["de:0.99"], "booktitle": "Suppius, Christoph Eusebius: Oden und Lieder. Gotha, 1749."}, "poem": {"stanza.1": {"line.1": {"text": "Du Mann von ungemeiner G\u00fcte!", "tokens": ["Du", "Mann", "von", "un\u00b7ge\u00b7mei\u00b7ner", "G\u00fc\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die deutsche Pferde-Nation", "tokens": ["Die", "deut\u00b7sche", "Pfer\u00b7de\u00b7Na\u00b7ti\u00b7on"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "R\u00fchmt dein gef\u00e4lliges Gem\u00fcthe", "tokens": ["R\u00fchmt", "dein", "ge\u00b7f\u00e4l\u00b7li\u00b7ges", "Ge\u00b7m\u00fc\u00b7the"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Seit fast vier Pferde-Altern schon;", "tokens": ["Seit", "fast", "vier", "Pfer\u00b7de\u00b7Al\u00b7tern", "schon", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie aber sollen wir erkennen,", "tokens": ["Wie", "a\u00b7ber", "sol\u00b7len", "wir", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was deine Wissenschaft gebiehrt,", "tokens": ["Was", "dei\u00b7ne", "Wis\u00b7sen\u00b7schaft", "ge\u00b7biehrt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Indem sie unser wildes Rennen", "tokens": ["In\u00b7dem", "sie", "un\u00b7ser", "wil\u00b7des", "Ren\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Mit edlem Wesen ausgeziert.", "tokens": ["Mit", "ed\u00b7lem", "We\u00b7sen", "aus\u00b7ge\u00b7ziert", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Nimm, werther Tamm, nach unserm Stande", "tokens": ["Nimm", ",", "wert\u00b7her", "Tamm", ",", "nach", "un\u00b7serm", "Stan\u00b7de"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "ADJA", "NN", "$,", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die\u00df Zeichen der Erk\u00e4nntlichkeit!", "tokens": ["Die\u00df", "Zei\u00b7chen", "der", "Er\u00b7k\u00e4nnt\u00b7lich\u00b7keit", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es ist dir wirklich keine Schande,", "tokens": ["Es", "ist", "dir", "wirk\u00b7lich", "kei\u00b7ne", "Schan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sonst w\u00e4r es uns von Herzen leid,", "tokens": ["Sonst", "w\u00e4r", "es", "uns", "von", "Her\u00b7zen", "leid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Vielleicht wirst du gestehen m\u00fcssen,", "tokens": ["Viel\u00b7leicht", "wirst", "du", "ge\u00b7ste\u00b7hen", "m\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wie dein Erfahren es befindt,", "tokens": ["Wie", "dein", "Er\u00b7fah\u00b7ren", "es", "be\u00b7findt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Da\u00df unter Leibern von vier F\u00fcssen", "tokens": ["Da\u00df", "un\u00b7ter", "Lei\u00b7bern", "von", "vier", "F\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "NN", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wir an Vernunft die ersten sind.", "tokens": ["Wir", "an", "Ver\u00b7nunft", "die", "ers\u00b7ten", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "NN", "ART", "ADJA", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wir stammen aus verschiednen Landen", "tokens": ["Wir", "stam\u00b7men", "aus", "ver\u00b7schied\u00b7nen", "Lan\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie dein Erkennen r\u00fchmlich schaut,", "tokens": ["Wie", "dein", "Er\u00b7ken\u00b7nen", "r\u00fchm\u00b7lich", "schaut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sind deswegen hier vorhanden,", "tokens": ["Und", "sind", "des\u00b7we\u00b7gen", "hier", "vor\u00b7han\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PAV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil unsre Leiber wohlgebaut,", "tokens": ["Weil", "uns\u00b7re", "Lei\u00b7ber", "wohl\u00b7ge\u00b7baut", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wir haben l\u00e4ngst ein Gl\u00fcck ermessen,", "tokens": ["Wir", "ha\u00b7ben", "l\u00e4ngst", "ein", "Gl\u00fcck", "er\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das bey der Wahl uns wiederfuhr,", "tokens": ["Das", "bey", "der", "Wahl", "uns", "wie\u00b7der\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "So wahr wir t\u00e4glich Hafer fressen!", "tokens": ["So", "wahr", "wir", "t\u00e4g\u00b7lich", "Ha\u00b7fer", "fres\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "ADJD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Das ist bey uns der h\u00f6chste Schwur.", "tokens": ["Das", "ist", "bey", "uns", "der", "h\u00f6chs\u00b7te", "Schwur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Man z\u00e4hle schlechten Kreaturen", "tokens": ["Man", "z\u00e4h\u00b7le", "schlech\u00b7ten", "Kre\u00b7a\u00b7tu\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Pferde-Volk nur nicht mehr bey,", "tokens": ["Das", "Pfer\u00b7de\u00b7Volk", "nur", "nicht", "mehr", "bey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKNEG", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn rare Schriften zeigen Spuren,", "tokens": ["Denn", "ra\u00b7re", "Schrif\u00b7ten", "zei\u00b7gen", "Spu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df es von edler Ankunft sey;", "tokens": ["Da\u00df", "es", "von", "ed\u00b7ler", "An\u00b7kunft", "sey", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie sind bis jetzo aufbehalten,", "tokens": ["Sie", "sind", "bis", "jet\u00b7zo", "auf\u00b7be\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und die Benennung wird nicht schwer,", "tokens": ["Und", "die", "Be\u00b7nen\u00b7nung", "wird", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Homerus war es bey den Alten,", "tokens": ["Ho\u00b7me\u00b7rus", "war", "es", "bey", "den", "Al\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Zu unsern Zeiten Gulliver.", "tokens": ["Zu", "un\u00b7sern", "Zei\u00b7ten", "Gul\u00b7li\u00b7ver", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Wir wissen von ber\u00fchmten Leuten,", "tokens": ["Wir", "wis\u00b7sen", "von", "be\u00b7r\u00fchm\u00b7ten", "Leu\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und es ist nicht ein blo\u00df Geschrey,", "tokens": ["Und", "es", "ist", "nicht", "ein", "blo\u00df", "Ge\u00b7schrey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PTKNEG", "ART", "ADV", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Es mu\u00df was Wirkliches bedeuten,", "tokens": ["Es", "mu\u00df", "was", "Wirk\u00b7li\u00b7ches", "be\u00b7deu\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PWS", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Zeit-Geschichte stimmet bey,", "tokens": ["Die", "Zeit\u00b7Ge\u00b7schich\u00b7te", "stim\u00b7met", "bey", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wenn Rom von unsern eignen Orden", "tokens": ["Wenn", "Rom", "von", "un\u00b7sern", "eig\u00b7nen", "Or\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den Burgemeister nehmen mu\u00df,", "tokens": ["Den", "Bur\u00b7ge\u00b7meis\u00b7ter", "neh\u00b7men", "mu\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wie gut ists Anherr Hengsten worden", "tokens": ["Wie", "gut", "ists", "An\u00b7herr", "Hengs\u00b7ten", "wor\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VAFIN", "NN", "NN", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Von dem Heliogabalus!", "tokens": ["Von", "dem", "He\u00b7lio\u00b7ga\u00b7ba\u00b7lus", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Ber\u00fchmte Schriften selber melden,", "tokens": ["Be\u00b7r\u00fchm\u00b7te", "Schrif\u00b7ten", "sel\u00b7ber", "mel\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sparen der Unsterblichkeit", "tokens": ["Und", "spa\u00b7ren", "der", "U\u00b7nsterb\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Sehr viele unsrer Pferde Helden,", "tokens": ["Sehr", "vie\u00b7le", "uns\u00b7rer", "Pfer\u00b7de", "Hel\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Besonders aus der alten Zeit,", "tokens": ["Be\u00b7son\u00b7ders", "aus", "der", "al\u00b7ten", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da ist Pherenices zu finden,", "tokens": ["Da", "ist", "Phe\u00b7re\u00b7ni\u00b7ces", "zu", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "PTKZU", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.6": {"text": "Der Hiero den Sieg gebracht,", "tokens": ["Der", "Hie\u00b7ro", "den", "Sieg", "ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ART", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Carabalus, schnell gleich den Winden,", "tokens": ["Ca\u00b7ra\u00b7ba\u00b7lus", ",", "schnell", "gleich", "den", "Win\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.8": {"text": "F\u00fchrt Selim gl\u00fccklich aus der Schlacht.", "tokens": ["F\u00fchrt", "Se\u00b7lim", "gl\u00fcck\u00b7lich", "aus", "der", "Schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Wie? schreibt man nicht aus welschen Landen,", "tokens": ["Wie", "?", "schreibt", "man", "nicht", "aus", "wel\u00b7schen", "Lan\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "VVFIN", "PIS", "PTKNEG", "APPR", "PWAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df jenes Pferd gefunden w\u00e4r,", "tokens": ["Da\u00df", "je\u00b7nes", "Pferd", "ge\u00b7fun\u00b7den", "w\u00e4r", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So auf dem Capitol gestanden,", "tokens": ["So", "auf", "dem", "Ca\u00b7pi\u00b7tol", "ge\u00b7stan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die\u00df bringt uns wirklich Ruhm und Ehr;", "tokens": ["Die\u00df", "bringt", "uns", "wirk\u00b7lich", "Ruhm", "und", "Ehr", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und was hat Jlium zerst\u00f6ret?", "tokens": ["Und", "was", "hat", "Jlium", "zer\u00b7st\u00f6\u00b7ret", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "NE", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Ein grosses Pferd von Holz gemacht,", "tokens": ["Ein", "gros\u00b7ses", "Pferd", "von", "Holz", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ja, wer von dem Darius h\u00f6ret,", "tokens": ["Ja", ",", "wer", "von", "dem", "Da\u00b7ri\u00b7us", "h\u00f6\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWS", "APPR", "ART", "NE", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.8": {"text": "Der wei\u00df, was ihn zum Reich gebracht.", "tokens": ["Der", "wei\u00df", ",", "was", "ihn", "zum", "Reich", "ge\u00b7bracht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PRELS", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Der Ruhm ist vor uns nicht geringer,", "tokens": ["Der", "Ruhm", "ist", "vor", "uns", "nicht", "ge\u00b7rin\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "PPER", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie man leicht darzuthun getraut,", "tokens": ["Wie", "man", "leicht", "dar\u00b7zu\u00b7thun", "ge\u00b7traut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADJD", "PAV", "VVPP", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Was einstmahl gar ein Weltbezwinger", "tokens": ["Was", "einst\u00b7mahl", "gar", "ein", "Welt\u00b7be\u00b7zwin\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dem Pferd zu Ehren aufgebaut,", "tokens": ["Dem", "Pferd", "zu", "Eh\u00b7ren", "auf\u00b7ge\u00b7baut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Kurz, unser Orden, Herr Bereuter!", "tokens": ["Kurz", ",", "un\u00b7ser", "Or\u00b7den", ",", "Herr", "Be\u00b7reu\u00b7ter", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPOSAT", "NN", "$,", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ist deinem Ruhm gar vortheilhaft,", "tokens": ["Ist", "dei\u00b7nem", "Ruhm", "gar", "vor\u00b7theil\u00b7haft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Drum so vernimm nun ietzo weiter,", "tokens": ["Drum", "so", "ver\u00b7nimm", "nun", "iet\u00b7zo", "wei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "ADJD", "ADV", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Was uns dein Gl\u00fcck f\u00fcr Lust verschafft.", "tokens": ["Was", "uns", "dein", "Gl\u00fcck", "f\u00fcr", "Lust", "ver\u00b7schafft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPOSAT", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Sobald an diesem frohen Morgen", "tokens": ["So\u00b7bald", "an", "die\u00b7sem", "fro\u00b7hen", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der M\u00e4hnen sehr verwirrtes Haar,", "tokens": ["Der", "M\u00e4h\u00b7nen", "sehr", "ver\u00b7wirr\u00b7tes", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch reger H\u00e4nde wachsam Sorgen,", "tokens": ["Durch", "re\u00b7ger", "H\u00e4n\u00b7de", "wach\u00b7sam", "Sor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Gek\u00e4mmt und zubereitet war,", "tokens": ["Ge\u00b7k\u00e4mmt", "und", "zu\u00b7be\u00b7rei\u00b7tet", "war", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wir auch das Morgenbrodt erhielten,", "tokens": ["Wir", "auch", "das", "Mor\u00b7gen\u00b7brodt", "er\u00b7hiel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "An Hafer, Heckerling und Heu,", "tokens": ["An", "Ha\u00b7fer", ",", "He\u00b7cker\u00b7ling", "und", "Heu", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Hernach mit denen Halftern spielten,", "tokens": ["Her\u00b7nach", "mit", "de\u00b7nen", "Half\u00b7tern", "spiel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRELS", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "So wurden wir auf einmahl scheu.", "tokens": ["So", "wur\u00b7den", "wir", "auf", "ein\u00b7mahl", "scheu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Wir fingen an, uns aufzub\u00e4umen,", "tokens": ["Wir", "fin\u00b7gen", "an", ",", "uns", "auf\u00b7zu\u00b7b\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Ohren stunden hochgespitzt,", "tokens": ["Die", "Oh\u00b7ren", "stun\u00b7den", "hoch\u00b7ge\u00b7spitzt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An denen angelegten Z\u00e4umen", "tokens": ["An", "de\u00b7nen", "an\u00b7ge\u00b7leg\u00b7ten", "Z\u00e4u\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "War bald der weisse Schaum gespritzt;", "tokens": ["War", "bald", "der", "weis\u00b7se", "Schaum", "ge\u00b7spritzt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wir hauten mit den Vorder-Hufen,", "tokens": ["Wir", "hau\u00b7ten", "mit", "den", "Vor\u00b7der\u00b7Hu\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und wieherten mit aller Macht,", "tokens": ["Und", "wie\u00b7her\u00b7ten", "mit", "al\u00b7ler", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "Bis Peitsche, Stock und donnernd Rufen", "tokens": ["Bis", "Peit\u00b7sche", ",", "Stock", "und", "don\u00b7nernd", "Ru\u00b7fen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "NN", "KON", "ADJD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Uns wieder zu uns selbst gebracht.", "tokens": ["Uns", "wie\u00b7der", "zu", "uns", "selbst", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPR", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Ja wohl! es trieb uns so zu Paaren,", "tokens": ["Ja", "wohl", "!", "es", "trieb", "uns", "so", "zu", "Paa\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "$.", "PPER", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als wie der Nothstall und der Zwang,", "tokens": ["Als", "wie", "der", "Noth\u00b7stall", "und", "der", "Zwang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bis durch das Leibgewand von Haaren", "tokens": ["Bis", "durch", "das", "Leib\u00b7ge\u00b7wand", "von", "Haa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der Angstschwei\u00df wie das Wasser drang;", "tokens": ["Der", "A\u00b7ngstschwei\u00df", "wie", "das", "Was\u00b7ser", "drang", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Es schlich der Schmerz bis zu der Leber,", "tokens": ["Es", "schlich", "der", "Schmerz", "bis", "zu", "der", "Le\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und machte fast noch mehr Verdru\u00df,", "tokens": ["Und", "mach\u00b7te", "fast", "noch", "mehr", "Ver\u00b7dru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Als H\u00e4mmerling, der Todtengr\u00e4ber,", "tokens": ["Als", "H\u00e4m\u00b7mer\u00b7ling", ",", "der", "Tod\u00b7ten\u00b7gr\u00e4\u00b7ber", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wenn er sein Amt verrichten mu\u00df.", "tokens": ["Wenn", "er", "sein", "Amt", "ver\u00b7rich\u00b7ten", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Jedoch die Gro\u00dfmuth kann verzeihen,", "tokens": ["Je\u00b7doch", "die", "Gro\u00df\u00b7muth", "kann", "ver\u00b7zei\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir nehmen die Vers\u00f6hnung an,", "tokens": ["Wir", "neh\u00b7men", "die", "Ver\u00b7s\u00f6h\u00b7nung", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Weil unser freudenvolles Schreyen", "tokens": ["Weil", "un\u00b7ser", "freu\u00b7den\u00b7vol\u00b7les", "Schre\u00b7yen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Kein Stallbewohner wissen kann;", "tokens": ["Kein", "Stall\u00b7be\u00b7woh\u00b7ner", "wis\u00b7sen", "kann", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie merkten fast, was uns getrieben,", "tokens": ["Sie", "merk\u00b7ten", "fast", ",", "was", "uns", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Man sprach uns endlich freundlich zu,", "tokens": ["Man", "sprach", "uns", "end\u00b7lich", "freund\u00b7lich", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Indem wir Dich nicht wenig lieben,", "tokens": ["In\u00b7dem", "wir", "Dich", "nicht", "we\u00b7nig", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Denn sonsten halten wir schon Ruh.", "tokens": ["Denn", "sons\u00b7ten", "hal\u00b7ten", "wir", "schon", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Wahr ist es, ein geheimes Merken", "tokens": ["Wahr", "ist", "es", ",", "ein", "ge\u00b7hei\u00b7mes", "Mer\u00b7ken"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VAFIN", "PPER", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat uns vor Freuden aufgebracht,", "tokens": ["Hat", "uns", "vor", "Freu\u00b7den", "auf\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Reittag gestern wird best\u00e4rken,", "tokens": ["Der", "Reit\u00b7tag", "ge\u00b7stern", "wird", "be\u00b7st\u00e4r\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VAFIN", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Wie wir zusammen nachgedacht;", "tokens": ["Wie", "wir", "zu\u00b7sam\u00b7men", "nach\u00b7ge\u00b7dacht", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Uns schien, als w\u00fcrdest Du verj\u00fcnget,", "tokens": ["Uns", "schien", ",", "als", "w\u00fcr\u00b7dest", "Du", "ver\u00b7j\u00fcn\u00b7get", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und heut war kaum die Krippe leer,", "tokens": ["Und", "heut", "war", "kaum", "die", "Krip\u00b7pe", "leer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Da es in unsre Ohren klinget,", "tokens": ["Da", "es", "in", "uns\u00b7re", "Oh\u00b7ren", "klin\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Als ob die\u00df dein Gebuhrtstag w\u00e4r.", "tokens": ["Als", "ob", "die\u00df", "dein", "Ge\u00b7buhrts\u00b7tag", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PDS", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Wer h\u00e4tte nun nicht wiehern sollen?", "tokens": ["Wer", "h\u00e4t\u00b7te", "nun", "nicht", "wie\u00b7hern", "sol\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von unsern Nachbarn vor dem Schlo\u00df", "tokens": ["Von", "un\u00b7sern", "Nach\u00b7barn", "vor", "dem", "Schlo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat man uns auch versichern wollen,", "tokens": ["Hat", "man", "uns", "auch", "ver\u00b7si\u00b7chern", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bey ihnen sey der Henker los;", "tokens": ["Bey", "ih\u00b7nen", "sey", "der", "Hen\u00b7ker", "los", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nur f\u00fcrchten wir, da\u00df ihren Lenden,", "tokens": ["Nur", "f\u00fcrch\u00b7ten", "wir", ",", "da\u00df", "ih\u00b7ren", "Len\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wie unserm fr\u00f6lichen Geschrey,", "tokens": ["Wie", "un\u00b7serm", "fr\u00f6\u00b7li\u00b7chen", "Ge\u00b7schrey", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Auch von noch ungewaschnen H\u00e4nden", "tokens": ["Auch", "von", "noch", "un\u00b7ge\u00b7waschnen", "H\u00e4n\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Schlecht mitgespielet worden sey.", "tokens": ["Schlecht", "mit\u00b7ge\u00b7spie\u00b7let", "wor\u00b7den", "sey", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "VAPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Wiewohl, man achtet das geringe,", "tokens": ["Wie\u00b7wohl", ",", "man", "ach\u00b7tet", "das", "ge\u00b7rin\u00b7ge", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PIS", "VVFIN", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Weil es der Ehre wenig thut;", "tokens": ["Weil", "es", "der", "Eh\u00b7re", "we\u00b7nig", "thut", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An sich zwar sind es gute Dinge", "tokens": ["An", "sich", "zwar", "sind", "es", "gu\u00b7te", "Din\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRF", "ADV", "VAFIN", "PPER", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Vor einen Pferde-Uebermuth,", "tokens": ["Vor", "ei\u00b7nen", "Pfer\u00b7de\u00b7Ue\u00b7ber\u00b7muth", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Jedoch, was schadet es der Freude?", "tokens": ["Je\u00b7doch", ",", "was", "scha\u00b7det", "es", "der", "Freu\u00b7de", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWS", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die wird dadurch doch nicht verst\u00f6rt,", "tokens": ["Die", "wird", "da\u00b7durch", "doch", "nicht", "ver\u00b7st\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PAV", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Weil dieser Tag entfernt vom Leide", "tokens": ["Weil", "die\u00b7ser", "Tag", "ent\u00b7fernt", "vom", "Lei\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "NN", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zu unsern Guten mit geh\u00f6rt.", "tokens": ["Zu", "un\u00b7sern", "Gu\u00b7ten", "mit", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Du bist es, welcher unsern F\u00fcssen", "tokens": ["Du", "bist", "es", ",", "wel\u00b7cher", "un\u00b7sern", "F\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "$,", "PRELS", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jm Schreiten, vollen Lauf und Trab", "tokens": ["Jm", "Schrei\u00b7ten", ",", "vol\u00b7len", "Lauf", "und", "Trab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "$,", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nach viel und manchen Schwei\u00dfvergiessen,", "tokens": ["Nach", "viel", "und", "man\u00b7chen", "Schwei\u00df\u00b7ver\u00b7gies\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die abgeme\u00dfne Zierde gab;", "tokens": ["Die", "ab\u00b7ge\u00b7me\u00df\u00b7ne", "Zier\u00b7de", "gab", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wir kennen deines Amtes Eifer,", "tokens": ["Wir", "ken\u00b7nen", "dei\u00b7nes", "Am\u00b7tes", "Ei\u00b7fer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Vor dem manch stolzer Gaul erschrickt,", "tokens": ["Vor", "dem", "manch", "stol\u00b7zer", "Gaul", "er\u00b7schrickt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Zumahl ein ungezogner L\u00e4ufer,", "tokens": ["Zu\u00b7mahl", "ein", "un\u00b7ge\u00b7zog\u00b7ner", "L\u00e4u\u00b7fer", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den man erst in die Schule schickt.", "tokens": ["Den", "man", "erst", "in", "die", "Schu\u00b7le", "schickt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Da\u00df wir in sch\u00f6nen Kammern wohnen,", "tokens": ["Da\u00df", "wir", "in", "sch\u00f6\u00b7nen", "Kam\u00b7mern", "woh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df unsre Krippen wohl versehn,", "tokens": ["Da\u00df", "uns\u00b7re", "Krip\u00b7pen", "wohl", "ver\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, da\u00df man unser pflegt zu schonen,", "tokens": ["Ja", ",", "da\u00df", "man", "un\u00b7ser", "pflegt", "zu", "scho\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KOUS", "PIS", "PPOSAT", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn andre schwer im Zuge gehn,", "tokens": ["Wenn", "and\u00b7re", "schwer", "im", "Zu\u00b7ge", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADJD", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Das danken wir zwar jenen Gnaden,", "tokens": ["Das", "dan\u00b7ken", "wir", "zwar", "je\u00b7nen", "Gna\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die uns kein unbeqvemes Joch", "tokens": ["Die", "uns", "kein", "un\u00b7be\u00b7qve\u00b7mes", "Joch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Geruhen wollen aufzuladen,", "tokens": ["Ge\u00b7ru\u00b7hen", "wol\u00b7len", "auf\u00b7zu\u00b7la\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Nur ist ihr Lob f\u00fcr uns zu hoch.", "tokens": ["Nur", "ist", "ihr", "Lob", "f\u00fcr", "uns", "zu", "hoch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "APPR", "PPER", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Hingegen bist Du doch der Dritte,", "tokens": ["Hin\u00b7ge\u00b7gen", "bist", "Du", "doch", "der", "Drit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "$,"], "meter": "+--++--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "N\u00e4chst der gro\u00dfm\u00fcthigen Natur,", "tokens": ["N\u00e4chst", "der", "gro\u00df\u00b7m\u00fct\u00b7hi\u00b7gen", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Da\u00df unserm wohlgesetzten Schritte", "tokens": ["Da\u00df", "un\u00b7serm", "wohl\u00b7ge\u00b7setz\u00b7ten", "Schrit\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein solches Gl\u00fccke wiederfuhr,", "tokens": ["Ein", "sol\u00b7ches", "Gl\u00fc\u00b7cke", "wie\u00b7der\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Denn deine Hand, dein kluges Zwingen,", "tokens": ["Denn", "dei\u00b7ne", "Hand", ",", "dein", "klu\u00b7ges", "Zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was uns so mancherley bewegt,", "tokens": ["Was", "uns", "so", "man\u00b7cher\u00b7ley", "be\u00b7wegt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Hat unserm itzt gewohntem Springen", "tokens": ["Hat", "un\u00b7serm", "itzt", "ge\u00b7wohn\u00b7tem", "Sprin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Was recht Erhabnes eingepr\u00e4gt.", "tokens": ["Was", "recht", "Er\u00b7hab\u00b7nes", "ein\u00b7ge\u00b7pr\u00e4gt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Will unser Schutzgott sich erqvicken,", "tokens": ["Will", "un\u00b7ser", "Schutz\u00b7gott", "sich", "er\u00b7qvi\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPOSAT", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Befiehlet Er, und f\u00e4hret aus,", "tokens": ["Be\u00b7fieh\u00b7let", "Er", ",", "und", "f\u00e4h\u00b7ret", "aus", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KON", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und man pflegt uns erst anzuschm\u00fccken.", "tokens": ["Und", "man", "pflegt", "uns", "erst", "an\u00b7zu\u00b7schm\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PPER", "ADV", "VVIZU", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Wie br\u00fcsten wir uns nicht heraus!", "tokens": ["Wie", "br\u00fcs\u00b7ten", "wir", "uns", "nicht", "he\u00b7raus", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "PRF", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da werden recht sechs k\u00fchne Rosse", "tokens": ["Da", "wer\u00b7den", "recht", "sechs", "k\u00fch\u00b7ne", "Ros\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADJD", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Vom Geist der Edelmuth erregt,", "tokens": ["Vom", "Geist", "der", "E\u00b7del\u00b7muth", "er\u00b7regt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Als wenn Bucephalus der Grosse", "tokens": ["Als", "wenn", "Bu\u00b7ce\u00b7pha\u00b7lus", "der", "Gros\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "NE", "ART", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Den grossen Alexander tr\u00e4gt.", "tokens": ["Den", "gros\u00b7sen", "A\u00b7lex\u00b7an\u00b7der", "tr\u00e4gt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Ach schade! da\u00df nicht unser R\u00fccken", "tokens": ["Ach", "scha\u00b7de", "!", "da\u00df", "nicht", "un\u00b7ser", "R\u00fc\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "VVFIN", "$.", "KOUS", "PTKNEG", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll seines Leibes Schemel seyn!", "tokens": ["Soll", "sei\u00b7nes", "Lei\u00b7bes", "Sche\u00b7mel", "seyn", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPOSAT", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir w\u00fcrden uns vor Ehrfurcht b\u00fccken,", "tokens": ["Wir", "w\u00fcr\u00b7den", "uns", "vor", "Ehr\u00b7furcht", "b\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Vorsicht w\u00e4re ungemein,", "tokens": ["Die", "Vor\u00b7sicht", "w\u00e4\u00b7re", "un\u00b7ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wiewohl es w\u00fcrfe diese Ehre", "tokens": ["Wie\u00b7wohl", "es", "w\u00fcr\u00b7fe", "die\u00b7se", "Eh\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VVFIN", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nur unter uns der Zwietracht Frucht,", "tokens": ["Nur", "un\u00b7ter", "uns", "der", "Zwiet\u00b7racht", "Frucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Weil es die n\u00e4chste Strasse w\u00e4re,", "tokens": ["Weil", "es", "die", "n\u00e4chs\u00b7te", "Stras\u00b7se", "w\u00e4\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zu ganz gewisser Eifersucht.", "tokens": ["Zu", "ganz", "ge\u00b7wis\u00b7ser", "Ei\u00b7fer\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Als in den abgewichnen Zeiten", "tokens": ["Als", "in", "den", "ab\u00b7ge\u00b7wich\u00b7nen", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dich unser thierischer Verstand", "tokens": ["Dich", "un\u00b7ser", "thie\u00b7ri\u00b7scher", "Ver\u00b7stand"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sehr lange, weder sahe reiten,", "tokens": ["Sehr", "lan\u00b7ge", ",", "we\u00b7der", "sa\u00b7he", "rei\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KON", "VVFIN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Noch selbsten auf der Bahne fand,", "tokens": ["Noch", "selbs\u00b7ten", "auf", "der", "Bah\u00b7ne", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da war bey uns so grosser Jammer,", "tokens": ["Da", "war", "bey", "uns", "so", "gros\u00b7ser", "Jam\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "PPER", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wir wurden vor Verdru\u00df so toll,", "tokens": ["Wir", "wur\u00b7den", "vor", "Ver\u00b7dru\u00df", "so", "toll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Als wenn ein ungeschliffner Hammer", "tokens": ["Als", "wenn", "ein", "un\u00b7ge\u00b7schliff\u00b7ner", "Ham\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Schuh uns neu besohlen soll.", "tokens": ["Die", "Schuh", "uns", "neu", "be\u00b7soh\u00b7len", "soll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Und neulich, als in vielen Tagen", "tokens": ["Und", "neu\u00b7lich", ",", "als", "in", "vie\u00b7len", "Ta\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "KOUS", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Streich, kein Hieb uns aufgebracht,", "tokens": ["Kein", "Streich", ",", "kein", "Hieb", "uns", "auf\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "War auch ein allgemeines Sagen,", "tokens": ["War", "auch", "ein", "all\u00b7ge\u00b7mei\u00b7nes", "Sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Du h\u00e4ttest Feyertag gemacht;", "tokens": ["Du", "h\u00e4t\u00b7test", "Fe\u00b7yer\u00b7tag", "ge\u00b7macht", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wir waren schon in vollem Ueben,", "tokens": ["Wir", "wa\u00b7ren", "schon", "in", "vol\u00b7lem", "Ue\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Wunsch gerieth zur Gegenwart,", "tokens": ["Ein", "Wunsch", "ge\u00b7rieth", "zur", "Ge\u00b7gen\u00b7wart", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Doch macht ein allgemein Betr\u00fcben,", "tokens": ["Doch", "macht", "ein", "all\u00b7ge\u00b7mein", "Be\u00b7tr\u00fc\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df wir es bis hieher gespahrt.", "tokens": ["Da\u00df", "wir", "es", "bis", "hie\u00b7her", "ge\u00b7spahrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PAV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "So nimm ihn denn von einem Volke,", "tokens": ["So", "nimm", "ihn", "denn", "von", "ei\u00b7nem", "Vol\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das seine Sprache m\u00f6glich macht;", "tokens": ["Das", "sei\u00b7ne", "Spra\u00b7che", "m\u00f6g\u00b7lich", "macht", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es bringe keine tr\u00fcbe Wolke", "tokens": ["Es", "brin\u00b7ge", "kei\u00b7ne", "tr\u00fc\u00b7be", "Wol\u00b7ke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dir noch sobald die Mitternacht!", "tokens": ["Dir", "noch", "so\u00b7bald", "die", "Mit\u00b7ter\u00b7nacht", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der Tag erscheine k\u00fcnftig weiter,", "tokens": ["Der", "Tag", "er\u00b7schei\u00b7ne", "k\u00fcnf\u00b7tig", "wei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Weil dieses unsern Muth belebt,", "tokens": ["Weil", "die\u00b7ses", "un\u00b7sern", "Muth", "be\u00b7lebt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wenn Dich, den theuresten Bereuter,", "tokens": ["Wenn", "Dich", ",", "den", "theu\u00b7res\u00b7ten", "Be\u00b7reu\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "Sobald nichts aus dem Sattel hebt.", "tokens": ["So\u00b7bald", "nichts", "aus", "dem", "Sat\u00b7tel", "hebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Indessen soll dein Unterrichten", "tokens": ["In\u00b7des\u00b7sen", "soll", "dein", "Un\u00b7ter\u00b7rich\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Uns sonderlich zum Ruhm gedeyn,", "tokens": ["Uns", "son\u00b7der\u00b7lich", "zum", "Ruhm", "ge\u00b7deyn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Weil wir den k\u00fcnftigen Geschichten", "tokens": ["Weil", "wir", "den", "k\u00fcnf\u00b7ti\u00b7gen", "Ge\u00b7schich\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit Dir zugleich verbunden seyn;", "tokens": ["Mit", "Dir", "zu\u00b7gleich", "ver\u00b7bun\u00b7den", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dein Alter bey verj\u00fcngten Kr\u00e4ften,", "tokens": ["Dein", "Al\u00b7ter", "bey", "ver\u00b7j\u00fcng\u00b7ten", "Kr\u00e4f\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und der ohnunterbrochne Ritt", "tokens": ["Und", "der", "ohn\u00b7un\u00b7ter\u00b7broch\u00b7ne", "Ritt"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "In anvertrauten Amtsgesch\u00e4ften,", "tokens": ["In", "an\u00b7ver\u00b7trau\u00b7ten", "Amts\u00b7ge\u00b7sch\u00e4f\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Geh\u00f6rt zu raren Sachen mit.", "tokens": ["Ge\u00b7h\u00f6rt", "zu", "ra\u00b7ren", "Sa\u00b7chen", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Jedoch wird ein behutsam Schreiten", "tokens": ["Je\u00b7doch", "wird", "ein", "be\u00b7hut\u00b7sam", "Schrei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bey dem auf unsern R\u00fcckenruhn", "tokens": ["Bey", "dem", "auf", "un\u00b7sern", "R\u00fc\u00b7cken\u00b7ruhn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nicht deinen hochbejahrten Zeiten", "tokens": ["Nicht", "dei\u00b7nen", "hoch\u00b7be\u00b7jahr\u00b7ten", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKNEG", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch falsche Tritte Eintrag thun;", "tokens": ["Durch", "fal\u00b7sche", "Trit\u00b7te", "Ein\u00b7trag", "thun", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und sollte gar wer \u00fcberschlagen,", "tokens": ["Und", "soll\u00b7te", "gar", "wer", "\u00fc\u00b7bersc\u00b7hla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "PWS", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "So willigen wir allesammt:", "tokens": ["So", "wil\u00b7li\u00b7gen", "wir", "al\u00b7le\u00b7sammt", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ein solcher sey zu b\u00f6fen Tagen,", "tokens": ["Ein", "sol\u00b7cher", "sey", "zu", "b\u00f6\u00b7fen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "VAFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zu Peitsche, Karn und Spreu verdammt!", "tokens": ["Zu", "Peit\u00b7sche", ",", "Karn", "und", "Spreu", "ver\u00b7dammt", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}