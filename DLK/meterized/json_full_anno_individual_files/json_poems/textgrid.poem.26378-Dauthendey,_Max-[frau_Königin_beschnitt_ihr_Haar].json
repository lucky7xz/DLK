{"textgrid.poem.26378": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[frau K\u00f6nigin beschnitt ihr Haar]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Frau K\u00f6nigin beschnitt ihr Haar", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "be\u00b7schnitt", "ihr", "Haar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stets, wenn der Mond zunehmend war.", "tokens": ["Stets", ",", "wenn", "der", "Mond", "zu\u00b7neh\u00b7mend", "war", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbman wirft kein L\u00f6ckchen aus dem Fenster,", "tokens": ["\u00bb", "man", "wirft", "kein", "L\u00f6ck\u00b7chen", "aus", "dem", "Fens\u00b7ter", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Haare locken die Gespenster,", "tokens": ["Denn", "Haa\u00b7re", "lo\u00b7cken", "die", "Ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und V\u00f6gel, welche Nester bauen,", "tokens": ["Und", "V\u00f6\u00b7gel", ",", "wel\u00b7che", "Nes\u00b7ter", "bau\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PWAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auch denen soll man niemals trauen,", "tokens": ["Auch", "de\u00b7nen", "soll", "man", "nie\u00b7mals", "trau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Denn wenn sie deine Haare finden,", "tokens": ["Denn", "wenn", "sie", "dei\u00b7ne", "Haa\u00b7re", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00df Wahnsinn deinen Geist erblinden.\u00ab", "tokens": ["Mu\u00df", "Wahn\u00b7sinn", "dei\u00b7nen", "Geist", "er\u00b7blin\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Sie sprach: \u00bbDies sagt die Mutter mein,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Dies", "sagt", "die", "Mut\u00b7ter", "mein", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PDS", "VVFIN", "ART", "NN", "PPOSAT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch braucht es ja nicht wahr zu sein.\u00ab", "tokens": ["Doch", "braucht", "es", "ja", "nicht", "wahr", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "\u00bbja,\u00ab sagt ich, \u00bbsicher ist es wahr,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sagt", "ich", ",", "\u00bb", "si\u00b7cher", "ist", "es", "wahr", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADJD", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Heimlich schnitt jemand mir mein Haar,", "tokens": ["Heim\u00b7lich", "schnitt", "je\u00b7mand", "mir", "mein", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIS", "PPER", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.7": {"line.1": {"text": "Und V\u00f6gel taten es dann holen,", "tokens": ["Und", "V\u00f6\u00b7gel", "ta\u00b7ten", "es", "dann", "ho\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So ward mir mein Verstand gestohlen.", "tokens": ["So", "ward", "mir", "mein", "Ver\u00b7stand", "ge\u00b7stoh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wir leben wie die Kinder hin,", "tokens": ["Wir", "le\u00b7ben", "wie", "die", "Kin\u00b7der", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' nichts mehr in den Taschen drin,", "tokens": ["Hab'", "nichts", "mehr", "in", "den", "Ta\u00b7schen", "drin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Mit ohne Geld backt man kein Brot,", "tokens": ["Mit", "oh\u00b7ne", "Geld", "backt", "man", "kein", "Brot", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "NN", "VVFIN", "PIS", "PIAT", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Und t\u00f6dlich ist die Hungersnot.\u00ab", "tokens": ["Und", "t\u00f6d\u00b7lich", "ist", "die", "Hun\u00b7gers\u00b7not", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "\u00bbach,\u00ab lachte sie, \u00bbwie tut das wohl,", "tokens": ["\u00bb", "ach", ",", "\u00ab", "lach\u00b7te", "sie", ",", "\u00bb", "wie", "tut", "das", "wohl", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "PWAV", "VVFIN", "PDS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn man mal wirklich hungern soll.", "tokens": ["Wenn", "man", "mal", "wirk\u00b7lich", "hun\u00b7gern", "soll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Was macht uns das, dann sterben wir,", "tokens": ["Was", "macht", "uns", "das", ",", "dann", "ster\u00b7ben", "wir", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PDS", "$,", "ADV", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und st\u00fcndlich lieg' ich dann bei dir,", "tokens": ["Und", "st\u00fcnd\u00b7lich", "lieg'", "ich", "dann", "bei", "dir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Und gar nicht stehen wir mehr auf,", "tokens": ["Und", "gar", "nicht", "ste\u00b7hen", "wir", "mehr", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dies w\u00e4r' der rechte Lebenslauf.\u00ab", "tokens": ["Dies", "w\u00e4r'", "der", "rech\u00b7te", "Le\u00b7bens\u00b7lauf", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "\u00bbja,\u00ab sprach ich, \u00bbeinmal wird sich's geben,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sprach", "ich", ",", "\u00bb", "ein\u00b7mal", "wird", "sich's", "ge\u00b7ben", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADV", "VAFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis dahin mu\u00df man weiterleben.\u00ab", "tokens": ["Bis", "da\u00b7hin", "mu\u00df", "man", "wei\u00b7ter\u00b7le\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PAV", "VMFIN", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Zieht man den letzten Ring noch aus,", "tokens": ["Zieht", "man", "den", "letz\u00b7ten", "Ring", "noch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann ist's schon etwas leer im Haus,", "tokens": ["Dann", "ist's", "schon", "et\u00b7was", "leer", "im", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Und kann man diesen Ring verborgen,", "tokens": ["Und", "kann", "man", "die\u00b7sen", "Ring", "ver\u00b7bor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "PDAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann lebt man noch am n\u00e4chsten Morgen.", "tokens": ["Dann", "lebt", "man", "noch", "am", "n\u00e4chs\u00b7ten", "Mor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Doch \u00fcbermorgen ist nicht weit,", "tokens": ["Doch", "\u00fc\u00b7ber\u00b7mor\u00b7gen", "ist", "nicht", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hat man dann kein Geld bereit,", "tokens": ["Und", "hat", "man", "dann", "kein", "Geld", "be\u00b7reit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ADV", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "So klopft der Hunger an den Magen,", "tokens": ["So", "klopft", "der", "Hun\u00b7ger", "an", "den", "Ma\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und gar zu gern t\u00e4t man's ertragen.", "tokens": ["Und", "gar", "zu", "gern", "t\u00e4t", "man's", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKA", "ADV", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Wir fanden noch in einer Weste", "tokens": ["Wir", "fan\u00b7den", "noch", "in", "ei\u00b7ner", "Wes\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "An Geld kupferne \u00dcberreste", "tokens": ["An", "Geld", "kup\u00b7fer\u00b7ne", "\u00dc\u00b7ber\u00b7res\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Und kauften etwas Spiritus,", "tokens": ["Und", "kauf\u00b7ten", "et\u00b7was", "Spi\u00b7ri\u00b7tus", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kochten uns ein St\u00e4rkemus,", "tokens": ["Und", "koch\u00b7ten", "uns", "ein", "St\u00e4r\u00b7ke\u00b7mus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Gef\u00e4rbt mit wenig Schokolade,", "tokens": ["Ge\u00b7f\u00e4rbt", "mit", "we\u00b7nig", "Scho\u00b7ko\u00b7la\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann schmeckt die St\u00e4rke nicht zu fade.", "tokens": ["Dann", "schmeckt", "die", "St\u00e4r\u00b7ke", "nicht", "zu", "fa\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PTKNEG", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Wir r\u00fchrten in der Kasseroll", "tokens": ["Wir", "r\u00fchr\u00b7ten", "in", "der", "Kas\u00b7se\u00b7roll"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wu\u00dften nicht, was werden soll.", "tokens": ["Und", "wu\u00df\u00b7ten", "nicht", ",", "was", "wer\u00b7den", "soll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "PRELS", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Und sonderbar, sie hatte recht,", "tokens": ["Und", "son\u00b7der\u00b7bar", ",", "sie", "hat\u00b7te", "recht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Hunger schmeckte nicht so schlecht,", "tokens": ["Der", "Hun\u00b7ger", "schmeck\u00b7te", "nicht", "so", "schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Und vornehm taten wir ihn tragen", "tokens": ["Und", "vor\u00b7nehm", "ta\u00b7ten", "wir", "ihn", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Geadelt von dem leeren Magen.", "tokens": ["Ge\u00b7a\u00b7delt", "von", "dem", "lee\u00b7ren", "Ma\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Doch gehen nachts die Sterne auf,", "tokens": ["Doch", "ge\u00b7hen", "nachts", "die", "Ster\u00b7ne", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So sieht man meistens mal hinauf.", "tokens": ["So", "sieht", "man", "meis\u00b7tens", "mal", "hin\u00b7auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "In der Nacht machen sie mich toll,", "tokens": ["In", "der", "Nacht", "ma\u00b7chen", "sie", "mich", "toll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPER", "PRF", "ADJD", "$,"], "meter": "--++-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Wu\u00dft' noch nicht, wie's uns werden soll.", "tokens": ["Wu\u00dft'", "noch", "nicht", ",", "wie's", "uns", "wer\u00b7den", "soll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKNEG", "$,", "KOUS", "PPER", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "\u00bbihr Weltst\u00fccke dort in der Nacht,", "tokens": ["\u00bb", "ihr", "Welt\u00b7st\u00fc\u00b7cke", "dort", "in", "der", "Nacht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Sagt mir, warum bin ich gemacht?", "tokens": ["Sagt", "mir", ",", "wa\u00b7rum", "bin", "ich", "ge\u00b7macht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Ich tue meinen Willen sp\u00fcren,", "tokens": ["Ich", "tue", "mei\u00b7nen", "Wil\u00b7len", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und trotzdem tut man mich regieren.", "tokens": ["Und", "trotz\u00b7dem", "tut", "man", "mich", "re\u00b7gie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PIS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Hat einer Recht, mich zu bezwingen,", "tokens": ["Hat", "ei\u00b7ner", "Recht", ",", "mich", "zu", "be\u00b7zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Geb\u00e4ren mich und umzubringen?", "tokens": ["Ge\u00b7b\u00e4\u00b7ren", "mich", "und", "um\u00b7zu\u00b7brin\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "KON", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Mein Weib soll mich unfehlbar sehn,", "tokens": ["Mein", "Weib", "soll", "mich", "un\u00b7fehl\u00b7bar", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich w\u00fcnsche mich sonst ungeschehn.", "tokens": ["Ich", "w\u00fcn\u00b7sche", "mich", "sonst", "un\u00b7ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Liebe ist Herr, hat alles Recht,", "tokens": ["Lie\u00b7be", "ist", "Herr", ",", "hat", "al\u00b7les", "Recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "NN", "$,", "VAFIN", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Nur ungeliebt ist man ein Knecht.", "tokens": ["Nur", "un\u00b7ge\u00b7liebt", "ist", "man", "ein", "Knecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Es reizt mich gar nicht, mich zu t\u00f6ten,", "tokens": ["Es", "reizt", "mich", "gar", "nicht", ",", "mich", "zu", "t\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das rettet nicht vor Zukunftsn\u00f6te.", "tokens": ["Das", "ret\u00b7tet", "nicht", "vor", "Zu\u00b7kunfts\u00b7n\u00f6\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Ein Herr bin ich und bin ein Mann,", "tokens": ["Ein", "Herr", "bin", "ich", "und", "bin", "ein", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "KON", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der keinen Zwang mehr dulden kann,", "tokens": ["Der", "kei\u00b7nen", "Zwang", "mehr", "dul\u00b7den", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Mein ganzes Leben sei vergessen,", "tokens": ["Mein", "gan\u00b7zes", "Le\u00b7ben", "sei", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' ich nicht morgen was zu essen.", "tokens": ["Hab'", "ich", "nicht", "mor\u00b7gen", "was", "zu", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Nacht, undurchdringliche Pupille,", "tokens": ["Nacht", ",", "un\u00b7durch\u00b7dring\u00b7li\u00b7che", "Pu\u00b7pil\u00b7le", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mein Fedehandschuh sei mein Wille!\u00ab", "tokens": ["Mein", "Fe\u00b7de\u00b7hand\u00b7schuh", "sei", "mein", "Wil\u00b7le", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Und sonderbar, in dieser Nacht", "tokens": ["Und", "son\u00b7der\u00b7bar", ",", "in", "die\u00b7ser", "Nacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bin z\u00e4hneklappernd ich erwacht.", "tokens": ["Bin", "z\u00e4h\u00b7ne\u00b7klap\u00b7pernd", "ich", "er\u00b7wacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Unheimlich war ein Traum gekommen,", "tokens": ["Un\u00b7heim\u00b7lich", "war", "ein", "Traum", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat meinen K\u00f6rper mir genommen.", "tokens": ["Hat", "mei\u00b7nen", "K\u00f6r\u00b7per", "mir", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Es war in einem hohen Haus,", "tokens": ["Es", "war", "in", "ei\u00b7nem", "ho\u00b7hen", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Ganze sah verlassen aus,", "tokens": ["Das", "Gan\u00b7ze", "sah", "ver\u00b7las\u00b7sen", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Der letzte Mensch kam an die T\u00fcr", "tokens": ["Der", "letz\u00b7te", "Mensch", "kam", "an", "die", "T\u00fcr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00df mich dann allein mit mir.", "tokens": ["Und", "lie\u00df", "mich", "dann", "al\u00b7lein", "mit", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "In mir war selbst nichts mehr zu lesen,", "tokens": ["In", "mir", "war", "selbst", "nichts", "mehr", "zu", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "ADV", "PIS", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn nichts an mir schien je gewesen,", "tokens": ["Denn", "nichts", "an", "mir", "schien", "je", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "PPER", "VVFIN", "ADV", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "Hatte nur furchtbar viele Zeit,", "tokens": ["Hat\u00b7te", "nur", "furcht\u00b7bar", "vie\u00b7le", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "F\u00fchlte, \u2013 dies war die Ewigkeit.", "tokens": ["F\u00fchl\u00b7te", ",", "\u2013", "dies", "war", "die", "E\u00b7wig\u00b7keit", "."], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "$(", "PDS", "VAFIN", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.41": {"line.1": {"text": "Zwar wu\u00dft' ich noch nicht, was ich soll,", "tokens": ["Zwar", "wu\u00dft'", "ich", "noch", "nicht", ",", "was", "ich", "soll", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch Hunger z\u00e4hmte Zoll um Zoll.", "tokens": ["Doch", "Hun\u00b7ger", "z\u00e4hm\u00b7te", "Zoll", "um", "Zoll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Und als der n\u00e4chste Abend kam,", "tokens": ["Und", "als", "der", "n\u00e4chs\u00b7te", "A\u00b7bend", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich anst\u00e4ndiger mich benahm.", "tokens": ["Ich", "an\u00b7st\u00e4n\u00b7di\u00b7ger", "mich", "be\u00b7nahm", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Ich bat: \u00bbIhr Sternenungeheuer,", "tokens": ["Ich", "bat", ":", "\u00bb", "Ihr", "Ster\u00b7ne\u00b7nun\u00b7ge\u00b7heu\u00b7er", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gold, wei\u00df ich, schwimmt in eurem Feuer.", "tokens": ["Gold", ",", "wei\u00df", "ich", ",", "schwimmt", "in", "eu\u00b7rem", "Feu\u00b7er", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "PPER", "$,", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Nur eine Handvoll m\u00f6gt ihr geben,", "tokens": ["Nur", "ei\u00b7ne", "Hand\u00b7voll", "m\u00f6gt", "ihr", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vorl\u00e4ufig h\u00e4tt' ich dann zum Leben.", "tokens": ["Vor\u00b7l\u00e4u\u00b7fig", "h\u00e4tt'", "ich", "dann", "zum", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Ihr Br\u00fcder, habt ihr mich vergessen?", "tokens": ["Ihr", "Br\u00fc\u00b7der", ",", "habt", "ihr", "mich", "ver\u00b7ges\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VAFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich liebe und hab' nichts zu essen.\u00ab", "tokens": ["Ich", "lie\u00b7be", "und", "hab'", "nichts", "zu", "es\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "KON", "VAFIN", "PIS", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Da endlich mich die Antwort traf,", "tokens": ["Da", "end\u00b7lich", "mich", "die", "Ant\u00b7wort", "traf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ins Ohr sprach jemand mir im Schlaf:", "tokens": ["Ins", "Ohr", "sprach", "je\u00b7mand", "mir", "im", "Schlaf", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "\u00bbmein Junge, du wirst noch nicht sterben,", "tokens": ["\u00bb", "mein", "Jun\u00b7ge", ",", "du", "wirst", "noch", "nicht", "ster\u00b7ben", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PPER", "VAFIN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dein Vater stirbt, und du wirst erben.\u00ab", "tokens": ["Dein", "Va\u00b7ter", "stirbt", ",", "und", "du", "wirst", "er\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "KON", "PPER", "VAFIN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Der Traum, der hat mich aufgeschreckt,", "tokens": ["Der", "Traum", ",", "der", "hat", "mich", "auf\u00b7ge\u00b7schreckt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau K\u00f6nigin hab' ich geweckt,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "hab'", "ich", "ge\u00b7weckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Erz\u00e4hlte ihr, wie alles war,", "tokens": ["Er\u00b7z\u00e4hl\u00b7te", "ihr", ",", "wie", "al\u00b7les", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch sie findet nichts sonderbar.", "tokens": ["Doch", "sie", "fin\u00b7det", "nichts", "son\u00b7der\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIS", "ADJD", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.50": {"line.1": {"text": "Sprach: \u00bbDa\u00df der Himmel Botschaft sendet,", "tokens": ["Sprach", ":", "\u00bb", "Da\u00df", "der", "Him\u00b7mel", "Bot\u00b7schaft", "sen\u00b7det", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "KOUS", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist gut, denn wir sind ausgepf\u00e4ndet.", "tokens": ["Ist", "gut", ",", "denn", "wir", "sind", "aus\u00b7ge\u00b7pf\u00e4n\u00b7det", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "KON", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Zwar, da\u00df der Vater stirbt, tut weh,", "tokens": ["Zwar", ",", "da\u00df", "der", "Va\u00b7ter", "stirbt", ",", "tut", "weh", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "VVFIN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch da ich keine Rettung seh',", "tokens": ["Doch", "da", "ich", "kei\u00b7ne", "Ret\u00b7tung", "seh'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Und da es unser Schicksal will,", "tokens": ["Und", "da", "es", "un\u00b7ser", "Schick\u00b7sal", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPOSAT", "NN", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So erben wir und trauern still.", "tokens": ["So", "er\u00b7ben", "wir", "und", "trau\u00b7ern", "still", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Doch ist der Traum dir nur gelogen,", "tokens": ["Doch", "ist", "der", "Traum", "dir", "nur", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat um die Ruh' er mich betrogen:", "tokens": ["Hat", "um", "die", "Ruh'", "er", "mich", "be\u00b7tro\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Just bin im Traum ich satt gewesen", "tokens": ["Just", "bin", "im", "Traum", "ich", "satt", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "APPRART", "NN", "PPER", "ADJD", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und hatte wundervoll gegessen.\u00ab", "tokens": ["Und", "hat\u00b7te", "wun\u00b7der\u00b7voll", "ge\u00b7ges\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADJD", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Am Morgen sprach ich: \u00bbDenke kaum", "tokens": ["Am", "Mor\u00b7gen", "sprach", "ich", ":", "\u00bb", "Den\u00b7ke", "kaum"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "$.", "$(", "VVIMP", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An diesen b\u00f6sen Erbschaftstraum.", "tokens": ["An", "die\u00b7sen", "b\u00f6\u00b7sen", "Erb\u00b7schafts\u00b7traum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Es ist ein Frevel, so zu denken.", "tokens": ["Es", "ist", "ein", "Fre\u00b7vel", ",", "so", "zu", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Will nur dem Leben Glauben schenken.", "tokens": ["Will", "nur", "dem", "Le\u00b7ben", "Glau\u00b7ben", "schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Sieh, immer fand ich wunderbar", "tokens": ["Sieh", ",", "im\u00b7mer", "fand", "ich", "wun\u00b7der\u00b7bar"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$,", "ADV", "VVFIN", "PPER", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Seidenzeug, das aus Japan war,", "tokens": ["Sei\u00b7den\u00b7zeug", ",", "das", "aus", "Ja\u00b7pan", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NE", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.58": {"line.1": {"text": "Es war mir bunte Augenweide,", "tokens": ["Es", "war", "mir", "bun\u00b7te", "Au\u00b7gen\u00b7wei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die ganze Welt scheint dort aus Seide.", "tokens": ["Die", "gan\u00b7ze", "Welt", "scheint", "dort", "aus", "Sei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Und Japan fiel mir heute ein,", "tokens": ["Und", "Ja\u00b7pan", "fiel", "mir", "heu\u00b7te", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und jetzt soll uns geholfen sein.", "tokens": ["Und", "jetzt", "soll", "uns", "ge\u00b7hol\u00b7fen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Ich zeichne Bilder auf die Seide,", "tokens": ["Ich", "zeich\u00b7ne", "Bil\u00b7der", "auf", "die", "Sei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und du stickst sie zur Augenweide.", "tokens": ["Und", "du", "stickst", "sie", "zur", "Au\u00b7gen\u00b7wei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Damit werden wir Geld verdienen", "tokens": ["Da\u00b7mit", "wer\u00b7den", "wir", "Geld", "ver\u00b7die\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "NN", "VVINF"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und legen ab die Hungermienen.\u00ab", "tokens": ["Und", "le\u00b7gen", "ab", "die", "Hun\u00b7ger\u00b7mie\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.62": {"line.1": {"text": "\u00bbach nein,\u00ab sagte Frau K\u00f6nigin,", "tokens": ["\u00bb", "ach", "nein", ",", "\u00ab", "sag\u00b7te", "Frau", "K\u00f6\u00b7ni\u00b7gin", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "XY", "PTKANT", "$,", "$(", "VVFIN", "NN", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "\u00bbdas Sticken, das verdreht den Sinn.", "tokens": ["\u00bb", "das", "Sti\u00b7cken", ",", "das", "ver\u00b7dreht", "den", "Sinn", "."], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "PDS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Doch, wenn ich h\u00f6re meine Stimme,", "tokens": ["Doch", ",", "wenn", "ich", "h\u00f6\u00b7re", "mei\u00b7ne", "Stim\u00b7me", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist's, als ob ich im Himmel schwimme.", "tokens": ["Ist's", ",", "als", "ob", "ich", "im", "Him\u00b7mel", "schwim\u00b7me", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOKOM", "KOUS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Die Stimme, sie ist zwar noch klein,", "tokens": ["Die", "Stim\u00b7me", ",", "sie", "ist", "zwar", "noch", "klein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VAFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Und deshalb \u00fcb' ich sie erst ein.", "tokens": ["Und", "des\u00b7halb", "\u00fcb'", "ich", "sie", "erst", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Dann singe ich auf allen Stra\u00dfen,", "tokens": ["Dann", "sin\u00b7ge", "ich", "auf", "al\u00b7len", "Stra\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Geld sind wir dann nie verlassen.\u00ab", "tokens": ["Von", "Geld", "sind", "wir", "dann", "nie", "ver\u00b7las\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "\u00bbja,\u00ab sprach ich, \u00bb\u00fcbe dich nur ein,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sprach", "ich", ",", "\u00bb", "\u00fc\u00b7be", "dich", "nur", "ein", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und la\u00df das Sticken mir allein.", "tokens": ["Und", "la\u00df", "das", "Sti\u00b7cken", "mir", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "ART", "NN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "Sorgen, sie hindern mich am Schnaufen,", "tokens": ["Sor\u00b7gen", ",", "sie", "hin\u00b7dern", "mich", "am", "Schnau\u00b7fen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wer gibt uns Geld zum Seidekaufen?", "tokens": ["Wer", "gibt", "uns", "Geld", "zum", "Sei\u00b7de\u00b7kau\u00b7fen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Ja, Seide braucht man, das ist wahr.\u00ab", "tokens": ["Ja", ",", "Sei\u00b7de", "braucht", "man", ",", "das", "ist", "wahr", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PTKANT", "$,", "NE", "VVFIN", "PIS", "$,", "PDS", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Anbetend sah ich auf ihr Haar.", "tokens": ["An\u00b7be\u00b7tend", "sah", "ich", "auf", "ihr", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.69": {"line.1": {"text": "Sie scherzte: \u00bbWenn ich 's Haar abschneide,", "tokens": ["Sie", "scherz\u00b7te", ":", "\u00bb", "Wenn", "ich", "'s", "Haar", "ab\u00b7schnei\u00b7de", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "KOUS", "PPER", "PPER", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dann brauchen wir kein Geld zur Seide.\u00ab", "tokens": ["Dann", "brau\u00b7chen", "wir", "kein", "Geld", "zur", "Sei\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.70": {"line.1": {"text": "Da grollte ich dumpf wie ein B\u00e4r:", "tokens": ["Da", "groll\u00b7te", "ich", "dumpf", "wie", "ein", "B\u00e4r", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "\u00bbvon deinem Haar geb' ich nichts her,", "tokens": ["\u00bb", "von", "dei\u00b7nem", "Haar", "geb'", "ich", "nichts", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "PPOSAT", "NN", "VVFIN", "PPER", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.71": {"line.1": {"text": "Das w\u00e4r', als ob ich dich verkaufe.", "tokens": ["Das", "w\u00e4r'", ",", "als", "ob", "ich", "dich", "ver\u00b7kau\u00b7fe", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "$,", "KOKOM", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ach, da\u00df ich gleich mein Herz ausraufe.\u00ab", "tokens": ["Ach", ",", "da\u00df", "ich", "gleich", "mein", "Herz", "aus\u00b7rau\u00b7fe", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ITJ", "$,", "KOUS", "PPER", "ADV", "PPOSAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Doch lie\u00df sie nicht ihr Scherzen sein", "tokens": ["Doch", "lie\u00df", "sie", "nicht", "ihr", "Scher\u00b7zen", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "PPOSAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00e4delte ein Haar selbst ein,", "tokens": ["Und", "f\u00e4\u00b7del\u00b7te", "ein", "Haar", "selbst", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "Dr\u00fcckt mir die Nadel in die Hand;", "tokens": ["Dr\u00fcckt", "mir", "die", "Na\u00b7del", "in", "die", "Hand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hab' mich d\u00fcster abgewandt.", "tokens": ["Ich", "hab'", "mich", "d\u00fcs\u00b7ter", "ab\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Ich setzte mich ans Fenster hin,", "tokens": ["Ich", "setz\u00b7te", "mich", "ans", "Fens\u00b7ter", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und drunten ging Frau K\u00f6nigin", "tokens": ["Und", "drun\u00b7ten", "ging", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Am frommen Kleefeld auf und nieder", "tokens": ["Am", "from\u00b7men", "Klee\u00b7feld", "auf", "und", "nie\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "PTKVZ", "KON", "PTKVZ"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und dehnte im Gesang ihr Mieder.", "tokens": ["Und", "dehn\u00b7te", "im", "Ge\u00b7sang", "ihr", "Mie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.76": {"line.1": {"text": "Da wurde mir so wohl im Blut,", "tokens": ["Da", "wur\u00b7de", "mir", "so", "wohl", "im", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fchlte mich wie der Klee so gut,", "tokens": ["F\u00fchl\u00b7te", "mich", "wie", "der", "Klee", "so", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.77": {"line.1": {"text": "F\u00fchlte mich Staub und Meeressand,", "tokens": ["F\u00fchl\u00b7te", "mich", "Staub", "und", "Mee\u00b7res\u00b7sand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Stach mir die Nadel in die Hand,", "tokens": ["Stach", "mir", "die", "Na\u00b7del", "in", "die", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.78": {"line.1": {"text": "Hing an ihr Haar ein Tr\u00f6pflein Blut,", "tokens": ["Hing", "an", "ihr", "Haar", "ein", "Tr\u00f6pf\u00b7lein", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' nie so nah bei ihr geruht.", "tokens": ["Hab'", "nie", "so", "nah", "bei", "ihr", "ge\u00b7ruht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "APPR", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Ihr Stimmlein tat mich selig heben,", "tokens": ["Ihr", "Stimm\u00b7lein", "tat", "mich", "se\u00b7lig", "he\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tat \u00fcber allen Hungern schweben.", "tokens": ["Tat", "\u00fc\u00b7ber", "al\u00b7len", "Hun\u00b7gern", "schwe\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Doch wei\u00df ich nicht, wie es dann kam,", "tokens": ["Doch", "wei\u00df", "ich", "nicht", ",", "wie", "es", "dann", "kam", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "$,", "PWAV", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich die Hand zur Nase nahm,", "tokens": ["Da\u00df", "ich", "die", "Hand", "zur", "Na\u00b7se", "nahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "Sie roch wie Zigarettenrauch,", "tokens": ["Sie", "roch", "wie", "Zi\u00b7ga\u00b7ret\u00b7ten\u00b7rauch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ganz so roch stets mein Vater auch.", "tokens": ["Ganz", "so", "roch", "stets", "mein", "Va\u00b7ter", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "PPOSAT", "NN", "ADV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.82": {"line.1": {"text": "Dreimal wusch ich mir beide H\u00e4nde,", "tokens": ["Drei\u00b7mal", "wusch", "ich", "mir", "bei\u00b7de", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PIAT", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und immer war's, als ob ich f\u00e4nde", "tokens": ["Und", "im\u00b7mer", "wa\u00b7r's", ",", "als", "ob", "ich", "f\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "$,", "KOKOM", "KOUS", "PPER", "VVFIN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.83": {"line.1": {"text": "Des Vaters Atem nahe hier,", "tokens": ["Des", "Va\u00b7ters", "A\u00b7tem", "na\u00b7he", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADJD", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ganz unheimlich war das mir.", "tokens": ["Und", "ganz", "un\u00b7heim\u00b7lich", "war", "das", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VAFIN", "ART", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Und ehe noch der Abend kam,", "tokens": ["Und", "e\u00b7he", "noch", "der", "A\u00b7bend", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erhielt ich kurz ein Telegramm.", "tokens": ["Er\u00b7hielt", "ich", "kurz", "ein", "Te\u00b7le\u00b7gramm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.85": {"line.1": {"text": "Ich konnt' es nicht vor Tr\u00e4nen lesen:", "tokens": ["Ich", "konnt'", "es", "nicht", "vor", "Tr\u00e4\u00b7nen", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der ernste Traum ist wahr gewesen.", "tokens": ["Der", "erns\u00b7te", "Traum", "ist", "wahr", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "Doch dieses Trauertelegramm", "tokens": ["Doch", "die\u00b7ses", "Trau\u00b7er\u00b7te\u00b7le\u00b7gramm"], "token_info": ["word", "word", "word"], "pos": ["KON", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erweckt auf meinen Wangen Scham.", "tokens": ["Er\u00b7weckt", "auf", "mei\u00b7nen", "Wan\u00b7gen", "Scham", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.87": {"line.1": {"text": "Ich nahm es zu dem Kaufmann mit,", "tokens": ["Ich", "nahm", "es", "zu", "dem", "Kauf\u00b7mann", "mit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und er gab uns sofort Kredit,", "tokens": ["Und", "er", "gab", "uns", "so\u00b7fort", "Kre\u00b7dit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.88": {"line.1": {"text": "Und alles ward uns reich bemessen,", "tokens": ["Und", "al\u00b7les", "ward", "uns", "reich", "be\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Haben uns weinend satt gegessen,", "tokens": ["Ha\u00b7ben", "uns", "wei\u00b7nend", "satt", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.89": {"line.1": {"text": "Wir konnten uns nicht selbst betr\u00fcgen,", "tokens": ["Wir", "konn\u00b7ten", "uns", "nicht", "selbst", "be\u00b7tr\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir a\u00dfen beinah mit Vergn\u00fcgen.", "tokens": ["Wir", "a\u00b7\u00dfen", "bei\u00b7nah", "mit", "Ver\u00b7gn\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.90": {"line.1": {"text": "Ich fand uns da im Grund nicht besser", "tokens": ["Ich", "fand", "uns", "da", "im", "Grund", "nicht", "bes\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKNEG", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als zwei bewu\u00dfte Menschenfresser.", "tokens": ["Als", "zwei", "be\u00b7wu\u00df\u00b7te", "Men\u00b7schen\u00b7fres\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.91": {"line.1": {"text": "Auch stolz machte mich ganz und gar,", "tokens": ["Auch", "stolz", "mach\u00b7te", "mich", "ganz", "und", "gar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "ADV", "KON", "ADV", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da\u00df ich so auserw\u00e4hlet war,", "tokens": ["Da\u00df", "ich", "so", "au\u00b7ser\u00b7w\u00e4h\u00b7let", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.92": {"line.1": {"text": "Da\u00df Gott nachts selbst zu mir gesprochen", "tokens": ["Da\u00df", "Gott", "nachts", "selbst", "zu", "mir", "ge\u00b7spro\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "ADV", "ADV", "APPR", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sein Inkognito gebrochen. \u2013", "tokens": ["Und", "sein", "In\u00b7kog\u00b7ni\u00b7to", "ge\u00b7bro\u00b7chen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.93": {"line.1": {"text": "Wenn man im Grab wen kennen lernt,", "tokens": ["Wenn", "man", "im", "Grab", "wen", "ken\u00b7nen", "lernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVINF", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist's schlimm, verwandt oder entfernt.", "tokens": ["Ist's", "schlimm", ",", "ver\u00b7wandt", "o\u00b7der", "ent\u00b7fernt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "$,", "VVPP", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.94": {"line.1": {"text": "Frau K\u00f6nigin tat es so gehn,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "tat", "es", "so", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als sie am frischen Grab tat stehn,", "tokens": ["Als", "sie", "am", "fri\u00b7schen", "Grab", "tat", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADJA", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.95": {"line.1": {"text": "Sie sprach: \u00bbIch glaube nicht daran,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Ich", "glau\u00b7be", "nicht", "da\u00b7ran", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PTKNEG", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man im Grabe sterben kann.", "tokens": ["Da\u00df", "man", "im", "Gra\u00b7be", "ster\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.96": {"line.1": {"text": "Menschen, die einem vieles schenken,", "tokens": ["Men\u00b7schen", ",", "die", "ei\u00b7nem", "vie\u00b7les", "schen\u00b7ken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ART", "PIS", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kann man sich gar nicht sterben denken.\u00ab", "tokens": ["Kann", "man", "sich", "gar", "nicht", "ster\u00b7ben", "den\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PIS", "PRF", "ADV", "PTKNEG", "VVINF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.97": {"line.1": {"text": "\u00bbja,\u00ab sagte ich, \u00bblebte er weiter,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sag\u00b7te", "ich", ",", "\u00bb", "leb\u00b7te", "er", "wei\u00b7ter", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "So spr\u00e4ch er jetzt: \u203aKinder, seid heiter,", "tokens": ["So", "spr\u00e4ch", "er", "jetzt", ":", "\u203a", "Kin\u00b7der", ",", "seid", "hei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$.", "$(", "NN", "$,", "VAFIN", "ADJD", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.98": {"line.1": {"text": "Ein jeder wird es mal allm\u00e4hlich,", "tokens": ["Ein", "je\u00b7der", "wird", "es", "mal", "all\u00b7m\u00e4h\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und lebend ist man wirklich selig.", "tokens": ["Und", "le\u00b7bend", "ist", "man", "wirk\u00b7lich", "se\u00b7lig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PIS", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.99": {"line.1": {"text": "Steht euch die Welt jetzt auf der H\u00f6he,", "tokens": ["Steht", "euch", "die", "Welt", "jetzt", "auf", "der", "H\u00f6\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei\u00dfen respektvoller die Fl\u00f6he;", "tokens": ["Bei\u00b7\u00dfen", "res\u00b7pekt\u00b7vol\u00b7ler", "die", "Fl\u00f6\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.100": {"line.1": {"text": "Die Liebe ist nicht blind erfunden,", "tokens": ["Die", "Lie\u00b7be", "ist", "nicht", "blind", "er\u00b7fun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Haltet euch an die Liebesstunden.", "tokens": ["Hal\u00b7tet", "euch", "an", "die", "Lie\u00b7bess\u00b7tun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.101": {"line.1": {"text": "Erlebt stets die Realit\u00e4t,", "tokens": ["Er\u00b7lebt", "stets", "die", "Re\u00b7a\u00b7li\u00b7t\u00e4t", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hei\u00df wie der Topf am Feuer steht.", "tokens": ["Hei\u00df", "wie", "der", "Topf", "am", "Feu\u00b7er", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.102": {"line.1": {"text": "Und jetzt sollt ihr Siesta halten,", "tokens": ["Und", "jetzt", "sollt", "ihr", "Sies\u00b7ta", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Und legt die Stirn in keine Falten,", "tokens": ["Und", "legt", "die", "Stirn", "in", "kei\u00b7ne", "Fal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.103": {"line.1": {"text": "F\u00fcrs Leben es euch zw\u00f6lf Uhr schlug,", "tokens": ["F\u00fcrs", "Le\u00b7ben", "es", "euch", "zw\u00f6lf", "Uhr", "schlug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "PPER", "PPER", "CARD", "NN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Habt jetzt vom Vormittag genug,", "tokens": ["Habt", "jetzt", "vom", "Vor\u00b7mit\u00b7tag", "ge\u00b7nug", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.104": {"line.1": {"text": "Und dehnt die Liebe auch noch aus,", "tokens": ["Und", "dehnt", "die", "Lie\u00b7be", "auch", "noch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geht die Siesta mal hinaus.\u2039\u00ab", "tokens": ["Geht", "die", "Sies\u00b7ta", "mal", "hin\u00b7aus", ".", "\u2039", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$(", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.105": {"line.1": {"text": "Frau K\u00f6nigin beschnitt ihr Haar", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "be\u00b7schnitt", "ihr", "Haar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stets, wenn der Mond zunehmend war.", "tokens": ["Stets", ",", "wenn", "der", "Mond", "zu\u00b7neh\u00b7mend", "war", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.106": {"line.1": {"text": "\u00bbman wirft kein L\u00f6ckchen aus dem Fenster,", "tokens": ["\u00bb", "man", "wirft", "kein", "L\u00f6ck\u00b7chen", "aus", "dem", "Fens\u00b7ter", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Haare locken die Gespenster,", "tokens": ["Denn", "Haa\u00b7re", "lo\u00b7cken", "die", "Ge\u00b7spens\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.107": {"line.1": {"text": "Und V\u00f6gel, welche Nester bauen,", "tokens": ["Und", "V\u00f6\u00b7gel", ",", "wel\u00b7che", "Nes\u00b7ter", "bau\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PWAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auch denen soll man niemals trauen,", "tokens": ["Auch", "de\u00b7nen", "soll", "man", "nie\u00b7mals", "trau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.108": {"line.1": {"text": "Denn wenn sie deine Haare finden,", "tokens": ["Denn", "wenn", "sie", "dei\u00b7ne", "Haa\u00b7re", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00df Wahnsinn deinen Geist erblinden.\u00ab", "tokens": ["Mu\u00df", "Wahn\u00b7sinn", "dei\u00b7nen", "Geist", "er\u00b7blin\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.109": {"line.1": {"text": "Sie sprach: \u00bbDies sagt die Mutter mein,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Dies", "sagt", "die", "Mut\u00b7ter", "mein", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PDS", "VVFIN", "ART", "NN", "PPOSAT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch braucht es ja nicht wahr zu sein.\u00ab", "tokens": ["Doch", "braucht", "es", "ja", "nicht", "wahr", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.110": {"line.1": {"text": "\u00bbja,\u00ab sagt ich, \u00bbsicher ist es wahr,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sagt", "ich", ",", "\u00bb", "si\u00b7cher", "ist", "es", "wahr", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADJD", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Heimlich schnitt jemand mir mein Haar,", "tokens": ["Heim\u00b7lich", "schnitt", "je\u00b7mand", "mir", "mein", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIS", "PPER", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.111": {"line.1": {"text": "Und V\u00f6gel taten es dann holen,", "tokens": ["Und", "V\u00f6\u00b7gel", "ta\u00b7ten", "es", "dann", "ho\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So ward mir mein Verstand gestohlen.", "tokens": ["So", "ward", "mir", "mein", "Ver\u00b7stand", "ge\u00b7stoh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.112": {"line.1": {"text": "Wir leben wie die Kinder hin,", "tokens": ["Wir", "le\u00b7ben", "wie", "die", "Kin\u00b7der", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' nichts mehr in den Taschen drin,", "tokens": ["Hab'", "nichts", "mehr", "in", "den", "Ta\u00b7schen", "drin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.113": {"line.1": {"text": "Mit ohne Geld backt man kein Brot,", "tokens": ["Mit", "oh\u00b7ne", "Geld", "backt", "man", "kein", "Brot", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "NN", "VVFIN", "PIS", "PIAT", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Und t\u00f6dlich ist die Hungersnot.\u00ab", "tokens": ["Und", "t\u00f6d\u00b7lich", "ist", "die", "Hun\u00b7gers\u00b7not", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.114": {"line.1": {"text": "\u00bbach,\u00ab lachte sie, \u00bbwie tut das wohl,", "tokens": ["\u00bb", "ach", ",", "\u00ab", "lach\u00b7te", "sie", ",", "\u00bb", "wie", "tut", "das", "wohl", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "PWAV", "VVFIN", "PDS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn man mal wirklich hungern soll.", "tokens": ["Wenn", "man", "mal", "wirk\u00b7lich", "hun\u00b7gern", "soll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.115": {"line.1": {"text": "Was macht uns das, dann sterben wir,", "tokens": ["Was", "macht", "uns", "das", ",", "dann", "ster\u00b7ben", "wir", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PDS", "$,", "ADV", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und st\u00fcndlich lieg' ich dann bei dir,", "tokens": ["Und", "st\u00fcnd\u00b7lich", "lieg'", "ich", "dann", "bei", "dir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.116": {"line.1": {"text": "Und gar nicht stehen wir mehr auf,", "tokens": ["Und", "gar", "nicht", "ste\u00b7hen", "wir", "mehr", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dies w\u00e4r' der rechte Lebenslauf.\u00ab", "tokens": ["Dies", "w\u00e4r'", "der", "rech\u00b7te", "Le\u00b7bens\u00b7lauf", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.117": {"line.1": {"text": "\u00bbja,\u00ab sprach ich, \u00bbeinmal wird sich's geben,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sprach", "ich", ",", "\u00bb", "ein\u00b7mal", "wird", "sich's", "ge\u00b7ben", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADV", "VAFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis dahin mu\u00df man weiterleben.\u00ab", "tokens": ["Bis", "da\u00b7hin", "mu\u00df", "man", "wei\u00b7ter\u00b7le\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PAV", "VMFIN", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.118": {"line.1": {"text": "Zieht man den letzten Ring noch aus,", "tokens": ["Zieht", "man", "den", "letz\u00b7ten", "Ring", "noch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann ist's schon etwas leer im Haus,", "tokens": ["Dann", "ist's", "schon", "et\u00b7was", "leer", "im", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.119": {"line.1": {"text": "Und kann man diesen Ring verborgen,", "tokens": ["Und", "kann", "man", "die\u00b7sen", "Ring", "ver\u00b7bor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "PDAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann lebt man noch am n\u00e4chsten Morgen.", "tokens": ["Dann", "lebt", "man", "noch", "am", "n\u00e4chs\u00b7ten", "Mor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.120": {"line.1": {"text": "Doch \u00fcbermorgen ist nicht weit,", "tokens": ["Doch", "\u00fc\u00b7ber\u00b7mor\u00b7gen", "ist", "nicht", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hat man dann kein Geld bereit,", "tokens": ["Und", "hat", "man", "dann", "kein", "Geld", "be\u00b7reit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ADV", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.121": {"line.1": {"text": "So klopft der Hunger an den Magen,", "tokens": ["So", "klopft", "der", "Hun\u00b7ger", "an", "den", "Ma\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und gar zu gern t\u00e4t man's ertragen.", "tokens": ["Und", "gar", "zu", "gern", "t\u00e4t", "man's", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKA", "ADV", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.122": {"line.1": {"text": "Wir fanden noch in einer Weste", "tokens": ["Wir", "fan\u00b7den", "noch", "in", "ei\u00b7ner", "Wes\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "An Geld kupferne \u00dcberreste", "tokens": ["An", "Geld", "kup\u00b7fer\u00b7ne", "\u00dc\u00b7ber\u00b7res\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.123": {"line.1": {"text": "Und kauften etwas Spiritus,", "tokens": ["Und", "kauf\u00b7ten", "et\u00b7was", "Spi\u00b7ri\u00b7tus", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kochten uns ein St\u00e4rkemus,", "tokens": ["Und", "koch\u00b7ten", "uns", "ein", "St\u00e4r\u00b7ke\u00b7mus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.124": {"line.1": {"text": "Gef\u00e4rbt mit wenig Schokolade,", "tokens": ["Ge\u00b7f\u00e4rbt", "mit", "we\u00b7nig", "Scho\u00b7ko\u00b7la\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann schmeckt die St\u00e4rke nicht zu fade.", "tokens": ["Dann", "schmeckt", "die", "St\u00e4r\u00b7ke", "nicht", "zu", "fa\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PTKNEG", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.125": {"line.1": {"text": "Wir r\u00fchrten in der Kasseroll", "tokens": ["Wir", "r\u00fchr\u00b7ten", "in", "der", "Kas\u00b7se\u00b7roll"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wu\u00dften nicht, was werden soll.", "tokens": ["Und", "wu\u00df\u00b7ten", "nicht", ",", "was", "wer\u00b7den", "soll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "PRELS", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.126": {"line.1": {"text": "Und sonderbar, sie hatte recht,", "tokens": ["Und", "son\u00b7der\u00b7bar", ",", "sie", "hat\u00b7te", "recht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Hunger schmeckte nicht so schlecht,", "tokens": ["Der", "Hun\u00b7ger", "schmeck\u00b7te", "nicht", "so", "schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.127": {"line.1": {"text": "Und vornehm taten wir ihn tragen", "tokens": ["Und", "vor\u00b7nehm", "ta\u00b7ten", "wir", "ihn", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Geadelt von dem leeren Magen.", "tokens": ["Ge\u00b7a\u00b7delt", "von", "dem", "lee\u00b7ren", "Ma\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.128": {"line.1": {"text": "Doch gehen nachts die Sterne auf,", "tokens": ["Doch", "ge\u00b7hen", "nachts", "die", "Ster\u00b7ne", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So sieht man meistens mal hinauf.", "tokens": ["So", "sieht", "man", "meis\u00b7tens", "mal", "hin\u00b7auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.129": {"line.1": {"text": "In der Nacht machen sie mich toll,", "tokens": ["In", "der", "Nacht", "ma\u00b7chen", "sie", "mich", "toll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPER", "PRF", "ADJD", "$,"], "meter": "--++-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Wu\u00dft' noch nicht, wie's uns werden soll.", "tokens": ["Wu\u00dft'", "noch", "nicht", ",", "wie's", "uns", "wer\u00b7den", "soll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKNEG", "$,", "KOUS", "PPER", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.130": {"line.1": {"text": "\u00bbihr Weltst\u00fccke dort in der Nacht,", "tokens": ["\u00bb", "ihr", "Welt\u00b7st\u00fc\u00b7cke", "dort", "in", "der", "Nacht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Sagt mir, warum bin ich gemacht?", "tokens": ["Sagt", "mir", ",", "wa\u00b7rum", "bin", "ich", "ge\u00b7macht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.131": {"line.1": {"text": "Ich tue meinen Willen sp\u00fcren,", "tokens": ["Ich", "tue", "mei\u00b7nen", "Wil\u00b7len", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und trotzdem tut man mich regieren.", "tokens": ["Und", "trotz\u00b7dem", "tut", "man", "mich", "re\u00b7gie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PIS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.132": {"line.1": {"text": "Hat einer Recht, mich zu bezwingen,", "tokens": ["Hat", "ei\u00b7ner", "Recht", ",", "mich", "zu", "be\u00b7zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Geb\u00e4ren mich und umzubringen?", "tokens": ["Ge\u00b7b\u00e4\u00b7ren", "mich", "und", "um\u00b7zu\u00b7brin\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "KON", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.133": {"line.1": {"text": "Mein Weib soll mich unfehlbar sehn,", "tokens": ["Mein", "Weib", "soll", "mich", "un\u00b7fehl\u00b7bar", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich w\u00fcnsche mich sonst ungeschehn.", "tokens": ["Ich", "w\u00fcn\u00b7sche", "mich", "sonst", "un\u00b7ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.134": {"line.1": {"text": "Liebe ist Herr, hat alles Recht,", "tokens": ["Lie\u00b7be", "ist", "Herr", ",", "hat", "al\u00b7les", "Recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "NN", "$,", "VAFIN", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Nur ungeliebt ist man ein Knecht.", "tokens": ["Nur", "un\u00b7ge\u00b7liebt", "ist", "man", "ein", "Knecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.135": {"line.1": {"text": "Es reizt mich gar nicht, mich zu t\u00f6ten,", "tokens": ["Es", "reizt", "mich", "gar", "nicht", ",", "mich", "zu", "t\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das rettet nicht vor Zukunftsn\u00f6te.", "tokens": ["Das", "ret\u00b7tet", "nicht", "vor", "Zu\u00b7kunfts\u00b7n\u00f6\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.136": {"line.1": {"text": "Ein Herr bin ich und bin ein Mann,", "tokens": ["Ein", "Herr", "bin", "ich", "und", "bin", "ein", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "KON", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der keinen Zwang mehr dulden kann,", "tokens": ["Der", "kei\u00b7nen", "Zwang", "mehr", "dul\u00b7den", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.137": {"line.1": {"text": "Mein ganzes Leben sei vergessen,", "tokens": ["Mein", "gan\u00b7zes", "Le\u00b7ben", "sei", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' ich nicht morgen was zu essen.", "tokens": ["Hab'", "ich", "nicht", "mor\u00b7gen", "was", "zu", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.138": {"line.1": {"text": "Nacht, undurchdringliche Pupille,", "tokens": ["Nacht", ",", "un\u00b7durch\u00b7dring\u00b7li\u00b7che", "Pu\u00b7pil\u00b7le", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mein Fedehandschuh sei mein Wille!\u00ab", "tokens": ["Mein", "Fe\u00b7de\u00b7hand\u00b7schuh", "sei", "mein", "Wil\u00b7le", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.139": {"line.1": {"text": "Und sonderbar, in dieser Nacht", "tokens": ["Und", "son\u00b7der\u00b7bar", ",", "in", "die\u00b7ser", "Nacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bin z\u00e4hneklappernd ich erwacht.", "tokens": ["Bin", "z\u00e4h\u00b7ne\u00b7klap\u00b7pernd", "ich", "er\u00b7wacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.140": {"line.1": {"text": "Unheimlich war ein Traum gekommen,", "tokens": ["Un\u00b7heim\u00b7lich", "war", "ein", "Traum", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat meinen K\u00f6rper mir genommen.", "tokens": ["Hat", "mei\u00b7nen", "K\u00f6r\u00b7per", "mir", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.141": {"line.1": {"text": "Es war in einem hohen Haus,", "tokens": ["Es", "war", "in", "ei\u00b7nem", "ho\u00b7hen", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Ganze sah verlassen aus,", "tokens": ["Das", "Gan\u00b7ze", "sah", "ver\u00b7las\u00b7sen", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.142": {"line.1": {"text": "Der letzte Mensch kam an die T\u00fcr", "tokens": ["Der", "letz\u00b7te", "Mensch", "kam", "an", "die", "T\u00fcr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00df mich dann allein mit mir.", "tokens": ["Und", "lie\u00df", "mich", "dann", "al\u00b7lein", "mit", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.143": {"line.1": {"text": "In mir war selbst nichts mehr zu lesen,", "tokens": ["In", "mir", "war", "selbst", "nichts", "mehr", "zu", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "ADV", "PIS", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn nichts an mir schien je gewesen,", "tokens": ["Denn", "nichts", "an", "mir", "schien", "je", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "PPER", "VVFIN", "ADV", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.144": {"line.1": {"text": "Hatte nur furchtbar viele Zeit,", "tokens": ["Hat\u00b7te", "nur", "furcht\u00b7bar", "vie\u00b7le", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "F\u00fchlte, \u2013 dies war die Ewigkeit.", "tokens": ["F\u00fchl\u00b7te", ",", "\u2013", "dies", "war", "die", "E\u00b7wig\u00b7keit", "."], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "$(", "PDS", "VAFIN", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.145": {"line.1": {"text": "Zwar wu\u00dft' ich noch nicht, was ich soll,", "tokens": ["Zwar", "wu\u00dft'", "ich", "noch", "nicht", ",", "was", "ich", "soll", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch Hunger z\u00e4hmte Zoll um Zoll.", "tokens": ["Doch", "Hun\u00b7ger", "z\u00e4hm\u00b7te", "Zoll", "um", "Zoll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.146": {"line.1": {"text": "Und als der n\u00e4chste Abend kam,", "tokens": ["Und", "als", "der", "n\u00e4chs\u00b7te", "A\u00b7bend", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich anst\u00e4ndiger mich benahm.", "tokens": ["Ich", "an\u00b7st\u00e4n\u00b7di\u00b7ger", "mich", "be\u00b7nahm", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.147": {"line.1": {"text": "Ich bat: \u00bbIhr Sternenungeheuer,", "tokens": ["Ich", "bat", ":", "\u00bb", "Ihr", "Ster\u00b7ne\u00b7nun\u00b7ge\u00b7heu\u00b7er", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gold, wei\u00df ich, schwimmt in eurem Feuer.", "tokens": ["Gold", ",", "wei\u00df", "ich", ",", "schwimmt", "in", "eu\u00b7rem", "Feu\u00b7er", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "PPER", "$,", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.148": {"line.1": {"text": "Nur eine Handvoll m\u00f6gt ihr geben,", "tokens": ["Nur", "ei\u00b7ne", "Hand\u00b7voll", "m\u00f6gt", "ihr", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vorl\u00e4ufig h\u00e4tt' ich dann zum Leben.", "tokens": ["Vor\u00b7l\u00e4u\u00b7fig", "h\u00e4tt'", "ich", "dann", "zum", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.149": {"line.1": {"text": "Ihr Br\u00fcder, habt ihr mich vergessen?", "tokens": ["Ihr", "Br\u00fc\u00b7der", ",", "habt", "ihr", "mich", "ver\u00b7ges\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VAFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich liebe und hab' nichts zu essen.\u00ab", "tokens": ["Ich", "lie\u00b7be", "und", "hab'", "nichts", "zu", "es\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "KON", "VAFIN", "PIS", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.150": {"line.1": {"text": "Da endlich mich die Antwort traf,", "tokens": ["Da", "end\u00b7lich", "mich", "die", "Ant\u00b7wort", "traf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ins Ohr sprach jemand mir im Schlaf:", "tokens": ["Ins", "Ohr", "sprach", "je\u00b7mand", "mir", "im", "Schlaf", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.151": {"line.1": {"text": "\u00bbmein Junge, du wirst noch nicht sterben,", "tokens": ["\u00bb", "mein", "Jun\u00b7ge", ",", "du", "wirst", "noch", "nicht", "ster\u00b7ben", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PPER", "VAFIN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dein Vater stirbt, und du wirst erben.\u00ab", "tokens": ["Dein", "Va\u00b7ter", "stirbt", ",", "und", "du", "wirst", "er\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "KON", "PPER", "VAFIN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.152": {"line.1": {"text": "Der Traum, der hat mich aufgeschreckt,", "tokens": ["Der", "Traum", ",", "der", "hat", "mich", "auf\u00b7ge\u00b7schreckt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau K\u00f6nigin hab' ich geweckt,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "hab'", "ich", "ge\u00b7weckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.153": {"line.1": {"text": "Erz\u00e4hlte ihr, wie alles war,", "tokens": ["Er\u00b7z\u00e4hl\u00b7te", "ihr", ",", "wie", "al\u00b7les", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch sie findet nichts sonderbar.", "tokens": ["Doch", "sie", "fin\u00b7det", "nichts", "son\u00b7der\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIS", "ADJD", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.154": {"line.1": {"text": "Sprach: \u00bbDa\u00df der Himmel Botschaft sendet,", "tokens": ["Sprach", ":", "\u00bb", "Da\u00df", "der", "Him\u00b7mel", "Bot\u00b7schaft", "sen\u00b7det", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "KOUS", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist gut, denn wir sind ausgepf\u00e4ndet.", "tokens": ["Ist", "gut", ",", "denn", "wir", "sind", "aus\u00b7ge\u00b7pf\u00e4n\u00b7det", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "KON", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.155": {"line.1": {"text": "Zwar, da\u00df der Vater stirbt, tut weh,", "tokens": ["Zwar", ",", "da\u00df", "der", "Va\u00b7ter", "stirbt", ",", "tut", "weh", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "VVFIN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch da ich keine Rettung seh',", "tokens": ["Doch", "da", "ich", "kei\u00b7ne", "Ret\u00b7tung", "seh'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.156": {"line.1": {"text": "Und da es unser Schicksal will,", "tokens": ["Und", "da", "es", "un\u00b7ser", "Schick\u00b7sal", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPOSAT", "NN", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So erben wir und trauern still.", "tokens": ["So", "er\u00b7ben", "wir", "und", "trau\u00b7ern", "still", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.157": {"line.1": {"text": "Doch ist der Traum dir nur gelogen,", "tokens": ["Doch", "ist", "der", "Traum", "dir", "nur", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat um die Ruh' er mich betrogen:", "tokens": ["Hat", "um", "die", "Ruh'", "er", "mich", "be\u00b7tro\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.158": {"line.1": {"text": "Just bin im Traum ich satt gewesen", "tokens": ["Just", "bin", "im", "Traum", "ich", "satt", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "APPRART", "NN", "PPER", "ADJD", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und hatte wundervoll gegessen.\u00ab", "tokens": ["Und", "hat\u00b7te", "wun\u00b7der\u00b7voll", "ge\u00b7ges\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADJD", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.159": {"line.1": {"text": "Am Morgen sprach ich: \u00bbDenke kaum", "tokens": ["Am", "Mor\u00b7gen", "sprach", "ich", ":", "\u00bb", "Den\u00b7ke", "kaum"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "$.", "$(", "VVIMP", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An diesen b\u00f6sen Erbschaftstraum.", "tokens": ["An", "die\u00b7sen", "b\u00f6\u00b7sen", "Erb\u00b7schafts\u00b7traum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.160": {"line.1": {"text": "Es ist ein Frevel, so zu denken.", "tokens": ["Es", "ist", "ein", "Fre\u00b7vel", ",", "so", "zu", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Will nur dem Leben Glauben schenken.", "tokens": ["Will", "nur", "dem", "Le\u00b7ben", "Glau\u00b7ben", "schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.161": {"line.1": {"text": "Sieh, immer fand ich wunderbar", "tokens": ["Sieh", ",", "im\u00b7mer", "fand", "ich", "wun\u00b7der\u00b7bar"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$,", "ADV", "VVFIN", "PPER", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Seidenzeug, das aus Japan war,", "tokens": ["Sei\u00b7den\u00b7zeug", ",", "das", "aus", "Ja\u00b7pan", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NE", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.162": {"line.1": {"text": "Es war mir bunte Augenweide,", "tokens": ["Es", "war", "mir", "bun\u00b7te", "Au\u00b7gen\u00b7wei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die ganze Welt scheint dort aus Seide.", "tokens": ["Die", "gan\u00b7ze", "Welt", "scheint", "dort", "aus", "Sei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.163": {"line.1": {"text": "Und Japan fiel mir heute ein,", "tokens": ["Und", "Ja\u00b7pan", "fiel", "mir", "heu\u00b7te", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und jetzt soll uns geholfen sein.", "tokens": ["Und", "jetzt", "soll", "uns", "ge\u00b7hol\u00b7fen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.164": {"line.1": {"text": "Ich zeichne Bilder auf die Seide,", "tokens": ["Ich", "zeich\u00b7ne", "Bil\u00b7der", "auf", "die", "Sei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und du stickst sie zur Augenweide.", "tokens": ["Und", "du", "stickst", "sie", "zur", "Au\u00b7gen\u00b7wei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.165": {"line.1": {"text": "Damit werden wir Geld verdienen", "tokens": ["Da\u00b7mit", "wer\u00b7den", "wir", "Geld", "ver\u00b7die\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "NN", "VVINF"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und legen ab die Hungermienen.\u00ab", "tokens": ["Und", "le\u00b7gen", "ab", "die", "Hun\u00b7ger\u00b7mie\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.166": {"line.1": {"text": "\u00bbach nein,\u00ab sagte Frau K\u00f6nigin,", "tokens": ["\u00bb", "ach", "nein", ",", "\u00ab", "sag\u00b7te", "Frau", "K\u00f6\u00b7ni\u00b7gin", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "XY", "PTKANT", "$,", "$(", "VVFIN", "NN", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "\u00bbdas Sticken, das verdreht den Sinn.", "tokens": ["\u00bb", "das", "Sti\u00b7cken", ",", "das", "ver\u00b7dreht", "den", "Sinn", "."], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "PDS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.167": {"line.1": {"text": "Doch, wenn ich h\u00f6re meine Stimme,", "tokens": ["Doch", ",", "wenn", "ich", "h\u00f6\u00b7re", "mei\u00b7ne", "Stim\u00b7me", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist's, als ob ich im Himmel schwimme.", "tokens": ["Ist's", ",", "als", "ob", "ich", "im", "Him\u00b7mel", "schwim\u00b7me", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOKOM", "KOUS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.168": {"line.1": {"text": "Die Stimme, sie ist zwar noch klein,", "tokens": ["Die", "Stim\u00b7me", ",", "sie", "ist", "zwar", "noch", "klein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VAFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Und deshalb \u00fcb' ich sie erst ein.", "tokens": ["Und", "des\u00b7halb", "\u00fcb'", "ich", "sie", "erst", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.169": {"line.1": {"text": "Dann singe ich auf allen Stra\u00dfen,", "tokens": ["Dann", "sin\u00b7ge", "ich", "auf", "al\u00b7len", "Stra\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Geld sind wir dann nie verlassen.\u00ab", "tokens": ["Von", "Geld", "sind", "wir", "dann", "nie", "ver\u00b7las\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.170": {"line.1": {"text": "\u00bbja,\u00ab sprach ich, \u00bb\u00fcbe dich nur ein,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sprach", "ich", ",", "\u00bb", "\u00fc\u00b7be", "dich", "nur", "ein", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und la\u00df das Sticken mir allein.", "tokens": ["Und", "la\u00df", "das", "Sti\u00b7cken", "mir", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "ART", "NN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.171": {"line.1": {"text": "Sorgen, sie hindern mich am Schnaufen,", "tokens": ["Sor\u00b7gen", ",", "sie", "hin\u00b7dern", "mich", "am", "Schnau\u00b7fen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wer gibt uns Geld zum Seidekaufen?", "tokens": ["Wer", "gibt", "uns", "Geld", "zum", "Sei\u00b7de\u00b7kau\u00b7fen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.172": {"line.1": {"text": "Ja, Seide braucht man, das ist wahr.\u00ab", "tokens": ["Ja", ",", "Sei\u00b7de", "braucht", "man", ",", "das", "ist", "wahr", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PTKANT", "$,", "NE", "VVFIN", "PIS", "$,", "PDS", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Anbetend sah ich auf ihr Haar.", "tokens": ["An\u00b7be\u00b7tend", "sah", "ich", "auf", "ihr", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.173": {"line.1": {"text": "Sie scherzte: \u00bbWenn ich 's Haar abschneide,", "tokens": ["Sie", "scherz\u00b7te", ":", "\u00bb", "Wenn", "ich", "'s", "Haar", "ab\u00b7schnei\u00b7de", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "KOUS", "PPER", "PPER", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dann brauchen wir kein Geld zur Seide.\u00ab", "tokens": ["Dann", "brau\u00b7chen", "wir", "kein", "Geld", "zur", "Sei\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.174": {"line.1": {"text": "Da grollte ich dumpf wie ein B\u00e4r:", "tokens": ["Da", "groll\u00b7te", "ich", "dumpf", "wie", "ein", "B\u00e4r", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "\u00bbvon deinem Haar geb' ich nichts her,", "tokens": ["\u00bb", "von", "dei\u00b7nem", "Haar", "geb'", "ich", "nichts", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "PPOSAT", "NN", "VVFIN", "PPER", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.175": {"line.1": {"text": "Das w\u00e4r', als ob ich dich verkaufe.", "tokens": ["Das", "w\u00e4r'", ",", "als", "ob", "ich", "dich", "ver\u00b7kau\u00b7fe", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "$,", "KOKOM", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ach, da\u00df ich gleich mein Herz ausraufe.\u00ab", "tokens": ["Ach", ",", "da\u00df", "ich", "gleich", "mein", "Herz", "aus\u00b7rau\u00b7fe", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ITJ", "$,", "KOUS", "PPER", "ADV", "PPOSAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.176": {"line.1": {"text": "Doch lie\u00df sie nicht ihr Scherzen sein", "tokens": ["Doch", "lie\u00df", "sie", "nicht", "ihr", "Scher\u00b7zen", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "PPOSAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00e4delte ein Haar selbst ein,", "tokens": ["Und", "f\u00e4\u00b7del\u00b7te", "ein", "Haar", "selbst", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.177": {"line.1": {"text": "Dr\u00fcckt mir die Nadel in die Hand;", "tokens": ["Dr\u00fcckt", "mir", "die", "Na\u00b7del", "in", "die", "Hand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hab' mich d\u00fcster abgewandt.", "tokens": ["Ich", "hab'", "mich", "d\u00fcs\u00b7ter", "ab\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.178": {"line.1": {"text": "Ich setzte mich ans Fenster hin,", "tokens": ["Ich", "setz\u00b7te", "mich", "ans", "Fens\u00b7ter", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und drunten ging Frau K\u00f6nigin", "tokens": ["Und", "drun\u00b7ten", "ging", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.179": {"line.1": {"text": "Am frommen Kleefeld auf und nieder", "tokens": ["Am", "from\u00b7men", "Klee\u00b7feld", "auf", "und", "nie\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "PTKVZ", "KON", "PTKVZ"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und dehnte im Gesang ihr Mieder.", "tokens": ["Und", "dehn\u00b7te", "im", "Ge\u00b7sang", "ihr", "Mie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.180": {"line.1": {"text": "Da wurde mir so wohl im Blut,", "tokens": ["Da", "wur\u00b7de", "mir", "so", "wohl", "im", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fchlte mich wie der Klee so gut,", "tokens": ["F\u00fchl\u00b7te", "mich", "wie", "der", "Klee", "so", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.181": {"line.1": {"text": "F\u00fchlte mich Staub und Meeressand,", "tokens": ["F\u00fchl\u00b7te", "mich", "Staub", "und", "Mee\u00b7res\u00b7sand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Stach mir die Nadel in die Hand,", "tokens": ["Stach", "mir", "die", "Na\u00b7del", "in", "die", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.182": {"line.1": {"text": "Hing an ihr Haar ein Tr\u00f6pflein Blut,", "tokens": ["Hing", "an", "ihr", "Haar", "ein", "Tr\u00f6pf\u00b7lein", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' nie so nah bei ihr geruht.", "tokens": ["Hab'", "nie", "so", "nah", "bei", "ihr", "ge\u00b7ruht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "APPR", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.183": {"line.1": {"text": "Ihr Stimmlein tat mich selig heben,", "tokens": ["Ihr", "Stimm\u00b7lein", "tat", "mich", "se\u00b7lig", "he\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tat \u00fcber allen Hungern schweben.", "tokens": ["Tat", "\u00fc\u00b7ber", "al\u00b7len", "Hun\u00b7gern", "schwe\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.184": {"line.1": {"text": "Doch wei\u00df ich nicht, wie es dann kam,", "tokens": ["Doch", "wei\u00df", "ich", "nicht", ",", "wie", "es", "dann", "kam", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "$,", "PWAV", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich die Hand zur Nase nahm,", "tokens": ["Da\u00df", "ich", "die", "Hand", "zur", "Na\u00b7se", "nahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.185": {"line.1": {"text": "Sie roch wie Zigarettenrauch,", "tokens": ["Sie", "roch", "wie", "Zi\u00b7ga\u00b7ret\u00b7ten\u00b7rauch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ganz so roch stets mein Vater auch.", "tokens": ["Ganz", "so", "roch", "stets", "mein", "Va\u00b7ter", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "PPOSAT", "NN", "ADV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.186": {"line.1": {"text": "Dreimal wusch ich mir beide H\u00e4nde,", "tokens": ["Drei\u00b7mal", "wusch", "ich", "mir", "bei\u00b7de", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PIAT", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und immer war's, als ob ich f\u00e4nde", "tokens": ["Und", "im\u00b7mer", "wa\u00b7r's", ",", "als", "ob", "ich", "f\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "$,", "KOKOM", "KOUS", "PPER", "VVFIN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.187": {"line.1": {"text": "Des Vaters Atem nahe hier,", "tokens": ["Des", "Va\u00b7ters", "A\u00b7tem", "na\u00b7he", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADJD", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ganz unheimlich war das mir.", "tokens": ["Und", "ganz", "un\u00b7heim\u00b7lich", "war", "das", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VAFIN", "ART", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.188": {"line.1": {"text": "Und ehe noch der Abend kam,", "tokens": ["Und", "e\u00b7he", "noch", "der", "A\u00b7bend", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erhielt ich kurz ein Telegramm.", "tokens": ["Er\u00b7hielt", "ich", "kurz", "ein", "Te\u00b7le\u00b7gramm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.189": {"line.1": {"text": "Ich konnt' es nicht vor Tr\u00e4nen lesen:", "tokens": ["Ich", "konnt'", "es", "nicht", "vor", "Tr\u00e4\u00b7nen", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der ernste Traum ist wahr gewesen.", "tokens": ["Der", "erns\u00b7te", "Traum", "ist", "wahr", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.190": {"line.1": {"text": "Doch dieses Trauertelegramm", "tokens": ["Doch", "die\u00b7ses", "Trau\u00b7er\u00b7te\u00b7le\u00b7gramm"], "token_info": ["word", "word", "word"], "pos": ["KON", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erweckt auf meinen Wangen Scham.", "tokens": ["Er\u00b7weckt", "auf", "mei\u00b7nen", "Wan\u00b7gen", "Scham", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.191": {"line.1": {"text": "Ich nahm es zu dem Kaufmann mit,", "tokens": ["Ich", "nahm", "es", "zu", "dem", "Kauf\u00b7mann", "mit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und er gab uns sofort Kredit,", "tokens": ["Und", "er", "gab", "uns", "so\u00b7fort", "Kre\u00b7dit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.192": {"line.1": {"text": "Und alles ward uns reich bemessen,", "tokens": ["Und", "al\u00b7les", "ward", "uns", "reich", "be\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Haben uns weinend satt gegessen,", "tokens": ["Ha\u00b7ben", "uns", "wei\u00b7nend", "satt", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.193": {"line.1": {"text": "Wir konnten uns nicht selbst betr\u00fcgen,", "tokens": ["Wir", "konn\u00b7ten", "uns", "nicht", "selbst", "be\u00b7tr\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir a\u00dfen beinah mit Vergn\u00fcgen.", "tokens": ["Wir", "a\u00b7\u00dfen", "bei\u00b7nah", "mit", "Ver\u00b7gn\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.194": {"line.1": {"text": "Ich fand uns da im Grund nicht besser", "tokens": ["Ich", "fand", "uns", "da", "im", "Grund", "nicht", "bes\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKNEG", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als zwei bewu\u00dfte Menschenfresser.", "tokens": ["Als", "zwei", "be\u00b7wu\u00df\u00b7te", "Men\u00b7schen\u00b7fres\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.195": {"line.1": {"text": "Auch stolz machte mich ganz und gar,", "tokens": ["Auch", "stolz", "mach\u00b7te", "mich", "ganz", "und", "gar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "ADV", "KON", "ADV", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da\u00df ich so auserw\u00e4hlet war,", "tokens": ["Da\u00df", "ich", "so", "au\u00b7ser\u00b7w\u00e4h\u00b7let", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.196": {"line.1": {"text": "Da\u00df Gott nachts selbst zu mir gesprochen", "tokens": ["Da\u00df", "Gott", "nachts", "selbst", "zu", "mir", "ge\u00b7spro\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "ADV", "ADV", "APPR", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sein Inkognito gebrochen. \u2013", "tokens": ["Und", "sein", "In\u00b7kog\u00b7ni\u00b7to", "ge\u00b7bro\u00b7chen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.197": {"line.1": {"text": "Wenn man im Grab wen kennen lernt,", "tokens": ["Wenn", "man", "im", "Grab", "wen", "ken\u00b7nen", "lernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVINF", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist's schlimm, verwandt oder entfernt.", "tokens": ["Ist's", "schlimm", ",", "ver\u00b7wandt", "o\u00b7der", "ent\u00b7fernt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "$,", "VVPP", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.198": {"line.1": {"text": "Frau K\u00f6nigin tat es so gehn,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "tat", "es", "so", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als sie am frischen Grab tat stehn,", "tokens": ["Als", "sie", "am", "fri\u00b7schen", "Grab", "tat", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADJA", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.199": {"line.1": {"text": "Sie sprach: \u00bbIch glaube nicht daran,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Ich", "glau\u00b7be", "nicht", "da\u00b7ran", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PTKNEG", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man im Grabe sterben kann.", "tokens": ["Da\u00df", "man", "im", "Gra\u00b7be", "ster\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.200": {"line.1": {"text": "Menschen, die einem vieles schenken,", "tokens": ["Men\u00b7schen", ",", "die", "ei\u00b7nem", "vie\u00b7les", "schen\u00b7ken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ART", "PIS", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kann man sich gar nicht sterben denken.\u00ab", "tokens": ["Kann", "man", "sich", "gar", "nicht", "ster\u00b7ben", "den\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PIS", "PRF", "ADV", "PTKNEG", "VVINF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.201": {"line.1": {"text": "\u00bbja,\u00ab sagte ich, \u00bblebte er weiter,", "tokens": ["\u00bb", "ja", ",", "\u00ab", "sag\u00b7te", "ich", ",", "\u00bb", "leb\u00b7te", "er", "wei\u00b7ter", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "So spr\u00e4ch er jetzt: \u203aKinder, seid heiter,", "tokens": ["So", "spr\u00e4ch", "er", "jetzt", ":", "\u203a", "Kin\u00b7der", ",", "seid", "hei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$.", "$(", "NN", "$,", "VAFIN", "ADJD", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.202": {"line.1": {"text": "Ein jeder wird es mal allm\u00e4hlich,", "tokens": ["Ein", "je\u00b7der", "wird", "es", "mal", "all\u00b7m\u00e4h\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und lebend ist man wirklich selig.", "tokens": ["Und", "le\u00b7bend", "ist", "man", "wirk\u00b7lich", "se\u00b7lig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PIS", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.203": {"line.1": {"text": "Steht euch die Welt jetzt auf der H\u00f6he,", "tokens": ["Steht", "euch", "die", "Welt", "jetzt", "auf", "der", "H\u00f6\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei\u00dfen respektvoller die Fl\u00f6he;", "tokens": ["Bei\u00b7\u00dfen", "res\u00b7pekt\u00b7vol\u00b7ler", "die", "Fl\u00f6\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.204": {"line.1": {"text": "Die Liebe ist nicht blind erfunden,", "tokens": ["Die", "Lie\u00b7be", "ist", "nicht", "blind", "er\u00b7fun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Haltet euch an die Liebesstunden.", "tokens": ["Hal\u00b7tet", "euch", "an", "die", "Lie\u00b7bess\u00b7tun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.205": {"line.1": {"text": "Erlebt stets die Realit\u00e4t,", "tokens": ["Er\u00b7lebt", "stets", "die", "Re\u00b7a\u00b7li\u00b7t\u00e4t", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hei\u00df wie der Topf am Feuer steht.", "tokens": ["Hei\u00df", "wie", "der", "Topf", "am", "Feu\u00b7er", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.206": {"line.1": {"text": "Und jetzt sollt ihr Siesta halten,", "tokens": ["Und", "jetzt", "sollt", "ihr", "Sies\u00b7ta", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Und legt die Stirn in keine Falten,", "tokens": ["Und", "legt", "die", "Stirn", "in", "kei\u00b7ne", "Fal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.207": {"line.1": {"text": "F\u00fcrs Leben es euch zw\u00f6lf Uhr schlug,", "tokens": ["F\u00fcrs", "Le\u00b7ben", "es", "euch", "zw\u00f6lf", "Uhr", "schlug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "PPER", "PPER", "CARD", "NN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Habt jetzt vom Vormittag genug,", "tokens": ["Habt", "jetzt", "vom", "Vor\u00b7mit\u00b7tag", "ge\u00b7nug", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.208": {"line.1": {"text": "Und dehnt die Liebe auch noch aus,", "tokens": ["Und", "dehnt", "die", "Lie\u00b7be", "auch", "noch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geht die Siesta mal hinaus.\u2039\u00ab", "tokens": ["Geht", "die", "Sies\u00b7ta", "mal", "hin\u00b7aus", ".", "\u2039", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$(", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}