{"dta.poem.19684": {"metadata": {"author": {"name": "Brentano, Clemens", "birth": "N.A.", "death": "N.A."}, "title": "1.  Schlacht bey Leipzig .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1808", "urn": "urn:nbn:de:kobv:b4-20090519168", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Ich hab den Schweden mit Augen gesehn,               ", "tokens": ["Ich", "hab", "den", "Schwe\u00b7den", "mit", "Au\u00b7gen", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NE", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er thut mir wohlgefallen,", "tokens": ["Er", "thut", "mir", "wohl\u00b7ge\u00b7fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Geliebt mir in dem Herzen mein,", "tokens": ["Ge\u00b7liebt", "mir", "in", "dem", "Her\u00b7zen", "mein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "PPOSAT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vor andern K\u00f6nigen allen.", "tokens": ["Vor", "an\u00b7dern", "K\u00f6\u00b7ni\u00b7gen", "al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PIAT", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Er hat der sch\u00f6nen Reiter soviel,", "tokens": ["Er", "hat", "der", "sch\u00f6\u00b7nen", "Rei\u00b7ter", "so\u00b7viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "L\u00e4st sich nicht lang vexieren,", "tokens": ["L\u00e4st", "sich", "nicht", "lang", "ve\u00b7xie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Er hat der sch\u00f6nen St\u00fcck so viel,", "tokens": ["Er", "hat", "der", "sch\u00f6\u00b7nen", "St\u00fcck", "so", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Viel tausend Musketierer.", "tokens": ["Viel", "tau\u00b7send", "Mus\u00b7ke\u00b7tie\u00b7rer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Das Frankenland ist ein sch\u00f6nes Land,", "tokens": ["Das", "Fran\u00b7ken\u00b7land", "ist", "ein", "sch\u00f6\u00b7nes", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Es hat viel sch\u00f6ne Strassen,", "tokens": ["Es", "hat", "viel", "sch\u00f6\u00b7ne", "Stras\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es hat so mancher brave Soldat,", "tokens": ["Es", "hat", "so", "man\u00b7cher", "bra\u00b7ve", "Sol\u00b7dat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Sein junges Leben gelassen.", "tokens": ["Sein", "jun\u00b7ges", "Le\u00b7ben", "ge\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Das Sachsenland ist ein einiges Land,", "tokens": ["Das", "Sach\u00b7sen\u00b7land", "ist", "ein", "ei\u00b7ni\u00b7ges", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Es dienet Gott dem Herren,", "tokens": ["Es", "die\u00b7net", "Gott", "dem", "Her\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und wenn wir kommen ins Bayerland,", "tokens": ["Und", "wenn", "wir", "kom\u00b7men", "ins", "Bay\u00b7er\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Frey tapfer wollen wir uns wehren.", "tokens": ["Frey", "tap\u00b7fer", "wol\u00b7len", "wir", "uns", "weh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Der Oberst Baudi\u00df beym Schweden thut seyn,", "tokens": ["Der", "O\u00b7berst", "Bau\u00b7di\u00df", "beym", "Schwe\u00b7den", "thut", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPRART", "NE", "VVFIN", "VAINF", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und thut sich tapfer halten,", "tokens": ["Und", "thut", "sich", "tap\u00b7fer", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ist unverzagt mit dem Pappenheim", "tokens": ["Ist", "un\u00b7ver\u00b7zagt", "mit", "dem", "Pap\u00b7pen\u00b7heim"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ein Schlacht, zwey, drey zu halten.", "tokens": ["Ein", "Schlacht", ",", "zwey", ",", "drey", "zu", "hal\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "CARD", "$,", "CARD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Der Tilly hat ein Garn gespannt,", "tokens": ["Der", "Til\u00b7ly", "hat", "ein", "Garn", "ge\u00b7spannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es wird ihm bald zerreissen,", "tokens": ["Es", "wird", "ihm", "bald", "zer\u00b7reis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Schwede ist bekannt im Land,", "tokens": ["Der", "Schwe\u00b7de", "ist", "be\u00b7kannt", "im", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wohl in dem Lande Meissen.", "tokens": ["Wohl", "in", "dem", "Lan\u00b7de", "Meis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "NE", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Mit ihren Karthaunen und St\u00fccken gro\u00df,", "tokens": ["Mit", "ih\u00b7ren", "Kar\u00b7thau\u00b7nen", "und", "St\u00fc\u00b7cken", "gro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "NN", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "So tapfer thun unter sie krachen,", "tokens": ["So", "tap\u00b7fer", "thun", "un\u00b7ter", "sie", "kra\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVINF", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und geben dem Garn so manchen Sto\u00df,", "tokens": ["Und", "ge\u00b7ben", "dem", "Garn", "so", "man\u00b7chen", "Sto\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PIAT", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da\u00df alle F\u00e4den brachen.", "tokens": ["Da\u00df", "al\u00b7le", "F\u00e4\u00b7den", "bra\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Der Tilly ins Land zu Meissen zog,", "tokens": ["Der", "Til\u00b7ly", "ins", "Land", "zu", "Meis\u00b7sen", "zog", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "APPRART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er freut sich sehr von Herzen,", "tokens": ["Er", "freut", "sich", "sehr", "von", "Her\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und wie er wieder weichen mu\u00df,", "tokens": ["Und", "wie", "er", "wie\u00b7der", "wei\u00b7chen", "mu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Th\u00e4t er sich sehr entsetzen.", "tokens": ["Th\u00e4t", "er", "sich", "sehr", "ent\u00b7set\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.9": {"line.1": {"text": "Nun wei\u00df ich noch ein Cavallier", "tokens": ["Nun", "wei\u00df", "ich", "noch", "ein", "Ca\u00b7val\u00b7lier"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der wird genannt der Holle,", "tokens": ["Der", "wird", "ge\u00b7nannt", "der", "Hol\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "VVPP", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vom spanschen Wein und Malvasier", "tokens": ["Vom", "span\u00b7schen", "Wein", "und", "Mal\u00b7va\u00b7sier"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da kriegte er die Kolke.", "tokens": ["Da", "krieg\u00b7te", "er", "die", "Kol\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Das Confeckt wohl vergiftet war,", "tokens": ["Das", "Con\u00b7feckt", "wohl", "ver\u00b7gif\u00b7tet", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich thus mit Wahrheit sagen,", "tokens": ["Ich", "thus", "mit", "Wahr\u00b7heit", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Schwed dem Tilly schor den Bart,", "tokens": ["Der", "Schwed", "dem", "Til\u00b7ly", "schor", "den", "Bart", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NE", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und aus dem Land thut jagen.", "tokens": ["Und", "aus", "dem", "Land", "thut", "ja\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Wie liefen die Krabaten davon,", "tokens": ["Wie", "lie\u00b7fen", "die", "Kra\u00b7ba\u00b7ten", "da\u00b7von", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "PAV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dazu die Welschen Br\u00fcder:", "tokens": ["Da\u00b7zu", "die", "Wel\u00b7schen", "Br\u00fc\u00b7der", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "\u201eade Leipzig behalt deine Mahlzeit,", "tokens": ["\u201e", "a\u00b7de", "Leip\u00b7zig", "be\u00b7halt", "dei\u00b7ne", "Mahl\u00b7zeit", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "NE", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+--++-+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "\u201ezu dir komm ich nicht wieder.\u201c", "tokens": ["\u201e", "zu", "dir", "komm", "ich", "nicht", "wie\u00b7der", ".", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "APPR", "PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Also hat dieses Lied ein End,", "tokens": ["Al\u00b7so", "hat", "die\u00b7ses", "Lied", "ein", "End", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PDAT", "NN", "ART", "NN", "$,"], "meter": "+----+-+", "measure": "dactylic.init"}, "line.2": {"text": "Das sey zu Ehren gesungen", "tokens": ["Das", "sey", "zu", "Eh\u00b7ren", "ge\u00b7sun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "VVPP"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Dem K\u00f6nig in Schweden gar behend,", "tokens": ["Dem", "K\u00f6\u00b7nig", "in", "Schwe\u00b7den", "gar", "be\u00b7hend", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "ADV", "ADJD", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Der Tilly ist ihm entsprungen.", "tokens": ["Der", "Til\u00b7ly", "ist", "ihm", "ent\u00b7sprun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}