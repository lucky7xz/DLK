{"textgrid.poem.49613": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "1L: Freudenschwanger h\u00e4ngt die Wolke", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Freudenschwanger h\u00e4ngt die Wolke", "tokens": ["Freu\u00b7den\u00b7schwan\u00b7ger", "h\u00e4ngt", "die", "Wol\u00b7ke"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00dcber allem Preu\u00dfenvolke,", "tokens": ["\u00dc\u00b7ber", "al\u00b7lem", "Preu\u00b7\u00dfen\u00b7vol\u00b7ke", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jeder Gute hofft und bangt,", "tokens": ["Je\u00b7der", "Gu\u00b7te", "hofft", "und", "bangt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df ein Prinzlein angelangt.", "tokens": ["Da\u00df", "ein", "Prinz\u00b7lein", "an\u00b7ge\u00b7langt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Von dem Tage der Verm\u00e4hlung", "tokens": ["Von", "dem", "Ta\u00b7ge", "der", "Ver\u00b7m\u00e4h\u00b7lung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und bis jetzt ergibt die Z\u00e4hlung,", "tokens": ["Und", "bis", "jetzt", "er\u00b7gibt", "die", "Z\u00e4h\u00b7lung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df der Zeitpunkt eigentlich", "tokens": ["Da\u00df", "der", "Zeit\u00b7punkt", "ei\u00b7gent\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Allbereits und schon verstrich.", "tokens": ["All\u00b7be\u00b7reits", "und", "schon", "ver\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Pastor Demmel, den man fragte,", "tokens": ["Pas\u00b7tor", "Dem\u00b7mel", ",", "den", "man", "frag\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "War's, der patriotisch sagte:", "tokens": ["Wa\u00b7r's", ",", "der", "pat\u00b7ri\u00b7o\u00b7tisch", "sag\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PRELS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbseiner Zeit und immer war", "tokens": ["\u00bb", "sei\u00b7ner", "Zeit", "und", "im\u00b7mer", "war"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "PPOSAT", "NN", "KON", "ADV", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "P\u00fcnktlich unser Zollernaar.\u00ab", "tokens": ["P\u00fcnkt\u00b7lich", "un\u00b7ser", "Zol\u00b7ler\u00b7naar", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ADJD", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Und er f\u00fcgte bei: \u00bbIndessen", "tokens": ["Und", "er", "f\u00fcg\u00b7te", "bei", ":", "\u00bb", "In\u00b7des\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "$.", "$(", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Darf man niemals nicht vergessen,", "tokens": ["Darf", "man", "nie\u00b7mals", "nicht", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Herr auch dieses lenkt;", "tokens": ["Da\u00df", "der", "Herr", "auch", "die\u00b7ses", "lenkt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "PDS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Manchmal anders, wie man's denkt.", "tokens": ["Manch\u00b7mal", "an\u00b7ders", ",", "wie", "man's", "denkt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "PWAV", "PIS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Unerforschlich ist sein Walten,", "tokens": ["Un\u00b7er\u00b7for\u00b7schlich", "ist", "sein", "Wal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Denn er kann das Kind gestalten", "tokens": ["Denn", "er", "kann", "das", "Kind", "ge\u00b7stal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "ART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00e4nnlich, weil wir im Gebet", "tokens": ["M\u00e4nn\u00b7lich", ",", "weil", "wir", "im", "Ge\u00b7bet"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-++-+-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Ihn um dieses angefleht.", "tokens": ["Ihn", "um", "die\u00b7ses", "an\u00b7ge\u00b7fleht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PDAT", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Wenn's auch gegenteilig w\u00e4re,", "tokens": ["Wenn's", "auch", "ge\u00b7gen\u00b7tei\u00b7lig", "w\u00e4\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ihm sei Lob und Preis und Ehre!", "tokens": ["Ihm", "sei", "Lob", "und", "Preis", "und", "Eh\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Immer kommt es, wie es mu\u00df.", "tokens": ["Im\u00b7mer", "kommt", "es", ",", "wie", "es", "mu\u00df", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PWAV", "PPER", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hosianna! Amen! Schlu\u00df!\u00ab", "tokens": ["Ho\u00b7si\u00b7an\u00b7na", "!", "A\u00b7men", "!", "Schlu\u00df", "!", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "NN", "$.", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Schon bedeutend objektiver", "tokens": ["Schon", "be\u00b7deu\u00b7tend", "ob\u00b7jek\u00b7ti\u00b7ver"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADJD", "ADJA"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Sprach Professor Doktor Kiefer:", "tokens": ["Sprach", "Pro\u00b7fes\u00b7sor", "Dok\u00b7tor", "Kie\u00b7fer", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbneunmal drei\u00dfig Tage sind", "tokens": ["\u00bb", "neun\u00b7mal", "drei\u00b7\u00dfig", "Ta\u00b7ge", "sind"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "ADV", "CARD", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das Normale f\u00fcr ein Kind.", "tokens": ["Das", "Nor\u00b7ma\u00b7le", "f\u00fcr", "ein", "Kind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.8": {"line.1": {"text": "Doch bei F\u00fcrsten wie bei Bauern", "tokens": ["Doch", "bei", "F\u00fcrs\u00b7ten", "wie", "bei", "Bau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "KOKOM", "APPR", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Kann es manchmal l\u00e4nger dauern;", "tokens": ["Kann", "es", "manch\u00b7mal", "l\u00e4n\u00b7ger", "dau\u00b7ern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Machen wir daraus kein Hehl,", "tokens": ["Ma\u00b7chen", "wir", "da\u00b7raus", "kein", "Hehl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PAV", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00d6fter schl\u00e4gt es g\u00e4nzlich fehl.", "tokens": ["\u00d6f\u00b7ter", "schl\u00e4gt", "es", "g\u00e4nz\u00b7lich", "fehl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Kurz, man kann nichts \u00fcberst\u00fcrzen,", "tokens": ["Kurz", ",", "man", "kann", "nichts", "\u00fc\u00b7bers\u00b7t\u00fcr\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PIS", "VMFIN", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nichts verl\u00e4ngern, nichts verk\u00fcrzen;", "tokens": ["Nichts", "ver\u00b7l\u00e4n\u00b7gern", ",", "nichts", "ver\u00b7k\u00fcr\u00b7zen", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVINF", "$,", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Neunmal drei\u00dfig ist als Zahl", "tokens": ["Neun\u00b7mal", "drei\u00b7\u00dfig", "ist", "als", "Zahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "KOKOM", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nur die Regel und normal.", "tokens": ["Nur", "die", "Re\u00b7gel", "und", "nor\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "KON", "ADV", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.10": {"line.1": {"text": "Kommt ein Kind, dann unausbleiblich", "tokens": ["Kommt", "ein", "Kind", ",", "dann", "un\u00b7aus\u00b7bleib\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist es m\u00e4nnlich oder weiblich,", "tokens": ["Ist", "es", "m\u00e4nn\u00b7lich", "o\u00b7der", "weib\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Welches aber von den zwein,", "tokens": ["Wel\u00b7ches", "a\u00b7ber", "von", "den", "zwein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wei\u00df der Arzt erst hinterdrein.\u00ab", "tokens": ["Wei\u00df", "der", "Arzt", "erst", "hin\u00b7ter\u00b7drein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Wissenschaft und frommes Hoffen", "tokens": ["Wis\u00b7sen\u00b7schaft", "und", "from\u00b7mes", "Hof\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lie\u00dfen so die Frage offen,", "tokens": ["Lie\u00b7\u00dfen", "so", "die", "Fra\u00b7ge", "of\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die bei Hof und auch im Land", "tokens": ["Die", "bei", "Hof", "und", "auch", "im", "Land"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "NE", "KON", "ADV", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Viele auf die Folter spannt.", "tokens": ["Vie\u00b7le", "auf", "die", "Fol\u00b7ter", "spannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Niemand hat so schwer empfunden", "tokens": ["Nie\u00b7mand", "hat", "so", "schwer", "emp\u00b7fun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "ADV", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die erwartungsvollen Stunden", "tokens": ["Die", "er\u00b7war\u00b7tungs\u00b7vol\u00b7len", "Stun\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie der Hohenzollernaar,", "tokens": ["Wie", "der", "Ho\u00b7hen\u00b7zol\u00b7ler\u00b7naar", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Weil er hauptbeteiligt war.", "tokens": ["Weil", "er", "haupt\u00b7be\u00b7tei\u00b7ligt", "war", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Sp\u00e4hend mu\u00df er sitzen bleiben,", "tokens": ["Sp\u00e4\u00b7hend", "mu\u00df", "er", "sit\u00b7zen", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PPER", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df sich ihm die Federn str\u00e4uben,", "tokens": ["Da\u00df", "sich", "ihm", "die", "Fe\u00b7dern", "str\u00e4u\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.3": {"text": "W\u00e4hrend er sich Zweifel macht,", "tokens": ["W\u00e4h\u00b7rend", "er", "sich", "Zwei\u00b7fel", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ob es hunderteinmal kracht.", "tokens": ["Ob", "es", "hun\u00b7der\u00b7tein\u00b7mal", "kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Mancherlei Prophetenzeugnis", "tokens": ["Man\u00b7cher\u00b7lei", "Pro\u00b7phe\u00b7ten\u00b7zeug\u00b7nis"], "token_info": ["word", "word"], "pos": ["PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "H\u00f6rt man \u00fcber das Ereignis.", "tokens": ["H\u00f6rt", "man", "\u00fc\u00b7ber", "das", "Er\u00b7eig\u00b7nis", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Meistens g\u00fcnstig; unterweil", "tokens": ["Meis\u00b7tens", "g\u00fcns\u00b7tig", ";", "un\u00b7ter\u00b7weil"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADV", "ADJD", "$.", "XY"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sprach man auch das Gegenteil.", "tokens": ["Sprach", "man", "auch", "das", "Ge\u00b7gen\u00b7teil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Eine gute Frauenseele", "tokens": ["Ei\u00b7ne", "gu\u00b7te", "Frau\u00b7en\u00b7see\u00b7le"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Namens Probst in Hundekehle", "tokens": ["Na\u00b7mens", "Probst", "in", "Hun\u00b7de\u00b7keh\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "War noch im besondern klug,", "tokens": ["War", "noch", "im", "be\u00b7son\u00b7dern", "klug", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "ADJA", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Auch indem sie Karten schlug.", "tokens": ["Auch", "in\u00b7dem", "sie", "Kar\u00b7ten", "schlug", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "NN", "VVFIN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.16": {"line.1": {"text": "Bei der Nacht, wo sie erwachte", "tokens": ["Bei", "der", "Nacht", ",", "wo", "sie", "er\u00b7wach\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "PWAV", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und an ihren K\u00f6nig dachte,", "tokens": ["Und", "an", "ih\u00b7ren", "K\u00f6\u00b7nig", "dach\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Sah sie deutlich \u00fcberm Bett", "tokens": ["Sah", "sie", "deut\u00b7lich", "\u00fc\u00b7berm", "Bett"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Etwas, was die Mannsform h\u00e4tt'.", "tokens": ["Et\u00b7was", ",", "was", "die", "Manns\u00b7form", "h\u00e4tt'", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PRELS", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Als sie's n\u00e4her wollt' erkunden,", "tokens": ["Als", "sie's", "n\u00e4\u00b7her", "wollt'", "er\u00b7kun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "War es pl\u00f6tzlich weg, verschwunden,", "tokens": ["War", "es", "pl\u00f6tz\u00b7lich", "weg", ",", "ver\u00b7schwun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "PTKVZ", "$,", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und da ward ihr offenbar,", "tokens": ["Und", "da", "ward", "ihr", "of\u00b7fen\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df es blo\u00df ein Zeichen war.", "tokens": ["Da\u00df", "es", "blo\u00df", "ein", "Zei\u00b7chen", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Auch bei Kulickes in Zossen", "tokens": ["Auch", "bei", "Ku\u00b7li\u00b7ckes", "in", "Zos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Legt ein Huhn ganz unverdrossen", "tokens": ["Legt", "ein", "Huhn", "ganz", "un\u00b7ver\u00b7dros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jedesmal ein m\u00e4nnlich Ei,", "tokens": ["Je\u00b7des\u00b7mal", "ein", "m\u00e4nn\u00b7lich", "Ei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJD", "NN", "$,"], "meter": "+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Da\u00df es drin ein Gockel sei.", "tokens": ["Da\u00df", "es", "drin", "ein", "Go\u00b7ckel", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.19": {"line.1": {"text": "W\u00e4hrend dieser Wartepoche", "tokens": ["W\u00e4h\u00b7rend", "die\u00b7ser", "War\u00b7te\u00b7po\u00b7che"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "PDAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat Herr Goldstein f\u00fcr die \u00bbWoche\u00ab", "tokens": ["Hat", "Herr", "Gold\u00b7stein", "f\u00fcr", "die", "\u00bb", "Wo\u00b7che", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "NN", "NN", "APPR", "ART", "$(", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Den Artikel reserviert,", "tokens": ["Den", "Ar\u00b7ti\u00b7kel", "re\u00b7ser\u00b7viert", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Falls das Kind ein Kn\u00e4blich wird.", "tokens": ["Falls", "das", "Kind", "ein", "Kn\u00e4b\u00b7lich", "wird", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "Er beschrieb mit Dichtergabe,", "tokens": ["Er", "be\u00b7schrieb", "mit", "Dich\u00b7ter\u00b7ga\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Welche Freude alles habe", "tokens": ["Wel\u00b7che", "Freu\u00b7de", "al\u00b7les", "ha\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAT", "NN", "PIS", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der H\u00fctte bis zum Thron.", "tokens": ["Von", "der", "H\u00fct\u00b7te", "bis", "zum", "Thron", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Dann beschrieb er auch den Sohn.", "tokens": ["Dann", "be\u00b7schrieb", "er", "auch", "den", "Sohn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.21": {"line.1": {"text": "Dann beschrieb er auch mit R\u00fchrung", "tokens": ["Dann", "be\u00b7schrieb", "er", "auch", "mit", "R\u00fch\u00b7rung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gottes gnadenreiche F\u00fchrung.", "tokens": ["Got\u00b7tes", "gna\u00b7den\u00b7rei\u00b7che", "F\u00fch\u00b7rung", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und dann legt' er mit Geduld", "tokens": ["Und", "dann", "legt'", "er", "mit", "Ge\u00b7duld"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Artikel in das Pult.", "tokens": ["Den", "Ar\u00b7ti\u00b7kel", "in", "das", "Pult", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.22": {"line.1": {"text": "Als es immer l\u00e4nger w\u00e4hrte", "tokens": ["Als", "es", "im\u00b7mer", "l\u00e4n\u00b7ger", "w\u00e4hr\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die Ungeduld sich mehrte,", "tokens": ["Und", "die", "Un\u00b7ge\u00b7duld", "sich", "mehr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Kam der Aar zum Storch heran,", "tokens": ["Kam", "der", "Aar", "zum", "Storch", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er haucht ihn grimmig an.", "tokens": ["Und", "er", "haucht", "ihn", "grim\u00b7mig", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Ob er wei\u00df, um was sich's handelt,", "tokens": ["Ob", "er", "wei\u00df", ",", "um", "was", "sich's", "han\u00b7delt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$,", "APPR", "PRELS", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df er so gem\u00e4chlich wandelt?", "tokens": ["Da\u00df", "er", "so", "ge\u00b7m\u00e4ch\u00b7lich", "wan\u00b7delt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob es nicht f\u00fcr Majest\u00e4t", "tokens": ["Ob", "es", "nicht", "f\u00fcr", "Ma\u00b7jes\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ganz bedeutend fixer geht?", "tokens": ["Ganz", "be\u00b7deu\u00b7tend", "fi\u00b7xer", "geht", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Fischt vielleicht man in den Binsen", "tokens": ["Fischt", "viel\u00b7leicht", "man", "in", "den", "Bin\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "PIS", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nur so nebenbei die Prinzen?", "tokens": ["Nur", "so", "ne\u00b7ben\u00b7bei", "die", "Prin\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Ob man nicht die Ehre kennt?", "tokens": ["Ob", "man", "nicht", "die", "Eh\u00b7re", "kennt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Himmel Herrgottsakrament!", "tokens": ["Him\u00b7mel", "Herr\u00b7gott\u00b7sa\u00b7kra\u00b7ment", "!"], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Als der Storch es ganz vernommen,", "tokens": ["Als", "der", "Storch", "es", "ganz", "ver\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist er zornig heimgekommen,", "tokens": ["Ist", "er", "zor\u00b7nig", "heim\u00b7ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und er sprach mit voller Kraft:", "tokens": ["Und", "er", "sprach", "mit", "vol\u00b7ler", "Kraft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbdieser Aar ist l\u00fcmmelhaft.\u00ab", "tokens": ["\u00bb", "die\u00b7ser", "Aar", "ist", "l\u00fcm\u00b7mel\u00b7haft", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PDAT", "NN", "VAFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "\u00bbja, gewi\u00df, er ist ein Flegel,\u00ab", "tokens": ["\u00bb", "ja", ",", "ge\u00b7wi\u00df", ",", "er", "ist", "ein", "Fle\u00b7gel", ",", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PTKANT", "$,", "ADV", "$,", "PPER", "VAFIN", "ART", "NN", "$,", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sagt Frau St\u00f6rchin, \u00bbin der Regel", "tokens": ["Sagt", "Frau", "St\u00f6r\u00b7chin", ",", "\u00bb", "in", "der", "Re\u00b7gel"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["VVFIN", "NN", "NE", "$,", "$(", "APPR", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Kommt das bei den Gro\u00dfen vor,", "tokens": ["Kommt", "das", "bei", "den", "Gro\u00b7\u00dfen", "vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Du mu\u00dft klug sein, Adebor!", "tokens": ["Du", "mu\u00dft", "klug", "sein", ",", "A\u00b7de\u00b7bor", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VAINF", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "Du bist fein, und deinesgleichen", "tokens": ["Du", "bist", "fein", ",", "und", "dei\u00b7nes\u00b7glei\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "KON", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kann mit Grobheit nichts erreichen,", "tokens": ["Kann", "mit", "Grob\u00b7heit", "nichts", "er\u00b7rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn er gibt's zur\u00fcck mit Zins.", "tokens": ["Denn", "er", "gibt's", "zu\u00b7r\u00fcck", "mit", "Zins", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Bring ihm doch den Zollernprinz!\u00ab", "tokens": ["Bring", "ihm", "doch", "den", "Zol\u00b7lern\u00b7prinz", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Und so kam's. Nach wenig Tagen", "tokens": ["Und", "so", "kam'", "s.", "Nach", "we\u00b7nig", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "NE", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat die Weihestund' geschlagen;", "tokens": ["Hat", "die", "Wei\u00b7he\u00b7stund'", "ge\u00b7schla\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In dem Hohenzollernschlo\u00df", "tokens": ["In", "dem", "Ho\u00b7hen\u00b7zol\u00b7lern\u00b7schlo\u00df"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Gab es einen Kaiserspro\u00df.", "tokens": ["Gab", "es", "ei\u00b7nen", "Kai\u00b7ser\u00b7spro\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Was die Witwe Probst gesehen,", "tokens": ["Was", "die", "Wit\u00b7we", "Probst", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist in Wirklichkeit geschehen,", "tokens": ["Ist", "in", "Wirk\u00b7lich\u00b7keit", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Herr Pastor Demmel sprach:", "tokens": ["Und", "Herr", "Pas\u00b7tor", "Dem\u00b7mel", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "NN", "VVFIN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "\u00bbdas Gebet hilft allgemach.\u00ab", "tokens": ["\u00bb", "das", "Ge\u00b7bet", "hilft", "all\u00b7ge\u00b7mach", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "ADV", "$.", "$("], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.30": {"line.1": {"text": "Und in Preu\u00dfen herrschte Wonne,", "tokens": ["Und", "in", "Preu\u00b7\u00dfen", "herrschte", "Won\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "ADJA", "NN", "$,"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und die Wolke wich der Sonne,", "tokens": ["Und", "die", "Wol\u00b7ke", "wich", "der", "Son\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Herrn Kulicke sein Ei", "tokens": ["Und", "Herrn", "Ku\u00b7li\u00b7cke", "sein", "Ei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "NN", "PPOSAT", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Hatte recht auch nebenbei.", "tokens": ["Hat\u00b7te", "recht", "auch", "ne\u00b7ben\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.31": {"line.1": {"text": "Und auch Goldstein freut's erheblich:", "tokens": ["Und", "auch", "Gold\u00b7stein", "freut's", "er\u00b7heb\u00b7lich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NN", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was er \u00fcber diesen Kn\u00e4blich", "tokens": ["Was", "er", "\u00fc\u00b7ber", "die\u00b7sen", "Kn\u00e4b\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "PDAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ahnungsvoll der \u00bbWoche\u00ab schickt,", "tokens": ["Ah\u00b7nungs\u00b7voll", "der", "\u00bb", "Wo\u00b7che", "\u00ab", "schickt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "ART", "$(", "NN", "$(", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ward bezahlt und fett gedr\u00fcckt.", "tokens": ["Ward", "be\u00b7zahlt", "und", "fett", "ge\u00b7dr\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "Und die alten Gener\u00e4le", "tokens": ["Und", "die", "al\u00b7ten", "Ge\u00b7ne\u00b7r\u00e4\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schl\u00fcrften in die K\u00f6nigss\u00e4le,", "tokens": ["Schl\u00fcrf\u00b7ten", "in", "die", "K\u00f6\u00b7nigs\u00b7s\u00e4\u00b7le", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie fl\u00fcstern sich ins Ohr:", "tokens": ["Und", "sie", "fl\u00fcs\u00b7tern", "sich", "ins", "Ohr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PRF", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbhohenzollernblut h\u00e4lt vor.", "tokens": ["\u00bb", "ho\u00b7hen\u00b7zol\u00b7lern\u00b7blut", "h\u00e4lt", "vor", "."], "token_info": ["punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Det jibt wieder en Soldaten", "tokens": ["Det", "jibt", "wie\u00b7der", "en", "Sol\u00b7da\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jut jebaut und wohl jeraten,", "tokens": ["Jut", "je\u00b7baut", "und", "wohl", "je\u00b7ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "KON", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Immer stramm und immer stramm;", "tokens": ["Im\u00b7mer", "stramm", "und", "im\u00b7mer", "stramm", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADV", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "'s is en janz famoser Stamm.", "tokens": ["'s", "is", "en", "janz", "fa\u00b7mo\u00b7ser", "Stamm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "FM", "FM", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Tja, da kann woll jar nischt jegen;", "tokens": ["Tja", ",", "da", "kann", "woll", "jar", "nischt", "je\u00b7gen", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "VMFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Immer fix mit Kindersegen!", "tokens": ["Im\u00b7mer", "fix", "mit", "Kin\u00b7der\u00b7se\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Heirat und gleich schwuppdi bum! \u2013", "tokens": ["Hei\u00b7rat", "und", "gleich", "schwupp\u00b7di", "bum", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "KON", "ADV", "FM", "FM", "$.", "$("], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": "\u2013 Pst! Man dreht sich nach uns um.\u00ab", "tokens": ["\u2013", "Pst", "!", "Man", "dreht", "sich", "nach", "uns", "um", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM.xy", "$.", "PIS", "VVFIN", "PRF", "APPR", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.35": {"line.1": {"text": "Auch zwei alte Kammerchaisen", "tokens": ["Auch", "zwei", "al\u00b7te", "Kam\u00b7mer\u00b7chai\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "CARD", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sind voll Wonnigkeit gewesen,", "tokens": ["Sind", "voll", "Won\u00b7nig\u00b7keit", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "NN", "VAPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie pispern hinterr\u00fccks", "tokens": ["Und", "sie", "pis\u00b7pern", "hin\u00b7ter\u00b7r\u00fccks"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00dcber diesen Fall des Gl\u00fccks.", "tokens": ["\u00dc\u00b7ber", "die\u00b7sen", "Fall", "des", "Gl\u00fccks", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "\u00bbha, mon Dieu! Und so was Rundes,", "tokens": ["\u00bb", "ha", ",", "mon", "Di\u00b7eu", "!", "Und", "so", "was", "Run\u00b7des", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "NE", "NE", "$.", "KON", "ADV", "PWS", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Dickes, Fettes und Gesundes!", "tokens": ["Di\u00b7ckes", ",", "Fet\u00b7tes", "und", "Ge\u00b7sun\u00b7des", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Teure Gr\u00e4fin, sehn Sie dies?", "tokens": ["Teu\u00b7re", "Gr\u00e4\u00b7fin", ",", "sehn", "Sie", "dies", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "PPER", "PDS", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie entz\u00fcckend! Hoh! Wie s\u00fc\u00df!\u00ab", "tokens": ["Wie", "ent\u00b7z\u00fc\u00b7ckend", "!", "Hoh", "!", "Wie", "s\u00fc\u00df", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVPP", "$.", "NE", "$.", "PWAV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.37": {"line.1": {"text": "\u00bbhat es schon?\u00ab \u2013 \u00bbGewi\u00df, Komtesse!", "tokens": ["\u00bb", "hat", "es", "schon", "?", "\u00ab", "\u2013", "\u00bb", "Ge\u00b7wi\u00df", ",", "Kom\u00b7tes\u00b7se", "!"], "token_info": ["punct", "word", "word", "word", "punct", "punct", "punct", "punct", "word", "punct", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ADV", "$.", "$(", "$(", "$(", "PTKANT", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In dem Bettchen war noch N\u00e4sse.\u00ab", "tokens": ["In", "dem", "Bett\u00b7chen", "war", "noch", "N\u00e4s\u00b7se", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "ADV", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbteure Gr\u00e4fin sahen dies?\u00ab", "tokens": ["\u00bb", "teu\u00b7re", "Gr\u00e4\u00b7fin", "sa\u00b7hen", "dies", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "VVFIN", "PDS", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbnu nat\u00fcrlich!\u00ab \u2013 \u00bbHoh, wie s\u00fc\u00df!\u00ab", "tokens": ["\u00bb", "nu", "na\u00b7t\u00fcr\u00b7lich", "!", "\u00ab", "\u2013", "\u00bb", "Hoh", ",", "wie", "s\u00fc\u00df", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADV", "$.", "$(", "$(", "$(", "NE", "$,", "PWAV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.38": {"line.1": {"text": "Preu\u00dfens ganze K\u00f6nigstreue", "tokens": ["Preu\u00b7\u00dfens", "gan\u00b7ze", "K\u00f6\u00b7nigs\u00b7treu\u00b7e"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zeigte heute sich aufs neue,", "tokens": ["Zeig\u00b7te", "heu\u00b7te", "sich", "aufs", "neu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PRF", "APPRART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie erschien im Volksgedr\u00e4ng", "tokens": ["Sie", "er\u00b7schien", "im", "Volks\u00b7ge\u00b7dr\u00e4ng"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und im Frack und Eskarp\u00e4ng.", "tokens": ["Und", "im", "Frack", "und", "Es\u00b7kar\u00b7p\u00e4ng", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "KON", "NN", "$."], "meter": "+-+-+--", "measure": "unknown.measure.tri"}}, "stanza.39": {"line.1": {"text": "Unter ihrem Schiffhut schworen", "tokens": ["Un\u00b7ter", "ih\u00b7rem", "Schiff\u00b7hut", "schwo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Altgediente Direktoren,", "tokens": ["Alt\u00b7ge\u00b7dien\u00b7te", "Di\u00b7rek\u00b7to\u00b7ren", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df sie auch dem neuen Kind", "tokens": ["Da\u00df", "sie", "auch", "dem", "neu\u00b7en", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcrchterlich ergeben sind.", "tokens": ["F\u00fcrch\u00b7ter\u00b7lich", "er\u00b7ge\u00b7ben", "sind", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.40": {"line.1": {"text": "Richter, Schreiber, Staatsanw\u00e4lte", "tokens": ["Rich\u00b7ter", ",", "Schrei\u00b7ber", ",", "Staats\u00b7an\u00b7w\u00e4l\u00b7te"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "NN", "$,", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Legen ab die Herzensk\u00e4lte,", "tokens": ["Le\u00b7gen", "ab", "die", "Her\u00b7zens\u00b7k\u00e4l\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00d6ffnen ihre enge Brust", "tokens": ["\u00d6ff\u00b7nen", "ih\u00b7re", "en\u00b7ge", "Brust"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Froher Untertanenlust.", "tokens": ["Fro\u00b7her", "Un\u00b7ter\u00b7ta\u00b7nen\u00b7lust", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.41": {"line.1": {"text": "Und in manchem Sekret\u00e4re", "tokens": ["Und", "in", "man\u00b7chem", "Se\u00b7kre\u00b7t\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lag die Ahnung heut, er w\u00e4re", "tokens": ["Lag", "die", "Ah\u00b7nung", "heut", ",", "er", "w\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "ART", "NN", "ADV", "$,", "PPER", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu Verschiedenem imstand", "tokens": ["Zu", "Ver\u00b7schie\u00b7de\u00b7nem", "ims\u00b7tand"], "token_info": ["word", "word", "word"], "pos": ["APPR", "NE", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr sein teures Vaterland.", "tokens": ["F\u00fcr", "sein", "teu\u00b7res", "Va\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.42": {"line.1": {"text": "Auch in den Kasernen waren", "tokens": ["Auch", "in", "den", "Ka\u00b7ser\u00b7nen", "wa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Aufgestellt Rekrutenscharen.", "tokens": ["Auf\u00b7ge\u00b7stellt", "Rek\u00b7ru\u00b7ten\u00b7scha\u00b7ren", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Heute wurde nicht geschimpft,", "tokens": ["Heu\u00b7te", "wur\u00b7de", "nicht", "ge\u00b7schimpft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sondern Treue eingeimpft.", "tokens": ["Son\u00b7dern", "Treu\u00b7e", "ein\u00b7ge\u00b7impft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.43": {"line.1": {"text": "Da\u00df der Tag auch den Soldaten", "tokens": ["Da\u00df", "der", "Tag", "auch", "den", "Sol\u00b7da\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Heilig bleibe, gab es Braten.", "tokens": ["Hei\u00b7lig", "blei\u00b7be", ",", "gab", "es", "Bra\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$,", "VVFIN", "PPER", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Feiernd seinen Herrscherstamm", "tokens": ["Fei\u00b7ernd", "sei\u00b7nen", "Herr\u00b7scher\u00b7stamm"], "token_info": ["word", "word", "word"], "pos": ["ADJD", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "A\u00df ein jeder hundert Gramm.", "tokens": ["A\u00df", "ein", "je\u00b7der", "hun\u00b7dert", "Gram\u00b7m."], "token_info": ["word", "word", "word", "word", "abbreviation"], "pos": ["VVFIN", "ART", "PIAT", "CARD", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.44": {"line.1": {"text": "Kurz und gut, im Lande Preu\u00dfen", "tokens": ["Kurz", "und", "gut", ",", "im", "Lan\u00b7de", "Preu\u00b7\u00dfen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "KON", "ADJD", "$,", "APPRART", "NN", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wollt' ein jeder sich beflei\u00dfen,", "tokens": ["Wollt'", "ein", "je\u00b7der", "sich", "be\u00b7flei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "PIAT", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Tag auch feierlich", "tokens": ["Da\u00df", "der", "Tag", "auch", "fei\u00b7er\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und mit W\u00fcrdigkeit verstrich.", "tokens": ["Und", "mit", "W\u00fcr\u00b7dig\u00b7keit", "ver\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.45": {"line.1": {"text": "Doch wie waren die Gef\u00fchle", "tokens": ["Doch", "wie", "wa\u00b7ren", "die", "Ge\u00b7f\u00fch\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "VAFIN", "ART", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Weiter s\u00fcdlich? Ziemlich k\u00fchle.", "tokens": ["Wei\u00b7ter", "s\u00fcd\u00b7lich", "?", "Ziem\u00b7lich", "k\u00fch\u00b7le", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "NE", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Oben h\u00f6flich, aber flau,", "tokens": ["O\u00b7ben", "h\u00f6f\u00b7lich", ",", "a\u00b7ber", "flau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Unten ganz betr\u00e4chtlich mau.", "tokens": ["Un\u00b7ten", "ganz", "be\u00b7tr\u00e4cht\u00b7lich", "mau", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.46": {"line.1": {"text": "Der Fassadenmaurer Huber", "tokens": ["Der", "Fas\u00b7sa\u00b7den\u00b7mau\u00b7rer", "Hu\u00b7ber"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "NE"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Stand an seinem M\u00f6rtelzuber;", "tokens": ["Stand", "an", "sei\u00b7nem", "M\u00f6r\u00b7tel\u00b7zu\u00b7ber", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als man ihm die Nachricht bracht',", "tokens": ["Als", "man", "ihm", "die", "Nach\u00b7richt", "bracht'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat er sich nichts draus gemacht.", "tokens": ["Hat", "er", "sich", "nichts", "draus", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "PIS", "PAV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.47": {"line.1": {"text": "Holte seine Tabakflasche", "tokens": ["Hol\u00b7te", "sei\u00b7ne", "Ta\u00b7bak\u00b7fla\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aus der linken Westentasche,", "tokens": ["Aus", "der", "lin\u00b7ken", "Wes\u00b7ten\u00b7ta\u00b7sche", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sagte: \u00bbWas? A Preu\u00df? A Prinz?", "tokens": ["Sag\u00b7te", ":", "\u00bb", "Was", "?", "A", "Preu\u00df", "?", "A", "Prinz", "?"], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$.", "$(", "PWS", "$.", "NE", "NE", "$.", "NE", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ja, was k\u00fcmmert denn d\u00f6s ins?", "tokens": ["Ja", ",", "was", "k\u00fcm\u00b7mert", "denn", "d\u00f6s", "ins", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWS", "VVFIN", "ADV", "ADV", "APPRART", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.48": {"line.1": {"text": "D\u00f6s bek\u00fcmmert ins ganz wenig;", "tokens": ["D\u00f6s", "be\u00b7k\u00fcm\u00b7mert", "ins", "ganz", "we\u00b7nig", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "ADV", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der werd halt amal a K\u00f6nig", "tokens": ["Der", "werd", "halt", "a\u00b7mal", "a", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "NE", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bei die Preu\u00dfen. Net bei ins.", "tokens": ["Bei", "die", "Preu\u00b7\u00dfen", ".", "Net", "bei", "ins", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "NE", "APPR", "APPRART", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "So? Da ham s' an neuen Prinz?\u00ab", "tokens": ["So", "?", "Da", "ham", "s'", "an", "neu\u00b7en", "Prinz", "?", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "$.", "KOUS", "NE", "NE", "APPR", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Freudenschwanger h\u00e4ngt die Wolke", "tokens": ["Freu\u00b7den\u00b7schwan\u00b7ger", "h\u00e4ngt", "die", "Wol\u00b7ke"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00dcber allem Preu\u00dfenvolke,", "tokens": ["\u00dc\u00b7ber", "al\u00b7lem", "Preu\u00b7\u00dfen\u00b7vol\u00b7ke", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jeder Gute hofft und bangt,", "tokens": ["Je\u00b7der", "Gu\u00b7te", "hofft", "und", "bangt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df ein Prinzlein angelangt.", "tokens": ["Da\u00df", "ein", "Prinz\u00b7lein", "an\u00b7ge\u00b7langt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.50": {"line.1": {"text": "Von dem Tage der Verm\u00e4hlung", "tokens": ["Von", "dem", "Ta\u00b7ge", "der", "Ver\u00b7m\u00e4h\u00b7lung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und bis jetzt ergibt die Z\u00e4hlung,", "tokens": ["Und", "bis", "jetzt", "er\u00b7gibt", "die", "Z\u00e4h\u00b7lung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df der Zeitpunkt eigentlich", "tokens": ["Da\u00df", "der", "Zeit\u00b7punkt", "ei\u00b7gent\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Allbereits und schon verstrich.", "tokens": ["All\u00b7be\u00b7reits", "und", "schon", "ver\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.51": {"line.1": {"text": "Pastor Demmel, den man fragte,", "tokens": ["Pas\u00b7tor", "Dem\u00b7mel", ",", "den", "man", "frag\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "War's, der patriotisch sagte:", "tokens": ["Wa\u00b7r's", ",", "der", "pat\u00b7ri\u00b7o\u00b7tisch", "sag\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PRELS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbseiner Zeit und immer war", "tokens": ["\u00bb", "sei\u00b7ner", "Zeit", "und", "im\u00b7mer", "war"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "PPOSAT", "NN", "KON", "ADV", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "P\u00fcnktlich unser Zollernaar.\u00ab", "tokens": ["P\u00fcnkt\u00b7lich", "un\u00b7ser", "Zol\u00b7ler\u00b7naar", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ADJD", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.52": {"line.1": {"text": "Und er f\u00fcgte bei: \u00bbIndessen", "tokens": ["Und", "er", "f\u00fcg\u00b7te", "bei", ":", "\u00bb", "In\u00b7des\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "$.", "$(", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Darf man niemals nicht vergessen,", "tokens": ["Darf", "man", "nie\u00b7mals", "nicht", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Herr auch dieses lenkt;", "tokens": ["Da\u00df", "der", "Herr", "auch", "die\u00b7ses", "lenkt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "PDS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Manchmal anders, wie man's denkt.", "tokens": ["Manch\u00b7mal", "an\u00b7ders", ",", "wie", "man's", "denkt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "PWAV", "PIS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.53": {"line.1": {"text": "Unerforschlich ist sein Walten,", "tokens": ["Un\u00b7er\u00b7for\u00b7schlich", "ist", "sein", "Wal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Denn er kann das Kind gestalten", "tokens": ["Denn", "er", "kann", "das", "Kind", "ge\u00b7stal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "ART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00e4nnlich, weil wir im Gebet", "tokens": ["M\u00e4nn\u00b7lich", ",", "weil", "wir", "im", "Ge\u00b7bet"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-++-+-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Ihn um dieses angefleht.", "tokens": ["Ihn", "um", "die\u00b7ses", "an\u00b7ge\u00b7fleht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PDAT", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.54": {"line.1": {"text": "Wenn's auch gegenteilig w\u00e4re,", "tokens": ["Wenn's", "auch", "ge\u00b7gen\u00b7tei\u00b7lig", "w\u00e4\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ihm sei Lob und Preis und Ehre!", "tokens": ["Ihm", "sei", "Lob", "und", "Preis", "und", "Eh\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Immer kommt es, wie es mu\u00df.", "tokens": ["Im\u00b7mer", "kommt", "es", ",", "wie", "es", "mu\u00df", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PWAV", "PPER", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hosianna! Amen! Schlu\u00df!\u00ab", "tokens": ["Ho\u00b7si\u00b7an\u00b7na", "!", "A\u00b7men", "!", "Schlu\u00df", "!", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "NN", "$.", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.55": {"line.1": {"text": "Schon bedeutend objektiver", "tokens": ["Schon", "be\u00b7deu\u00b7tend", "ob\u00b7jek\u00b7ti\u00b7ver"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADJD", "ADJA"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Sprach Professor Doktor Kiefer:", "tokens": ["Sprach", "Pro\u00b7fes\u00b7sor", "Dok\u00b7tor", "Kie\u00b7fer", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbneunmal drei\u00dfig Tage sind", "tokens": ["\u00bb", "neun\u00b7mal", "drei\u00b7\u00dfig", "Ta\u00b7ge", "sind"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "ADV", "CARD", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das Normale f\u00fcr ein Kind.", "tokens": ["Das", "Nor\u00b7ma\u00b7le", "f\u00fcr", "ein", "Kind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.56": {"line.1": {"text": "Doch bei F\u00fcrsten wie bei Bauern", "tokens": ["Doch", "bei", "F\u00fcrs\u00b7ten", "wie", "bei", "Bau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "KOKOM", "APPR", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Kann es manchmal l\u00e4nger dauern;", "tokens": ["Kann", "es", "manch\u00b7mal", "l\u00e4n\u00b7ger", "dau\u00b7ern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Machen wir daraus kein Hehl,", "tokens": ["Ma\u00b7chen", "wir", "da\u00b7raus", "kein", "Hehl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PAV", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00d6fter schl\u00e4gt es g\u00e4nzlich fehl.", "tokens": ["\u00d6f\u00b7ter", "schl\u00e4gt", "es", "g\u00e4nz\u00b7lich", "fehl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.57": {"line.1": {"text": "Kurz, man kann nichts \u00fcberst\u00fcrzen,", "tokens": ["Kurz", ",", "man", "kann", "nichts", "\u00fc\u00b7bers\u00b7t\u00fcr\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PIS", "VMFIN", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nichts verl\u00e4ngern, nichts verk\u00fcrzen;", "tokens": ["Nichts", "ver\u00b7l\u00e4n\u00b7gern", ",", "nichts", "ver\u00b7k\u00fcr\u00b7zen", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVINF", "$,", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Neunmal drei\u00dfig ist als Zahl", "tokens": ["Neun\u00b7mal", "drei\u00b7\u00dfig", "ist", "als", "Zahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "KOKOM", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nur die Regel und normal.", "tokens": ["Nur", "die", "Re\u00b7gel", "und", "nor\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "KON", "ADV", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.58": {"line.1": {"text": "Kommt ein Kind, dann unausbleiblich", "tokens": ["Kommt", "ein", "Kind", ",", "dann", "un\u00b7aus\u00b7bleib\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist es m\u00e4nnlich oder weiblich,", "tokens": ["Ist", "es", "m\u00e4nn\u00b7lich", "o\u00b7der", "weib\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Welches aber von den zwein,", "tokens": ["Wel\u00b7ches", "a\u00b7ber", "von", "den", "zwein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wei\u00df der Arzt erst hinterdrein.\u00ab", "tokens": ["Wei\u00df", "der", "Arzt", "erst", "hin\u00b7ter\u00b7drein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.59": {"line.1": {"text": "Wissenschaft und frommes Hoffen", "tokens": ["Wis\u00b7sen\u00b7schaft", "und", "from\u00b7mes", "Hof\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lie\u00dfen so die Frage offen,", "tokens": ["Lie\u00b7\u00dfen", "so", "die", "Fra\u00b7ge", "of\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die bei Hof und auch im Land", "tokens": ["Die", "bei", "Hof", "und", "auch", "im", "Land"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "NE", "KON", "ADV", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Viele auf die Folter spannt.", "tokens": ["Vie\u00b7le", "auf", "die", "Fol\u00b7ter", "spannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.60": {"line.1": {"text": "Niemand hat so schwer empfunden", "tokens": ["Nie\u00b7mand", "hat", "so", "schwer", "emp\u00b7fun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "ADV", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die erwartungsvollen Stunden", "tokens": ["Die", "er\u00b7war\u00b7tungs\u00b7vol\u00b7len", "Stun\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie der Hohenzollernaar,", "tokens": ["Wie", "der", "Ho\u00b7hen\u00b7zol\u00b7ler\u00b7naar", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Weil er hauptbeteiligt war.", "tokens": ["Weil", "er", "haupt\u00b7be\u00b7tei\u00b7ligt", "war", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Sp\u00e4hend mu\u00df er sitzen bleiben,", "tokens": ["Sp\u00e4\u00b7hend", "mu\u00df", "er", "sit\u00b7zen", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PPER", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df sich ihm die Federn str\u00e4uben,", "tokens": ["Da\u00df", "sich", "ihm", "die", "Fe\u00b7dern", "str\u00e4u\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.3": {"text": "W\u00e4hrend er sich Zweifel macht,", "tokens": ["W\u00e4h\u00b7rend", "er", "sich", "Zwei\u00b7fel", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ob es hunderteinmal kracht.", "tokens": ["Ob", "es", "hun\u00b7der\u00b7tein\u00b7mal", "kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.62": {"line.1": {"text": "Mancherlei Prophetenzeugnis", "tokens": ["Man\u00b7cher\u00b7lei", "Pro\u00b7phe\u00b7ten\u00b7zeug\u00b7nis"], "token_info": ["word", "word"], "pos": ["PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "H\u00f6rt man \u00fcber das Ereignis.", "tokens": ["H\u00f6rt", "man", "\u00fc\u00b7ber", "das", "Er\u00b7eig\u00b7nis", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Meistens g\u00fcnstig; unterweil", "tokens": ["Meis\u00b7tens", "g\u00fcns\u00b7tig", ";", "un\u00b7ter\u00b7weil"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADV", "ADJD", "$.", "XY"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sprach man auch das Gegenteil.", "tokens": ["Sprach", "man", "auch", "das", "Ge\u00b7gen\u00b7teil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.63": {"line.1": {"text": "Eine gute Frauenseele", "tokens": ["Ei\u00b7ne", "gu\u00b7te", "Frau\u00b7en\u00b7see\u00b7le"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Namens Probst in Hundekehle", "tokens": ["Na\u00b7mens", "Probst", "in", "Hun\u00b7de\u00b7keh\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "War noch im besondern klug,", "tokens": ["War", "noch", "im", "be\u00b7son\u00b7dern", "klug", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "ADJA", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Auch indem sie Karten schlug.", "tokens": ["Auch", "in\u00b7dem", "sie", "Kar\u00b7ten", "schlug", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "NN", "VVFIN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.64": {"line.1": {"text": "Bei der Nacht, wo sie erwachte", "tokens": ["Bei", "der", "Nacht", ",", "wo", "sie", "er\u00b7wach\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "PWAV", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und an ihren K\u00f6nig dachte,", "tokens": ["Und", "an", "ih\u00b7ren", "K\u00f6\u00b7nig", "dach\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Sah sie deutlich \u00fcberm Bett", "tokens": ["Sah", "sie", "deut\u00b7lich", "\u00fc\u00b7berm", "Bett"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Etwas, was die Mannsform h\u00e4tt'.", "tokens": ["Et\u00b7was", ",", "was", "die", "Manns\u00b7form", "h\u00e4tt'", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PRELS", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.65": {"line.1": {"text": "Als sie's n\u00e4her wollt' erkunden,", "tokens": ["Als", "sie's", "n\u00e4\u00b7her", "wollt'", "er\u00b7kun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "War es pl\u00f6tzlich weg, verschwunden,", "tokens": ["War", "es", "pl\u00f6tz\u00b7lich", "weg", ",", "ver\u00b7schwun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "PTKVZ", "$,", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und da ward ihr offenbar,", "tokens": ["Und", "da", "ward", "ihr", "of\u00b7fen\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df es blo\u00df ein Zeichen war.", "tokens": ["Da\u00df", "es", "blo\u00df", "ein", "Zei\u00b7chen", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.66": {"line.1": {"text": "Auch bei Kulickes in Zossen", "tokens": ["Auch", "bei", "Ku\u00b7li\u00b7ckes", "in", "Zos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Legt ein Huhn ganz unverdrossen", "tokens": ["Legt", "ein", "Huhn", "ganz", "un\u00b7ver\u00b7dros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jedesmal ein m\u00e4nnlich Ei,", "tokens": ["Je\u00b7des\u00b7mal", "ein", "m\u00e4nn\u00b7lich", "Ei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJD", "NN", "$,"], "meter": "+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Da\u00df es drin ein Gockel sei.", "tokens": ["Da\u00df", "es", "drin", "ein", "Go\u00b7ckel", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.67": {"line.1": {"text": "W\u00e4hrend dieser Wartepoche", "tokens": ["W\u00e4h\u00b7rend", "die\u00b7ser", "War\u00b7te\u00b7po\u00b7che"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "PDAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat Herr Goldstein f\u00fcr die \u00bbWoche\u00ab", "tokens": ["Hat", "Herr", "Gold\u00b7stein", "f\u00fcr", "die", "\u00bb", "Wo\u00b7che", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "NN", "NN", "APPR", "ART", "$(", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Den Artikel reserviert,", "tokens": ["Den", "Ar\u00b7ti\u00b7kel", "re\u00b7ser\u00b7viert", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Falls das Kind ein Kn\u00e4blich wird.", "tokens": ["Falls", "das", "Kind", "ein", "Kn\u00e4b\u00b7lich", "wird", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.68": {"line.1": {"text": "Er beschrieb mit Dichtergabe,", "tokens": ["Er", "be\u00b7schrieb", "mit", "Dich\u00b7ter\u00b7ga\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Welche Freude alles habe", "tokens": ["Wel\u00b7che", "Freu\u00b7de", "al\u00b7les", "ha\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAT", "NN", "PIS", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der H\u00fctte bis zum Thron.", "tokens": ["Von", "der", "H\u00fct\u00b7te", "bis", "zum", "Thron", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Dann beschrieb er auch den Sohn.", "tokens": ["Dann", "be\u00b7schrieb", "er", "auch", "den", "Sohn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.69": {"line.1": {"text": "Dann beschrieb er auch mit R\u00fchrung", "tokens": ["Dann", "be\u00b7schrieb", "er", "auch", "mit", "R\u00fch\u00b7rung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gottes gnadenreiche F\u00fchrung.", "tokens": ["Got\u00b7tes", "gna\u00b7den\u00b7rei\u00b7che", "F\u00fch\u00b7rung", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und dann legt' er mit Geduld", "tokens": ["Und", "dann", "legt'", "er", "mit", "Ge\u00b7duld"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Artikel in das Pult.", "tokens": ["Den", "Ar\u00b7ti\u00b7kel", "in", "das", "Pult", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.70": {"line.1": {"text": "Als es immer l\u00e4nger w\u00e4hrte", "tokens": ["Als", "es", "im\u00b7mer", "l\u00e4n\u00b7ger", "w\u00e4hr\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die Ungeduld sich mehrte,", "tokens": ["Und", "die", "Un\u00b7ge\u00b7duld", "sich", "mehr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Kam der Aar zum Storch heran,", "tokens": ["Kam", "der", "Aar", "zum", "Storch", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er haucht ihn grimmig an.", "tokens": ["Und", "er", "haucht", "ihn", "grim\u00b7mig", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.71": {"line.1": {"text": "Ob er wei\u00df, um was sich's handelt,", "tokens": ["Ob", "er", "wei\u00df", ",", "um", "was", "sich's", "han\u00b7delt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$,", "APPR", "PRELS", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df er so gem\u00e4chlich wandelt?", "tokens": ["Da\u00df", "er", "so", "ge\u00b7m\u00e4ch\u00b7lich", "wan\u00b7delt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob es nicht f\u00fcr Majest\u00e4t", "tokens": ["Ob", "es", "nicht", "f\u00fcr", "Ma\u00b7jes\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ganz bedeutend fixer geht?", "tokens": ["Ganz", "be\u00b7deu\u00b7tend", "fi\u00b7xer", "geht", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.72": {"line.1": {"text": "Fischt vielleicht man in den Binsen", "tokens": ["Fischt", "viel\u00b7leicht", "man", "in", "den", "Bin\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "PIS", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nur so nebenbei die Prinzen?", "tokens": ["Nur", "so", "ne\u00b7ben\u00b7bei", "die", "Prin\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Ob man nicht die Ehre kennt?", "tokens": ["Ob", "man", "nicht", "die", "Eh\u00b7re", "kennt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Himmel Herrgottsakrament!", "tokens": ["Him\u00b7mel", "Herr\u00b7gott\u00b7sa\u00b7kra\u00b7ment", "!"], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.73": {"line.1": {"text": "Als der Storch es ganz vernommen,", "tokens": ["Als", "der", "Storch", "es", "ganz", "ver\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist er zornig heimgekommen,", "tokens": ["Ist", "er", "zor\u00b7nig", "heim\u00b7ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und er sprach mit voller Kraft:", "tokens": ["Und", "er", "sprach", "mit", "vol\u00b7ler", "Kraft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbdieser Aar ist l\u00fcmmelhaft.\u00ab", "tokens": ["\u00bb", "die\u00b7ser", "Aar", "ist", "l\u00fcm\u00b7mel\u00b7haft", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PDAT", "NN", "VAFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.74": {"line.1": {"text": "\u00bbja, gewi\u00df, er ist ein Flegel,\u00ab", "tokens": ["\u00bb", "ja", ",", "ge\u00b7wi\u00df", ",", "er", "ist", "ein", "Fle\u00b7gel", ",", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PTKANT", "$,", "ADV", "$,", "PPER", "VAFIN", "ART", "NN", "$,", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sagt Frau St\u00f6rchin, \u00bbin der Regel", "tokens": ["Sagt", "Frau", "St\u00f6r\u00b7chin", ",", "\u00bb", "in", "der", "Re\u00b7gel"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["VVFIN", "NN", "NE", "$,", "$(", "APPR", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Kommt das bei den Gro\u00dfen vor,", "tokens": ["Kommt", "das", "bei", "den", "Gro\u00b7\u00dfen", "vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Du mu\u00dft klug sein, Adebor!", "tokens": ["Du", "mu\u00dft", "klug", "sein", ",", "A\u00b7de\u00b7bor", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VAINF", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.75": {"line.1": {"text": "Du bist fein, und deinesgleichen", "tokens": ["Du", "bist", "fein", ",", "und", "dei\u00b7nes\u00b7glei\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "KON", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kann mit Grobheit nichts erreichen,", "tokens": ["Kann", "mit", "Grob\u00b7heit", "nichts", "er\u00b7rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn er gibt's zur\u00fcck mit Zins.", "tokens": ["Denn", "er", "gibt's", "zu\u00b7r\u00fcck", "mit", "Zins", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Bring ihm doch den Zollernprinz!\u00ab", "tokens": ["Bring", "ihm", "doch", "den", "Zol\u00b7lern\u00b7prinz", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.76": {"line.1": {"text": "Und so kam's. Nach wenig Tagen", "tokens": ["Und", "so", "kam'", "s.", "Nach", "we\u00b7nig", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "NE", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat die Weihestund' geschlagen;", "tokens": ["Hat", "die", "Wei\u00b7he\u00b7stund'", "ge\u00b7schla\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In dem Hohenzollernschlo\u00df", "tokens": ["In", "dem", "Ho\u00b7hen\u00b7zol\u00b7lern\u00b7schlo\u00df"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Gab es einen Kaiserspro\u00df.", "tokens": ["Gab", "es", "ei\u00b7nen", "Kai\u00b7ser\u00b7spro\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.77": {"line.1": {"text": "Was die Witwe Probst gesehen,", "tokens": ["Was", "die", "Wit\u00b7we", "Probst", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist in Wirklichkeit geschehen,", "tokens": ["Ist", "in", "Wirk\u00b7lich\u00b7keit", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Herr Pastor Demmel sprach:", "tokens": ["Und", "Herr", "Pas\u00b7tor", "Dem\u00b7mel", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "NN", "VVFIN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "\u00bbdas Gebet hilft allgemach.\u00ab", "tokens": ["\u00bb", "das", "Ge\u00b7bet", "hilft", "all\u00b7ge\u00b7mach", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "ADV", "$.", "$("], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.78": {"line.1": {"text": "Und in Preu\u00dfen herrschte Wonne,", "tokens": ["Und", "in", "Preu\u00b7\u00dfen", "herrschte", "Won\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "ADJA", "NN", "$,"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und die Wolke wich der Sonne,", "tokens": ["Und", "die", "Wol\u00b7ke", "wich", "der", "Son\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Herrn Kulicke sein Ei", "tokens": ["Und", "Herrn", "Ku\u00b7li\u00b7cke", "sein", "Ei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "NN", "PPOSAT", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Hatte recht auch nebenbei.", "tokens": ["Hat\u00b7te", "recht", "auch", "ne\u00b7ben\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.79": {"line.1": {"text": "Und auch Goldstein freut's erheblich:", "tokens": ["Und", "auch", "Gold\u00b7stein", "freut's", "er\u00b7heb\u00b7lich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NN", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was er \u00fcber diesen Kn\u00e4blich", "tokens": ["Was", "er", "\u00fc\u00b7ber", "die\u00b7sen", "Kn\u00e4b\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "PDAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ahnungsvoll der \u00bbWoche\u00ab schickt,", "tokens": ["Ah\u00b7nungs\u00b7voll", "der", "\u00bb", "Wo\u00b7che", "\u00ab", "schickt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "ART", "$(", "NN", "$(", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ward bezahlt und fett gedr\u00fcckt.", "tokens": ["Ward", "be\u00b7zahlt", "und", "fett", "ge\u00b7dr\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.80": {"line.1": {"text": "Und die alten Gener\u00e4le", "tokens": ["Und", "die", "al\u00b7ten", "Ge\u00b7ne\u00b7r\u00e4\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schl\u00fcrften in die K\u00f6nigss\u00e4le,", "tokens": ["Schl\u00fcrf\u00b7ten", "in", "die", "K\u00f6\u00b7nigs\u00b7s\u00e4\u00b7le", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie fl\u00fcstern sich ins Ohr:", "tokens": ["Und", "sie", "fl\u00fcs\u00b7tern", "sich", "ins", "Ohr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PRF", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbhohenzollernblut h\u00e4lt vor.", "tokens": ["\u00bb", "ho\u00b7hen\u00b7zol\u00b7lern\u00b7blut", "h\u00e4lt", "vor", "."], "token_info": ["punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.81": {"line.1": {"text": "Det jibt wieder en Soldaten", "tokens": ["Det", "jibt", "wie\u00b7der", "en", "Sol\u00b7da\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jut jebaut und wohl jeraten,", "tokens": ["Jut", "je\u00b7baut", "und", "wohl", "je\u00b7ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "KON", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Immer stramm und immer stramm;", "tokens": ["Im\u00b7mer", "stramm", "und", "im\u00b7mer", "stramm", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADV", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "'s is en janz famoser Stamm.", "tokens": ["'s", "is", "en", "janz", "fa\u00b7mo\u00b7ser", "Stamm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "FM", "FM", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.82": {"line.1": {"text": "Tja, da kann woll jar nischt jegen;", "tokens": ["Tja", ",", "da", "kann", "woll", "jar", "nischt", "je\u00b7gen", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "VMFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Immer fix mit Kindersegen!", "tokens": ["Im\u00b7mer", "fix", "mit", "Kin\u00b7der\u00b7se\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Heirat und gleich schwuppdi bum! \u2013", "tokens": ["Hei\u00b7rat", "und", "gleich", "schwupp\u00b7di", "bum", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "KON", "ADV", "FM", "FM", "$.", "$("], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": "\u2013 Pst! Man dreht sich nach uns um.\u00ab", "tokens": ["\u2013", "Pst", "!", "Man", "dreht", "sich", "nach", "uns", "um", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM.xy", "$.", "PIS", "VVFIN", "PRF", "APPR", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.83": {"line.1": {"text": "Auch zwei alte Kammerchaisen", "tokens": ["Auch", "zwei", "al\u00b7te", "Kam\u00b7mer\u00b7chai\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "CARD", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sind voll Wonnigkeit gewesen,", "tokens": ["Sind", "voll", "Won\u00b7nig\u00b7keit", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "NN", "VAPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie pispern hinterr\u00fccks", "tokens": ["Und", "sie", "pis\u00b7pern", "hin\u00b7ter\u00b7r\u00fccks"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00dcber diesen Fall des Gl\u00fccks.", "tokens": ["\u00dc\u00b7ber", "die\u00b7sen", "Fall", "des", "Gl\u00fccks", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.84": {"line.1": {"text": "\u00bbha, mon Dieu! Und so was Rundes,", "tokens": ["\u00bb", "ha", ",", "mon", "Di\u00b7eu", "!", "Und", "so", "was", "Run\u00b7des", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "NE", "NE", "$.", "KON", "ADV", "PWS", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Dickes, Fettes und Gesundes!", "tokens": ["Di\u00b7ckes", ",", "Fet\u00b7tes", "und", "Ge\u00b7sun\u00b7des", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Teure Gr\u00e4fin, sehn Sie dies?", "tokens": ["Teu\u00b7re", "Gr\u00e4\u00b7fin", ",", "sehn", "Sie", "dies", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "PPER", "PDS", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie entz\u00fcckend! Hoh! Wie s\u00fc\u00df!\u00ab", "tokens": ["Wie", "ent\u00b7z\u00fc\u00b7ckend", "!", "Hoh", "!", "Wie", "s\u00fc\u00df", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVPP", "$.", "NE", "$.", "PWAV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.85": {"line.1": {"text": "\u00bbhat es schon?\u00ab \u2013 \u00bbGewi\u00df, Komtesse!", "tokens": ["\u00bb", "hat", "es", "schon", "?", "\u00ab", "\u2013", "\u00bb", "Ge\u00b7wi\u00df", ",", "Kom\u00b7tes\u00b7se", "!"], "token_info": ["punct", "word", "word", "word", "punct", "punct", "punct", "punct", "word", "punct", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ADV", "$.", "$(", "$(", "$(", "PTKANT", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In dem Bettchen war noch N\u00e4sse.\u00ab", "tokens": ["In", "dem", "Bett\u00b7chen", "war", "noch", "N\u00e4s\u00b7se", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "ADV", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbteure Gr\u00e4fin sahen dies?\u00ab", "tokens": ["\u00bb", "teu\u00b7re", "Gr\u00e4\u00b7fin", "sa\u00b7hen", "dies", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "VVFIN", "PDS", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbnu nat\u00fcrlich!\u00ab \u2013 \u00bbHoh, wie s\u00fc\u00df!\u00ab", "tokens": ["\u00bb", "nu", "na\u00b7t\u00fcr\u00b7lich", "!", "\u00ab", "\u2013", "\u00bb", "Hoh", ",", "wie", "s\u00fc\u00df", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADV", "$.", "$(", "$(", "$(", "NE", "$,", "PWAV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.86": {"line.1": {"text": "Preu\u00dfens ganze K\u00f6nigstreue", "tokens": ["Preu\u00b7\u00dfens", "gan\u00b7ze", "K\u00f6\u00b7nigs\u00b7treu\u00b7e"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zeigte heute sich aufs neue,", "tokens": ["Zeig\u00b7te", "heu\u00b7te", "sich", "aufs", "neu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PRF", "APPRART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie erschien im Volksgedr\u00e4ng", "tokens": ["Sie", "er\u00b7schien", "im", "Volks\u00b7ge\u00b7dr\u00e4ng"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und im Frack und Eskarp\u00e4ng.", "tokens": ["Und", "im", "Frack", "und", "Es\u00b7kar\u00b7p\u00e4ng", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "KON", "NN", "$."], "meter": "+-+-+--", "measure": "unknown.measure.tri"}}, "stanza.87": {"line.1": {"text": "Unter ihrem Schiffhut schworen", "tokens": ["Un\u00b7ter", "ih\u00b7rem", "Schiff\u00b7hut", "schwo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Altgediente Direktoren,", "tokens": ["Alt\u00b7ge\u00b7dien\u00b7te", "Di\u00b7rek\u00b7to\u00b7ren", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df sie auch dem neuen Kind", "tokens": ["Da\u00df", "sie", "auch", "dem", "neu\u00b7en", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcrchterlich ergeben sind.", "tokens": ["F\u00fcrch\u00b7ter\u00b7lich", "er\u00b7ge\u00b7ben", "sind", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.88": {"line.1": {"text": "Richter, Schreiber, Staatsanw\u00e4lte", "tokens": ["Rich\u00b7ter", ",", "Schrei\u00b7ber", ",", "Staats\u00b7an\u00b7w\u00e4l\u00b7te"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "NN", "$,", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Legen ab die Herzensk\u00e4lte,", "tokens": ["Le\u00b7gen", "ab", "die", "Her\u00b7zens\u00b7k\u00e4l\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00d6ffnen ihre enge Brust", "tokens": ["\u00d6ff\u00b7nen", "ih\u00b7re", "en\u00b7ge", "Brust"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Froher Untertanenlust.", "tokens": ["Fro\u00b7her", "Un\u00b7ter\u00b7ta\u00b7nen\u00b7lust", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.89": {"line.1": {"text": "Und in manchem Sekret\u00e4re", "tokens": ["Und", "in", "man\u00b7chem", "Se\u00b7kre\u00b7t\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lag die Ahnung heut, er w\u00e4re", "tokens": ["Lag", "die", "Ah\u00b7nung", "heut", ",", "er", "w\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "ART", "NN", "ADV", "$,", "PPER", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu Verschiedenem imstand", "tokens": ["Zu", "Ver\u00b7schie\u00b7de\u00b7nem", "ims\u00b7tand"], "token_info": ["word", "word", "word"], "pos": ["APPR", "NE", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr sein teures Vaterland.", "tokens": ["F\u00fcr", "sein", "teu\u00b7res", "Va\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.90": {"line.1": {"text": "Auch in den Kasernen waren", "tokens": ["Auch", "in", "den", "Ka\u00b7ser\u00b7nen", "wa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Aufgestellt Rekrutenscharen.", "tokens": ["Auf\u00b7ge\u00b7stellt", "Rek\u00b7ru\u00b7ten\u00b7scha\u00b7ren", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Heute wurde nicht geschimpft,", "tokens": ["Heu\u00b7te", "wur\u00b7de", "nicht", "ge\u00b7schimpft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sondern Treue eingeimpft.", "tokens": ["Son\u00b7dern", "Treu\u00b7e", "ein\u00b7ge\u00b7impft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.91": {"line.1": {"text": "Da\u00df der Tag auch den Soldaten", "tokens": ["Da\u00df", "der", "Tag", "auch", "den", "Sol\u00b7da\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Heilig bleibe, gab es Braten.", "tokens": ["Hei\u00b7lig", "blei\u00b7be", ",", "gab", "es", "Bra\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$,", "VVFIN", "PPER", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Feiernd seinen Herrscherstamm", "tokens": ["Fei\u00b7ernd", "sei\u00b7nen", "Herr\u00b7scher\u00b7stamm"], "token_info": ["word", "word", "word"], "pos": ["ADJD", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "A\u00df ein jeder hundert Gramm.", "tokens": ["A\u00df", "ein", "je\u00b7der", "hun\u00b7dert", "Gram\u00b7m."], "token_info": ["word", "word", "word", "word", "abbreviation"], "pos": ["VVFIN", "ART", "PIAT", "CARD", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.92": {"line.1": {"text": "Kurz und gut, im Lande Preu\u00dfen", "tokens": ["Kurz", "und", "gut", ",", "im", "Lan\u00b7de", "Preu\u00b7\u00dfen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "KON", "ADJD", "$,", "APPRART", "NN", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wollt' ein jeder sich beflei\u00dfen,", "tokens": ["Wollt'", "ein", "je\u00b7der", "sich", "be\u00b7flei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "PIAT", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Tag auch feierlich", "tokens": ["Da\u00df", "der", "Tag", "auch", "fei\u00b7er\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und mit W\u00fcrdigkeit verstrich.", "tokens": ["Und", "mit", "W\u00fcr\u00b7dig\u00b7keit", "ver\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.93": {"line.1": {"text": "Doch wie waren die Gef\u00fchle", "tokens": ["Doch", "wie", "wa\u00b7ren", "die", "Ge\u00b7f\u00fch\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "VAFIN", "ART", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Weiter s\u00fcdlich? Ziemlich k\u00fchle.", "tokens": ["Wei\u00b7ter", "s\u00fcd\u00b7lich", "?", "Ziem\u00b7lich", "k\u00fch\u00b7le", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "NE", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Oben h\u00f6flich, aber flau,", "tokens": ["O\u00b7ben", "h\u00f6f\u00b7lich", ",", "a\u00b7ber", "flau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Unten ganz betr\u00e4chtlich mau.", "tokens": ["Un\u00b7ten", "ganz", "be\u00b7tr\u00e4cht\u00b7lich", "mau", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.94": {"line.1": {"text": "Der Fassadenmaurer Huber", "tokens": ["Der", "Fas\u00b7sa\u00b7den\u00b7mau\u00b7rer", "Hu\u00b7ber"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "NE"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Stand an seinem M\u00f6rtelzuber;", "tokens": ["Stand", "an", "sei\u00b7nem", "M\u00f6r\u00b7tel\u00b7zu\u00b7ber", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als man ihm die Nachricht bracht',", "tokens": ["Als", "man", "ihm", "die", "Nach\u00b7richt", "bracht'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat er sich nichts draus gemacht.", "tokens": ["Hat", "er", "sich", "nichts", "draus", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "PIS", "PAV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.95": {"line.1": {"text": "Holte seine Tabakflasche", "tokens": ["Hol\u00b7te", "sei\u00b7ne", "Ta\u00b7bak\u00b7fla\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aus der linken Westentasche,", "tokens": ["Aus", "der", "lin\u00b7ken", "Wes\u00b7ten\u00b7ta\u00b7sche", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sagte: \u00bbWas? A Preu\u00df? A Prinz?", "tokens": ["Sag\u00b7te", ":", "\u00bb", "Was", "?", "A", "Preu\u00df", "?", "A", "Prinz", "?"], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$.", "$(", "PWS", "$.", "NE", "NE", "$.", "NE", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ja, was k\u00fcmmert denn d\u00f6s ins?", "tokens": ["Ja", ",", "was", "k\u00fcm\u00b7mert", "denn", "d\u00f6s", "ins", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWS", "VVFIN", "ADV", "ADV", "APPRART", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.96": {"line.1": {"text": "D\u00f6s bek\u00fcmmert ins ganz wenig;", "tokens": ["D\u00f6s", "be\u00b7k\u00fcm\u00b7mert", "ins", "ganz", "we\u00b7nig", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "ADV", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der werd halt amal a K\u00f6nig", "tokens": ["Der", "werd", "halt", "a\u00b7mal", "a", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "NE", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bei die Preu\u00dfen. Net bei ins.", "tokens": ["Bei", "die", "Preu\u00b7\u00dfen", ".", "Net", "bei", "ins", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "NE", "APPR", "APPRART", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "So? Da ham s' an neuen Prinz?\u00ab", "tokens": ["So", "?", "Da", "ham", "s'", "an", "neu\u00b7en", "Prinz", "?", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "$.", "KOUS", "NE", "NE", "APPR", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}