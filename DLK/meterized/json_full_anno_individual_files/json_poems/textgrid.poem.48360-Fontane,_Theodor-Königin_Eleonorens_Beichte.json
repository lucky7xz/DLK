{"textgrid.poem.48360": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "K\u00f6nigin Eleonorens Beichte", "genre": "verse", "period": "N.A.", "pub_year": 1855, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Todkrank lag K\u00f6nigin Eleonor',", "tokens": ["Tod\u00b7krank", "lag", "K\u00f6\u00b7ni\u00b7gin", "E\u00b7le\u00b7o\u00b7nor'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "NN", "NE", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Sie wu\u00dfte, da\u00df schlecht es st\u00fcnde:", "tokens": ["Sie", "wu\u00df\u00b7te", ",", "da\u00df", "schlecht", "es", "st\u00fcn\u00b7de", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "VVFIN", "PPER", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbschickt mir zwei M\u00f6nche von Frankreich her,", "tokens": ["\u00bb", "schickt", "mir", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "CARD", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Da\u00df ich beichte meine S\u00fcnde.\u00ab", "tokens": ["Da\u00df", "ich", "beich\u00b7te", "mei\u00b7ne", "S\u00fcn\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Der K\u00f6nig rief seine Haushalt-Lords,", "tokens": ["Der", "K\u00f6\u00b7nig", "rief", "sei\u00b7ne", "Haus\u00b7hal\u00b7t\u00b7L\u00b7ords", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seinen ersten und seinen zweiten:", "tokens": ["Sei\u00b7nen", "ers\u00b7ten", "und", "sei\u00b7nen", "zwei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "KON", "PPOSAT", "ADJA", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}, "line.4": {"text": "Lord Marschall, du sollst mich begleiten.\u00ab", "tokens": ["Lord", "Mar\u00b7schall", ",", "du", "sollst", "mich", "be\u00b7glei\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "NE", "$,", "PPER", "VMFIN", "PRF", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.3": {"line.1": {"text": "\u00bblord Marschall, steh auf, ich verpf\u00e4nde mein Wort", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "steh", "auf", ",", "ich", "ver\u00b7pf\u00e4n\u00b7de", "mein", "Wort"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "NE", "NE", "$,", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Woll' mir zuvor versprechen,", "tokens": ["Woll'", "mir", "zu\u00b7vor", "ver\u00b7spre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was auch die K\u00f6nigin beichten mag,", "tokens": ["Was", "auch", "die", "K\u00f6\u00b7ni\u00b7gin", "beich\u00b7ten", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "An mir es nimmer zu r\u00e4chen.\u00ab", "tokens": ["An", "mir", "es", "nim\u00b7mer", "zu", "r\u00e4\u00b7chen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "PPER", "ADV", "PTKZU", "VVINF", "$.", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.4": {"line.1": {"text": "\u00bblord Marschall, steh auf, ich verpf\u00e4nde mein Wort", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "steh", "auf", ",", "ich", "ver\u00b7pf\u00e4n\u00b7de", "mein", "Wort"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "NE", "NE", "$,", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Und ganz England zu meinen F\u00fc\u00dfen,", "tokens": ["Und", "ganz", "En\u00b7gland", "zu", "mei\u00b7nen", "F\u00fc\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was auch die K\u00f6nigin beichten mag,", "tokens": ["Was", "auch", "die", "K\u00f6\u00b7ni\u00b7gin", "beich\u00b7ten", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Du sollst es nimmer b\u00fc\u00dfen.", "tokens": ["Du", "sollst", "es", "nim\u00b7mer", "b\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Wir legen an ein m\u00f6nchisch Gewand-", "tokens": ["Wir", "le\u00b7gen", "an", "ein", "m\u00f6n\u00b7chisch", "Ge\u00b7wan\u00b7d"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJD", "TRUNC"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In Kapuze und grauem Kleide,", "tokens": ["In", "Ka\u00b7pu\u00b7ze", "und", "grau\u00b7em", "Klei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "So kommen wir betend von Frankreich her", "tokens": ["So", "kom\u00b7men", "wir", "be\u00b7tend", "von", "Fran\u00b7kreich", "her"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "VVPP", "APPR", "NE", "APZR"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und h\u00f6ren die Beichte beide.\u00ab", "tokens": ["Und", "h\u00f6\u00b7ren", "die", "Beich\u00b7te", "bei\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PIS", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Sie legten an ein m\u00f6nchisch Gewand;", "tokens": ["Sie", "leg\u00b7ten", "an", "ein", "m\u00f6n\u00b7chisch", "Ge\u00b7wand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Als gen Whitehall sie schritten,", "tokens": ["Als", "gen", "Whi\u00b7te\u00b7hall", "sie", "schrit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Des Volkes Menge begleitete sie", "tokens": ["Des", "Vol\u00b7kes", "Men\u00b7ge", "be\u00b7glei\u00b7te\u00b7te", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "PPER"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit Kniefall und frommen Bitten.", "tokens": ["Mit", "Knie\u00b7fall", "und", "from\u00b7men", "Bit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Sie traten hin vor die K\u00f6nigin", "tokens": ["Sie", "tra\u00b7ten", "hin", "vor", "die", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und sprachen mit H\u00e4ndefalten:", "tokens": ["Und", "spra\u00b7chen", "mit", "H\u00e4n\u00b7de\u00b7fal\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbvergib, es haben Wetter und Wind", "tokens": ["\u00bb", "ver\u00b7gib", ",", "es", "ha\u00b7ben", "Wet\u00b7ter", "und", "Wind"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "$,", "PPER", "VAFIN", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Unsren Dienst zur\u00fcckgehalten.\u00ab", "tokens": ["Un\u00b7sren", "Dienst", "zu\u00b7r\u00fcck\u00b7ge\u00b7hal\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "\u00bbwenn ihr zwei M\u00f6nche von Frankreich seid,", "tokens": ["\u00bb", "wenn", "ihr", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "seid", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "CARD", "NN", "APPR", "NE", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Kann ich euer S\u00e4umen nicht schelten;", "tokens": ["Kann", "ich", "eu\u00b7er", "S\u00e4u\u00b7men", "nicht", "schel\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPOSAT", "NN", "PTKNEG", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Wenn ihr zwei englische M\u00f6nche seid,", "tokens": ["Wenn", "ihr", "zwei", "eng\u00b7li\u00b7sche", "M\u00f6n\u00b7che", "seid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "ADJA", "NN", "VAFIN", "$,"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sollt ihr's am Leben entgelten.\u00ab", "tokens": ["Sollt", "ih\u00b7r's", "am", "Le\u00b7ben", "ent\u00b7gel\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PIS", "APPRART", "NN", "VVINF", "$.", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "\u00bbwir sind zwei M\u00f6nche von Frankreich her,", "tokens": ["\u00bb", "wir", "sind", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "CARD", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Drum beichte ohne Bangen,", "tokens": ["Drum", "beich\u00b7te", "oh\u00b7ne", "Ban\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir haben noch keine Messe geh\u00f6rt,", "tokens": ["Wir", "ha\u00b7ben", "noch", "kei\u00b7ne", "Mes\u00b7se", "ge\u00b7h\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Seit wir zu Schiff gegangen.\u00ab", "tokens": ["Seit", "wir", "zu", "Schiff", "ge\u00b7gan\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "\u00bbdie erste S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "ers\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hat andre gro\u00df gezogen:", "tokens": ["Hat", "and\u00b7re", "gro\u00df", "ge\u00b7zo\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Lord Marschall hab' ich zuvor geliebt", "tokens": ["Lord", "Mar\u00b7schall", "hab'", "ich", "zu\u00b7vor", "ge\u00b7liebt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und den K\u00f6nig hab' ich betrogen.\u00ab", "tokens": ["Und", "den", "K\u00f6\u00b7nig", "hab'", "ich", "be\u00b7tro\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "VVPP", "$.", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.11": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "\u00bbdie zweite S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "zwei\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die will ich zum andern bekennen,", "tokens": ["Die", "will", "ich", "zum", "an\u00b7dern", "be\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "APPRART", "PIS", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ich mischt' einen Trunk, der sollte mich rasch", "tokens": ["Ich", "mischt'", "ei\u00b7nen", "Trunk", ",", "der", "soll\u00b7te", "mich", "rasch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PRELS", "VMFIN", "PRF", "ADJD"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von K\u00f6nig Heinrich trennen.\u00ab", "tokens": ["Von", "K\u00f6\u00b7nig", "Hein\u00b7rich", "tren\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NE", "NE", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "\u00bbdie dritte S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "drit\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die will zum dritten ich beichten,", "tokens": ["Die", "will", "zum", "drit\u00b7ten", "ich", "beich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "APPRART", "ADJA", "PPER", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Meine H\u00e4nde waren's, die Becher und Gift", "tokens": ["Mei\u00b7ne", "H\u00e4n\u00b7de", "wa\u00b7ren's", ",", "die", "Be\u00b7cher", "und", "Gift"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "$,", "ART", "NN", "KON", "NN"], "meter": "+-+----+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "An Rosamunden reichten.\u00ab", "tokens": ["An", "Ro\u00b7sa\u00b7mun\u00b7den", "reich\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "\u00bbseht in der Halle den Knaben dort,", "tokens": ["\u00bb", "seht", "in", "der", "Hal\u00b7le", "den", "Kna\u00b7ben", "dort", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "ADV", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Er spielt mit dem Federballe,", "tokens": ["Er", "spielt", "mit", "dem", "Fe\u00b7der\u00b7bal\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Das ist Lord Marschalls \u00e4ltester Sohn,", "tokens": ["Das", "ist", "Lord", "Mar\u00b7schalls", "\u00e4l\u00b7tes\u00b7ter", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "NE", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und ich lieb' ihn mehr als alle.", "tokens": ["Und", "ich", "lieb'", "ihn", "mehr", "als", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "KOUS", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Und seht in der Halle den zweiten dort,", "tokens": ["Und", "seht", "in", "der", "Hal\u00b7le", "den", "zwei\u00b7ten", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ART", "ADJA", "ADV", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Er hascht nach dem fliegenden Balle,", "tokens": ["Er", "hascht", "nach", "dem", "flie\u00b7gen\u00b7den", "Bal\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Das ist K\u00f6nig Heinrichs j\u00fcngster Sohn,", "tokens": ["Das", "ist", "K\u00f6\u00b7nig", "Hein\u00b7richs", "j\u00fcngs\u00b7ter", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "NE", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Und ich ha\u00df' ihn mehr als alle.", "tokens": ["Und", "ich", "ha\u00df'", "ihn", "mehr", "als", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "KOUS", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Er hat einen Kopf wie ein Warwick-Stier", "tokens": ["Er", "hat", "ei\u00b7nen", "Kopf", "wie", "ein", "Wa\u00b7rwick\u00b7S\u00b7tier"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "KOKOM", "ART", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und ist t\u00e4ppisch wie ein B\u00e4r\u00ab;", "tokens": ["Und", "ist", "t\u00e4p\u00b7pisch", "wie", "ein", "B\u00e4r", "\u00ab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADJD", "KOKOM", "ART", "NN", "$(", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "\u00bbmag sein\u00ab, rief K\u00f6nig Heinrich da,", "tokens": ["\u00bb", "mag", "sein", "\u00ab", ",", "rief", "K\u00f6\u00b7nig", "Hein\u00b7rich", "da", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPOSAT", "$(", "$,", "VVFIN", "NE", "NE", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbich lieb' ihn desto mehr.\u00ab", "tokens": ["\u00bb", "ich", "lieb'", "ihn", "des\u00b7to", "mehr", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Ab ri\u00df er Kapuze und M\u00f6nchsgewand,", "tokens": ["Ab", "ri\u00df", "er", "Ka\u00b7pu\u00b7ze", "und", "M\u00f6n\u00b7chs\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "+---+--++-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Sein Antlitz war blutrot,", "tokens": ["Sein", "Ant\u00b7litz", "war", "blut\u00b7rot", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Leonore schrie auf und rang die H\u00e4nd' \u2013", "tokens": ["Le\u00b7o\u00b7no\u00b7re", "schrie", "auf", "und", "rang", "die", "H\u00e4nd'", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKVZ", "KON", "VVFIN", "ART", "NN", "$("], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ihre Beichte war ihr Tod.", "tokens": ["Ih\u00b7re", "Beich\u00b7te", "war", "ihr", "Tod", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "Der K\u00f6nig \u00fcber die Schulter sah,", "tokens": ["Der", "K\u00f6\u00b7nig", "\u00fc\u00b7ber", "die", "Schul\u00b7ter", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Vielgrimmig sah er drein:", "tokens": ["Viel\u00b7grim\u00b7mig", "sah", "er", "drein", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bblord Marschall, w\u00e4r's nicht um mein Wort,", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "w\u00e4r's", "nicht", "um", "mein", "Wort", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "NE", "$,", "VAFIN", "PTKNEG", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Du solltest gehangen sein.\u00ab", "tokens": ["Du", "soll\u00b7test", "ge\u00b7han\u00b7gen", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "VVPP", "VAINF", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.21": {"line.1": {"text": "Todkrank lag K\u00f6nigin Eleonor',", "tokens": ["Tod\u00b7krank", "lag", "K\u00f6\u00b7ni\u00b7gin", "E\u00b7le\u00b7o\u00b7nor'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "NN", "NE", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Sie wu\u00dfte, da\u00df schlecht es st\u00fcnde:", "tokens": ["Sie", "wu\u00df\u00b7te", ",", "da\u00df", "schlecht", "es", "st\u00fcn\u00b7de", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "VVFIN", "PPER", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbschickt mir zwei M\u00f6nche von Frankreich her,", "tokens": ["\u00bb", "schickt", "mir", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "CARD", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Da\u00df ich beichte meine S\u00fcnde.\u00ab", "tokens": ["Da\u00df", "ich", "beich\u00b7te", "mei\u00b7ne", "S\u00fcn\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Der K\u00f6nig rief seine Haushalt-Lords,", "tokens": ["Der", "K\u00f6\u00b7nig", "rief", "sei\u00b7ne", "Haus\u00b7hal\u00b7t\u00b7L\u00b7ords", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seinen ersten und seinen zweiten:", "tokens": ["Sei\u00b7nen", "ers\u00b7ten", "und", "sei\u00b7nen", "zwei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "KON", "PPOSAT", "ADJA", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}, "line.4": {"text": "Lord Marschall, du sollst mich begleiten.\u00ab", "tokens": ["Lord", "Mar\u00b7schall", ",", "du", "sollst", "mich", "be\u00b7glei\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "NE", "$,", "PPER", "VMFIN", "PRF", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.23": {"line.1": {"text": "\u00bblord Marschall, steh auf, ich verpf\u00e4nde mein Wort", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "steh", "auf", ",", "ich", "ver\u00b7pf\u00e4n\u00b7de", "mein", "Wort"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "NE", "NE", "$,", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Woll' mir zuvor versprechen,", "tokens": ["Woll'", "mir", "zu\u00b7vor", "ver\u00b7spre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was auch die K\u00f6nigin beichten mag,", "tokens": ["Was", "auch", "die", "K\u00f6\u00b7ni\u00b7gin", "beich\u00b7ten", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "An mir es nimmer zu r\u00e4chen.\u00ab", "tokens": ["An", "mir", "es", "nim\u00b7mer", "zu", "r\u00e4\u00b7chen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "PPER", "ADV", "PTKZU", "VVINF", "$.", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.24": {"line.1": {"text": "\u00bblord Marschall, steh auf, ich verpf\u00e4nde mein Wort", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "steh", "auf", ",", "ich", "ver\u00b7pf\u00e4n\u00b7de", "mein", "Wort"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "NE", "NE", "$,", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Und ganz England zu meinen F\u00fc\u00dfen,", "tokens": ["Und", "ganz", "En\u00b7gland", "zu", "mei\u00b7nen", "F\u00fc\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was auch die K\u00f6nigin beichten mag,", "tokens": ["Was", "auch", "die", "K\u00f6\u00b7ni\u00b7gin", "beich\u00b7ten", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Du sollst es nimmer b\u00fc\u00dfen.", "tokens": ["Du", "sollst", "es", "nim\u00b7mer", "b\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Wir legen an ein m\u00f6nchisch Gewand-", "tokens": ["Wir", "le\u00b7gen", "an", "ein", "m\u00f6n\u00b7chisch", "Ge\u00b7wan\u00b7d"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJD", "TRUNC"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In Kapuze und grauem Kleide,", "tokens": ["In", "Ka\u00b7pu\u00b7ze", "und", "grau\u00b7em", "Klei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "So kommen wir betend von Frankreich her", "tokens": ["So", "kom\u00b7men", "wir", "be\u00b7tend", "von", "Fran\u00b7kreich", "her"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "VVPP", "APPR", "NE", "APZR"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und h\u00f6ren die Beichte beide.\u00ab", "tokens": ["Und", "h\u00f6\u00b7ren", "die", "Beich\u00b7te", "bei\u00b7de", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PIS", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.26": {"line.1": {"text": "Sie legten an ein m\u00f6nchisch Gewand;", "tokens": ["Sie", "leg\u00b7ten", "an", "ein", "m\u00f6n\u00b7chisch", "Ge\u00b7wand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Als gen Whitehall sie schritten,", "tokens": ["Als", "gen", "Whi\u00b7te\u00b7hall", "sie", "schrit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Des Volkes Menge begleitete sie", "tokens": ["Des", "Vol\u00b7kes", "Men\u00b7ge", "be\u00b7glei\u00b7te\u00b7te", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "PPER"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit Kniefall und frommen Bitten.", "tokens": ["Mit", "Knie\u00b7fall", "und", "from\u00b7men", "Bit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.27": {"line.1": {"text": "Sie traten hin vor die K\u00f6nigin", "tokens": ["Sie", "tra\u00b7ten", "hin", "vor", "die", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und sprachen mit H\u00e4ndefalten:", "tokens": ["Und", "spra\u00b7chen", "mit", "H\u00e4n\u00b7de\u00b7fal\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbvergib, es haben Wetter und Wind", "tokens": ["\u00bb", "ver\u00b7gib", ",", "es", "ha\u00b7ben", "Wet\u00b7ter", "und", "Wind"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "$,", "PPER", "VAFIN", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Unsren Dienst zur\u00fcckgehalten.\u00ab", "tokens": ["Un\u00b7sren", "Dienst", "zu\u00b7r\u00fcck\u00b7ge\u00b7hal\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "\u00bbwenn ihr zwei M\u00f6nche von Frankreich seid,", "tokens": ["\u00bb", "wenn", "ihr", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "seid", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "CARD", "NN", "APPR", "NE", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Kann ich euer S\u00e4umen nicht schelten;", "tokens": ["Kann", "ich", "eu\u00b7er", "S\u00e4u\u00b7men", "nicht", "schel\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPOSAT", "NN", "PTKNEG", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Wenn ihr zwei englische M\u00f6nche seid,", "tokens": ["Wenn", "ihr", "zwei", "eng\u00b7li\u00b7sche", "M\u00f6n\u00b7che", "seid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "ADJA", "NN", "VAFIN", "$,"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sollt ihr's am Leben entgelten.\u00ab", "tokens": ["Sollt", "ih\u00b7r's", "am", "Le\u00b7ben", "ent\u00b7gel\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PIS", "APPRART", "NN", "VVINF", "$.", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.29": {"line.1": {"text": "\u00bbwir sind zwei M\u00f6nche von Frankreich her,", "tokens": ["\u00bb", "wir", "sind", "zwei", "M\u00f6n\u00b7che", "von", "Fran\u00b7kreich", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "CARD", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Drum beichte ohne Bangen,", "tokens": ["Drum", "beich\u00b7te", "oh\u00b7ne", "Ban\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir haben noch keine Messe geh\u00f6rt,", "tokens": ["Wir", "ha\u00b7ben", "noch", "kei\u00b7ne", "Mes\u00b7se", "ge\u00b7h\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Seit wir zu Schiff gegangen.\u00ab", "tokens": ["Seit", "wir", "zu", "Schiff", "ge\u00b7gan\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.30": {"line.1": {"text": "\u00bbdie erste S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "ers\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hat andre gro\u00df gezogen:", "tokens": ["Hat", "and\u00b7re", "gro\u00df", "ge\u00b7zo\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Lord Marschall hab' ich zuvor geliebt", "tokens": ["Lord", "Mar\u00b7schall", "hab'", "ich", "zu\u00b7vor", "ge\u00b7liebt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und den K\u00f6nig hab' ich betrogen.\u00ab", "tokens": ["Und", "den", "K\u00f6\u00b7nig", "hab'", "ich", "be\u00b7tro\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "VVPP", "$.", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.31": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.32": {"line.1": {"text": "\u00bbdie zweite S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "zwei\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die will ich zum andern bekennen,", "tokens": ["Die", "will", "ich", "zum", "an\u00b7dern", "be\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "APPRART", "PIS", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ich mischt' einen Trunk, der sollte mich rasch", "tokens": ["Ich", "mischt'", "ei\u00b7nen", "Trunk", ",", "der", "soll\u00b7te", "mich", "rasch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PRELS", "VMFIN", "PRF", "ADJD"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von K\u00f6nig Heinrich trennen.\u00ab", "tokens": ["Von", "K\u00f6\u00b7nig", "Hein\u00b7rich", "tren\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NE", "NE", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.34": {"line.1": {"text": "\u00bbdie dritte S\u00fcnde, die ich beging,", "tokens": ["\u00bb", "die", "drit\u00b7te", "S\u00fcn\u00b7de", ",", "die", "ich", "be\u00b7ging", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die will zum dritten ich beichten,", "tokens": ["Die", "will", "zum", "drit\u00b7ten", "ich", "beich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "APPRART", "ADJA", "PPER", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Meine H\u00e4nde waren's, die Becher und Gift", "tokens": ["Mei\u00b7ne", "H\u00e4n\u00b7de", "wa\u00b7ren's", ",", "die", "Be\u00b7cher", "und", "Gift"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "$,", "ART", "NN", "KON", "NN"], "meter": "+-+----+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "An Rosamunden reichten.\u00ab", "tokens": ["An", "Ro\u00b7sa\u00b7mun\u00b7den", "reich\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.35": {"line.1": {"text": "\u00bbeine schwere S\u00fcnde! ich l\u00f6se sie doch", "tokens": ["\u00bb", "ei\u00b7ne", "schwe\u00b7re", "S\u00fcn\u00b7de", "!", "ich", "l\u00f6\u00b7se", "sie", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In Gottes und Christi Namen.\u00ab", "tokens": ["In", "Got\u00b7tes", "und", "Chris\u00b7ti", "Na\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NE", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der K\u00f6nig spricht's, Lord Marschall bebt", "tokens": ["Der", "K\u00f6\u00b7nig", "spricht's", ",", "Lord", "Mar\u00b7schall", "bebt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und murmelt: \u00bbAmen, Amen.\u00ab", "tokens": ["Und", "mur\u00b7melt", ":", "\u00bb", "A\u00b7men", ",", "A\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.36": {"line.1": {"text": "\u00bbseht in der Halle den Knaben dort,", "tokens": ["\u00bb", "seht", "in", "der", "Hal\u00b7le", "den", "Kna\u00b7ben", "dort", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "ADV", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Er spielt mit dem Federballe,", "tokens": ["Er", "spielt", "mit", "dem", "Fe\u00b7der\u00b7bal\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Das ist Lord Marschalls \u00e4ltester Sohn,", "tokens": ["Das", "ist", "Lord", "Mar\u00b7schalls", "\u00e4l\u00b7tes\u00b7ter", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "NE", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und ich lieb' ihn mehr als alle.", "tokens": ["Und", "ich", "lieb'", "ihn", "mehr", "als", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "KOUS", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.37": {"line.1": {"text": "Und seht in der Halle den zweiten dort,", "tokens": ["Und", "seht", "in", "der", "Hal\u00b7le", "den", "zwei\u00b7ten", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ART", "ADJA", "ADV", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Er hascht nach dem fliegenden Balle,", "tokens": ["Er", "hascht", "nach", "dem", "flie\u00b7gen\u00b7den", "Bal\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Das ist K\u00f6nig Heinrichs j\u00fcngster Sohn,", "tokens": ["Das", "ist", "K\u00f6\u00b7nig", "Hein\u00b7richs", "j\u00fcngs\u00b7ter", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "NE", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Und ich ha\u00df' ihn mehr als alle.", "tokens": ["Und", "ich", "ha\u00df'", "ihn", "mehr", "als", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "KOUS", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.38": {"line.1": {"text": "Er hat einen Kopf wie ein Warwick-Stier", "tokens": ["Er", "hat", "ei\u00b7nen", "Kopf", "wie", "ein", "Wa\u00b7rwick\u00b7S\u00b7tier"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "KOKOM", "ART", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und ist t\u00e4ppisch wie ein B\u00e4r\u00ab;", "tokens": ["Und", "ist", "t\u00e4p\u00b7pisch", "wie", "ein", "B\u00e4r", "\u00ab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADJD", "KOKOM", "ART", "NN", "$(", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "\u00bbmag sein\u00ab, rief K\u00f6nig Heinrich da,", "tokens": ["\u00bb", "mag", "sein", "\u00ab", ",", "rief", "K\u00f6\u00b7nig", "Hein\u00b7rich", "da", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPOSAT", "$(", "$,", "VVFIN", "NE", "NE", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbich lieb' ihn desto mehr.\u00ab", "tokens": ["\u00bb", "ich", "lieb'", "ihn", "des\u00b7to", "mehr", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.39": {"line.1": {"text": "Ab ri\u00df er Kapuze und M\u00f6nchsgewand,", "tokens": ["Ab", "ri\u00df", "er", "Ka\u00b7pu\u00b7ze", "und", "M\u00f6n\u00b7chs\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "+---+--++-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Sein Antlitz war blutrot,", "tokens": ["Sein", "Ant\u00b7litz", "war", "blut\u00b7rot", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Leonore schrie auf und rang die H\u00e4nd' \u2013", "tokens": ["Le\u00b7o\u00b7no\u00b7re", "schrie", "auf", "und", "rang", "die", "H\u00e4nd'", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKVZ", "KON", "VVFIN", "ART", "NN", "$("], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ihre Beichte war ihr Tod.", "tokens": ["Ih\u00b7re", "Beich\u00b7te", "war", "ihr", "Tod", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.40": {"line.1": {"text": "Der K\u00f6nig \u00fcber die Schulter sah,", "tokens": ["Der", "K\u00f6\u00b7nig", "\u00fc\u00b7ber", "die", "Schul\u00b7ter", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Vielgrimmig sah er drein:", "tokens": ["Viel\u00b7grim\u00b7mig", "sah", "er", "drein", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bblord Marschall, w\u00e4r's nicht um mein Wort,", "tokens": ["\u00bb", "lord", "Mar\u00b7schall", ",", "w\u00e4r's", "nicht", "um", "mein", "Wort", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "NE", "$,", "VAFIN", "PTKNEG", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Du solltest gehangen sein.\u00ab", "tokens": ["Du", "soll\u00b7test", "ge\u00b7han\u00b7gen", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "VVPP", "VAINF", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}}}}