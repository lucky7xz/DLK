{"textgrid.poem.48325": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Einzug", "genre": "verse", "period": "N.A.", "pub_year": 1864, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "F\u00fcnf Regimenter von D\u00fcppel her.", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", "von", "D\u00fcp\u00b7pel", "her", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NE", "PTKVZ", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "F\u00fcnf Regimenter vom dritten Korps", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", "vom", "drit\u00b7ten", "Korps"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "APPRART", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "R\u00fccken durchs Brandenburger Tor;", "tokens": ["R\u00fc\u00b7cken", "durchs", "Bran\u00b7den\u00b7bur\u00b7ger", "Tor", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "NE", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Prinz Friedrich Karl, Wrangel, Manstein,", "tokens": ["Prinz", "Fried\u00b7rich", "Karl", ",", "Wran\u00b7gel", ",", "Mans\u00b7tein", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "NE", "$,", "NE", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "General Roeder, General Canstein,", "tokens": ["Ge\u00b7ne\u00b7ral", "Roe\u00b7der", ",", "Ge\u00b7ne\u00b7ral", "Cans\u00b7tein", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "F\u00fcnf Regimenter, vom Sundewitt", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", ",", "vom", "Sun\u00b7de\u00b7witt"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["CARD", "NN", "$,", "APPRART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "R\u00fccken sie an in Schritt und Tritt.", "tokens": ["R\u00fc\u00b7cken", "sie", "an", "in", "Schritt", "und", "Tritt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "APPR", "NN", "KON", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Zuerst die ", "tokens": ["Zu\u00b7erst", "die"], "token_info": ["word", "word"], "pos": ["ADV", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.3": {"text": "Die Achter; Hut ab, Sapperment,", "tokens": ["Die", "Ach\u00b7ter", ";", "Hut", "ab", ",", "Sap\u00b7per\u00b7ment", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$.", "NN", "PTKVZ", "$,", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vor dem Yorkschen Leibregiment;", "tokens": ["Vor", "dem", "Y\u00b7ork\u00b7schen", "Leib\u00b7re\u00b7gi\u00b7ment", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Schanze neun und Schanze drei", "tokens": ["Schan\u00b7ze", "neun", "und", "Schan\u00b7ze", "drei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "CARD", "KON", "NN", "CARD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Waren keine Spielerei.", "tokens": ["Wa\u00b7ren", "kei\u00b7ne", "Spie\u00b7le\u00b7rei", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Hut ab und Hurra ohne End',", "tokens": ["Hut", "ab", "und", "Hur\u00b7ra", "oh\u00b7ne", "End'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "KON", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Allemal hoch das Leibregiment!", "tokens": ["Al\u00b7le\u00b7mal", "hoch", "das", "Leib\u00b7re\u00b7gi\u00b7ment", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, die Vierundzwanziger.", "tokens": ["Hur\u00b7ra", ",", "die", "Vie\u00b7rund\u00b7zwan\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Guten Tag, guten Tag und gehorsamster Diener!", "tokens": ["Gu\u00b7ten", "Tag", ",", "gu\u00b7ten", "Tag", "und", "ge\u00b7hor\u00b7sams\u00b7ter", "Die\u00b7ner", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.4": {"text": "Ei, das sind ja meine Ruppiner;", "tokens": ["Ei", ",", "das", "sind", "ja", "mei\u00b7ne", "Rup\u00b7pi\u00b7ner", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PDS", "VAFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Flinke Kerle, ohne Flattusen,", "tokens": ["Flin\u00b7ke", "Ker\u00b7le", ",", "oh\u00b7ne", "Flat\u00b7tu\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "KOUI", "NN", "$,"], "meter": "+-+-+-++-", "measure": "unknown.measure.penta"}, "line.6": {"text": "Gr\u00fc\u00df' Gott dich, G\u00f6rschen und Brockhusen!", "tokens": ["Gr\u00fc\u00df'", "Gott", "dich", ",", "G\u00f6r\u00b7schen", "und", "Brock\u00b7hu\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "PPER", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+--++-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "M\u00f6chte manchen von euch umhalsen,", "tokens": ["M\u00f6ch\u00b7te", "man\u00b7chen", "von", "euch", "um\u00b7hal\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "PPER", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "D\u00fcppel war gut, besser war Alsen \u2013", "tokens": ["D\u00fcp\u00b7pel", "war", "gut", ",", "bes\u00b7ser", "war", "Al\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "$,", "ADJD", "VAFIN", "NN", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "'s war keine Kunst, euch half ja die Fee,", "tokens": ["'s", "war", "kei\u00b7ne", "Kunst", ",", "euch", "half", "ja", "die", "Fee", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$,", "PPER", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "Die Wasserfee vom Ruppiner See.", "tokens": ["Die", "Was\u00b7ser\u00b7fee", "vom", "Rup\u00b7pi\u00b7ner", "See", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, die Vierundsechziger.", "tokens": ["Hur\u00b7ra", ",", "die", "Vie\u00b7rund\u00b7sech\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hurra, die sind wieder breiter und st\u00e4rker,", "tokens": ["Hur\u00b7ra", ",", "die", "sind", "wie\u00b7der", "brei\u00b7ter", "und", "st\u00e4r\u00b7ker", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PRELS", "VAFIN", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Das macht, es sind richtige Uckerm\u00e4rker,", "tokens": ["Das", "macht", ",", "es", "sind", "rich\u00b7ti\u00b7ge", "U\u00b7cker\u00b7m\u00e4r\u00b7ker", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PPER", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Die sind schon mehr f\u00fcr Kolbe und Kn\u00fcppel,", "tokens": ["Die", "sind", "schon", "mehr", "f\u00fcr", "Kol\u00b7be", "und", "Kn\u00fcp\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Conferatur Wester- und Oster-D\u00fcppel,", "tokens": ["Con\u00b7fe\u00b7ra\u00b7tur", "Wes\u00b7ter", "und", "Os\u00b7ter\u00b7D\u00fcp\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "TRUNC", "KON", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Verstehen sich \u00fcbrigens auch auf Gewehre,", "tokens": ["Ver\u00b7ste\u00b7hen", "sich", "\u00fcb\u00b7ri\u00b7gens", "auch", "auf", "Ge\u00b7weh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Siehe Fohlenkoppel und Arnkiel-\u00d6re \u2013", "tokens": ["Sie\u00b7he", "Foh\u00b7len\u00b7kop\u00b7pel", "und", "Arn\u00b7kiel\u00b7\u00d6\u00b7re", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "KON", "NE", "$("], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.9": {"text": "F\u00fcnfzig d\u00e4nische Feuerschl\u00fcnde", "tokens": ["F\u00fcnf\u00b7zig", "d\u00e4\u00b7ni\u00b7sche", "Feu\u00b7er\u00b7schl\u00fcn\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["CARD", "ADJA", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.10": {"text": "K\u00f6nnen nichts gegen Prenzlau und Angerm\u00fcnde.", "tokens": ["K\u00f6n\u00b7nen", "nichts", "ge\u00b7gen", "Prenz\u00b7lau", "und", "An\u00b7ger\u00b7m\u00fcn\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "NE", "KON", "NN", "$."], "meter": "+-+--++-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.5": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "F\u00fcsiliere, Funfunddrei\u00dfiger.", "tokens": ["F\u00fc\u00b7si\u00b7lie\u00b7re", ",", "Fun\u00b7fund\u00b7drei\u00b7\u00dfi\u00b7ger", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Hurra, das wirbelt und schreitet geschwinder,", "tokens": ["Hur\u00b7ra", ",", "das", "wir\u00b7belt", "und", "schrei\u00b7tet", "ge\u00b7schwin\u00b7der", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDS", "VVFIN", "KON", "VVFIN", "ADJD", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.4": {"text": "Hurra, das sind ", "tokens": ["Hur\u00b7ra", ",", "das", "sind"], "token_info": ["word", "punct", "word", "word"], "pos": ["NE", "$,", "PDS", "VAFIN"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "Jeder, als ob er ein G\u00e4rtner w\u00e4re,", "tokens": ["Je\u00b7der", ",", "als", "ob", "er", "ein", "G\u00e4rt\u00b7ner", "w\u00e4\u00b7re", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "KOKOM", "KOUS", "PPER", "ART", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Tr\u00e4gt drei Str\u00e4u\u00dfer auf seinem Gewehre.", "tokens": ["Tr\u00e4gt", "drei", "Str\u00e4u\u00b7\u00dfer", "auf", "sei\u00b7nem", "Ge\u00b7weh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "CARD", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.7": {"text": "G\u00e4rtner freilich, gegraben, geschanzt,", "tokens": ["G\u00e4rt\u00b7ner", "frei\u00b7lich", ",", "ge\u00b7gra\u00b7ben", ",", "ge\u00b7schanzt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "ADV", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Dann sich selber eingepflanzt,", "tokens": ["Dann", "sich", "sel\u00b7ber", "ein\u00b7ge\u00b7pflanzt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "Eingepflanzt auf Schanze zwei \u2013", "tokens": ["Ein\u00b7ge\u00b7pflanzt", "auf", "Schan\u00b7ze", "zwei", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "CARD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.10": {"text": "Die flinken Berliner sind vorbei.", "tokens": ["Die", "flin\u00b7ken", "Ber\u00b7li\u00b7ner", "sind", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, unsre Sechziger.", "tokens": ["Hur\u00b7ra", ",", "uns\u00b7re", "Sech\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "PPOSAT", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Oberst ", "tokens": ["O\u00b7berst"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Gr\u00fc\u00dft mit seiner S\u00e4belspitze.", "tokens": ["Gr\u00fc\u00dft", "mit", "sei\u00b7ner", "S\u00e4\u00b7bel\u00b7spit\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hut ab und heraus die T\u00fccher!", "tokens": ["Hut", "ab", "und", "he\u00b7raus", "die", "T\u00fc\u00b7cher", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "KON", "ADV", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Das sind unsere Oderbr\u00fccher.", "tokens": ["Das", "sind", "un\u00b7se\u00b7re", "O\u00b7der\u00b7br\u00fc\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Keine Knattrer und blo\u00dfe Verschluser,", "tokens": ["Kei\u00b7ne", "Knatt\u00b7rer", "und", "blo\u00b7\u00dfe", "Ver\u00b7schlu\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "ADJA", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Lauter Barnimer und Lebuser;", "tokens": ["Lau\u00b7ter", "Bar\u00b7ni\u00b7mer", "und", "Le\u00b7bu\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.9": {"text": "Fest ihr Tritt, frank und frei \u2013", "tokens": ["Fest", "ihr", "Tritt", ",", "frank", "und", "frei", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$,", "VVFIN", "KON", "ADJD", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.10": {"text": "Major ", "tokens": ["Ma\u00b7jor"], "token_info": ["word"], "pos": ["NE"], "meter": "-+", "measure": "iambic.single"}}, "stanza.7": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Artillerie und Ingenieur';", "tokens": ["Ar\u00b7til\u00b7le\u00b7rie", "und", "In\u00b7ge\u00b7ni\u00b7eu\u00b7r'", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Elfte Ulanen, Zieten-Husaren,", "tokens": ["Elf\u00b7te", "U\u00b7la\u00b7nen", ",", "Zi\u00b7e\u00b7ten\u00b7Husa\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ADJA", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Paukenwirbel und Fanfaren.", "tokens": ["Pau\u00b7ken\u00b7wir\u00b7bel", "und", "Fan\u00b7fa\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Halt! \u2013 Der ganze Waffenblitz", "tokens": ["Halt", "!", "\u2013", "Der", "gan\u00b7ze", "Waf\u00b7fen\u00b7blitz"], "token_info": ["word", "punct", "punct", "word", "word", "word"], "pos": ["VVIMP", "$.", "$(", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Pr\u00e4sentiert vor K\u00f6nig ", "tokens": ["Pr\u00e4\u00b7sen\u00b7tiert", "vor", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word"], "pos": ["NE", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "Alles still, kein Pferdegeschnauf,", "tokens": ["Al\u00b7les", "still", ",", "kein", "Pfer\u00b7de\u00b7ge\u00b7schnauf", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PIAT", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Zehntausend blicken zu ihm auf;", "tokens": ["Zehn\u00b7tau\u00b7send", "bli\u00b7cken", "zu", "ihm", "auf", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Der neigt sich leise und l\u00fcpft den Hut:", "tokens": ["Der", "neigt", "sich", "lei\u00b7se", "und", "l\u00fcpft", "den", "Hut", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADJD", "KON", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbkonzediere, es war gut.\u00ab", "tokens": ["\u00bb", "kon\u00b7ze\u00b7die\u00b7re", ",", "es", "war", "gut", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "$,", "PPER", "VAFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "F\u00fcnf Regimenter von D\u00fcppel her.", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", "von", "D\u00fcp\u00b7pel", "her", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NE", "PTKVZ", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "F\u00fcnf Regimenter vom dritten Korps", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", "vom", "drit\u00b7ten", "Korps"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "APPRART", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "R\u00fccken durchs Brandenburger Tor;", "tokens": ["R\u00fc\u00b7cken", "durchs", "Bran\u00b7den\u00b7bur\u00b7ger", "Tor", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "NE", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Prinz Friedrich Karl, Wrangel, Manstein,", "tokens": ["Prinz", "Fried\u00b7rich", "Karl", ",", "Wran\u00b7gel", ",", "Mans\u00b7tein", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "NE", "$,", "NE", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "General Roeder, General Canstein,", "tokens": ["Ge\u00b7ne\u00b7ral", "Roe\u00b7der", ",", "Ge\u00b7ne\u00b7ral", "Cans\u00b7tein", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "F\u00fcnf Regimenter, vom Sundewitt", "tokens": ["F\u00fcnf", "Re\u00b7gi\u00b7men\u00b7ter", ",", "vom", "Sun\u00b7de\u00b7witt"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["CARD", "NN", "$,", "APPRART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "R\u00fccken sie an in Schritt und Tritt.", "tokens": ["R\u00fc\u00b7cken", "sie", "an", "in", "Schritt", "und", "Tritt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "APPR", "NN", "KON", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.9": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Zuerst die ", "tokens": ["Zu\u00b7erst", "die"], "token_info": ["word", "word"], "pos": ["ADV", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.3": {"text": "Die Achter; Hut ab, Sapperment,", "tokens": ["Die", "Ach\u00b7ter", ";", "Hut", "ab", ",", "Sap\u00b7per\u00b7ment", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$.", "NN", "PTKVZ", "$,", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vor dem Yorkschen Leibregiment;", "tokens": ["Vor", "dem", "Y\u00b7ork\u00b7schen", "Leib\u00b7re\u00b7gi\u00b7ment", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Schanze neun und Schanze drei", "tokens": ["Schan\u00b7ze", "neun", "und", "Schan\u00b7ze", "drei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "CARD", "KON", "NN", "CARD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Waren keine Spielerei.", "tokens": ["Wa\u00b7ren", "kei\u00b7ne", "Spie\u00b7le\u00b7rei", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Hut ab und Hurra ohne End',", "tokens": ["Hut", "ab", "und", "Hur\u00b7ra", "oh\u00b7ne", "End'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "KON", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Allemal hoch das Leibregiment!", "tokens": ["Al\u00b7le\u00b7mal", "hoch", "das", "Leib\u00b7re\u00b7gi\u00b7ment", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, die Vierundzwanziger.", "tokens": ["Hur\u00b7ra", ",", "die", "Vie\u00b7rund\u00b7zwan\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Guten Tag, guten Tag und gehorsamster Diener!", "tokens": ["Gu\u00b7ten", "Tag", ",", "gu\u00b7ten", "Tag", "und", "ge\u00b7hor\u00b7sams\u00b7ter", "Die\u00b7ner", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.4": {"text": "Ei, das sind ja meine Ruppiner;", "tokens": ["Ei", ",", "das", "sind", "ja", "mei\u00b7ne", "Rup\u00b7pi\u00b7ner", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PDS", "VAFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Flinke Kerle, ohne Flattusen,", "tokens": ["Flin\u00b7ke", "Ker\u00b7le", ",", "oh\u00b7ne", "Flat\u00b7tu\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "KOUI", "NN", "$,"], "meter": "+-+-+-++-", "measure": "unknown.measure.penta"}, "line.6": {"text": "Gr\u00fc\u00df' Gott dich, G\u00f6rschen und Brockhusen!", "tokens": ["Gr\u00fc\u00df'", "Gott", "dich", ",", "G\u00f6r\u00b7schen", "und", "Brock\u00b7hu\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "PPER", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+--++-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "M\u00f6chte manchen von euch umhalsen,", "tokens": ["M\u00f6ch\u00b7te", "man\u00b7chen", "von", "euch", "um\u00b7hal\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "PPER", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "D\u00fcppel war gut, besser war Alsen \u2013", "tokens": ["D\u00fcp\u00b7pel", "war", "gut", ",", "bes\u00b7ser", "war", "Al\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "$,", "ADJD", "VAFIN", "NN", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "'s war keine Kunst, euch half ja die Fee,", "tokens": ["'s", "war", "kei\u00b7ne", "Kunst", ",", "euch", "half", "ja", "die", "Fee", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$,", "PPER", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "Die Wasserfee vom Ruppiner See.", "tokens": ["Die", "Was\u00b7ser\u00b7fee", "vom", "Rup\u00b7pi\u00b7ner", "See", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, die Vierundsechziger.", "tokens": ["Hur\u00b7ra", ",", "die", "Vie\u00b7rund\u00b7sech\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hurra, die sind wieder breiter und st\u00e4rker,", "tokens": ["Hur\u00b7ra", ",", "die", "sind", "wie\u00b7der", "brei\u00b7ter", "und", "st\u00e4r\u00b7ker", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PRELS", "VAFIN", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Das macht, es sind richtige Uckerm\u00e4rker,", "tokens": ["Das", "macht", ",", "es", "sind", "rich\u00b7ti\u00b7ge", "U\u00b7cker\u00b7m\u00e4r\u00b7ker", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PPER", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Die sind schon mehr f\u00fcr Kolbe und Kn\u00fcppel,", "tokens": ["Die", "sind", "schon", "mehr", "f\u00fcr", "Kol\u00b7be", "und", "Kn\u00fcp\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Conferatur Wester- und Oster-D\u00fcppel,", "tokens": ["Con\u00b7fe\u00b7ra\u00b7tur", "Wes\u00b7ter", "und", "Os\u00b7ter\u00b7D\u00fcp\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "TRUNC", "KON", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Verstehen sich \u00fcbrigens auch auf Gewehre,", "tokens": ["Ver\u00b7ste\u00b7hen", "sich", "\u00fcb\u00b7ri\u00b7gens", "auch", "auf", "Ge\u00b7weh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Siehe Fohlenkoppel und Arnkiel-\u00d6re \u2013", "tokens": ["Sie\u00b7he", "Foh\u00b7len\u00b7kop\u00b7pel", "und", "Arn\u00b7kiel\u00b7\u00d6\u00b7re", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "KON", "NE", "$("], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.9": {"text": "F\u00fcnfzig d\u00e4nische Feuerschl\u00fcnde", "tokens": ["F\u00fcnf\u00b7zig", "d\u00e4\u00b7ni\u00b7sche", "Feu\u00b7er\u00b7schl\u00fcn\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["CARD", "ADJA", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.10": {"text": "K\u00f6nnen nichts gegen Prenzlau und Angerm\u00fcnde.", "tokens": ["K\u00f6n\u00b7nen", "nichts", "ge\u00b7gen", "Prenz\u00b7lau", "und", "An\u00b7ger\u00b7m\u00fcn\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "NE", "KON", "NN", "$."], "meter": "+-+--++-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.12": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "F\u00fcsiliere, Funfunddrei\u00dfiger.", "tokens": ["F\u00fc\u00b7si\u00b7lie\u00b7re", ",", "Fun\u00b7fund\u00b7drei\u00b7\u00dfi\u00b7ger", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Hurra, das wirbelt und schreitet geschwinder,", "tokens": ["Hur\u00b7ra", ",", "das", "wir\u00b7belt", "und", "schrei\u00b7tet", "ge\u00b7schwin\u00b7der", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDS", "VVFIN", "KON", "VVFIN", "ADJD", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.4": {"text": "Hurra, das sind ", "tokens": ["Hur\u00b7ra", ",", "das", "sind"], "token_info": ["word", "punct", "word", "word"], "pos": ["NE", "$,", "PDS", "VAFIN"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "Jeder, als ob er ein G\u00e4rtner w\u00e4re,", "tokens": ["Je\u00b7der", ",", "als", "ob", "er", "ein", "G\u00e4rt\u00b7ner", "w\u00e4\u00b7re", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "KOKOM", "KOUS", "PPER", "ART", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Tr\u00e4gt drei Str\u00e4u\u00dfer auf seinem Gewehre.", "tokens": ["Tr\u00e4gt", "drei", "Str\u00e4u\u00b7\u00dfer", "auf", "sei\u00b7nem", "Ge\u00b7weh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "CARD", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.7": {"text": "G\u00e4rtner freilich, gegraben, geschanzt,", "tokens": ["G\u00e4rt\u00b7ner", "frei\u00b7lich", ",", "ge\u00b7gra\u00b7ben", ",", "ge\u00b7schanzt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "ADV", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Dann sich selber eingepflanzt,", "tokens": ["Dann", "sich", "sel\u00b7ber", "ein\u00b7ge\u00b7pflanzt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "Eingepflanzt auf Schanze zwei \u2013", "tokens": ["Ein\u00b7ge\u00b7pflanzt", "auf", "Schan\u00b7ze", "zwei", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "CARD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.10": {"text": "Die flinken Berliner sind vorbei.", "tokens": ["Die", "flin\u00b7ken", "Ber\u00b7li\u00b7ner", "sind", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Hurra, unsre Sechziger.", "tokens": ["Hur\u00b7ra", ",", "uns\u00b7re", "Sech\u00b7zi\u00b7ger", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "PPOSAT", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Oberst ", "tokens": ["O\u00b7berst"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Gr\u00fc\u00dft mit seiner S\u00e4belspitze.", "tokens": ["Gr\u00fc\u00dft", "mit", "sei\u00b7ner", "S\u00e4\u00b7bel\u00b7spit\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hut ab und heraus die T\u00fccher!", "tokens": ["Hut", "ab", "und", "he\u00b7raus", "die", "T\u00fc\u00b7cher", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "KON", "ADV", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Das sind unsere Oderbr\u00fccher.", "tokens": ["Das", "sind", "un\u00b7se\u00b7re", "O\u00b7der\u00b7br\u00fc\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Keine Knattrer und blo\u00dfe Verschluser,", "tokens": ["Kei\u00b7ne", "Knatt\u00b7rer", "und", "blo\u00b7\u00dfe", "Ver\u00b7schlu\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "ADJA", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Lauter Barnimer und Lebuser;", "tokens": ["Lau\u00b7ter", "Bar\u00b7ni\u00b7mer", "und", "Le\u00b7bu\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.9": {"text": "Fest ihr Tritt, frank und frei \u2013", "tokens": ["Fest", "ihr", "Tritt", ",", "frank", "und", "frei", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$,", "VVFIN", "KON", "ADJD", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.10": {"text": "Major ", "tokens": ["Ma\u00b7jor"], "token_info": ["word"], "pos": ["NE"], "meter": "-+", "measure": "iambic.single"}}, "stanza.14": {"line.1": {"text": "Wer kommt? wer? \u2013", "tokens": ["Wer", "kommt", "?", "wer", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$.", "PWS", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Artillerie und Ingenieur';", "tokens": ["Ar\u00b7til\u00b7le\u00b7rie", "und", "In\u00b7ge\u00b7ni\u00b7eu\u00b7r'", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Elfte Ulanen, Zieten-Husaren,", "tokens": ["Elf\u00b7te", "U\u00b7la\u00b7nen", ",", "Zi\u00b7e\u00b7ten\u00b7Husa\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ADJA", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Paukenwirbel und Fanfaren.", "tokens": ["Pau\u00b7ken\u00b7wir\u00b7bel", "und", "Fan\u00b7fa\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Halt! \u2013 Der ganze Waffenblitz", "tokens": ["Halt", "!", "\u2013", "Der", "gan\u00b7ze", "Waf\u00b7fen\u00b7blitz"], "token_info": ["word", "punct", "punct", "word", "word", "word"], "pos": ["VVIMP", "$.", "$(", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Pr\u00e4sentiert vor K\u00f6nig ", "tokens": ["Pr\u00e4\u00b7sen\u00b7tiert", "vor", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word"], "pos": ["NE", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "Alles still, kein Pferdegeschnauf,", "tokens": ["Al\u00b7les", "still", ",", "kein", "Pfer\u00b7de\u00b7ge\u00b7schnauf", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PIAT", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Zehntausend blicken zu ihm auf;", "tokens": ["Zehn\u00b7tau\u00b7send", "bli\u00b7cken", "zu", "ihm", "auf", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Der neigt sich leise und l\u00fcpft den Hut:", "tokens": ["Der", "neigt", "sich", "lei\u00b7se", "und", "l\u00fcpft", "den", "Hut", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADJD", "KON", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbkonzediere, es war gut.\u00ab", "tokens": ["\u00bb", "kon\u00b7ze\u00b7die\u00b7re", ",", "es", "war", "gut", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "$,", "PPER", "VAFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}