{"textgrid.poem.48202": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Lebenswege", "genre": "verse", "period": "N.A.", "pub_year": 1888, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "F\u00fcnfzig Jahre werden es ehstens sein,", "tokens": ["F\u00fcnf\u00b7zig", "Jah\u00b7re", "wer\u00b7den", "es", "ehs\u00b7tens", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "ADV", "VAINF", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Da trat ich in meinen ersten \u00bbVerein\u00ab.", "tokens": ["Da", "trat", "ich", "in", "mei\u00b7nen", "ers\u00b7ten", "\u00bb", "Ver\u00b7ein", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "$(", "NN", "$(", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Nat\u00fcrlich Dichter. Blutjunge Ware:", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "Dich\u00b7ter.", "Blut\u00b7jun\u00b7ge", "Wa\u00b7re", ":"], "token_info": ["word", "abbreviation", "word", "word", "punct"], "pos": ["ADV", "NE", "NN", "VAFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Studenten, Leutnants, Refrendare.", "tokens": ["Stu\u00b7den\u00b7ten", ",", "Leut\u00b7nants", ",", "Re\u00b7fren\u00b7da\u00b7re", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Rang gab's nicht, ", "tokens": ["Rang", "gab's", "nicht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKNEG", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}}, "stanza.2": {"line.1": {"text": "So stand es, als Anno 40 wir schrieben;", "tokens": ["So", "stand", "es", ",", "als", "An\u00b7no", "40", "wir", "schrie\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "number", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "NN", "CARD", "PPER", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Aber ach, wo bist du Sonne geblieben?", "tokens": ["A\u00b7ber", "ach", ",", "wo", "bist", "du", "Son\u00b7ne", "ge\u00b7blie\u00b7ben", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "XY", "$,", "PWAV", "VAFIN", "PPER", "NN", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Ich bin noch immer, was damals ich war,", "tokens": ["Ich", "bin", "noch", "im\u00b7mer", ",", "was", "da\u00b7mals", "ich", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$,", "PRELS", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein Lichtlein auf demselben Altar,", "tokens": ["Ein", "Licht\u00b7lein", "auf", "dem\u00b7sel\u00b7ben", "Al\u00b7tar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Aus den Leutnants aber und Studenten", "tokens": ["Aus", "den", "Leut\u00b7nants", "a\u00b7ber", "und", "Stu\u00b7den\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADV", "KON", "NN"], "meter": "--+-+--+--", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Wurden Gen'r\u00e4le und Chefpr\u00e4sidenten.", "tokens": ["Wur\u00b7den", "Gen'\u00b7r\u00e4\u00b7le", "und", "Chef\u00b7pr\u00e4\u00b7si\u00b7den\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "$."], "meter": "+-++--+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Und mitunter, auf stillem Tiergartenpfade,", "tokens": ["Und", "mi\u00b7tun\u00b7ter", ",", "auf", "stil\u00b7lem", "Tier\u00b7gar\u00b7ten\u00b7pfa\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Bei \u00bbK\u00f6n'gin Luise\u00ab trifft man sich grade.", "tokens": ["Bei", "\u00bb", "K\u00f6n'\u00b7gin", "Lu\u00b7i\u00b7se", "\u00ab", "trifft", "man", "sich", "gra\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$(", "NN", "NE", "$(", "VVFIN", "PIS", "PRF", "ADV", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbnun, lieber F., noch immer bei Wege?\u00ab", "tokens": ["\u00bb", "nun", ",", "lie\u00b7ber", "F.", ",", "noch", "im\u00b7mer", "bei", "We\u00b7ge", "?", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "abbreviation", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "$,", "ADV", "NE", "$,", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbgott sei Dank, Exzellenz ... Trotz Nackenschl\u00e4ge ...\u00ab", "tokens": ["\u00bb", "gott", "sei", "Dank", ",", "Ex\u00b7zel\u00b7lenz", "...", "Trotz", "Na\u00b7cken\u00b7schl\u00e4\u00b7ge", "...", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "VAFIN", "NN", "$,", "NN", "$(", "APPR", "NN", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "\u00bbkenn' ich, kenn' ich. Das Leben ist flau ...", "tokens": ["\u00bb", "kenn'", "ich", ",", "kenn'", "ich", ".", "Das", "Le\u00b7ben", "ist", "flau", "..."], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "VVFIN", "PPER", "$.", "ART", "NN", "VAFIN", "ADJD", "$("], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Gr\u00fc\u00dfen Sie Ihre liebe Frau.\u00ab", "tokens": ["Gr\u00fc\u00b7\u00dfen", "Sie", "Ih\u00b7re", "lie\u00b7be", "Frau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "PPOSAT", "ADJA", "NN", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.5": {"line.1": {"text": "F\u00fcnfzig Jahre werden es ehstens sein,", "tokens": ["F\u00fcnf\u00b7zig", "Jah\u00b7re", "wer\u00b7den", "es", "ehs\u00b7tens", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "ADV", "VAINF", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Da trat ich in meinen ersten \u00bbVerein\u00ab.", "tokens": ["Da", "trat", "ich", "in", "mei\u00b7nen", "ers\u00b7ten", "\u00bb", "Ver\u00b7ein", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "$(", "NN", "$(", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Nat\u00fcrlich Dichter. Blutjunge Ware:", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "Dich\u00b7ter.", "Blut\u00b7jun\u00b7ge", "Wa\u00b7re", ":"], "token_info": ["word", "abbreviation", "word", "word", "punct"], "pos": ["ADV", "NE", "NN", "VAFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Studenten, Leutnants, Refrendare.", "tokens": ["Stu\u00b7den\u00b7ten", ",", "Leut\u00b7nants", ",", "Re\u00b7fren\u00b7da\u00b7re", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Rang gab's nicht, ", "tokens": ["Rang", "gab's", "nicht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKNEG", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}}, "stanza.6": {"line.1": {"text": "So stand es, als Anno 40 wir schrieben;", "tokens": ["So", "stand", "es", ",", "als", "An\u00b7no", "40", "wir", "schrie\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "number", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "NN", "CARD", "PPER", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Aber ach, wo bist du Sonne geblieben?", "tokens": ["A\u00b7ber", "ach", ",", "wo", "bist", "du", "Son\u00b7ne", "ge\u00b7blie\u00b7ben", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "XY", "$,", "PWAV", "VAFIN", "PPER", "NN", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Ich bin noch immer, was damals ich war,", "tokens": ["Ich", "bin", "noch", "im\u00b7mer", ",", "was", "da\u00b7mals", "ich", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$,", "PRELS", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein Lichtlein auf demselben Altar,", "tokens": ["Ein", "Licht\u00b7lein", "auf", "dem\u00b7sel\u00b7ben", "Al\u00b7tar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Aus den Leutnants aber und Studenten", "tokens": ["Aus", "den", "Leut\u00b7nants", "a\u00b7ber", "und", "Stu\u00b7den\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADV", "KON", "NN"], "meter": "--+-+--+--", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Wurden Gen'r\u00e4le und Chefpr\u00e4sidenten.", "tokens": ["Wur\u00b7den", "Gen'\u00b7r\u00e4\u00b7le", "und", "Chef\u00b7pr\u00e4\u00b7si\u00b7den\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "$."], "meter": "+-++--+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Und mitunter, auf stillem Tiergartenpfade,", "tokens": ["Und", "mi\u00b7tun\u00b7ter", ",", "auf", "stil\u00b7lem", "Tier\u00b7gar\u00b7ten\u00b7pfa\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Bei \u00bbK\u00f6n'gin Luise\u00ab trifft man sich grade.", "tokens": ["Bei", "\u00bb", "K\u00f6n'\u00b7gin", "Lu\u00b7i\u00b7se", "\u00ab", "trifft", "man", "sich", "gra\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$(", "NN", "NE", "$(", "VVFIN", "PIS", "PRF", "ADV", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbnun, lieber F., noch immer bei Wege?\u00ab", "tokens": ["\u00bb", "nun", ",", "lie\u00b7ber", "F.", ",", "noch", "im\u00b7mer", "bei", "We\u00b7ge", "?", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "abbreviation", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "$,", "ADV", "NE", "$,", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbgott sei Dank, Exzellenz ... Trotz Nackenschl\u00e4ge ...\u00ab", "tokens": ["\u00bb", "gott", "sei", "Dank", ",", "Ex\u00b7zel\u00b7lenz", "...", "Trotz", "Na\u00b7cken\u00b7schl\u00e4\u00b7ge", "...", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "VAFIN", "NN", "$,", "NN", "$(", "APPR", "NN", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "\u00bbkenn' ich, kenn' ich. Das Leben ist flau ...", "tokens": ["\u00bb", "kenn'", "ich", ",", "kenn'", "ich", ".", "Das", "Le\u00b7ben", "ist", "flau", "..."], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "VVFIN", "PPER", "$.", "ART", "NN", "VAFIN", "ADJD", "$("], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Gr\u00fc\u00dfen Sie Ihre liebe Frau.\u00ab", "tokens": ["Gr\u00fc\u00b7\u00dfen", "Sie", "Ih\u00b7re", "lie\u00b7be", "Frau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "PPOSAT", "ADJA", "NN", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}}}}