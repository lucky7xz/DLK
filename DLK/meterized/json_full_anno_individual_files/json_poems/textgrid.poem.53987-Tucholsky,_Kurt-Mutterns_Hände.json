{"textgrid.poem.53987": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Mutterns H\u00e4nde", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hast uns Stulln jeschnitten", "tokens": ["Hast", "uns", "Stulln", "je\u00b7schnit\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NE", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "un Kaffe jekocht", "tokens": ["un", "Kaf\u00b7fe", "je\u00b7kocht"], "token_info": ["word", "word", "word"], "pos": ["FM", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "un de T\u00f6ppe r\u00fcbajeschohm \u2013", "tokens": ["un", "de", "T\u00f6p\u00b7pe", "r\u00fc\u00b7ba\u00b7je\u00b7schohm", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "FM", "FM", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "un jewischt und jen\u00e4ht", "tokens": ["un", "je\u00b7wischt", "und", "je\u00b7n\u00e4ht"], "token_info": ["word", "word", "word", "word"], "pos": ["FM", "FM", "KON", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "un jemacht und jedreht . . .", "tokens": ["un", "je\u00b7macht", "und", "je\u00b7dreht", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["FM", "FM", "KON", "VVFIN", "$.", "$.", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "alles mit deine H\u00e4nde.", "tokens": ["al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.2": {"line.1": {"text": "Hast de Milch zujedeckt,", "tokens": ["Hast", "de", "Milch", "zu\u00b7je\u00b7deckt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NN", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "uns Bobongs zujesteckt", "tokens": ["uns", "Bo\u00b7bongs", "zu\u00b7jes\u00b7teckt"], "token_info": ["word", "word", "word"], "pos": ["PPER", "NE", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "un Zeitungen ausjetragen \u2013", "tokens": ["un", "Zei\u00b7tun\u00b7gen", "aus\u00b7je\u00b7tra\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["FM", "NN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "hast die Hemden jez\u00e4hlt", "tokens": ["hast", "die", "Hem\u00b7den", "je\u00b7z\u00e4hlt"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "VVFIN"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "und Kartoffeln jesch\u00e4lt . . .", "tokens": ["und", "Kar\u00b7tof\u00b7feln", "jesc\u00b7h\u00e4lt", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "NN", "VVFIN", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "alles mit deine H\u00e4nde.", "tokens": ["al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.3": {"line.1": {"text": "Hast uns manches Mal", "tokens": ["Hast", "uns", "man\u00b7ches", "Mal"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PIAT", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "bei jro\u00dfen Schkandal", "tokens": ["bei", "jro\u00b7\u00dfen", "Sch\u00b7kan\u00b7dal"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "auch 'n Katzenkopp jejeben.", "tokens": ["auch", "'n", "Kat\u00b7zen\u00b7kopp", "je\u00b7je\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PDAT", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hast uns hochjebracht.", "tokens": ["Hast", "uns", "hoch\u00b7jeb\u00b7racht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Wir wahn Sticker acht,", "tokens": ["Wir", "wahn", "Sti\u00b7cker", "acht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "CARD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.6": {"text": "sechse sind noch am Leben . . .", "tokens": ["sech\u00b7se", "sind", "noch", "am", "Le\u00b7ben", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "ADV", "APPRART", "NN", "$.", "$.", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.7": {"text": "Alles mit deine H\u00e4nde.", "tokens": ["Al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.4": {"line.1": {"text": "Hei\u00df warn se un kalt.", "tokens": ["Hei\u00df", "warn", "se", "un", "kalt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "FM", "FM", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Nu sind se alt.", "tokens": ["Nu", "sind", "se", "alt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Nu bist du bald am Ende.", "tokens": ["Nu", "bist", "du", "bald", "am", "En\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da stehn wa nu hier,", "tokens": ["Da", "stehn", "wa", "nu", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ADV", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "und denn komm wir bei dir", "tokens": ["und", "denn", "komm", "wir", "bei", "dir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "und streicheln deine H\u00e4nde.", "tokens": ["und", "strei\u00b7cheln", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Hast uns Stulln jeschnitten", "tokens": ["Hast", "uns", "Stulln", "je\u00b7schnit\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NE", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "un Kaffe jekocht", "tokens": ["un", "Kaf\u00b7fe", "je\u00b7kocht"], "token_info": ["word", "word", "word"], "pos": ["FM", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "un de T\u00f6ppe r\u00fcbajeschohm \u2013", "tokens": ["un", "de", "T\u00f6p\u00b7pe", "r\u00fc\u00b7ba\u00b7je\u00b7schohm", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "FM", "FM", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "un jewischt und jen\u00e4ht", "tokens": ["un", "je\u00b7wischt", "und", "je\u00b7n\u00e4ht"], "token_info": ["word", "word", "word", "word"], "pos": ["FM", "FM", "KON", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "un jemacht und jedreht . . .", "tokens": ["un", "je\u00b7macht", "und", "je\u00b7dreht", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["FM", "FM", "KON", "VVFIN", "$.", "$.", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "alles mit deine H\u00e4nde.", "tokens": ["al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.6": {"line.1": {"text": "Hast de Milch zujedeckt,", "tokens": ["Hast", "de", "Milch", "zu\u00b7je\u00b7deckt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NN", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "uns Bobongs zujesteckt", "tokens": ["uns", "Bo\u00b7bongs", "zu\u00b7jes\u00b7teckt"], "token_info": ["word", "word", "word"], "pos": ["PPER", "NE", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "un Zeitungen ausjetragen \u2013", "tokens": ["un", "Zei\u00b7tun\u00b7gen", "aus\u00b7je\u00b7tra\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["FM", "NN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "hast die Hemden jez\u00e4hlt", "tokens": ["hast", "die", "Hem\u00b7den", "je\u00b7z\u00e4hlt"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "VVFIN"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "und Kartoffeln jesch\u00e4lt . . .", "tokens": ["und", "Kar\u00b7tof\u00b7feln", "jesc\u00b7h\u00e4lt", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "NN", "VVFIN", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "alles mit deine H\u00e4nde.", "tokens": ["al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.7": {"line.1": {"text": "Hast uns manches Mal", "tokens": ["Hast", "uns", "man\u00b7ches", "Mal"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PIAT", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "bei jro\u00dfen Schkandal", "tokens": ["bei", "jro\u00b7\u00dfen", "Sch\u00b7kan\u00b7dal"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "auch 'n Katzenkopp jejeben.", "tokens": ["auch", "'n", "Kat\u00b7zen\u00b7kopp", "je\u00b7je\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PDAT", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hast uns hochjebracht.", "tokens": ["Hast", "uns", "hoch\u00b7jeb\u00b7racht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Wir wahn Sticker acht,", "tokens": ["Wir", "wahn", "Sti\u00b7cker", "acht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "CARD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.6": {"text": "sechse sind noch am Leben . . .", "tokens": ["sech\u00b7se", "sind", "noch", "am", "Le\u00b7ben", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "ADV", "APPRART", "NN", "$.", "$.", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.7": {"text": "Alles mit deine H\u00e4nde.", "tokens": ["Al\u00b7les", "mit", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.8": {"line.1": {"text": "Hei\u00df warn se un kalt.", "tokens": ["Hei\u00df", "warn", "se", "un", "kalt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "FM", "FM", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Nu sind se alt.", "tokens": ["Nu", "sind", "se", "alt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Nu bist du bald am Ende.", "tokens": ["Nu", "bist", "du", "bald", "am", "En\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da stehn wa nu hier,", "tokens": ["Da", "stehn", "wa", "nu", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ADV", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "und denn komm wir bei dir", "tokens": ["und", "denn", "komm", "wir", "bei", "dir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "und streicheln deine H\u00e4nde.", "tokens": ["und", "strei\u00b7cheln", "dei\u00b7ne", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}