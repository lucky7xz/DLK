{"textgrid.poem.60726": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie kommt's, da\u00df uns so oft \u00c4sop", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie kommt's, da\u00df uns so oft \u00c4sop", "tokens": ["Wie", "kommt's", ",", "da\u00df", "uns", "so", "oft", "\u00c4\u00b7sop"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gesungen hat des Fuchses Lob \u2013", "tokens": ["Ge\u00b7sun\u00b7gen", "hat", "des", "Fuch\u00b7ses", "Lob", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein Lob als allerschlauster Wicht?", "tokens": ["Sein", "Lob", "als", "al\u00b7ler\u00b7schlaus\u00b7ter", "Wicht", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich such den Grund und find ihn nicht.", "tokens": ["Ich", "such", "den", "Grund", "und", "find", "ihn", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wenn ich den Wolf mit ihm vergleiche:", "tokens": ["Wenn", "ich", "den", "Wolf", "mit", "ihm", "ver\u00b7glei\u00b7che", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NE", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Hat der sich seiner Haut zu wehren,", "tokens": ["Hat", "der", "sich", "sei\u00b7ner", "Haut", "zu", "weh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "PRF", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Lockt den nach Beute sein Begehren,", "tokens": ["Lockt", "den", "nach", "Beu\u00b7te", "sein", "Be\u00b7geh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "So \u00fcbt er wohl noch be\u00dfre Streiche.", "tokens": ["So", "\u00fcbt", "er", "wohl", "noch", "be\u00df\u00b7re", "Strei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Ich glaub's \u2013 und w\u00e4re ich nur dreister,", "tokens": ["Ich", "glaub's", "\u2013", "und", "w\u00e4\u00b7re", "ich", "nur", "dreis\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "$(", "KON", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "So widerspr\u00e4che ich dem Meister", "tokens": ["So", "wi\u00b7der\u00b7spr\u00e4\u00b7che", "ich", "dem", "Meis\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Und seiner anfechtbaren Lehre.", "tokens": ["Und", "sei\u00b7ner", "an\u00b7fecht\u00b7ba\u00b7ren", "Leh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Doch hier ein Fall, wo alle Ehre", "tokens": ["Doch", "hier", "ein", "Fall", ",", "wo", "al\u00b7le", "Eh\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "$,", "PWAV", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Tats\u00e4chlich dem geb\u00fchrt, der Malepart bewohnt.", "tokens": ["Tat\u00b7s\u00e4ch\u00b7lich", "dem", "ge\u00b7b\u00fchrt", ",", "der", "Ma\u00b7le\u00b7part", "be\u00b7wohnt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "VVPP", "$,", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Im Brunnengrund sah eines Nachts der Fuchs den Mond \u2013", "tokens": ["Im", "Brun\u00b7nen\u00b7grund", "sah", "ei\u00b7nes", "Nachts", "der", "Fuchs", "den", "Mond", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ART", "ADV", "ART", "NE", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Kreisrundes Bild, das ihm ein voller K\u00e4se schien.", "tokens": ["Kreis\u00b7run\u00b7des", "Bild", ",", "das", "ihm", "ein", "vol\u00b7ler", "K\u00e4\u00b7se", "schien", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Zwei Eimer dienten da, wechselnd das Na\u00df zu ziehn,", "tokens": ["Zwei", "Ei\u00b7mer", "dien\u00b7ten", "da", ",", "wech\u00b7selnd", "das", "Na\u00df", "zu", "ziehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ADV", "$,", "ADJD", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "War einer oben, hing der andre in der Tiefe.", "tokens": ["War", "ei\u00b7ner", "o\u00b7ben", ",", "hing", "der", "and\u00b7re", "in", "der", "Tie\u00b7fe", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADV", "$,", "VVFIN", "ART", "ADJA", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nun war dem Hungerherrn,", "tokens": ["Nun", "war", "dem", "Hun\u00b7ge\u00b7rherrn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Als ob der K\u00e4se riefe,", "tokens": ["Als", "ob", "der", "K\u00e4\u00b7se", "rie\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Er m\u00f6chte herzlich gern", "tokens": ["Er", "m\u00f6ch\u00b7te", "herz\u00b7lich", "gern"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADJD", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Von ihm gefressen sein;", "tokens": ["Von", "ihm", "ge\u00b7fres\u00b7sen", "sein", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Und also stieg er in den obern Eimer ein", "tokens": ["Und", "al\u00b7so", "stieg", "er", "in", "den", "o\u00b7bern", "Ei\u00b7mer", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "ART"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und sauste nieder \u2013 um den Irrtum einzusehen.", "tokens": ["Und", "saus\u00b7te", "nie\u00b7der", "\u2013", "um", "den", "Irr\u00b7tum", "ein\u00b7zu\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$(", "APPR", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "O welche Not! Er sieht, es ist um ihn geschehen.", "tokens": ["O", "wel\u00b7che", "Not", "!", "Er", "sieht", ",", "es", "ist", "um", "ihn", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PWAT", "NN", "$.", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wie k\u00f6nnt er je zur\u00fcck, wenn nicht ein andrer k\u00e4me", "tokens": ["Wie", "k\u00f6nnt", "er", "je", "zu\u00b7r\u00fcck", ",", "wenn", "nicht", "ein", "an\u00b7drer", "k\u00e4\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "KOUS", "PTKNEG", "ART", "ADJA", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und gleich wie er ber\u00fcckt den andern Eimer n\u00e4hme,", "tokens": ["Und", "gleich", "wie", "er", "be\u00b7r\u00fcckt", "den", "an\u00b7dern", "Ei\u00b7mer", "n\u00e4h\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "PPER", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Da\u00df seiner wieder aufw\u00e4rts stieg zum Brunnenrand?", "tokens": ["Da\u00df", "sei\u00b7ner", "wie\u00b7der", "auf\u00b7w\u00e4rts", "stieg", "zum", "Brun\u00b7nen\u00b7rand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADV", "ADV", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Doch keiner kam \u2013 und schon der zweite Tag entschwand.", "tokens": ["Doch", "kei\u00b7ner", "kam", "\u2013", "und", "schon", "der", "zwei\u00b7te", "Tag", "ent\u00b7schwand", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$(", "KON", "ADV", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Zwei N\u00e4chte hatten merklich jetzt, wie ihr begreift,", "tokens": ["Zwei", "N\u00e4ch\u00b7te", "hat\u00b7ten", "merk\u00b7lich", "jetzt", ",", "wie", "ihr", "be\u00b7greift", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "ADJD", "ADV", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das volle Rund des Monds am Rande ausgeschweift.", "tokens": ["Das", "vol\u00b7le", "Rund", "des", "Monds", "am", "Ran\u00b7de", "aus\u00b7ge\u00b7schweift", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Verzweifelnd st\u00f6hnt der Fuchs \u2013 da kommt der Nimmersatt", "tokens": ["Ver\u00b7zwei\u00b7felnd", "st\u00f6hnt", "der", "Fuchs", "\u2013", "da", "kommt", "der", "Nim\u00b7mer\u00b7satt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "VVFIN", "ART", "NE", "$(", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von Wolf vorbei. \u00bbKamrad!\u00ab rief jener laut, \u00bbschau her,", "tokens": ["Von", "Wolf", "vor\u00b7bei", ".", "\u00bb", "Kam\u00b7rad", "!", "\u00ab", "rief", "je\u00b7ner", "laut", ",", "\u00bb", "schau", "her", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKVZ", "$.", "$(", "NN", "$.", "$(", "VVFIN", "PDAT", "ADJD", "$,", "$(", "ADJD", "PTKVZ", "$,"], "meter": "-+--++-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Erblickst du meinen Fund? Ein K\u00e4se rund und schwer,", "tokens": ["Er\u00b7blickst", "du", "mei\u00b7nen", "Fund", "?", "Ein", "K\u00e4\u00b7se", "rund", "und", "schwer", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$.", "ART", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Den Faun, der Gott, aus Ios Milch geknetet hat!", "tokens": ["Den", "Faun", ",", "der", "Gott", ",", "aus", "I\u00b7os", "Milch", "ge\u00b7kne\u00b7tet", "hat", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "APPR", "NE", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Selbst Jupiter gew\u00e4nne, w\u00e4r er krank,", "tokens": ["Selbst", "Ju\u00b7pi\u00b7ter", "ge\u00b7w\u00e4n\u00b7ne", ",", "w\u00e4r", "er", "krank", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "$,", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Durch ihn den Appetit zur\u00fcck", "tokens": ["Durch", "ihn", "den", "Ap\u00b7pe\u00b7tit", "zu\u00b7r\u00fcck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nach seinem edlen Nektartrank.", "tokens": ["Nach", "sei\u00b7nem", "ed\u00b7len", "Nekt\u00b7ar\u00b7trank", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich a\u00df vom Rande hier ein St\u00fcck,", "tokens": ["Ich", "a\u00df", "vom", "Ran\u00b7de", "hier", "ein", "St\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der Rest wird dir als Mahl gen\u00fcgen.", "tokens": ["Der", "Rest", "wird", "dir", "als", "Mahl", "ge\u00b7n\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "KOUS", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Ich g\u00f6nnte keinem andern,\u00ab fuhr er fort zu l\u00fcgen,", "tokens": ["Ich", "g\u00f6nn\u00b7te", "kei\u00b7nem", "an\u00b7dern", ",", "\u00ab", "fuhr", "er", "fort", "zu", "l\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "$,", "$(", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "\u00bbein solch olympisch Schlemmergl\u00fcck,", "tokens": ["\u00bb", "ein", "solch", "o\u00b7lym\u00b7pisch", "Schlem\u00b7mer\u00b7gl\u00fcck", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "PIAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Nur dir, mein liebster Freund, allein.", "tokens": ["Nur", "dir", ",", "mein", "liebs\u00b7ter", "Freund", ",", "al\u00b7lein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Genie\u00dfe dies mit vollen Z\u00fcgen.", "tokens": ["Ge\u00b7nie\u00b7\u00dfe", "dies", "mit", "vol\u00b7len", "Z\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Steig in den Eimer oben ein,", "tokens": ["Steig", "in", "den", "Ei\u00b7mer", "o\u00b7ben", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ADV", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.12": {"text": "Den ich f\u00fcr dich dort hingeh\u00e4ngt,", "tokens": ["Den", "ich", "f\u00fcr", "dich", "dort", "hin\u00b7ge\u00b7h\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Und fahr herab, um hier so recht vergn\u00fcgt zu sein.\u00ab", "tokens": ["Und", "fahr", "her\u00b7ab", ",", "um", "hier", "so", "recht", "ver\u00b7gn\u00fcgt", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "$,", "KOUI", "ADV", "ADV", "ADJD", "VVPP", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "H\u00fcbsch war die Rede ausgeschm\u00fcckt.", "tokens": ["H\u00fcbsch", "war", "die", "Re\u00b7de", "aus\u00b7ge\u00b7schm\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Der Wolf ist Narr genug, da\u00df er sich selber f\u00e4ngt.", "tokens": ["Der", "Wolf", "ist", "Narr", "ge\u00b7nug", ",", "da\u00df", "er", "sich", "sel\u00b7ber", "f\u00e4ngt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "NN", "ADV", "$,", "KOUS", "PPER", "PRF", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Der Rote lacht, wie alles gl\u00fcckt.", "tokens": ["Der", "Ro\u00b7te", "lacht", ",", "wie", "al\u00b7les", "gl\u00fcckt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PWAV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Der Eimer sinkt, und sein Gewicht", "tokens": ["Der", "Ei\u00b7mer", "sinkt", ",", "und", "sein", "Ge\u00b7wicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Entf\u00fchrt den andern Teil:", "tokens": ["Ent\u00b7f\u00fchrt", "den", "an\u00b7dern", "Teil", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.19": {"text": "Es steigt der Fuchs zum Sternenlicht,", "tokens": ["Es", "steigt", "der", "Fuchs", "zum", "Ster\u00b7nen\u00b7licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Nicht zu des Wolfes Heil.", "tokens": ["Nicht", "zu", "des", "Wol\u00b7fes", "Heil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Doch spottet dieses Toren nicht!", "tokens": ["Doch", "spot\u00b7tet", "die\u00b7ses", "To\u00b7ren", "nicht", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr selber la\u00dft euch gern und oft", "tokens": ["Ihr", "sel\u00b7ber", "la\u00dft", "euch", "gern", "und", "oft"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "PPER", "ADV", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Verf\u00fchren von so ungewissen Dingen.", "tokens": ["Ver\u00b7f\u00fch\u00b7ren", "von", "so", "un\u00b7ge\u00b7wis\u00b7sen", "Din\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Gar leicht ist's, euch in gleiche Not zu bringen,", "tokens": ["Gar", "leicht", "ist's", ",", "euch", "in", "glei\u00b7che", "Not", "zu", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "$,", "PPER", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Da jeder glaubt, was er erhofft.", "tokens": ["Da", "je\u00b7der", "glaubt", ",", "was", "er", "er\u00b7hofft", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wie kommt's, da\u00df uns so oft \u00c4sop", "tokens": ["Wie", "kommt's", ",", "da\u00df", "uns", "so", "oft", "\u00c4\u00b7sop"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gesungen hat des Fuchses Lob \u2013", "tokens": ["Ge\u00b7sun\u00b7gen", "hat", "des", "Fuch\u00b7ses", "Lob", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein Lob als allerschlauster Wicht?", "tokens": ["Sein", "Lob", "als", "al\u00b7ler\u00b7schlaus\u00b7ter", "Wicht", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich such den Grund und find ihn nicht.", "tokens": ["Ich", "such", "den", "Grund", "und", "find", "ihn", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wenn ich den Wolf mit ihm vergleiche:", "tokens": ["Wenn", "ich", "den", "Wolf", "mit", "ihm", "ver\u00b7glei\u00b7che", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NE", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Hat der sich seiner Haut zu wehren,", "tokens": ["Hat", "der", "sich", "sei\u00b7ner", "Haut", "zu", "weh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "PRF", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Lockt den nach Beute sein Begehren,", "tokens": ["Lockt", "den", "nach", "Beu\u00b7te", "sein", "Be\u00b7geh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "So \u00fcbt er wohl noch be\u00dfre Streiche.", "tokens": ["So", "\u00fcbt", "er", "wohl", "noch", "be\u00df\u00b7re", "Strei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Ich glaub's \u2013 und w\u00e4re ich nur dreister,", "tokens": ["Ich", "glaub's", "\u2013", "und", "w\u00e4\u00b7re", "ich", "nur", "dreis\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "$(", "KON", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "So widerspr\u00e4che ich dem Meister", "tokens": ["So", "wi\u00b7der\u00b7spr\u00e4\u00b7che", "ich", "dem", "Meis\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Und seiner anfechtbaren Lehre.", "tokens": ["Und", "sei\u00b7ner", "an\u00b7fecht\u00b7ba\u00b7ren", "Leh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Doch hier ein Fall, wo alle Ehre", "tokens": ["Doch", "hier", "ein", "Fall", ",", "wo", "al\u00b7le", "Eh\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "$,", "PWAV", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Tats\u00e4chlich dem geb\u00fchrt, der Malepart bewohnt.", "tokens": ["Tat\u00b7s\u00e4ch\u00b7lich", "dem", "ge\u00b7b\u00fchrt", ",", "der", "Ma\u00b7le\u00b7part", "be\u00b7wohnt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "VVPP", "$,", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Im Brunnengrund sah eines Nachts der Fuchs den Mond \u2013", "tokens": ["Im", "Brun\u00b7nen\u00b7grund", "sah", "ei\u00b7nes", "Nachts", "der", "Fuchs", "den", "Mond", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ART", "ADV", "ART", "NE", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Kreisrundes Bild, das ihm ein voller K\u00e4se schien.", "tokens": ["Kreis\u00b7run\u00b7des", "Bild", ",", "das", "ihm", "ein", "vol\u00b7ler", "K\u00e4\u00b7se", "schien", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Zwei Eimer dienten da, wechselnd das Na\u00df zu ziehn,", "tokens": ["Zwei", "Ei\u00b7mer", "dien\u00b7ten", "da", ",", "wech\u00b7selnd", "das", "Na\u00df", "zu", "ziehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ADV", "$,", "ADJD", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "War einer oben, hing der andre in der Tiefe.", "tokens": ["War", "ei\u00b7ner", "o\u00b7ben", ",", "hing", "der", "and\u00b7re", "in", "der", "Tie\u00b7fe", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADV", "$,", "VVFIN", "ART", "ADJA", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nun war dem Hungerherrn,", "tokens": ["Nun", "war", "dem", "Hun\u00b7ge\u00b7rherrn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Als ob der K\u00e4se riefe,", "tokens": ["Als", "ob", "der", "K\u00e4\u00b7se", "rie\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Er m\u00f6chte herzlich gern", "tokens": ["Er", "m\u00f6ch\u00b7te", "herz\u00b7lich", "gern"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADJD", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Von ihm gefressen sein;", "tokens": ["Von", "ihm", "ge\u00b7fres\u00b7sen", "sein", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Und also stieg er in den obern Eimer ein", "tokens": ["Und", "al\u00b7so", "stieg", "er", "in", "den", "o\u00b7bern", "Ei\u00b7mer", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "ART"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und sauste nieder \u2013 um den Irrtum einzusehen.", "tokens": ["Und", "saus\u00b7te", "nie\u00b7der", "\u2013", "um", "den", "Irr\u00b7tum", "ein\u00b7zu\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$(", "APPR", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "O welche Not! Er sieht, es ist um ihn geschehen.", "tokens": ["O", "wel\u00b7che", "Not", "!", "Er", "sieht", ",", "es", "ist", "um", "ihn", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PWAT", "NN", "$.", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wie k\u00f6nnt er je zur\u00fcck, wenn nicht ein andrer k\u00e4me", "tokens": ["Wie", "k\u00f6nnt", "er", "je", "zu\u00b7r\u00fcck", ",", "wenn", "nicht", "ein", "an\u00b7drer", "k\u00e4\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "KOUS", "PTKNEG", "ART", "ADJA", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und gleich wie er ber\u00fcckt den andern Eimer n\u00e4hme,", "tokens": ["Und", "gleich", "wie", "er", "be\u00b7r\u00fcckt", "den", "an\u00b7dern", "Ei\u00b7mer", "n\u00e4h\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "PPER", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Da\u00df seiner wieder aufw\u00e4rts stieg zum Brunnenrand?", "tokens": ["Da\u00df", "sei\u00b7ner", "wie\u00b7der", "auf\u00b7w\u00e4rts", "stieg", "zum", "Brun\u00b7nen\u00b7rand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADV", "ADV", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Doch keiner kam \u2013 und schon der zweite Tag entschwand.", "tokens": ["Doch", "kei\u00b7ner", "kam", "\u2013", "und", "schon", "der", "zwei\u00b7te", "Tag", "ent\u00b7schwand", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$(", "KON", "ADV", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Zwei N\u00e4chte hatten merklich jetzt, wie ihr begreift,", "tokens": ["Zwei", "N\u00e4ch\u00b7te", "hat\u00b7ten", "merk\u00b7lich", "jetzt", ",", "wie", "ihr", "be\u00b7greift", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "ADJD", "ADV", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das volle Rund des Monds am Rande ausgeschweift.", "tokens": ["Das", "vol\u00b7le", "Rund", "des", "Monds", "am", "Ran\u00b7de", "aus\u00b7ge\u00b7schweift", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Verzweifelnd st\u00f6hnt der Fuchs \u2013 da kommt der Nimmersatt", "tokens": ["Ver\u00b7zwei\u00b7felnd", "st\u00f6hnt", "der", "Fuchs", "\u2013", "da", "kommt", "der", "Nim\u00b7mer\u00b7satt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "VVFIN", "ART", "NE", "$(", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von Wolf vorbei. \u00bbKamrad!\u00ab rief jener laut, \u00bbschau her,", "tokens": ["Von", "Wolf", "vor\u00b7bei", ".", "\u00bb", "Kam\u00b7rad", "!", "\u00ab", "rief", "je\u00b7ner", "laut", ",", "\u00bb", "schau", "her", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKVZ", "$.", "$(", "NN", "$.", "$(", "VVFIN", "PDAT", "ADJD", "$,", "$(", "ADJD", "PTKVZ", "$,"], "meter": "-+--++-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Erblickst du meinen Fund? Ein K\u00e4se rund und schwer,", "tokens": ["Er\u00b7blickst", "du", "mei\u00b7nen", "Fund", "?", "Ein", "K\u00e4\u00b7se", "rund", "und", "schwer", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$.", "ART", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Den Faun, der Gott, aus Ios Milch geknetet hat!", "tokens": ["Den", "Faun", ",", "der", "Gott", ",", "aus", "I\u00b7os", "Milch", "ge\u00b7kne\u00b7tet", "hat", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "APPR", "NE", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Selbst Jupiter gew\u00e4nne, w\u00e4r er krank,", "tokens": ["Selbst", "Ju\u00b7pi\u00b7ter", "ge\u00b7w\u00e4n\u00b7ne", ",", "w\u00e4r", "er", "krank", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "$,", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Durch ihn den Appetit zur\u00fcck", "tokens": ["Durch", "ihn", "den", "Ap\u00b7pe\u00b7tit", "zu\u00b7r\u00fcck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nach seinem edlen Nektartrank.", "tokens": ["Nach", "sei\u00b7nem", "ed\u00b7len", "Nekt\u00b7ar\u00b7trank", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich a\u00df vom Rande hier ein St\u00fcck,", "tokens": ["Ich", "a\u00df", "vom", "Ran\u00b7de", "hier", "ein", "St\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der Rest wird dir als Mahl gen\u00fcgen.", "tokens": ["Der", "Rest", "wird", "dir", "als", "Mahl", "ge\u00b7n\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "KOUS", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Ich g\u00f6nnte keinem andern,\u00ab fuhr er fort zu l\u00fcgen,", "tokens": ["Ich", "g\u00f6nn\u00b7te", "kei\u00b7nem", "an\u00b7dern", ",", "\u00ab", "fuhr", "er", "fort", "zu", "l\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "$,", "$(", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "\u00bbein solch olympisch Schlemmergl\u00fcck,", "tokens": ["\u00bb", "ein", "solch", "o\u00b7lym\u00b7pisch", "Schlem\u00b7mer\u00b7gl\u00fcck", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "PIAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Nur dir, mein liebster Freund, allein.", "tokens": ["Nur", "dir", ",", "mein", "liebs\u00b7ter", "Freund", ",", "al\u00b7lein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Genie\u00dfe dies mit vollen Z\u00fcgen.", "tokens": ["Ge\u00b7nie\u00b7\u00dfe", "dies", "mit", "vol\u00b7len", "Z\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Steig in den Eimer oben ein,", "tokens": ["Steig", "in", "den", "Ei\u00b7mer", "o\u00b7ben", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ADV", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.12": {"text": "Den ich f\u00fcr dich dort hingeh\u00e4ngt,", "tokens": ["Den", "ich", "f\u00fcr", "dich", "dort", "hin\u00b7ge\u00b7h\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Und fahr herab, um hier so recht vergn\u00fcgt zu sein.\u00ab", "tokens": ["Und", "fahr", "her\u00b7ab", ",", "um", "hier", "so", "recht", "ver\u00b7gn\u00fcgt", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "$,", "KOUI", "ADV", "ADV", "ADJD", "VVPP", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "H\u00fcbsch war die Rede ausgeschm\u00fcckt.", "tokens": ["H\u00fcbsch", "war", "die", "Re\u00b7de", "aus\u00b7ge\u00b7schm\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Der Wolf ist Narr genug, da\u00df er sich selber f\u00e4ngt.", "tokens": ["Der", "Wolf", "ist", "Narr", "ge\u00b7nug", ",", "da\u00df", "er", "sich", "sel\u00b7ber", "f\u00e4ngt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "NN", "ADV", "$,", "KOUS", "PPER", "PRF", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Der Rote lacht, wie alles gl\u00fcckt.", "tokens": ["Der", "Ro\u00b7te", "lacht", ",", "wie", "al\u00b7les", "gl\u00fcckt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PWAV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Der Eimer sinkt, und sein Gewicht", "tokens": ["Der", "Ei\u00b7mer", "sinkt", ",", "und", "sein", "Ge\u00b7wicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Entf\u00fchrt den andern Teil:", "tokens": ["Ent\u00b7f\u00fchrt", "den", "an\u00b7dern", "Teil", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.19": {"text": "Es steigt der Fuchs zum Sternenlicht,", "tokens": ["Es", "steigt", "der", "Fuchs", "zum", "Ster\u00b7nen\u00b7licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Nicht zu des Wolfes Heil.", "tokens": ["Nicht", "zu", "des", "Wol\u00b7fes", "Heil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Doch spottet dieses Toren nicht!", "tokens": ["Doch", "spot\u00b7tet", "die\u00b7ses", "To\u00b7ren", "nicht", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr selber la\u00dft euch gern und oft", "tokens": ["Ihr", "sel\u00b7ber", "la\u00dft", "euch", "gern", "und", "oft"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "PPER", "ADV", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Verf\u00fchren von so ungewissen Dingen.", "tokens": ["Ver\u00b7f\u00fch\u00b7ren", "von", "so", "un\u00b7ge\u00b7wis\u00b7sen", "Din\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Gar leicht ist's, euch in gleiche Not zu bringen,", "tokens": ["Gar", "leicht", "ist's", ",", "euch", "in", "glei\u00b7che", "Not", "zu", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "$,", "PPER", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Da jeder glaubt, was er erhofft.", "tokens": ["Da", "je\u00b7der", "glaubt", ",", "was", "er", "er\u00b7hofft", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}