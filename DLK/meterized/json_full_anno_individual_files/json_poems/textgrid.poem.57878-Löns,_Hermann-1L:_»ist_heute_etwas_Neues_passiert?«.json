{"textgrid.poem.57878": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "1L: \u00bbist heute etwas Neues passiert?\u00ab", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbist heute etwas Neues passiert?\u00ab", "tokens": ["\u00bb", "ist", "heu\u00b7te", "et\u00b7was", "Neu\u00b7es", "pas\u00b7siert", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "ADV", "PIAT", "NN", "VVFIN", "$.", "$("], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "So fragt man, wenn ich erscheine", "tokens": ["So", "fragt", "man", ",", "wenn", "ich", "er\u00b7schei\u00b7ne"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "$,", "KOUS", "PPER", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Am Stammtisch, aber meistenteils", "tokens": ["Am", "Stamm\u00b7tisch", ",", "a\u00b7ber", "meis\u00b7ten\u00b7teils"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPRART", "NN", "$,", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich diese Frage verneine.", "tokens": ["Ich", "die\u00b7se", "Fra\u00b7ge", "ver\u00b7nei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Doch als ich gestern zum Stammtisch ging,", "tokens": ["Doch", "als", "ich", "ge\u00b7stern", "zum", "Stamm\u00b7tisch", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Da hielt ich den Kopf viel h\u00f6her,", "tokens": ["Da", "hielt", "ich", "den", "Kopf", "viel", "h\u00f6\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Beinahe so hoch wie ein eben erst", "tokens": ["Bei\u00b7na\u00b7he", "so", "hoch", "wie", "ein", "e\u00b7ben", "erst"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "KOKOM", "ART", "ADV", "ADV"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Gew\u00e4hlter B\u00fcrgervorsteher.", "tokens": ["Ge\u00b7w\u00e4hl\u00b7ter", "B\u00fcr\u00b7ger\u00b7vor\u00b7ste\u00b7her", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Voll Stolz und Eifer lie\u00df ich mein Wort,", "tokens": ["Voll", "Stolz", "und", "Ei\u00b7fer", "lie\u00df", "ich", "mein", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Bevor ich mich setzte, erschallen:", "tokens": ["Be\u00b7vor", "ich", "mich", "setz\u00b7te", ",", "er\u00b7schal\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "$,", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "An der Sandstra\u00dfen\u00fcberf\u00fchrung ist", "tokens": ["An", "der", "Sand\u00b7stra\u00b7\u00dfen\u00b7\u00fc\u00b7berf\u00b7\u00fch\u00b7rung", "ist"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VAFIN"], "meter": "+-++-+-+-+", "measure": "zehnsilber"}, "line.4": {"text": "Ein Junge \u00fcberfallen.", "tokens": ["Ein", "Jun\u00b7ge", "\u00fc\u00b7berf\u00b7al\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "\u00bbwenn's weiter nichts ist,\u00ab sprach Schorse da,", "tokens": ["\u00bb", "wenn's", "wei\u00b7ter", "nichts", "ist", ",", "\u00ab", "sprach", "Schor\u00b7se", "da", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "ADV", "PIS", "VAFIN", "$,", "$(", "VVFIN", "NN", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbdas ist doch nicht wert des Geschreies;", "tokens": ["\u00bb", "das", "ist", "doch", "nicht", "wert", "des", "Ge\u00b7schrei\u00b7es", ";"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "ADV", "PTKNEG", "ADJD", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ein \u00dcberfall an der Sandstra\u00dfenbr\u00fcck',", "tokens": ["Ein", "\u00dc\u00b7berf\u00b7all", "an", "der", "Sand\u00b7stra\u00b7\u00dfen\u00b7br\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das ist doch wahrhaftig nichts Neues!\u00ab", "tokens": ["Das", "ist", "doch", "wahr\u00b7haf\u00b7tig", "nichts", "Neu\u00b7es", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "PIAT", "NN", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "\u00bbist heute etwas Neues passiert?\u00ab", "tokens": ["\u00bb", "ist", "heu\u00b7te", "et\u00b7was", "Neu\u00b7es", "pas\u00b7siert", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "ADV", "PIAT", "NN", "VVFIN", "$.", "$("], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "So fragt man, wenn ich erscheine", "tokens": ["So", "fragt", "man", ",", "wenn", "ich", "er\u00b7schei\u00b7ne"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "$,", "KOUS", "PPER", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Am Stammtisch, aber meistenteils", "tokens": ["Am", "Stamm\u00b7tisch", ",", "a\u00b7ber", "meis\u00b7ten\u00b7teils"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPRART", "NN", "$,", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich diese Frage verneine.", "tokens": ["Ich", "die\u00b7se", "Fra\u00b7ge", "ver\u00b7nei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Doch als ich gestern zum Stammtisch ging,", "tokens": ["Doch", "als", "ich", "ge\u00b7stern", "zum", "Stamm\u00b7tisch", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Da hielt ich den Kopf viel h\u00f6her,", "tokens": ["Da", "hielt", "ich", "den", "Kopf", "viel", "h\u00f6\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Beinahe so hoch wie ein eben erst", "tokens": ["Bei\u00b7na\u00b7he", "so", "hoch", "wie", "ein", "e\u00b7ben", "erst"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "KOKOM", "ART", "ADV", "ADV"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Gew\u00e4hlter B\u00fcrgervorsteher.", "tokens": ["Ge\u00b7w\u00e4hl\u00b7ter", "B\u00fcr\u00b7ger\u00b7vor\u00b7ste\u00b7her", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Voll Stolz und Eifer lie\u00df ich mein Wort,", "tokens": ["Voll", "Stolz", "und", "Ei\u00b7fer", "lie\u00df", "ich", "mein", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Bevor ich mich setzte, erschallen:", "tokens": ["Be\u00b7vor", "ich", "mich", "setz\u00b7te", ",", "er\u00b7schal\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "$,", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "An der Sandstra\u00dfen\u00fcberf\u00fchrung ist", "tokens": ["An", "der", "Sand\u00b7stra\u00b7\u00dfen\u00b7\u00fc\u00b7berf\u00b7\u00fch\u00b7rung", "ist"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VAFIN"], "meter": "+-++-+-+-+", "measure": "zehnsilber"}, "line.4": {"text": "Ein Junge \u00fcberfallen.", "tokens": ["Ein", "Jun\u00b7ge", "\u00fc\u00b7berf\u00b7al\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "\u00bbwenn's weiter nichts ist,\u00ab sprach Schorse da,", "tokens": ["\u00bb", "wenn's", "wei\u00b7ter", "nichts", "ist", ",", "\u00ab", "sprach", "Schor\u00b7se", "da", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "ADV", "PIS", "VAFIN", "$,", "$(", "VVFIN", "NN", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbdas ist doch nicht wert des Geschreies;", "tokens": ["\u00bb", "das", "ist", "doch", "nicht", "wert", "des", "Ge\u00b7schrei\u00b7es", ";"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "ADV", "PTKNEG", "ADJD", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ein \u00dcberfall an der Sandstra\u00dfenbr\u00fcck',", "tokens": ["Ein", "\u00dc\u00b7berf\u00b7all", "an", "der", "Sand\u00b7stra\u00b7\u00dfen\u00b7br\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das ist doch wahrhaftig nichts Neues!\u00ab", "tokens": ["Das", "ist", "doch", "wahr\u00b7haf\u00b7tig", "nichts", "Neu\u00b7es", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "PIAT", "NN", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}}}}