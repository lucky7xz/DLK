{"textgrid.poem.49647": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Splendid isolation", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Man war Hans Dampf in allen Gassen,", "tokens": ["Man", "war", "Hans", "Dampf", "in", "al\u00b7len", "Gas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "NE", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Blies jede Suppe weit und breit,", "tokens": ["Blies", "je\u00b7de", "Sup\u00b7pe", "weit", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt sind wir pl\u00f6tzlich ganz verlassen", "tokens": ["Jetzt", "sind", "wir", "pl\u00f6tz\u00b7lich", "ganz", "ver\u00b7las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hei\u00dfen's sch\u00f6ne Einsamkeit.", "tokens": ["Und", "hei\u00b7\u00dfen's", "sch\u00f6\u00b7ne", "Ein\u00b7sam\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wir teilten schmerzlich Ru\u00dflands N\u00f6te", "tokens": ["Wir", "teil\u00b7ten", "schmerz\u00b7lich", "Ru\u00df\u00b7lands", "N\u00f6\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "NE", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und waren tiefen Mitleids voll,", "tokens": ["Und", "wa\u00b7ren", "tie\u00b7fen", "Mit\u00b7leids", "voll", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJA", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir lieferten nach Rom den Goethe,", "tokens": ["Wir", "lie\u00b7fer\u00b7ten", "nach", "Rom", "den", "Goe\u00b7the", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "ART", "NE", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Und gratis zwar, mit Fracht und Zoll.", "tokens": ["Und", "gra\u00b7tis", "zwar", ",", "mit", "Fracht", "und", "Zoll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "$,", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Auch England hat uns oft gesehen,", "tokens": ["Auch", "En\u00b7gland", "hat", "uns", "oft", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir brachten unsre Liebe dar,", "tokens": ["Wir", "brach\u00b7ten", "uns\u00b7re", "Lie\u00b7be", "dar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir mu\u00dften neulich br\u00fcnstig flehen,", "tokens": ["Wir", "mu\u00df\u00b7ten", "neu\u00b7lich", "br\u00fcns\u00b7tig", "fle\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als Spaniens K\u00f6nig hiesig war.", "tokens": ["Als", "Spa\u00b7ni\u00b7ens", "K\u00f6\u00b7nig", "hie\u00b7sig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NN", "ADJD", "VAFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Wir liefern junge F\u00fcrstent\u00f6chter,", "tokens": ["Wir", "lie\u00b7fern", "jun\u00b7ge", "F\u00fcrs\u00b7ten\u00b7t\u00f6ch\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nach Holland auch den Prinzgemahl,", "tokens": ["Nach", "Hol\u00b7land", "auch", "den", "Prinz\u00b7ge\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, die regierenden Geschlechter", "tokens": ["Ja", ",", "die", "re\u00b7gie\u00b7ren\u00b7den", "Ge\u00b7schlech\u00b7ter"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sind meistens deutsches Material.", "tokens": ["Sind", "meis\u00b7tens", "deut\u00b7sches", "Ma\u00b7te\u00b7ri\u00b7al", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.5": {"line.1": {"text": "Wir lassen niemand ungeschoren", "tokens": ["Wir", "las\u00b7sen", "nie\u00b7mand", "un\u00b7ge\u00b7scho\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sind in allen F\u00e4llen da,", "tokens": ["Und", "sind", "in", "al\u00b7len", "F\u00e4l\u00b7len", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir tauschen alte Professoren", "tokens": ["Wir", "tau\u00b7schen", "al\u00b7te", "Pro\u00b7fes\u00b7so\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Gr\u00fc\u00dfe mit Amerika.", "tokens": ["Und", "Gr\u00fc\u00b7\u00dfe", "mit", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.6": {"line.1": {"text": "Geburten, Taufen oder Leichen", "tokens": ["Ge\u00b7bur\u00b7ten", ",", "Tau\u00b7fen", "o\u00b7der", "Lei\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Erfolgen niemals unbewacht,", "tokens": ["Er\u00b7fol\u00b7gen", "nie\u00b7mals", "un\u00b7be\u00b7wacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und Trauer- oder Freudenzeichen", "tokens": ["Und", "Trau\u00b7e\u00b7r", "o\u00b7der", "Freu\u00b7den\u00b7zei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "TRUNC", "KON", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hat stets der Telegraph gebracht.", "tokens": ["Hat", "stets", "der", "Te\u00b7le\u00b7gra\u00b7ph", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Recht wie ein Schmock, der dienstbeflissen", "tokens": ["Recht", "wie", "ein", "Schmock", ",", "der", "dienst\u00b7be\u00b7flis\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KOKOM", "ART", "NN", "$,", "PRELS", "PDAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Achtung vor sich selbst verliert,", "tokens": ["Die", "Ach\u00b7tung", "vor", "sich", "selbst", "ver\u00b7liert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PRF", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zudringlich und hinausgeschmissen, \u2013", "tokens": ["Zu\u00b7dring\u00b7lich", "und", "hin\u00b7aus\u00b7ge\u00b7schmis\u00b7sen", ",", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ADV", "KON", "VVPP", "$,", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das hei\u00dft man ", "tokens": ["Das", "hei\u00dft", "man"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VVFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.8": {"line.1": {"text": "Man war Hans Dampf in allen Gassen,", "tokens": ["Man", "war", "Hans", "Dampf", "in", "al\u00b7len", "Gas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "NE", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Blies jede Suppe weit und breit,", "tokens": ["Blies", "je\u00b7de", "Sup\u00b7pe", "weit", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt sind wir pl\u00f6tzlich ganz verlassen", "tokens": ["Jetzt", "sind", "wir", "pl\u00f6tz\u00b7lich", "ganz", "ver\u00b7las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hei\u00dfen's sch\u00f6ne Einsamkeit.", "tokens": ["Und", "hei\u00b7\u00dfen's", "sch\u00f6\u00b7ne", "Ein\u00b7sam\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wir teilten schmerzlich Ru\u00dflands N\u00f6te", "tokens": ["Wir", "teil\u00b7ten", "schmerz\u00b7lich", "Ru\u00df\u00b7lands", "N\u00f6\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "NE", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und waren tiefen Mitleids voll,", "tokens": ["Und", "wa\u00b7ren", "tie\u00b7fen", "Mit\u00b7leids", "voll", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJA", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir lieferten nach Rom den Goethe,", "tokens": ["Wir", "lie\u00b7fer\u00b7ten", "nach", "Rom", "den", "Goe\u00b7the", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "ART", "NE", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Und gratis zwar, mit Fracht und Zoll.", "tokens": ["Und", "gra\u00b7tis", "zwar", ",", "mit", "Fracht", "und", "Zoll", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "$,", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Auch England hat uns oft gesehen,", "tokens": ["Auch", "En\u00b7gland", "hat", "uns", "oft", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir brachten unsre Liebe dar,", "tokens": ["Wir", "brach\u00b7ten", "uns\u00b7re", "Lie\u00b7be", "dar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir mu\u00dften neulich br\u00fcnstig flehen,", "tokens": ["Wir", "mu\u00df\u00b7ten", "neu\u00b7lich", "br\u00fcns\u00b7tig", "fle\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als Spaniens K\u00f6nig hiesig war.", "tokens": ["Als", "Spa\u00b7ni\u00b7ens", "K\u00f6\u00b7nig", "hie\u00b7sig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NN", "ADJD", "VAFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Wir liefern junge F\u00fcrstent\u00f6chter,", "tokens": ["Wir", "lie\u00b7fern", "jun\u00b7ge", "F\u00fcrs\u00b7ten\u00b7t\u00f6ch\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nach Holland auch den Prinzgemahl,", "tokens": ["Nach", "Hol\u00b7land", "auch", "den", "Prinz\u00b7ge\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, die regierenden Geschlechter", "tokens": ["Ja", ",", "die", "re\u00b7gie\u00b7ren\u00b7den", "Ge\u00b7schlech\u00b7ter"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sind meistens deutsches Material.", "tokens": ["Sind", "meis\u00b7tens", "deut\u00b7sches", "Ma\u00b7te\u00b7ri\u00b7al", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.12": {"line.1": {"text": "Wir lassen niemand ungeschoren", "tokens": ["Wir", "las\u00b7sen", "nie\u00b7mand", "un\u00b7ge\u00b7scho\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sind in allen F\u00e4llen da,", "tokens": ["Und", "sind", "in", "al\u00b7len", "F\u00e4l\u00b7len", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir tauschen alte Professoren", "tokens": ["Wir", "tau\u00b7schen", "al\u00b7te", "Pro\u00b7fes\u00b7so\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Gr\u00fc\u00dfe mit Amerika.", "tokens": ["Und", "Gr\u00fc\u00b7\u00dfe", "mit", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.13": {"line.1": {"text": "Geburten, Taufen oder Leichen", "tokens": ["Ge\u00b7bur\u00b7ten", ",", "Tau\u00b7fen", "o\u00b7der", "Lei\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Erfolgen niemals unbewacht,", "tokens": ["Er\u00b7fol\u00b7gen", "nie\u00b7mals", "un\u00b7be\u00b7wacht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und Trauer- oder Freudenzeichen", "tokens": ["Und", "Trau\u00b7e\u00b7r", "o\u00b7der", "Freu\u00b7den\u00b7zei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "TRUNC", "KON", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hat stets der Telegraph gebracht.", "tokens": ["Hat", "stets", "der", "Te\u00b7le\u00b7gra\u00b7ph", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Recht wie ein Schmock, der dienstbeflissen", "tokens": ["Recht", "wie", "ein", "Schmock", ",", "der", "dienst\u00b7be\u00b7flis\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KOKOM", "ART", "NN", "$,", "PRELS", "PDAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Achtung vor sich selbst verliert,", "tokens": ["Die", "Ach\u00b7tung", "vor", "sich", "selbst", "ver\u00b7liert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PRF", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zudringlich und hinausgeschmissen, \u2013", "tokens": ["Zu\u00b7dring\u00b7lich", "und", "hin\u00b7aus\u00b7ge\u00b7schmis\u00b7sen", ",", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ADV", "KON", "VVPP", "$,", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das hei\u00dft man ", "tokens": ["Das", "hei\u00dft", "man"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VVFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}}}}}