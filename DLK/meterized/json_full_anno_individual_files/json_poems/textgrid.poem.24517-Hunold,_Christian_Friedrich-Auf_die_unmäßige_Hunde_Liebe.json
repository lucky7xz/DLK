{"textgrid.poem.24517": {"metadata": {"author": {"name": "Hunold, Christian Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Auf die unm\u00e4\u00dfige Hunde Liebe", "genre": "verse", "period": "N.A.", "pub_year": 1701, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie? sind die Hunde mehr/ als Menschen dein Ergetzen?", "tokens": ["Wie", "?", "sind", "die", "Hun\u00b7de", "mehr", "/", "als", "Men\u00b7schen", "dein", "Er\u00b7get\u00b7zen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "VAFIN", "ART", "NN", "ADV", "$(", "KOUS", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Sind sie in der Vernunft dir etwan gleich zu sch\u00e4tzen?", "tokens": ["Sind", "sie", "in", "der", "Ver\u00b7nunft", "dir", "et\u00b7wan", "gleich", "zu", "sch\u00e4t\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ART", "NN", "PPER", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was vor Vergn\u00fcgung hat doch ihr Gespr\u00e4ch in sich?", "tokens": ["Was", "vor", "Ver\u00b7gn\u00fc\u00b7gung", "hat", "doch", "ihr", "Ge\u00b7spr\u00e4ch", "in", "sich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "VAFIN", "ADV", "PPOSAT", "NN", "APPR", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Erbauen sie dein Hertz und unterrichten dich?", "tokens": ["Er\u00b7bau\u00b7en", "sie", "dein", "Hertz", "und", "un\u00b7ter\u00b7rich\u00b7ten", "dich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPOSAT", "NN", "KON", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Verdienen sie ihr Brod? O Armer/ sie verzehren", "tokens": ["Ver\u00b7die\u00b7nen", "sie", "ihr", "Brod", "?", "O", "Ar\u00b7mer", "/", "sie", "ver\u00b7zeh\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$.", "NE", "NN", "$(", "PPER", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Dich und dein ", "tokens": ["Dich", "und", "dein"], "token_info": ["word", "word", "word"], "pos": ["PPER", "KON", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Dieweil nun/ wie man sagt/ sich gleich und gleich gesellt/", "tokens": ["Die\u00b7weil", "nun", "/", "wie", "man", "sagt", "/", "sich", "gleich", "und", "gleich", "ge\u00b7sellt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "$(", "PWAV", "PIS", "VVFIN", "$(", "PRF", "ADV", "KON", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Was wunders/ da\u00df dein Hertz so viel von Hunden h\u00e4lt:", "tokens": ["Was", "wun\u00b7ders", "/", "da\u00df", "dein", "Hertz", "so", "viel", "von", "Hun\u00b7den", "h\u00e4lt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "$(", "KOUS", "PPOSAT", "NN", "ADV", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du bist dem Leben nach nicht in der Menschen Orden/", "tokens": ["Du", "bist", "dem", "Le\u00b7ben", "nach", "nicht", "in", "der", "Men\u00b7schen", "Or\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PTKNEG", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Und bi\u00df aus die Gestalt bereits zum Thier geworden.", "tokens": ["Und", "bi\u00df", "aus", "die", "Ge\u00b7stalt", "be\u00b7reits", "zum", "Thier", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "ADV", "APPRART", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wie? sind die Hunde mehr/ als Menschen dein Ergetzen?", "tokens": ["Wie", "?", "sind", "die", "Hun\u00b7de", "mehr", "/", "als", "Men\u00b7schen", "dein", "Er\u00b7get\u00b7zen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "VAFIN", "ART", "NN", "ADV", "$(", "KOUS", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Sind sie in der Vernunft dir etwan gleich zu sch\u00e4tzen?", "tokens": ["Sind", "sie", "in", "der", "Ver\u00b7nunft", "dir", "et\u00b7wan", "gleich", "zu", "sch\u00e4t\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ART", "NN", "PPER", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was vor Vergn\u00fcgung hat doch ihr Gespr\u00e4ch in sich?", "tokens": ["Was", "vor", "Ver\u00b7gn\u00fc\u00b7gung", "hat", "doch", "ihr", "Ge\u00b7spr\u00e4ch", "in", "sich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "VAFIN", "ADV", "PPOSAT", "NN", "APPR", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Erbauen sie dein Hertz und unterrichten dich?", "tokens": ["Er\u00b7bau\u00b7en", "sie", "dein", "Hertz", "und", "un\u00b7ter\u00b7rich\u00b7ten", "dich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPOSAT", "NN", "KON", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Verdienen sie ihr Brod? O Armer/ sie verzehren", "tokens": ["Ver\u00b7die\u00b7nen", "sie", "ihr", "Brod", "?", "O", "Ar\u00b7mer", "/", "sie", "ver\u00b7zeh\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$.", "NE", "NN", "$(", "PPER", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Dich und dein ", "tokens": ["Dich", "und", "dein"], "token_info": ["word", "word", "word"], "pos": ["PPER", "KON", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Dieweil nun/ wie man sagt/ sich gleich und gleich gesellt/", "tokens": ["Die\u00b7weil", "nun", "/", "wie", "man", "sagt", "/", "sich", "gleich", "und", "gleich", "ge\u00b7sellt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "$(", "PWAV", "PIS", "VVFIN", "$(", "PRF", "ADV", "KON", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Was wunders/ da\u00df dein Hertz so viel von Hunden h\u00e4lt:", "tokens": ["Was", "wun\u00b7ders", "/", "da\u00df", "dein", "Hertz", "so", "viel", "von", "Hun\u00b7den", "h\u00e4lt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "$(", "KOUS", "PPOSAT", "NN", "ADV", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du bist dem Leben nach nicht in der Menschen Orden/", "tokens": ["Du", "bist", "dem", "Le\u00b7ben", "nach", "nicht", "in", "der", "Men\u00b7schen", "Or\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PTKNEG", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Und bi\u00df aus die Gestalt bereits zum Thier geworden.", "tokens": ["Und", "bi\u00df", "aus", "die", "Ge\u00b7stalt", "be\u00b7reits", "zum", "Thier", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "ADV", "APPRART", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}