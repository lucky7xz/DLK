{"textgrid.poem.52917": {"metadata": {"author": {"name": "Dingelstedt, Franz von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Da\u00df sich mein Lied so grell ver\u00e4ndert,", "genre": "verse", "period": "N.A.", "pub_year": 1847, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Da\u00df sich mein Lied so grell ver\u00e4ndert,", "tokens": ["Da\u00df", "sich", "mein", "Lied", "so", "grell", "ver\u00b7\u00e4n\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPOSAT", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll keine Seele Wunder nehmen:", "tokens": ["Soll", "kei\u00b7ne", "See\u00b7le", "Wun\u00b7der", "neh\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIAT", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wer so umher im Lande schlendert,", "tokens": ["Wer", "so", "um\u00b7her", "im", "Lan\u00b7de", "schlen\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PTKVZ", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Lernt bald sich schicken und bequemen.", "tokens": ["Lernt", "bald", "sich", "schi\u00b7cken", "und", "be\u00b7que\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PRF", "VVINF", "KON", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der feine Ton, das noble Wesen,", "tokens": ["Der", "fei\u00b7ne", "Ton", ",", "das", "nob\u00b7le", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es \u00fcbt sich alle Stunde besser;", "tokens": ["Es", "\u00fcbt", "sich", "al\u00b7le", "Stun\u00b7de", "bes\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Schon kann ich jede Zeitung lesen", "tokens": ["Schon", "kann", "ich", "je\u00b7de", "Zei\u00b7tung", "le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PIAT", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hei\u00dfe \u00fcberall Professer.", "tokens": ["Und", "hei\u00b7\u00dfe", "\u00fc\u00b7be\u00b7rall", "Pro\u00b7fes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Sonst machten Kleider doch nur Leute,", "tokens": ["Sonst", "mach\u00b7ten", "Klei\u00b7der", "doch", "nur", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt auch Verstand und Witz und Wissen,", "tokens": ["Jetzt", "auch", "Ver\u00b7stand", "und", "Witz", "und", "Wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "KON", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Autoren sind die Schneider heute,", "tokens": ["Au\u00b7to\u00b7ren", "sind", "die", "Schnei\u00b7der", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NE", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und nur ein F\u00fcrst geht noch zerrissen.", "tokens": ["Und", "nur", "ein", "F\u00fcrst", "geht", "noch", "zer\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ein ander' St\u00e4dtchen, ander' M\u00e4dchen,", "tokens": ["Ein", "an\u00b7der'", "St\u00e4dt\u00b7chen", ",", "an\u00b7der'", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So sangen einst die Studiosen;", "tokens": ["So", "san\u00b7gen", "einst", "die", "Stu\u00b7di\u00b7o\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt lautet es: ein ander' St\u00e4dtchen,", "tokens": ["Jetzt", "lau\u00b7tet", "es", ":", "ein", "an\u00b7der'", "St\u00e4dt\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein andrer Rock und andre Hosen.", "tokens": ["Ein", "an\u00b7drer", "Rock", "und", "and\u00b7re", "Ho\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Der steht in seinem Reisepasse", "tokens": ["Der", "steht", "in", "sei\u00b7nem", "Rei\u00b7se\u00b7pas\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als Hoftheater-Lampenputzer,", "tokens": ["Als", "Hof\u00b7the\u00b7a\u00b7ter\u00b7Lam\u00b7pen\u00b7put\u00b7zer", ","], "token_info": ["word", "word", "punct"], "pos": ["KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begegnest du ihm auf der Gasse,", "tokens": ["Be\u00b7geg\u00b7nest", "du", "ihm", "auf", "der", "Gas\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Riecht er nach Haar\u00f6l nur, der Stutzer.", "tokens": ["Riecht", "er", "nach", "Haa\u00b7r\u00f6l", "nur", ",", "der", "Stut\u00b7zer", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "ADV", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Als Vagabond zog mancher L\u00fcmmel", "tokens": ["Als", "Va\u00b7ga\u00b7bond", "zog", "man\u00b7cher", "L\u00fcm\u00b7mel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Schusters Rappen aus dem Tore;", "tokens": ["Auf", "Schus\u00b7ters", "Rap\u00b7pen", "aus", "dem", "To\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie kommt er wieder? Hoch zu Schimmel,", "tokens": ["Wie", "kommt", "er", "wie\u00b7der", "?", "Hoch", "zu", "Schim\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "$.", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Den Hut auf einem (Esels-) Ohre.", "tokens": ["Den", "Hut", "auf", "ei\u00b7nem", "(", "E\u00b7sels", ")", "Oh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "$(", "TRUNC", "$(", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Man tituliert ihn Euer Gnaden,", "tokens": ["Man", "ti\u00b7tu\u00b7liert", "ihn", "Eu\u00b7er", "Gna\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er h\u00e4lt sich Hunde und Lakaien,", "tokens": ["Er", "h\u00e4lt", "sich", "Hun\u00b7de", "und", "La\u00b7kai\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und h\u00e4tt' er nicht so dicke Waden,", "tokens": ["Und", "h\u00e4tt'", "er", "nicht", "so", "di\u00b7cke", "Wa\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PTKNEG", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man w\u00fcrd' als Grafen ihn verschreien.", "tokens": ["Man", "w\u00fcrd'", "als", "Gra\u00b7fen", "ihn", "ver\u00b7schrei\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "KOKOM", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Die Zeit hat alles ausgeglichen,", "tokens": ["Die", "Zeit", "hat", "al\u00b7les", "aus\u00b7ge\u00b7gli\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und was scheinst, das bist du eben;", "tokens": ["Und", "was", "scheinst", ",", "das", "bist", "du", "e\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "$,", "PDS", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dem Adel wird sein \u00bbVon\u00ab gestrichen,", "tokens": ["Dem", "A\u00b7del", "wird", "sein", "\u00bb", "Von", "\u00ab", "ge\u00b7stri\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "$(", "APPR", "$(", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Um es dem Schreiber aufzukleben.", "tokens": ["Um", "es", "dem", "Schrei\u00b7ber", "auf\u00b7zu\u00b7kle\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Sollt' ich der Mode mich nicht f\u00fcgen,", "tokens": ["Sollt'", "ich", "der", "Mo\u00b7de", "mich", "nicht", "f\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Weltgesetz f\u00fcr alles Wandern?", "tokens": ["Dem", "Welt\u00b7ge\u00b7setz", "f\u00fcr", "al\u00b7les", "Wan\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nachtw\u00e4chter gar und Dichter liegen", "tokens": ["Nacht\u00b7w\u00e4ch\u00b7ter", "gar", "und", "Dich\u00b7ter", "lie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "KON", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der eine nicht so weit vom andern.", "tokens": ["Der", "ei\u00b7ne", "nicht", "so", "weit", "vom", "an\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "PTKNEG", "ADV", "ADJD", "APPRART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Die graue Puppe ist zerbrochen,", "tokens": ["Die", "grau\u00b7e", "Pup\u00b7pe", "ist", "zer\u00b7bro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nun steh' ich im Phal\u00e4nen-Alter,", "tokens": ["Nun", "steh'", "ich", "im", "Pha\u00b7l\u00e4\u00b7nen\u00b7Al\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und was aus jener ausgekrochen,", "tokens": ["Und", "was", "aus", "je\u00b7ner", "aus\u00b7ge\u00b7kro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "PDAT", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sagt, ists ein Nacht-, ein Tage-Falter??", "tokens": ["Sagt", ",", "ists", "ein", "Nacht", ",", "ein", "Ta\u00b7ge\u00b7Fal\u00b7ter", "??"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VAFIN", "ART", "TRUNC", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Da\u00df sich mein Lied so grell ver\u00e4ndert,", "tokens": ["Da\u00df", "sich", "mein", "Lied", "so", "grell", "ver\u00b7\u00e4n\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPOSAT", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll keine Seele Wunder nehmen:", "tokens": ["Soll", "kei\u00b7ne", "See\u00b7le", "Wun\u00b7der", "neh\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIAT", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wer so umher im Lande schlendert,", "tokens": ["Wer", "so", "um\u00b7her", "im", "Lan\u00b7de", "schlen\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PTKVZ", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Lernt bald sich schicken und bequemen.", "tokens": ["Lernt", "bald", "sich", "schi\u00b7cken", "und", "be\u00b7que\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PRF", "VVINF", "KON", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Der feine Ton, das noble Wesen,", "tokens": ["Der", "fei\u00b7ne", "Ton", ",", "das", "nob\u00b7le", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es \u00fcbt sich alle Stunde besser;", "tokens": ["Es", "\u00fcbt", "sich", "al\u00b7le", "Stun\u00b7de", "bes\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Schon kann ich jede Zeitung lesen", "tokens": ["Schon", "kann", "ich", "je\u00b7de", "Zei\u00b7tung", "le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PIAT", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hei\u00dfe \u00fcberall Professer.", "tokens": ["Und", "hei\u00b7\u00dfe", "\u00fc\u00b7be\u00b7rall", "Pro\u00b7fes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Sonst machten Kleider doch nur Leute,", "tokens": ["Sonst", "mach\u00b7ten", "Klei\u00b7der", "doch", "nur", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt auch Verstand und Witz und Wissen,", "tokens": ["Jetzt", "auch", "Ver\u00b7stand", "und", "Witz", "und", "Wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "KON", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Autoren sind die Schneider heute,", "tokens": ["Au\u00b7to\u00b7ren", "sind", "die", "Schnei\u00b7der", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NE", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und nur ein F\u00fcrst geht noch zerrissen.", "tokens": ["Und", "nur", "ein", "F\u00fcrst", "geht", "noch", "zer\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Ein ander' St\u00e4dtchen, ander' M\u00e4dchen,", "tokens": ["Ein", "an\u00b7der'", "St\u00e4dt\u00b7chen", ",", "an\u00b7der'", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So sangen einst die Studiosen;", "tokens": ["So", "san\u00b7gen", "einst", "die", "Stu\u00b7di\u00b7o\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt lautet es: ein ander' St\u00e4dtchen,", "tokens": ["Jetzt", "lau\u00b7tet", "es", ":", "ein", "an\u00b7der'", "St\u00e4dt\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein andrer Rock und andre Hosen.", "tokens": ["Ein", "an\u00b7drer", "Rock", "und", "and\u00b7re", "Ho\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Der steht in seinem Reisepasse", "tokens": ["Der", "steht", "in", "sei\u00b7nem", "Rei\u00b7se\u00b7pas\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als Hoftheater-Lampenputzer,", "tokens": ["Als", "Hof\u00b7the\u00b7a\u00b7ter\u00b7Lam\u00b7pen\u00b7put\u00b7zer", ","], "token_info": ["word", "word", "punct"], "pos": ["KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begegnest du ihm auf der Gasse,", "tokens": ["Be\u00b7geg\u00b7nest", "du", "ihm", "auf", "der", "Gas\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Riecht er nach Haar\u00f6l nur, der Stutzer.", "tokens": ["Riecht", "er", "nach", "Haa\u00b7r\u00f6l", "nur", ",", "der", "Stut\u00b7zer", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "ADV", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Als Vagabond zog mancher L\u00fcmmel", "tokens": ["Als", "Va\u00b7ga\u00b7bond", "zog", "man\u00b7cher", "L\u00fcm\u00b7mel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Schusters Rappen aus dem Tore;", "tokens": ["Auf", "Schus\u00b7ters", "Rap\u00b7pen", "aus", "dem", "To\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie kommt er wieder? Hoch zu Schimmel,", "tokens": ["Wie", "kommt", "er", "wie\u00b7der", "?", "Hoch", "zu", "Schim\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "$.", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Den Hut auf einem (Esels-) Ohre.", "tokens": ["Den", "Hut", "auf", "ei\u00b7nem", "(", "E\u00b7sels", ")", "Oh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "$(", "TRUNC", "$(", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Man tituliert ihn Euer Gnaden,", "tokens": ["Man", "ti\u00b7tu\u00b7liert", "ihn", "Eu\u00b7er", "Gna\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er h\u00e4lt sich Hunde und Lakaien,", "tokens": ["Er", "h\u00e4lt", "sich", "Hun\u00b7de", "und", "La\u00b7kai\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und h\u00e4tt' er nicht so dicke Waden,", "tokens": ["Und", "h\u00e4tt'", "er", "nicht", "so", "di\u00b7cke", "Wa\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PTKNEG", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man w\u00fcrd' als Grafen ihn verschreien.", "tokens": ["Man", "w\u00fcrd'", "als", "Gra\u00b7fen", "ihn", "ver\u00b7schrei\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "KOKOM", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Die Zeit hat alles ausgeglichen,", "tokens": ["Die", "Zeit", "hat", "al\u00b7les", "aus\u00b7ge\u00b7gli\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und was scheinst, das bist du eben;", "tokens": ["Und", "was", "scheinst", ",", "das", "bist", "du", "e\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "$,", "PDS", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dem Adel wird sein \u00bbVon\u00ab gestrichen,", "tokens": ["Dem", "A\u00b7del", "wird", "sein", "\u00bb", "Von", "\u00ab", "ge\u00b7stri\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "$(", "APPR", "$(", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Um es dem Schreiber aufzukleben.", "tokens": ["Um", "es", "dem", "Schrei\u00b7ber", "auf\u00b7zu\u00b7kle\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Sollt' ich der Mode mich nicht f\u00fcgen,", "tokens": ["Sollt'", "ich", "der", "Mo\u00b7de", "mich", "nicht", "f\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Weltgesetz f\u00fcr alles Wandern?", "tokens": ["Dem", "Welt\u00b7ge\u00b7setz", "f\u00fcr", "al\u00b7les", "Wan\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nachtw\u00e4chter gar und Dichter liegen", "tokens": ["Nacht\u00b7w\u00e4ch\u00b7ter", "gar", "und", "Dich\u00b7ter", "lie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "KON", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der eine nicht so weit vom andern.", "tokens": ["Der", "ei\u00b7ne", "nicht", "so", "weit", "vom", "an\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "PTKNEG", "ADV", "ADJD", "APPRART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Die graue Puppe ist zerbrochen,", "tokens": ["Die", "grau\u00b7e", "Pup\u00b7pe", "ist", "zer\u00b7bro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nun steh' ich im Phal\u00e4nen-Alter,", "tokens": ["Nun", "steh'", "ich", "im", "Pha\u00b7l\u00e4\u00b7nen\u00b7Al\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und was aus jener ausgekrochen,", "tokens": ["Und", "was", "aus", "je\u00b7ner", "aus\u00b7ge\u00b7kro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "PDAT", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sagt, ists ein Nacht-, ein Tage-Falter??", "tokens": ["Sagt", ",", "ists", "ein", "Nacht", ",", "ein", "Ta\u00b7ge\u00b7Fal\u00b7ter", "??"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VAFIN", "ART", "TRUNC", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}