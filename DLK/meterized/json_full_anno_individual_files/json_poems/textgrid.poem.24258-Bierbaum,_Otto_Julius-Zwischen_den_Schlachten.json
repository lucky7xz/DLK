{"textgrid.poem.24258": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "Zwischen den Schlachten", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das Gesch\u00e4ft in Bomben und Torpedos geht", "tokens": ["Das", "Ge\u00b7sch\u00e4ft", "in", "Bom\u00b7ben", "und", "Tor\u00b7pe\u00b7dos", "geht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NE", "VVFIN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Augenblicklich in Ostasien ziemlich stille.", "tokens": ["Au\u00b7gen\u00b7blick\u00b7lich", "in", "O\u00b7sta\u00b7si\u00b7en", "ziem\u00b7lich", "stil\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "ADV", "VVFIN", "$."], "meter": "+-+---+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Seitdem die japanische Flotte nach Wladiwostock", "tokens": ["Seit\u00b7dem", "die", "ja\u00b7pa\u00b7ni\u00b7sche", "Flot\u00b7te", "nach", "Wla\u00b7di\u00b7wos\u00b7tock"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "APPR", "NE"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "F\u00fcr zweimalhunderttausend Rubel Stahlzylinder geschmissen hat,", "tokens": ["F\u00fcr", "zwei\u00b7mal\u00b7hun\u00b7dert\u00b7tau\u00b7send", "Ru\u00b7bel", "Stahl\u00b7zy\u00b7lin\u00b7der", "ge\u00b7schmis\u00b7sen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.5": {"text": "Ohne betr\u00e4chtlichen Schaden anzurichten und,", "tokens": ["Oh\u00b7ne", "be\u00b7tr\u00e4cht\u00b7li\u00b7chen", "Scha\u00b7den", "an\u00b7zu\u00b7rich\u00b7ten", "und", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVIZU", "KON", "$,"], "meter": "+--+--+-+-+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "Infolgedessen, ohne der Weltgeschichte", "tokens": ["In\u00b7fol\u00b7ge\u00b7des\u00b7sen", ",", "oh\u00b7ne", "der", "Welt\u00b7ge\u00b7schich\u00b7te"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "KOUI", "ART", "NN"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Ein neues Kapitel einzuverleiben, ist", "tokens": ["Ein", "neu\u00b7es", "Ka\u00b7pi\u00b7tel", "ein\u00b7zu\u00b7ver\u00b7lei\u00b7ben", ",", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VVIZU", "$,", "VAFIN"], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "So gut wie noch weniger passiert, es sei denn,", "tokens": ["So", "gut", "wie", "noch", "we\u00b7ni\u00b7ger", "pas\u00b7siert", ",", "es", "sei", "denn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ADV", "ADV", "VVFIN", "$,", "PPER", "VAFIN", "ADV", "$,"], "meter": "-+--+---+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Da\u00df ich die \u00c4u\u00dferung jenes Adjutanten", "tokens": ["Da\u00df", "ich", "die", "\u00c4u\u00b7\u00dfe\u00b7rung", "je\u00b7nes", "Ad\u00b7ju\u00b7tan\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "PDAT", "NN"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Des Generals Kuropatkin erw\u00e4hne, der", "tokens": ["Des", "Ge\u00b7ne\u00b7rals", "Ku\u00b7ro\u00b7pat\u00b7kin", "er\u00b7w\u00e4h\u00b7ne", ",", "der"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "$,", "PRELS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Sich heute schon einen alten Hut voll freut, indem er", "tokens": ["Sich", "heu\u00b7te", "schon", "ei\u00b7nen", "al\u00b7ten", "Hut", "voll", "freut", ",", "in\u00b7dem", "er"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PRF", "ADV", "ADV", "ART", "ADJA", "NN", "ADJD", "VVFIN", "$,", "KOUS", "PPER"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Sich vorstellt, wie er mit den \u00fcbrigen Helden", "tokens": ["Sich", "vor\u00b7stellt", ",", "wie", "er", "mit", "den", "\u00fcb\u00b7ri\u00b7gen", "Hel\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PRF", "VVFIN", "$,", "PWAV", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "Des heiligen Ru\u00dfland eine Spritztour durch Japan", "tokens": ["Des", "hei\u00b7li\u00b7gen", "Ru\u00df\u00b7land", "ei\u00b7ne", "Spritz\u00b7tour", "durch", "Ja\u00b7pan"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "ART", "NN", "APPR", "NE"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.14": {"text": "Macht und die niedlichen Geishas aus n\u00e4chster N\u00e4he", "tokens": ["Macht", "und", "die", "nied\u00b7li\u00b7chen", "Geis\u00b7has", "aus", "n\u00e4chs\u00b7ter", "N\u00e4\u00b7he"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ART", "ADJA", "NN", "APPR", "ADJA", "NN"], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.15": {"text": "Kennen lernt und statt Wuttki Sake s\u00e4uft.", "tokens": ["Ken\u00b7nen", "lernt", "und", "statt", "Wutt\u00b7ki", "Sa\u00b7ke", "s\u00e4uft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "APPR", "NE", "NE", "VVFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Aus diesem Grunde scheint es angebracht, Betrachtungen", "tokens": ["Aus", "die\u00b7sem", "Grun\u00b7de", "scheint", "es", "an\u00b7ge\u00b7bracht", ",", "Be\u00b7trach\u00b7tun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "VVPP", "$,", "NN"], "meter": "-+-+-+-+-+-+--", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Ganz allgemeiner Natur dar\u00fcber anzustellen,", "tokens": ["Ganz", "all\u00b7ge\u00b7mei\u00b7ner", "Na\u00b7tur", "da\u00b7r\u00fc\u00b7ber", "an\u00b7zu\u00b7stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "PAV", "VVIZU", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Wohin sich nun wohl eigentlich unsre", "tokens": ["Wo\u00b7hin", "sich", "nun", "wohl", "ei\u00b7gent\u00b7lich", "uns\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "ADV", "ADV", "ADV", "PPOSAT"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sympathien zu wenden haben; denn", "tokens": ["Sym\u00b7pa\u00b7thi\u00b7en", "zu", "wen\u00b7den", "ha\u00b7ben", ";", "denn"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["NN", "PTKZU", "VVINF", "VAFIN", "$.", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Das Vergn\u00fcgen an einem ausw\u00e4rtigen Kriege ist nur halb,", "tokens": ["Das", "Ver\u00b7gn\u00fc\u00b7gen", "an", "ei\u00b7nem", "aus\u00b7w\u00e4r\u00b7ti\u00b7gen", "Krie\u00b7ge", "ist", "nur", "halb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "--+--+--+--+-+-+", "measure": "anapaest.tetra.plus"}, "line.6": {"text": "Wenn man nicht ganz genau und sicher wei\u00df:", "tokens": ["Wenn", "man", "nicht", "ganz", "ge\u00b7nau", "und", "si\u00b7cher", "wei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "ADV", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Welcher der beiden ist meiner Teilnahme w\u00fcrdig?", "tokens": ["Wel\u00b7cher", "der", "bei\u00b7den", "ist", "mei\u00b7ner", "Teil\u00b7nah\u00b7me", "w\u00fcr\u00b7dig", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "ART", "PIAT", "VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}}, "stanza.3": {"line.1": {"text": "Nun k\u00f6nnte man freilich sagen: \u00bbDummes Zeug, sie sind", "tokens": ["Nun", "k\u00f6nn\u00b7te", "man", "frei\u00b7lich", "sa\u00b7gen", ":", "\u00bb", "Dum\u00b7mes", "Zeug", ",", "sie", "sind"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "VVINF", "$.", "$(", "ADJA", "NN", "$,", "PPER", "VAFIN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Mir alle beide gleicherma\u00dfen pipe,\u00ab \u2013 aber", "tokens": ["Mir", "al\u00b7le", "bei\u00b7de", "glei\u00b7cher\u00b7ma\u00b7\u00dfen", "pi\u00b7pe", ",", "\u00ab", "\u2013", "a\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "PIAT", "PIS", "ADV", "VVFIN", "$,", "$(", "$(", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dann ist die Sache eben ohne jeden Reiz. \u2013 Nein:", "tokens": ["Dann", "ist", "die", "Sa\u00b7che", "e\u00b7ben", "oh\u00b7ne", "je\u00b7den", "Reiz", ".", "\u2013", "Nein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "APPR", "PIAT", "NN", "$.", "$(", "PTKANT", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "Ich m\u00f6chte wirklich wissen: W\u00fcnsche ich", "tokens": ["Ich", "m\u00f6ch\u00b7te", "wirk\u00b7lich", "wis\u00b7sen", ":", "W\u00fcn\u00b7sche", "ich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$.", "NN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "V\u00e4terchen den Sieg oder dem Mikado?", "tokens": ["V\u00e4\u00b7ter\u00b7chen", "den", "Sieg", "o\u00b7der", "dem", "Mi\u00b7ka\u00b7do", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.4": {"line.1": {"text": "V\u00e4terchen ist mir wohlbekannt; er ist", "tokens": ["V\u00e4\u00b7ter\u00b7chen", "ist", "mir", "wohl\u00b7be\u00b7kannt", ";", "er", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADJD", "$.", "PPER", "VAFIN"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Mit dem Gro\u00dfherzoge von Hessen verwandt, und", "tokens": ["Mit", "dem", "Gro\u00df\u00b7her\u00b7zo\u00b7ge", "von", "Hes\u00b7sen", "ver\u00b7wandt", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "ART", "NN", "APPR", "NE", "VVPP", "$,", "KON"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Jedes Jahr wohnt er ein paar Wochen in Darmstadt.", "tokens": ["Je\u00b7des", "Jahr", "wohnt", "er", "ein", "paar", "Wo\u00b7chen", "in", "Darm\u00b7stadt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "ART", "PIAT", "NN", "APPR", "NE", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Dort geht er spazieren wie ein gew\u00f6hnlicher Mensch,", "tokens": ["Dort", "geht", "er", "spa\u00b7zie\u00b7ren", "wie", "ein", "ge\u00b7w\u00f6hn\u00b7li\u00b7cher", "Mensch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "VVINF", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-+--+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Hat ein kleines, weiches H\u00fctchen auf und interessiert sich", "tokens": ["Hat", "ein", "klei\u00b7nes", ",", "wei\u00b7ches", "H\u00fct\u00b7chen", "auf", "und", "in\u00b7ter\u00b7es\u00b7siert", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "$,", "ADJA", "NN", "PTKVZ", "KON", "VVFIN", "PRF"], "meter": "+-+-+-+-+-+--+-", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "F\u00fcr Professor Olbrichs Dreieckornamente.", "tokens": ["F\u00fcr", "Pro\u00b7fes\u00b7sor", "O\u00b7lbrichs", "Drei\u00b7ec\u00b7kor\u00b7na\u00b7men\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NE", "NE", "$."], "meter": "+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.7": {"text": "Manchmal unterh\u00e4lt er sich mit Ernst Ludwig", "tokens": ["Manch\u00b7mal", "un\u00b7ter\u00b7h\u00e4lt", "er", "sich", "mit", "Ernst", "Lud\u00b7wig"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "NE", "NE"], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.8": {"text": "\u00dcber die verflossene K\u00fcnstlerkolonie und", "tokens": ["\u00dc\u00b7ber", "die", "ver\u00b7flos\u00b7se\u00b7ne", "K\u00fcnst\u00b7ler\u00b7ko\u00b7lo\u00b7nie", "und"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "KON"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.9": {"text": "\u00dcber das Wetter: Da\u00df es ver\u00e4nderlich ist,", "tokens": ["\u00dc\u00b7ber", "das", "Wet\u00b7ter", ":", "Da\u00df", "es", "ver\u00b7\u00e4n\u00b7der\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "KOUS", "PPER", "ADJD", "VAFIN", "$,"], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}, "line.10": {"text": "Wie F\u00fcrstenlaunen, und manchmal l\u00e4\u00dft er", "tokens": ["Wie", "F\u00fcrs\u00b7ten\u00b7lau\u00b7nen", ",", "und", "manch\u00b7mal", "l\u00e4\u00dft", "er"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "$,", "KON", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Eine Bemerkung dar\u00fcber fallen, da\u00df", "tokens": ["Ei\u00b7ne", "Be\u00b7mer\u00b7kung", "da\u00b7r\u00fc\u00b7ber", "fal\u00b7len", ",", "da\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "PAV", "VVINF", "$,", "KOUS"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.12": {"text": "Seinem Geschmacke Darm-Athen besser behagt, als", "tokens": ["Sei\u00b7nem", "Ge\u00b7schma\u00b7cke", "Dar\u00b7mA\u00b7then", "bes\u00b7ser", "be\u00b7hagt", ",", "als"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPOSAT", "NN", "NE", "ADJD", "VVPP", "$,", "KOUS"], "meter": "+--+--+-+--+-", "measure": "dactylic.di.plus"}, "line.13": {"text": "Berlin an der Spree, obwohl oder weil in dieser Stadt ... jedoch", "tokens": ["Ber\u00b7lin", "an", "der", "Spree", ",", "ob\u00b7wohl", "o\u00b7der", "weil", "in", "die\u00b7ser", "Stadt", "...", "je\u00b7doch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NE", "APPR", "ART", "NE", "$,", "KOUS", "KON", "KOUS", "APPR", "PDAT", "NN", "$(", "ADV"], "meter": "-+--+--+-+-+-+-+", "measure": "amphibrach.tri.plus"}, "line.14": {"text": "Das f\u00fchrt zu weit. \u2013 Vom japanischen Mikado wei\u00df", "tokens": ["Das", "f\u00fchrt", "zu", "weit", ".", "\u2013", "Vom", "ja\u00b7pa\u00b7ni\u00b7schen", "Mi\u00b7ka\u00b7do", "wei\u00df"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PTKA", "ADJD", "$.", "$(", "APPRART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "Ich weniger. Das Bild, das Sullivan", "tokens": ["Ich", "we\u00b7ni\u00b7ger", ".", "Das", "Bild", ",", "das", "Sul\u00b7li\u00b7van"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "ADV", "$.", "ART", "NN", "$,", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Von ihm in Walzertakten entworfen hat,", "tokens": ["Von", "ihm", "in", "Wal\u00b7zer\u00b7tak\u00b7ten", "ent\u00b7wor\u00b7fen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.17": {"text": "Scheint stark geschmeichelt zu sein; es hei\u00dft,", "tokens": ["Scheint", "stark", "ge\u00b7schmei\u00b7chelt", "zu", "sein", ";", "es", "hei\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VVPP", "PTKZU", "VAINF", "$.", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Er sei nicht halb so am\u00fcsant in Wirklichkeit; doch", "tokens": ["Er", "sei", "nicht", "halb", "so", "a\u00b7m\u00fcs\u00b7ant", "in", "Wirk\u00b7lich\u00b7keit", ";", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "ADV", "ADJD", "APPR", "NN", "$.", "ADV"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.19": {"text": "Soll er einen Garten voll Chrysanthemen besitzen, in dem", "tokens": ["Soll", "er", "ei\u00b7nen", "Gar\u00b7ten", "voll", "Chry\u00b7san\u00b7the\u00b7men", "be\u00b7sit\u00b7zen", ",", "in", "dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "PPER", "ART", "NN", "ADJD", "NN", "VVINF", "$,", "APPR", "ART"], "meter": "+-+-+--++-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.20": {"text": "So viele Arten dieser Blume wachsen, wie", "tokens": ["So", "vie\u00b7le", "Ar\u00b7ten", "die\u00b7ser", "Blu\u00b7me", "wach\u00b7sen", ",", "wie"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "PIAT", "NN", "PDAT", "NN", "VVINF", "$,", "PWAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Ein Europ\u00e4er es sich durchaus nicht vorstellen kann.", "tokens": ["Ein", "Eu\u00b7ro\u00b7p\u00e4\u00b7er", "es", "sich", "durc\u00b7haus", "nicht", "vor\u00b7stel\u00b7len", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PRF", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.5": {"line.1": {"text": "Demnach st\u00fcnde der Zar mir zweifellos n\u00e4her, und", "tokens": ["Dem\u00b7nach", "st\u00fcn\u00b7de", "der", "Zar", "mir", "zwei\u00b7fel\u00b7los", "n\u00e4\u00b7her", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PAV", "VVFIN", "ART", "NN", "PPER", "ADJD", "ADJD", "$,", "KON"], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ich habe auch wirklich einige Neigung, ihm", "tokens": ["Ich", "ha\u00b7be", "auch", "wirk\u00b7lich", "ei\u00b7ni\u00b7ge", "Nei\u00b7gung", ",", "ihm"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "PIAT", "NN", "$,", "PPER"], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Den Sieg zu w\u00fcnschen, aber ich sage mir", "tokens": ["Den", "Sieg", "zu", "w\u00fcn\u00b7schen", ",", "a\u00b7ber", "ich", "sa\u00b7ge", "mir"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "KON", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Dennoch manchmal: ein paar Hiebe", "tokens": ["Den\u00b7noch", "manch\u00b7mal", ":", "ein", "paar", "Hie\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$.", "ART", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "K\u00f6nnten den Russen auch nicht schaden, denn", "tokens": ["K\u00f6nn\u00b7ten", "den", "Rus\u00b7sen", "auch", "nicht", "scha\u00b7den", ",", "denn"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VMFIN", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$,", "KON"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.6": {"text": "Schie\u00dft die Knute (das Bild ist k\u00fchn) zu sehr ins Kraut,", "tokens": ["Schie\u00dft", "die", "Knu\u00b7te", "(", "das", "Bild", "ist", "k\u00fchn", ")", "zu", "sehr", "ins", "Kraut", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$(", "ART", "NN", "VAFIN", "ADJD", "$(", "PTKA", "ADV", "APPRART", "NN", "$,"], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "Langt sie am Ende zu uns her\u00fcber, und", "tokens": ["Langt", "sie", "am", "En\u00b7de", "zu", "uns", "her\u00b7\u00fc\u00b7ber", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "APPR", "PPER", "ADV", "$,", "KON"], "meter": "++-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.8": {"text": "Eigentlich haben wir selber schon genug", "tokens": ["Ei\u00b7gent\u00b7lich", "ha\u00b7ben", "wir", "sel\u00b7ber", "schon", "ge\u00b7nug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "ADV"], "meter": "---+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Knuto\u00efde Einrichtungen im Deutschen Reiche.", "tokens": ["Knu\u00b7to\u00ef\u00b7de", "Ein\u00b7rich\u00b7tun\u00b7gen", "im", "Deut\u00b7schen", "Rei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPRART", "ADJA", "NE", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Wendet sich aber mein Sinn sympathisch dann", "tokens": ["Wen\u00b7det", "sich", "a\u00b7ber", "mein", "Sinn", "sym\u00b7pa\u00b7thisch", "dann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "PPOSAT", "NN", "ADJD", "ADV"], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Hin zum Reiche der aufgehenden Sonne, so", "tokens": ["Hin", "zum", "Rei\u00b7che", "der", "auf\u00b7ge\u00b7hen\u00b7den", "Son\u00b7ne", ",", "so"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NN", "APPRART", "NE", "ART", "ADJA", "NN", "$,", "ADV"], "meter": "+-+--+---+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wird mir doch gleich bange, denn schlie\u00dflich:", "tokens": ["Wird", "mir", "doch", "gleich", "ban\u00b7ge", ",", "denn", "schlie\u00df\u00b7lich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "$,", "KON", "ADJD", "$."], "meter": "----+--+-", "measure": "iambic.di.relaxed"}, "line.4": {"text": "Was in aller Welt geht mich denn Japan an?", "tokens": ["Was", "in", "al\u00b7ler", "Welt", "geht", "mich", "denn", "Ja\u00b7pan", "an", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PIAT", "NN", "VVFIN", "PPER", "ADV", "NE", "PTKVZ", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.5": {"text": "Kawakami zwar hat in Erstaunen mich,", "tokens": ["Ka\u00b7wa\u00b7ka\u00b7mi", "zwar", "hat", "in", "Er\u00b7stau\u00b7nen", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "APPR", "NN", "PPER", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.6": {"text": "Mu\u00df ich gestehen, heftiger gesetzt, als selbst", "tokens": ["Mu\u00df", "ich", "ge\u00b7ste\u00b7hen", ",", "hef\u00b7ti\u00b7ger", "ge\u00b7setzt", ",", "als", "selbst"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "PPER", "VVPP", "$,", "ADJD", "VVPP", "$,", "KOUS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Josef Kainz, denn sein Harakiri", "tokens": ["Jo\u00b7sef", "Kainz", ",", "denn", "sein", "Ha\u00b7ra\u00b7ki\u00b7ri"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "$,", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "War eine angenehme Leistung, und seine reizende Frau,", "tokens": ["War", "ei\u00b7ne", "an\u00b7ge\u00b7neh\u00b7me", "Leis\u00b7tung", ",", "und", "sei\u00b7ne", "rei\u00b7zen\u00b7de", "Frau", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$,", "KON", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}, "line.9": {"text": "Sadda \u2013 Yakko, ist ein s\u00fc\u00dfes Ding, das", "tokens": ["Sad\u00b7da", "\u2013", "Y\u00b7ak\u00b7ko", ",", "ist", "ein", "s\u00fc\u00b7\u00dfes", "Ding", ",", "das"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "$,", "VAFIN", "ART", "ADJA", "NN", "$,", "PRELS"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Nur mit immer neuer R\u00fchrung ich", "tokens": ["Nur", "mit", "im\u00b7mer", "neu\u00b7er", "R\u00fch\u00b7rung", "ich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN", "PPER"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.11": {"text": "Lachen und weinen als Kesah sah. Aber,", "tokens": ["La\u00b7chen", "und", "wei\u00b7nen", "als", "Ke\u00b7sah", "sah", ".", "A\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "KON", "VVINF", "KOKOM", "NE", "VVFIN", "$.", "KON", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.12": {"text": "Selbst wenn ich Hokusai und Utamaro und", "tokens": ["Selbst", "wenn", "ich", "Ho\u00b7ku\u00b7sai", "und", "Ut\u00b7a\u00b7ma\u00b7ro", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "NE", "KON", "NE", "KON"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Noch ein Dutzend schwer merkbarer Namen mir", "tokens": ["Noch", "ein", "Dut\u00b7zend", "schwer", "merk\u00b7ba\u00b7rer", "Na\u00b7men", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "ADJD", "ADJA", "NN", "PPER"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.14": {"text": "Ins Ged\u00e4chtnis rufe und mit Dankbarkeit", "tokens": ["Ins", "Ge\u00b7d\u00e4cht\u00b7nis", "ru\u00b7fe", "und", "mit", "Dank\u00b7bar\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "KON", "APPR", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.15": {"text": "An Lackschatullen denke und R\u00e4uchergef\u00e4\u00dfe", "tokens": ["An", "Lack\u00b7scha\u00b7tul\u00b7len", "den\u00b7ke", "und", "R\u00e4u\u00b7cher\u00b7ge\u00b7f\u00e4\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "KON", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Und seidene Kockemonos und die Dichterin sei Schonagon, \u2013 ich", "tokens": ["Und", "sei\u00b7de\u00b7ne", "Ko\u00b7cke\u00b7mo\u00b7nos", "und", "die", "Dich\u00b7te\u00b7rin", "sei", "Scho\u00b7na\u00b7gon", ",", "\u2013", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word"], "pos": ["KON", "PPOSAT", "NN", "KON", "ART", "NN", "VAFIN", "NE", "$,", "$(", "PPER"], "meter": "-+--+--+--+-+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.17": {"text": "Kann mir nicht helfen, mir wird nicht warm dabei;", "tokens": ["Kann", "mir", "nicht", "hel\u00b7fen", ",", "mir", "wird", "nicht", "warm", "da\u00b7bei", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKNEG", "VVINF", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "PAV", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.18": {"text": "Die gelben \u00c4ffchen bleiben mir ewig Hose wie Jacke.", "tokens": ["Die", "gel\u00b7ben", "\u00c4ff\u00b7chen", "blei\u00b7ben", "mir", "e\u00b7wig", "Ho\u00b7se", "wie", "Ja\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADJD", "NN", "KOKOM", "NE", "$."], "meter": "-+-+-+--+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.19": {"text": "Was also tu ich mit meiner Sympathie?", "tokens": ["Was", "al\u00b7so", "tu", "ich", "mit", "mei\u00b7ner", "Sym\u00b7pa\u00b7thie", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "Z\u00e4hl ich die Kn\u00f6pfe an meinem \u00dcberrock ab, oder", "tokens": ["Z\u00e4hl", "ich", "die", "Kn\u00f6p\u00b7fe", "an", "mei\u00b7nem", "\u00dc\u00b7ber\u00b7rock", "ab", ",", "o\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NN", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$,", "KON"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.21": {"text": "Rupf ich die Bl\u00e4ttchen einem Chrysanthem\u00fcmchen aus:", "tokens": ["Rupf", "ich", "die", "Bl\u00e4tt\u00b7chen", "ei\u00b7nem", "Chry\u00b7san\u00b7the\u00b7m\u00fcm\u00b7chen", "aus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "Mikado \u2013 V\u00e4terchen, Mikado \u2013 V\u00e4terchen? Oder", "tokens": ["Mi\u00b7ka\u00b7do", "\u2013", "V\u00e4\u00b7ter\u00b7chen", ",", "Mi\u00b7ka\u00b7do", "\u2013", "V\u00e4\u00b7ter\u00b7chen", "?", "O\u00b7der"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$(", "NN", "$,", "NE", "$(", "NN", "$.", "NE"], "meter": "-+-+---+-+--+-", "measure": "iambic.penta.relaxed"}, "line.23": {"text": "Wart ichs ergeben ab, was Bernhard B\u00fclow in seiner Eigenschaft", "tokens": ["Wart", "ichs", "er\u00b7ge\u00b7ben", "ab", ",", "was", "Bern\u00b7hard", "B\u00fc\u00b7low", "in", "sei\u00b7ner", "Ei\u00b7gen\u00b7schaft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "VVFIN", "PTKVZ", "$,", "PRELS", "NE", "NE", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.24": {"text": "Als Kanzler des Deutschen Reiches f\u00fcr richtig finden wird?", "tokens": ["Als", "Kanz\u00b7ler", "des", "Deut\u00b7schen", "Rei\u00b7ches", "f\u00fcr", "rich\u00b7tig", "fin\u00b7den", "wird", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "ADJA", "NN", "APPR", "ADJD", "VVINF", "VAFIN", "$."], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "Oder gedulde ich mich so lange, bis der m\u00e4nnermordende", "tokens": ["O\u00b7der", "ge\u00b7dul\u00b7de", "ich", "mich", "so", "lan\u00b7ge", ",", "bis", "der", "m\u00e4n\u00b7ner\u00b7mor\u00b7den\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PRF", "ADV", "ADV", "$,", "APPR", "ART", "ADJA"], "meter": "-+-+--+-+-+-+-+--", "measure": "iambic.septa.relaxed"}, "line.26": {"text": "Gott der Schlachten mit ", "tokens": ["Gott", "der", "Schlach\u00b7ten", "mit"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "APPR"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.27": {"text": "Von den beiden er ", "tokens": ["Von", "den", "bei\u00b7den", "er"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "PIAT", "PPER"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.28": {"text": "Nein, nichts von alledem gedenke ich zu tun: ich", "tokens": ["Nein", ",", "nichts", "von", "al\u00b7le\u00b7dem", "ge\u00b7den\u00b7ke", "ich", "zu", "tun", ":", "ich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKANT", "$,", "PIS", "APPR", "PIS", "VVFIN", "PPER", "PTKZU", "VVINF", "$.", "PPER"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Lege mein n\u00e4chstes Honorar (und w\u00e4rens gleich zwanzig Mark)", "tokens": ["Le\u00b7ge", "mein", "n\u00e4chs\u00b7tes", "Ho\u00b7no\u00b7rar", "(", "und", "w\u00e4\u00b7rens", "gleich", "zwan\u00b7zig", "Mark", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "ADJA", "NN", "$(", "KON", "VAFIN", "ADV", "CARD", "NN", "$("], "meter": "+--+-+-+-+--+-+", "measure": "iambic.septa.invert"}, "line.30": {"text": "In Japan- oder Russen-Papieren an, je nachdem", "tokens": ["In", "Ja\u00b7pan", "o\u00b7der", "Rus\u00b7sen\u00b7Pa\u00b7pie\u00b7ren", "an", ",", "je", "nach\u00b7dem"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "TRUNC", "KON", "NN", "PTKVZ", "$,", "ADV", "KOUS"], "meter": "--+--+--+--+--", "measure": "anapaest.tetra.plus"}, "line.31": {"text": "Mein Leibbankier die Konjunktur beurteilt, \u2013 und", "tokens": ["Mein", "Leib\u00b7ban\u00b7kier", "die", "Kon\u00b7junk\u00b7tur", "beur\u00b7teilt", ",", "\u2013", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word"], "pos": ["PPOSAT", "NN", "ART", "NN", "VVFIN", "$,", "$(", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.32": {"text": "Von diesem Augenblicke an wei\u00df ich bestimmt, wohin", "tokens": ["Von", "die\u00b7sem", "Au\u00b7gen\u00b7bli\u00b7cke", "an", "wei\u00df", "ich", "be\u00b7stimmt", ",", "wo\u00b7hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PDAT", "NN", "APPR", "VVFIN", "PPER", "VVPP", "$,", "PWAV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.33": {"text": "Die Nadel meiner Sympathie sich wenden mu\u00df.", "tokens": ["Die", "Na\u00b7del", "mei\u00b7ner", "Sym\u00b7pa\u00b7thie", "sich", "wen\u00b7den", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Das Gesch\u00e4ft in Bomben und Torpedos geht", "tokens": ["Das", "Ge\u00b7sch\u00e4ft", "in", "Bom\u00b7ben", "und", "Tor\u00b7pe\u00b7dos", "geht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NE", "VVFIN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Augenblicklich in Ostasien ziemlich stille.", "tokens": ["Au\u00b7gen\u00b7blick\u00b7lich", "in", "O\u00b7sta\u00b7si\u00b7en", "ziem\u00b7lich", "stil\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "ADV", "VVFIN", "$."], "meter": "+-+---+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Seitdem die japanische Flotte nach Wladiwostock", "tokens": ["Seit\u00b7dem", "die", "ja\u00b7pa\u00b7ni\u00b7sche", "Flot\u00b7te", "nach", "Wla\u00b7di\u00b7wos\u00b7tock"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "APPR", "NE"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "F\u00fcr zweimalhunderttausend Rubel Stahlzylinder geschmissen hat,", "tokens": ["F\u00fcr", "zwei\u00b7mal\u00b7hun\u00b7dert\u00b7tau\u00b7send", "Ru\u00b7bel", "Stahl\u00b7zy\u00b7lin\u00b7der", "ge\u00b7schmis\u00b7sen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.5": {"text": "Ohne betr\u00e4chtlichen Schaden anzurichten und,", "tokens": ["Oh\u00b7ne", "be\u00b7tr\u00e4cht\u00b7li\u00b7chen", "Scha\u00b7den", "an\u00b7zu\u00b7rich\u00b7ten", "und", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVIZU", "KON", "$,"], "meter": "+--+--+-+-+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "Infolgedessen, ohne der Weltgeschichte", "tokens": ["In\u00b7fol\u00b7ge\u00b7des\u00b7sen", ",", "oh\u00b7ne", "der", "Welt\u00b7ge\u00b7schich\u00b7te"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "KOUI", "ART", "NN"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Ein neues Kapitel einzuverleiben, ist", "tokens": ["Ein", "neu\u00b7es", "Ka\u00b7pi\u00b7tel", "ein\u00b7zu\u00b7ver\u00b7lei\u00b7ben", ",", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VVIZU", "$,", "VAFIN"], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "So gut wie noch weniger passiert, es sei denn,", "tokens": ["So", "gut", "wie", "noch", "we\u00b7ni\u00b7ger", "pas\u00b7siert", ",", "es", "sei", "denn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ADV", "ADV", "VVFIN", "$,", "PPER", "VAFIN", "ADV", "$,"], "meter": "-+--+---+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Da\u00df ich die \u00c4u\u00dferung jenes Adjutanten", "tokens": ["Da\u00df", "ich", "die", "\u00c4u\u00b7\u00dfe\u00b7rung", "je\u00b7nes", "Ad\u00b7ju\u00b7tan\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "PDAT", "NN"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Des Generals Kuropatkin erw\u00e4hne, der", "tokens": ["Des", "Ge\u00b7ne\u00b7rals", "Ku\u00b7ro\u00b7pat\u00b7kin", "er\u00b7w\u00e4h\u00b7ne", ",", "der"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "$,", "PRELS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Sich heute schon einen alten Hut voll freut, indem er", "tokens": ["Sich", "heu\u00b7te", "schon", "ei\u00b7nen", "al\u00b7ten", "Hut", "voll", "freut", ",", "in\u00b7dem", "er"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PRF", "ADV", "ADV", "ART", "ADJA", "NN", "ADJD", "VVFIN", "$,", "KOUS", "PPER"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Sich vorstellt, wie er mit den \u00fcbrigen Helden", "tokens": ["Sich", "vor\u00b7stellt", ",", "wie", "er", "mit", "den", "\u00fcb\u00b7ri\u00b7gen", "Hel\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PRF", "VVFIN", "$,", "PWAV", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "Des heiligen Ru\u00dfland eine Spritztour durch Japan", "tokens": ["Des", "hei\u00b7li\u00b7gen", "Ru\u00df\u00b7land", "ei\u00b7ne", "Spritz\u00b7tour", "durch", "Ja\u00b7pan"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "ART", "NN", "APPR", "NE"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.14": {"text": "Macht und die niedlichen Geishas aus n\u00e4chster N\u00e4he", "tokens": ["Macht", "und", "die", "nied\u00b7li\u00b7chen", "Geis\u00b7has", "aus", "n\u00e4chs\u00b7ter", "N\u00e4\u00b7he"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ART", "ADJA", "NN", "APPR", "ADJA", "NN"], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.15": {"text": "Kennen lernt und statt Wuttki Sake s\u00e4uft.", "tokens": ["Ken\u00b7nen", "lernt", "und", "statt", "Wutt\u00b7ki", "Sa\u00b7ke", "s\u00e4uft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "APPR", "NE", "NE", "VVFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "Aus diesem Grunde scheint es angebracht, Betrachtungen", "tokens": ["Aus", "die\u00b7sem", "Grun\u00b7de", "scheint", "es", "an\u00b7ge\u00b7bracht", ",", "Be\u00b7trach\u00b7tun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "VVPP", "$,", "NN"], "meter": "-+-+-+-+-+-+--", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Ganz allgemeiner Natur dar\u00fcber anzustellen,", "tokens": ["Ganz", "all\u00b7ge\u00b7mei\u00b7ner", "Na\u00b7tur", "da\u00b7r\u00fc\u00b7ber", "an\u00b7zu\u00b7stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "PAV", "VVIZU", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Wohin sich nun wohl eigentlich unsre", "tokens": ["Wo\u00b7hin", "sich", "nun", "wohl", "ei\u00b7gent\u00b7lich", "uns\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "ADV", "ADV", "ADV", "PPOSAT"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sympathien zu wenden haben; denn", "tokens": ["Sym\u00b7pa\u00b7thi\u00b7en", "zu", "wen\u00b7den", "ha\u00b7ben", ";", "denn"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["NN", "PTKZU", "VVINF", "VAFIN", "$.", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Das Vergn\u00fcgen an einem ausw\u00e4rtigen Kriege ist nur halb,", "tokens": ["Das", "Ver\u00b7gn\u00fc\u00b7gen", "an", "ei\u00b7nem", "aus\u00b7w\u00e4r\u00b7ti\u00b7gen", "Krie\u00b7ge", "ist", "nur", "halb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "--+--+--+--+-+-+", "measure": "anapaest.tetra.plus"}, "line.6": {"text": "Wenn man nicht ganz genau und sicher wei\u00df:", "tokens": ["Wenn", "man", "nicht", "ganz", "ge\u00b7nau", "und", "si\u00b7cher", "wei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "ADV", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Welcher der beiden ist meiner Teilnahme w\u00fcrdig?", "tokens": ["Wel\u00b7cher", "der", "bei\u00b7den", "ist", "mei\u00b7ner", "Teil\u00b7nah\u00b7me", "w\u00fcr\u00b7dig", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "ART", "PIAT", "VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}}, "stanza.9": {"line.1": {"text": "Nun k\u00f6nnte man freilich sagen: \u00bbDummes Zeug, sie sind", "tokens": ["Nun", "k\u00f6nn\u00b7te", "man", "frei\u00b7lich", "sa\u00b7gen", ":", "\u00bb", "Dum\u00b7mes", "Zeug", ",", "sie", "sind"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "VVINF", "$.", "$(", "ADJA", "NN", "$,", "PPER", "VAFIN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Mir alle beide gleicherma\u00dfen pipe,\u00ab \u2013 aber", "tokens": ["Mir", "al\u00b7le", "bei\u00b7de", "glei\u00b7cher\u00b7ma\u00b7\u00dfen", "pi\u00b7pe", ",", "\u00ab", "\u2013", "a\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "PIAT", "PIS", "ADV", "VVFIN", "$,", "$(", "$(", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dann ist die Sache eben ohne jeden Reiz. \u2013 Nein:", "tokens": ["Dann", "ist", "die", "Sa\u00b7che", "e\u00b7ben", "oh\u00b7ne", "je\u00b7den", "Reiz", ".", "\u2013", "Nein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "APPR", "PIAT", "NN", "$.", "$(", "PTKANT", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "Ich m\u00f6chte wirklich wissen: W\u00fcnsche ich", "tokens": ["Ich", "m\u00f6ch\u00b7te", "wirk\u00b7lich", "wis\u00b7sen", ":", "W\u00fcn\u00b7sche", "ich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$.", "NN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "V\u00e4terchen den Sieg oder dem Mikado?", "tokens": ["V\u00e4\u00b7ter\u00b7chen", "den", "Sieg", "o\u00b7der", "dem", "Mi\u00b7ka\u00b7do", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.10": {"line.1": {"text": "V\u00e4terchen ist mir wohlbekannt; er ist", "tokens": ["V\u00e4\u00b7ter\u00b7chen", "ist", "mir", "wohl\u00b7be\u00b7kannt", ";", "er", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADJD", "$.", "PPER", "VAFIN"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Mit dem Gro\u00dfherzoge von Hessen verwandt, und", "tokens": ["Mit", "dem", "Gro\u00df\u00b7her\u00b7zo\u00b7ge", "von", "Hes\u00b7sen", "ver\u00b7wandt", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "ART", "NN", "APPR", "NE", "VVPP", "$,", "KON"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Jedes Jahr wohnt er ein paar Wochen in Darmstadt.", "tokens": ["Je\u00b7des", "Jahr", "wohnt", "er", "ein", "paar", "Wo\u00b7chen", "in", "Darm\u00b7stadt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "ART", "PIAT", "NN", "APPR", "NE", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Dort geht er spazieren wie ein gew\u00f6hnlicher Mensch,", "tokens": ["Dort", "geht", "er", "spa\u00b7zie\u00b7ren", "wie", "ein", "ge\u00b7w\u00f6hn\u00b7li\u00b7cher", "Mensch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "VVINF", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-+--+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Hat ein kleines, weiches H\u00fctchen auf und interessiert sich", "tokens": ["Hat", "ein", "klei\u00b7nes", ",", "wei\u00b7ches", "H\u00fct\u00b7chen", "auf", "und", "in\u00b7ter\u00b7es\u00b7siert", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "$,", "ADJA", "NN", "PTKVZ", "KON", "VVFIN", "PRF"], "meter": "+-+-+-+-+-+--+-", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "F\u00fcr Professor Olbrichs Dreieckornamente.", "tokens": ["F\u00fcr", "Pro\u00b7fes\u00b7sor", "O\u00b7lbrichs", "Drei\u00b7ec\u00b7kor\u00b7na\u00b7men\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NE", "NE", "$."], "meter": "+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.7": {"text": "Manchmal unterh\u00e4lt er sich mit Ernst Ludwig", "tokens": ["Manch\u00b7mal", "un\u00b7ter\u00b7h\u00e4lt", "er", "sich", "mit", "Ernst", "Lud\u00b7wig"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "NE", "NE"], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.8": {"text": "\u00dcber die verflossene K\u00fcnstlerkolonie und", "tokens": ["\u00dc\u00b7ber", "die", "ver\u00b7flos\u00b7se\u00b7ne", "K\u00fcnst\u00b7ler\u00b7ko\u00b7lo\u00b7nie", "und"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "KON"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.9": {"text": "\u00dcber das Wetter: Da\u00df es ver\u00e4nderlich ist,", "tokens": ["\u00dc\u00b7ber", "das", "Wet\u00b7ter", ":", "Da\u00df", "es", "ver\u00b7\u00e4n\u00b7der\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "KOUS", "PPER", "ADJD", "VAFIN", "$,"], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}, "line.10": {"text": "Wie F\u00fcrstenlaunen, und manchmal l\u00e4\u00dft er", "tokens": ["Wie", "F\u00fcrs\u00b7ten\u00b7lau\u00b7nen", ",", "und", "manch\u00b7mal", "l\u00e4\u00dft", "er"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "$,", "KON", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Eine Bemerkung dar\u00fcber fallen, da\u00df", "tokens": ["Ei\u00b7ne", "Be\u00b7mer\u00b7kung", "da\u00b7r\u00fc\u00b7ber", "fal\u00b7len", ",", "da\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "PAV", "VVINF", "$,", "KOUS"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.12": {"text": "Seinem Geschmacke Darm-Athen besser behagt, als", "tokens": ["Sei\u00b7nem", "Ge\u00b7schma\u00b7cke", "Dar\u00b7mA\u00b7then", "bes\u00b7ser", "be\u00b7hagt", ",", "als"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPOSAT", "NN", "NE", "ADJD", "VVPP", "$,", "KOUS"], "meter": "+--+--+-+--+-", "measure": "dactylic.di.plus"}, "line.13": {"text": "Berlin an der Spree, obwohl oder weil in dieser Stadt ... jedoch", "tokens": ["Ber\u00b7lin", "an", "der", "Spree", ",", "ob\u00b7wohl", "o\u00b7der", "weil", "in", "die\u00b7ser", "Stadt", "...", "je\u00b7doch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NE", "APPR", "ART", "NE", "$,", "KOUS", "KON", "KOUS", "APPR", "PDAT", "NN", "$(", "ADV"], "meter": "-+--+--+-+-+-+-+", "measure": "amphibrach.tri.plus"}, "line.14": {"text": "Das f\u00fchrt zu weit. \u2013 Vom japanischen Mikado wei\u00df", "tokens": ["Das", "f\u00fchrt", "zu", "weit", ".", "\u2013", "Vom", "ja\u00b7pa\u00b7ni\u00b7schen", "Mi\u00b7ka\u00b7do", "wei\u00df"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PTKA", "ADJD", "$.", "$(", "APPRART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "Ich weniger. Das Bild, das Sullivan", "tokens": ["Ich", "we\u00b7ni\u00b7ger", ".", "Das", "Bild", ",", "das", "Sul\u00b7li\u00b7van"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "ADV", "$.", "ART", "NN", "$,", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Von ihm in Walzertakten entworfen hat,", "tokens": ["Von", "ihm", "in", "Wal\u00b7zer\u00b7tak\u00b7ten", "ent\u00b7wor\u00b7fen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.17": {"text": "Scheint stark geschmeichelt zu sein; es hei\u00dft,", "tokens": ["Scheint", "stark", "ge\u00b7schmei\u00b7chelt", "zu", "sein", ";", "es", "hei\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VVPP", "PTKZU", "VAINF", "$.", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Er sei nicht halb so am\u00fcsant in Wirklichkeit; doch", "tokens": ["Er", "sei", "nicht", "halb", "so", "a\u00b7m\u00fcs\u00b7ant", "in", "Wirk\u00b7lich\u00b7keit", ";", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "ADV", "ADJD", "APPR", "NN", "$.", "ADV"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.19": {"text": "Soll er einen Garten voll Chrysanthemen besitzen, in dem", "tokens": ["Soll", "er", "ei\u00b7nen", "Gar\u00b7ten", "voll", "Chry\u00b7san\u00b7the\u00b7men", "be\u00b7sit\u00b7zen", ",", "in", "dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "PPER", "ART", "NN", "ADJD", "NN", "VVINF", "$,", "APPR", "ART"], "meter": "+-+-+--++-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.20": {"text": "So viele Arten dieser Blume wachsen, wie", "tokens": ["So", "vie\u00b7le", "Ar\u00b7ten", "die\u00b7ser", "Blu\u00b7me", "wach\u00b7sen", ",", "wie"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "PIAT", "NN", "PDAT", "NN", "VVINF", "$,", "PWAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Ein Europ\u00e4er es sich durchaus nicht vorstellen kann.", "tokens": ["Ein", "Eu\u00b7ro\u00b7p\u00e4\u00b7er", "es", "sich", "durc\u00b7haus", "nicht", "vor\u00b7stel\u00b7len", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PRF", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.11": {"line.1": {"text": "Demnach st\u00fcnde der Zar mir zweifellos n\u00e4her, und", "tokens": ["Dem\u00b7nach", "st\u00fcn\u00b7de", "der", "Zar", "mir", "zwei\u00b7fel\u00b7los", "n\u00e4\u00b7her", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PAV", "VVFIN", "ART", "NN", "PPER", "ADJD", "ADJD", "$,", "KON"], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ich habe auch wirklich einige Neigung, ihm", "tokens": ["Ich", "ha\u00b7be", "auch", "wirk\u00b7lich", "ei\u00b7ni\u00b7ge", "Nei\u00b7gung", ",", "ihm"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "PIAT", "NN", "$,", "PPER"], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Den Sieg zu w\u00fcnschen, aber ich sage mir", "tokens": ["Den", "Sieg", "zu", "w\u00fcn\u00b7schen", ",", "a\u00b7ber", "ich", "sa\u00b7ge", "mir"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "KON", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Dennoch manchmal: ein paar Hiebe", "tokens": ["Den\u00b7noch", "manch\u00b7mal", ":", "ein", "paar", "Hie\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$.", "ART", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "K\u00f6nnten den Russen auch nicht schaden, denn", "tokens": ["K\u00f6nn\u00b7ten", "den", "Rus\u00b7sen", "auch", "nicht", "scha\u00b7den", ",", "denn"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VMFIN", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$,", "KON"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.6": {"text": "Schie\u00dft die Knute (das Bild ist k\u00fchn) zu sehr ins Kraut,", "tokens": ["Schie\u00dft", "die", "Knu\u00b7te", "(", "das", "Bild", "ist", "k\u00fchn", ")", "zu", "sehr", "ins", "Kraut", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$(", "ART", "NN", "VAFIN", "ADJD", "$(", "PTKA", "ADV", "APPRART", "NN", "$,"], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "Langt sie am Ende zu uns her\u00fcber, und", "tokens": ["Langt", "sie", "am", "En\u00b7de", "zu", "uns", "her\u00b7\u00fc\u00b7ber", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "APPR", "PPER", "ADV", "$,", "KON"], "meter": "++-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.8": {"text": "Eigentlich haben wir selber schon genug", "tokens": ["Ei\u00b7gent\u00b7lich", "ha\u00b7ben", "wir", "sel\u00b7ber", "schon", "ge\u00b7nug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "ADV"], "meter": "---+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Knuto\u00efde Einrichtungen im Deutschen Reiche.", "tokens": ["Knu\u00b7to\u00ef\u00b7de", "Ein\u00b7rich\u00b7tun\u00b7gen", "im", "Deut\u00b7schen", "Rei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPRART", "ADJA", "NE", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Wendet sich aber mein Sinn sympathisch dann", "tokens": ["Wen\u00b7det", "sich", "a\u00b7ber", "mein", "Sinn", "sym\u00b7pa\u00b7thisch", "dann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "PPOSAT", "NN", "ADJD", "ADV"], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Hin zum Reiche der aufgehenden Sonne, so", "tokens": ["Hin", "zum", "Rei\u00b7che", "der", "auf\u00b7ge\u00b7hen\u00b7den", "Son\u00b7ne", ",", "so"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NN", "APPRART", "NE", "ART", "ADJA", "NN", "$,", "ADV"], "meter": "+-+--+---+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wird mir doch gleich bange, denn schlie\u00dflich:", "tokens": ["Wird", "mir", "doch", "gleich", "ban\u00b7ge", ",", "denn", "schlie\u00df\u00b7lich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "$,", "KON", "ADJD", "$."], "meter": "----+--+-", "measure": "iambic.di.relaxed"}, "line.4": {"text": "Was in aller Welt geht mich denn Japan an?", "tokens": ["Was", "in", "al\u00b7ler", "Welt", "geht", "mich", "denn", "Ja\u00b7pan", "an", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PIAT", "NN", "VVFIN", "PPER", "ADV", "NE", "PTKVZ", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.5": {"text": "Kawakami zwar hat in Erstaunen mich,", "tokens": ["Ka\u00b7wa\u00b7ka\u00b7mi", "zwar", "hat", "in", "Er\u00b7stau\u00b7nen", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "APPR", "NN", "PPER", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.6": {"text": "Mu\u00df ich gestehen, heftiger gesetzt, als selbst", "tokens": ["Mu\u00df", "ich", "ge\u00b7ste\u00b7hen", ",", "hef\u00b7ti\u00b7ger", "ge\u00b7setzt", ",", "als", "selbst"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "PPER", "VVPP", "$,", "ADJD", "VVPP", "$,", "KOUS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Josef Kainz, denn sein Harakiri", "tokens": ["Jo\u00b7sef", "Kainz", ",", "denn", "sein", "Ha\u00b7ra\u00b7ki\u00b7ri"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "$,", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "War eine angenehme Leistung, und seine reizende Frau,", "tokens": ["War", "ei\u00b7ne", "an\u00b7ge\u00b7neh\u00b7me", "Leis\u00b7tung", ",", "und", "sei\u00b7ne", "rei\u00b7zen\u00b7de", "Frau", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$,", "KON", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}, "line.9": {"text": "Sadda \u2013 Yakko, ist ein s\u00fc\u00dfes Ding, das", "tokens": ["Sad\u00b7da", "\u2013", "Y\u00b7ak\u00b7ko", ",", "ist", "ein", "s\u00fc\u00b7\u00dfes", "Ding", ",", "das"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "$,", "VAFIN", "ART", "ADJA", "NN", "$,", "PRELS"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Nur mit immer neuer R\u00fchrung ich", "tokens": ["Nur", "mit", "im\u00b7mer", "neu\u00b7er", "R\u00fch\u00b7rung", "ich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN", "PPER"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.11": {"text": "Lachen und weinen als Kesah sah. Aber,", "tokens": ["La\u00b7chen", "und", "wei\u00b7nen", "als", "Ke\u00b7sah", "sah", ".", "A\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "KON", "VVINF", "KOKOM", "NE", "VVFIN", "$.", "KON", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.12": {"text": "Selbst wenn ich Hokusai und Utamaro und", "tokens": ["Selbst", "wenn", "ich", "Ho\u00b7ku\u00b7sai", "und", "Ut\u00b7a\u00b7ma\u00b7ro", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "NE", "KON", "NE", "KON"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Noch ein Dutzend schwer merkbarer Namen mir", "tokens": ["Noch", "ein", "Dut\u00b7zend", "schwer", "merk\u00b7ba\u00b7rer", "Na\u00b7men", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "ADJD", "ADJA", "NN", "PPER"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.14": {"text": "Ins Ged\u00e4chtnis rufe und mit Dankbarkeit", "tokens": ["Ins", "Ge\u00b7d\u00e4cht\u00b7nis", "ru\u00b7fe", "und", "mit", "Dank\u00b7bar\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "KON", "APPR", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.15": {"text": "An Lackschatullen denke und R\u00e4uchergef\u00e4\u00dfe", "tokens": ["An", "Lack\u00b7scha\u00b7tul\u00b7len", "den\u00b7ke", "und", "R\u00e4u\u00b7cher\u00b7ge\u00b7f\u00e4\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "KON", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Und seidene Kockemonos und die Dichterin sei Schonagon, \u2013 ich", "tokens": ["Und", "sei\u00b7de\u00b7ne", "Ko\u00b7cke\u00b7mo\u00b7nos", "und", "die", "Dich\u00b7te\u00b7rin", "sei", "Scho\u00b7na\u00b7gon", ",", "\u2013", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word"], "pos": ["KON", "PPOSAT", "NN", "KON", "ART", "NN", "VAFIN", "NE", "$,", "$(", "PPER"], "meter": "-+--+--+--+-+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.17": {"text": "Kann mir nicht helfen, mir wird nicht warm dabei;", "tokens": ["Kann", "mir", "nicht", "hel\u00b7fen", ",", "mir", "wird", "nicht", "warm", "da\u00b7bei", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKNEG", "VVINF", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "PAV", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.18": {"text": "Die gelben \u00c4ffchen bleiben mir ewig Hose wie Jacke.", "tokens": ["Die", "gel\u00b7ben", "\u00c4ff\u00b7chen", "blei\u00b7ben", "mir", "e\u00b7wig", "Ho\u00b7se", "wie", "Ja\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADJD", "NN", "KOKOM", "NE", "$."], "meter": "-+-+-+--+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.19": {"text": "Was also tu ich mit meiner Sympathie?", "tokens": ["Was", "al\u00b7so", "tu", "ich", "mit", "mei\u00b7ner", "Sym\u00b7pa\u00b7thie", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "Z\u00e4hl ich die Kn\u00f6pfe an meinem \u00dcberrock ab, oder", "tokens": ["Z\u00e4hl", "ich", "die", "Kn\u00f6p\u00b7fe", "an", "mei\u00b7nem", "\u00dc\u00b7ber\u00b7rock", "ab", ",", "o\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["NN", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$,", "KON"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.21": {"text": "Rupf ich die Bl\u00e4ttchen einem Chrysanthem\u00fcmchen aus:", "tokens": ["Rupf", "ich", "die", "Bl\u00e4tt\u00b7chen", "ei\u00b7nem", "Chry\u00b7san\u00b7the\u00b7m\u00fcm\u00b7chen", "aus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "Mikado \u2013 V\u00e4terchen, Mikado \u2013 V\u00e4terchen? Oder", "tokens": ["Mi\u00b7ka\u00b7do", "\u2013", "V\u00e4\u00b7ter\u00b7chen", ",", "Mi\u00b7ka\u00b7do", "\u2013", "V\u00e4\u00b7ter\u00b7chen", "?", "O\u00b7der"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$(", "NN", "$,", "NE", "$(", "NN", "$.", "NE"], "meter": "-+-+---+-+--+-", "measure": "iambic.penta.relaxed"}, "line.23": {"text": "Wart ichs ergeben ab, was Bernhard B\u00fclow in seiner Eigenschaft", "tokens": ["Wart", "ichs", "er\u00b7ge\u00b7ben", "ab", ",", "was", "Bern\u00b7hard", "B\u00fc\u00b7low", "in", "sei\u00b7ner", "Ei\u00b7gen\u00b7schaft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "VVFIN", "PTKVZ", "$,", "PRELS", "NE", "NE", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.24": {"text": "Als Kanzler des Deutschen Reiches f\u00fcr richtig finden wird?", "tokens": ["Als", "Kanz\u00b7ler", "des", "Deut\u00b7schen", "Rei\u00b7ches", "f\u00fcr", "rich\u00b7tig", "fin\u00b7den", "wird", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "ADJA", "NN", "APPR", "ADJD", "VVINF", "VAFIN", "$."], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "Oder gedulde ich mich so lange, bis der m\u00e4nnermordende", "tokens": ["O\u00b7der", "ge\u00b7dul\u00b7de", "ich", "mich", "so", "lan\u00b7ge", ",", "bis", "der", "m\u00e4n\u00b7ner\u00b7mor\u00b7den\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PRF", "ADV", "ADV", "$,", "APPR", "ART", "ADJA"], "meter": "-+-+--+-+-+-+-+--", "measure": "iambic.septa.relaxed"}, "line.26": {"text": "Gott der Schlachten mit ", "tokens": ["Gott", "der", "Schlach\u00b7ten", "mit"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "APPR"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.27": {"text": "Von den beiden er ", "tokens": ["Von", "den", "bei\u00b7den", "er"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "PIAT", "PPER"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.28": {"text": "Nein, nichts von alledem gedenke ich zu tun: ich", "tokens": ["Nein", ",", "nichts", "von", "al\u00b7le\u00b7dem", "ge\u00b7den\u00b7ke", "ich", "zu", "tun", ":", "ich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKANT", "$,", "PIS", "APPR", "PIS", "VVFIN", "PPER", "PTKZU", "VVINF", "$.", "PPER"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Lege mein n\u00e4chstes Honorar (und w\u00e4rens gleich zwanzig Mark)", "tokens": ["Le\u00b7ge", "mein", "n\u00e4chs\u00b7tes", "Ho\u00b7no\u00b7rar", "(", "und", "w\u00e4\u00b7rens", "gleich", "zwan\u00b7zig", "Mark", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "ADJA", "NN", "$(", "KON", "VAFIN", "ADV", "CARD", "NN", "$("], "meter": "+--+-+-+-+--+-+", "measure": "iambic.septa.invert"}, "line.30": {"text": "In Japan- oder Russen-Papieren an, je nachdem", "tokens": ["In", "Ja\u00b7pan", "o\u00b7der", "Rus\u00b7sen\u00b7Pa\u00b7pie\u00b7ren", "an", ",", "je", "nach\u00b7dem"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "TRUNC", "KON", "NN", "PTKVZ", "$,", "ADV", "KOUS"], "meter": "--+--+--+--+--", "measure": "anapaest.tetra.plus"}, "line.31": {"text": "Mein Leibbankier die Konjunktur beurteilt, \u2013 und", "tokens": ["Mein", "Leib\u00b7ban\u00b7kier", "die", "Kon\u00b7junk\u00b7tur", "beur\u00b7teilt", ",", "\u2013", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word"], "pos": ["PPOSAT", "NN", "ART", "NN", "VVFIN", "$,", "$(", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.32": {"text": "Von diesem Augenblicke an wei\u00df ich bestimmt, wohin", "tokens": ["Von", "die\u00b7sem", "Au\u00b7gen\u00b7bli\u00b7cke", "an", "wei\u00df", "ich", "be\u00b7stimmt", ",", "wo\u00b7hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PDAT", "NN", "APPR", "VVFIN", "PPER", "VVPP", "$,", "PWAV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.33": {"text": "Die Nadel meiner Sympathie sich wenden mu\u00df.", "tokens": ["Die", "Na\u00b7del", "mei\u00b7ner", "Sym\u00b7pa\u00b7thie", "sich", "wen\u00b7den", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}