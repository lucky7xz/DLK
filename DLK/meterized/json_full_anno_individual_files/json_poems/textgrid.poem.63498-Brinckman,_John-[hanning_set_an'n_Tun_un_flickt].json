{"textgrid.poem.63498": {"metadata": {"author": {"name": "Brinckman, John", "birth": "N.A.", "death": "N.A."}, "title": "[hanning set an'n Tun un flickt]", "genre": "verse", "period": "N.A.", "pub_year": 1842, "urn": "N.A.", "language": ["de:0.85", "sv:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hanning set an'n Tun un flickt", "tokens": ["Han\u00b7ning", "set", "an'n", "Tun", "un", "flickt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "FM", "FM", "FM", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Reester up sin Schoh,", "tokens": ["Rees\u00b7ter", "up", "sin", "Schoh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "Lischen mit ehr Kn\u00fctt de kickt", "tokens": ["Li\u00b7schen", "mit", "ehr", "Kn\u00fctt", "de", "kickt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "NN", "NE", "NE", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hanning flitig to.", "tokens": ["Han\u00b7ning", "fli\u00b7tig", "to", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJD", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Hanning s\u00e4d to Lischen: \u00bbKik,", "tokens": ["Han\u00b7ning", "s\u00e4d", "to", "Li\u00b7schen", ":", "\u00bb", "Kik", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["NE", "NE", "NE", "NE", "$.", "$(", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Lischen, weest du dat?", "tokens": ["Li\u00b7schen", ",", "weest", "du", "dat", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Fri du mi! Denn k\u00fcmmst du glik", "tokens": ["Fri", "du", "mi", "!", "Denn", "k\u00fcmmst", "du", "glik"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "$.", "KON", "VVFIN", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "weg... von de Strat.", "tokens": ["weg", "...", "von", "de", "Strat", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "APPR", "NE", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.3": {"line.1": {"text": "Hunnert Daler heww ick all", "tokens": ["Hun\u00b7nert", "Da\u00b7ler", "heww", "ick", "all"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "PPER", "PIAT"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "in min Strumpschacht bunn',", "tokens": ["in", "min", "Strumpsc\u00b7hacht", "bunn'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un sonn Katen un sonn Stall", "tokens": ["un", "sonn", "Ka\u00b7ten", "un", "sonn", "Stall"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["FM", "FM", "FM", "FM", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "de is ball nog funn'.", "tokens": ["de", "is", "ball", "nog", "funn'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "--+-+", "measure": "anapaest.init"}}, "stanza.4": {"line.1": {"text": "Wenn so klok as ick du b\u00fcst,", "tokens": ["Wenn", "so", "klok", "as", "ick", "du", "b\u00fcst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJD", "VVFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "smit wi unsen Kram,", "tokens": ["smit", "wi", "un\u00b7sen", "Kram", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un wenn du wist,", "tokens": ["un", "wenn", "du", "wist", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM", "KOUS", "PPER", "VVFIN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "uppe St\u00e4d tosam.\u00ab", "tokens": ["up\u00b7pe", "St\u00e4d", "to\u00b7sam", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$.", "$("], "meter": "+-+--", "measure": "unknown.measure.di"}}, "stanza.5": {"line.1": {"text": "Lischen kek dunn up de Kn\u00fctt,", "tokens": ["Li\u00b7schen", "kek", "dunn", "up", "de", "Kn\u00fctt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "NE", "NE", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "up 'ne Masch, de full,", "tokens": ["up", "'ne", "Masch", ",", "de", "full", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "NE", "NE", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un dat Blot, dat Blot dat sch\u00fctt", "tokens": ["un", "dat", "Blot", ",", "dat", "Blot", "dat", "sch\u00fctt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["FM", "ART", "NN", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "in den Kopp ehr dull.", "tokens": ["in", "den", "Kopp", "ehr", "dull", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VMFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "\u00bbhunnert is to wenig, Hans,", "tokens": ["\u00bb", "hun\u00b7nert", "is", "to", "we\u00b7nig", ",", "Hans", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "CARD", "FM", "FM", "PIS", "$,", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "dusend is nich v\u00e4l.", "tokens": ["du\u00b7send", "is", "nich", "v\u00e4l", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "FM", "PTKNEG", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wenn du mi man liden kannst,", "tokens": ["Wenn", "du", "mi", "man", "li\u00b7den", "kannst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NE", "PIS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "is dat all glik v\u00e4l.", "tokens": ["is", "dat", "all", "glik", "v\u00e4l", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "ART", "PIAT", "NN", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.7": {"line.1": {"text": "Denn eens lurrer Wehdag eens,", "tokens": ["Denn", "e\u00b7ens", "lur\u00b7rer", "Weh\u00b7dag", "e\u00b7ens", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "denn eens lurrer Freud,", "tokens": ["denn", "e\u00b7ens", "lur\u00b7rer", "Freud", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "heww wi Geld un heww wi keen,", "tokens": ["heww", "wi", "Geld", "un", "heww", "wi", "ke\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "heww wi uns doch beid.", "tokens": ["heww", "wi", "uns", "doch", "beid", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "PPER", "ADV", "PTKVZ", "$."], "meter": "----+", "measure": "unknown.measure.single"}}, "stanza.8": {"line.1": {"text": "Denn eens dr\u00f6g un denn eens natt,", "tokens": ["Denn", "e\u00b7ens", "dr\u00f6g", "un", "denn", "e\u00b7ens", "natt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "FM", "KON", "ADV", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "kam dat, as dat k\u00fcmmt,", "tokens": ["kam", "dat", ",", "as", "dat", "k\u00fcmmt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "$,", "NE", "NE", "VVFIN", "$,"], "meter": "+----", "measure": "dactylic.init"}, "line.3": {"text": "denn eens krus un denn eens glatt,", "tokens": ["denn", "e\u00b7ens", "krus", "un", "denn", "e\u00b7ens", "glatt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "FM", "KON", "ADV", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "grad as sonn Poor Str\u00fcmp.\u00ab", "tokens": ["grad", "as", "sonn", "Poor", "Str\u00fcmp", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Hanning set an'n Tun un flickt", "tokens": ["Han\u00b7ning", "set", "an'n", "Tun", "un", "flickt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "FM", "FM", "FM", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Reester up sin Schoh,", "tokens": ["Rees\u00b7ter", "up", "sin", "Schoh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "Lischen mit ehr Kn\u00fctt de kickt", "tokens": ["Li\u00b7schen", "mit", "ehr", "Kn\u00fctt", "de", "kickt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "NN", "NE", "NE", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hanning flitig to.", "tokens": ["Han\u00b7ning", "fli\u00b7tig", "to", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJD", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.10": {"line.1": {"text": "Hanning s\u00e4d to Lischen: \u00bbKik,", "tokens": ["Han\u00b7ning", "s\u00e4d", "to", "Li\u00b7schen", ":", "\u00bb", "Kik", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["NE", "NE", "NE", "NE", "$.", "$(", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Lischen, weest du dat?", "tokens": ["Li\u00b7schen", ",", "weest", "du", "dat", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Fri du mi! Denn k\u00fcmmst du glik", "tokens": ["Fri", "du", "mi", "!", "Denn", "k\u00fcmmst", "du", "glik"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "$.", "KON", "VVFIN", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "weg... von de Strat.", "tokens": ["weg", "...", "von", "de", "Strat", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "APPR", "NE", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.11": {"line.1": {"text": "Hunnert Daler heww ick all", "tokens": ["Hun\u00b7nert", "Da\u00b7ler", "heww", "ick", "all"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "PPER", "PIAT"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "in min Strumpschacht bunn',", "tokens": ["in", "min", "Strumpsc\u00b7hacht", "bunn'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un sonn Katen un sonn Stall", "tokens": ["un", "sonn", "Ka\u00b7ten", "un", "sonn", "Stall"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["FM", "FM", "FM", "FM", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "de is ball nog funn'.", "tokens": ["de", "is", "ball", "nog", "funn'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "--+-+", "measure": "anapaest.init"}}, "stanza.12": {"line.1": {"text": "Wenn so klok as ick du b\u00fcst,", "tokens": ["Wenn", "so", "klok", "as", "ick", "du", "b\u00fcst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJD", "VVFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "smit wi unsen Kram,", "tokens": ["smit", "wi", "un\u00b7sen", "Kram", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un wenn du wist,", "tokens": ["un", "wenn", "du", "wist", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM", "KOUS", "PPER", "VVFIN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "uppe St\u00e4d tosam.\u00ab", "tokens": ["up\u00b7pe", "St\u00e4d", "to\u00b7sam", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$.", "$("], "meter": "+-+--", "measure": "unknown.measure.di"}}, "stanza.13": {"line.1": {"text": "Lischen kek dunn up de Kn\u00fctt,", "tokens": ["Li\u00b7schen", "kek", "dunn", "up", "de", "Kn\u00fctt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "NE", "NE", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "up 'ne Masch, de full,", "tokens": ["up", "'ne", "Masch", ",", "de", "full", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "NE", "NE", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "un dat Blot, dat Blot dat sch\u00fctt", "tokens": ["un", "dat", "Blot", ",", "dat", "Blot", "dat", "sch\u00fctt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["FM", "ART", "NN", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "in den Kopp ehr dull.", "tokens": ["in", "den", "Kopp", "ehr", "dull", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VMFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.14": {"line.1": {"text": "\u00bbhunnert is to wenig, Hans,", "tokens": ["\u00bb", "hun\u00b7nert", "is", "to", "we\u00b7nig", ",", "Hans", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "CARD", "FM", "FM", "PIS", "$,", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "dusend is nich v\u00e4l.", "tokens": ["du\u00b7send", "is", "nich", "v\u00e4l", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "FM", "PTKNEG", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wenn du mi man liden kannst,", "tokens": ["Wenn", "du", "mi", "man", "li\u00b7den", "kannst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NE", "PIS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "is dat all glik v\u00e4l.", "tokens": ["is", "dat", "all", "glik", "v\u00e4l", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "ART", "PIAT", "NN", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.15": {"line.1": {"text": "Denn eens lurrer Wehdag eens,", "tokens": ["Denn", "e\u00b7ens", "lur\u00b7rer", "Weh\u00b7dag", "e\u00b7ens", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "denn eens lurrer Freud,", "tokens": ["denn", "e\u00b7ens", "lur\u00b7rer", "Freud", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "heww wi Geld un heww wi keen,", "tokens": ["heww", "wi", "Geld", "un", "heww", "wi", "ke\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "heww wi uns doch beid.", "tokens": ["heww", "wi", "uns", "doch", "beid", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "PPER", "ADV", "PTKVZ", "$."], "meter": "----+", "measure": "unknown.measure.single"}}, "stanza.16": {"line.1": {"text": "Denn eens dr\u00f6g un denn eens natt,", "tokens": ["Denn", "e\u00b7ens", "dr\u00f6g", "un", "denn", "e\u00b7ens", "natt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "FM", "KON", "ADV", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "kam dat, as dat k\u00fcmmt,", "tokens": ["kam", "dat", ",", "as", "dat", "k\u00fcmmt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "$,", "NE", "NE", "VVFIN", "$,"], "meter": "+----", "measure": "dactylic.init"}, "line.3": {"text": "denn eens krus un denn eens glatt,", "tokens": ["denn", "e\u00b7ens", "krus", "un", "denn", "e\u00b7ens", "glatt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "FM", "FM", "FM", "KON", "ADV", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "grad as sonn Poor Str\u00fcmp.\u00ab", "tokens": ["grad", "as", "sonn", "Poor", "Str\u00fcmp", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}