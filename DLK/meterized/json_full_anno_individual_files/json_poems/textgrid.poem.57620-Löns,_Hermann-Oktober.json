{"textgrid.poem.57620": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Oktober", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein grauer Schleier h\u00e4lt die Stadt umwickelt,", "tokens": ["Ein", "grau\u00b7er", "Schlei\u00b7er", "h\u00e4lt", "die", "Stadt", "um\u00b7wi\u00b7ckelt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auf zwanzig Schritte macht das Auge Schicht,", "tokens": ["Auf", "zwan\u00b7zig", "Schrit\u00b7te", "macht", "das", "Au\u00b7ge", "Schicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Der nasse Staub mir in den Schnurrbart prickelt", "tokens": ["Der", "nas\u00b7se", "Staub", "mir", "in", "den", "Schnurr\u00b7bart", "pri\u00b7ckelt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PPER", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und rinnt mir kitzelnd \u00fcber das Gesicht.", "tokens": ["Und", "rinnt", "mir", "kit\u00b7zelnd", "\u00fc\u00b7ber", "das", "Ge\u00b7sicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Schwer tropft das Wasser von den stummen B\u00e4umen,", "tokens": ["Schwer", "tropft", "das", "Was\u00b7ser", "von", "den", "stum\u00b7men", "B\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Als geisterblasser, fahler Strahlenkranz", "tokens": ["Als", "geis\u00b7ter\u00b7blas\u00b7ser", ",", "fah\u00b7ler", "Strah\u00b7len\u00b7kranz"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["KOUS", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Gewaltig lange Nebelstreifen s\u00e4umen", "tokens": ["Ge\u00b7wal\u00b7tig", "lan\u00b7ge", "Ne\u00b7bel\u00b7strei\u00b7fen", "s\u00e4u\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Der Gaslaternen halbverwischten Glanz.", "tokens": ["Der", "Gas\u00b7la\u00b7ter\u00b7nen", "halb\u00b7ver\u00b7wischten", "Glanz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.3": {"line.1": {"text": "Es wandelt vor mir her ein Liebesp\u00e4rchen", "tokens": ["Es", "wan\u00b7delt", "vor", "mir", "her", "ein", "Lie\u00b7be\u00b7sp\u00e4r\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und h\u00e4lt sich fest und innighei\u00df umschmiegt \u2013", "tokens": ["Und", "h\u00e4lt", "sich", "fest", "und", "in\u00b7nig\u00b7hei\u00df", "um\u00b7schmiegt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Das ist das alte, oft erz\u00e4hlte M\u00e4rchen", "tokens": ["Das", "ist", "das", "al\u00b7te", ",", "oft", "er\u00b7z\u00e4hl\u00b7te", "M\u00e4r\u00b7chen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "$,", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Von ihm und ihr und da\u00df sie sich gekriegt.", "tokens": ["Von", "ihm", "und", "ihr", "und", "da\u00df", "sie", "sich", "ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "KON", "PPER", "KON", "KOUS", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Ein Mann geht neben mir mit festem Tritte", "tokens": ["Ein", "Mann", "geht", "ne\u00b7ben", "mir", "mit", "fes\u00b7tem", "Trit\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Und pfeift ein Gassenliedchen laut und klar,", "tokens": ["Und", "pfeift", "ein", "Gas\u00b7sen\u00b7lied\u00b7chen", "laut", "und", "klar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Er wei\u00df, da\u00df ihn in warmer, eigner H\u00fctte", "tokens": ["Er", "wei\u00df", ",", "da\u00df", "ihn", "in", "war\u00b7mer", ",", "eig\u00b7ner", "H\u00fct\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "APPR", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sein Weib erwartet und der Kinder Schar.", "tokens": ["Sein", "Weib", "er\u00b7war\u00b7tet", "und", "der", "Kin\u00b7der", "Schar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "KON", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Der Hoffnungsnebel h\u00e4lt sie all' umfangen,", "tokens": ["Der", "Hoff\u00b7nungs\u00b7ne\u00b7bel", "h\u00e4lt", "sie", "all'", "um\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+---+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Durchschimmern sehn sie ihren Zukunftsbau,", "tokens": ["Durch\u00b7schim\u00b7mern", "sehn", "sie", "ih\u00b7ren", "Zu\u00b7kunfts\u00b7bau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Vor meinen Augen ist der Dunst zergangen", "tokens": ["Vor", "mei\u00b7nen", "Au\u00b7gen", "ist", "der", "Dunst", "zer\u00b7gan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und ich wei\u00df l\u00e4ngst, da\u00df alles schwarz und grau.", "tokens": ["Und", "ich", "wei\u00df", "l\u00e4ngst", ",", "da\u00df", "al\u00b7les", "schwarz", "und", "grau", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "$,", "KOUS", "PIS", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Vor meinem Blick zerri\u00df der Nebelfetzen", "tokens": ["Vor", "mei\u00b7nem", "Blick", "zer\u00b7ri\u00df", "der", "Ne\u00b7bel\u00b7fet\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und was ich sah, war schlimmer als der Tod,", "tokens": ["Und", "was", "ich", "sah", ",", "war", "schlim\u00b7mer", "als", "der", "Tod", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Denn grausig, wie der Babylonier G\u00f6tzen,", "tokens": ["Denn", "grau\u00b7sig", ",", "wie", "der", "Ba\u00b7by\u00b7lo\u00b7nier", "G\u00f6t\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PWAV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Hat mir die Unbefriedigung zugedroht.", "tokens": ["Hat", "mir", "die", "Un\u00b7be\u00b7frie\u00b7di\u00b7gung", "zu\u00b7ge\u00b7droht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Kein Lebensweg f\u00fchrt an ein festes Ende,", "tokens": ["Kein", "Le\u00b7bens\u00b7weg", "f\u00fchrt", "an", "ein", "fes\u00b7tes", "En\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ein Ende jedes Strebens \u2013 eine Kluft!", "tokens": ["Ein", "En\u00b7de", "je\u00b7des", "Stre\u00b7bens", "\u2013", "ei\u00b7ne", "Kluft", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "NN", "$(", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Nach festem Boden fassen deine H\u00e4nde", "tokens": ["Nach", "fes\u00b7tem", "Bo\u00b7den", "fas\u00b7sen", "dei\u00b7ne", "H\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und fassen haltlos in die graue Luft.", "tokens": ["Und", "fas\u00b7sen", "halt\u00b7los", "in", "die", "grau\u00b7e", "Luft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "So gro\u00dfe Worte und so kleine Triebe!", "tokens": ["So", "gro\u00b7\u00dfe", "Wor\u00b7te", "und", "so", "klei\u00b7ne", "Trie\u00b7be", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "KON", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der Ruhm? \u2013 Die Sucht, ein gr\u00f6\u00dfrer Narr zu sein,", "tokens": ["Der", "Ruhm", "?", "\u2013", "Die", "Sucht", ",", "ein", "gr\u00f6\u00df\u00b7rer", "Narr", "zu", "sein", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "$(", "ART", "NN", "$,", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bem\u00e4ntelter Geschlechtstrieb \u2013 das hei\u00dft Liebe,", "tokens": ["Be\u00b7m\u00e4n\u00b7tel\u00b7ter", "Ge\u00b7schlecht\u00b7strieb", "\u2013", "das", "hei\u00dft", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "PDS", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die Wissenschaft \u2013 nutzlose Spielerein.", "tokens": ["Die", "Wis\u00b7sen\u00b7schaft", "\u2013", "nutz\u00b7lo\u00b7se", "Spie\u00b7ler\u00b7ein", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Und wenn du auch ein gro\u00dfes Ziel erstritten,", "tokens": ["Und", "wenn", "du", "auch", "ein", "gro\u00b7\u00dfes", "Ziel", "er\u00b7strit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und dich stolzl\u00e4chelnd in das Grab gelegt \u2013", "tokens": ["Und", "dich", "stolz\u00b7l\u00e4\u00b7chelnd", "in", "das", "Grab", "ge\u00b7legt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJD", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ach, Millionen haben schon gelitten,", "tokens": ["Ach", ",", "Mil\u00b7lion\u00b7en", "ha\u00b7ben", "schon", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Gleichg\u00fcltig ist die Zeit vorbeigefegt.", "tokens": ["Gleich\u00b7g\u00fcl\u00b7tig", "ist", "die", "Zeit", "vor\u00b7bei\u00b7ge\u00b7fegt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Ach, Weltverbesserung und Mitleidsschmerzen,", "tokens": ["Ach", ",", "Welt\u00b7ver\u00b7bes\u00b7se\u00b7rung", "und", "Mit\u00b7leids\u00b7schmer\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie stopfen nicht das unheilbare Loch,", "tokens": ["Sie", "stop\u00b7fen", "nicht", "das", "un\u00b7heil\u00b7ba\u00b7re", "Loch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Es bluteten Millionen Menschenherzen", "tokens": ["Es", "blu\u00b7te\u00b7ten", "Mil\u00b7lion\u00b7en", "Men\u00b7schen\u00b7her\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und Millionen werden bluten noch.", "tokens": ["Und", "Mil\u00b7lion\u00b7en", "wer\u00b7den", "blu\u00b7ten", "noch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAINF", "VVFIN", "ADV", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}}, "stanza.11": {"line.1": {"text": "Und Hunger, Wahnsinn, Morden, L\u00fcgen, Rauben,", "tokens": ["Und", "Hun\u00b7ger", ",", "Wahn\u00b7sinn", ",", "Mor\u00b7den", ",", "L\u00fc\u00b7gen", ",", "Rau\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die werden sein, solang' die Welt besteht \u2013", "tokens": ["Die", "wer\u00b7den", "sein", ",", "so\u00b7lang'", "die", "Welt", "be\u00b7steht", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "VAINF", "$,", "VMFIN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "D'rum h\u00fcll' dich ein in Hoffen oder Glauben", "tokens": ["D'\u00b7rum", "h\u00fcll'", "dich", "ein", "in", "Hof\u00b7fen", "o\u00b7der", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PRF", "ART", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "Und la\u00df es ruhig gehen wie es geht.", "tokens": ["Und", "la\u00df", "es", "ru\u00b7hig", "ge\u00b7hen", "wie", "es", "geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "ADJD", "VVINF", "KOKOM", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Ein grauer Schleier h\u00e4lt die Stadt umwickelt,", "tokens": ["Ein", "grau\u00b7er", "Schlei\u00b7er", "h\u00e4lt", "die", "Stadt", "um\u00b7wi\u00b7ckelt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auf zwanzig Schritte macht das Auge Schicht,", "tokens": ["Auf", "zwan\u00b7zig", "Schrit\u00b7te", "macht", "das", "Au\u00b7ge", "Schicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Der nasse Staub mir in den Schnurrbart prickelt", "tokens": ["Der", "nas\u00b7se", "Staub", "mir", "in", "den", "Schnurr\u00b7bart", "pri\u00b7ckelt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PPER", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und rinnt mir kitzelnd \u00fcber das Gesicht.", "tokens": ["Und", "rinnt", "mir", "kit\u00b7zelnd", "\u00fc\u00b7ber", "das", "Ge\u00b7sicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Schwer tropft das Wasser von den stummen B\u00e4umen,", "tokens": ["Schwer", "tropft", "das", "Was\u00b7ser", "von", "den", "stum\u00b7men", "B\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Als geisterblasser, fahler Strahlenkranz", "tokens": ["Als", "geis\u00b7ter\u00b7blas\u00b7ser", ",", "fah\u00b7ler", "Strah\u00b7len\u00b7kranz"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["KOUS", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Gewaltig lange Nebelstreifen s\u00e4umen", "tokens": ["Ge\u00b7wal\u00b7tig", "lan\u00b7ge", "Ne\u00b7bel\u00b7strei\u00b7fen", "s\u00e4u\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Der Gaslaternen halbverwischten Glanz.", "tokens": ["Der", "Gas\u00b7la\u00b7ter\u00b7nen", "halb\u00b7ver\u00b7wischten", "Glanz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.14": {"line.1": {"text": "Es wandelt vor mir her ein Liebesp\u00e4rchen", "tokens": ["Es", "wan\u00b7delt", "vor", "mir", "her", "ein", "Lie\u00b7be\u00b7sp\u00e4r\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und h\u00e4lt sich fest und innighei\u00df umschmiegt \u2013", "tokens": ["Und", "h\u00e4lt", "sich", "fest", "und", "in\u00b7nig\u00b7hei\u00df", "um\u00b7schmiegt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Das ist das alte, oft erz\u00e4hlte M\u00e4rchen", "tokens": ["Das", "ist", "das", "al\u00b7te", ",", "oft", "er\u00b7z\u00e4hl\u00b7te", "M\u00e4r\u00b7chen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "$,", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Von ihm und ihr und da\u00df sie sich gekriegt.", "tokens": ["Von", "ihm", "und", "ihr", "und", "da\u00df", "sie", "sich", "ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "KON", "PPER", "KON", "KOUS", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Ein Mann geht neben mir mit festem Tritte", "tokens": ["Ein", "Mann", "geht", "ne\u00b7ben", "mir", "mit", "fes\u00b7tem", "Trit\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Und pfeift ein Gassenliedchen laut und klar,", "tokens": ["Und", "pfeift", "ein", "Gas\u00b7sen\u00b7lied\u00b7chen", "laut", "und", "klar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Er wei\u00df, da\u00df ihn in warmer, eigner H\u00fctte", "tokens": ["Er", "wei\u00df", ",", "da\u00df", "ihn", "in", "war\u00b7mer", ",", "eig\u00b7ner", "H\u00fct\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "APPR", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sein Weib erwartet und der Kinder Schar.", "tokens": ["Sein", "Weib", "er\u00b7war\u00b7tet", "und", "der", "Kin\u00b7der", "Schar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "KON", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.16": {"line.1": {"text": "Der Hoffnungsnebel h\u00e4lt sie all' umfangen,", "tokens": ["Der", "Hoff\u00b7nungs\u00b7ne\u00b7bel", "h\u00e4lt", "sie", "all'", "um\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+---+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Durchschimmern sehn sie ihren Zukunftsbau,", "tokens": ["Durch\u00b7schim\u00b7mern", "sehn", "sie", "ih\u00b7ren", "Zu\u00b7kunfts\u00b7bau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Vor meinen Augen ist der Dunst zergangen", "tokens": ["Vor", "mei\u00b7nen", "Au\u00b7gen", "ist", "der", "Dunst", "zer\u00b7gan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und ich wei\u00df l\u00e4ngst, da\u00df alles schwarz und grau.", "tokens": ["Und", "ich", "wei\u00df", "l\u00e4ngst", ",", "da\u00df", "al\u00b7les", "schwarz", "und", "grau", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "$,", "KOUS", "PIS", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Vor meinem Blick zerri\u00df der Nebelfetzen", "tokens": ["Vor", "mei\u00b7nem", "Blick", "zer\u00b7ri\u00df", "der", "Ne\u00b7bel\u00b7fet\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und was ich sah, war schlimmer als der Tod,", "tokens": ["Und", "was", "ich", "sah", ",", "war", "schlim\u00b7mer", "als", "der", "Tod", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Denn grausig, wie der Babylonier G\u00f6tzen,", "tokens": ["Denn", "grau\u00b7sig", ",", "wie", "der", "Ba\u00b7by\u00b7lo\u00b7nier", "G\u00f6t\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PWAV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Hat mir die Unbefriedigung zugedroht.", "tokens": ["Hat", "mir", "die", "Un\u00b7be\u00b7frie\u00b7di\u00b7gung", "zu\u00b7ge\u00b7droht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "Kein Lebensweg f\u00fchrt an ein festes Ende,", "tokens": ["Kein", "Le\u00b7bens\u00b7weg", "f\u00fchrt", "an", "ein", "fes\u00b7tes", "En\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ein Ende jedes Strebens \u2013 eine Kluft!", "tokens": ["Ein", "En\u00b7de", "je\u00b7des", "Stre\u00b7bens", "\u2013", "ei\u00b7ne", "Kluft", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "NN", "$(", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Nach festem Boden fassen deine H\u00e4nde", "tokens": ["Nach", "fes\u00b7tem", "Bo\u00b7den", "fas\u00b7sen", "dei\u00b7ne", "H\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und fassen haltlos in die graue Luft.", "tokens": ["Und", "fas\u00b7sen", "halt\u00b7los", "in", "die", "grau\u00b7e", "Luft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.19": {"line.1": {"text": "So gro\u00dfe Worte und so kleine Triebe!", "tokens": ["So", "gro\u00b7\u00dfe", "Wor\u00b7te", "und", "so", "klei\u00b7ne", "Trie\u00b7be", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "KON", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der Ruhm? \u2013 Die Sucht, ein gr\u00f6\u00dfrer Narr zu sein,", "tokens": ["Der", "Ruhm", "?", "\u2013", "Die", "Sucht", ",", "ein", "gr\u00f6\u00df\u00b7rer", "Narr", "zu", "sein", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "$(", "ART", "NN", "$,", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bem\u00e4ntelter Geschlechtstrieb \u2013 das hei\u00dft Liebe,", "tokens": ["Be\u00b7m\u00e4n\u00b7tel\u00b7ter", "Ge\u00b7schlecht\u00b7strieb", "\u2013", "das", "hei\u00dft", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "PDS", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die Wissenschaft \u2013 nutzlose Spielerein.", "tokens": ["Die", "Wis\u00b7sen\u00b7schaft", "\u2013", "nutz\u00b7lo\u00b7se", "Spie\u00b7ler\u00b7ein", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.20": {"line.1": {"text": "Und wenn du auch ein gro\u00dfes Ziel erstritten,", "tokens": ["Und", "wenn", "du", "auch", "ein", "gro\u00b7\u00dfes", "Ziel", "er\u00b7strit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und dich stolzl\u00e4chelnd in das Grab gelegt \u2013", "tokens": ["Und", "dich", "stolz\u00b7l\u00e4\u00b7chelnd", "in", "das", "Grab", "ge\u00b7legt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJD", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ach, Millionen haben schon gelitten,", "tokens": ["Ach", ",", "Mil\u00b7lion\u00b7en", "ha\u00b7ben", "schon", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Gleichg\u00fcltig ist die Zeit vorbeigefegt.", "tokens": ["Gleich\u00b7g\u00fcl\u00b7tig", "ist", "die", "Zeit", "vor\u00b7bei\u00b7ge\u00b7fegt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.21": {"line.1": {"text": "Ach, Weltverbesserung und Mitleidsschmerzen,", "tokens": ["Ach", ",", "Welt\u00b7ver\u00b7bes\u00b7se\u00b7rung", "und", "Mit\u00b7leids\u00b7schmer\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie stopfen nicht das unheilbare Loch,", "tokens": ["Sie", "stop\u00b7fen", "nicht", "das", "un\u00b7heil\u00b7ba\u00b7re", "Loch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Es bluteten Millionen Menschenherzen", "tokens": ["Es", "blu\u00b7te\u00b7ten", "Mil\u00b7lion\u00b7en", "Men\u00b7schen\u00b7her\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und Millionen werden bluten noch.", "tokens": ["Und", "Mil\u00b7lion\u00b7en", "wer\u00b7den", "blu\u00b7ten", "noch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAINF", "VVFIN", "ADV", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}}, "stanza.22": {"line.1": {"text": "Und Hunger, Wahnsinn, Morden, L\u00fcgen, Rauben,", "tokens": ["Und", "Hun\u00b7ger", ",", "Wahn\u00b7sinn", ",", "Mor\u00b7den", ",", "L\u00fc\u00b7gen", ",", "Rau\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die werden sein, solang' die Welt besteht \u2013", "tokens": ["Die", "wer\u00b7den", "sein", ",", "so\u00b7lang'", "die", "Welt", "be\u00b7steht", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "VAINF", "$,", "VMFIN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "D'rum h\u00fcll' dich ein in Hoffen oder Glauben", "tokens": ["D'\u00b7rum", "h\u00fcll'", "dich", "ein", "in", "Hof\u00b7fen", "o\u00b7der", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PRF", "ART", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "Und la\u00df es ruhig gehen wie es geht.", "tokens": ["Und", "la\u00df", "es", "ru\u00b7hig", "ge\u00b7hen", "wie", "es", "geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "ADJD", "VVINF", "KOKOM", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}