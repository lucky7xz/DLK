{"dta.poem.20329": {"metadata": {"author": {"name": "Hofmann von Hofmannswaldau, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1695", "urn": "urn:nbn:de:kobv:b4-200905197751", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Albanie gebrauche deiner zeit/", "tokens": ["Al\u00b7ba\u00b7nie", "ge\u00b7brau\u00b7che", "dei\u00b7ner", "zeit", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPOSAT", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Und la\u00df den liebes-l\u00fcsten freyen z\u00fcgel/", "tokens": ["Und", "la\u00df", "den", "lie\u00b7bes\u00b7l\u00fcs\u00b7ten", "frey\u00b7en", "z\u00fc\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wenn uns der schnee der jahre hat beschneyt/", "tokens": ["Wenn", "uns", "der", "schnee", "der", "jah\u00b7re", "hat", "be\u00b7schneyt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So schmeckt kein ku\u00df/ der liebe wahres siegel/", "tokens": ["So", "schmeckt", "kein", "ku\u00df", "/", "der", "lie\u00b7be", "wah\u00b7res", "sie\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "$(", "ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Im gr\u00fcnen m\u00e4y gr\u00fcnt nur der bunte klee.", "tokens": ["Im", "gr\u00fc\u00b7nen", "m\u00e4y", "gr\u00fcnt", "nur", "der", "bun\u00b7te", "klee", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NE", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+---+-+", "measure": "zehnsilber"}, "line.6": {"text": "Albanie.", "tokens": ["Al\u00b7ba\u00b7nie", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Albanie/ der sch\u00f6nen augen licht/", "tokens": ["Al\u00b7ba\u00b7nie", "/", "der", "sch\u00f6\u00b7nen", "au\u00b7gen", "licht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "ART", "ADJA", "NN", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der leib/ und was auff den beliebten wangen/", "tokens": ["Der", "leib", "/", "und", "was", "auff", "den", "be\u00b7lieb\u00b7ten", "wan\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "KON", "PWS", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Ist nicht vor dich/ vor uns nur zugericht/", "tokens": ["Ist", "nicht", "vor", "dich", "/", "vor", "uns", "nur", "zu\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "APPR", "PPER", "$(", "APPR", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die \u00e4pffel/ so auff deinen br\u00fcsten prangen/", "tokens": ["Die", "\u00e4pf\u00b7fel", "/", "so", "auff", "dei\u00b7nen", "br\u00fcs\u00b7ten", "pran\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ADV", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Sind unsre lust/ und s\u00fcsse anmuths-see.", "tokens": ["Sind", "uns\u00b7re", "lust", "/", "und", "s\u00fcs\u00b7se", "an\u00b7muths\u00b7see", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "$(", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Albanie.", "tokens": ["Al\u00b7ba\u00b7nie", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Albanie/ was qv\u00e4len wir uns viel/", "tokens": ["Al\u00b7ba\u00b7nie", "/", "was", "qv\u00e4\u00b7len", "wir", "uns", "viel", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "PWS", "VVFIN", "PPER", "PRF", "ADV", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Und z\u00fcchtigen die nieren und die lenden?", "tokens": ["Und", "z\u00fcch\u00b7ti\u00b7gen", "die", "nie\u00b7ren", "und", "die", "len\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "KON", "ART", "NN", "$."], "meter": "-+-+-+--+--", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Nur frisch gewagt das angenehme spiel/", "tokens": ["Nur", "frisch", "ge\u00b7wagt", "das", "an\u00b7ge\u00b7neh\u00b7me", "spiel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Jedwedes glied ist ja gemacht zum wenden/", "tokens": ["Jed\u00b7we\u00b7des", "glied", "ist", "ja", "ge\u00b7macht", "zum", "wen\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADV", "VVPP", "APPRART", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Und wendet doch die sonn sich in die h\u00f6h.", "tokens": ["Und", "wen\u00b7det", "doch", "die", "sonn", "sich", "in", "die", "h\u00f6h", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "PRF", "APPR", "ART", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Albanie/ soll denn dein warmer schoo\u00df", "tokens": ["Al\u00b7ba\u00b7nie", "/", "soll", "denn", "dein", "war\u00b7mer", "schoo\u00df"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "VMFIN", "ADV", "PPOSAT", "ADJA", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "So \u00f6d und w\u00fcst/ und unbebauet liegen?", "tokens": ["So", "\u00f6d", "und", "w\u00fcst", "/", "und", "un\u00b7be\u00b7bau\u00b7et", "lie\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "VVFIN", "$(", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im paradie\u00df/ da gieng man nackt und blo\u00df/", "tokens": ["Im", "pa\u00b7ra\u00b7die\u00df", "/", "da", "gieng", "man", "nackt", "und", "blo\u00df", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$(", "ADV", "VVFIN", "PIS", "ADJD", "KON", "ADV", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und durffte frey die liebes-\u00e4cker pfl\u00fcgen/", "tokens": ["Und", "durff\u00b7te", "frey", "die", "lie\u00b7bes\u00b7\u00e4\u00b7cker", "pfl\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADJD", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Welch menschen-satz macht uns di\u00df neue weh?", "tokens": ["Welch", "men\u00b7schen\u00b7satz", "macht", "uns", "di\u00df", "neu\u00b7e", "weh", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "PDS", "ADJA", "PTKVZ", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.6": {"text": "Albanie.", "tokens": ["Al\u00b7ba\u00b7nie", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Albanie/ wer kan die s\u00fc\u00dfigkeit/", "tokens": ["Al\u00b7ba\u00b7nie", "/", "wer", "kan", "die", "s\u00fc\u00b7\u00dfig\u00b7keit", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "PWS", "VMFIN", "ART", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der zwey vermischten geister recht entdecken?", "tokens": ["Der", "zwey", "ver\u00b7mischten", "geis\u00b7ter", "recht", "ent\u00b7de\u00b7cken", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "ADJA", "NN", "ADJD", "VVINF", "$."], "meter": "-+-++-+-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Wenn lieb und lust ein essen uns bereit/", "tokens": ["Wenn", "lieb", "und", "lust", "ein", "es\u00b7sen", "uns", "be\u00b7reit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "KON", "VVFIN", "ART", "VVFIN", "PPER", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Das wiederhohlt am besten pflegt zu schmecken/", "tokens": ["Das", "wie\u00b7der\u00b7hohlt", "am", "bes\u00b7ten", "pflegt", "zu", "schme\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKA", "ADJD", "VVFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "W\u00fcnscht nicht ein hertz/ da\u00df es dabey vergeh?", "tokens": ["W\u00fcnscht", "nicht", "ein", "hertz", "/", "da\u00df", "es", "da\u00b7bey", "ver\u00b7geh", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "ART", "NN", "$(", "KOUS", "PPER", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Albanie.", "tokens": ["Al\u00b7ba\u00b7nie", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Albanie/ weil noch der wollust-thau", "tokens": ["Al\u00b7ba\u00b7nie", "/", "weil", "noch", "der", "wol\u00b7lust\u00b7\u00b7thau"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$(", "KOUS", "ADV", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Die glieder netzt/ und das gebl\u00fcte springet/", "tokens": ["Die", "glie\u00b7der", "netzt", "/", "und", "das", "ge\u00b7bl\u00fc\u00b7te", "sprin\u00b7get", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "KON", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So la\u00df doch zu/ da\u00df auff der Venus-au", "tokens": ["So", "la\u00df", "doch", "zu", "/", "da\u00df", "auff", "der", "Ve\u00b7nus\u00b7au"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "ADV", "PTKZU", "$(", "KOUS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein br\u00fcnstger geist dir kniend opffer bringet/", "tokens": ["Ein", "br\u00fcnst\u00b7ger", "geist", "dir", "kni\u00b7end", "opf\u00b7fer", "brin\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "VVPP", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Da\u00df er vor dir in voller andacht steh.", "tokens": ["Da\u00df", "er", "vor", "dir", "in", "vol\u00b7ler", "an\u00b7dacht", "steh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPER", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Albanie.", "tokens": ["Al\u00b7ba\u00b7nie", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}