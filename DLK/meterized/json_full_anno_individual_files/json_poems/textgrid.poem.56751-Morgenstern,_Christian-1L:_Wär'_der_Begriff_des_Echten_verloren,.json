{"textgrid.poem.56751": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: W\u00e4r' der Begriff des Echten verloren,", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "W\u00e4r' der Begriff des Echten verloren,", "tokens": ["W\u00e4r'", "der", "Be\u00b7griff", "des", "Ech\u00b7ten", "ver\u00b7lo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "in Dir w\u00e4r' er wiedergeboren.", "tokens": ["in", "Dir", "w\u00e4r'", "er", "wie\u00b7der\u00b7ge\u00b7bo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "PPER", "VVINF", "$."], "meter": "-++-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Als Ha\u00df mir nach der Wurzel schlug,", "tokens": ["Als", "Ha\u00df", "mir", "nach", "der", "Wur\u00b7zel", "schlug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "warst Du bei mir, das war genug,", "tokens": ["warst", "Du", "bei", "mir", ",", "das", "war", "ge\u00b7nug", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPER", "$,", "PDS", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "hast mir zu Deinem Leben", "tokens": ["hast", "mir", "zu", "Dei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "das meine neu gegeben.", "tokens": ["das", "mei\u00b7ne", "neu", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Zehn Jahre zusammen!", "tokens": ["Zehn", "Jah\u00b7re", "zu\u00b7sam\u00b7men", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "NN", "PTKVZ", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Es l\u00f6st sich der Dunst.", "tokens": ["Es", "l\u00f6st", "sich", "der", "Dunst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Auf schlagen die Flammen", "tokens": ["Auf", "schla\u00b7gen", "die", "Flam\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "VVFIN", "ART", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "W\u00e4r' der Begriff des Echten verloren,", "tokens": ["W\u00e4r'", "der", "Be\u00b7griff", "des", "Ech\u00b7ten", "ver\u00b7lo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "in Dir w\u00e4r' er wiedergeboren.", "tokens": ["in", "Dir", "w\u00e4r'", "er", "wie\u00b7der\u00b7ge\u00b7bo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "PPER", "VVINF", "$."], "meter": "-++-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Als Ha\u00df mir nach der Wurzel schlug,", "tokens": ["Als", "Ha\u00df", "mir", "nach", "der", "Wur\u00b7zel", "schlug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "warst Du bei mir, das war genug,", "tokens": ["warst", "Du", "bei", "mir", ",", "das", "war", "ge\u00b7nug", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPER", "$,", "PDS", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "hast mir zu Deinem Leben", "tokens": ["hast", "mir", "zu", "Dei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "das meine neu gegeben.", "tokens": ["das", "mei\u00b7ne", "neu", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Zehn Jahre zusammen!", "tokens": ["Zehn", "Jah\u00b7re", "zu\u00b7sam\u00b7men", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "NN", "PTKVZ", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Es l\u00f6st sich der Dunst.", "tokens": ["Es", "l\u00f6st", "sich", "der", "Dunst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Auf schlagen die Flammen", "tokens": ["Auf", "schla\u00b7gen", "die", "Flam\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "VVFIN", "ART", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}}}}