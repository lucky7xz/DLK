{"textgrid.poem.54001": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Ich habe mich erk\u00e4ltet", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich wei\u00df dicht, was bit beider Dase ist \u2013", "tokens": ["Ich", "wei\u00df", "dicht", ",", "was", "bit", "bei\u00b7der", "Da\u00b7se", "ist", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$,", "PRELS", "APPR", "PIAT", "PDS", "VAFIN", "$("], "meter": "-++-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "da ist was dridd . . .", "tokens": ["da", "ist", "was", "dridd", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Doch soll bich dies dicht hindern,", "tokens": ["Doch", "soll", "bich", "dies", "dicht", "hin\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "PDS", "ADJD", "VVINF", "$,"], "meter": "-+--++-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "euch, lieben Kindern,", "tokens": ["euch", ",", "lie\u00b7ben", "Kin\u00b7dern", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "ein deutsches Lied zu singen \u2013 uns allen zum Gewidd \u2013:", "tokens": ["ein", "deut\u00b7sches", "Lied", "zu", "sin\u00b7gen", "\u2013", "uns", "al\u00b7len", "zum", "Ge\u00b7widd", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$(", "PPER", "PIAT", "APPRART", "NN", "$(", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.2": {"line.1": {"text": "Barkig schallt der Ruf der deutschen Bannen:", "tokens": ["Bar\u00b7kig", "schallt", "der", "Ruf", "der", "deut\u00b7schen", "Ban\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbheil deb gro\u00dfen Zeppeliend!", "tokens": ["\u00bb", "heil", "deb", "gro\u00b7\u00dfen", "Zep\u00b7pe\u00b7liend", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Welcher butig flog von dannen,", "tokens": ["Wel\u00b7cher", "bu\u00b7tig", "flog", "von", "dan\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "ADJD", "VVFIN", "APPR", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00fcber alle Welten hiend!\u00ab", "tokens": ["\u00fc\u00b7ber", "al\u00b7le", "Wel\u00b7ten", "hiend", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Alle Benschen konnten ihn sehnd!", "tokens": ["Al\u00b7le", "Ben\u00b7schen", "konn\u00b7ten", "ihn", "sehnd", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVFIN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Welch ein Ph\u00e4dobeend \u2013!", "tokens": ["Welch", "ein", "Ph\u00e4\u00b7do\u00b7beend", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PIAT", "ART", "NN", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Donnen, Deger und berlider Dutten", "tokens": ["Don\u00b7nen", ",", "De\u00b7ger", "und", "ber\u00b7li\u00b7der", "Dut\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "labten sich an seinemb Bild \u2013", "tokens": ["lab\u00b7ten", "sich", "an", "sei\u00b7nemb", "Bild", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$("], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "ohmb schrieben sie mit Underwoodn,", "tokens": ["ohmb", "schrie\u00b7ben", "sie", "mit", "Un\u00b7der\u00b7wo\u00b7odn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und sie a\u00dfen Hubber, Lachs und Wild,", "tokens": ["und", "sie", "a\u00b7\u00dfen", "Hub\u00b7ber", ",", "Lachs", "und", "Wild", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "NE", "$,", "NN", "KON", "NE", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "sowie auch die leckre Barbelade \u2013", "tokens": ["so\u00b7wie", "auch", "die", "leck\u00b7re", "Bar\u00b7be\u00b7la\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "da\u00df ich dicht dabei war, das war schade.", "tokens": ["da\u00df", "ich", "dicht", "da\u00b7bei", "war", ",", "das", "war", "scha\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "PAV", "VAFIN", "$,", "PDS", "VAFIN", "ADJD", "$."], "meter": "--+-+-+-+-", "measure": "anapaest.init"}}, "stanza.4": {"line.1": {"text": "Eckners Namb' sollt man id Barbor ritzen,", "tokens": ["Eck\u00b7ners", "Namb'", "sollt", "man", "id", "Bar\u00b7bor", "rit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VMFIN", "PIS", "NE", "NE", "VVFIN", "$,"], "meter": "+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "auf Zigarren, id ded Steid vom Dobido \u2013", "tokens": ["auf", "Zi\u00b7gar\u00b7ren", ",", "id", "ded", "Steid", "vom", "Do\u00b7bi\u00b7do", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "auf deb Pr\u00e4sidentenstuhle sollt er sitzen,", "tokens": ["auf", "deb", "Pr\u00e4\u00b7si\u00b7den\u00b7ten\u00b7stuh\u00b7le", "sollt", "er", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "daf\u00fcr neblich ist derselbe do . . .", "tokens": ["da\u00b7f\u00fcr", "neb\u00b7lich", "ist", "der\u00b7sel\u00b7be", "do", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PAV", "ADJD", "VAFIN", "PDAT", "ADV", "$.", "$.", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Alle, alle kedden ihnd ja schond,", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "ked\u00b7den", "ihnd", "ja", "schond", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "PIS", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "selbst Biss Babbitt und Frau Dathadsohnd.", "tokens": ["selbst", "Biss", "Bab\u00b7bitt", "und", "Frau", "Dath\u00b7ad\u00b7sohnd", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NE", "KON", "NN", "NE", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}}, "stanza.5": {"line.1": {"text": "Kein Bobent kann dieser Ruhmb sich wandeln,", "tokens": ["Kein", "Bo\u00b7bent", "kann", "die\u00b7ser", "Ruhmb", "sich", "wan\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PDAT", "NN", "PRF", "VVFIN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Darumb bache ich ihmb dies Gedicht.", "tokens": ["Da\u00b7rumb", "ba\u00b7che", "ich", "ihmb", "dies", "Ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "PDS", "NN", "$."], "meter": "-----+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Was ist in der Dase . . . oder in ded Bandeln . . .", "tokens": ["Was", "ist", "in", "der", "Da\u00b7se", ".", ".", ".", "o\u00b7der", "in", "ded", "Ban\u00b7deln", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWS", "VAFIN", "APPR", "ART", "NN", "$.", "$.", "$.", "KON", "APPR", "ART", "NN", "$.", "$.", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Aber Gottseidank: ban berkt es dicht.", "tokens": ["A\u00b7ber", "Gott\u00b7sei\u00b7dank", ":", "ban", "berkt", "es", "dicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$.", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Ich wei\u00df dicht, was bit beider Dase ist \u2013", "tokens": ["Ich", "wei\u00df", "dicht", ",", "was", "bit", "bei\u00b7der", "Da\u00b7se", "ist", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$,", "PRELS", "APPR", "PIAT", "PDS", "VAFIN", "$("], "meter": "-++-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "da ist was dridd . . .", "tokens": ["da", "ist", "was", "dridd", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Doch soll bich dies dicht hindern,", "tokens": ["Doch", "soll", "bich", "dies", "dicht", "hin\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "PDS", "ADJD", "VVINF", "$,"], "meter": "-+--++-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "euch, lieben Kindern,", "tokens": ["euch", ",", "lie\u00b7ben", "Kin\u00b7dern", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "ein deutsches Lied zu singen \u2013 uns allen zum Gewidd \u2013:", "tokens": ["ein", "deut\u00b7sches", "Lied", "zu", "sin\u00b7gen", "\u2013", "uns", "al\u00b7len", "zum", "Ge\u00b7widd", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$(", "PPER", "PIAT", "APPRART", "NN", "$(", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.7": {"line.1": {"text": "Barkig schallt der Ruf der deutschen Bannen:", "tokens": ["Bar\u00b7kig", "schallt", "der", "Ruf", "der", "deut\u00b7schen", "Ban\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbheil deb gro\u00dfen Zeppeliend!", "tokens": ["\u00bb", "heil", "deb", "gro\u00b7\u00dfen", "Zep\u00b7pe\u00b7liend", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Welcher butig flog von dannen,", "tokens": ["Wel\u00b7cher", "bu\u00b7tig", "flog", "von", "dan\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "ADJD", "VVFIN", "APPR", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00fcber alle Welten hiend!\u00ab", "tokens": ["\u00fc\u00b7ber", "al\u00b7le", "Wel\u00b7ten", "hiend", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Alle Benschen konnten ihn sehnd!", "tokens": ["Al\u00b7le", "Ben\u00b7schen", "konn\u00b7ten", "ihn", "sehnd", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVFIN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Welch ein Ph\u00e4dobeend \u2013!", "tokens": ["Welch", "ein", "Ph\u00e4\u00b7do\u00b7beend", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PIAT", "ART", "NN", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.8": {"line.1": {"text": "Donnen, Deger und berlider Dutten", "tokens": ["Don\u00b7nen", ",", "De\u00b7ger", "und", "ber\u00b7li\u00b7der", "Dut\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "labten sich an seinemb Bild \u2013", "tokens": ["lab\u00b7ten", "sich", "an", "sei\u00b7nemb", "Bild", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$("], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "ohmb schrieben sie mit Underwoodn,", "tokens": ["ohmb", "schrie\u00b7ben", "sie", "mit", "Un\u00b7der\u00b7wo\u00b7odn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und sie a\u00dfen Hubber, Lachs und Wild,", "tokens": ["und", "sie", "a\u00b7\u00dfen", "Hub\u00b7ber", ",", "Lachs", "und", "Wild", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "NE", "$,", "NN", "KON", "NE", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "sowie auch die leckre Barbelade \u2013", "tokens": ["so\u00b7wie", "auch", "die", "leck\u00b7re", "Bar\u00b7be\u00b7la\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "da\u00df ich dicht dabei war, das war schade.", "tokens": ["da\u00df", "ich", "dicht", "da\u00b7bei", "war", ",", "das", "war", "scha\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "PAV", "VAFIN", "$,", "PDS", "VAFIN", "ADJD", "$."], "meter": "--+-+-+-+-", "measure": "anapaest.init"}}, "stanza.9": {"line.1": {"text": "Eckners Namb' sollt man id Barbor ritzen,", "tokens": ["Eck\u00b7ners", "Namb'", "sollt", "man", "id", "Bar\u00b7bor", "rit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VMFIN", "PIS", "NE", "NE", "VVFIN", "$,"], "meter": "+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "auf Zigarren, id ded Steid vom Dobido \u2013", "tokens": ["auf", "Zi\u00b7gar\u00b7ren", ",", "id", "ded", "Steid", "vom", "Do\u00b7bi\u00b7do", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "auf deb Pr\u00e4sidentenstuhle sollt er sitzen,", "tokens": ["auf", "deb", "Pr\u00e4\u00b7si\u00b7den\u00b7ten\u00b7stuh\u00b7le", "sollt", "er", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "daf\u00fcr neblich ist derselbe do . . .", "tokens": ["da\u00b7f\u00fcr", "neb\u00b7lich", "ist", "der\u00b7sel\u00b7be", "do", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PAV", "ADJD", "VAFIN", "PDAT", "ADV", "$.", "$.", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Alle, alle kedden ihnd ja schond,", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "ked\u00b7den", "ihnd", "ja", "schond", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "PIS", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "selbst Biss Babbitt und Frau Dathadsohnd.", "tokens": ["selbst", "Biss", "Bab\u00b7bitt", "und", "Frau", "Dath\u00b7ad\u00b7sohnd", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NE", "KON", "NN", "NE", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}}, "stanza.10": {"line.1": {"text": "Kein Bobent kann dieser Ruhmb sich wandeln,", "tokens": ["Kein", "Bo\u00b7bent", "kann", "die\u00b7ser", "Ruhmb", "sich", "wan\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PDAT", "NN", "PRF", "VVFIN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Darumb bache ich ihmb dies Gedicht.", "tokens": ["Da\u00b7rumb", "ba\u00b7che", "ich", "ihmb", "dies", "Ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "PDS", "NN", "$."], "meter": "-----+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Was ist in der Dase . . . oder in ded Bandeln . . .", "tokens": ["Was", "ist", "in", "der", "Da\u00b7se", ".", ".", ".", "o\u00b7der", "in", "ded", "Ban\u00b7deln", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWS", "VAFIN", "APPR", "ART", "NN", "$.", "$.", "$.", "KON", "APPR", "ART", "NN", "$.", "$.", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Aber Gottseidank: ban berkt es dicht.", "tokens": ["A\u00b7ber", "Gott\u00b7sei\u00b7dank", ":", "ban", "berkt", "es", "dicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$.", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}}}}