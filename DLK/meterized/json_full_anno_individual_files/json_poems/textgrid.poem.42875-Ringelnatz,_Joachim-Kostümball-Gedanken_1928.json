{"textgrid.poem.42875": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Kost\u00fcmball-Gedanken 1928", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es wechseln die Moden.", "tokens": ["Es", "wech\u00b7seln", "die", "Mo\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Aber der Hosenboden", "tokens": ["A\u00b7ber", "der", "Ho\u00b7sen\u00b7bo\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["KON", "ART", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Sitzt sinngem\u00e4\u00df", "tokens": ["Sitzt", "sinn\u00b7ge\u00b7m\u00e4\u00df"], "token_info": ["word", "word"], "pos": ["FM.la", "FM.la"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Immer unterm Ges\u00e4\u00df.", "tokens": ["Im\u00b7mer", "un\u00b7term", "Ge\u00b7s\u00e4\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.2": {"line.1": {"text": "M\u00fccken und Massenfische", "tokens": ["M\u00fc\u00b7cken", "und", "Mas\u00b7sen\u00b7fi\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Schwimmen ganz anders umeinand.", "tokens": ["Schwim\u00b7men", "ganz", "an\u00b7ders", "um\u00b7ei\u00b7nand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "VVFIN", "$."], "meter": "+--+---+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Beine wissen sich unter dem Tische", "tokens": ["Bei\u00b7ne", "wis\u00b7sen", "sich", "un\u00b7ter", "dem", "Ti\u00b7sche"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Zu benehmen, niemals die Hand.", "tokens": ["Zu", "be\u00b7neh\u00b7men", ",", "nie\u00b7mals", "die", "Hand", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "ADV", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.3": {"line.1": {"text": "Keine Teile schalten", "tokens": ["Kei\u00b7ne", "Tei\u00b7le", "schal\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Aus; ein jedes spielt Spiel.", "tokens": ["Aus", ";", "ein", "je\u00b7des", "spielt", "Spiel", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$.", "ART", "PIAT", "VVFIN", "NN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Strumpffalten zum Beispiel enthalten", "tokens": ["Strumpf\u00b7fal\u00b7ten", "zum", "Bei\u00b7spiel", "ent\u00b7hal\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "An Bedeutung viel.", "tokens": ["An", "Be\u00b7deu\u00b7tung", "viel", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Jedes tut, als ob w\u00e4r.", "tokens": ["Je\u00b7des", "tut", ",", "als", "ob", "w\u00e4r", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOKOM", "KOUS", "VAFIN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Scheinbar will niemand fischen.", "tokens": ["Schein\u00b7bar", "will", "nie\u00b7mand", "fi\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PIS", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Diesmal ist viel Revolution\u00e4r", "tokens": ["Dies\u00b7mal", "ist", "viel", "Re\u00b7vo\u00b7lu\u00b7ti\u00b7o\u00b7n\u00e4r"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIAT", "NN"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und Junges dazwischen.", "tokens": ["Und", "Jun\u00b7ges", "da\u00b7zwi\u00b7schen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.5": {"line.1": {"text": "Stierk\u00e4mpfer und Kuhfraun,", "tokens": ["Stier\u00b7k\u00e4mp\u00b7fer", "und", "Kuh\u00b7fraun", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Cowboys und Kurze Wichs.", "tokens": ["Cow\u00b7boys", "und", "Kur\u00b7ze", "Wichs", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die nur humorlos zuschaun,", "tokens": ["Die", "nur", "hu\u00b7mor\u00b7los", "zu\u00b7schaun", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind nix.", "tokens": ["Sind", "nix", "."], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "NE", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.6": {"line.1": {"text": "D\u00fcnner Nepp oder Dick-Nepp \u2013", "tokens": ["D\u00fcn\u00b7ner", "Nepp", "o\u00b7der", "Dick\u00b7Nepp", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie man sich gegenwagt \u2013", "tokens": ["Wie", "man", "sich", "ge\u00b7gen\u00b7wagt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Erzielt \u2013 wie man in Virginia sagt \u2013", "tokens": ["Er\u00b7zielt", "\u2013", "wie", "man", "in", "Vir\u00b7gi\u00b7nia", "sagt", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWAV", "PIS", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Back-door-quick-step.", "tokens": ["Back\u00b7door\u00b7quick\u00b7step", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.7": {"line.1": {"text": "Rhythmus macht viel ... Auch Haare.", "tokens": ["Rhyth\u00b7mus", "macht", "viel", "...", "Auch", "Haa\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "$(", "ADV", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Selten rei\u00dfen gedachte Stellen entzwei.", "tokens": ["Sel\u00b7ten", "rei\u00b7\u00dfen", "ge\u00b7dach\u00b7te", "Stel\u00b7len", "ent\u00b7zwei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "PTKVZ", "$."], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Leider ist alle Jahre", "tokens": ["Lei\u00b7der", "ist", "al\u00b7le", "Jah\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIAT", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Wieder die alte Ziege dabei.", "tokens": ["Wie\u00b7der", "die", "al\u00b7te", "Zie\u00b7ge", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "PAV", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "W\u00e4rmend sind zwischendurch und durch", "tokens": ["W\u00e4r\u00b7mend", "sind", "zwi\u00b7schen\u00b7durch", "und", "durch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "APPR", "KON", "APPR"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schn\u00e4pse und Sekte.", "tokens": ["Schn\u00e4p\u00b7se", "und", "Sek\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Abk\u00fchlend wie ein Lurch oder Schirurch", "tokens": ["Ab\u00b7k\u00fch\u00b7lend", "wie", "ein", "Lurch", "o\u00b7der", "Schi\u00b7rurch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "KOKOM", "ART", "NN", "KON", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wirken Dialekte.", "tokens": ["Wir\u00b7ken", "Di\u00b7a\u00b7lek\u00b7te", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.9": {"line.1": {"text": "Bunt stimmt viel froher", "tokens": ["Bunt", "stimmt", "viel", "fro\u00b7her"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PIAT", "ADJA"], "meter": "++-+-", "measure": "iambic.di"}, "line.2": {"text": "Als beispielsweise Grau.", "tokens": ["Als", "bei\u00b7spiels\u00b7wei\u00b7se", "Grau", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Aber viel sowiesoer", "tokens": ["A\u00b7ber", "viel", "so\u00b7wi\u00b7e\u00b7soer"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Reizt der Busen der Frau.", "tokens": ["Reizt", "der", "Bu\u00b7sen", "der", "Frau", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Sch\u00f6n ist stets das Originelle,", "tokens": ["Sch\u00f6n", "ist", "stets", "das", "O\u00b7rig\u00b7i\u00b7nel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Weil's von Erfindung zeugt.", "tokens": ["Weil's", "von", "Er\u00b7fin\u00b7dung", "zeugt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch das pa\u00dft nicht: wenn eine Sardelle", "tokens": ["Doch", "das", "pa\u00dft", "nicht", ":", "wenn", "ei\u00b7ne", "Sar\u00b7del\u00b7le"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PDS", "VVFIN", "PTKNEG", "$.", "KOUS", "ART", "NN"], "meter": "-+---+-+--", "measure": "dactylic.init"}, "line.4": {"text": "Vor dem Auerhahn ihr Knie beugt.", "tokens": ["Vor", "dem", "Au\u00b7er\u00b7hahn", "ihr", "Knie", "beugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Das n\u00e4chste Mal gedenke ich", "tokens": ["Das", "n\u00e4chs\u00b7te", "Mal", "ge\u00b7den\u00b7ke", "ich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als ganz Nackter mitzumachen.", "tokens": ["Als", "ganz", "Nack\u00b7ter", "mit\u00b7zu\u00b7ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und auch dies Kost\u00fcm verschenke ich.", "tokens": ["Und", "auch", "dies", "Kos\u00b7t\u00fcm", "ver\u00b7schen\u00b7ke", "ich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PDS", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nur damit die Leute lachen.", "tokens": ["Nur", "da\u00b7mit", "die", "Leu\u00b7te", "la\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PAV", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Es wechseln die Moden.", "tokens": ["Es", "wech\u00b7seln", "die", "Mo\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Aber der Hosenboden", "tokens": ["A\u00b7ber", "der", "Ho\u00b7sen\u00b7bo\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["KON", "ART", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Sitzt sinngem\u00e4\u00df", "tokens": ["Sitzt", "sinn\u00b7ge\u00b7m\u00e4\u00df"], "token_info": ["word", "word"], "pos": ["FM.la", "FM.la"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Immer unterm Ges\u00e4\u00df.", "tokens": ["Im\u00b7mer", "un\u00b7term", "Ge\u00b7s\u00e4\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.13": {"line.1": {"text": "M\u00fccken und Massenfische", "tokens": ["M\u00fc\u00b7cken", "und", "Mas\u00b7sen\u00b7fi\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Schwimmen ganz anders umeinand.", "tokens": ["Schwim\u00b7men", "ganz", "an\u00b7ders", "um\u00b7ei\u00b7nand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "VVFIN", "$."], "meter": "+--+---+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Beine wissen sich unter dem Tische", "tokens": ["Bei\u00b7ne", "wis\u00b7sen", "sich", "un\u00b7ter", "dem", "Ti\u00b7sche"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Zu benehmen, niemals die Hand.", "tokens": ["Zu", "be\u00b7neh\u00b7men", ",", "nie\u00b7mals", "die", "Hand", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "ADV", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.14": {"line.1": {"text": "Keine Teile schalten", "tokens": ["Kei\u00b7ne", "Tei\u00b7le", "schal\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Aus; ein jedes spielt Spiel.", "tokens": ["Aus", ";", "ein", "je\u00b7des", "spielt", "Spiel", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$.", "ART", "PIAT", "VVFIN", "NN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Strumpffalten zum Beispiel enthalten", "tokens": ["Strumpf\u00b7fal\u00b7ten", "zum", "Bei\u00b7spiel", "ent\u00b7hal\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "An Bedeutung viel.", "tokens": ["An", "Be\u00b7deu\u00b7tung", "viel", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.15": {"line.1": {"text": "Jedes tut, als ob w\u00e4r.", "tokens": ["Je\u00b7des", "tut", ",", "als", "ob", "w\u00e4r", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOKOM", "KOUS", "VAFIN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Scheinbar will niemand fischen.", "tokens": ["Schein\u00b7bar", "will", "nie\u00b7mand", "fi\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PIS", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Diesmal ist viel Revolution\u00e4r", "tokens": ["Dies\u00b7mal", "ist", "viel", "Re\u00b7vo\u00b7lu\u00b7ti\u00b7o\u00b7n\u00e4r"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIAT", "NN"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und Junges dazwischen.", "tokens": ["Und", "Jun\u00b7ges", "da\u00b7zwi\u00b7schen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.16": {"line.1": {"text": "Stierk\u00e4mpfer und Kuhfraun,", "tokens": ["Stier\u00b7k\u00e4mp\u00b7fer", "und", "Kuh\u00b7fraun", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Cowboys und Kurze Wichs.", "tokens": ["Cow\u00b7boys", "und", "Kur\u00b7ze", "Wichs", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die nur humorlos zuschaun,", "tokens": ["Die", "nur", "hu\u00b7mor\u00b7los", "zu\u00b7schaun", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind nix.", "tokens": ["Sind", "nix", "."], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "NE", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.17": {"line.1": {"text": "D\u00fcnner Nepp oder Dick-Nepp \u2013", "tokens": ["D\u00fcn\u00b7ner", "Nepp", "o\u00b7der", "Dick\u00b7Nepp", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie man sich gegenwagt \u2013", "tokens": ["Wie", "man", "sich", "ge\u00b7gen\u00b7wagt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Erzielt \u2013 wie man in Virginia sagt \u2013", "tokens": ["Er\u00b7zielt", "\u2013", "wie", "man", "in", "Vir\u00b7gi\u00b7nia", "sagt", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWAV", "PIS", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Back-door-quick-step.", "tokens": ["Back\u00b7door\u00b7quick\u00b7step", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.18": {"line.1": {"text": "Rhythmus macht viel ... Auch Haare.", "tokens": ["Rhyth\u00b7mus", "macht", "viel", "...", "Auch", "Haa\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "$(", "ADV", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Selten rei\u00dfen gedachte Stellen entzwei.", "tokens": ["Sel\u00b7ten", "rei\u00b7\u00dfen", "ge\u00b7dach\u00b7te", "Stel\u00b7len", "ent\u00b7zwei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "PTKVZ", "$."], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Leider ist alle Jahre", "tokens": ["Lei\u00b7der", "ist", "al\u00b7le", "Jah\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIAT", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Wieder die alte Ziege dabei.", "tokens": ["Wie\u00b7der", "die", "al\u00b7te", "Zie\u00b7ge", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "PAV", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.19": {"line.1": {"text": "W\u00e4rmend sind zwischendurch und durch", "tokens": ["W\u00e4r\u00b7mend", "sind", "zwi\u00b7schen\u00b7durch", "und", "durch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "APPR", "KON", "APPR"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schn\u00e4pse und Sekte.", "tokens": ["Schn\u00e4p\u00b7se", "und", "Sek\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Abk\u00fchlend wie ein Lurch oder Schirurch", "tokens": ["Ab\u00b7k\u00fch\u00b7lend", "wie", "ein", "Lurch", "o\u00b7der", "Schi\u00b7rurch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "KOKOM", "ART", "NN", "KON", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wirken Dialekte.", "tokens": ["Wir\u00b7ken", "Di\u00b7a\u00b7lek\u00b7te", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.20": {"line.1": {"text": "Bunt stimmt viel froher", "tokens": ["Bunt", "stimmt", "viel", "fro\u00b7her"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PIAT", "ADJA"], "meter": "++-+-", "measure": "iambic.di"}, "line.2": {"text": "Als beispielsweise Grau.", "tokens": ["Als", "bei\u00b7spiels\u00b7wei\u00b7se", "Grau", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Aber viel sowiesoer", "tokens": ["A\u00b7ber", "viel", "so\u00b7wi\u00b7e\u00b7soer"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Reizt der Busen der Frau.", "tokens": ["Reizt", "der", "Bu\u00b7sen", "der", "Frau", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.21": {"line.1": {"text": "Sch\u00f6n ist stets das Originelle,", "tokens": ["Sch\u00f6n", "ist", "stets", "das", "O\u00b7rig\u00b7i\u00b7nel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Weil's von Erfindung zeugt.", "tokens": ["Weil's", "von", "Er\u00b7fin\u00b7dung", "zeugt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch das pa\u00dft nicht: wenn eine Sardelle", "tokens": ["Doch", "das", "pa\u00dft", "nicht", ":", "wenn", "ei\u00b7ne", "Sar\u00b7del\u00b7le"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PDS", "VVFIN", "PTKNEG", "$.", "KOUS", "ART", "NN"], "meter": "-+---+-+--", "measure": "dactylic.init"}, "line.4": {"text": "Vor dem Auerhahn ihr Knie beugt.", "tokens": ["Vor", "dem", "Au\u00b7er\u00b7hahn", "ihr", "Knie", "beugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Das n\u00e4chste Mal gedenke ich", "tokens": ["Das", "n\u00e4chs\u00b7te", "Mal", "ge\u00b7den\u00b7ke", "ich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als ganz Nackter mitzumachen.", "tokens": ["Als", "ganz", "Nack\u00b7ter", "mit\u00b7zu\u00b7ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und auch dies Kost\u00fcm verschenke ich.", "tokens": ["Und", "auch", "dies", "Kos\u00b7t\u00fcm", "ver\u00b7schen\u00b7ke", "ich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PDS", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nur damit die Leute lachen.", "tokens": ["Nur", "da\u00b7mit", "die", "Leu\u00b7te", "la\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PAV", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}