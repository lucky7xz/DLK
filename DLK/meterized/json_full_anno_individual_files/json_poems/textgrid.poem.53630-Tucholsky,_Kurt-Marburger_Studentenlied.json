{"textgrid.poem.53630": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Marburger Studentenlied", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.85", "da:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Stimmt an mit hellem, hohem Klang,", "tokens": ["Stimmt", "an", "mit", "hel\u00b7lem", ",", "ho\u00b7hem", "Klang", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "APPR", "ADJA", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "stimmt an das Lied der Lieder!", "tokens": ["stimmt", "an", "das", "Lied", "der", "Lie\u00b7der", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Des Vaterlandes Hochgesang,", "tokens": ["Des", "Va\u00b7ter\u00b7lan\u00b7des", "Hoch\u00b7ge\u00b7sang", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "das Waldtal hallt ihn wi\u2013hi\u2013der!", "tokens": ["das", "Wald\u00b7tal", "hallt", "ihn", "wi", "\u2013", "hi", "\u2013", "der", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "NE", "$(", "FM", "$(", "ART", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Der alten Barden Kriegsgericht,", "tokens": ["Der", "al\u00b7ten", "Bar\u00b7den", "Kriegs\u00b7ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem Kriegsgericht der Treue \u2013", "tokens": ["dem", "Kriegs\u00b7ge\u00b7richt", "der", "Treu\u00b7e", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "wir wissen, du verknackst uns nicht \u2013", "tokens": ["wir", "wis\u00b7sen", ",", "du", "ver\u00b7knackst", "uns", "nicht", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "dir weihn wir uns aufs Neue!", "tokens": ["dir", "weihn", "wir", "uns", "aufs", "Neu\u00b7e", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PRF", "APPRART", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Wir fingen fuffzehn von dem Pack,", "tokens": ["Wir", "fin\u00b7gen", "fuff\u00b7zehn", "von", "dem", "Pack", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "CARD", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "das unser Preu\u00dfen sch\u00e4digt.", "tokens": ["das", "un\u00b7ser", "Preu\u00b7\u00dfen", "sch\u00e4\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es war ein sch\u00f6ner M\u00e4rzentag.", "tokens": ["Es", "war", "ein", "sch\u00f6\u00b7ner", "M\u00e4r\u00b7zen\u00b7tag", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir haben sie erledigt.", "tokens": ["Wir", "ha\u00b7ben", "sie", "er\u00b7le\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Sie sind von uns erschossen worn.", "tokens": ["Sie", "sind", "von", "uns", "er\u00b7schos\u00b7sen", "worn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch ganz in Recht und Z\u00fcchten.", "tokens": ["Doch", "ganz", "in", "Recht", "und", "Z\u00fcch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zwar sitzen ihre Wunden vorn . . .", "tokens": ["Zwar", "sit\u00b7zen", "ih\u00b7re", "Wun\u00b7den", "vorn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ADV", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man kann auch r\u00fcckw\u00e4rts fl\u00fcchten.", "tokens": ["Man", "kann", "auch", "r\u00fcck\u00b7w\u00e4rts", "fl\u00fcch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Wir wissen jeden krummen Weg.", "tokens": ["Wir", "wis\u00b7sen", "je\u00b7den", "krum\u00b7men", "Weg", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Uns kann man nicht erweichen.", "tokens": ["Uns", "kann", "man", "nicht", "er\u00b7wei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Mediziner im Kolleg", "tokens": ["Der", "Me\u00b7di\u00b7zi\u00b7ner", "im", "Kol\u00b7leg"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "braucht Leichen, Leichen, Leichen!", "tokens": ["braucht", "Lei\u00b7chen", ",", "Lei\u00b7chen", ",", "Lei\u00b7chen", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Uns tut kein deutscher Richter nichts", "tokens": ["Uns", "tut", "kein", "deut\u00b7scher", "Rich\u00b7ter", "nichts"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und auch kein Staatsanwalte.", "tokens": ["und", "auch", "kein", "Staats\u00b7an\u00b7wal\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Schranken unsres Kriegsgerichts", "tokens": ["Die", "Schran\u00b7ken", "uns\u00b7res", "Kriegs\u00b7ge\u00b7richts"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der liebe Gott erhalte!", "tokens": ["der", "lie\u00b7be", "Gott", "er\u00b7hal\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Zur Ahnentugend wir uns weihn,", "tokens": ["Zur", "Ah\u00b7nen\u00b7tu\u00b7gend", "wir", "uns", "weihn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zum Schutze deiner H\u00fctten!", "tokens": ["zum", "Schut\u00b7ze", "dei\u00b7ner", "H\u00fct\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir lieben deutsches Fr\u00f6hlichsein", "tokens": ["Wir", "lie\u00b7ben", "deut\u00b7sches", "Fr\u00f6h\u00b7lich\u00b7sein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und echte deutsche Sitten!", "tokens": ["und", "ech\u00b7te", "deut\u00b7sche", "Sit\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ad exercitium executionis parati estisne?", "tokens": ["Ad", "ex\u00b7er\u00b7ci\u00b7tium", "ex\u00b7e\u00b7cu\u00b7ti\u00b7o\u00b7nis", "pa\u00b7ra\u00b7ti", "es\u00b7tis\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "+--+-+-+--+-+--+-", "measure": "iambic.septa.invert"}, "line.6": {"text": "Sumus!", "tokens": ["Su\u00b7mus", "!"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.8": {"line.1": {"text": "Stimmt an mit hellem, hohem Klang,", "tokens": ["Stimmt", "an", "mit", "hel\u00b7lem", ",", "ho\u00b7hem", "Klang", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "APPR", "ADJA", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "stimmt an das Lied der Lieder!", "tokens": ["stimmt", "an", "das", "Lied", "der", "Lie\u00b7der", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Des Vaterlandes Hochgesang,", "tokens": ["Des", "Va\u00b7ter\u00b7lan\u00b7des", "Hoch\u00b7ge\u00b7sang", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "das Waldtal hallt ihn wi\u2013hi\u2013der!", "tokens": ["das", "Wald\u00b7tal", "hallt", "ihn", "wi", "\u2013", "hi", "\u2013", "der", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "NE", "$(", "FM", "$(", "ART", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "Der alten Barden Kriegsgericht,", "tokens": ["Der", "al\u00b7ten", "Bar\u00b7den", "Kriegs\u00b7ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem Kriegsgericht der Treue \u2013", "tokens": ["dem", "Kriegs\u00b7ge\u00b7richt", "der", "Treu\u00b7e", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "wir wissen, du verknackst uns nicht \u2013", "tokens": ["wir", "wis\u00b7sen", ",", "du", "ver\u00b7knackst", "uns", "nicht", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "dir weihn wir uns aufs Neue!", "tokens": ["dir", "weihn", "wir", "uns", "aufs", "Neu\u00b7e", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PRF", "APPRART", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Wir fingen fuffzehn von dem Pack,", "tokens": ["Wir", "fin\u00b7gen", "fuff\u00b7zehn", "von", "dem", "Pack", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "CARD", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "das unser Preu\u00dfen sch\u00e4digt.", "tokens": ["das", "un\u00b7ser", "Preu\u00b7\u00dfen", "sch\u00e4\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es war ein sch\u00f6ner M\u00e4rzentag.", "tokens": ["Es", "war", "ein", "sch\u00f6\u00b7ner", "M\u00e4r\u00b7zen\u00b7tag", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir haben sie erledigt.", "tokens": ["Wir", "ha\u00b7ben", "sie", "er\u00b7le\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Sie sind von uns erschossen worn.", "tokens": ["Sie", "sind", "von", "uns", "er\u00b7schos\u00b7sen", "worn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch ganz in Recht und Z\u00fcchten.", "tokens": ["Doch", "ganz", "in", "Recht", "und", "Z\u00fcch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zwar sitzen ihre Wunden vorn . . .", "tokens": ["Zwar", "sit\u00b7zen", "ih\u00b7re", "Wun\u00b7den", "vorn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ADV", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man kann auch r\u00fcckw\u00e4rts fl\u00fcchten.", "tokens": ["Man", "kann", "auch", "r\u00fcck\u00b7w\u00e4rts", "fl\u00fcch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Wir wissen jeden krummen Weg.", "tokens": ["Wir", "wis\u00b7sen", "je\u00b7den", "krum\u00b7men", "Weg", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Uns kann man nicht erweichen.", "tokens": ["Uns", "kann", "man", "nicht", "er\u00b7wei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Mediziner im Kolleg", "tokens": ["Der", "Me\u00b7di\u00b7zi\u00b7ner", "im", "Kol\u00b7leg"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "braucht Leichen, Leichen, Leichen!", "tokens": ["braucht", "Lei\u00b7chen", ",", "Lei\u00b7chen", ",", "Lei\u00b7chen", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Uns tut kein deutscher Richter nichts", "tokens": ["Uns", "tut", "kein", "deut\u00b7scher", "Rich\u00b7ter", "nichts"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und auch kein Staatsanwalte.", "tokens": ["und", "auch", "kein", "Staats\u00b7an\u00b7wal\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Schranken unsres Kriegsgerichts", "tokens": ["Die", "Schran\u00b7ken", "uns\u00b7res", "Kriegs\u00b7ge\u00b7richts"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der liebe Gott erhalte!", "tokens": ["der", "lie\u00b7be", "Gott", "er\u00b7hal\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Zur Ahnentugend wir uns weihn,", "tokens": ["Zur", "Ah\u00b7nen\u00b7tu\u00b7gend", "wir", "uns", "weihn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zum Schutze deiner H\u00fctten!", "tokens": ["zum", "Schut\u00b7ze", "dei\u00b7ner", "H\u00fct\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir lieben deutsches Fr\u00f6hlichsein", "tokens": ["Wir", "lie\u00b7ben", "deut\u00b7sches", "Fr\u00f6h\u00b7lich\u00b7sein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und echte deutsche Sitten!", "tokens": ["und", "ech\u00b7te", "deut\u00b7sche", "Sit\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ad exercitium executionis parati estisne?", "tokens": ["Ad", "ex\u00b7er\u00b7ci\u00b7tium", "ex\u00b7e\u00b7cu\u00b7ti\u00b7o\u00b7nis", "pa\u00b7ra\u00b7ti", "es\u00b7tis\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "+--+-+-+--+-+--+-", "measure": "iambic.septa.invert"}, "line.6": {"text": "Sumus!", "tokens": ["Su\u00b7mus", "!"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}}}}}