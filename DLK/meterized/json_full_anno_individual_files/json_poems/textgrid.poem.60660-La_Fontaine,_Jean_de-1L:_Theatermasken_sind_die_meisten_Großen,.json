{"textgrid.poem.60660": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Theatermasken sind die meisten Gro\u00dfen,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Theatermasken sind die meisten Gro\u00dfen,", "tokens": ["The\u00b7a\u00b7ter\u00b7mas\u00b7ken", "sind", "die", "meis\u00b7ten", "Gro\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Verstehn zu wirken auf die Anspruchslosen.", "tokens": ["Ver\u00b7stehn", "zu", "wir\u00b7ken", "auf", "die", "An\u00b7spruchs\u00b7lo\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PTKZU", "VVINF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Esel bewerten das nur, was sie sehen;", "tokens": ["E\u00b7sel", "be\u00b7wer\u00b7ten", "das", "nur", ",", "was", "sie", "se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PDS", "ADV", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Der Fuchs jedoch wei\u00df auf den Grund zu gehen:", "tokens": ["Der", "Fuchs", "je\u00b7doch", "wei\u00df", "auf", "den", "Grund", "zu", "ge\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "VVFIN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Betrachtet solchen Kopf von allen Seiten her,", "tokens": ["Be\u00b7trach\u00b7tet", "sol\u00b7chen", "Kopf", "von", "al\u00b7len", "Sei\u00b7ten", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und merkt er, da\u00df sein Wert allein", "tokens": ["Und", "merkt", "er", ",", "da\u00df", "sein", "Wert", "al\u00b7lein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Im sch\u00f6nen \u00c4u\u00dfern steckt, so richtet er", "tokens": ["Im", "sch\u00f6\u00b7nen", "\u00c4u\u00b7\u00dfern", "steckt", ",", "so", "rich\u00b7tet", "er"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$,", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Ein Wort an ihn, das er recht fein", "tokens": ["Ein", "Wort", "an", "ihn", ",", "das", "er", "recht", "fein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "PPER", "$,", "PRELS", "PPER", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Auf eine Heldenb\u00fcste einst gepr\u00e4gt,", "tokens": ["Auf", "ei\u00b7ne", "Hel\u00b7den\u00b7b\u00fcs\u00b7te", "einst", "ge\u00b7pr\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Die man ihm, gro\u00df und hohl, in seinen Weg gelegt.", "tokens": ["Die", "man", "ihm", ",", "gro\u00df", "und", "hohl", ",", "in", "sei\u00b7nen", "Weg", "ge\u00b7legt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "$,", "ADJD", "KON", "ADJD", "$,", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Er nahm sie lobend gleich in Augenschein:", "tokens": ["Er", "nahm", "sie", "lo\u00b7bend", "gleich", "in", "Au\u00b7gen\u00b7schein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "\u00bbein sch\u00f6ner Kopf \u2013 nur da\u00df kein Hirn darin!\u00ab", "tokens": ["\u00bb", "ein", "sch\u00f6\u00b7ner", "Kopf", "\u2013", "nur", "da\u00df", "kein", "Hirn", "da\u00b7rin", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$(", "ADV", "KOUS", "PIAT", "NN", "PAV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Wie viele hohe \u00bbB\u00fcsten\u00ab gibt's in diesem Sinn!", "tokens": ["Wie", "vie\u00b7le", "ho\u00b7he", "\u00bb", "B\u00fcs\u00b7ten", "\u00ab", "gibt's", "in", "die\u00b7sem", "Sinn", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "ADJA", "$(", "NN", "$(", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Theatermasken sind die meisten Gro\u00dfen,", "tokens": ["The\u00b7a\u00b7ter\u00b7mas\u00b7ken", "sind", "die", "meis\u00b7ten", "Gro\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Verstehn zu wirken auf die Anspruchslosen.", "tokens": ["Ver\u00b7stehn", "zu", "wir\u00b7ken", "auf", "die", "An\u00b7spruchs\u00b7lo\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PTKZU", "VVINF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Esel bewerten das nur, was sie sehen;", "tokens": ["E\u00b7sel", "be\u00b7wer\u00b7ten", "das", "nur", ",", "was", "sie", "se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PDS", "ADV", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Der Fuchs jedoch wei\u00df auf den Grund zu gehen:", "tokens": ["Der", "Fuchs", "je\u00b7doch", "wei\u00df", "auf", "den", "Grund", "zu", "ge\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "VVFIN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Betrachtet solchen Kopf von allen Seiten her,", "tokens": ["Be\u00b7trach\u00b7tet", "sol\u00b7chen", "Kopf", "von", "al\u00b7len", "Sei\u00b7ten", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und merkt er, da\u00df sein Wert allein", "tokens": ["Und", "merkt", "er", ",", "da\u00df", "sein", "Wert", "al\u00b7lein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Im sch\u00f6nen \u00c4u\u00dfern steckt, so richtet er", "tokens": ["Im", "sch\u00f6\u00b7nen", "\u00c4u\u00b7\u00dfern", "steckt", ",", "so", "rich\u00b7tet", "er"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$,", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Ein Wort an ihn, das er recht fein", "tokens": ["Ein", "Wort", "an", "ihn", ",", "das", "er", "recht", "fein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "PPER", "$,", "PRELS", "PPER", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Auf eine Heldenb\u00fcste einst gepr\u00e4gt,", "tokens": ["Auf", "ei\u00b7ne", "Hel\u00b7den\u00b7b\u00fcs\u00b7te", "einst", "ge\u00b7pr\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Die man ihm, gro\u00df und hohl, in seinen Weg gelegt.", "tokens": ["Die", "man", "ihm", ",", "gro\u00df", "und", "hohl", ",", "in", "sei\u00b7nen", "Weg", "ge\u00b7legt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "$,", "ADJD", "KON", "ADJD", "$,", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Er nahm sie lobend gleich in Augenschein:", "tokens": ["Er", "nahm", "sie", "lo\u00b7bend", "gleich", "in", "Au\u00b7gen\u00b7schein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "\u00bbein sch\u00f6ner Kopf \u2013 nur da\u00df kein Hirn darin!\u00ab", "tokens": ["\u00bb", "ein", "sch\u00f6\u00b7ner", "Kopf", "\u2013", "nur", "da\u00df", "kein", "Hirn", "da\u00b7rin", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "$(", "ADV", "KOUS", "PIAT", "NN", "PAV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Wie viele hohe \u00bbB\u00fcsten\u00ab gibt's in diesem Sinn!", "tokens": ["Wie", "vie\u00b7le", "ho\u00b7he", "\u00bb", "B\u00fcs\u00b7ten", "\u00ab", "gibt's", "in", "die\u00b7sem", "Sinn", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "ADJA", "$(", "NN", "$(", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}