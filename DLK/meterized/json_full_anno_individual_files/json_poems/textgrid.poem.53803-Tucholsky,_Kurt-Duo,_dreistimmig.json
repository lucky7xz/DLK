{"textgrid.poem.53803": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Duo, dreistimmig", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "G\u00f6tz von Berlichingen und der General Cambronne", "tokens": ["G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "und", "der", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "NE", "KON", "ART", "NN", "NE"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.2": {"text": "(derselbe, der damals in der Schlacht von", "tokens": ["(", "der\u00b7sel\u00b7be", ",", "der", "da\u00b7mals", "in", "der", "Schlacht", "von"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PDAT", "$,", "PRELS", "ADV", "APPR", "ART", "NN", "APPR"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Waterloo nicht gesagt hat wie im Heldengedicht:", "tokens": ["Wa\u00b7ter\u00b7loo", "nicht", "ge\u00b7sagt", "hat", "wie", "im", "Hel\u00b7den\u00b7ge\u00b7dicht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "VVPP", "VAFIN", "KOKOM", "APPRART", "NN", "$."], "meter": "+-+--+-+-+--+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "\u00bbdie Garde stirbt, doch sie ergibt sich nicht!\u00ab", "tokens": ["\u00bb", "die", "Gar\u00b7de", "stirbt", ",", "doch", "sie", "er\u00b7gibt", "sich", "nicht", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "$,", "KON", "PPER", "VVFIN", "PRF", "PTKNEG", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sondern er sagte nur schlicht:", "tokens": ["Son\u00b7dern", "er", "sag\u00b7te", "nur", "schlicht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.6": {"text": "\u00bbmerde!\u00ab) \u2013", "tokens": ["\u00bb", "mer\u00b7de", "!", "\u00ab", ")", "\u2013"], "token_info": ["punct", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "VVFIN", "$.", "$(", "$(", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "dieser General Cambronne und G\u00f6tz von Berlichingen", "tokens": ["die\u00b7ser", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne", "und", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "NE", "KON", "NN", "APPR", "NN"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.8": {"text": "trafen sich neulich im Caf\u00e9 und t\u00e4ten daselbst singen:", "tokens": ["tra\u00b7fen", "sich", "neu\u00b7lich", "im", "Ca\u00b7f\u00e9", "und", "t\u00e4\u00b7ten", "da\u00b7selbst", "sin\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "APPRART", "NN", "KON", "VVFIN", "PAV", "VVINF", "$."], "meter": "+--+-+-+-+-+-+-", "measure": "iambic.septa.invert"}}, "stanza.2": {"line.1": {"text": "\u00bbwir, die Nationalheiligen zweier Nationen,", "tokens": ["\u00bb", "wir", ",", "die", "Na\u00b7ti\u00b7o\u00b7nal\u00b7hei\u00b7li\u00b7gen", "zwei\u00b7er", "Na\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.2": {"text": "die man uns anruft, wo nur Franzosen und Deutsche wohnen,", "tokens": ["die", "man", "uns", "an\u00b7ruft", ",", "wo", "nur", "Fran\u00b7zo\u00b7sen", "und", "Deut\u00b7sche", "woh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "VVFIN", "$,", "PWAV", "ADV", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+--+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "haben uns hier pro Nase einen Mokka Dubel bestellt", "tokens": ["ha\u00b7ben", "uns", "hier", "pro", "Na\u00b7se", "ei\u00b7nen", "Mok\u00b7ka", "Du\u00b7bel", "be\u00b7stellt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN", "ART", "NE", "NE", "VVFIN"], "meter": "+--+-+-+--+-+-+", "measure": "iambic.septa.invert"}, "line.4": {"text": "und betrachten zur Abwechslung einmal den Lauf der Welt.\u00ab", "tokens": ["und", "be\u00b7trach\u00b7ten", "zur", "Ab\u00b7wechs\u00b7lung", "ein\u00b7mal", "den", "Lauf", "der", "Welt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "ART", "NN", "$.", "$("], "meter": "--+--+--+--+-+", "measure": "anapaest.tetra.plus"}}, "stanza.3": {"line.1": {"text": "Der G\u00f6tz begann:", "tokens": ["Der", "G\u00f6tz", "be\u00b7gann", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "\u00bbwas h\u00e4ltst du, Bruderherz, von den Demokraten,", "tokens": ["\u00bb", "was", "h\u00e4ltst", "du", ",", "Bru\u00b7der\u00b7herz", ",", "von", "den", "De\u00b7mo\u00b7kra\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "$,", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "die noch in jeden Wein ihr Wasser abschlagen taten,", "tokens": ["die", "noch", "in", "je\u00b7den", "Wein", "ihr", "Was\u00b7ser", "ab\u00b7schla\u00b7gen", "ta\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "PIAT", "NN", "PPOSAT", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "vorsichtig,", "tokens": ["vor\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "umsichtig,", "tokens": ["um\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "nachsichtig,", "tokens": ["nach\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "kurzsichtig \u2013", "tokens": ["kurz\u00b7sich\u00b7tig", "\u2013"], "token_info": ["word", "punct"], "pos": ["ADJD", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "und liegen immer unten. Was h\u00e4ltst du davon \u2013?\u00ab", "tokens": ["und", "lie\u00b7gen", "im\u00b7mer", "un\u00b7ten", ".", "Was", "h\u00e4ltst", "du", "da\u00b7von", "\u2013", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "$.", "PWS", "VVFIN", "PPER", "PAV", "$(", "$.", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "\u00bbmerde \u2013!\u00ab sagte Cambronne.", "tokens": ["\u00bb", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", "."], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.4": {"line.1": {"text": "\u00bbwas aber h\u00e4ltst du, Bruder, von den preu\u00dfischen Richtern,", "tokens": ["\u00bb", "was", "a\u00b7ber", "h\u00e4ltst", "du", ",", "Bru\u00b7der", ",", "von", "den", "preu\u00b7\u00dfi\u00b7schen", "Rich\u00b7tern", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "ADV", "VVFIN", "PPER", "$,", "NN", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "diesen Vollzugsbeamten von Denkern und Dichtern?", "tokens": ["die\u00b7sen", "Voll\u00b7zugs\u00b7be\u00b7am\u00b7ten", "von", "Den\u00b7kern", "und", "Dich\u00b7tern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wie sie nichts h\u00f6ren und nichts sehn \u2013 aber zuschlagen", "tokens": ["Wie", "sie", "nichts", "h\u00f6\u00b7ren", "und", "nichts", "sehn", "\u2013", "a\u00b7ber", "zu\u00b7schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "KON", "PIS", "VVINF", "$(", "ADV", "VVINF"], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und um sich Jammer verbreiten und Klagen.", "tokens": ["und", "um", "sich", "Jam\u00b7mer", "ver\u00b7brei\u00b7ten", "und", "Kla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PRF", "NN", "VVINF", "KON", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Wie sie die Wehrlosen fangen in ihren Schlingen . . . ?\u00ab", "tokens": ["Wie", "sie", "die", "Wehr\u00b7lo\u00b7sen", "fan\u00b7gen", "in", "ih\u00b7ren", "Schlin\u00b7gen", ".", ".", ".", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$.", "$.", "$.", "$("], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.6": {"text": "\u00bb . . . . . . !\u00ab sagte der G\u00f6tz von Berlichingen.", "tokens": ["\u00bb", ".", ".", ".", ".", ".", ".", "!", "\u00ab", "sag\u00b7te", "der", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$(", "VVFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.5": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbkennst du aber die uniformierten Burschen in allen L\u00e4ndern,", "tokens": ["\u00bb", "kennst", "du", "a\u00b7ber", "die", "u\u00b7nif\u00b7or\u00b7mier\u00b7ten", "Bur\u00b7schen", "in", "al\u00b7len", "L\u00e4n\u00b7dern", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+--+--+-+--+-+-", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "die in ihren bekleckerten Indianergew\u00e4ndern", "tokens": ["die", "in", "ih\u00b7ren", "be\u00b7kle\u00b7cker\u00b7ten", "In\u00b7di\u00b7a\u00b7ner\u00b7ge\u00b7w\u00e4n\u00b7dern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "--+--+--+-+--+-", "measure": "anapaest.tri.plus"}, "line.4": {"text": "den n\u00e4chsten Krieg vorbereiten? Mit dem Anspruch aufs Panth\u00e9on?\u00ab", "tokens": ["den", "n\u00e4chs\u00b7ten", "Krieg", "vor\u00b7be\u00b7rei\u00b7ten", "?", "Mit", "dem", "An\u00b7spruch", "aufs", "Pan\u00b7th\u00e9on", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$.", "APPR", "ART", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+--+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "\u00bbah merde \u2013!\u00ab sagte Cambronne.", "tokens": ["\u00bb", "ah", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", "."], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "FM.la", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.6": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbkennst du aber die Theaterdirektoren?", "tokens": ["\u00bb", "kennst", "du", "a\u00b7ber", "die", "The\u00b7a\u00b7ter\u00b7di\u00b7rek\u00b7to\u00b7ren", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Jedem ist gerade ein neues Genie geboren,", "tokens": ["Je\u00b7dem", "ist", "ge\u00b7ra\u00b7de", "ein", "neu\u00b7es", "Ge\u00b7nie", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+--+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "und besiehst du dir n\u00e4her die g\u00f6ttliche Ware,", "tokens": ["und", "be\u00b7siehst", "du", "dir", "n\u00e4\u00b7her", "die", "g\u00f6tt\u00b7li\u00b7che", "Wa\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADJD", "ART", "ADJA", "NN", "$,"], "meter": "--+--+--+--+-", "measure": "anapaest.tetra.plus"}, "line.5": {"text": "ists ein Genie vom vorigen Jahre.", "tokens": ["ists", "ein", "Ge\u00b7nie", "vom", "vo\u00b7ri\u00b7gen", "Jah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "++-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Haben einen Augenfehler: schielen auf die Kritik", "tokens": ["Ha\u00b7ben", "ei\u00b7nen", "Au\u00b7gen\u00b7feh\u00b7ler", ":", "schie\u00b7len", "auf", "die", "Kri\u00b7tik"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "$.", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+---", "measure": "unknown.measure.hexa"}, "line.7": {"text": "und sitzen in einer Konjunktur-Fabrik.", "tokens": ["und", "sit\u00b7zen", "in", "ei\u00b7ner", "Kon\u00b7junk\u00b7tur\u00b7Fa\u00b7brik", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "W\u00e4r gar nicht \u00fcbel. Nur:", "tokens": ["W\u00e4r", "gar", "nicht", "\u00fc\u00b7bel", ".", "Nur", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADJD", "$.", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "es ist immer die falsche Konjunktur.", "tokens": ["es", "ist", "im\u00b7mer", "die", "fal\u00b7sche", "Kon\u00b7junk\u00b7tur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Wirr. Unzuverl\u00e4ssig. Ja, was k\u00f6nnen sie denn vor allen Dingen \u2013?\u00ab", "tokens": ["Wirr", ".", "Un\u00b7zu\u00b7ver\u00b7l\u00e4s\u00b7sig", ".", "Ja", ",", "was", "k\u00f6n\u00b7nen", "sie", "denn", "vor", "al\u00b7len", "Din\u00b7gen", "\u2013", "?", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "$.", "ADJD", "$.", "PTKANT", "$,", "PWS", "VMFIN", "PPER", "ADV", "APPR", "PIAT", "NN", "$(", "$.", "$("], "meter": "-+--+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.11": {"text": "Da sagte es der G\u00f6tz von Berlichingen.", "tokens": ["Da", "sag\u00b7te", "es", "der", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbwas h\u00e4ltst du aber hingegen von den Parlamenten?", "tokens": ["\u00bb", "was", "h\u00e4ltst", "du", "a\u00b7ber", "hin\u00b7ge\u00b7gen", "von", "den", "Par\u00b7la\u00b7men\u00b7ten", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Mit ihren Kommissionssitzungen und ihren Re- und Korreferenten?", "tokens": ["Mit", "ih\u00b7ren", "Kom\u00b7mis\u00b7si\u00b7ons\u00b7sit\u00b7zun\u00b7gen", "und", "ih\u00b7ren", "Re", "und", "Kor\u00b7re\u00b7fe\u00b7ren\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "PPOSAT", "TRUNC", "KON", "NN", "$."], "meter": "-+--+-+-+--+-+-+--+-", "measure": "iambic.octa.plus.relaxed"}, "line.4": {"text": "Bruder, sag mir, ist es bei euch das gleiche", "tokens": ["Bru\u00b7der", ",", "sag", "mir", ",", "ist", "es", "bei", "euch", "das", "glei\u00b7che"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "VVFIN", "PPER", "$,", "VAFIN", "PPER", "APPR", "PPER", "ART", "ADJA"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "wie in unserm republikanischen Kaiserreiche?", "tokens": ["wie", "in", "un\u00b7serm", "re\u00b7pub\u00b7li\u00b7ka\u00b7ni\u00b7schen", "Kai\u00b7ser\u00b7rei\u00b7che", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+--+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.6": {"text": "Das Ganze nennt man Demokratie \u2013", "tokens": ["Das", "Gan\u00b7ze", "nennt", "man", "De\u00b7mo\u00b7kra\u00b7tie", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "ART", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "ist aber nur eine politische Schwerindustrie.", "tokens": ["ist", "a\u00b7ber", "nur", "ei\u00b7ne", "po\u00b7li\u00b7ti\u00b7sche", "Schwe\u00b7rin\u00b7dust\u00b7rie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+---+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Gut vor hundert Jahren. Heute: so alt, so alt \u2013", "tokens": ["Gut", "vor", "hun\u00b7dert", "Jah\u00b7ren", ".", "Heu\u00b7te", ":", "so", "alt", ",", "so", "alt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "APPR", "CARD", "NN", "$.", "ADV", "$.", "ADV", "ADJD", "$,", "ADV", "ADJD", "$("], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.9": {"text": "Kluge verlangen eine neue Staatengestalt.", "tokens": ["Klu\u00b7ge", "ver\u00b7lan\u00b7gen", "ei\u00b7ne", "neu\u00b7e", "Staa\u00b7ten\u00b7ge\u00b7stalt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+--+", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Dumme beharren bei ihrem kindlichen Eifer \u2013", "tokens": ["Dum\u00b7me", "be\u00b7har\u00b7ren", "bei", "ih\u00b7rem", "kind\u00b7li\u00b7chen", "Ei\u00b7fer", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "+--+--+-+--+-", "measure": "dactylic.di.plus"}, "line.11": {"text": "Habt ihr auch sozialdemokratische Dudelsackpfeifer?", "tokens": ["Habt", "ihr", "auch", "so\u00b7zi\u00b7al\u00b7de\u00b7mo\u00b7kra\u00b7ti\u00b7sche", "Du\u00b7del\u00b7sack\u00b7pfei\u00b7fer", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJA", "NN", "$."], "meter": "-----+--+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Wir haben sie. Prost, lieber Bruder, du!", "tokens": ["Wir", "ha\u00b7ben", "sie", ".", "Prost", ",", "lie\u00b7ber", "Bru\u00b7der", ",", "du", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "$.", "NN", "$,", "ADV", "NN", "$,", "PPER", "$."], "meter": "-+-+++-+-+", "measure": "zehnsilber"}, "line.13": {"text": "Was sagen nur unsre respektiven W\u00e4hler dazu \u2013?", "tokens": ["Was", "sa\u00b7gen", "nur", "uns\u00b7re", "res\u00b7pek\u00b7ti\u00b7ven", "W\u00e4h\u00b7ler", "da\u00b7zu", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "PAV", "$(", "$."], "meter": "-+--+--+--+--+", "measure": "amphibrach.penta.plus"}}, "stanza.8": {"line.1": {"text": "Pfeift das nicht alles auf dem vorletzten Loche:", "tokens": ["Pfeift", "das", "nicht", "al\u00b7les", "auf", "dem", "vor\u00b7letz\u00b7ten", "Lo\u00b7che", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "PTKNEG", "PIS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+--+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Demokraten,", "tokens": ["De\u00b7mo\u00b7kra\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Theater,", "tokens": ["The\u00b7a\u00b7ter", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Offiziere,", "tokens": ["Of\u00b7fi\u00b7zie\u00b7re", ","], "token_info": ["word", "punct"], "pos": ["FM.la", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Richter \u2013", "tokens": ["Rich\u00b7ter", "\u2013"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.6": {"text": "Was sagen sie \u00fcberhaupt zu dieser Epoche \u2013?\u00ab", "tokens": ["Was", "sa\u00b7gen", "sie", "\u00fc\u00b7ber\u00b7haupt", "zu", "die\u00b7ser", "E\u00b7po\u00b7che", "\u2013", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "NN", "$(", "$.", "$("], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Da standen beide auf: der G\u00f6tz und der General Cambronne", "tokens": ["Da", "stan\u00b7den", "bei\u00b7de", "auf", ":", "der", "G\u00f6tz", "und", "der", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PTKVZ", "$.", "ART", "NN", "KON", "ART", "NN", "NE"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "und zogen laut rufend die Konsequenz davon.", "tokens": ["und", "zo\u00b7gen", "laut", "ru\u00b7fend", "die", "Kon\u00b7se\u00b7quenz", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "VVPP", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Jeder sagte seinen Spruch. Die Tassen bebten. Und allen schien,", "tokens": ["Je\u00b7der", "sag\u00b7te", "sei\u00b7nen", "Spruch", ".", "Die", "Tas\u00b7sen", "beb\u00b7ten", ".", "Und", "al\u00b7len", "schien", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "NN", "$.", "ART", "NN", "VVFIN", "$.", "KON", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-+-+--+-+", "measure": "trochaic.octa.plus.relaxed"}, "line.10": {"text": "als werde hier einem Weltenwunsch Ausdruck verliehn . . .", "tokens": ["als", "wer\u00b7de", "hier", "ei\u00b7nem", "Wel\u00b7ten\u00b7wunsch", "Aus\u00b7druck", "ver\u00b7liehn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KOKOM", "VAFIN", "ADV", "ART", "NN", "NN", "VVINF", "$.", "$.", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "\u00bbmerde \u2013!\u00ab sagte Cambronne. Und der andre der beiden Recken:", "tokens": ["\u00bb", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", ".", "Und", "der", "and\u00b7re", "der", "bei\u00b7den", "Re\u00b7cken", ":"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$.", "KON", "ART", "PIS", "ART", "PIAT", "NN", "$."], "meter": "--+-+--+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "\u00bbsag ihnen allen, sie k\u00f6nnten mich und so weiter beklecken!\u00ab", "tokens": ["\u00bb", "sag", "ih\u00b7nen", "al\u00b7len", ",", "sie", "k\u00f6nn\u00b7ten", "mich", "und", "so", "wei\u00b7ter", "be\u00b7kle\u00b7cken", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "PIAT", "$,", "PPER", "VMFIN", "PPER", "KON", "ADV", "ADV", "VVINF", "$.", "$("], "meter": "---+--+--+-+--+-", "measure": "iambic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "An der Wand, ganz heimlich, in guter Ruh,", "tokens": ["An", "der", "Wand", ",", "ganz", "heim\u00b7lich", ",", "in", "gu\u00b7ter", "Ruh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ADV", "ADJD", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "steht Theobald Tiger und gibt seinen Segen dazu.", "tokens": ["steht", "Theo\u00b7bald", "Ti\u00b7ger", "und", "gibt", "sei\u00b7nen", "Se\u00b7gen", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NN", "KON", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "G\u00f6tz von Berlichingen und der General Cambronne", "tokens": ["G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "und", "der", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "NE", "KON", "ART", "NN", "NE"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.2": {"text": "(derselbe, der damals in der Schlacht von", "tokens": ["(", "der\u00b7sel\u00b7be", ",", "der", "da\u00b7mals", "in", "der", "Schlacht", "von"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PDAT", "$,", "PRELS", "ADV", "APPR", "ART", "NN", "APPR"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Waterloo nicht gesagt hat wie im Heldengedicht:", "tokens": ["Wa\u00b7ter\u00b7loo", "nicht", "ge\u00b7sagt", "hat", "wie", "im", "Hel\u00b7den\u00b7ge\u00b7dicht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "VVPP", "VAFIN", "KOKOM", "APPRART", "NN", "$."], "meter": "+-+--+-+-+--+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "\u00bbdie Garde stirbt, doch sie ergibt sich nicht!\u00ab", "tokens": ["\u00bb", "die", "Gar\u00b7de", "stirbt", ",", "doch", "sie", "er\u00b7gibt", "sich", "nicht", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "$,", "KON", "PPER", "VVFIN", "PRF", "PTKNEG", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sondern er sagte nur schlicht:", "tokens": ["Son\u00b7dern", "er", "sag\u00b7te", "nur", "schlicht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.6": {"text": "\u00bbmerde!\u00ab) \u2013", "tokens": ["\u00bb", "mer\u00b7de", "!", "\u00ab", ")", "\u2013"], "token_info": ["punct", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "VVFIN", "$.", "$(", "$(", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "dieser General Cambronne und G\u00f6tz von Berlichingen", "tokens": ["die\u00b7ser", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne", "und", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "NE", "KON", "NN", "APPR", "NN"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.8": {"text": "trafen sich neulich im Caf\u00e9 und t\u00e4ten daselbst singen:", "tokens": ["tra\u00b7fen", "sich", "neu\u00b7lich", "im", "Ca\u00b7f\u00e9", "und", "t\u00e4\u00b7ten", "da\u00b7selbst", "sin\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "APPRART", "NN", "KON", "VVFIN", "PAV", "VVINF", "$."], "meter": "+--+-+-+-+-+-+-", "measure": "iambic.septa.invert"}}, "stanza.11": {"line.1": {"text": "\u00bbwir, die Nationalheiligen zweier Nationen,", "tokens": ["\u00bb", "wir", ",", "die", "Na\u00b7ti\u00b7o\u00b7nal\u00b7hei\u00b7li\u00b7gen", "zwei\u00b7er", "Na\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.2": {"text": "die man uns anruft, wo nur Franzosen und Deutsche wohnen,", "tokens": ["die", "man", "uns", "an\u00b7ruft", ",", "wo", "nur", "Fran\u00b7zo\u00b7sen", "und", "Deut\u00b7sche", "woh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "VVFIN", "$,", "PWAV", "ADV", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+--+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "haben uns hier pro Nase einen Mokka Dubel bestellt", "tokens": ["ha\u00b7ben", "uns", "hier", "pro", "Na\u00b7se", "ei\u00b7nen", "Mok\u00b7ka", "Du\u00b7bel", "be\u00b7stellt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN", "ART", "NE", "NE", "VVFIN"], "meter": "+--+-+-+--+-+-+", "measure": "iambic.septa.invert"}, "line.4": {"text": "und betrachten zur Abwechslung einmal den Lauf der Welt.\u00ab", "tokens": ["und", "be\u00b7trach\u00b7ten", "zur", "Ab\u00b7wechs\u00b7lung", "ein\u00b7mal", "den", "Lauf", "der", "Welt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "ART", "NN", "$.", "$("], "meter": "--+--+--+--+-+", "measure": "anapaest.tetra.plus"}}, "stanza.12": {"line.1": {"text": "Der G\u00f6tz begann:", "tokens": ["Der", "G\u00f6tz", "be\u00b7gann", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "\u00bbwas h\u00e4ltst du, Bruderherz, von den Demokraten,", "tokens": ["\u00bb", "was", "h\u00e4ltst", "du", ",", "Bru\u00b7der\u00b7herz", ",", "von", "den", "De\u00b7mo\u00b7kra\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "$,", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "die noch in jeden Wein ihr Wasser abschlagen taten,", "tokens": ["die", "noch", "in", "je\u00b7den", "Wein", "ihr", "Was\u00b7ser", "ab\u00b7schla\u00b7gen", "ta\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "PIAT", "NN", "PPOSAT", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "vorsichtig,", "tokens": ["vor\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "umsichtig,", "tokens": ["um\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "nachsichtig,", "tokens": ["nach\u00b7sich\u00b7tig", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "kurzsichtig \u2013", "tokens": ["kurz\u00b7sich\u00b7tig", "\u2013"], "token_info": ["word", "punct"], "pos": ["ADJD", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "und liegen immer unten. Was h\u00e4ltst du davon \u2013?\u00ab", "tokens": ["und", "lie\u00b7gen", "im\u00b7mer", "un\u00b7ten", ".", "Was", "h\u00e4ltst", "du", "da\u00b7von", "\u2013", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "$.", "PWS", "VVFIN", "PPER", "PAV", "$(", "$.", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "\u00bbmerde \u2013!\u00ab sagte Cambronne.", "tokens": ["\u00bb", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", "."], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.13": {"line.1": {"text": "\u00bbwas aber h\u00e4ltst du, Bruder, von den preu\u00dfischen Richtern,", "tokens": ["\u00bb", "was", "a\u00b7ber", "h\u00e4ltst", "du", ",", "Bru\u00b7der", ",", "von", "den", "preu\u00b7\u00dfi\u00b7schen", "Rich\u00b7tern", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "ADV", "VVFIN", "PPER", "$,", "NN", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "diesen Vollzugsbeamten von Denkern und Dichtern?", "tokens": ["die\u00b7sen", "Voll\u00b7zugs\u00b7be\u00b7am\u00b7ten", "von", "Den\u00b7kern", "und", "Dich\u00b7tern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wie sie nichts h\u00f6ren und nichts sehn \u2013 aber zuschlagen", "tokens": ["Wie", "sie", "nichts", "h\u00f6\u00b7ren", "und", "nichts", "sehn", "\u2013", "a\u00b7ber", "zu\u00b7schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "KON", "PIS", "VVINF", "$(", "ADV", "VVINF"], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und um sich Jammer verbreiten und Klagen.", "tokens": ["und", "um", "sich", "Jam\u00b7mer", "ver\u00b7brei\u00b7ten", "und", "Kla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PRF", "NN", "VVINF", "KON", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Wie sie die Wehrlosen fangen in ihren Schlingen . . . ?\u00ab", "tokens": ["Wie", "sie", "die", "Wehr\u00b7lo\u00b7sen", "fan\u00b7gen", "in", "ih\u00b7ren", "Schlin\u00b7gen", ".", ".", ".", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$.", "$.", "$.", "$("], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.6": {"text": "\u00bb . . . . . . !\u00ab sagte der G\u00f6tz von Berlichingen.", "tokens": ["\u00bb", ".", ".", ".", ".", ".", ".", "!", "\u00ab", "sag\u00b7te", "der", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$(", "VVFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.14": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbkennst du aber die uniformierten Burschen in allen L\u00e4ndern,", "tokens": ["\u00bb", "kennst", "du", "a\u00b7ber", "die", "u\u00b7nif\u00b7or\u00b7mier\u00b7ten", "Bur\u00b7schen", "in", "al\u00b7len", "L\u00e4n\u00b7dern", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+--+--+-+--+-+-", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "die in ihren bekleckerten Indianergew\u00e4ndern", "tokens": ["die", "in", "ih\u00b7ren", "be\u00b7kle\u00b7cker\u00b7ten", "In\u00b7di\u00b7a\u00b7ner\u00b7ge\u00b7w\u00e4n\u00b7dern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "--+--+--+-+--+-", "measure": "anapaest.tri.plus"}, "line.4": {"text": "den n\u00e4chsten Krieg vorbereiten? Mit dem Anspruch aufs Panth\u00e9on?\u00ab", "tokens": ["den", "n\u00e4chs\u00b7ten", "Krieg", "vor\u00b7be\u00b7rei\u00b7ten", "?", "Mit", "dem", "An\u00b7spruch", "aufs", "Pan\u00b7th\u00e9on", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$.", "APPR", "ART", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+--+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "\u00bbah merde \u2013!\u00ab sagte Cambronne.", "tokens": ["\u00bb", "ah", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", "."], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "FM.la", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.15": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbkennst du aber die Theaterdirektoren?", "tokens": ["\u00bb", "kennst", "du", "a\u00b7ber", "die", "The\u00b7a\u00b7ter\u00b7di\u00b7rek\u00b7to\u00b7ren", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Jedem ist gerade ein neues Genie geboren,", "tokens": ["Je\u00b7dem", "ist", "ge\u00b7ra\u00b7de", "ein", "neu\u00b7es", "Ge\u00b7nie", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+--+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "und besiehst du dir n\u00e4her die g\u00f6ttliche Ware,", "tokens": ["und", "be\u00b7siehst", "du", "dir", "n\u00e4\u00b7her", "die", "g\u00f6tt\u00b7li\u00b7che", "Wa\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADJD", "ART", "ADJA", "NN", "$,"], "meter": "--+--+--+--+-", "measure": "anapaest.tetra.plus"}, "line.5": {"text": "ists ein Genie vom vorigen Jahre.", "tokens": ["ists", "ein", "Ge\u00b7nie", "vom", "vo\u00b7ri\u00b7gen", "Jah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "++-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Haben einen Augenfehler: schielen auf die Kritik", "tokens": ["Ha\u00b7ben", "ei\u00b7nen", "Au\u00b7gen\u00b7feh\u00b7ler", ":", "schie\u00b7len", "auf", "die", "Kri\u00b7tik"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "$.", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+---", "measure": "unknown.measure.hexa"}, "line.7": {"text": "und sitzen in einer Konjunktur-Fabrik.", "tokens": ["und", "sit\u00b7zen", "in", "ei\u00b7ner", "Kon\u00b7junk\u00b7tur\u00b7Fa\u00b7brik", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "W\u00e4r gar nicht \u00fcbel. Nur:", "tokens": ["W\u00e4r", "gar", "nicht", "\u00fc\u00b7bel", ".", "Nur", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADJD", "$.", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "es ist immer die falsche Konjunktur.", "tokens": ["es", "ist", "im\u00b7mer", "die", "fal\u00b7sche", "Kon\u00b7junk\u00b7tur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Wirr. Unzuverl\u00e4ssig. Ja, was k\u00f6nnen sie denn vor allen Dingen \u2013?\u00ab", "tokens": ["Wirr", ".", "Un\u00b7zu\u00b7ver\u00b7l\u00e4s\u00b7sig", ".", "Ja", ",", "was", "k\u00f6n\u00b7nen", "sie", "denn", "vor", "al\u00b7len", "Din\u00b7gen", "\u2013", "?", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "$.", "ADJD", "$.", "PTKANT", "$,", "PWS", "VMFIN", "PPER", "ADV", "APPR", "PIAT", "NN", "$(", "$.", "$("], "meter": "-+--+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.11": {"text": "Da sagte es der G\u00f6tz von Berlichingen.", "tokens": ["Da", "sag\u00b7te", "es", "der", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.16": {"line.1": {"text": "Und fuhr fort:", "tokens": ["Und", "fuhr", "fort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "\u00bbwas h\u00e4ltst du aber hingegen von den Parlamenten?", "tokens": ["\u00bb", "was", "h\u00e4ltst", "du", "a\u00b7ber", "hin\u00b7ge\u00b7gen", "von", "den", "Par\u00b7la\u00b7men\u00b7ten", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Mit ihren Kommissionssitzungen und ihren Re- und Korreferenten?", "tokens": ["Mit", "ih\u00b7ren", "Kom\u00b7mis\u00b7si\u00b7ons\u00b7sit\u00b7zun\u00b7gen", "und", "ih\u00b7ren", "Re", "und", "Kor\u00b7re\u00b7fe\u00b7ren\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "PPOSAT", "TRUNC", "KON", "NN", "$."], "meter": "-+--+-+-+--+-+-+--+-", "measure": "iambic.octa.plus.relaxed"}, "line.4": {"text": "Bruder, sag mir, ist es bei euch das gleiche", "tokens": ["Bru\u00b7der", ",", "sag", "mir", ",", "ist", "es", "bei", "euch", "das", "glei\u00b7che"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "VVFIN", "PPER", "$,", "VAFIN", "PPER", "APPR", "PPER", "ART", "ADJA"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "wie in unserm republikanischen Kaiserreiche?", "tokens": ["wie", "in", "un\u00b7serm", "re\u00b7pub\u00b7li\u00b7ka\u00b7ni\u00b7schen", "Kai\u00b7ser\u00b7rei\u00b7che", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+--+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.6": {"text": "Das Ganze nennt man Demokratie \u2013", "tokens": ["Das", "Gan\u00b7ze", "nennt", "man", "De\u00b7mo\u00b7kra\u00b7tie", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "ART", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "ist aber nur eine politische Schwerindustrie.", "tokens": ["ist", "a\u00b7ber", "nur", "ei\u00b7ne", "po\u00b7li\u00b7ti\u00b7sche", "Schwe\u00b7rin\u00b7dust\u00b7rie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+---+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Gut vor hundert Jahren. Heute: so alt, so alt \u2013", "tokens": ["Gut", "vor", "hun\u00b7dert", "Jah\u00b7ren", ".", "Heu\u00b7te", ":", "so", "alt", ",", "so", "alt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "APPR", "CARD", "NN", "$.", "ADV", "$.", "ADV", "ADJD", "$,", "ADV", "ADJD", "$("], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.9": {"text": "Kluge verlangen eine neue Staatengestalt.", "tokens": ["Klu\u00b7ge", "ver\u00b7lan\u00b7gen", "ei\u00b7ne", "neu\u00b7e", "Staa\u00b7ten\u00b7ge\u00b7stalt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+--+", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Dumme beharren bei ihrem kindlichen Eifer \u2013", "tokens": ["Dum\u00b7me", "be\u00b7har\u00b7ren", "bei", "ih\u00b7rem", "kind\u00b7li\u00b7chen", "Ei\u00b7fer", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "+--+--+-+--+-", "measure": "dactylic.di.plus"}, "line.11": {"text": "Habt ihr auch sozialdemokratische Dudelsackpfeifer?", "tokens": ["Habt", "ihr", "auch", "so\u00b7zi\u00b7al\u00b7de\u00b7mo\u00b7kra\u00b7ti\u00b7sche", "Du\u00b7del\u00b7sack\u00b7pfei\u00b7fer", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJA", "NN", "$."], "meter": "-----+--+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Wir haben sie. Prost, lieber Bruder, du!", "tokens": ["Wir", "ha\u00b7ben", "sie", ".", "Prost", ",", "lie\u00b7ber", "Bru\u00b7der", ",", "du", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "$.", "NN", "$,", "ADV", "NN", "$,", "PPER", "$."], "meter": "-+-+++-+-+", "measure": "zehnsilber"}, "line.13": {"text": "Was sagen nur unsre respektiven W\u00e4hler dazu \u2013?", "tokens": ["Was", "sa\u00b7gen", "nur", "uns\u00b7re", "res\u00b7pek\u00b7ti\u00b7ven", "W\u00e4h\u00b7ler", "da\u00b7zu", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "PAV", "$(", "$."], "meter": "-+--+--+--+--+", "measure": "amphibrach.penta.plus"}}, "stanza.17": {"line.1": {"text": "Pfeift das nicht alles auf dem vorletzten Loche:", "tokens": ["Pfeift", "das", "nicht", "al\u00b7les", "auf", "dem", "vor\u00b7letz\u00b7ten", "Lo\u00b7che", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "PTKNEG", "PIS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+--+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Demokraten,", "tokens": ["De\u00b7mo\u00b7kra\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Theater,", "tokens": ["The\u00b7a\u00b7ter", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Offiziere,", "tokens": ["Of\u00b7fi\u00b7zie\u00b7re", ","], "token_info": ["word", "punct"], "pos": ["FM.la", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Richter \u2013", "tokens": ["Rich\u00b7ter", "\u2013"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.6": {"text": "Was sagen sie \u00fcberhaupt zu dieser Epoche \u2013?\u00ab", "tokens": ["Was", "sa\u00b7gen", "sie", "\u00fc\u00b7ber\u00b7haupt", "zu", "die\u00b7ser", "E\u00b7po\u00b7che", "\u2013", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "NN", "$(", "$.", "$("], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Da standen beide auf: der G\u00f6tz und der General Cambronne", "tokens": ["Da", "stan\u00b7den", "bei\u00b7de", "auf", ":", "der", "G\u00f6tz", "und", "der", "Ge\u00b7ne\u00b7ral", "Cam\u00b7bron\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PTKVZ", "$.", "ART", "NN", "KON", "ART", "NN", "NE"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "und zogen laut rufend die Konsequenz davon.", "tokens": ["und", "zo\u00b7gen", "laut", "ru\u00b7fend", "die", "Kon\u00b7se\u00b7quenz", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "VVPP", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Jeder sagte seinen Spruch. Die Tassen bebten. Und allen schien,", "tokens": ["Je\u00b7der", "sag\u00b7te", "sei\u00b7nen", "Spruch", ".", "Die", "Tas\u00b7sen", "beb\u00b7ten", ".", "Und", "al\u00b7len", "schien", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "NN", "$.", "ART", "NN", "VVFIN", "$.", "KON", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-+-+--+-+", "measure": "trochaic.octa.plus.relaxed"}, "line.10": {"text": "als werde hier einem Weltenwunsch Ausdruck verliehn . . .", "tokens": ["als", "wer\u00b7de", "hier", "ei\u00b7nem", "Wel\u00b7ten\u00b7wunsch", "Aus\u00b7druck", "ver\u00b7liehn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KOKOM", "VAFIN", "ADV", "ART", "NN", "NN", "VVINF", "$.", "$.", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "\u00bbmerde \u2013!\u00ab sagte Cambronne. Und der andre der beiden Recken:", "tokens": ["\u00bb", "mer\u00b7de", "\u2013", "!", "\u00ab", "sag\u00b7te", "Cam\u00b7bron\u00b7ne", ".", "Und", "der", "and\u00b7re", "der", "bei\u00b7den", "Re\u00b7cken", ":"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$(", "$.", "$(", "VVFIN", "NE", "$.", "KON", "ART", "PIS", "ART", "PIAT", "NN", "$."], "meter": "--+-+--+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "\u00bbsag ihnen allen, sie k\u00f6nnten mich und so weiter beklecken!\u00ab", "tokens": ["\u00bb", "sag", "ih\u00b7nen", "al\u00b7len", ",", "sie", "k\u00f6nn\u00b7ten", "mich", "und", "so", "wei\u00b7ter", "be\u00b7kle\u00b7cken", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "PIAT", "$,", "PPER", "VMFIN", "PPER", "KON", "ADV", "ADV", "VVINF", "$.", "$("], "meter": "---+--+--+-+--+-", "measure": "iambic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "An der Wand, ganz heimlich, in guter Ruh,", "tokens": ["An", "der", "Wand", ",", "ganz", "heim\u00b7lich", ",", "in", "gu\u00b7ter", "Ruh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ADV", "ADJD", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "steht Theobald Tiger und gibt seinen Segen dazu.", "tokens": ["steht", "Theo\u00b7bald", "Ti\u00b7ger", "und", "gibt", "sei\u00b7nen", "Se\u00b7gen", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NN", "KON", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}}}}}