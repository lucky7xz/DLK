{"textgrid.poem.38993": {"metadata": {"author": {"name": "Tieck, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "1L: Zu dir wall' ich, alte Stadt,", "genre": "verse", "period": "N.A.", "pub_year": 1813, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Zu dir wall' ich, alte Stadt,", "tokens": ["Zu", "dir", "wall'", "ich", ",", "al\u00b7te", "Stadt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Um den alten Goldschmidt,", "tokens": ["Um", "den", "al\u00b7ten", "Gold\u00b7schmidt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "ADJA", "NN", "$,"], "meter": "--+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Den theuren Freund,", "tokens": ["Den", "theu\u00b7ren", "Freund", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "N\u00e4her und n\u00e4her zu kennen.", "tokens": ["N\u00e4\u00b7her", "und", "n\u00e4\u00b7her", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "PTKZU", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Welch k\u00fchnes Wollen", "tokens": ["Welch", "k\u00fch\u00b7nes", "Wol\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "ADJA", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Verk\u00fcnden uns hier die Bilder Francia's!", "tokens": ["Ver\u00b7k\u00fcn\u00b7den", "uns", "hier", "die", "Bil\u00b7der", "Fran\u00b7cia's", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "NN", "NE", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Edler Greis,", "tokens": ["Ed\u00b7ler", "Greis", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Der du so sehns\u00fcchtig", "tokens": ["Der", "du", "so", "sehn\u00b7s\u00fcch\u00b7tig"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "PPER", "ADV", "ADJD"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Ein Werk des verwandten", "tokens": ["Ein", "Werk", "des", "ver\u00b7wand\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Gr\u00f6\u00dfern Rafaels erharrtest.", "tokens": ["Gr\u00f6\u00b7\u00dfern", "Ra\u00b7faels", "er\u00b7harr\u00b7test", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.3": {"line.1": {"text": "Wer darf die Kunst ausmessen", "tokens": ["Wer", "darf", "die", "Kunst", "aus\u00b7mes\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und ihre Grenzen ziehn?", "tokens": ["Und", "ih\u00b7re", "Gren\u00b7zen", "ziehn", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wer kann die Ewigkeit beschr\u00e4nken? \u2013", "tokens": ["Wer", "kann", "die", "E\u00b7wig\u00b7keit", "be\u00b7schr\u00e4n\u00b7ken", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nur wer die kleine Gegenwart", "tokens": ["Nur", "wer", "die", "klei\u00b7ne", "Ge\u00b7gen\u00b7wart"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PWS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Als den Mittelpunkt alles Daseyns erkennt.", "tokens": ["Als", "den", "Mit\u00b7tel\u00b7punkt", "al\u00b7les", "Da\u00b7seyns", "er\u00b7kennt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PIAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.4": {"line.1": {"text": "Zu dir wall' ich, alte Stadt,", "tokens": ["Zu", "dir", "wall'", "ich", ",", "al\u00b7te", "Stadt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Um den alten Goldschmidt,", "tokens": ["Um", "den", "al\u00b7ten", "Gold\u00b7schmidt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "ADJA", "NN", "$,"], "meter": "--+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Den theuren Freund,", "tokens": ["Den", "theu\u00b7ren", "Freund", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "N\u00e4her und n\u00e4her zu kennen.", "tokens": ["N\u00e4\u00b7her", "und", "n\u00e4\u00b7her", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "PTKZU", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.5": {"line.1": {"text": "Welch k\u00fchnes Wollen", "tokens": ["Welch", "k\u00fch\u00b7nes", "Wol\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "ADJA", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Verk\u00fcnden uns hier die Bilder Francia's!", "tokens": ["Ver\u00b7k\u00fcn\u00b7den", "uns", "hier", "die", "Bil\u00b7der", "Fran\u00b7cia's", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "NN", "NE", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Edler Greis,", "tokens": ["Ed\u00b7ler", "Greis", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Der du so sehns\u00fcchtig", "tokens": ["Der", "du", "so", "sehn\u00b7s\u00fcch\u00b7tig"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "PPER", "ADV", "ADJD"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Ein Werk des verwandten", "tokens": ["Ein", "Werk", "des", "ver\u00b7wand\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Gr\u00f6\u00dfern Rafaels erharrtest.", "tokens": ["Gr\u00f6\u00b7\u00dfern", "Ra\u00b7faels", "er\u00b7harr\u00b7test", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.6": {"line.1": {"text": "Wer darf die Kunst ausmessen", "tokens": ["Wer", "darf", "die", "Kunst", "aus\u00b7mes\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und ihre Grenzen ziehn?", "tokens": ["Und", "ih\u00b7re", "Gren\u00b7zen", "ziehn", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wer kann die Ewigkeit beschr\u00e4nken? \u2013", "tokens": ["Wer", "kann", "die", "E\u00b7wig\u00b7keit", "be\u00b7schr\u00e4n\u00b7ken", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nur wer die kleine Gegenwart", "tokens": ["Nur", "wer", "die", "klei\u00b7ne", "Ge\u00b7gen\u00b7wart"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PWS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Als den Mittelpunkt alles Daseyns erkennt.", "tokens": ["Als", "den", "Mit\u00b7tel\u00b7punkt", "al\u00b7les", "Da\u00b7seyns", "er\u00b7kennt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PIAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}}}}