{"dta.poem.4257": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Eine neue Betrachtung \u00fcber  \n Schmetterlinge.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Ich sahe neulich, mit Vergn\u00fcgen,", "tokens": ["Ich", "sa\u00b7he", "neu\u00b7lich", ",", "mit", "Ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verschiedne Schmetterlinge fliegen;", "tokens": ["Ver\u00b7schied\u00b7ne", "Schmet\u00b7ter\u00b7lin\u00b7ge", "flie\u00b7gen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich dachte dieses Thiers verworfnem Ursprung nach,", "tokens": ["Ich", "dach\u00b7te", "die\u00b7ses", "Thiers", "ver\u00b7worf\u00b7nem", "Ur\u00b7sprung", "nach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wor\u00fcber ich denn zu mir selber sprach:", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "ich", "denn", "zu", "mir", "sel\u00b7ber", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "APPR", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Vom sch\u00f6nen Schmetterling l\u00e4\u00dft, nach dem Augenschein,", "tokens": ["Vom", "sch\u00f6\u00b7nen", "Schmet\u00b7ter\u00b7ling", "l\u00e4\u00dft", ",", "nach", "dem", "Au\u00b7gen\u00b7schein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die Raupe nur der Embrion zu seyn.", "tokens": ["Die", "Rau\u00b7pe", "nur", "der", "Emb\u00b7ri\u00b7on", "zu", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ART", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Mich deucht, da\u00df ich darinn entdecke,", "tokens": ["Mich", "deucht", ",", "da\u00df", "ich", "da\u00b7rinn", "ent\u00b7de\u00b7cke", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wie ein Geheimni\u00df- volles Bild,", "tokens": ["Wie", "ein", "Ge\u00b7heim\u00b7ni\u00df", "vol\u00b7les", "Bild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "TRUNC", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Mit reichem Trost f\u00fcr uns erf\u00fcllt,", "tokens": ["Mit", "rei\u00b7chem", "Trost", "f\u00fcr", "uns", "er\u00b7f\u00fcllt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "In ihrer Aenderung verborgnen Ordnung, stecke.", "tokens": ["In", "ih\u00b7rer", "A\u00b7en\u00b7de\u00b7rung", "ver\u00b7borg\u00b7nen", "Ord\u00b7nung", ",", "ste\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJA", "NN", "$,", "VVFIN", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.2": {"line.1": {"text": "So lange hier die Raup\u2019 im ersten Zustand ist,", "tokens": ["So", "lan\u00b7ge", "hier", "die", "Raup'", "im", "ers\u00b7ten", "Zu\u00b7stand", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ART", "NN", "APPRART", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Seh\u2019n wir, wie sie sich n\u00e4hrt und fri\u00dft.", "tokens": ["Seh'n", "wir", ",", "wie", "sie", "sich", "n\u00e4hrt", "und", "fri\u00dft", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "PPER", "PRF", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So bald sie den vollkommnen Grad", "tokens": ["So", "bald", "sie", "den", "voll\u00b7komm\u00b7nen", "Grad"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von ihrem Wesen nun erhalten hat;", "tokens": ["Von", "ih\u00b7rem", "We\u00b7sen", "nun", "er\u00b7hal\u00b7ten", "hat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Gebraucht sie keiner Nahrung mehr.", "tokens": ["Ge\u00b7braucht", "sie", "kei\u00b7ner", "Nah\u00b7rung", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PIAT", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Man siehet sie zuletzt von allem Mangel leer,", "tokens": ["Man", "sie\u00b7het", "sie", "zu\u00b7letzt", "von", "al\u00b7lem", "Man\u00b7gel", "leer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "APPR", "PIS", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "In unver\u00e4ndertem Vergn\u00fcgen,", "tokens": ["In", "un\u00b7ver\u00b7\u00e4n\u00b7der\u00b7tem", "Ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Auf Lieb\u2019 allein bedacht, in heitern L\u00fcften fliegen.", "tokens": ["Auf", "Lieb'", "al\u00b7lein", "be\u00b7dacht", ",", "in", "hei\u00b7tern", "L\u00fcf\u00b7ten", "flie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVPP", "$,", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Vielleicht ist uns in ihm ein Vorbild vorgestellt,", "tokens": ["Viel\u00b7leicht", "ist", "uns", "in", "ihm", "ein", "Vor\u00b7bild", "vor\u00b7ge\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wir sind vielleicht, so lang\u2019 wir auf der Welt,", "tokens": ["Wir", "sind", "viel\u00b7leicht", ",", "so", "lang'", "wir", "auf", "der", "Welt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$,", "ADV", "ADV", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ein blosser Embrion allein", "tokens": ["Ein", "blos\u00b7ser", "Emb\u00b7ri\u00b7on", "al\u00b7lein"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von dem, was wir dereinst zu seyn,", "tokens": ["Von", "dem", ",", "was", "wir", "de\u00b7reinst", "zu", "seyn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PPER", "ADV", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Uns Hoffnung machen k\u00f6nnen,", "tokens": ["Uns", "Hoff\u00b7nung", "ma\u00b7chen", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und was wir Seligkeit und Himmel nennen.", "tokens": ["Und", "was", "wir", "Se\u00b7lig\u00b7keit", "und", "Him\u00b7mel", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "So lange wir allhier auf Erden,", "tokens": ["So", "lan\u00b7ge", "wir", "all\u00b7hier", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bringt, weil man vieles n\u00f6htig hat,", "tokens": ["Bringt", ",", "weil", "man", "vie\u00b7les", "n\u00f6h\u00b7tig", "hat", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PIS", "PIS", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Uns mancher Mangel fr\u00fch und spat", "tokens": ["Uns", "man\u00b7cher", "Man\u00b7gel", "fr\u00fch", "und", "spat"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "PIAT", "NN", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verschiedne Sorgen und Beschwehrden.", "tokens": ["Ver\u00b7schied\u00b7ne", "Sor\u00b7gen", "und", "Be\u00b7schwehr\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Sonst sind wir \u00fcberall auf uns allein bedacht,", "tokens": ["Sonst", "sind", "wir", "\u00fc\u00b7be\u00b7rall", "auf", "uns", "al\u00b7lein", "be\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wir wollen sonst, da\u00df alles, was gemacht,", "tokens": ["Wir", "wol\u00b7len", "sonst", ",", "da\u00df", "al\u00b7les", ",", "was", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "$,", "KOUS", "PIS", "$,", "PRELS", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Sey blo\u00df f\u00fcr uns hervorgebracht;", "tokens": ["Sey", "blo\u00df", "f\u00fcr", "uns", "her\u00b7vor\u00b7ge\u00b7bracht", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vom allerkleinsten bis zum gr\u00f6\u00dften", "tokens": ["Vom", "al\u00b7ler\u00b7kleins\u00b7ten", "bis", "zum", "gr\u00f6\u00df\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "APPR", "APPRART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Sey alles blo\u00df f\u00fcr uns, und uns zum Besten.", "tokens": ["Sey", "al\u00b7les", "blo\u00df", "f\u00fcr", "uns", ",", "und", "uns", "zum", "Bes\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "PPER", "$,", "KON", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Warum denn eignen wir", "tokens": ["Wa\u00b7rum", "denn", "eig\u00b7nen", "wir"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ADJA", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Uns die Verwandlungen allhier", "tokens": ["Uns", "die", "Ver\u00b7wand\u00b7lun\u00b7gen", "all\u00b7hier"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Nicht zu, und stellen sie uns, als ein Lehr-Bild, f\u00fcr,", "tokens": ["Nicht", "zu", ",", "und", "stel\u00b7len", "sie", "uns", ",", "als", "ein", "Lehr\u00b7Bild", ",", "f\u00fcr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "PTKVZ", "$,", "KON", "VVFIN", "PPER", "PPER", "$,", "KOUS", "ART", "NN", "$,", "APPR", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So uns vermuhtlich auf der Welt", "tokens": ["So", "uns", "ver\u00b7muht\u00b7lich", "auf", "der", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Zu einem Gleichni\u00df vorgestellt,", "tokens": ["Zu", "ei\u00b7nem", "Gleich\u00b7ni\u00df", "vor\u00b7ge\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Und einer Art Beweis, wie es der Gottheit doch", "tokens": ["Und", "ei\u00b7ner", "Art", "Be\u00b7weis", ",", "wie", "es", "der", "Got\u00b7theit", "doch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NN", "$,", "PWAV", "PPER", "ART", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "So m\u00f6glich und so leicht, auch unsern Stand zu bessern,", "tokens": ["So", "m\u00f6g\u00b7lich", "und", "so", "leicht", ",", "auch", "un\u00b7sern", "Stand", "zu", "bes\u00b7sern", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADV", "ADJD", "$,", "ADV", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und unsers Wesens Stoff, auch wenn wir gleich erkalten,", "tokens": ["Und", "un\u00b7sers", "We\u00b7sens", "Stoff", ",", "auch", "wenn", "wir", "gleich", "er\u00b7kal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "$,", "ADV", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Nicht nur die Seele zu erhalten,", "tokens": ["Nicht", "nur", "die", "See\u00b7le", "zu", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Auch selbst der C\u00f6rper Schmuck noch zu vergr\u00f6ssern.", "tokens": ["Auch", "selbst", "der", "C\u00f6r\u00b7per", "Schmuck", "noch", "zu", "ver\u00b7gr\u00f6s\u00b7sern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Es wird hiedurch zugleich dir ja viel leichter scheinen,", "tokens": ["Es", "wird", "hie\u00b7durch", "zu\u00b7gleich", "dir", "ja", "viel", "leich\u00b7ter", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PAV", "ADV", "PPER", "ADV", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Die Theile zu vereinen,", "tokens": ["Die", "Thei\u00b7le", "zu", "ver\u00b7ei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.18": {"text": "Als, aus ganz anderm Stoff, ganz andre Sachen,", "tokens": ["Als", ",", "aus", "ganz", "an\u00b7derm", "Stoff", ",", "ganz", "and\u00b7re", "Sa\u00b7chen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "APPR", "ADV", "ADJA", "NN", "$,", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "An Farben und Figur,", "tokens": ["An", "Far\u00b7ben", "und", "Fi\u00b7gur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.20": {"text": "An Absicht und Natur,", "tokens": ["An", "Ab\u00b7sicht", "und", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.21": {"text": "Hervor zu bringen und zu machen.", "tokens": ["Her\u00b7vor", "zu", "brin\u00b7gen", "und", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}