{"textgrid.poem.42764": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Ringkampf", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Gibson (sehr nervig), Australien,", "tokens": ["Gib\u00b7son", "(", "sehr", "ner\u00b7vig", ")", ",", "Aust\u00b7ra\u00b7li\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "punct", "punct", "word", "punct"], "pos": ["NE", "$(", "ADV", "ADJD", "$(", "$,", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Schulze, Berlin (ziemlich gro\u00df).", "tokens": ["Schul\u00b7ze", ",", "Ber\u00b7lin", "(", "ziem\u00b7lich", "gro\u00df", ")", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$,", "NE", "$(", "ADV", "ADJD", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Bei\u00dfen und Genitalien", "tokens": ["Bei\u00b7\u00dfen", "und", "Ge\u00b7ni\u00b7ta\u00b7li\u00b7en"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+--+----", "measure": "dactylic.di.plus"}, "line.4": {"text": "Kratzen verboten. \u2013 Nun los!", "tokens": ["Krat\u00b7zen", "ver\u00b7bo\u00b7ten", ".", "\u2013", "Nun", "los", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$.", "$(", "ADV", "PTKVZ", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Ob sie wohl seelisch sehr leiden?", "tokens": ["Ob", "sie", "wohl", "see\u00b7lisch", "sehr", "lei\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "ADV", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Gibson ist bla\u00df und auch Schulz.", "tokens": ["Gib\u00b7son", "ist", "bla\u00df", "und", "auch", "Schulz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "KON", "ADV", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Warum f\u00fchlen die beiden", "tokens": ["Wa\u00b7rum", "f\u00fch\u00b7len", "die", "bei\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "PIAT"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Wechselnd einander den Puls?", "tokens": ["Wech\u00b7selnd", "ein\u00b7an\u00b7der", "den", "Puls", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "PRF", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.3": {"line.1": {"text": "\u00c4ngstlich hustet jetzt Gibson.", "tokens": ["\u00c4ngst\u00b7lich", "hus\u00b7tet", "jetzt", "Gib\u00b7son", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Darauf schluckt Schulze Cachou.", "tokens": ["Da\u00b7rauf", "schluckt", "Schul\u00b7ze", "Cac\u00b7hou", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "NE", "$."], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.3": {"text": "Gibson will Schulzen jetzt stipsen.", "tokens": ["Gib\u00b7son", "will", "Schul\u00b7zen", "jetzt", "stip\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "NN", "ADV", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ha! Nun greifen sie zu.", "tokens": ["Ha", "!", "Nun", "grei\u00b7fen", "sie", "zu", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.4": {"line.1": {"text": "Packen sich an, auf, hinter, neben, in,", "tokens": ["Pa\u00b7cken", "sich", "an", ",", "auf", ",", "hin\u00b7ter", ",", "ne\u00b7ben", ",", "in", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKVZ", "$,", "PTKVZ", "$,", "APPR", "$,", "APPR", "$,", "APPR", "$,"], "meter": "+--+-+-+--", "measure": "iambic.tetra.invert"}, "line.2": {"text": "\u00dcber, unter, vor und zwischen,", "tokens": ["\u00dc\u00b7ber", ",", "un\u00b7ter", ",", "vor", "und", "zwi\u00b7schen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "$,", "PTKVZ", "KON", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Statt, auch l\u00e4ngs, zufolge, trotz", "tokens": ["Statt", ",", "auch", "l\u00e4ngs", ",", "zu\u00b7fol\u00b7ge", ",", "trotz"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "ADV", "ADV", "$,", "VVFIN", "$,", "XY"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Stehen auf die Frage wessen.", "tokens": ["Ste\u00b7hen", "auf", "die", "Fra\u00b7ge", "wes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Doch ist hier nicht zu vergessen,", "tokens": ["Doch", "ist", "hier", "nicht", "zu", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da\u00df bei diesen letzten drei", "tokens": ["Da\u00df", "bei", "die\u00b7sen", "letz\u00b7ten", "drei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "PDAT", "ADJA", "CARD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch der Dativ richtig sei.", "tokens": ["Auch", "der", "Da\u00b7tiv", "rich\u00b7tig", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.6": {"line.1": {"text": "(pfeife des Schiedsrichters.)", "tokens": ["(", "pfei\u00b7fe", "des", "Schieds\u00b7rich\u00b7ters", ".", ")"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "ART", "NN", "$.", "$("], "meter": "+---+-", "measure": "dactylic.init"}}, "stanza.7": {"line.1": {"text": "Wo sind die Beine von Schulze?", "tokens": ["Wo", "sind", "die", "Bei\u00b7ne", "von", "Schul\u00b7ze", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wem geh\u00f6rt denn das Knie?", "tokens": ["Wem", "ge\u00b7h\u00f6rt", "denn", "das", "Knie", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Wirr wie lebendige Sulze,", "tokens": ["Wirr", "wie", "le\u00b7ben\u00b7di\u00b7ge", "Sul\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Mengt sich die Anatomie.", "tokens": ["Mengt", "sich", "die", "A\u00b7nat\u00b7o\u00b7mie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$."], "meter": "+---+-+", "measure": "dactylic.init"}}, "stanza.8": {"line.1": {"text": "Ist das ein Kopf aus Australien?", "tokens": ["Ist", "das", "ein", "Kopf", "aus", "Aust\u00b7ra\u00b7li\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Oder Ges\u00e4\u00df aus Berlin?", "tokens": ["O\u00b7der", "Ge\u00b7s\u00e4\u00df", "aus", "Ber\u00b7lin", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Jeder versucht Repressalien,", "tokens": ["Je\u00b7der", "ver\u00b7sucht", "Re\u00b7pres\u00b7sa\u00b7li\u00b7en", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NE", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Jeder l\u00e4\u00dft keinen entfliehn.", "tokens": ["Je\u00b7der", "l\u00e4\u00dft", "kei\u00b7nen", "ent\u00b7fliehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIAT", "VVINF", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.9": {"line.1": {"text": "Hat sich der Schiedsmann bemeistert,", "tokens": ["Hat", "sich", "der", "Schieds\u00b7mann", "be\u00b7meis\u00b7tert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lange parteilos zu sein;", "tokens": ["Lan\u00b7ge", "par\u00b7tei\u00b7los", "zu", "sein", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber nun br\u00fcllt er begeistert:", "tokens": ["A\u00b7ber", "nun", "br\u00fcllt", "er", "be\u00b7geis\u00b7tert", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "VVPP", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "\u00bbschulze, stell ihm ein Bein!", "tokens": ["\u00bb", "schul\u00b7ze", ",", "stell", "ihm", "ein", "Bein", "!"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "ADJD", "PPER", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Zwinge den Mann mit den Nerven", "tokens": ["Zwin\u00b7ge", "den", "Mann", "mit", "den", "Ner\u00b7ven"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "APPR", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Nieder nach Sitte und Jus.", "tokens": ["Nie\u00b7der", "nach", "Sit\u00b7te", "und", "Jus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NE", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Kannst du dich \u00fcber ihn werfen", "tokens": ["Kannst", "du", "dich", "\u00fc\u00b7ber", "ihn", "wer\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "PRF", "APPR", "PPER", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Just wie im Koi, dann tu's!\u00ab", "tokens": ["Just", "wie", "im", "Koi", ",", "dann", "tu's", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "KOKOM", "APPRART", "NN", "$,", "ADV", "NE", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Gibson (sehr nervig), Australien,", "tokens": ["Gib\u00b7son", "(", "sehr", "ner\u00b7vig", ")", ",", "Aust\u00b7ra\u00b7li\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "punct", "punct", "word", "punct"], "pos": ["NE", "$(", "ADV", "ADJD", "$(", "$,", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Schulze, Berlin (ziemlich gro\u00df).", "tokens": ["Schul\u00b7ze", ",", "Ber\u00b7lin", "(", "ziem\u00b7lich", "gro\u00df", ")", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$,", "NE", "$(", "ADV", "ADJD", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Bei\u00dfen und Genitalien", "tokens": ["Bei\u00b7\u00dfen", "und", "Ge\u00b7ni\u00b7ta\u00b7li\u00b7en"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+--+----", "measure": "dactylic.di.plus"}, "line.4": {"text": "Kratzen verboten. \u2013 Nun los!", "tokens": ["Krat\u00b7zen", "ver\u00b7bo\u00b7ten", ".", "\u2013", "Nun", "los", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$.", "$(", "ADV", "PTKVZ", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.12": {"line.1": {"text": "Ob sie wohl seelisch sehr leiden?", "tokens": ["Ob", "sie", "wohl", "see\u00b7lisch", "sehr", "lei\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "ADV", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Gibson ist bla\u00df und auch Schulz.", "tokens": ["Gib\u00b7son", "ist", "bla\u00df", "und", "auch", "Schulz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "KON", "ADV", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Warum f\u00fchlen die beiden", "tokens": ["Wa\u00b7rum", "f\u00fch\u00b7len", "die", "bei\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "PIAT"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Wechselnd einander den Puls?", "tokens": ["Wech\u00b7selnd", "ein\u00b7an\u00b7der", "den", "Puls", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "PRF", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.13": {"line.1": {"text": "\u00c4ngstlich hustet jetzt Gibson.", "tokens": ["\u00c4ngst\u00b7lich", "hus\u00b7tet", "jetzt", "Gib\u00b7son", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Darauf schluckt Schulze Cachou.", "tokens": ["Da\u00b7rauf", "schluckt", "Schul\u00b7ze", "Cac\u00b7hou", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "NE", "$."], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.3": {"text": "Gibson will Schulzen jetzt stipsen.", "tokens": ["Gib\u00b7son", "will", "Schul\u00b7zen", "jetzt", "stip\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "NN", "ADV", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ha! Nun greifen sie zu.", "tokens": ["Ha", "!", "Nun", "grei\u00b7fen", "sie", "zu", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.14": {"line.1": {"text": "Packen sich an, auf, hinter, neben, in,", "tokens": ["Pa\u00b7cken", "sich", "an", ",", "auf", ",", "hin\u00b7ter", ",", "ne\u00b7ben", ",", "in", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKVZ", "$,", "PTKVZ", "$,", "APPR", "$,", "APPR", "$,", "APPR", "$,"], "meter": "+--+-+-+--", "measure": "iambic.tetra.invert"}, "line.2": {"text": "\u00dcber, unter, vor und zwischen,", "tokens": ["\u00dc\u00b7ber", ",", "un\u00b7ter", ",", "vor", "und", "zwi\u00b7schen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "$,", "PTKVZ", "KON", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Statt, auch l\u00e4ngs, zufolge, trotz", "tokens": ["Statt", ",", "auch", "l\u00e4ngs", ",", "zu\u00b7fol\u00b7ge", ",", "trotz"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "ADV", "ADV", "$,", "VVFIN", "$,", "XY"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Stehen auf die Frage wessen.", "tokens": ["Ste\u00b7hen", "auf", "die", "Fra\u00b7ge", "wes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Doch ist hier nicht zu vergessen,", "tokens": ["Doch", "ist", "hier", "nicht", "zu", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da\u00df bei diesen letzten drei", "tokens": ["Da\u00df", "bei", "die\u00b7sen", "letz\u00b7ten", "drei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "PDAT", "ADJA", "CARD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch der Dativ richtig sei.", "tokens": ["Auch", "der", "Da\u00b7tiv", "rich\u00b7tig", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.16": {"line.1": {"text": "(pfeife des Schiedsrichters.)", "tokens": ["(", "pfei\u00b7fe", "des", "Schieds\u00b7rich\u00b7ters", ".", ")"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "ART", "NN", "$.", "$("], "meter": "+---+-", "measure": "dactylic.init"}}, "stanza.17": {"line.1": {"text": "Wo sind die Beine von Schulze?", "tokens": ["Wo", "sind", "die", "Bei\u00b7ne", "von", "Schul\u00b7ze", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wem geh\u00f6rt denn das Knie?", "tokens": ["Wem", "ge\u00b7h\u00f6rt", "denn", "das", "Knie", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Wirr wie lebendige Sulze,", "tokens": ["Wirr", "wie", "le\u00b7ben\u00b7di\u00b7ge", "Sul\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Mengt sich die Anatomie.", "tokens": ["Mengt", "sich", "die", "A\u00b7nat\u00b7o\u00b7mie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$."], "meter": "+---+-+", "measure": "dactylic.init"}}, "stanza.18": {"line.1": {"text": "Ist das ein Kopf aus Australien?", "tokens": ["Ist", "das", "ein", "Kopf", "aus", "Aust\u00b7ra\u00b7li\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Oder Ges\u00e4\u00df aus Berlin?", "tokens": ["O\u00b7der", "Ge\u00b7s\u00e4\u00df", "aus", "Ber\u00b7lin", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Jeder versucht Repressalien,", "tokens": ["Je\u00b7der", "ver\u00b7sucht", "Re\u00b7pres\u00b7sa\u00b7li\u00b7en", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NE", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Jeder l\u00e4\u00dft keinen entfliehn.", "tokens": ["Je\u00b7der", "l\u00e4\u00dft", "kei\u00b7nen", "ent\u00b7fliehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIAT", "VVINF", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.19": {"line.1": {"text": "Hat sich der Schiedsmann bemeistert,", "tokens": ["Hat", "sich", "der", "Schieds\u00b7mann", "be\u00b7meis\u00b7tert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lange parteilos zu sein;", "tokens": ["Lan\u00b7ge", "par\u00b7tei\u00b7los", "zu", "sein", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber nun br\u00fcllt er begeistert:", "tokens": ["A\u00b7ber", "nun", "br\u00fcllt", "er", "be\u00b7geis\u00b7tert", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "VVPP", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "\u00bbschulze, stell ihm ein Bein!", "tokens": ["\u00bb", "schul\u00b7ze", ",", "stell", "ihm", "ein", "Bein", "!"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "ADJD", "PPER", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.20": {"line.1": {"text": "Zwinge den Mann mit den Nerven", "tokens": ["Zwin\u00b7ge", "den", "Mann", "mit", "den", "Ner\u00b7ven"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "APPR", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Nieder nach Sitte und Jus.", "tokens": ["Nie\u00b7der", "nach", "Sit\u00b7te", "und", "Jus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NE", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Kannst du dich \u00fcber ihn werfen", "tokens": ["Kannst", "du", "dich", "\u00fc\u00b7ber", "ihn", "wer\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "PRF", "APPR", "PPER", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Just wie im Koi, dann tu's!\u00ab", "tokens": ["Just", "wie", "im", "Koi", ",", "dann", "tu's", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "KOKOM", "APPRART", "NN", "$,", "ADV", "NE", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}