{"dta.poem.11495": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "Abbildung etlicher thorheiten, so sonderlich  \n auf universit\u00e4ten pa\u00dfiren.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1709", "urn": "urn:nbn:de:kobv:b4-20283-5", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Beh nur! geh, verbuhlte dirne!", "tokens": ["Beh", "nur", "!", "geh", ",", "ver\u00b7buhl\u00b7te", "dir\u00b7ne", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$.", "VVFIN", "$,", "VVFIN", "PDS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ich begehre keinen ku\u00df:", "tokens": ["Ich", "be\u00b7geh\u00b7re", "kei\u00b7nen", "ku\u00df", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob an deiner geilen stirne", "tokens": ["Ob", "an", "dei\u00b7ner", "gei\u00b7len", "stir\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Gleich manch schiff zerscheitern mu\u00df.", "tokens": ["Gleich", "manch", "schiff", "zer\u00b7schei\u00b7tern", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "ADJD", "VVINF", "VMFIN", "$."], "meter": "++--+-+", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Die sich dir zum selaven machen,", "tokens": ["Die", "sich", "dir", "zum", "se\u00b7la\u00b7ven", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "PPER", "APPRART", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Sind wahrhafftig auszulachen.", "tokens": ["Sind", "wahr\u00b7haff\u00b7tig", "aus\u00b7zu\u00b7la\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Heisst das nicht rechtschaffen rasen,", "tokens": ["Heisst", "das", "nicht", "recht\u00b7schaf\u00b7fen", "ra\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "PTKNEG", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn ein kerl sich so verliebt?", "tokens": ["Wenn", "ein", "kerl", "sich", "so", "ver\u00b7liebt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PRF", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ungeacht man seiner nasen", "tokens": ["Un\u00b7ge\u00b7acht", "man", "sei\u00b7ner", "na\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PIS", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Einen bock zu riechen giebt;", "tokens": ["Ei\u00b7nen", "bock", "zu", "rie\u00b7chen", "giebt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wenn wir auf so faule pf\u00fctzen", "tokens": ["Wenn", "wir", "auf", "so", "fau\u00b7le", "pf\u00fct\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Unser ", "tokens": ["Un\u00b7ser"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.3": {"line.1": {"text": "Doch, es l\u00e4\u00dft bey nah noch kahler,", "tokens": ["Doch", ",", "es", "l\u00e4\u00dft", "bey", "nah", "noch", "kah\u00b7ler", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PPER", "VVFIN", "APPR", "ADJD", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn der thor die schritte mi\u00dft:", "tokens": ["Wenn", "der", "thor", "die", "schrit\u00b7te", "mi\u00dft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn der aufgeblasne prahler", "tokens": ["Wenn", "der", "auf\u00b7ge\u00b7blas\u00b7ne", "prah\u00b7ler"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hundert kerl\u2019 und degen fri\u00dft;", "tokens": ["Hun\u00b7dert", "kerl'", "und", "de\u00b7gen", "fri\u00dft", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "PDS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Da sich doch vor seinen fl\u00fcchen", "tokens": ["Da", "sich", "doch", "vor", "sei\u00b7nen", "fl\u00fc\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "ADV", "APPR", "PPOSAT", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Selbst die m\u00e4use nicht verkriechen.", "tokens": ["Selbst", "die", "m\u00e4u\u00b7se", "nicht", "ver\u00b7krie\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Unterweilen gro\u00dfzusprechen,", "tokens": ["Un\u00b7ter\u00b7wei\u00b7len", "gro\u00df\u00b7zu\u00b7spre\u00b7chen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist gar eine schlechte kunst:", "tokens": ["Ist", "gar", "ei\u00b7ne", "schlech\u00b7te", "kunst", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber mit dem h\u00e4lse-brechen,", "tokens": ["A\u00b7ber", "mit", "dem", "h\u00e4lse\u00b7bre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Ist es manchmahl lauter dunst;", "tokens": ["Ist", "es", "manch\u00b7mahl", "lau\u00b7ter", "dunst", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn vor einem guten pr\u00fcgel", "tokens": ["Denn", "vor", "ei\u00b7nem", "gu\u00b7ten", "pr\u00fc\u00b7gel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Hengt herr Rodomont die fl\u00fcgel.", "tokens": ["Hengt", "herr", "Ro\u00b7do\u00b7mont", "die", "fl\u00fc\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Doch ist das nicht auch ein jecke,", "tokens": ["Doch", "ist", "das", "nicht", "auch", "ein", "je\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PDS", "PTKNEG", "ADV", "ART", "NN", "$,"], "meter": "-+----+-", "measure": "dactylic.init"}, "line.2": {"text": "Der um einen pfenning bei\u00dft?", "tokens": ["Der", "um", "ei\u00b7nen", "pfen\u00b7ning", "bei\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Der mit einem ball voll d \u2026", "tokens": ["Der", "mit", "ei\u00b7nem", "ball", "voll", "d", "\u2026"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADJD", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Dorten nach dem hunde schmei\u00dft?", "tokens": ["Dor\u00b7ten", "nach", "dem", "hun\u00b7de", "schmei\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Der den beutel flei\u00dfig spicket,", "tokens": ["Der", "den", "beu\u00b7tel", "flei\u00b7\u00dfig", "spi\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Und ihm selbst die schuhe flicket?", "tokens": ["Und", "ihm", "selbst", "die", "schu\u00b7he", "fli\u00b7cket", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "ART", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Aber sagt, ist der viel kl\u00fcger,", "tokens": ["A\u00b7ber", "sagt", ",", "ist", "der", "viel", "kl\u00fc\u00b7ger", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "VAFIN", "ART", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der den teufel ", "tokens": ["Der", "den", "teu\u00b7fel"], "token_info": ["word", "word", "word"], "pos": ["ART", "ART", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Weil ihn irgend ein betr\u00fcger", "tokens": ["Weil", "ihn", "ir\u00b7gend", "ein", "be\u00b7tr\u00fc\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Um den beutel was vexirt;", "tokens": ["Um", "den", "beu\u00b7tel", "was", "ve\u00b7xirt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "NN", "PWS", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Und die gelder zum verprassen,", "tokens": ["Und", "die", "gel\u00b7der", "zum", "ver\u00b7pras\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "APPRART", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.6": {"text": "Nicht will l\u00e4nger bey ihm lassen?", "tokens": ["Nicht", "will", "l\u00e4n\u00b7ger", "bey", "ihm", "las\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "ADJD", "APPR", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Hat er sich dazu besoffen,", "tokens": ["Hat", "er", "sich", "da\u00b7zu", "be\u00b7sof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "PAV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn geht erst der bettel an.", "tokens": ["Denn", "geht", "erst", "der", "bet\u00b7tel", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Alles, was ihn itzt betroffen,", "tokens": ["Al\u00b7les", ",", "was", "ihn", "itzt", "be\u00b7trof\u00b7fen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "PRELS", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat der arme stein gethan;", "tokens": ["Hat", "der", "ar\u00b7me", "stein", "ge\u00b7than", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Darum mu\u00df er, sich zu r\u00e4chen,", "tokens": ["Da\u00b7rum", "mu\u00df", "er", ",", "sich", "zu", "r\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "$,", "PRF", "PTKZU", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.6": {"text": "Seine kling an ihm zerbrechen.", "tokens": ["Sei\u00b7ne", "kling", "an", "ihm", "zer\u00b7bre\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "VVFIN", "APPR", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Manchmahl kommt zu grossem gl\u00fccke", "tokens": ["Manch\u00b7mahl", "kommt", "zu", "gros\u00b7sem", "gl\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Noch die h\u00e4scher-schaar darzu,", "tokens": ["Noch", "die", "h\u00e4\u00b7scher\u00b7schaar", "dar\u00b7zu", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und bringt die bego\u00dfne ficke", "tokens": ["Und", "bringt", "die", "be\u00b7go\u00df\u00b7ne", "fi\u00b7cke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "In ein finstres loch zur ruh;", "tokens": ["In", "ein", "finst\u00b7res", "loch", "zur", "ruh", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Weil die steine vor den hieben", "tokens": ["Weil", "die", "stei\u00b7ne", "vor", "den", "hie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "APPR", "ART", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sonst nicht auf der gaffe blieben.", "tokens": ["Sonst", "nicht", "auf", "der", "gaf\u00b7fe", "blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Die zu \u2026 einsprechen,", "tokens": ["Die", "zu", "\u2026", "ein\u00b7spre\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "PTKZU", "$(", "VVIZU", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Denen solte zwar mein kiel", "tokens": ["De\u00b7nen", "sol\u00b7te", "zwar", "mein", "kiel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "ADV", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch allhier den jecken stechen:", "tokens": ["Auch", "all\u00b7hier", "den", "je\u00b7cken", "ste\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Doch es w\u00e4re gar zu viel;", "tokens": ["Doch", "es", "w\u00e4\u00b7re", "gar", "zu", "viel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADV", "PTKA", "PIS", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Weil ohn dem die tr\u00fcben zeiten", "tokens": ["Weil", "ohn", "dem", "die", "tr\u00fc\u00b7ben", "zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sie zuletzt anheim begleiten.", "tokens": ["Sie", "zu\u00b7letzt", "an\u00b7heim", "be\u00b7glei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Aber die versoffnen schweine,", "tokens": ["A\u00b7ber", "die", "ver\u00b7soff\u00b7nen", "schwei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die mir stets den kopff voll schreyn:", "tokens": ["Die", "mir", "stets", "den", "kopff", "voll", "schreyn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ART", "NN", "ADJD", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn sie von vier-pfennig-weine", "tokens": ["Wenn", "sie", "von", "vier\u00b7pfen\u00b7nig\u00b7wei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NE"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Voll courag\u2019 und feuer seyn;", "tokens": ["Voll", "cou\u00b7rag'", "und", "feu\u00b7er", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "KON", "ADJD", "VAINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "M\u00fcssen auf ihr schreyn und speyen", "tokens": ["M\u00fcs\u00b7sen", "auf", "ihr", "schreyn", "und", "spe\u00b7yen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PPOSAT", "NN", "KON", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Eines auf die schnautze kreyen.", "tokens": ["Ei\u00b7nes", "auf", "die", "schnaut\u00b7ze", "kre\u00b7yen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "Leute, die so offte g\u00f6cken,", "tokens": ["Leu\u00b7te", ",", "die", "so", "off\u00b7te", "g\u00f6\u00b7cken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ADV", "ADJA", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kriegen einen leeren kopff:", "tokens": ["Krie\u00b7gen", "ei\u00b7nen", "lee\u00b7ren", "kopff", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und ertappt man sie im bl\u00f6cken,", "tokens": ["Und", "er\u00b7tappt", "man", "sie", "im", "bl\u00f6\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PPER", "APPRART", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "O! so steht der arme tropff", "tokens": ["O", "!", "so", "steht", "der", "ar\u00b7me", "tropff"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Bey dem allgemeinen spotte,", "tokens": ["Bey", "dem", "all\u00b7ge\u00b7mei\u00b7nen", "spot\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Wie ein s\u00fcnder von der sprotte.", "tokens": ["Wie", "ein", "s\u00fcn\u00b7der", "von", "der", "sprot\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "APPR", "ART", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Recht! spricht ein gelehrter knauser,", "tokens": ["Recht", "!", "spricht", "ein", "ge\u00b7lehr\u00b7ter", "knau\u00b7ser", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die haluncken sind es werth;", "tokens": ["Die", "ha\u00b7lun\u00b7cken", "sind", "es", "werth", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch geduld, du grkllen-lauser!", "tokens": ["Doch", "ge\u00b7duld", ",", "du", "grkllen\u00b7lau\u00b7ser", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPER", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Dir ist auch noch was beschert,", "tokens": ["Dir", "ist", "auch", "noch", "was", "be\u00b7schert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIS", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Die gelehrten b\u00e4ncklein-s\u00e4nger", "tokens": ["Die", "ge\u00b7lehr\u00b7ten", "b\u00e4n\u00b7ck\u00b7lein\u00b7s\u00e4n\u00b7ger"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Sind die \u00e4rgsten m\u00fc\u00dfig-g\u00e4nger.", "tokens": ["Sind", "die", "\u00e4rgs\u00b7ten", "m\u00fc\u00b7\u00dfig\u00b7g\u00e4n\u00b7ger", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Sich mit tausend grillen schlagen,", "tokens": ["Sich", "mit", "tau\u00b7send", "gril\u00b7len", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "CARD", "ADJA", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist kein ehren-werther fleis,", "tokens": ["Ist", "kein", "eh\u00b7ren\u00b7wer\u00b7ther", "fleis", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn dein kopff gleich alle fragen", "tokens": ["Wenn", "dein", "kopff", "gleich", "al\u00b7le", "fra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PIS", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Aus dem Scot\u2019 und Thomas wei\u00df,", "tokens": ["Aus", "dem", "Scot'", "und", "Tho\u00b7mas", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "NE", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ja das gras selbst wachsen h\u00f6ret;", "tokens": ["Ja", "das", "gras", "selbst", "wach\u00b7sen", "h\u00f6\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ART", "NN", "ADV", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Hei\u00dft es doch: Du bist beth\u00f6ret.", "tokens": ["Hei\u00dft", "es", "doch", ":", "Du", "bist", "be\u00b7th\u00f6\u00b7ret", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$.", "PPER", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Doch ich irre, weil ich h\u00f6re,", "tokens": ["Doch", "ich", "ir\u00b7re", ",", "weil", "ich", "h\u00f6\u00b7re", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df der herr \u2026 ist,", "tokens": ["Da\u00df", "der", "herr", "\u2026", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$(", "VAFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Und des Stagiriten lehre", "tokens": ["Und", "des", "Sta\u00b7gi\u00b7ri\u00b7ten", "leh\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mit dem gr\u00f6sten beyfall liest.", "tokens": ["Mit", "dem", "gr\u00f6s\u00b7ten", "bey\u00b7fall", "liest", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn ich rede von studenten,", "tokens": ["Denn", "ich", "re\u00b7de", "von", "stu\u00b7den\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Und nicht von so grossen ", "tokens": ["Und", "nicht", "von", "so", "gros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PTKNEG", "APPR", "ADV", "ADJA"], "meter": "+-+-+-", "measure": "trochaic.tri"}}}}}