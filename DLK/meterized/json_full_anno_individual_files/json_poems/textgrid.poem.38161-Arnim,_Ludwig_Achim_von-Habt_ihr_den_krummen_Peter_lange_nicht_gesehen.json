{"textgrid.poem.38161": {"metadata": {"author": {"name": "Arnim, Ludwig Achim von", "birth": "N.A.", "death": "N.A."}, "title": "Habt ihr den krummen Peter lange nicht gesehen", "genre": "verse", "period": "N.A.", "pub_year": 1806, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hab ich dann schon rothe Haar, rothe Haar,", "tokens": ["Hab", "ich", "dann", "schon", "ro\u00b7the", "Haar", ",", "ro\u00b7the", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Leid ich d'rum noch kein Gefahr.", "tokens": ["Leid", "ich", "d'\u00b7rum", "noch", "kein", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Rothe Haar die Leut nicht sch\u00e4nden,", "tokens": ["Ro\u00b7the", "Haar", "die", "Leut", "nicht", "sch\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "'s ist, da\u00df mich die Leute kennen,", "tokens": ["'s", "ist", ",", "da\u00df", "mich", "die", "Leu\u00b7te", "ken\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Hab ich dann schon rothe Haar, rothe Haar,", "tokens": ["Hab", "ich", "dann", "schon", "ro\u00b7the", "Haar", ",", "ro\u00b7the", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Leid ich d'rum noch kein Gefahr.", "tokens": ["Leid", "ich", "d'\u00b7rum", "noch", "kein", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Hab ich schon ein schieles Aug, schieles Aug,", "tokens": ["Hab", "ich", "schon", "ein", "schie\u00b7les", "Aug", ",", "schie\u00b7les", "Aug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Krieg ich doch ein sch\u00f6ne Frau.", "tokens": ["Krieg", "ich", "doch", "ein", "sch\u00f6\u00b7ne", "Frau", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Mancher hat zwey sch\u00f6ne Augen,", "tokens": ["Man\u00b7cher", "hat", "zwey", "sch\u00f6\u00b7ne", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "CARD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mu\u00df doch durch die Brille schauen,", "tokens": ["Mu\u00df", "doch", "durch", "die", "Bril\u00b7le", "schau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Wann ich schon ein wenig schiel, wenig schiel,", "tokens": ["Wann", "ich", "schon", "ein", "we\u00b7nig", "schiel", ",", "we\u00b7nig", "schiel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "ART", "PIS", "VVFIN", "$,", "PIS", "VVFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Brauche ich doch keine Brill.", "tokens": ["Brau\u00b7che", "ich", "doch", "kei\u00b7ne", "Brill", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Hab ich schon ein stumpfe Nas, stumpfe Nas,", "tokens": ["Hab", "ich", "schon", "ein", "stump\u00b7fe", "Nas", ",", "stump\u00b7fe", "Nas", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Bin ich doch ein schlauer Haas.", "tokens": ["Bin", "ich", "doch", "ein", "schlau\u00b7er", "Haas", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kann doch sch\u00f6n die Teller lecken,", "tokens": ["Kann", "doch", "sch\u00f6n", "die", "Tel\u00b7ler", "le\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bleibt mir keiner am N\u00e4schen stecken,", "tokens": ["Bleibt", "mir", "kei\u00b7ner", "am", "N\u00e4\u00b7schen", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPRART", "NN", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Hab ich schon ein stumpfe Nas, stumpfe Nas,", "tokens": ["Hab", "ich", "schon", "ein", "stump\u00b7fe", "Nas", ",", "stump\u00b7fe", "Nas", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Bin ich doch ein schlauer Haas.", "tokens": ["Bin", "ich", "doch", "ein", "schlau\u00b7er", "Haas", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Hab ich schon ein krummen Fu\u00df, krummen Fu\u00df,", "tokens": ["Hab", "ich", "schon", "ein", "krum\u00b7men", "Fu\u00df", ",", "krum\u00b7men", "Fu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "--+-+-++-+", "measure": "anapaest.init"}, "line.2": {"text": "Wei\u00df ich, da\u00df ich h\u00fcpfen mu\u00df,", "tokens": ["Wei\u00df", "ich", ",", "da\u00df", "ich", "h\u00fcp\u00b7fen", "mu\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Mancher hat fein grade Glieder,", "tokens": ["Man\u00b7cher", "hat", "fein", "gra\u00b7de", "Glie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hinkt und h\u00fcpft doch hin und wieder,", "tokens": ["Hinkt", "und", "h\u00fcpft", "doch", "hin", "und", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "ADV", "PTKVZ", "KON", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hab ich einen krummen Fu\u00df, krummen Fu\u00df,", "tokens": ["Hab", "ich", "ei\u00b7nen", "krum\u00b7men", "Fu\u00df", ",", "krum\u00b7men", "Fu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.6": {"text": "Wei\u00df ich, da\u00df ich h\u00fcpfen mu\u00df.", "tokens": ["Wei\u00df", "ich", ",", "da\u00df", "ich", "h\u00fcp\u00b7fen", "mu\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Leb ich schon inkognito, inkognito,", "tokens": ["Leb", "ich", "schon", "in\u00b7kog\u00b7ni\u00b7to", ",", "in\u00b7kog\u00b7ni\u00b7to", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "$,", "ADV", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Scher ich mich auch nichts darum,", "tokens": ["Scher", "ich", "mich", "auch", "nichts", "da\u00b7rum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PRF", "ADV", "PIS", "PAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gut gelebt und seelig gestorben,", "tokens": ["Gut", "ge\u00b7lebt", "und", "see\u00b7lig", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "KON", "ADJD", "VVPP", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Ist dem Teufel die Rechnung verdorben,", "tokens": ["Ist", "dem", "Teu\u00b7fel", "die", "Rech\u00b7nung", "ver\u00b7dor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Leb ich schon inkognito, inkognito,", "tokens": ["Leb", "ich", "schon", "in\u00b7kog\u00b7ni\u00b7to", ",", "in\u00b7kog\u00b7ni\u00b7to", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "$,", "ADV", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.6": {"text": "Scher ich mich auch nichts darum.", "tokens": ["Scher", "ich", "mich", "auch", "nichts", "da\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PRF", "ADV", "PIS", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Hab ich dann schon rothe Haar, rothe Haar,", "tokens": ["Hab", "ich", "dann", "schon", "ro\u00b7the", "Haar", ",", "ro\u00b7the", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Leid ich d'rum noch kein Gefahr.", "tokens": ["Leid", "ich", "d'\u00b7rum", "noch", "kein", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Rothe Haar die Leut nicht sch\u00e4nden,", "tokens": ["Ro\u00b7the", "Haar", "die", "Leut", "nicht", "sch\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "'s ist, da\u00df mich die Leute kennen,", "tokens": ["'s", "ist", ",", "da\u00df", "mich", "die", "Leu\u00b7te", "ken\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Hab ich dann schon rothe Haar, rothe Haar,", "tokens": ["Hab", "ich", "dann", "schon", "ro\u00b7the", "Haar", ",", "ro\u00b7the", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Leid ich d'rum noch kein Gefahr.", "tokens": ["Leid", "ich", "d'\u00b7rum", "noch", "kein", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Hab ich schon ein schieles Aug, schieles Aug,", "tokens": ["Hab", "ich", "schon", "ein", "schie\u00b7les", "Aug", ",", "schie\u00b7les", "Aug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Krieg ich doch ein sch\u00f6ne Frau.", "tokens": ["Krieg", "ich", "doch", "ein", "sch\u00f6\u00b7ne", "Frau", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Mancher hat zwey sch\u00f6ne Augen,", "tokens": ["Man\u00b7cher", "hat", "zwey", "sch\u00f6\u00b7ne", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "CARD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mu\u00df doch durch die Brille schauen,", "tokens": ["Mu\u00df", "doch", "durch", "die", "Bril\u00b7le", "schau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Wann ich schon ein wenig schiel, wenig schiel,", "tokens": ["Wann", "ich", "schon", "ein", "we\u00b7nig", "schiel", ",", "we\u00b7nig", "schiel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "ART", "PIS", "VVFIN", "$,", "PIS", "VVFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Brauche ich doch keine Brill.", "tokens": ["Brau\u00b7che", "ich", "doch", "kei\u00b7ne", "Brill", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Hab ich schon ein stumpfe Nas, stumpfe Nas,", "tokens": ["Hab", "ich", "schon", "ein", "stump\u00b7fe", "Nas", ",", "stump\u00b7fe", "Nas", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Bin ich doch ein schlauer Haas.", "tokens": ["Bin", "ich", "doch", "ein", "schlau\u00b7er", "Haas", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kann doch sch\u00f6n die Teller lecken,", "tokens": ["Kann", "doch", "sch\u00f6n", "die", "Tel\u00b7ler", "le\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bleibt mir keiner am N\u00e4schen stecken,", "tokens": ["Bleibt", "mir", "kei\u00b7ner", "am", "N\u00e4\u00b7schen", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPRART", "NN", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Hab ich schon ein stumpfe Nas, stumpfe Nas,", "tokens": ["Hab", "ich", "schon", "ein", "stump\u00b7fe", "Nas", ",", "stump\u00b7fe", "Nas", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Bin ich doch ein schlauer Haas.", "tokens": ["Bin", "ich", "doch", "ein", "schlau\u00b7er", "Haas", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Hab ich schon ein krummen Fu\u00df, krummen Fu\u00df,", "tokens": ["Hab", "ich", "schon", "ein", "krum\u00b7men", "Fu\u00df", ",", "krum\u00b7men", "Fu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "--+-+-++-+", "measure": "anapaest.init"}, "line.2": {"text": "Wei\u00df ich, da\u00df ich h\u00fcpfen mu\u00df,", "tokens": ["Wei\u00df", "ich", ",", "da\u00df", "ich", "h\u00fcp\u00b7fen", "mu\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Mancher hat fein grade Glieder,", "tokens": ["Man\u00b7cher", "hat", "fein", "gra\u00b7de", "Glie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hinkt und h\u00fcpft doch hin und wieder,", "tokens": ["Hinkt", "und", "h\u00fcpft", "doch", "hin", "und", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "ADV", "PTKVZ", "KON", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hab ich einen krummen Fu\u00df, krummen Fu\u00df,", "tokens": ["Hab", "ich", "ei\u00b7nen", "krum\u00b7men", "Fu\u00df", ",", "krum\u00b7men", "Fu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.6": {"text": "Wei\u00df ich, da\u00df ich h\u00fcpfen mu\u00df.", "tokens": ["Wei\u00df", "ich", ",", "da\u00df", "ich", "h\u00fcp\u00b7fen", "mu\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Leb ich schon inkognito, inkognito,", "tokens": ["Leb", "ich", "schon", "in\u00b7kog\u00b7ni\u00b7to", ",", "in\u00b7kog\u00b7ni\u00b7to", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "$,", "ADV", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Scher ich mich auch nichts darum,", "tokens": ["Scher", "ich", "mich", "auch", "nichts", "da\u00b7rum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PRF", "ADV", "PIS", "PAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gut gelebt und seelig gestorben,", "tokens": ["Gut", "ge\u00b7lebt", "und", "see\u00b7lig", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "KON", "ADJD", "VVPP", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Ist dem Teufel die Rechnung verdorben,", "tokens": ["Ist", "dem", "Teu\u00b7fel", "die", "Rech\u00b7nung", "ver\u00b7dor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Leb ich schon inkognito, inkognito,", "tokens": ["Leb", "ich", "schon", "in\u00b7kog\u00b7ni\u00b7to", ",", "in\u00b7kog\u00b7ni\u00b7to", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "$,", "ADV", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.6": {"text": "Scher ich mich auch nichts darum.", "tokens": ["Scher", "ich", "mich", "auch", "nichts", "da\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PRF", "ADV", "PIS", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}