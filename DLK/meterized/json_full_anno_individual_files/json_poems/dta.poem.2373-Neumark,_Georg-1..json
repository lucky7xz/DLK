{"dta.poem.2373": {"metadata": {"author": {"name": "Neumark, Georg", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1652", "urn": "urn:nbn:de:kobv:b4-20428-0", "language": ["de:0.99"], "booktitle": "Neumark, Georg: Poetisch- und Musikalisches Lustw\u00e4ldchen. Hamburg, 1652."}, "poem": {"stanza.1": {"line.1": {"text": "Mein/ bin ich denn dar\u00fcm zu schm\u00e4hen/", "tokens": ["Mein", "/", "bin", "ich", "denn", "da\u00b7r\u00fcm", "zu", "schm\u00e4\u00b7hen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "$(", "VAFIN", "PPER", "ADV", "PAV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df mein verliebter F\u00fcrstensinn/", "tokens": ["Da\u00df", "mein", "ver\u00b7lieb\u00b7ter", "F\u00fcrs\u00b7ten\u00b7sinn", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jhm\u2019 eine schlechte Sch\u00e4ferinn/", "tokens": ["Jhm'", "ei\u00b7ne", "schlech\u00b7te", "Sch\u00e4\u00b7fe\u00b7rinn", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zur Allerliebsten ausersehen?", "tokens": ["Zur", "Al\u00b7ler\u00b7liebs\u00b7ten", "au\u00b7ser\u00b7se\u00b7hen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Soll denn hiedurch mein hoher Schein/", "tokens": ["Soll", "denn", "hie\u00b7durch", "mein", "ho\u00b7her", "Schein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PAV", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Verdunkelt und verloschen sein.", "tokens": ["Ver\u00b7dun\u00b7kelt", "und", "ver\u00b7lo\u00b7schen", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ach nein/ es ist ja weit gefehlet?", "tokens": ["Ach", "nein", "/", "es", "ist", "ja", "weit", "ge\u00b7feh\u00b7let", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKANT", "$(", "PPER", "VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein tapfrer Sinn und Heldengeist/", "tokens": ["Ein", "tapf\u00b7rer", "Sinn", "und", "Hel\u00b7den\u00b7geist", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein edles Hertz in dem erweist/", "tokens": ["Sein", "ed\u00b7les", "Hertz", "in", "dem", "er\u00b7weist", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "APPR", "ART", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df Er sich nicht \u00fcm Hoheit qu\u00e4let/", "tokens": ["Da\u00df", "Er", "sich", "nicht", "\u00fcm", "Ho\u00b7heit", "qu\u00e4\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Da offt die Falschheit/ Trug und List/", "tokens": ["Da", "offt", "die", "Falschheit", "/", "Trug", "und", "List", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "$(", "NN", "KON", "NN", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Verzukkert und verg\u00f6ldet ist.", "tokens": ["Ver\u00b7zuk\u00b7kert", "und", "ver\u00b7g\u00f6l\u00b7det", "ist", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein Tugendliebendes Gem\u00fchte/", "tokens": ["Ein", "Tu\u00b7gend\u00b7lie\u00b7ben\u00b7des", "Ge\u00b7m\u00fch\u00b7te", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sucht ihm ein Seelchen da\u00df ihm gleicht/", "tokens": ["Sucht", "ihm", "ein", "Seel\u00b7chen", "da\u00df", "ihm", "gleicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "KOUS", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das nicht aus seinen Schranken weicht/", "tokens": ["Das", "nicht", "aus", "sei\u00b7nen", "Schran\u00b7ken", "weicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PTKNEG", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ob schon es nicht aus dem Gebl\u00fcte/", "tokens": ["Ob", "schon", "es", "nicht", "aus", "dem", "Ge\u00b7bl\u00fc\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Welchs mit schein\u00e4dlen Titteln prangt", "tokens": ["Welchs", "mit", "schei\u00b7n\u00e4d\u00b7len", "Tit\u00b7teln", "prangt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "APPR", "ADJA", "NN", "VVFIN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.6": {"text": "Und grosses Ansehn hat erlangt.", "tokens": ["Und", "gros\u00b7ses", "An\u00b7sehn", "hat", "er\u00b7langt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ein hoher Sinn ist wol vergn\u00fcget/", "tokens": ["Ein", "ho\u00b7her", "Sinn", "ist", "wol", "ver\u00b7gn\u00fc\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jmfall ihm solch ein liebes Kind", "tokens": ["Jm\u00b7fall", "ihm", "solch", "ein", "lie\u00b7bes", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "PPER", "PIAT", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da Sch\u00f6nheit sich bey Tugend findt/", "tokens": ["Da", "Sch\u00f6n\u00b7heit", "sich", "bey", "Tu\u00b7gend", "findt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PRF", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird von dem Himmel beygef\u00fcget/", "tokens": ["Wird", "von", "dem", "Him\u00b7mel", "bey\u00b7ge\u00b7f\u00fc\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Erlanget die\u00df ein \u00e4dler Muht/", "tokens": ["Er\u00b7lan\u00b7get", "die\u00df", "ein", "\u00e4d\u00b7ler", "Muht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So fragt Er nichts nach Geld und Gut.", "tokens": ["So", "fragt", "Er", "nichts", "nach", "Geld", "und", "Gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Er siehet nicht nach gro\u00dfem Stande/", "tokens": ["Er", "sie\u00b7het", "nicht", "nach", "gro\u00b7\u00dfem", "Stan\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie ungemenschte Menschen nun", "tokens": ["Wie", "un\u00b7ge\u00b7menschte", "Men\u00b7schen", "nun"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJA", "NN", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bey diesen b\u00f6sen Zeiten thun/", "tokens": ["Bey", "die\u00b7sen", "b\u00f6\u00b7sen", "Zei\u00b7ten", "thun", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er achtet es vor keine Schande", "tokens": ["Er", "ach\u00b7tet", "es", "vor", "kei\u00b7ne", "Schan\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wenn Er ein schlechtes M\u00e4gdchen liebt/", "tokens": ["Wenn", "Er", "ein", "schlech\u00b7tes", "M\u00e4gd\u00b7chen", "liebt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die ihm ihr keusches Hertze giebt.", "tokens": ["Die", "ihm", "ihr", "keu\u00b7sches", "Hert\u00b7ze", "giebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PPOSAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ist Er nur sonst von gutem Adel/", "tokens": ["Ist", "Er", "nur", "sonst", "von", "gu\u00b7tem", "A\u00b7del", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich meine nicht den Stamm allein.", "tokens": ["Ich", "mei\u00b7ne", "nicht", "den", "Stamm", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Sinnen m\u00fcssen Edel sein/", "tokens": ["Die", "Sin\u00b7nen", "m\u00fcs\u00b7sen", "E\u00b7del", "sein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "NN", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und das Gem\u00fchte sonder Tadel/", "tokens": ["Und", "das", "Ge\u00b7m\u00fch\u00b7te", "son\u00b7der", "Ta\u00b7del", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Es bleibt sein Adel unbeflekkt", "tokens": ["Es", "bleibt", "sein", "A\u00b7del", "un\u00b7be\u00b7flekkt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und durch die Lieb\u2019 unangestekkt.", "tokens": ["Und", "durch", "die", "Lieb'", "un\u00b7an\u00b7ges\u00b7tekkt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Dar\u00fcm hinweg ihr junge Frauen/", "tokens": ["Da\u00b7r\u00fcm", "hin\u00b7weg", "ihr", "jun\u00b7ge", "Frau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "APZR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit ", "tokens": ["Mit"], "token_info": ["word"], "pos": ["APPR"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Und ihr geschminkte Jungfern ihr/", "tokens": ["Und", "ihr", "ge\u00b7schmink\u00b7te", "Jung\u00b7fern", "ihr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "PPOSAT", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hinweg ihr aufgeblasne Pfauen/", "tokens": ["Hin\u00b7weg", "ihr", "auf\u00b7ge\u00b7blas\u00b7ne", "Pfau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich frage nichts nach eurem Pracht/", "tokens": ["Ich", "fra\u00b7ge", "nichts", "nach", "eu\u00b7rem", "Pracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der keuschen Sinnen Ekel macht.", "tokens": ["Der", "keu\u00b7schen", "Sin\u00b7nen", "E\u00b7kel", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Mein allersch\u00f6neste Bozene", "tokens": ["Mein", "al\u00b7ler\u00b7sch\u00f6\u00b7nes\u00b7te", "Bo\u00b7ze\u00b7ne"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Soll meine Hertzgeliebte sein", "tokens": ["Soll", "mei\u00b7ne", "Hertz\u00b7ge\u00b7lieb\u00b7te", "sein"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "PPOSAT", "NN", "PPOSAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jhr angebohrner Sch\u00f6nheits schein", "tokens": ["Ihr", "an\u00b7ge\u00b7bohr\u00b7ner", "Sch\u00f6n\u00b7heits", "schein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Macht Sie allein beliebt und sch\u00f6ne/", "tokens": ["Macht", "Sie", "al\u00b7lein", "be\u00b7liebt", "und", "sch\u00f6\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADJD", "KON", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Das auserw\u00e4hlte Tugendlicht/", "tokens": ["Das", "au\u00b7ser\u00b7w\u00e4hl\u00b7te", "Tu\u00b7gend\u00b7licht", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wei\u00df ja von keinem Hochmuht nicht.", "tokens": ["Wei\u00df", "ja", "von", "kei\u00b7nem", "Hoch\u00b7muht", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PIAT", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Sie ist dem Hoffart ungewogen/", "tokens": ["Sie", "ist", "dem", "Hof\u00b7fart", "un\u00b7ge\u00b7wo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie ist der Wollust spinnenfeind/", "tokens": ["Sie", "ist", "der", "Wol\u00b7lust", "spin\u00b7nen\u00b7feind", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Kein\u2019 Heucheley an ihr erscheint/", "tokens": ["Kein'", "Heu\u00b7che\u00b7ley", "an", "ihr", "er\u00b7scheint", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie ist bey Schafen auferzogen/", "tokens": ["Sie", "ist", "bey", "Scha\u00b7fen", "auf\u00b7er\u00b7zo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Die Uppigkeit und schn\u00f6de Lust/", "tokens": ["Die", "Up\u00b7pig\u00b7keit", "und", "schn\u00f6\u00b7de", "Lust", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ist ihren Sinnen unbewust.", "tokens": ["Ist", "ih\u00b7ren", "Sin\u00b7nen", "un\u00b7be\u00b7wust", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Die zierlich aufgesprosne Jugend/", "tokens": ["Die", "zier\u00b7lich", "auf\u00b7ge\u00b7spros\u00b7ne", "Ju\u00b7gend", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Unschuld schwanenweisses Kleid", "tokens": ["Der", "Un\u00b7schuld", "schwa\u00b7nen\u00b7weis\u00b7ses", "Kleid"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die wundersch\u00f6ne Freundligkeit/", "tokens": ["Die", "wun\u00b7der\u00b7sch\u00f6\u00b7ne", "Freund\u00b7lig\u00b7keit", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die l\u00f6blich\u2019 Einf\u00e4lt/ Zucht und Tugend/", "tokens": ["Die", "l\u00f6b\u00b7lich'", "Ein\u00b7f\u00e4lt", "/", "Zucht", "und", "Tu\u00b7gend", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Die werthe Keuschheit die Sie hat/", "tokens": ["Die", "wert\u00b7he", "Keuschheit", "die", "Sie", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "ART", "PPER", "VAFIN", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Steht mir an Goldestonnen statt.", "tokens": ["Steht", "mir", "an", "Gol\u00b7des\u00b7ton\u00b7nen", "statt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Ich wei\u00df da\u00df ich mit stoltzen Freuden/", "tokens": ["Ich", "wei\u00df", "da\u00df", "ich", "mit", "stolt\u00b7zen", "Freu\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOUS", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Zeit mit ihr verbringen kan/", "tokens": ["Die", "Zeit", "mit", "ihr", "ver\u00b7brin\u00b7gen", "kan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich bin versichert da\u00df kein Mann/", "tokens": ["Ich", "bin", "ver\u00b7si\u00b7chert", "da\u00df", "kein", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "KOUS", "PIAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mich ihrentwegen wird beneiden/", "tokens": ["Mich", "ih\u00b7rent\u00b7we\u00b7gen", "wird", "be\u00b7nei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PAV", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Jhr Armuht ist mein Gold und Geld/", "tokens": ["Ihr", "Ar\u00b7muht", "ist", "mein", "Gold", "und", "Geld", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und grosses Reichthum in der Welt.", "tokens": ["Und", "gros\u00b7ses", "Reicht\u00b7hum", "in", "der", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Dr\u00fcm mag mich Der und Der verd\u00e4nken", "tokens": ["Dr\u00fcm", "mag", "mich", "Der", "und", "Der", "ver\u00b7d\u00e4n\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PRF", "ART", "KON", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich die schlechte Sch\u00e4ferinn/", "tokens": ["Da\u00df", "ich", "die", "schlech\u00b7te", "Sch\u00e4\u00b7fe\u00b7rinn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Gefasset hab\u2019 in meinen Sinn/", "tokens": ["Ge\u00b7fas\u00b7set", "hab'", "in", "mei\u00b7nen", "Sinn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es wird und soll mich gar nicht kr\u00e4nken.", "tokens": ["Es", "wird", "und", "soll", "mich", "gar", "nicht", "kr\u00e4n\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KON", "VMFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich sage nochmals dieses frey/", "tokens": ["Ich", "sa\u00b7ge", "noch\u00b7mals", "die\u00b7ses", "frey", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PDAT", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da\u00df mir Bozen\u2019 am liebsten sey.", "tokens": ["Da\u00df", "mir", "Bo\u00b7zen'", "am", "liebs\u00b7ten", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "PTKA", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}