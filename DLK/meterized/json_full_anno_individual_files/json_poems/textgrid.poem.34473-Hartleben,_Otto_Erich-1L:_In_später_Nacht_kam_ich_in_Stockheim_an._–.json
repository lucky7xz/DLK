{"textgrid.poem.34473": {"metadata": {"author": {"name": "Hartleben, Otto Erich", "birth": "N.A.", "death": "N.A."}, "title": "1L: In sp\u00e4ter Nacht kam ich in Stockheim an. \u2013", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In sp\u00e4ter Nacht kam ich in Stockheim an. \u2013", "tokens": ["In", "sp\u00e4\u00b7ter", "Nacht", "kam", "ich", "in", "Stock\u00b7heim", "an", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Des \u00bbVogelschiessens\u00ab Wollust hatt ich noch", "tokens": ["Des", "\u00bb", "Vo\u00b7gel\u00b7schies\u00b7sens", "\u00ab", "Wol\u00b7lust", "hatt", "ich", "noch"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "$(", "NN", "$(", "NN", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "mit vollen Z\u00fcgen in der Stadt der Musen,", "tokens": ["mit", "vol\u00b7len", "Z\u00fc\u00b7gen", "in", "der", "Stadt", "der", "Mu\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "im alten Jena, galgenfroh genossen.", "tokens": ["im", "al\u00b7ten", "Je\u00b7na", ",", "gal\u00b7gen\u00b7froh", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NE", "$,", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Zum letztenmale \u2013 rief ich frech mir zu \u2013", "tokens": ["Zum", "letz\u00b7ten\u00b7ma\u00b7le", "\u2013", "rief", "ich", "frech", "mir", "zu", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$(", "VVFIN", "PPER", "ADJD", "PPER", "PTKZU", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "zum letztenmal lasst uns den Leib besaufen!", "tokens": ["zum", "letz\u00b7ten\u00b7mal", "lasst", "uns", "den", "Leib", "be\u00b7sau\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADV", "VVFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-++--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Schon morgen liegt er nass auf kalter Bleiche,", "tokens": ["Schon", "mor\u00b7gen", "liegt", "er", "nass", "auf", "kal\u00b7ter", "Blei\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Solidit\u00e4t, kaltwasserheilsam, schaurig,", "tokens": ["So\u00b7li\u00b7di\u00b7t\u00e4t", ",", "kalt\u00b7was\u00b7ser\u00b7heil\u00b7sam", ",", "schau\u00b7rig", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "ver\u00f6det seine Sinne, und ein Sitzbad", "tokens": ["ver\u00b7\u00f6\u00b7det", "sei\u00b7ne", "Sin\u00b7ne", ",", "und", "ein", "Sitz\u00b7bad"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "$,", "KON", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "umf\u00e4ngt das Hintertheil mit stillen Armen.", "tokens": ["um\u00b7f\u00e4ngt", "das", "Hin\u00b7ter\u00b7theil", "mit", "stil\u00b7len", "Ar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "So lasst uns heute noch der Freude denken,", "tokens": ["So", "lasst", "uns", "heu\u00b7te", "noch", "der", "Freu\u00b7de", "den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "der nervenspannenden, der bunten S\u00fcnde \u2013", "tokens": ["der", "ner\u00b7ven\u00b7span\u00b7nen\u00b7den", ",", "der", "bun\u00b7ten", "S\u00fcn\u00b7de", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.13": {"text": "ein frisches Glas, du weltgewandte Schenkin,", "tokens": ["ein", "fri\u00b7sches", "Glas", ",", "du", "welt\u00b7ge\u00b7wand\u00b7te", "Schen\u00b7kin", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "ein frisches Glas und einen letzten Kuss! \u2013", "tokens": ["ein", "fri\u00b7sches", "Glas", "und", "ei\u00b7nen", "letz\u00b7ten", "Kuss", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Ein Mann, der j\u00fcngst der Feder sich verschwor,", "tokens": ["Ein", "Mann", ",", "der", "j\u00fcngst", "der", "Fe\u00b7der", "sich", "ver\u00b7schwor", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "mit dem ich sonnigere Tage einst", "tokens": ["mit", "dem", "ich", "son\u00b7ni\u00b7ge\u00b7re", "Ta\u00b7ge", "einst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "PIAT", "NN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "an Limmatufern, an des \u00dctli Fuss", "tokens": ["an", "Lim\u00b7ma\u00b7tu\u00b7fern", ",", "an", "des", "\u00dct\u00b7li", "Fuss"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "APPR", "ART", "NE", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "frei, froh verlebt \u2013 ihn f\u00fchrte mir das Gl\u00fcck", "tokens": ["frei", ",", "froh", "ver\u00b7lebt", "\u2013", "ihn", "f\u00fchr\u00b7te", "mir", "das", "Gl\u00fcck"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "ADJD", "VVPP", "$(", "PPER", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "dort in den Weg. Vorm Sch\u00fctzenhause sass er,", "tokens": ["dort", "in", "den", "Weg", ".", "Vorm", "Sch\u00fct\u00b7zen\u00b7hau\u00b7se", "sass", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "APPRART", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "mit warmen W\u00fcrstchen pflegend seinen Bauch,", "tokens": ["mit", "war\u00b7men", "W\u00fcr\u00b7stchen", "pfle\u00b7gend", "sei\u00b7nen", "Bauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "und rief mich an, als ich vor\u00fcbereilte.", "tokens": ["und", "rief", "mich", "an", ",", "als", "ich", "vor\u00b7\u00fc\u00b7be\u00b7reil\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Auf seine Fragen musst ich ihm mein Loos", "tokens": ["Auf", "sei\u00b7ne", "Fra\u00b7gen", "musst", "ich", "ihm", "mein", "Loos"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "enth\u00fcllen, und ich sprach: Es ist der Weg", "tokens": ["ent\u00b7h\u00fcl\u00b7len", ",", "und", "ich", "sprach", ":", "Es", "ist", "der", "Weg"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "$,", "KON", "PPER", "VVFIN", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "des Irrenhauses, den ich trete \u2013 schonend", "tokens": ["des", "Ir\u00b7ren\u00b7hau\u00b7ses", ",", "den", "ich", "tre\u00b7te", "\u2013", "scho\u00b7nend"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$(", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "auch wohl Kaltwasserheilanstalt genannt.", "tokens": ["auch", "wohl", "Kalt\u00b7was\u00b7ser\u00b7hei\u00b7lan\u00b7stalt", "ge\u00b7nannt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Des edlen Oheims w\u00fcrdevolle Dummheit", "tokens": ["Des", "ed\u00b7len", "O\u00b7heims", "w\u00fcr\u00b7de\u00b7vol\u00b7le", "Dumm\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "hofft, dass ich dort durch kalte Dauerdouchen,", "tokens": ["hofft", ",", "dass", "ich", "dort", "durch", "kal\u00b7te", "Dau\u00b7er\u00b7dou\u00b7chen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.14": {"text": "geheilt von litterarischen All\u00fcren,", "tokens": ["ge\u00b7heilt", "von", "lit\u00b7te\u00b7ra\u00b7ri\u00b7schen", "Al\u00b7l\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "zum K\u00f6niglichen Landrath reifen werde. \u2013", "tokens": ["zum", "K\u00f6\u00b7nig\u00b7li\u00b7chen", "Land\u00b7ra\u00b7th", "rei\u00b7fen", "wer\u00b7de", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVINF", "VAFIN", "$.", "$("], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Die Stunden drauf im l\u00e4rmenden Gew\u00fchl", "tokens": ["Die", "Stun\u00b7den", "drauf", "im", "l\u00e4r\u00b7men\u00b7den", "Ge\u00b7w\u00fchl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PAV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "des staubigen Marktes waren kurz nur, doch", "tokens": ["des", "stau\u00b7bi\u00b7gen", "Mark\u00b7tes", "wa\u00b7ren", "kurz", "nur", ",", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "ADV", "$,", "ADV"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "wir haben herzhaft lustige draus gemacht", "tokens": ["wir", "ha\u00b7ben", "herz\u00b7haft", "lus\u00b7ti\u00b7ge", "draus", "ge\u00b7macht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "VVFIN", "PAV", "VVPP"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und herzhaft war der Affe, der uns kratzte.", "tokens": ["und", "herz\u00b7haft", "war", "der", "Af\u00b7fe", ",", "der", "uns", "kratz\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Dann auf die Bahn \u2013 und durch die dunklen Berge", "tokens": ["Dann", "auf", "die", "Bahn", "\u2013", "und", "durch", "die", "dunk\u00b7len", "Ber\u00b7ge"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "$(", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "gen S\u00fcden fuhr ich. Klare Sterne blitzten", "tokens": ["gen", "S\u00fc\u00b7den", "fuhr", "ich", ".", "Kla\u00b7re", "Ster\u00b7ne", "blitz\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "$.", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wie Goldesschmuck auf rabenschwarzem Haar", "tokens": ["wie", "Gol\u00b7des\u00b7schmuck", "auf", "ra\u00b7ben\u00b7schwar\u00b7zem", "Haar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "von d\u00fcstren Tannenh\u00fcgeln mir her\u00fcber ...", "tokens": ["von", "d\u00fcst\u00b7ren", "Tan\u00b7nen\u00b7h\u00fc\u00b7geln", "mir", "her\u00b7\u00fc\u00b7ber", "..."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "In sp\u00e4ter Nacht kam ich in Stockheim an.", "tokens": ["In", "sp\u00e4\u00b7ter", "Nacht", "kam", "ich", "in", "Stock\u00b7heim", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Der Mond begl\u00e4nzte nachtbewegte Flaggen,", "tokens": ["Der", "Mond", "be\u00b7gl\u00e4nz\u00b7te", "nacht\u00b7be\u00b7weg\u00b7te", "Flag\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "die rings von kranzgeschm\u00fcckten H\u00fctten wehten,", "tokens": ["die", "rings", "von", "kranz\u00b7ge\u00b7schm\u00fcck\u00b7ten", "H\u00fct\u00b7ten", "weh\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und selber macht er mir den sch\u00f6nsten Knix.", "tokens": ["und", "sel\u00b7ber", "macht", "er", "mir", "den", "sch\u00f6ns\u00b7ten", "Knix", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Ich dankte stillbegl\u00fcckt nach allen Seiten", "tokens": ["Ich", "dank\u00b7te", "still\u00b7be\u00b7gl\u00fcckt", "nach", "al\u00b7len", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "VVFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "und machte selbstbewusst mich auf den Weg.", "tokens": ["und", "mach\u00b7te", "selbst\u00b7be\u00b7wusst", "mich", "auf", "den", "Weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Da wurd ich mit Verwunderung gewahr,", "tokens": ["Da", "wurd", "ich", "mit", "Ver\u00b7wun\u00b7de\u00b7rung", "ge\u00b7wahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "dass (um mich, der Gelegenheit entsprechend,", "tokens": ["dass", "(", "um", "mich", ",", "der", "Ge\u00b7le\u00b7gen\u00b7heit", "ent\u00b7spre\u00b7chend", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "KOUI", "PPER", "$,", "ART", "NN", "VVPP", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "ein wenig \u00e0 la Goethe auszudr\u00fccken)", "tokens": ["ein", "we\u00b7nig", "\u00e0", "la", "Goe\u00b7the", "aus\u00b7zu\u00b7dr\u00fc\u00b7cken", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "FM", "FM", "NE", "VVIZU", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "dass nicht ein einziger edler B\u00fcrger Stockheims", "tokens": ["dass", "nicht", "ein", "ein\u00b7zi\u00b7ger", "ed\u00b7ler", "B\u00fcr\u00b7ger", "Stock\u00b7heims"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "ART", "ADJA", "ADJA", "NN", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.5": {"text": "auf meinem Wege mir entgegen kam,", "tokens": ["auf", "mei\u00b7nem", "We\u00b7ge", "mir", "ent\u00b7ge\u00b7gen", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PPER", "PTKVZ", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "bedeutsam und bescheiden mich zu gr\u00fcssen", "tokens": ["be\u00b7deut\u00b7sam", "und", "be\u00b7schei\u00b7den", "mich", "zu", "gr\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "und mit des Gastfreunds frohbewegtem Wort", "tokens": ["und", "mit", "des", "Gast\u00b7freunds", "froh\u00b7be\u00b7weg\u00b7tem", "Wort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "den Pfad zu weisen in ein reinlich Haus.", "tokens": ["den", "Pfad", "zu", "wei\u00b7sen", "in", "ein", "rein\u00b7lich", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Droschke! so rief ich m\u00fcrrisch durch die Nacht", "tokens": ["Droschke", "!", "so", "rief", "ich", "m\u00fcr\u00b7risch", "durch", "die", "Nacht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "ADV", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.10": {"text": "und drehte etwas indigniert dem Monde", "tokens": ["und", "dreh\u00b7te", "et\u00b7was", "in\u00b7di\u00b7gniert", "dem", "Mon\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "ART", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "den R\u00fccken zu. \u2013 Doch still bliebs wie zuvor.", "tokens": ["den", "R\u00fc\u00b7cken", "zu", ".", "\u2013", "Doch", "still", "bliebs", "wie", "zu\u00b7vor", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$.", "$(", "KON", "ADJD", "VVFIN", "KOKOM", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Da kams mir bald verdriesslich in den Sinn,", "tokens": ["Da", "kams", "mir", "bald", "ver\u00b7driess\u00b7lich", "in", "den", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "dass (um mich, der Gelegenheit entsprechend,", "tokens": ["dass", "(", "um", "mich", ",", "der", "Ge\u00b7le\u00b7gen\u00b7heit", "ent\u00b7spre\u00b7chend", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "KOUI", "PPER", "$,", "ART", "NN", "VVPP", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "[denn heute f\u00fchlt ich mich noch ganz als Dichter]", "tokens": ["denn", "heu\u00b7te", "f\u00fchlt", "ich", "mich", "noch", "ganz", "als", "Dich\u00b7ter"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "KOUS", "NN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "auch einmal wie Paul Lindau auszudr\u00fccken)", "tokens": ["auch", "ein\u00b7mal", "wie", "Paul", "Lin\u00b7dau", "aus\u00b7zu\u00b7dr\u00fc\u00b7cken", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KOKOM", "NE", "NE", "VVIZU", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "dass hier in diesem ganz verstockten Stockheim", "tokens": ["dass", "hier", "in", "die\u00b7sem", "ganz", "ver\u00b7stock\u00b7ten", "Stock\u00b7heim"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "APPR", "PDAT", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-++", "measure": "unknown.measure.hexa"}, "line.6": {"text": "die Droschke als Culturentwicklungsmittel", "tokens": ["die", "Droschke", "als", "Cul\u00b7tu\u00b7rent\u00b7wick\u00b7lungs\u00b7mit\u00b7tel"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "bis jetzt die ihr geb\u00fchrende Beachtung", "tokens": ["bis", "jetzt", "die", "ihr", "ge\u00b7b\u00fch\u00b7ren\u00b7de", "Be\u00b7ach\u00b7tung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "PPOSAT", "ADJA", "NN"], "meter": "---+-+---+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "vielleicht noch nicht gefunden haben m\u00f6chte.", "tokens": ["viel\u00b7leicht", "noch", "nicht", "ge\u00b7fun\u00b7den", "ha\u00b7ben", "m\u00f6ch\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKNEG", "VVPP", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Und d\u00fcster schritt ich meines Weges weiter.", "tokens": ["Und", "d\u00fcs\u00b7ter", "schritt", "ich", "mei\u00b7nes", "We\u00b7ges", "wei\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u2013 Doch da ich Realist zu sein mich m\u00fche,", "tokens": ["\u2013", "Doch", "da", "ich", "Re\u00b7a\u00b7list", "zu", "sein", "mich", "m\u00fc\u00b7he", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "KOUS", "PPER", "NN", "PTKZU", "VAINF", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und nichts erz\u00e4hle, was ich nicht erfahren", "tokens": ["und", "nichts", "er\u00b7z\u00e4h\u00b7le", ",", "was", "ich", "nicht", "er\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VVFIN", "$,", "PWS", "PPER", "PTKNEG", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und aufgenommen in den eignen Schatz", "tokens": ["und", "auf\u00b7ge\u00b7nom\u00b7men", "in", "den", "eig\u00b7nen", "Schatz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "des Vorgestellten, so erz\u00e4hl ich lieber \u2013", "tokens": ["des", "Vor\u00b7ge\u00b7stell\u00b7ten", ",", "so", "er\u00b7z\u00e4hl", "ich", "lie\u00b7ber", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "noch wo und wie mein Haupt gebettet lag.", "tokens": ["noch", "wo", "und", "wie", "mein", "Haupt", "ge\u00b7bet\u00b7tet", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWAV", "KON", "KOKOM", "PPOSAT", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Ich m\u00fcsste l\u00fcgen ...", "tokens": ["Ich", "m\u00fcss\u00b7te", "l\u00fc\u00b7gen", "..."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$("], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Genug: am Morgen weckte mich ein H\u00e4mmern.", "tokens": ["Ge\u00b7nug", ":", "am", "Mor\u00b7gen", "weck\u00b7te", "mich", "ein", "H\u00e4m\u00b7mern", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "APPRART", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Im Kopfe? Nein! Der Kater ist ein Hausthier,", "tokens": ["Im", "Kop\u00b7fe", "?", "Nein", "!", "Der", "Ka\u00b7ter", "ist", "ein", "Haus\u00b7thier", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "PTKANT", "$.", "ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "mich Heimathlosen hat er l\u00e4ngst verlassen,", "tokens": ["mich", "Hei\u00b7math\u00b7lo\u00b7sen", "hat", "er", "l\u00e4ngst", "ver\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "kriecht dort herum, wo frohe Menschen sind.", "tokens": ["kriecht", "dort", "he\u00b7rum", ",", "wo", "fro\u00b7he", "Men\u00b7schen", "sind", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "PWAV", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Ein wenig Fieber nur in schlaffen Adern", "tokens": ["Ein", "we\u00b7nig", "Fie\u00b7ber", "nur", "in", "schlaf\u00b7fen", "A\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "und unerfrischt, so kroch ich aus den Federn.", "tokens": ["und", "un\u00b7er\u00b7frischt", ",", "so", "kroch", "ich", "aus", "den", "Fe\u00b7dern", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Pardon! \u2013 Da zeigt sichs wieder mal frappant,", "tokens": ["Par\u00b7don", "!", "\u2013", "Da", "zeigt", "sichs", "wie\u00b7der", "mal", "frap\u00b7pant", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "ADV", "VVFIN", "PIS", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "wie stetiger Gebrauch gebrauchter Worte", "tokens": ["wie", "ste\u00b7ti\u00b7ger", "Ge\u00b7brauch", "ge\u00b7brauch\u00b7ter", "Wor\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "uns Sinn und Inhalt ganz vergessen macht.", "tokens": ["uns", "Sinn", "und", "In\u00b7halt", "ganz", "ver\u00b7ges\u00b7sen", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "ADV", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mechanisch kauen wir die leeren H\u00fclsen:", "tokens": ["Me\u00b7cha\u00b7nisch", "kau\u00b7en", "wir", "die", "lee\u00b7ren", "H\u00fcl\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "hohl bleibt der Kopf und hungrig das Gem\u00fcth.", "tokens": ["hohl", "bleibt", "der", "Kopf", "und", "hung\u00b7rig", "das", "Ge\u00b7m\u00fcth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "KON", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "So sagt ich denn, ich kr\u00f6che aus den Federn.", "tokens": ["So", "sagt", "ich", "denn", ",", "ich", "kr\u00f6\u00b7che", "aus", "den", "Fe\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich Schuft! Stroh war es, Stroh und dreimal Stroh!", "tokens": ["Ich", "Schuft", "!", "Stroh", "war", "es", ",", "Stroh", "und", "drei\u00b7mal", "Stroh", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$.", "NN", "VAFIN", "PPER", "$,", "NN", "KON", "ADV", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Die Sprache, die des Wortes Werth nicht kennt,", "tokens": ["Die", "Spra\u00b7che", ",", "die", "des", "Wor\u00b7tes", "Werth", "nicht", "kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "ADJA", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "der die Begriffe h\u00f6her gelten nicht", "tokens": ["der", "die", "Be\u00b7grif\u00b7fe", "h\u00f6\u00b7her", "gel\u00b7ten", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ADJD", "VVFIN", "PTKNEG"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "als schmutzige Karten in des Spielers Hand,", "tokens": ["als", "schmut\u00b7zi\u00b7ge", "Kar\u00b7ten", "in", "des", "Spie\u00b7lers", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "bald tr\u00e4g, bald wuchtig auf den Tisch geworfen,", "tokens": ["bald", "tr\u00e4g", ",", "bald", "wuch\u00b7tig", "auf", "den", "Tisch", "ge\u00b7wor\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "die Sprache, der das Blut der Sinne schwand,", "tokens": ["die", "Spra\u00b7che", ",", "der", "das", "Blut", "der", "Sin\u00b7ne", "schwand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "und deren Bl\u00e4sse Schminke nur verdeckt \u2013", "tokens": ["und", "de\u00b7ren", "Bl\u00e4s\u00b7se", "Schmin\u00b7ke", "nur", "ver\u00b7deckt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRELAT", "NN", "VVFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ins Grab mit ihr \u2013 sie hat zu lang gelebt \u2013", "tokens": ["ins", "Grab", "mit", "ihr", "\u2013", "sie", "hat", "zu", "lang", "ge\u00b7lebt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "PPOSAT", "$(", "PPER", "VAFIN", "PTKA", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "bringt sie den Schinderknechten auf den Anger,", "tokens": ["bringt", "sie", "den", "Schin\u00b7der\u00b7knech\u00b7ten", "auf", "den", "An\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.9": {"text": "den Oberlehrern und den Professoren! \u2013", "tokens": ["den", "O\u00b7ber\u00b7leh\u00b7rern", "und", "den", "Pro\u00b7fes\u00b7so\u00b7ren", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Ein H\u00e4mmern weckte mich, denn Fahnenweihe", "tokens": ["Ein", "H\u00e4m\u00b7mern", "weck\u00b7te", "mich", ",", "denn", "Fah\u00b7nen\u00b7wei\u00b7he"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "war heut in Stockheim: der Verein der Krieger", "tokens": ["war", "heut", "in", "Stock\u00b7heim", ":", "der", "Ver\u00b7ein", "der", "Krie\u00b7ger"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "NN", "$.", "ART", "NN", "ART", "NN"], "meter": "-+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "betrank sich treu f\u00fcr Gott und Vaterland,", "tokens": ["be\u00b7trank", "sich", "treu", "f\u00fcr", "Gott", "und", "Va\u00b7ter\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADJD", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "betrank sich fest, um seiner neuen Fahne", "tokens": ["be\u00b7trank", "sich", "fest", ",", "um", "sei\u00b7ner", "neu\u00b7en", "Fah\u00b7ne"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "PTKVZ", "$,", "KOUI", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "f\u00fcr alle Zukunft echten Glanz zu geben.", "tokens": ["f\u00fcr", "al\u00b7le", "Zu\u00b7kunft", "ech\u00b7ten", "Glanz", "zu", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Daher die Flaggen, daher diese Kr\u00e4nze ...", "tokens": ["Da\u00b7her", "die", "Flag\u00b7gen", ",", "da\u00b7her", "die\u00b7se", "Kr\u00e4n\u00b7ze", "..."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "$,", "PAV", "PDAT", "NN", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Schweig still, mein tiefbesch\u00e4mtes Dichterherz!", "tokens": ["Schweig", "still", ",", "mein", "tief\u00b7be\u00b7sch\u00e4m\u00b7tes", "Dich\u00b7ter\u00b7herz", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Und eine Nothdurft trat an mich heran,", "tokens": ["Und", "ei\u00b7ne", "Noth\u00b7durft", "trat", "an", "mich", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "APPR", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "zwang mich, die Kammer schleunigst zu verlassen.", "tokens": ["zwang", "mich", ",", "die", "Kam\u00b7mer", "schleu\u00b7nigst", "zu", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ART", "NN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Die Speisen, die der Mensch, wie jedes Thier,", "tokens": ["Die", "Spei\u00b7sen", ",", "die", "der", "Mensch", ",", "wie", "je\u00b7des", "Thier", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "$,", "PWAV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "um seinen Leib zu n\u00e4hren zu sich nimmt,", "tokens": ["um", "sei\u00b7nen", "Leib", "zu", "n\u00e4h\u00b7ren", "zu", "sich", "nimmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "beh\u00e4lt er nicht in vollem Umfang bei sich.", "tokens": ["be\u00b7h\u00e4lt", "er", "nicht", "in", "vol\u00b7lem", "Um\u00b7fang", "bei", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "APPR", "ADJA", "NN", "APPR", "PRF", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Bef\u00e4higt ist der K\u00f6rper, was da werthvoll", "tokens": ["Be\u00b7f\u00e4\u00b7higt", "ist", "der", "K\u00f6r\u00b7per", ",", "was", "da", "werth\u00b7voll"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "ART", "NN", "$,", "PRELS", "ADV", "ADJD"], "meter": "-+-+-+--+--", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "von dem, was minder wichtig, wohl zu sichten:", "tokens": ["von", "dem", ",", "was", "min\u00b7der", "wich\u00b7tig", ",", "wohl", "zu", "sich\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "ADV", "ADJD", "$,", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "das erstere nimmt er voll Schlauheit auf", "tokens": ["das", "ers\u00b7te\u00b7re", "nimmt", "er", "voll", "Schlau\u00b7heit", "auf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VVFIN", "PPER", "ADJD", "NN", "APPR"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.9": {"text": "und mit Bedacht ausscheidet er das andre.", "tokens": ["und", "mit", "Be\u00b7dacht", "aus\u00b7schei\u00b7det", "er", "das", "and\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVFIN", "PPER", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Auch mir ist dieses Menschliche nicht fremd.", "tokens": ["Auch", "mir", "ist", "die\u00b7ses", "Menschli\u00b7che", "nicht", "fremd", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PDAT", "NN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und als ich nun mit kindlich offner Seele", "tokens": ["Und", "als", "ich", "nun", "mit", "kind\u00b7lich", "off\u00b7ner", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "die alte schmutzbetriefte Pflegerin", "tokens": ["die", "al\u00b7te", "schmutz\u00b7be\u00b7trief\u00b7te", "Pfle\u00b7ge\u00b7rin"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "anging um einer Klause keusche Wohlthat,", "tokens": ["an\u00b7ging", "um", "ei\u00b7ner", "Klau\u00b7se", "keu\u00b7sche", "Wohlt\u00b7hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "wies sie mit unverst\u00e4ndlichem Gebrumm", "tokens": ["wies", "sie", "mit", "un\u00b7ver\u00b7st\u00e4nd\u00b7li\u00b7chem", "Ge\u00b7brumm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "mich aus der Hinterth\u00fcr und auf den Hof,", "tokens": ["mich", "aus", "der", "Hin\u00b7tert\u00b7h\u00fcr", "und", "auf", "den", "Hof", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "KON", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "wo goldnen Mist die fr\u00fche Sonne kr\u00f6nte.", "tokens": ["wo", "gold\u00b7nen", "Mist", "die", "fr\u00fc\u00b7he", "Son\u00b7ne", "kr\u00f6n\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.16": {"line.1": {"text": "Nachdem ich lange dort mich umgethan", "tokens": ["Nach\u00b7dem", "ich", "lan\u00b7ge", "dort", "mich", "um\u00b7ge\u00b7than"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPER", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "und hinter jede Bretterth\u00fcr gesp\u00e4ht,", "tokens": ["und", "hin\u00b7ter", "je\u00b7de", "Bret\u00b7tert\u00b7h\u00fcr", "ge\u00b7sp\u00e4ht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und hinter jeder nur \u2013 beliebte Thiere,", "tokens": ["und", "hin\u00b7ter", "je\u00b7der", "nur", "\u2013", "be\u00b7lieb\u00b7te", "Thie\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "PIS", "ADV", "$(", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "doch nie den trauten Sitz gefunden hatte \u2013", "tokens": ["doch", "nie", "den", "trau\u00b7ten", "Sitz", "ge\u00b7fun\u00b7den", "hat\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "da d\u00e4mmerte in meiner zagen Seele", "tokens": ["da", "d\u00e4m\u00b7mer\u00b7te", "in", "mei\u00b7ner", "za\u00b7gen", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "ein ungewollt begl\u00fcckender Gedanke.", "tokens": ["ein", "un\u00b7ge\u00b7wollt", "be\u00b7gl\u00fc\u00b7cken\u00b7der", "Ge\u00b7dan\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Natur! so rief ich, ewige heilige Mutter,", "tokens": ["Na\u00b7tur", "!", "so", "rief", "ich", ",", "e\u00b7wi\u00b7ge", "hei\u00b7li\u00b7ge", "Mut\u00b7ter", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "ADV", "VVFIN", "PPER", "$,", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "du ziehst den Halbverlornen machtvoll an!", "tokens": ["du", "ziehst", "den", "Halb\u00b7ver\u00b7lor\u00b7nen", "macht\u00b7voll", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Das trotzige \u2013 das reuevolle Kind", "tokens": ["Das", "trot\u00b7zi\u00b7ge", "\u2013", "das", "reu\u00b7e\u00b7vol\u00b7le", "Kind"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "ziehst du aufs neue sanft in deinen Schoss!", "tokens": ["ziehst", "du", "aufs", "neu\u00b7e", "sanft", "in", "dei\u00b7nen", "Schoss", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "ADJA", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "O dank, du gute, liebevolle Mutter!", "tokens": ["O", "dank", ",", "du", "gu\u00b7te", ",", "lie\u00b7be\u00b7vol\u00b7le", "Mut\u00b7ter", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "PPER", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.18": {"line.1": {"text": "So str\u00f6mten die Gef\u00fchle brausend \u00fcber ...", "tokens": ["So", "str\u00f6m\u00b7ten", "die", "Ge\u00b7f\u00fch\u00b7le", "brau\u00b7send", "\u00fc\u00b7ber", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "APPR", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Tief in mir klang es wie ein heilger Schwur:", "tokens": ["Tief", "in", "mir", "klang", "es", "wie", "ein", "heil\u00b7ger", "Schwur", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "VVFIN", "PPER", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Auf deinem Pfade will ich f\u00fcrder wandeln,", "tokens": ["Auf", "dei\u00b7nem", "Pfa\u00b7de", "will", "ich", "f\u00fcr\u00b7der", "wan\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "dir ewig folgen, Herrscherin Natur! \u2013", "tokens": ["dir", "e\u00b7wig", "fol\u00b7gen", ",", "Herr\u00b7sche\u00b7rin", "Na\u00b7tur", "!", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "ADJD", "VVINF", "$,", "NN", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Kein Machtgebot verirrter Menschen soll", "tokens": ["Kein", "Macht\u00b7ge\u00b7bot", "ver\u00b7irr\u00b7ter", "Men\u00b7schen", "soll"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "ADJA", "NN", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "entfernen mich von dir und meinem Eide:", "tokens": ["ent\u00b7fer\u00b7nen", "mich", "von", "dir", "und", "mei\u00b7nem", "Ei\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPER", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "mein Leib ist dein und f\u00fcrder meine Seele", "tokens": ["mein", "Leib", "ist", "dein", "und", "f\u00fcr\u00b7der", "mei\u00b7ne", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "KON", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "denn beide sind ", "tokens": ["denn", "bei\u00b7de", "sind"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIS", "VAFIN"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.19": {"line.1": {"text": "Und langsam \u2013 und erleichtert reckt ich mich", "tokens": ["Und", "lang\u00b7sam", "\u2013", "und", "er\u00b7leich\u00b7tert", "reckt", "ich", "mich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "$(", "KON", "VVPP", "VVFIN", "PPER", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "nach solchem tiefentquollnen Schwur empor,", "tokens": ["nach", "sol\u00b7chem", "tie\u00b7fent\u00b7quoll\u00b7nen", "Schwur", "em\u00b7por", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und dehnte mich und streckte meine Glieder.", "tokens": ["und", "dehn\u00b7te", "mich", "und", "streck\u00b7te", "mei\u00b7ne", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und selbstbewusst und h\u00f6chst vertraulich nickt ich", "tokens": ["Und", "selbst\u00b7be\u00b7wusst", "und", "h\u00f6chst", "ver\u00b7trau\u00b7lich", "nickt", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "KON", "ADV", "ADJD", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "der jungen Sonne zu, die frisch und blank", "tokens": ["der", "jun\u00b7gen", "Son\u00b7ne", "zu", ",", "die", "frisch", "und", "blank"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$,", "PRELS", "ADJD", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "dort auf dem dunklen Fichtenwalde lag \u2013", "tokens": ["dort", "auf", "dem", "dunk\u00b7len", "Fich\u00b7ten\u00b7wal\u00b7de", "lag", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ein nacktes Weib auf einer B\u00e4renhaut \u2013:", "tokens": ["ein", "nack\u00b7tes", "Weib", "auf", "ei\u00b7ner", "B\u00e4\u00b7ren\u00b7haut", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$(", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Sch\u00f6n guten Morgen \u2013 hast du ausgeschlafen?", "tokens": ["Sch\u00f6n", "gu\u00b7ten", "Mor\u00b7gen", "\u2013", "hast", "du", "aus\u00b7ge\u00b7schla\u00b7fen", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$(", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.20": {"line.1": {"text": "In sp\u00e4ter Nacht kam ich in Stockheim an. \u2013", "tokens": ["In", "sp\u00e4\u00b7ter", "Nacht", "kam", "ich", "in", "Stock\u00b7heim", "an", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Des \u00bbVogelschiessens\u00ab Wollust hatt ich noch", "tokens": ["Des", "\u00bb", "Vo\u00b7gel\u00b7schies\u00b7sens", "\u00ab", "Wol\u00b7lust", "hatt", "ich", "noch"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "$(", "NN", "$(", "NN", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "mit vollen Z\u00fcgen in der Stadt der Musen,", "tokens": ["mit", "vol\u00b7len", "Z\u00fc\u00b7gen", "in", "der", "Stadt", "der", "Mu\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "im alten Jena, galgenfroh genossen.", "tokens": ["im", "al\u00b7ten", "Je\u00b7na", ",", "gal\u00b7gen\u00b7froh", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NE", "$,", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Zum letztenmale \u2013 rief ich frech mir zu \u2013", "tokens": ["Zum", "letz\u00b7ten\u00b7ma\u00b7le", "\u2013", "rief", "ich", "frech", "mir", "zu", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$(", "VVFIN", "PPER", "ADJD", "PPER", "PTKZU", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "zum letztenmal lasst uns den Leib besaufen!", "tokens": ["zum", "letz\u00b7ten\u00b7mal", "lasst", "uns", "den", "Leib", "be\u00b7sau\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADV", "VVFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-++--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Schon morgen liegt er nass auf kalter Bleiche,", "tokens": ["Schon", "mor\u00b7gen", "liegt", "er", "nass", "auf", "kal\u00b7ter", "Blei\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Solidit\u00e4t, kaltwasserheilsam, schaurig,", "tokens": ["So\u00b7li\u00b7di\u00b7t\u00e4t", ",", "kalt\u00b7was\u00b7ser\u00b7heil\u00b7sam", ",", "schau\u00b7rig", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "ver\u00f6det seine Sinne, und ein Sitzbad", "tokens": ["ver\u00b7\u00f6\u00b7det", "sei\u00b7ne", "Sin\u00b7ne", ",", "und", "ein", "Sitz\u00b7bad"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "$,", "KON", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "umf\u00e4ngt das Hintertheil mit stillen Armen.", "tokens": ["um\u00b7f\u00e4ngt", "das", "Hin\u00b7ter\u00b7theil", "mit", "stil\u00b7len", "Ar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "So lasst uns heute noch der Freude denken,", "tokens": ["So", "lasst", "uns", "heu\u00b7te", "noch", "der", "Freu\u00b7de", "den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "der nervenspannenden, der bunten S\u00fcnde \u2013", "tokens": ["der", "ner\u00b7ven\u00b7span\u00b7nen\u00b7den", ",", "der", "bun\u00b7ten", "S\u00fcn\u00b7de", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.13": {"text": "ein frisches Glas, du weltgewandte Schenkin,", "tokens": ["ein", "fri\u00b7sches", "Glas", ",", "du", "welt\u00b7ge\u00b7wand\u00b7te", "Schen\u00b7kin", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "ein frisches Glas und einen letzten Kuss! \u2013", "tokens": ["ein", "fri\u00b7sches", "Glas", "und", "ei\u00b7nen", "letz\u00b7ten", "Kuss", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.21": {"line.1": {"text": "Ein Mann, der j\u00fcngst der Feder sich verschwor,", "tokens": ["Ein", "Mann", ",", "der", "j\u00fcngst", "der", "Fe\u00b7der", "sich", "ver\u00b7schwor", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "mit dem ich sonnigere Tage einst", "tokens": ["mit", "dem", "ich", "son\u00b7ni\u00b7ge\u00b7re", "Ta\u00b7ge", "einst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "PIAT", "NN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "an Limmatufern, an des \u00dctli Fuss", "tokens": ["an", "Lim\u00b7ma\u00b7tu\u00b7fern", ",", "an", "des", "\u00dct\u00b7li", "Fuss"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "APPR", "ART", "NE", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "frei, froh verlebt \u2013 ihn f\u00fchrte mir das Gl\u00fcck", "tokens": ["frei", ",", "froh", "ver\u00b7lebt", "\u2013", "ihn", "f\u00fchr\u00b7te", "mir", "das", "Gl\u00fcck"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "ADJD", "VVPP", "$(", "PPER", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "dort in den Weg. Vorm Sch\u00fctzenhause sass er,", "tokens": ["dort", "in", "den", "Weg", ".", "Vorm", "Sch\u00fct\u00b7zen\u00b7hau\u00b7se", "sass", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "APPRART", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "mit warmen W\u00fcrstchen pflegend seinen Bauch,", "tokens": ["mit", "war\u00b7men", "W\u00fcr\u00b7stchen", "pfle\u00b7gend", "sei\u00b7nen", "Bauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "und rief mich an, als ich vor\u00fcbereilte.", "tokens": ["und", "rief", "mich", "an", ",", "als", "ich", "vor\u00b7\u00fc\u00b7be\u00b7reil\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Auf seine Fragen musst ich ihm mein Loos", "tokens": ["Auf", "sei\u00b7ne", "Fra\u00b7gen", "musst", "ich", "ihm", "mein", "Loos"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "enth\u00fcllen, und ich sprach: Es ist der Weg", "tokens": ["ent\u00b7h\u00fcl\u00b7len", ",", "und", "ich", "sprach", ":", "Es", "ist", "der", "Weg"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "$,", "KON", "PPER", "VVFIN", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "des Irrenhauses, den ich trete \u2013 schonend", "tokens": ["des", "Ir\u00b7ren\u00b7hau\u00b7ses", ",", "den", "ich", "tre\u00b7te", "\u2013", "scho\u00b7nend"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$(", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "auch wohl Kaltwasserheilanstalt genannt.", "tokens": ["auch", "wohl", "Kalt\u00b7was\u00b7ser\u00b7hei\u00b7lan\u00b7stalt", "ge\u00b7nannt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Des edlen Oheims w\u00fcrdevolle Dummheit", "tokens": ["Des", "ed\u00b7len", "O\u00b7heims", "w\u00fcr\u00b7de\u00b7vol\u00b7le", "Dumm\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "hofft, dass ich dort durch kalte Dauerdouchen,", "tokens": ["hofft", ",", "dass", "ich", "dort", "durch", "kal\u00b7te", "Dau\u00b7er\u00b7dou\u00b7chen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.14": {"text": "geheilt von litterarischen All\u00fcren,", "tokens": ["ge\u00b7heilt", "von", "lit\u00b7te\u00b7ra\u00b7ri\u00b7schen", "Al\u00b7l\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "zum K\u00f6niglichen Landrath reifen werde. \u2013", "tokens": ["zum", "K\u00f6\u00b7nig\u00b7li\u00b7chen", "Land\u00b7ra\u00b7th", "rei\u00b7fen", "wer\u00b7de", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVINF", "VAFIN", "$.", "$("], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Die Stunden drauf im l\u00e4rmenden Gew\u00fchl", "tokens": ["Die", "Stun\u00b7den", "drauf", "im", "l\u00e4r\u00b7men\u00b7den", "Ge\u00b7w\u00fchl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PAV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "des staubigen Marktes waren kurz nur, doch", "tokens": ["des", "stau\u00b7bi\u00b7gen", "Mark\u00b7tes", "wa\u00b7ren", "kurz", "nur", ",", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "ADV", "$,", "ADV"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "wir haben herzhaft lustige draus gemacht", "tokens": ["wir", "ha\u00b7ben", "herz\u00b7haft", "lus\u00b7ti\u00b7ge", "draus", "ge\u00b7macht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "VVFIN", "PAV", "VVPP"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und herzhaft war der Affe, der uns kratzte.", "tokens": ["und", "herz\u00b7haft", "war", "der", "Af\u00b7fe", ",", "der", "uns", "kratz\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.23": {"line.1": {"text": "Dann auf die Bahn \u2013 und durch die dunklen Berge", "tokens": ["Dann", "auf", "die", "Bahn", "\u2013", "und", "durch", "die", "dunk\u00b7len", "Ber\u00b7ge"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "$(", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "gen S\u00fcden fuhr ich. Klare Sterne blitzten", "tokens": ["gen", "S\u00fc\u00b7den", "fuhr", "ich", ".", "Kla\u00b7re", "Ster\u00b7ne", "blitz\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "$.", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wie Goldesschmuck auf rabenschwarzem Haar", "tokens": ["wie", "Gol\u00b7des\u00b7schmuck", "auf", "ra\u00b7ben\u00b7schwar\u00b7zem", "Haar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "von d\u00fcstren Tannenh\u00fcgeln mir her\u00fcber ...", "tokens": ["von", "d\u00fcst\u00b7ren", "Tan\u00b7nen\u00b7h\u00fc\u00b7geln", "mir", "her\u00b7\u00fc\u00b7ber", "..."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.24": {"line.1": {"text": "In sp\u00e4ter Nacht kam ich in Stockheim an.", "tokens": ["In", "sp\u00e4\u00b7ter", "Nacht", "kam", "ich", "in", "Stock\u00b7heim", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Der Mond begl\u00e4nzte nachtbewegte Flaggen,", "tokens": ["Der", "Mond", "be\u00b7gl\u00e4nz\u00b7te", "nacht\u00b7be\u00b7weg\u00b7te", "Flag\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "die rings von kranzgeschm\u00fcckten H\u00fctten wehten,", "tokens": ["die", "rings", "von", "kranz\u00b7ge\u00b7schm\u00fcck\u00b7ten", "H\u00fct\u00b7ten", "weh\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und selber macht er mir den sch\u00f6nsten Knix.", "tokens": ["und", "sel\u00b7ber", "macht", "er", "mir", "den", "sch\u00f6ns\u00b7ten", "Knix", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Ich dankte stillbegl\u00fcckt nach allen Seiten", "tokens": ["Ich", "dank\u00b7te", "still\u00b7be\u00b7gl\u00fcckt", "nach", "al\u00b7len", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "VVFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "und machte selbstbewusst mich auf den Weg.", "tokens": ["und", "mach\u00b7te", "selbst\u00b7be\u00b7wusst", "mich", "auf", "den", "Weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.25": {"line.1": {"text": "Da wurd ich mit Verwunderung gewahr,", "tokens": ["Da", "wurd", "ich", "mit", "Ver\u00b7wun\u00b7de\u00b7rung", "ge\u00b7wahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "dass (um mich, der Gelegenheit entsprechend,", "tokens": ["dass", "(", "um", "mich", ",", "der", "Ge\u00b7le\u00b7gen\u00b7heit", "ent\u00b7spre\u00b7chend", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "KOUI", "PPER", "$,", "ART", "NN", "VVPP", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "ein wenig \u00e0 la Goethe auszudr\u00fccken)", "tokens": ["ein", "we\u00b7nig", "\u00e0", "la", "Goe\u00b7the", "aus\u00b7zu\u00b7dr\u00fc\u00b7cken", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "FM", "FM", "NE", "VVIZU", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "dass nicht ein einziger edler B\u00fcrger Stockheims", "tokens": ["dass", "nicht", "ein", "ein\u00b7zi\u00b7ger", "ed\u00b7ler", "B\u00fcr\u00b7ger", "Stock\u00b7heims"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "ART", "ADJA", "ADJA", "NN", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.5": {"text": "auf meinem Wege mir entgegen kam,", "tokens": ["auf", "mei\u00b7nem", "We\u00b7ge", "mir", "ent\u00b7ge\u00b7gen", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PPER", "PTKVZ", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "bedeutsam und bescheiden mich zu gr\u00fcssen", "tokens": ["be\u00b7deut\u00b7sam", "und", "be\u00b7schei\u00b7den", "mich", "zu", "gr\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "und mit des Gastfreunds frohbewegtem Wort", "tokens": ["und", "mit", "des", "Gast\u00b7freunds", "froh\u00b7be\u00b7weg\u00b7tem", "Wort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "den Pfad zu weisen in ein reinlich Haus.", "tokens": ["den", "Pfad", "zu", "wei\u00b7sen", "in", "ein", "rein\u00b7lich", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Droschke! so rief ich m\u00fcrrisch durch die Nacht", "tokens": ["Droschke", "!", "so", "rief", "ich", "m\u00fcr\u00b7risch", "durch", "die", "Nacht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "ADV", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.10": {"text": "und drehte etwas indigniert dem Monde", "tokens": ["und", "dreh\u00b7te", "et\u00b7was", "in\u00b7di\u00b7gniert", "dem", "Mon\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "ART", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "den R\u00fccken zu. \u2013 Doch still bliebs wie zuvor.", "tokens": ["den", "R\u00fc\u00b7cken", "zu", ".", "\u2013", "Doch", "still", "bliebs", "wie", "zu\u00b7vor", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$.", "$(", "KON", "ADJD", "VVFIN", "KOKOM", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.26": {"line.1": {"text": "Da kams mir bald verdriesslich in den Sinn,", "tokens": ["Da", "kams", "mir", "bald", "ver\u00b7driess\u00b7lich", "in", "den", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "dass (um mich, der Gelegenheit entsprechend,", "tokens": ["dass", "(", "um", "mich", ",", "der", "Ge\u00b7le\u00b7gen\u00b7heit", "ent\u00b7spre\u00b7chend", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "KOUI", "PPER", "$,", "ART", "NN", "VVPP", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "[denn heute f\u00fchlt ich mich noch ganz als Dichter]", "tokens": ["denn", "heu\u00b7te", "f\u00fchlt", "ich", "mich", "noch", "ganz", "als", "Dich\u00b7ter"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "KOUS", "NN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "auch einmal wie Paul Lindau auszudr\u00fccken)", "tokens": ["auch", "ein\u00b7mal", "wie", "Paul", "Lin\u00b7dau", "aus\u00b7zu\u00b7dr\u00fc\u00b7cken", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KOKOM", "NE", "NE", "VVIZU", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "dass hier in diesem ganz verstockten Stockheim", "tokens": ["dass", "hier", "in", "die\u00b7sem", "ganz", "ver\u00b7stock\u00b7ten", "Stock\u00b7heim"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "APPR", "PDAT", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-++", "measure": "unknown.measure.hexa"}, "line.6": {"text": "die Droschke als Culturentwicklungsmittel", "tokens": ["die", "Droschke", "als", "Cul\u00b7tu\u00b7rent\u00b7wick\u00b7lungs\u00b7mit\u00b7tel"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "bis jetzt die ihr geb\u00fchrende Beachtung", "tokens": ["bis", "jetzt", "die", "ihr", "ge\u00b7b\u00fch\u00b7ren\u00b7de", "Be\u00b7ach\u00b7tung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "PPOSAT", "ADJA", "NN"], "meter": "---+-+---+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "vielleicht noch nicht gefunden haben m\u00f6chte.", "tokens": ["viel\u00b7leicht", "noch", "nicht", "ge\u00b7fun\u00b7den", "ha\u00b7ben", "m\u00f6ch\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKNEG", "VVPP", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.27": {"line.1": {"text": "Und d\u00fcster schritt ich meines Weges weiter.", "tokens": ["Und", "d\u00fcs\u00b7ter", "schritt", "ich", "mei\u00b7nes", "We\u00b7ges", "wei\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u2013 Doch da ich Realist zu sein mich m\u00fche,", "tokens": ["\u2013", "Doch", "da", "ich", "Re\u00b7a\u00b7list", "zu", "sein", "mich", "m\u00fc\u00b7he", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "KOUS", "PPER", "NN", "PTKZU", "VAINF", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und nichts erz\u00e4hle, was ich nicht erfahren", "tokens": ["und", "nichts", "er\u00b7z\u00e4h\u00b7le", ",", "was", "ich", "nicht", "er\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VVFIN", "$,", "PWS", "PPER", "PTKNEG", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und aufgenommen in den eignen Schatz", "tokens": ["und", "auf\u00b7ge\u00b7nom\u00b7men", "in", "den", "eig\u00b7nen", "Schatz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "des Vorgestellten, so erz\u00e4hl ich lieber \u2013", "tokens": ["des", "Vor\u00b7ge\u00b7stell\u00b7ten", ",", "so", "er\u00b7z\u00e4hl", "ich", "lie\u00b7ber", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "noch wo und wie mein Haupt gebettet lag.", "tokens": ["noch", "wo", "und", "wie", "mein", "Haupt", "ge\u00b7bet\u00b7tet", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWAV", "KON", "KOKOM", "PPOSAT", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Ich m\u00fcsste l\u00fcgen ...", "tokens": ["Ich", "m\u00fcss\u00b7te", "l\u00fc\u00b7gen", "..."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$("], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.28": {"line.1": {"text": "Genug: am Morgen weckte mich ein H\u00e4mmern.", "tokens": ["Ge\u00b7nug", ":", "am", "Mor\u00b7gen", "weck\u00b7te", "mich", "ein", "H\u00e4m\u00b7mern", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "APPRART", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Im Kopfe? Nein! Der Kater ist ein Hausthier,", "tokens": ["Im", "Kop\u00b7fe", "?", "Nein", "!", "Der", "Ka\u00b7ter", "ist", "ein", "Haus\u00b7thier", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "PTKANT", "$.", "ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "mich Heimathlosen hat er l\u00e4ngst verlassen,", "tokens": ["mich", "Hei\u00b7math\u00b7lo\u00b7sen", "hat", "er", "l\u00e4ngst", "ver\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "kriecht dort herum, wo frohe Menschen sind.", "tokens": ["kriecht", "dort", "he\u00b7rum", ",", "wo", "fro\u00b7he", "Men\u00b7schen", "sind", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "PWAV", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Ein wenig Fieber nur in schlaffen Adern", "tokens": ["Ein", "we\u00b7nig", "Fie\u00b7ber", "nur", "in", "schlaf\u00b7fen", "A\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "und unerfrischt, so kroch ich aus den Federn.", "tokens": ["und", "un\u00b7er\u00b7frischt", ",", "so", "kroch", "ich", "aus", "den", "Fe\u00b7dern", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.29": {"line.1": {"text": "Pardon! \u2013 Da zeigt sichs wieder mal frappant,", "tokens": ["Par\u00b7don", "!", "\u2013", "Da", "zeigt", "sichs", "wie\u00b7der", "mal", "frap\u00b7pant", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "ADV", "VVFIN", "PIS", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "wie stetiger Gebrauch gebrauchter Worte", "tokens": ["wie", "ste\u00b7ti\u00b7ger", "Ge\u00b7brauch", "ge\u00b7brauch\u00b7ter", "Wor\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "uns Sinn und Inhalt ganz vergessen macht.", "tokens": ["uns", "Sinn", "und", "In\u00b7halt", "ganz", "ver\u00b7ges\u00b7sen", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "ADV", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mechanisch kauen wir die leeren H\u00fclsen:", "tokens": ["Me\u00b7cha\u00b7nisch", "kau\u00b7en", "wir", "die", "lee\u00b7ren", "H\u00fcl\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "hohl bleibt der Kopf und hungrig das Gem\u00fcth.", "tokens": ["hohl", "bleibt", "der", "Kopf", "und", "hung\u00b7rig", "das", "Ge\u00b7m\u00fcth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "KON", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.30": {"line.1": {"text": "So sagt ich denn, ich kr\u00f6che aus den Federn.", "tokens": ["So", "sagt", "ich", "denn", ",", "ich", "kr\u00f6\u00b7che", "aus", "den", "Fe\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich Schuft! Stroh war es, Stroh und dreimal Stroh!", "tokens": ["Ich", "Schuft", "!", "Stroh", "war", "es", ",", "Stroh", "und", "drei\u00b7mal", "Stroh", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$.", "NN", "VAFIN", "PPER", "$,", "NN", "KON", "ADV", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.31": {"line.1": {"text": "Die Sprache, die des Wortes Werth nicht kennt,", "tokens": ["Die", "Spra\u00b7che", ",", "die", "des", "Wor\u00b7tes", "Werth", "nicht", "kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "ADJA", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "der die Begriffe h\u00f6her gelten nicht", "tokens": ["der", "die", "Be\u00b7grif\u00b7fe", "h\u00f6\u00b7her", "gel\u00b7ten", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ADJD", "VVFIN", "PTKNEG"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "als schmutzige Karten in des Spielers Hand,", "tokens": ["als", "schmut\u00b7zi\u00b7ge", "Kar\u00b7ten", "in", "des", "Spie\u00b7lers", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "bald tr\u00e4g, bald wuchtig auf den Tisch geworfen,", "tokens": ["bald", "tr\u00e4g", ",", "bald", "wuch\u00b7tig", "auf", "den", "Tisch", "ge\u00b7wor\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "die Sprache, der das Blut der Sinne schwand,", "tokens": ["die", "Spra\u00b7che", ",", "der", "das", "Blut", "der", "Sin\u00b7ne", "schwand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "und deren Bl\u00e4sse Schminke nur verdeckt \u2013", "tokens": ["und", "de\u00b7ren", "Bl\u00e4s\u00b7se", "Schmin\u00b7ke", "nur", "ver\u00b7deckt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRELAT", "NN", "VVFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ins Grab mit ihr \u2013 sie hat zu lang gelebt \u2013", "tokens": ["ins", "Grab", "mit", "ihr", "\u2013", "sie", "hat", "zu", "lang", "ge\u00b7lebt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "PPOSAT", "$(", "PPER", "VAFIN", "PTKA", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "bringt sie den Schinderknechten auf den Anger,", "tokens": ["bringt", "sie", "den", "Schin\u00b7der\u00b7knech\u00b7ten", "auf", "den", "An\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.9": {"text": "den Oberlehrern und den Professoren! \u2013", "tokens": ["den", "O\u00b7ber\u00b7leh\u00b7rern", "und", "den", "Pro\u00b7fes\u00b7so\u00b7ren", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.32": {"line.1": {"text": "Ein H\u00e4mmern weckte mich, denn Fahnenweihe", "tokens": ["Ein", "H\u00e4m\u00b7mern", "weck\u00b7te", "mich", ",", "denn", "Fah\u00b7nen\u00b7wei\u00b7he"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "war heut in Stockheim: der Verein der Krieger", "tokens": ["war", "heut", "in", "Stock\u00b7heim", ":", "der", "Ver\u00b7ein", "der", "Krie\u00b7ger"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "NN", "$.", "ART", "NN", "ART", "NN"], "meter": "-+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "betrank sich treu f\u00fcr Gott und Vaterland,", "tokens": ["be\u00b7trank", "sich", "treu", "f\u00fcr", "Gott", "und", "Va\u00b7ter\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADJD", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "betrank sich fest, um seiner neuen Fahne", "tokens": ["be\u00b7trank", "sich", "fest", ",", "um", "sei\u00b7ner", "neu\u00b7en", "Fah\u00b7ne"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "PTKVZ", "$,", "KOUI", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "f\u00fcr alle Zukunft echten Glanz zu geben.", "tokens": ["f\u00fcr", "al\u00b7le", "Zu\u00b7kunft", "ech\u00b7ten", "Glanz", "zu", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Daher die Flaggen, daher diese Kr\u00e4nze ...", "tokens": ["Da\u00b7her", "die", "Flag\u00b7gen", ",", "da\u00b7her", "die\u00b7se", "Kr\u00e4n\u00b7ze", "..."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "$,", "PAV", "PDAT", "NN", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Schweig still, mein tiefbesch\u00e4mtes Dichterherz!", "tokens": ["Schweig", "still", ",", "mein", "tief\u00b7be\u00b7sch\u00e4m\u00b7tes", "Dich\u00b7ter\u00b7herz", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.33": {"line.1": {"text": "Und eine Nothdurft trat an mich heran,", "tokens": ["Und", "ei\u00b7ne", "Noth\u00b7durft", "trat", "an", "mich", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "APPR", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "zwang mich, die Kammer schleunigst zu verlassen.", "tokens": ["zwang", "mich", ",", "die", "Kam\u00b7mer", "schleu\u00b7nigst", "zu", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ART", "NN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Die Speisen, die der Mensch, wie jedes Thier,", "tokens": ["Die", "Spei\u00b7sen", ",", "die", "der", "Mensch", ",", "wie", "je\u00b7des", "Thier", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "$,", "PWAV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "um seinen Leib zu n\u00e4hren zu sich nimmt,", "tokens": ["um", "sei\u00b7nen", "Leib", "zu", "n\u00e4h\u00b7ren", "zu", "sich", "nimmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "beh\u00e4lt er nicht in vollem Umfang bei sich.", "tokens": ["be\u00b7h\u00e4lt", "er", "nicht", "in", "vol\u00b7lem", "Um\u00b7fang", "bei", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "APPR", "ADJA", "NN", "APPR", "PRF", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Bef\u00e4higt ist der K\u00f6rper, was da werthvoll", "tokens": ["Be\u00b7f\u00e4\u00b7higt", "ist", "der", "K\u00f6r\u00b7per", ",", "was", "da", "werth\u00b7voll"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "ART", "NN", "$,", "PRELS", "ADV", "ADJD"], "meter": "-+-+-+--+--", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "von dem, was minder wichtig, wohl zu sichten:", "tokens": ["von", "dem", ",", "was", "min\u00b7der", "wich\u00b7tig", ",", "wohl", "zu", "sich\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "ADV", "ADJD", "$,", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "das erstere nimmt er voll Schlauheit auf", "tokens": ["das", "ers\u00b7te\u00b7re", "nimmt", "er", "voll", "Schlau\u00b7heit", "auf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VVFIN", "PPER", "ADJD", "NN", "APPR"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.9": {"text": "und mit Bedacht ausscheidet er das andre.", "tokens": ["und", "mit", "Be\u00b7dacht", "aus\u00b7schei\u00b7det", "er", "das", "and\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVFIN", "PPER", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.34": {"line.1": {"text": "Auch mir ist dieses Menschliche nicht fremd.", "tokens": ["Auch", "mir", "ist", "die\u00b7ses", "Menschli\u00b7che", "nicht", "fremd", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PDAT", "NN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und als ich nun mit kindlich offner Seele", "tokens": ["Und", "als", "ich", "nun", "mit", "kind\u00b7lich", "off\u00b7ner", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "die alte schmutzbetriefte Pflegerin", "tokens": ["die", "al\u00b7te", "schmutz\u00b7be\u00b7trief\u00b7te", "Pfle\u00b7ge\u00b7rin"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "anging um einer Klause keusche Wohlthat,", "tokens": ["an\u00b7ging", "um", "ei\u00b7ner", "Klau\u00b7se", "keu\u00b7sche", "Wohlt\u00b7hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "wies sie mit unverst\u00e4ndlichem Gebrumm", "tokens": ["wies", "sie", "mit", "un\u00b7ver\u00b7st\u00e4nd\u00b7li\u00b7chem", "Ge\u00b7brumm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "mich aus der Hinterth\u00fcr und auf den Hof,", "tokens": ["mich", "aus", "der", "Hin\u00b7tert\u00b7h\u00fcr", "und", "auf", "den", "Hof", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "KON", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "wo goldnen Mist die fr\u00fche Sonne kr\u00f6nte.", "tokens": ["wo", "gold\u00b7nen", "Mist", "die", "fr\u00fc\u00b7he", "Son\u00b7ne", "kr\u00f6n\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.35": {"line.1": {"text": "Nachdem ich lange dort mich umgethan", "tokens": ["Nach\u00b7dem", "ich", "lan\u00b7ge", "dort", "mich", "um\u00b7ge\u00b7than"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPER", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "und hinter jede Bretterth\u00fcr gesp\u00e4ht,", "tokens": ["und", "hin\u00b7ter", "je\u00b7de", "Bret\u00b7tert\u00b7h\u00fcr", "ge\u00b7sp\u00e4ht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und hinter jeder nur \u2013 beliebte Thiere,", "tokens": ["und", "hin\u00b7ter", "je\u00b7der", "nur", "\u2013", "be\u00b7lieb\u00b7te", "Thie\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "PIS", "ADV", "$(", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "doch nie den trauten Sitz gefunden hatte \u2013", "tokens": ["doch", "nie", "den", "trau\u00b7ten", "Sitz", "ge\u00b7fun\u00b7den", "hat\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "da d\u00e4mmerte in meiner zagen Seele", "tokens": ["da", "d\u00e4m\u00b7mer\u00b7te", "in", "mei\u00b7ner", "za\u00b7gen", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "ein ungewollt begl\u00fcckender Gedanke.", "tokens": ["ein", "un\u00b7ge\u00b7wollt", "be\u00b7gl\u00fc\u00b7cken\u00b7der", "Ge\u00b7dan\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.36": {"line.1": {"text": "Natur! so rief ich, ewige heilige Mutter,", "tokens": ["Na\u00b7tur", "!", "so", "rief", "ich", ",", "e\u00b7wi\u00b7ge", "hei\u00b7li\u00b7ge", "Mut\u00b7ter", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "ADV", "VVFIN", "PPER", "$,", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "du ziehst den Halbverlornen machtvoll an!", "tokens": ["du", "ziehst", "den", "Halb\u00b7ver\u00b7lor\u00b7nen", "macht\u00b7voll", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Das trotzige \u2013 das reuevolle Kind", "tokens": ["Das", "trot\u00b7zi\u00b7ge", "\u2013", "das", "reu\u00b7e\u00b7vol\u00b7le", "Kind"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "ziehst du aufs neue sanft in deinen Schoss!", "tokens": ["ziehst", "du", "aufs", "neu\u00b7e", "sanft", "in", "dei\u00b7nen", "Schoss", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "ADJA", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "O dank, du gute, liebevolle Mutter!", "tokens": ["O", "dank", ",", "du", "gu\u00b7te", ",", "lie\u00b7be\u00b7vol\u00b7le", "Mut\u00b7ter", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "PPER", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.37": {"line.1": {"text": "So str\u00f6mten die Gef\u00fchle brausend \u00fcber ...", "tokens": ["So", "str\u00f6m\u00b7ten", "die", "Ge\u00b7f\u00fch\u00b7le", "brau\u00b7send", "\u00fc\u00b7ber", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "APPR", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Tief in mir klang es wie ein heilger Schwur:", "tokens": ["Tief", "in", "mir", "klang", "es", "wie", "ein", "heil\u00b7ger", "Schwur", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "VVFIN", "PPER", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Auf deinem Pfade will ich f\u00fcrder wandeln,", "tokens": ["Auf", "dei\u00b7nem", "Pfa\u00b7de", "will", "ich", "f\u00fcr\u00b7der", "wan\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "dir ewig folgen, Herrscherin Natur! \u2013", "tokens": ["dir", "e\u00b7wig", "fol\u00b7gen", ",", "Herr\u00b7sche\u00b7rin", "Na\u00b7tur", "!", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "ADJD", "VVINF", "$,", "NN", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Kein Machtgebot verirrter Menschen soll", "tokens": ["Kein", "Macht\u00b7ge\u00b7bot", "ver\u00b7irr\u00b7ter", "Men\u00b7schen", "soll"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "ADJA", "NN", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "entfernen mich von dir und meinem Eide:", "tokens": ["ent\u00b7fer\u00b7nen", "mich", "von", "dir", "und", "mei\u00b7nem", "Ei\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPER", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "mein Leib ist dein und f\u00fcrder meine Seele", "tokens": ["mein", "Leib", "ist", "dein", "und", "f\u00fcr\u00b7der", "mei\u00b7ne", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "PPOSAT", "KON", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "denn beide sind ", "tokens": ["denn", "bei\u00b7de", "sind"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIS", "VAFIN"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.38": {"line.1": {"text": "Und langsam \u2013 und erleichtert reckt ich mich", "tokens": ["Und", "lang\u00b7sam", "\u2013", "und", "er\u00b7leich\u00b7tert", "reckt", "ich", "mich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "$(", "KON", "VVPP", "VVFIN", "PPER", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "nach solchem tiefentquollnen Schwur empor,", "tokens": ["nach", "sol\u00b7chem", "tie\u00b7fent\u00b7quoll\u00b7nen", "Schwur", "em\u00b7por", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und dehnte mich und streckte meine Glieder.", "tokens": ["und", "dehn\u00b7te", "mich", "und", "streck\u00b7te", "mei\u00b7ne", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und selbstbewusst und h\u00f6chst vertraulich nickt ich", "tokens": ["Und", "selbst\u00b7be\u00b7wusst", "und", "h\u00f6chst", "ver\u00b7trau\u00b7lich", "nickt", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "KON", "ADV", "ADJD", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "der jungen Sonne zu, die frisch und blank", "tokens": ["der", "jun\u00b7gen", "Son\u00b7ne", "zu", ",", "die", "frisch", "und", "blank"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$,", "PRELS", "ADJD", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "dort auf dem dunklen Fichtenwalde lag \u2013", "tokens": ["dort", "auf", "dem", "dunk\u00b7len", "Fich\u00b7ten\u00b7wal\u00b7de", "lag", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ein nacktes Weib auf einer B\u00e4renhaut \u2013:", "tokens": ["ein", "nack\u00b7tes", "Weib", "auf", "ei\u00b7ner", "B\u00e4\u00b7ren\u00b7haut", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$(", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Sch\u00f6n guten Morgen \u2013 hast du ausgeschlafen?", "tokens": ["Sch\u00f6n", "gu\u00b7ten", "Mor\u00b7gen", "\u2013", "hast", "du", "aus\u00b7ge\u00b7schla\u00b7fen", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$(", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}