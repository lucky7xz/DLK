{"textgrid.poem.41360": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Der Wolf und das Pferd", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein matter Wolf voll Nahrungssorgen", "tokens": ["Ein", "mat\u00b7ter", "Wolf", "voll", "Nah\u00b7rungs\u00b7sor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "ADJD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Betrat an einem Fr\u00fchlingsmorgen", "tokens": ["Be\u00b7trat", "an", "ei\u00b7nem", "Fr\u00fch\u00b7lings\u00b7mor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der fetten Anger feuchtes Gr\u00fcn.", "tokens": ["Der", "fet\u00b7ten", "An\u00b7ger", "feuch\u00b7tes", "Gr\u00fcn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da sah er mit erw\u00fcnschten Freuden", "tokens": ["Da", "sah", "er", "mit", "er\u00b7w\u00fcnschten", "Freu\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Ein wohlbefleischtes F\u00fcllen weiden,", "tokens": ["Ein", "wohl\u00b7be\u00b7fleischtes", "F\u00fcl\u00b7len", "wei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Das seinem Hunger reizend schien.", "tokens": ["Das", "sei\u00b7nem", "Hun\u00b7ger", "rei\u00b7zend", "schien", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Er hatte gro\u00dfe Lust zur Beute;", "tokens": ["Er", "hat\u00b7te", "gro\u00b7\u00dfe", "Lust", "zur", "Beu\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nur da\u00df er jeden Gegner scheute,", "tokens": ["Nur", "da\u00df", "er", "je\u00b7den", "Geg\u00b7ner", "scheu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der st\u00e4rker war, als Lamm und Schaf.", "tokens": ["Der", "st\u00e4r\u00b7ker", "war", ",", "als", "Lamm", "und", "Schaf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VAFIN", "$,", "KOUS", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Drum sollt' es ihm durch List gelingen,", "tokens": ["Drum", "sollt'", "es", "ihm", "durch", "List", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Den jungen Streiter zu bezwingen,", "tokens": ["Den", "jun\u00b7gen", "Strei\u00b7ter", "zu", "be\u00b7zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der an Gewalt ihn \u00fcbertraf.", "tokens": ["Der", "an", "Ge\u00b7walt", "ihn", "\u00fc\u00b7bert\u00b7raf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Er n\u00e4hert sich dem stolzen Pferde:", "tokens": ["Er", "n\u00e4\u00b7hert", "sich", "dem", "stol\u00b7zen", "Pfer\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "ADJA", "NN", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Er schw\u00f6rt, da\u00df auf der ganzen Erde", "tokens": ["Er", "schw\u00f6rt", ",", "da\u00df", "auf", "der", "gan\u00b7zen", "Er\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kein Wurzelmann ihm \u00e4hnlich sei.", "tokens": ["Kein", "Wur\u00b7zel\u00b7mann", "ihm", "\u00e4hn\u00b7lich", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erhabner Houyhnhnm,", "tokens": ["Er\u00b7hab\u00b7ner", "Hou\u00b7yhnhnm", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$,"], "meter": "-+---", "measure": "dactylic.init"}, "line.5": {"text": "Ich kenne Stauden, Pflanzen, Kr\u00e4uter,", "tokens": ["Ich", "ken\u00b7ne", "Stau\u00b7den", ",", "Pflan\u00b7zen", ",", "Kr\u00e4u\u00b7ter", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Von hier bis in die Tartarei.", "tokens": ["Von", "hier", "bis", "in", "die", "Tar\u00b7ta\u00b7rei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich kann den Kranken Hilf' ertheilen,", "tokens": ["Ich", "kann", "den", "Kran\u00b7ken", "Hil\u00b7f'", "er\u00b7thei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "NE", "VVINF", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spatt, Kropf, Geschwulst, und alles heilen,", "tokens": ["Spatt", ",", "Kropf", ",", "Ge\u00b7schwulst", ",", "und", "al\u00b7les", "hei\u00b7len", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "KON", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Dem andrer Helfer Rath gebricht.", "tokens": ["Dem", "an\u00b7drer", "Hel\u00b7fer", "Rath", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir m\u00fcssen Krampf und W\u00fcrmer weichen;", "tokens": ["Mir", "m\u00fcs\u00b7sen", "Krampf", "und", "W\u00fcr\u00b7mer", "wei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Den Koller wei\u00df ich wegzuscheuchen!", "tokens": ["Den", "Kol\u00b7ler", "wei\u00df", "ich", "weg\u00b7zu\u00b7scheu\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und was versteh' ich sonsten nicht!", "tokens": ["Und", "was", "ver\u00b7steh'", "ich", "sons\u00b7ten", "nicht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "ADV", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Jetzt bin ich darum hier erschienen,", "tokens": ["Jetzt", "bin", "ich", "da\u00b7rum", "hier", "er\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit meiner Wissenschaft zu dienen;", "tokens": ["Mit", "mei\u00b7ner", "Wis\u00b7sen\u00b7schaft", "zu", "die\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn ihnen diese rathen kann.", "tokens": ["Wenn", "ih\u00b7nen", "die\u00b7se", "ra\u00b7then", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie gehn zu frei, zu rasch im Felde:", "tokens": ["Sie", "gehn", "zu", "frei", ",", "zu", "rasch", "im", "Fel\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKA", "ADJD", "$,", "PTKA", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dies zeigt, da\u00df ich die Wahrheit melde,", "tokens": ["Dies", "zeigt", ",", "da\u00df", "ich", "die", "Wahr\u00b7heit", "mel\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Uns Aerzten nicht viel Gutes an.", "tokens": ["Uns", "A\u00b7erz\u00b7ten", "nicht", "viel", "Gu\u00b7tes", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "PTKNEG", "PIAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "D\u00fcrft' ich, weil sie zu sehr sich regen,", "tokens": ["D\u00fcrft'", "ich", ",", "weil", "sie", "zu", "sehr", "sich", "re\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "KOUS", "PPER", "PTKA", "ADV", "PRF", "ADJA", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Ein Band um ihre Schenkel legen,", "tokens": ["Ein", "Band", "um", "ih\u00b7re", "Schen\u00b7kel", "le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Gewi\u00df, sie sollten Wunder sehn.", "tokens": ["Ge\u00b7wi\u00df", ",", "sie", "soll\u00b7ten", "Wun\u00b7der", "sehn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich fordre nichts f\u00fcr Cur und M\u00fche,", "tokens": ["Ich", "ford\u00b7re", "nichts", "f\u00fcr", "Cur", "und", "M\u00fc\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Weil ich den Geiz vor allem fliehe;", "tokens": ["Weil", "ich", "den", "Geiz", "vor", "al\u00b7lem", "flie\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die Heilung soll umsonst geschehn.", "tokens": ["Die", "Hei\u00b7lung", "soll", "um\u00b7sonst", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Das F\u00fcllen dankt ihm, und versetzet:", "tokens": ["Das", "F\u00fcl\u00b7len", "dankt", "ihm", ",", "und", "ver\u00b7set\u00b7zet", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich habe mich am Huf verletzet,", "tokens": ["Ich", "ha\u00b7be", "mich", "am", "Huf", "ver\u00b7let\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und sp\u00fcre dort die schwerste Pein.", "tokens": ["Und", "sp\u00fc\u00b7re", "dort", "die", "schwers\u00b7te", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Herr Doktor! kommt, beseht den Schaden,", "tokens": ["Herr", "Dok\u00b7tor", "!", "kommt", ",", "be\u00b7seht", "den", "Scha\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$.", "VVFIN", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "K\u00f6nnt ihr der Schmerzen mich entladen?", "tokens": ["K\u00f6nnt", "ihr", "der", "Schmer\u00b7zen", "mich", "ent\u00b7la\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nichts, spricht der Wolf, wird leichter sein.", "tokens": ["Nichts", ",", "spricht", "der", "Wolf", ",", "wird", "leich\u00b7ter", "sein", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "VVFIN", "ART", "NE", "$,", "VAFIN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Er will auch keine Zeit verlieren,", "tokens": ["Er", "will", "auch", "kei\u00b7ne", "Zeit", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und stellt, den Anschlag auszuf\u00fchren,", "tokens": ["Und", "stellt", ",", "den", "An\u00b7schlag", "aus\u00b7zu\u00b7f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sich unverz\u00fcglich hinters Pferd.", "tokens": ["Sich", "un\u00b7ver\u00b7z\u00fcg\u00b7lich", "hin\u00b7ters", "Pferd", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das will, aus gleichgeschwinden Pflichten,", "tokens": ["Das", "will", ",", "aus", "gleich\u00b7ge\u00b7schwin\u00b7den", "Pflich\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ihm zum voraus den Lohn entrichten;", "tokens": ["Ihm", "zum", "vo\u00b7raus", "den", "Lohn", "ent\u00b7rich\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPRART", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Arzt ist seines Lohnes werth.", "tokens": ["Ein", "Arzt", "ist", "sei\u00b7nes", "Loh\u00b7nes", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Der Houyhnhnm sucht ihn klug zu machen,", "tokens": ["Der", "Hou\u00b7yhnhnm", "sucht", "ihn", "klug", "zu", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schl\u00e4gt aus, zerquetscht des Wolfes Rachen,", "tokens": ["Schl\u00e4gt", "aus", ",", "zer\u00b7quetscht", "des", "Wol\u00b7fes", "Ra\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und wiehert ihm die Worte zu:", "tokens": ["Und", "wie\u00b7hert", "ihm", "die", "Wor\u00b7te", "zu", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Nichts gibt ein gr\u00f6\u00dferes Vergn\u00fcgen,", "tokens": ["Nichts", "gibt", "ein", "gr\u00f6\u00b7\u00dfe\u00b7res", "Ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Als den Betr\u00fcger zu betr\u00fcgen;", "tokens": ["Als", "den", "Be\u00b7tr\u00fc\u00b7ger", "zu", "be\u00b7tr\u00fc\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Freund! das beweisen ich und du.", "tokens": ["Freund", "!", "das", "be\u00b7wei\u00b7sen", "ich", "und", "du", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PDS", "VVFIN", "PPER", "KON", "PPER", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Ein matter Wolf voll Nahrungssorgen", "tokens": ["Ein", "mat\u00b7ter", "Wolf", "voll", "Nah\u00b7rungs\u00b7sor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "ADJD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Betrat an einem Fr\u00fchlingsmorgen", "tokens": ["Be\u00b7trat", "an", "ei\u00b7nem", "Fr\u00fch\u00b7lings\u00b7mor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der fetten Anger feuchtes Gr\u00fcn.", "tokens": ["Der", "fet\u00b7ten", "An\u00b7ger", "feuch\u00b7tes", "Gr\u00fcn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da sah er mit erw\u00fcnschten Freuden", "tokens": ["Da", "sah", "er", "mit", "er\u00b7w\u00fcnschten", "Freu\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Ein wohlbefleischtes F\u00fcllen weiden,", "tokens": ["Ein", "wohl\u00b7be\u00b7fleischtes", "F\u00fcl\u00b7len", "wei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Das seinem Hunger reizend schien.", "tokens": ["Das", "sei\u00b7nem", "Hun\u00b7ger", "rei\u00b7zend", "schien", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Er hatte gro\u00dfe Lust zur Beute;", "tokens": ["Er", "hat\u00b7te", "gro\u00b7\u00dfe", "Lust", "zur", "Beu\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nur da\u00df er jeden Gegner scheute,", "tokens": ["Nur", "da\u00df", "er", "je\u00b7den", "Geg\u00b7ner", "scheu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der st\u00e4rker war, als Lamm und Schaf.", "tokens": ["Der", "st\u00e4r\u00b7ker", "war", ",", "als", "Lamm", "und", "Schaf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VAFIN", "$,", "KOUS", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Drum sollt' es ihm durch List gelingen,", "tokens": ["Drum", "sollt'", "es", "ihm", "durch", "List", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Den jungen Streiter zu bezwingen,", "tokens": ["Den", "jun\u00b7gen", "Strei\u00b7ter", "zu", "be\u00b7zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der an Gewalt ihn \u00fcbertraf.", "tokens": ["Der", "an", "Ge\u00b7walt", "ihn", "\u00fc\u00b7bert\u00b7raf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Er n\u00e4hert sich dem stolzen Pferde:", "tokens": ["Er", "n\u00e4\u00b7hert", "sich", "dem", "stol\u00b7zen", "Pfer\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "ADJA", "NN", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Er schw\u00f6rt, da\u00df auf der ganzen Erde", "tokens": ["Er", "schw\u00f6rt", ",", "da\u00df", "auf", "der", "gan\u00b7zen", "Er\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kein Wurzelmann ihm \u00e4hnlich sei.", "tokens": ["Kein", "Wur\u00b7zel\u00b7mann", "ihm", "\u00e4hn\u00b7lich", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erhabner Houyhnhnm,", "tokens": ["Er\u00b7hab\u00b7ner", "Hou\u00b7yhnhnm", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$,"], "meter": "-+---", "measure": "dactylic.init"}, "line.5": {"text": "Ich kenne Stauden, Pflanzen, Kr\u00e4uter,", "tokens": ["Ich", "ken\u00b7ne", "Stau\u00b7den", ",", "Pflan\u00b7zen", ",", "Kr\u00e4u\u00b7ter", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Von hier bis in die Tartarei.", "tokens": ["Von", "hier", "bis", "in", "die", "Tar\u00b7ta\u00b7rei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ich kann den Kranken Hilf' ertheilen,", "tokens": ["Ich", "kann", "den", "Kran\u00b7ken", "Hil\u00b7f'", "er\u00b7thei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "NE", "VVINF", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spatt, Kropf, Geschwulst, und alles heilen,", "tokens": ["Spatt", ",", "Kropf", ",", "Ge\u00b7schwulst", ",", "und", "al\u00b7les", "hei\u00b7len", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "KON", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Dem andrer Helfer Rath gebricht.", "tokens": ["Dem", "an\u00b7drer", "Hel\u00b7fer", "Rath", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir m\u00fcssen Krampf und W\u00fcrmer weichen;", "tokens": ["Mir", "m\u00fcs\u00b7sen", "Krampf", "und", "W\u00fcr\u00b7mer", "wei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Den Koller wei\u00df ich wegzuscheuchen!", "tokens": ["Den", "Kol\u00b7ler", "wei\u00df", "ich", "weg\u00b7zu\u00b7scheu\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und was versteh' ich sonsten nicht!", "tokens": ["Und", "was", "ver\u00b7steh'", "ich", "sons\u00b7ten", "nicht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "ADV", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Jetzt bin ich darum hier erschienen,", "tokens": ["Jetzt", "bin", "ich", "da\u00b7rum", "hier", "er\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit meiner Wissenschaft zu dienen;", "tokens": ["Mit", "mei\u00b7ner", "Wis\u00b7sen\u00b7schaft", "zu", "die\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn ihnen diese rathen kann.", "tokens": ["Wenn", "ih\u00b7nen", "die\u00b7se", "ra\u00b7then", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie gehn zu frei, zu rasch im Felde:", "tokens": ["Sie", "gehn", "zu", "frei", ",", "zu", "rasch", "im", "Fel\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKA", "ADJD", "$,", "PTKA", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dies zeigt, da\u00df ich die Wahrheit melde,", "tokens": ["Dies", "zeigt", ",", "da\u00df", "ich", "die", "Wahr\u00b7heit", "mel\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Uns Aerzten nicht viel Gutes an.", "tokens": ["Uns", "A\u00b7erz\u00b7ten", "nicht", "viel", "Gu\u00b7tes", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "PTKNEG", "PIAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.15": {"line.1": {"text": "D\u00fcrft' ich, weil sie zu sehr sich regen,", "tokens": ["D\u00fcrft'", "ich", ",", "weil", "sie", "zu", "sehr", "sich", "re\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "KOUS", "PPER", "PTKA", "ADV", "PRF", "ADJA", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Ein Band um ihre Schenkel legen,", "tokens": ["Ein", "Band", "um", "ih\u00b7re", "Schen\u00b7kel", "le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Gewi\u00df, sie sollten Wunder sehn.", "tokens": ["Ge\u00b7wi\u00df", ",", "sie", "soll\u00b7ten", "Wun\u00b7der", "sehn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich fordre nichts f\u00fcr Cur und M\u00fche,", "tokens": ["Ich", "ford\u00b7re", "nichts", "f\u00fcr", "Cur", "und", "M\u00fc\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Weil ich den Geiz vor allem fliehe;", "tokens": ["Weil", "ich", "den", "Geiz", "vor", "al\u00b7lem", "flie\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die Heilung soll umsonst geschehn.", "tokens": ["Die", "Hei\u00b7lung", "soll", "um\u00b7sonst", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Das F\u00fcllen dankt ihm, und versetzet:", "tokens": ["Das", "F\u00fcl\u00b7len", "dankt", "ihm", ",", "und", "ver\u00b7set\u00b7zet", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich habe mich am Huf verletzet,", "tokens": ["Ich", "ha\u00b7be", "mich", "am", "Huf", "ver\u00b7let\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und sp\u00fcre dort die schwerste Pein.", "tokens": ["Und", "sp\u00fc\u00b7re", "dort", "die", "schwers\u00b7te", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Herr Doktor! kommt, beseht den Schaden,", "tokens": ["Herr", "Dok\u00b7tor", "!", "kommt", ",", "be\u00b7seht", "den", "Scha\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$.", "VVFIN", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "K\u00f6nnt ihr der Schmerzen mich entladen?", "tokens": ["K\u00f6nnt", "ihr", "der", "Schmer\u00b7zen", "mich", "ent\u00b7la\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nichts, spricht der Wolf, wird leichter sein.", "tokens": ["Nichts", ",", "spricht", "der", "Wolf", ",", "wird", "leich\u00b7ter", "sein", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "VVFIN", "ART", "NE", "$,", "VAFIN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Er will auch keine Zeit verlieren,", "tokens": ["Er", "will", "auch", "kei\u00b7ne", "Zeit", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und stellt, den Anschlag auszuf\u00fchren,", "tokens": ["Und", "stellt", ",", "den", "An\u00b7schlag", "aus\u00b7zu\u00b7f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sich unverz\u00fcglich hinters Pferd.", "tokens": ["Sich", "un\u00b7ver\u00b7z\u00fcg\u00b7lich", "hin\u00b7ters", "Pferd", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das will, aus gleichgeschwinden Pflichten,", "tokens": ["Das", "will", ",", "aus", "gleich\u00b7ge\u00b7schwin\u00b7den", "Pflich\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ihm zum voraus den Lohn entrichten;", "tokens": ["Ihm", "zum", "vo\u00b7raus", "den", "Lohn", "ent\u00b7rich\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPRART", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Arzt ist seines Lohnes werth.", "tokens": ["Ein", "Arzt", "ist", "sei\u00b7nes", "Loh\u00b7nes", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Der Houyhnhnm sucht ihn klug zu machen,", "tokens": ["Der", "Hou\u00b7yhnhnm", "sucht", "ihn", "klug", "zu", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schl\u00e4gt aus, zerquetscht des Wolfes Rachen,", "tokens": ["Schl\u00e4gt", "aus", ",", "zer\u00b7quetscht", "des", "Wol\u00b7fes", "Ra\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und wiehert ihm die Worte zu:", "tokens": ["Und", "wie\u00b7hert", "ihm", "die", "Wor\u00b7te", "zu", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Nichts gibt ein gr\u00f6\u00dferes Vergn\u00fcgen,", "tokens": ["Nichts", "gibt", "ein", "gr\u00f6\u00b7\u00dfe\u00b7res", "Ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Als den Betr\u00fcger zu betr\u00fcgen;", "tokens": ["Als", "den", "Be\u00b7tr\u00fc\u00b7ger", "zu", "be\u00b7tr\u00fc\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Freund! das beweisen ich und du.", "tokens": ["Freund", "!", "das", "be\u00b7wei\u00b7sen", "ich", "und", "du", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PDS", "VVFIN", "PPER", "KON", "PPER", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}}}}