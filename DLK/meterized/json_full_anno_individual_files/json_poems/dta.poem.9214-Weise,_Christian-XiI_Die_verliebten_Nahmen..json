{"dta.poem.9214": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "XiI  \n Die verliebten Nahmen.", "genre": "Lyrik; Drama; Prosa", "period": "N.A.", "pub_year": "1701", "urn": "urn:nbn:de:kobv:b4-25043-0", "language": ["de:0.99"], "booktitle": "Weise, Christian: \u00dcberfl\u00fc\u00dfige Gedancken Der gr\u00fcnenden jugend. Leipzig, 1701."}, "poem": {"stanza.1": {"line.1": {"text": "Nunmehr bedarff sie einen nahmen", "tokens": ["Nun\u00b7mehr", "be\u00b7darff", "sie", "ei\u00b7nen", "nah\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den sie dem neuen liebsten gibt/", "tokens": ["Den", "sie", "dem", "neu\u00b7en", "liebs\u00b7ten", "gibt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dann allesheist doch ja und amen/", "tokens": ["Dann", "al\u00b7les\u00b7heist", "doch", "ja", "und", "a\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "KON", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Uud was sie denckt/ das ist verliebt/", "tokens": ["U\u00b7ud", "was", "sie", "denckt", "/", "das", "ist", "ver\u00b7liebt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$(", "PDS", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Der halben mu\u00df ein wort allein", "tokens": ["Der", "hal\u00b7ben", "mu\u00df", "ein", "wort", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VMFIN", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der neuen freundschafft zeuge seyn.", "tokens": ["Der", "neu\u00b7en", "freund\u00b7schafft", "zeu\u00b7ge", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "2. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Vor diesem wars ein h\u00fcbsches wort:", "tokens": ["Vor", "die\u00b7sem", "wars", "ein", "h\u00fcb\u00b7sches", "wort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein liebgen schickt sich auf die freythe/", "tokens": ["Mein", "lieb\u00b7gen", "schickt", "sich", "auf", "die", "frey\u00b7the", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVFIN", "PRF", "APPR", "ART", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch nach der hochzeit mu\u00df es fort:", "tokens": ["Doch", "nach", "der", "hoch\u00b7zeit", "mu\u00df", "es", "fort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VMFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein sch\u00e4tzgen taugt f\u00fcrwar nicht viel/", "tokens": ["Mein", "sch\u00e4tz\u00b7gen", "taugt", "f\u00fcr\u00b7war", "nicht", "viel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVFIN", "ADV", "PTKNEG", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Als wann ich sie vexieren wil.", "tokens": ["Als", "wann", "ich", "sie", "ve\u00b7xie\u00b7ren", "wil", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "PPER", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "3. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Als wohl den lieben m\u00e4dgen an:", "tokens": ["Als", "wohl", "den", "lie\u00b7ben", "m\u00e4d\u00b7gen", "an", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein schneutzgen/ das bedarff ein brieffgen/", "tokens": ["Mein", "schneutz\u00b7gen", "/", "das", "be\u00b7darff", "ein", "brieff\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "$(", "PDS", "VVFIN", "ART", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df man es recht verstehen kan:", "tokens": ["Da\u00df", "man", "es", "recht", "ver\u00b7ste\u00b7hen", "kan", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein engel trifft gar selten ein/", "tokens": ["Mein", "en\u00b7gel", "trifft", "gar", "sel\u00b7ten", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "ADJD", "ART", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dann niemand wil der fl\u00fcgel seyn.", "tokens": ["Dann", "nie\u00b7mand", "wil", "der", "fl\u00fc\u00b7gel", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VMFIN", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "4. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Wann knecht und m\u00e4gd beysammen sind:", "tokens": ["Wann", "knecht", "und", "m\u00e4gd", "bey\u00b7sam\u00b7men", "sind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "KON", "ADJD", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein Vatter k\u00f6mmt zu ehrenveste/", "tokens": ["Mein", "Vat\u00b7ter", "k\u00f6mmt", "zu", "eh\u00b7ren\u00b7ves\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und reimt sich auff kein junges kind:", "tokens": ["Und", "reimt", "sich", "auff", "kein", "jun\u00b7ges", "kind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein zuckerbild/ mein nelcken-strau\u00df/", "tokens": ["Mein", "zu\u00b7cker\u00b7bild", "/", "mein", "nel\u00b7cken\u00b7strau\u00df", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das sieht mir so poetisch au\u00df.", "tokens": ["Das", "sieht", "mir", "so", "po\u00b7e\u00b7tisch", "au\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "5. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Wann man es nicht zu offte sagt:", "tokens": ["Wann", "man", "es", "nicht", "zu", "off\u00b7te", "sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPER", "PTKNEG", "PTKZU", "VVFIN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein seelgen l\u00e4st sich fein mit schweren/", "tokens": ["Mein", "seel\u00b7gen", "l\u00e4st", "sich", "fein", "mit", "schwe\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVFIN", "PRF", "ADJD", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wann man es sonst nicht gerne wagt:", "tokens": ["Wann", "man", "es", "sonst", "nicht", "ger\u00b7ne", "wagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPER", "ADV", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein spa\u00dfgalan/ mein Courtisan/", "tokens": ["Mein", "spa\u00df\u00b7ga\u00b7lan", "/", "mein", "Cour\u00b7ti\u00b7san", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "$(", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Steht nur den l\u00f6ffel k\u00e4tzgen an.", "tokens": ["Steht", "nur", "den", "l\u00f6f\u00b7fel", "k\u00e4tz\u00b7gen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "6. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Der sch\u00f6nste titul von der welt:", "tokens": ["Der", "sch\u00f6ns\u00b7te", "ti\u00b7tul", "von", "der", "welt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Mein freund kan alle mit bedeuten/", "tokens": ["Mein", "freund", "kan", "al\u00b7le", "mit", "be\u00b7deu\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PIS", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer sich ein bi\u00dfgen freundlich stellt:", "tokens": ["Wer", "sich", "ein", "bi\u00df\u00b7gen", "freund\u00b7lich", "stellt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "ART", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein tr\u00f6ster ist ein s\u00fcsser hohn/", "tokens": ["Mein", "tr\u00f6s\u00b7ter", "ist", "ein", "s\u00fcs\u00b7ser", "hohn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Es schwatzt sich nur nicht wohl davon.", "tokens": ["Es", "schwatzt", "sich", "nur", "nicht", "wohl", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "PTKNEG", "ADV", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "7. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Doch vor den leuten taug es nicht:", "tokens": ["Doch", "vor", "den", "leu\u00b7ten", "taug", "es", "nicht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein h\u00e4hngen ist bi\u00dfweilen wette/", "tokens": ["Mein", "h\u00e4hn\u00b7gen", "ist", "bi\u00df\u00b7wei\u00b7len", "wet\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VAFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wan er zuvor mein h\u00fchngen spricht:", "tokens": ["Wan", "er", "zu\u00b7vor", "mein", "h\u00fchn\u00b7gen", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PPOSAT", "ADJA", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Mein g\u00fcmpel das ist immerfort", "tokens": ["Mein", "g\u00fcm\u00b7pel", "das", "ist", "im\u00b7mer\u00b7fort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "PDS", "VAFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht mehr als nur ein huren-wort.", "tokens": ["Nicht", "mehr", "als", "nur", "ein", "hu\u00b7ren\u00b7wort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIAT", "KOKOM", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "8. ", "tokens": [], "token_info": [], "pos": []}, "line.2": {"text": "Doch der verstand ist vielerley;", "tokens": ["Doch", "der", "ver\u00b7stand", "ist", "vie\u00b7ler\u00b7ley", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein herr/ das geht mir nicht von hertzen/", "tokens": ["Mein", "herr", "/", "das", "geht", "mir", "nicht", "von", "hert\u00b7zen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PDS", "VVFIN", "PPER", "PTKNEG", "APPR", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man denckt doch immer narr dabey;", "tokens": ["Man", "denckt", "doch", "im\u00b7mer", "narr", "da\u00b7bey", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "ADV", "ADJD", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mein mann steht schlecht wie michs bed\u00fcnckt/", "tokens": ["Mein", "mann", "steht", "schlecht", "wie", "michs", "be\u00b7d\u00fcnckt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJD", "KOKOM", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dieweil mein weib so garstig klingt.", "tokens": ["Die\u00b7weil", "mein", "weib", "so", "gars\u00b7tig", "klingt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "9. Ich kan ihr nichts vor angen mahlen/", "tokens": ["Ich", "kan", "ihr", "nichts", "vor", "an\u00b7gen", "mah\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "APPR", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was sie vor reden f\u00fchren sol/", "tokens": ["Was", "sie", "vor", "re\u00b7den", "f\u00fch\u00b7ren", "sol", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "VVINF", "VVINF", "VMFIN", "$("], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Sie lerne nur fein niedlich thalen/", "tokens": ["Sie", "ler\u00b7ne", "nur", "fein", "nied\u00b7lich", "tha\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein freundlich wort das find sich wohl/", "tokens": ["Ein", "freund\u00b7lich", "wort", "das", "find", "sich", "wohl", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "PDS", "VVFIN", "PRF", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und wo sie nichts ersinnen kan/", "tokens": ["Und", "wo", "sie", "nichts", "er\u00b7sin\u00b7nen", "kan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "PIS", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nimmt er ein schlechtes m\u00e4ulgen an.", "tokens": ["Nimmt", "er", "ein", "schlech\u00b7tes", "m\u00e4ul\u00b7gen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}