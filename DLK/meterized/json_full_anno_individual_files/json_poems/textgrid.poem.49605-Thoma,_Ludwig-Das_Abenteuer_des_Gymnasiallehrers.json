{"textgrid.poem.49605": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Das Abenteuer des Gymnasiallehrers", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In Freising lebte ein Professer,", "tokens": ["In", "Frei\u00b7sing", "leb\u00b7te", "ein", "Pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der nicht aus Zufall Josef hie\u00df;", "tokens": ["Der", "nicht", "aus", "Zu\u00b7fall", "Jo\u00b7sef", "hie\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "APPR", "NN", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nein, er verdient den Namen besser", "tokens": ["Nein", ",", "er", "ver\u00b7dient", "den", "Na\u00b7men", "bes\u00b7ser"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch alles, was er unterlie\u00df.", "tokens": ["Durch", "al\u00b7les", ",", "was", "er", "un\u00b7ter\u00b7lie\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein Philolog und deutscher Gatte,", "tokens": ["Ein", "Phi\u00b7lo\u00b7log", "und", "deut\u00b7scher", "Gat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kannt' er die Liebe nur als Pflicht,", "tokens": ["Kannt'", "er", "die", "Lie\u00b7be", "nur", "als", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "ADV", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Zweck zur Volksvermehrung hatte,", "tokens": ["Die", "Zweck", "zur", "Volks\u00b7ver\u00b7meh\u00b7rung", "hat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch keine andern Reize nicht.", "tokens": ["Doch", "kei\u00b7ne", "an\u00b7dern", "Rei\u00b7ze", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Nun h\u00f6rte er von den Kollegen,", "tokens": ["Nun", "h\u00f6r\u00b7te", "er", "von", "den", "Kol\u00b7le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Wie man in M\u00fcnchen sich erg\u00f6tzt.", "tokens": ["Wie", "man", "in", "M\u00fcn\u00b7chen", "sich", "er\u00b7g\u00f6tzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er war schon im Prinzip dagegen,", "tokens": ["Er", "war", "schon", "im", "Prin\u00b7zip", "da\u00b7ge\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPRART", "NN", "PAV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und war im Vorhinein verletzt.", "tokens": ["Und", "war", "im", "Vor\u00b7hi\u00b7nein", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Er suchte gleich in diesen Bildern", "tokens": ["Er", "such\u00b7te", "gleich", "in", "die\u00b7sen", "Bil\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den eigentlichen Wesenskern,", "tokens": ["Den", "ei\u00b7gent\u00b7li\u00b7chen", "We\u00b7sens\u00b7kern", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Um sie mit Abscheu dann zu schildern;", "tokens": ["Um", "sie", "mit", "Ab\u00b7scheu", "dann", "zu", "schil\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "APPR", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn alles andre lag ihm fern.", "tokens": ["Denn", "al\u00b7les", "and\u00b7re", "lag", "ihm", "fern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PIS", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Doch als er sich damit befa\u00dfte,", "tokens": ["Doch", "als", "er", "sich", "da\u00b7mit", "be\u00b7fa\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PRF", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Beschlo\u00df er auch, dorthin zu gehn,", "tokens": ["Be\u00b7schlo\u00df", "er", "auch", ",", "dor\u00b7thin", "zu", "gehn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Um dieses Treiben, das er ha\u00dfte,", "tokens": ["Um", "die\u00b7ses", "Trei\u00b7ben", ",", "das", "er", "ha\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "PDAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sich einmal gr\u00fcndlich anzusehn.", "tokens": ["Sich", "ein\u00b7mal", "gr\u00fcnd\u00b7lich", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und so kam Josef an die St\u00e4tte,", "tokens": ["Und", "so", "kam", "Jo\u00b7sef", "an", "die", "St\u00e4t\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "NE", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo Bacch- und Venus sich vereint,", "tokens": ["Wo", "Bac\u00b7ch", "und", "Ve\u00b7nus", "sich", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "TRUNC", "KON", "NN", "PRF", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wo unsre Scham \u2013 wenn man sie h\u00e4tte \u2013", "tokens": ["Wo", "uns\u00b7re", "Scham", "\u2013", "wenn", "man", "sie", "h\u00e4t\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "$(", "KOUS", "PIS", "PPER", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Am Grabe unsrer Unschuld weint.", "tokens": ["Am", "Gra\u00b7be", "uns\u00b7rer", "Un\u00b7schuld", "weint", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "An hundert hochgew\u00f6lbte B\u00fcsten", "tokens": ["An", "hun\u00b7dert", "hoch\u00b7ge\u00b7w\u00f6lb\u00b7te", "B\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Umtanzen uns und dr\u00e4ngen her,", "tokens": ["Um\u00b7tan\u00b7zen", "uns", "und", "dr\u00e4n\u00b7gen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "KON", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und will man ", "tokens": ["Und", "will", "man"], "token_info": ["word", "word", "word"], "pos": ["KON", "VMFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "So sieht man ", "tokens": ["So", "sieht", "man"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.8": {"line.1": {"text": "Die Sittlichkeit ist hier nur Fabel,", "tokens": ["Die", "Sitt\u00b7lich\u00b7keit", "ist", "hier", "nur", "Fa\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und jeder merkt, hier weilt sie nie.", "tokens": ["Und", "je\u00b7der", "merkt", ",", "hier", "weilt", "sie", "nie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das Auge schweift bis an den Nabel,", "tokens": ["Das", "Au\u00b7ge", "schweift", "bis", "an", "den", "Na\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und weiter schweift die Phantasie.", "tokens": ["Und", "wei\u00b7ter", "schweift", "die", "Phan\u00b7ta\u00b7sie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Ein Rausch kommt \u00fcber Josefs Sinne,", "tokens": ["Ein", "Rausch", "kommt", "\u00fc\u00b7ber", "Jo\u00b7sefs", "Sin\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NE", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihn ergreift ein Sch\u00f6nheitsdurst.", "tokens": ["Und", "ihn", "er\u00b7greift", "ein", "Sch\u00f6n\u00b7heits\u00b7durst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit einmal sind ihm deutsche Minne", "tokens": ["Mit", "ein\u00b7mal", "sind", "ihm", "deut\u00b7sche", "Min\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "VAFIN", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und deutsche Treue ziemlich wurst.", "tokens": ["Und", "deut\u00b7sche", "Treu\u00b7e", "ziem\u00b7lich", "wurst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Er st\u00fcrzt sich in die Freudenwoge", "tokens": ["Er", "st\u00fcrzt", "sich", "in", "die", "Freu\u00b7den\u00b7wo\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und fragt ein M\u00e4dchen: \u00bbWillst auch du?\u00ab", "tokens": ["Und", "fragt", "ein", "M\u00e4d\u00b7chen", ":", "\u00bb", "Willst", "auch", "du", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "$(", "VMFIN", "ADV", "PPER", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie sagt: \u00bbSie sind wohl Philologe?", "tokens": ["Sie", "sagt", ":", "\u00bb", "Sie", "sind", "wohl", "Phi\u00b7lo\u00b7lo\u00b7ge", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man kennt's am abgelatschten Schuh;", "tokens": ["Man", "kennt's", "am", "ab\u00b7ge\u00b7latschten", "Schuh", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.11": {"line.1": {"text": "In Ihrem Barte h\u00e4ngen Reste", "tokens": ["In", "Ih\u00b7rem", "Bar\u00b7te", "h\u00e4n\u00b7gen", "Res\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Linsen und von Sauerkohl!", "tokens": ["Von", "Lin\u00b7sen", "und", "von", "Sau\u00b7er\u00b7kohl", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich danke Ihnen auf das beste,", "tokens": ["Ich", "dan\u00b7ke", "Ih\u00b7nen", "auf", "das", "bes\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In mir \u2013 da t\u00e4uschen Sie sich wohl?\u00ab", "tokens": ["In", "mir", "\u2013", "da", "t\u00e4u\u00b7schen", "Sie", "sich", "wohl", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "$(", "ADV", "VVFIN", "PPER", "PRF", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Mein Josef konnte es nicht fassen,", "tokens": ["Mein", "Jo\u00b7sef", "konn\u00b7te", "es", "nicht", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was seiner Tugend widerfuhr;", "tokens": ["Was", "sei\u00b7ner", "Tu\u00b7gend", "wi\u00b7der\u00b7fuhr", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er wollte sich herunterlassen \u2013", "tokens": ["Er", "woll\u00b7te", "sich", "her\u00b7un\u00b7ter\u00b7las\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dem Gesch\u00f6pf mi\u00dffiel es nur!", "tokens": ["Und", "dem", "Ge\u00b7sch\u00f6pf", "mi\u00df\u00b7fiel", "es", "nur", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Schon f\u00fchlt' er Ekel vor dem Treiben", "tokens": ["Schon", "f\u00fchlt'", "er", "E\u00b7kel", "vor", "dem", "Trei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlt' sich von Moral umweht;", "tokens": ["Und", "f\u00fchlt'", "sich", "von", "Mo\u00b7ral", "um\u00b7weht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "VVFIN", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Man kann ja niemals reiner bleiben,", "tokens": ["Man", "kann", "ja", "nie\u00b7mals", "rei\u00b7ner", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als wenn ein M\u00e4dchen uns verschm\u00e4ht.", "tokens": ["Als", "wenn", "ein", "M\u00e4d\u00b7chen", "uns", "ver\u00b7schm\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Indessen war im Schicksalsf\u00fcgen", "tokens": ["In\u00b7des\u00b7sen", "war", "im", "Schick\u00b7sals\u00b7f\u00fc\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr Josef H\u00e4rtres aufgespart.", "tokens": ["F\u00fcr", "Jo\u00b7sef", "H\u00e4r\u00b7tres", "auf\u00b7ge\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er st\u00fcrzte nochmal ins Vergn\u00fcgen", "tokens": ["Er", "st\u00fcrz\u00b7te", "noch\u00b7mal", "ins", "Ver\u00b7gn\u00fc\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und k\u00e4mmte vorher seinen Bart.", "tokens": ["Und", "k\u00e4mm\u00b7te", "vor\u00b7her", "sei\u00b7nen", "Bart", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Das zweite M\u00e4dchen \u2013 angesprochen \u2013", "tokens": ["Das", "zwei\u00b7te", "M\u00e4d\u00b7chen", "\u2013", "an\u00b7ge\u00b7spro\u00b7chen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hatt', etwas minder prezi\u00f6s,", "tokens": ["Hatt'", ",", "et\u00b7was", "min\u00b7der", "pre\u00b7zi\u00b7\u00f6s", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit manchem Vorurteil gebrochen", "tokens": ["Mit", "man\u00b7chem", "Vor\u00b7ur\u00b7teil", "ge\u00b7bro\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sagte blo\u00df: \u00bbAch, Sie sind b\u00f6s!\u00ab", "tokens": ["Und", "sag\u00b7te", "blo\u00df", ":", "\u00bb", "Ach", ",", "Sie", "sind", "b\u00f6s", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "$.", "$(", "ITJ", "$,", "PPER", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Mit dessen Gunst er sichtlich prahlte,", "tokens": ["Mit", "des\u00b7sen", "Gunst", "er", "sicht\u00b7lich", "prahl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und beide waren herzlich froh!", "tokens": ["Und", "bei\u00b7de", "wa\u00b7ren", "herz\u00b7lich", "froh", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Wie ein Moralprinzip verschwindet", "tokens": ["Wie", "ein", "Mo\u00b7ral\u00b7prin\u00b7zip", "ver\u00b7schwin\u00b7det"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Selbst aus dem st\u00e4rksten Intellekt,", "tokens": ["Selbst", "aus", "dem", "st\u00e4rks\u00b7ten", "In\u00b7tel\u00b7lekt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn man ein h\u00fcbsches M\u00e4dchen findet", "tokens": ["Wenn", "man", "ein", "h\u00fcb\u00b7sches", "M\u00e4d\u00b7chen", "fin\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und eine Flasche guten Sekt!", "tokens": ["Und", "ei\u00b7ne", "Fla\u00b7sche", "gu\u00b7ten", "Sekt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Auch Josef mu\u00dfte dies erfahren,", "tokens": ["Auch", "Jo\u00b7sef", "mu\u00df\u00b7te", "dies", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VMFIN", "PDS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und an sich selbst sah er die Spur", "tokens": ["Und", "an", "sich", "selbst", "sah", "er", "die", "Spur"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PRF", "ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Der ewig gleich unwandelbaren,", "tokens": ["Der", "e\u00b7wig", "gleich", "un\u00b7wan\u00b7del\u00b7ba\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das All beherrschenden Natur.", "tokens": ["Das", "All", "be\u00b7herr\u00b7schen\u00b7den", "Na\u00b7tur", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Schon wollt' er sich im Walzer drehen", "tokens": ["Schon", "wollt'", "er", "sich", "im", "Wal\u00b7zer", "dre\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "APPRART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sucht' im Tanze den Genu\u00df;", "tokens": ["Und", "sucht'", "im", "Tan\u00b7ze", "den", "Ge\u00b7nu\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch mu\u00dfte er sich eingestehen,", "tokens": ["Doch", "mu\u00df\u00b7te", "er", "sich", "ein\u00b7ge\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PRF", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df man auch dieses lernen mu\u00df.", "tokens": ["Da\u00df", "man", "auch", "die\u00b7ses", "ler\u00b7nen", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "PDAT", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Er m\u00fchte schwitzend sich im Kreise,", "tokens": ["Er", "m\u00fch\u00b7te", "schwit\u00b7zend", "sich", "im", "Krei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "PRF", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er drehte sich nach rechts und links,", "tokens": ["Er", "dreh\u00b7te", "sich", "nach", "rechts", "und", "links", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Versucht's auf die und andre Weise", "tokens": ["Ver\u00b7sucht's", "auf", "die", "und", "and\u00b7re", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "ART", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und fand's unm\u00f6glich schlechterdings.", "tokens": ["Und", "fan\u00b7d's", "un\u00b7m\u00f6g\u00b7lich", "schlech\u00b7ter\u00b7dings", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.21": {"line.1": {"text": "Er wu\u00dfte zwar von den Hellenen,", "tokens": ["Er", "wu\u00df\u00b7te", "zwar", "von", "den", "Hel\u00b7le\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie man im Auftakt sich bewegt,", "tokens": ["Wie", "man", "im", "Auf\u00b7takt", "sich", "be\u00b7wegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPRART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch lernt' er leider nicht bei jenen,", "tokens": ["Doch", "lernt'", "er", "lei\u00b7der", "nicht", "bei", "je\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "APPR", "PDAT", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie man das Schwergewicht verlegt.", "tokens": ["Wie", "man", "das", "Schwer\u00b7ge\u00b7wicht", "ver\u00b7legt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Mit stattlichem Gelehrtenschuhe", "tokens": ["Mit", "statt\u00b7li\u00b7chem", "Ge\u00b7lehr\u00b7ten\u00b7schu\u00b7he"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Trat er dem M\u00e4dchen auf die Zeh';", "tokens": ["Trat", "er", "dem", "M\u00e4d\u00b7chen", "auf", "die", "Zeh'", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie bat ihn flehentlich um Ruhe,", "tokens": ["Sie", "bat", "ihn", "fle\u00b7hent\u00b7lich", "um", "Ru\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Denn auf die Dauer tut es weh.", "tokens": ["Denn", "auf", "die", "Dau\u00b7er", "tut", "es", "weh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "So blieb ihm nichts mehr, als zu trinken;", "tokens": ["So", "blieb", "ihm", "nichts", "mehr", ",", "als", "zu", "trin\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "ADV", "$,", "KOUS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er war Germane, und er trank", "tokens": ["Er", "war", "Ger\u00b7ma\u00b7ne", ",", "und", "er", "trank"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "NE", "$,", "KON", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und durft' in Seligkeit versinken", "tokens": ["Und", "durft'", "in", "Se\u00b7lig\u00b7keit", "ver\u00b7sin\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit seinem M\u00e4dchen, und versank.", "tokens": ["Mit", "sei\u00b7nem", "M\u00e4d\u00b7chen", ",", "und", "ver\u00b7sank", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Er dacht' an Bacchus und Tribaden,", "tokens": ["Er", "dacht'", "an", "Bac\u00b7chus", "und", "Tri\u00b7ba\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wie so der Wirbel um ihn schwoll;", "tokens": ["Wie", "so", "der", "Wir\u00b7bel", "um", "ihn", "schwoll", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "APPR", "PPER", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Schon f\u00fchlte er die zarten Waden,", "tokens": ["Schon", "f\u00fchl\u00b7te", "er", "die", "zar\u00b7ten", "Wa\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wurde gl\u00fccklich, \u2013 wurde voll.", "tokens": ["Und", "wur\u00b7de", "gl\u00fcck\u00b7lich", ",", "\u2013", "wur\u00b7de", "voll", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "$,", "$(", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Es jauchzt um ihn mit gellen T\u00f6nen,", "tokens": ["Es", "jauchzt", "um", "ihn", "mit", "gel\u00b7len", "T\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein jeder Busen atmet wild,", "tokens": ["Ein", "je\u00b7der", "Bu\u00b7sen", "at\u00b7met", "wild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Haare l\u00f6sen sich der Sch\u00f6nen,", "tokens": ["Die", "Haa\u00b7re", "l\u00f6\u00b7sen", "sich", "der", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und immer wilder wird das Bild.", "tokens": ["Und", "im\u00b7mer", "wil\u00b7der", "wird", "das", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Ein Prosit allen, die sich lieben!", "tokens": ["Ein", "Pro\u00b7sit", "al\u00b7len", ",", "die", "sich", "lie\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "$,", "PRELS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Evo\u00eb f\u00fcr jede Braut!", "tokens": ["Und", "E\u00b7vo\u00eb", "f\u00fcr", "je\u00b7de", "Braut", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "Was ist Moral! Nur eine Blase,", "tokens": ["Was", "ist", "Mo\u00b7ral", "!", "Nur", "ei\u00b7ne", "Bla\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NN", "$.", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Steigt kr\u00e4nklich im Gehirne auf.", "tokens": ["Steigt", "kr\u00e4nk\u00b7lich", "im", "Ge\u00b7hir\u00b7ne", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die S\u00fcnde kommt uns in die Nase", "tokens": ["Die", "S\u00fcn\u00b7de", "kommt", "uns", "in", "die", "Na\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.4": {"text": "Und nimmt von selber ihren Lauf.", "tokens": ["Und", "nimmt", "von", "sel\u00b7ber", "ih\u00b7ren", "Lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Et cetera! So ging es weiter.", "tokens": ["Et", "ce\u00b7te\u00b7ra", "!", "So", "ging", "es", "wei\u00b7ter", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was hilft die Philologenzunft!", "tokens": ["Was", "hilft", "die", "Phi\u00b7lo\u00b7lo\u00b7gen\u00b7zunft", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch Professoren werden heiter", "tokens": ["Auch", "Pro\u00b7fes\u00b7so\u00b7ren", "wer\u00b7den", "hei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "NN", "VAFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und werden wild in ihrer Brunft.", "tokens": ["Und", "wer\u00b7den", "wild", "in", "ih\u00b7rer", "Brunft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Nach so viel Sekt und S\u00fc\u00dfigkeiten", "tokens": ["Nach", "so", "viel", "Sekt", "und", "S\u00fc\u00b7\u00dfig\u00b7kei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "PIAT", "NN", "KON", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schmeckt uns die Wei\u00dfwurst und das Bier.", "tokens": ["Schmeckt", "uns", "die", "Wei\u00df\u00b7wurst", "und", "das", "Bier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Abschlu\u00df ist das Heimbegleiten", "tokens": ["Der", "Ab\u00b7schlu\u00df", "ist", "das", "Heim\u00b7be\u00b7glei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr jedes Paar. Warum nicht hier?", "tokens": ["F\u00fcr", "je\u00b7des", "Paar", ".", "Wa\u00b7rum", "nicht", "hier", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$.", "PWAV", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Auch Josef sa\u00df in einem Wagen", "tokens": ["Auch", "Jo\u00b7sef", "sa\u00df", "in", "ei\u00b7nem", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlte, wie an ihn sich pre\u00dft',", "tokens": ["Und", "f\u00fchl\u00b7te", ",", "wie", "an", "ihn", "sich", "pre\u00dft'", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PWAV", "APPR", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was hier nicht unbefangen sagen,", "tokens": ["Was", "hier", "nicht", "un\u00b7be\u00b7fan\u00b7gen", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch sich sehr einfach denken l\u00e4\u00dft.", "tokens": ["Doch", "sich", "sehr", "ein\u00b7fach", "den\u00b7ken", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADV", "ADJD", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Er f\u00fchlte seine Pulse h\u00e4mmern,", "tokens": ["Er", "f\u00fchl\u00b7te", "sei\u00b7ne", "Pul\u00b7se", "h\u00e4m\u00b7mern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch wu\u00dft' er nicht, was sonst geschah;", "tokens": ["Doch", "wu\u00dft'", "er", "nicht", ",", "was", "sonst", "ge\u00b7schah", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "$,", "PRELS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn seinen Sinn umfing ein D\u00e4mmern,", "tokens": ["Denn", "sei\u00b7nen", "Sinn", "um\u00b7fing", "ein", "D\u00e4m\u00b7mern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df er nichts mehr Genaues sah.", "tokens": ["Da\u00df", "er", "nichts", "mehr", "Ge\u00b7nau\u00b7es", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "PIAT", "NN", "VVFIN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.32": {"line.1": {"text": "Er stolpert hastig \u00fcber Stiegen", "tokens": ["Er", "stol\u00b7pert", "has\u00b7tig", "\u00fc\u00b7ber", "Stie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00e4llt auch irgendwo ins Bett,", "tokens": ["Und", "f\u00e4llt", "auch", "ir\u00b7gend\u00b7wo", "ins", "Bett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und mu\u00df sehr lang darinnen liegen \u2013", "tokens": ["Und", "mu\u00df", "sehr", "lang", "da\u00b7rin\u00b7nen", "lie\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "ADJD", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das \u00fcbrige war wundernett.", "tokens": ["Das", "\u00fcb\u00b7ri\u00b7ge", "war", "wun\u00b7der\u00b7nett", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Er hat die Zeit bis abends sieben", "tokens": ["Er", "hat", "die", "Zeit", "bis", "a\u00b7bends", "sie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "ADV", "CARD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei diesem M\u00e4dchen zugebracht,", "tokens": ["Bei", "die\u00b7sem", "M\u00e4d\u00b7chen", "zu\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und fuhr alsdann zu seinen Lieben", "tokens": ["Und", "fuhr", "als\u00b7dann", "zu", "sei\u00b7nen", "Lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nach Freising etwa um halb acht.", "tokens": ["Nach", "Frei\u00b7sing", "et\u00b7wa", "um", "halb", "acht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "APPR", "ADJD", "CARD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.34": {"line.1": {"text": "Als er daheim nun angelangte,", "tokens": ["Als", "er", "da\u00b7heim", "nun", "an\u00b7ge\u00b7lang\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War er von solcher M\u00fcdigkeit,", "tokens": ["War", "er", "von", "sol\u00b7cher", "M\u00fc\u00b7dig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df seine Frau um ihn sich bangte;", "tokens": ["Da\u00df", "sei\u00b7ne", "Frau", "um", "ihn", "sich", "bang\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie macht' das Bett f\u00fcr ihn bereit.", "tokens": ["Sie", "macht'", "das", "Bett", "f\u00fcr", "ihn", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Und Josef hat sich ausgezogen", "tokens": ["Und", "Jo\u00b7sef", "hat", "sich", "aus\u00b7ge\u00b7zo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "PRF", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sprach, da\u00df er erk\u00e4ltet sei,", "tokens": ["Und", "sprach", ",", "da\u00df", "er", "er\u00b7k\u00e4l\u00b7tet", "sei", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PPER", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und hat noch dies und das gelogen,", "tokens": ["Und", "hat", "noch", "dies", "und", "das", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PDS", "KON", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn eine Frau fr\u00e4gt vielerlei.", "tokens": ["Denn", "ei\u00b7ne", "Frau", "fr\u00e4gt", "vie\u00b7ler\u00b7lei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Da\u00df L\u00fcgen kurze Beine tragen,", "tokens": ["Da\u00df", "L\u00fc\u00b7gen", "kur\u00b7ze", "Bei\u00b7ne", "tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das zeigte sich hier wunderbar;", "tokens": ["Das", "zeig\u00b7te", "sich", "hier", "wun\u00b7der\u00b7bar", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn Josef ward so ganz geschlagen,", "tokens": ["Denn", "Jo\u00b7sef", "ward", "so", "ganz", "ge\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df hier f\u00fcr ihn kein Ausweg war.", "tokens": ["Da\u00df", "hier", "f\u00fcr", "ihn", "kein", "Aus\u00b7weg", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "PPER", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Er trug \u2013 da gibt es kein Entrinnen", "tokens": ["Er", "trug", "\u2013", "da", "gibt", "es", "kein", "Ent\u00b7rin\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "ADV", "VVFIN", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und kein Erkl\u00e4ren so und so \u2013", "tokens": ["Und", "kein", "Er\u00b7kl\u00e4\u00b7ren", "so", "und", "so", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "KON", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er trug aus duftig wei\u00dfem Linnen", "tokens": ["Er", "trug", "aus", "duf\u00b7tig", "wei\u00b7\u00dfem", "Lin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "\u2013 \u2013 Das H\u00f6schen seines Domino \u2013 \u2013!", "tokens": ["\u2013", "\u2013", "Das", "H\u00f6\u00b7schen", "sei\u00b7nes", "Do\u00b7mi\u00b7no", "\u2013", "\u2013", "!"], "token_info": ["punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "$(", "ART", "NN", "PPOSAT", "NN", "$(", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "In Freising lebte ein Professer,", "tokens": ["In", "Frei\u00b7sing", "leb\u00b7te", "ein", "Pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der nicht aus Zufall Josef hie\u00df;", "tokens": ["Der", "nicht", "aus", "Zu\u00b7fall", "Jo\u00b7sef", "hie\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "APPR", "NN", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nein, er verdient den Namen besser", "tokens": ["Nein", ",", "er", "ver\u00b7dient", "den", "Na\u00b7men", "bes\u00b7ser"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch alles, was er unterlie\u00df.", "tokens": ["Durch", "al\u00b7les", ",", "was", "er", "un\u00b7ter\u00b7lie\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Ein Philolog und deutscher Gatte,", "tokens": ["Ein", "Phi\u00b7lo\u00b7log", "und", "deut\u00b7scher", "Gat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kannt' er die Liebe nur als Pflicht,", "tokens": ["Kannt'", "er", "die", "Lie\u00b7be", "nur", "als", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "ADV", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Zweck zur Volksvermehrung hatte,", "tokens": ["Die", "Zweck", "zur", "Volks\u00b7ver\u00b7meh\u00b7rung", "hat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch keine andern Reize nicht.", "tokens": ["Doch", "kei\u00b7ne", "an\u00b7dern", "Rei\u00b7ze", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "Nun h\u00f6rte er von den Kollegen,", "tokens": ["Nun", "h\u00f6r\u00b7te", "er", "von", "den", "Kol\u00b7le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Wie man in M\u00fcnchen sich erg\u00f6tzt.", "tokens": ["Wie", "man", "in", "M\u00fcn\u00b7chen", "sich", "er\u00b7g\u00f6tzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er war schon im Prinzip dagegen,", "tokens": ["Er", "war", "schon", "im", "Prin\u00b7zip", "da\u00b7ge\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPRART", "NN", "PAV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und war im Vorhinein verletzt.", "tokens": ["Und", "war", "im", "Vor\u00b7hi\u00b7nein", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Er suchte gleich in diesen Bildern", "tokens": ["Er", "such\u00b7te", "gleich", "in", "die\u00b7sen", "Bil\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den eigentlichen Wesenskern,", "tokens": ["Den", "ei\u00b7gent\u00b7li\u00b7chen", "We\u00b7sens\u00b7kern", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Um sie mit Abscheu dann zu schildern;", "tokens": ["Um", "sie", "mit", "Ab\u00b7scheu", "dann", "zu", "schil\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "APPR", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn alles andre lag ihm fern.", "tokens": ["Denn", "al\u00b7les", "and\u00b7re", "lag", "ihm", "fern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PIS", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Doch als er sich damit befa\u00dfte,", "tokens": ["Doch", "als", "er", "sich", "da\u00b7mit", "be\u00b7fa\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PRF", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Beschlo\u00df er auch, dorthin zu gehn,", "tokens": ["Be\u00b7schlo\u00df", "er", "auch", ",", "dor\u00b7thin", "zu", "gehn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Um dieses Treiben, das er ha\u00dfte,", "tokens": ["Um", "die\u00b7ses", "Trei\u00b7ben", ",", "das", "er", "ha\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "PDAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sich einmal gr\u00fcndlich anzusehn.", "tokens": ["Sich", "ein\u00b7mal", "gr\u00fcnd\u00b7lich", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Und so kam Josef an die St\u00e4tte,", "tokens": ["Und", "so", "kam", "Jo\u00b7sef", "an", "die", "St\u00e4t\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "NE", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo Bacch- und Venus sich vereint,", "tokens": ["Wo", "Bac\u00b7ch", "und", "Ve\u00b7nus", "sich", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "TRUNC", "KON", "NN", "PRF", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wo unsre Scham \u2013 wenn man sie h\u00e4tte \u2013", "tokens": ["Wo", "uns\u00b7re", "Scham", "\u2013", "wenn", "man", "sie", "h\u00e4t\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "$(", "KOUS", "PIS", "PPER", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Am Grabe unsrer Unschuld weint.", "tokens": ["Am", "Gra\u00b7be", "uns\u00b7rer", "Un\u00b7schuld", "weint", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "An hundert hochgew\u00f6lbte B\u00fcsten", "tokens": ["An", "hun\u00b7dert", "hoch\u00b7ge\u00b7w\u00f6lb\u00b7te", "B\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Umtanzen uns und dr\u00e4ngen her,", "tokens": ["Um\u00b7tan\u00b7zen", "uns", "und", "dr\u00e4n\u00b7gen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "KON", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und will man ", "tokens": ["Und", "will", "man"], "token_info": ["word", "word", "word"], "pos": ["KON", "VMFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "So sieht man ", "tokens": ["So", "sieht", "man"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.45": {"line.1": {"text": "Die Sittlichkeit ist hier nur Fabel,", "tokens": ["Die", "Sitt\u00b7lich\u00b7keit", "ist", "hier", "nur", "Fa\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und jeder merkt, hier weilt sie nie.", "tokens": ["Und", "je\u00b7der", "merkt", ",", "hier", "weilt", "sie", "nie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das Auge schweift bis an den Nabel,", "tokens": ["Das", "Au\u00b7ge", "schweift", "bis", "an", "den", "Na\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und weiter schweift die Phantasie.", "tokens": ["Und", "wei\u00b7ter", "schweift", "die", "Phan\u00b7ta\u00b7sie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Ein Rausch kommt \u00fcber Josefs Sinne,", "tokens": ["Ein", "Rausch", "kommt", "\u00fc\u00b7ber", "Jo\u00b7sefs", "Sin\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NE", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihn ergreift ein Sch\u00f6nheitsdurst.", "tokens": ["Und", "ihn", "er\u00b7greift", "ein", "Sch\u00f6n\u00b7heits\u00b7durst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit einmal sind ihm deutsche Minne", "tokens": ["Mit", "ein\u00b7mal", "sind", "ihm", "deut\u00b7sche", "Min\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "VAFIN", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und deutsche Treue ziemlich wurst.", "tokens": ["Und", "deut\u00b7sche", "Treu\u00b7e", "ziem\u00b7lich", "wurst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Er st\u00fcrzt sich in die Freudenwoge", "tokens": ["Er", "st\u00fcrzt", "sich", "in", "die", "Freu\u00b7den\u00b7wo\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und fragt ein M\u00e4dchen: \u00bbWillst auch du?\u00ab", "tokens": ["Und", "fragt", "ein", "M\u00e4d\u00b7chen", ":", "\u00bb", "Willst", "auch", "du", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "$(", "VMFIN", "ADV", "PPER", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie sagt: \u00bbSie sind wohl Philologe?", "tokens": ["Sie", "sagt", ":", "\u00bb", "Sie", "sind", "wohl", "Phi\u00b7lo\u00b7lo\u00b7ge", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man kennt's am abgelatschten Schuh;", "tokens": ["Man", "kennt's", "am", "ab\u00b7ge\u00b7latschten", "Schuh", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.48": {"line.1": {"text": "In Ihrem Barte h\u00e4ngen Reste", "tokens": ["In", "Ih\u00b7rem", "Bar\u00b7te", "h\u00e4n\u00b7gen", "Res\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Linsen und von Sauerkohl!", "tokens": ["Von", "Lin\u00b7sen", "und", "von", "Sau\u00b7er\u00b7kohl", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich danke Ihnen auf das beste,", "tokens": ["Ich", "dan\u00b7ke", "Ih\u00b7nen", "auf", "das", "bes\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In mir \u2013 da t\u00e4uschen Sie sich wohl?\u00ab", "tokens": ["In", "mir", "\u2013", "da", "t\u00e4u\u00b7schen", "Sie", "sich", "wohl", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "$(", "ADV", "VVFIN", "PPER", "PRF", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Mein Josef konnte es nicht fassen,", "tokens": ["Mein", "Jo\u00b7sef", "konn\u00b7te", "es", "nicht", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was seiner Tugend widerfuhr;", "tokens": ["Was", "sei\u00b7ner", "Tu\u00b7gend", "wi\u00b7der\u00b7fuhr", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er wollte sich herunterlassen \u2013", "tokens": ["Er", "woll\u00b7te", "sich", "her\u00b7un\u00b7ter\u00b7las\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dem Gesch\u00f6pf mi\u00dffiel es nur!", "tokens": ["Und", "dem", "Ge\u00b7sch\u00f6pf", "mi\u00df\u00b7fiel", "es", "nur", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Schon f\u00fchlt' er Ekel vor dem Treiben", "tokens": ["Schon", "f\u00fchlt'", "er", "E\u00b7kel", "vor", "dem", "Trei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlt' sich von Moral umweht;", "tokens": ["Und", "f\u00fchlt'", "sich", "von", "Mo\u00b7ral", "um\u00b7weht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "VVFIN", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Man kann ja niemals reiner bleiben,", "tokens": ["Man", "kann", "ja", "nie\u00b7mals", "rei\u00b7ner", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als wenn ein M\u00e4dchen uns verschm\u00e4ht.", "tokens": ["Als", "wenn", "ein", "M\u00e4d\u00b7chen", "uns", "ver\u00b7schm\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Indessen war im Schicksalsf\u00fcgen", "tokens": ["In\u00b7des\u00b7sen", "war", "im", "Schick\u00b7sals\u00b7f\u00fc\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr Josef H\u00e4rtres aufgespart.", "tokens": ["F\u00fcr", "Jo\u00b7sef", "H\u00e4r\u00b7tres", "auf\u00b7ge\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er st\u00fcrzte nochmal ins Vergn\u00fcgen", "tokens": ["Er", "st\u00fcrz\u00b7te", "noch\u00b7mal", "ins", "Ver\u00b7gn\u00fc\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und k\u00e4mmte vorher seinen Bart.", "tokens": ["Und", "k\u00e4mm\u00b7te", "vor\u00b7her", "sei\u00b7nen", "Bart", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Das zweite M\u00e4dchen \u2013 angesprochen \u2013", "tokens": ["Das", "zwei\u00b7te", "M\u00e4d\u00b7chen", "\u2013", "an\u00b7ge\u00b7spro\u00b7chen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hatt', etwas minder prezi\u00f6s,", "tokens": ["Hatt'", ",", "et\u00b7was", "min\u00b7der", "pre\u00b7zi\u00b7\u00f6s", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit manchem Vorurteil gebrochen", "tokens": ["Mit", "man\u00b7chem", "Vor\u00b7ur\u00b7teil", "ge\u00b7bro\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sagte blo\u00df: \u00bbAch, Sie sind b\u00f6s!\u00ab", "tokens": ["Und", "sag\u00b7te", "blo\u00df", ":", "\u00bb", "Ach", ",", "Sie", "sind", "b\u00f6s", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "$.", "$(", "ITJ", "$,", "PPER", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Mit dessen Gunst er sichtlich prahlte,", "tokens": ["Mit", "des\u00b7sen", "Gunst", "er", "sicht\u00b7lich", "prahl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und beide waren herzlich froh!", "tokens": ["Und", "bei\u00b7de", "wa\u00b7ren", "herz\u00b7lich", "froh", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Wie ein Moralprinzip verschwindet", "tokens": ["Wie", "ein", "Mo\u00b7ral\u00b7prin\u00b7zip", "ver\u00b7schwin\u00b7det"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Selbst aus dem st\u00e4rksten Intellekt,", "tokens": ["Selbst", "aus", "dem", "st\u00e4rks\u00b7ten", "In\u00b7tel\u00b7lekt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn man ein h\u00fcbsches M\u00e4dchen findet", "tokens": ["Wenn", "man", "ein", "h\u00fcb\u00b7sches", "M\u00e4d\u00b7chen", "fin\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und eine Flasche guten Sekt!", "tokens": ["Und", "ei\u00b7ne", "Fla\u00b7sche", "gu\u00b7ten", "Sekt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Auch Josef mu\u00dfte dies erfahren,", "tokens": ["Auch", "Jo\u00b7sef", "mu\u00df\u00b7te", "dies", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VMFIN", "PDS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und an sich selbst sah er die Spur", "tokens": ["Und", "an", "sich", "selbst", "sah", "er", "die", "Spur"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PRF", "ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Der ewig gleich unwandelbaren,", "tokens": ["Der", "e\u00b7wig", "gleich", "un\u00b7wan\u00b7del\u00b7ba\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das All beherrschenden Natur.", "tokens": ["Das", "All", "be\u00b7herr\u00b7schen\u00b7den", "Na\u00b7tur", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Schon wollt' er sich im Walzer drehen", "tokens": ["Schon", "wollt'", "er", "sich", "im", "Wal\u00b7zer", "dre\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "APPRART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sucht' im Tanze den Genu\u00df;", "tokens": ["Und", "sucht'", "im", "Tan\u00b7ze", "den", "Ge\u00b7nu\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch mu\u00dfte er sich eingestehen,", "tokens": ["Doch", "mu\u00df\u00b7te", "er", "sich", "ein\u00b7ge\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PRF", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df man auch dieses lernen mu\u00df.", "tokens": ["Da\u00df", "man", "auch", "die\u00b7ses", "ler\u00b7nen", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "PDAT", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Er m\u00fchte schwitzend sich im Kreise,", "tokens": ["Er", "m\u00fch\u00b7te", "schwit\u00b7zend", "sich", "im", "Krei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "PRF", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er drehte sich nach rechts und links,", "tokens": ["Er", "dreh\u00b7te", "sich", "nach", "rechts", "und", "links", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Versucht's auf die und andre Weise", "tokens": ["Ver\u00b7sucht's", "auf", "die", "und", "and\u00b7re", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "ART", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und fand's unm\u00f6glich schlechterdings.", "tokens": ["Und", "fan\u00b7d's", "un\u00b7m\u00f6g\u00b7lich", "schlech\u00b7ter\u00b7dings", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.58": {"line.1": {"text": "Er wu\u00dfte zwar von den Hellenen,", "tokens": ["Er", "wu\u00df\u00b7te", "zwar", "von", "den", "Hel\u00b7le\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie man im Auftakt sich bewegt,", "tokens": ["Wie", "man", "im", "Auf\u00b7takt", "sich", "be\u00b7wegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPRART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch lernt' er leider nicht bei jenen,", "tokens": ["Doch", "lernt'", "er", "lei\u00b7der", "nicht", "bei", "je\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "APPR", "PDAT", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie man das Schwergewicht verlegt.", "tokens": ["Wie", "man", "das", "Schwer\u00b7ge\u00b7wicht", "ver\u00b7legt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Mit stattlichem Gelehrtenschuhe", "tokens": ["Mit", "statt\u00b7li\u00b7chem", "Ge\u00b7lehr\u00b7ten\u00b7schu\u00b7he"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Trat er dem M\u00e4dchen auf die Zeh';", "tokens": ["Trat", "er", "dem", "M\u00e4d\u00b7chen", "auf", "die", "Zeh'", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie bat ihn flehentlich um Ruhe,", "tokens": ["Sie", "bat", "ihn", "fle\u00b7hent\u00b7lich", "um", "Ru\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Denn auf die Dauer tut es weh.", "tokens": ["Denn", "auf", "die", "Dau\u00b7er", "tut", "es", "weh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "So blieb ihm nichts mehr, als zu trinken;", "tokens": ["So", "blieb", "ihm", "nichts", "mehr", ",", "als", "zu", "trin\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "ADV", "$,", "KOUS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er war Germane, und er trank", "tokens": ["Er", "war", "Ger\u00b7ma\u00b7ne", ",", "und", "er", "trank"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "NE", "$,", "KON", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und durft' in Seligkeit versinken", "tokens": ["Und", "durft'", "in", "Se\u00b7lig\u00b7keit", "ver\u00b7sin\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit seinem M\u00e4dchen, und versank.", "tokens": ["Mit", "sei\u00b7nem", "M\u00e4d\u00b7chen", ",", "und", "ver\u00b7sank", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Er dacht' an Bacchus und Tribaden,", "tokens": ["Er", "dacht'", "an", "Bac\u00b7chus", "und", "Tri\u00b7ba\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wie so der Wirbel um ihn schwoll;", "tokens": ["Wie", "so", "der", "Wir\u00b7bel", "um", "ihn", "schwoll", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "APPR", "PPER", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Schon f\u00fchlte er die zarten Waden,", "tokens": ["Schon", "f\u00fchl\u00b7te", "er", "die", "zar\u00b7ten", "Wa\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wurde gl\u00fccklich, \u2013 wurde voll.", "tokens": ["Und", "wur\u00b7de", "gl\u00fcck\u00b7lich", ",", "\u2013", "wur\u00b7de", "voll", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "$,", "$(", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.62": {"line.1": {"text": "Es jauchzt um ihn mit gellen T\u00f6nen,", "tokens": ["Es", "jauchzt", "um", "ihn", "mit", "gel\u00b7len", "T\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein jeder Busen atmet wild,", "tokens": ["Ein", "je\u00b7der", "Bu\u00b7sen", "at\u00b7met", "wild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Haare l\u00f6sen sich der Sch\u00f6nen,", "tokens": ["Die", "Haa\u00b7re", "l\u00f6\u00b7sen", "sich", "der", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und immer wilder wird das Bild.", "tokens": ["Und", "im\u00b7mer", "wil\u00b7der", "wird", "das", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Ein Prosit allen, die sich lieben!", "tokens": ["Ein", "Pro\u00b7sit", "al\u00b7len", ",", "die", "sich", "lie\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "$,", "PRELS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Evo\u00eb f\u00fcr jede Braut!", "tokens": ["Und", "E\u00b7vo\u00eb", "f\u00fcr", "je\u00b7de", "Braut", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.64": {"line.1": {"text": "Was ist Moral! Nur eine Blase,", "tokens": ["Was", "ist", "Mo\u00b7ral", "!", "Nur", "ei\u00b7ne", "Bla\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NN", "$.", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Steigt kr\u00e4nklich im Gehirne auf.", "tokens": ["Steigt", "kr\u00e4nk\u00b7lich", "im", "Ge\u00b7hir\u00b7ne", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die S\u00fcnde kommt uns in die Nase", "tokens": ["Die", "S\u00fcn\u00b7de", "kommt", "uns", "in", "die", "Na\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.4": {"text": "Und nimmt von selber ihren Lauf.", "tokens": ["Und", "nimmt", "von", "sel\u00b7ber", "ih\u00b7ren", "Lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Et cetera! So ging es weiter.", "tokens": ["Et", "ce\u00b7te\u00b7ra", "!", "So", "ging", "es", "wei\u00b7ter", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was hilft die Philologenzunft!", "tokens": ["Was", "hilft", "die", "Phi\u00b7lo\u00b7lo\u00b7gen\u00b7zunft", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch Professoren werden heiter", "tokens": ["Auch", "Pro\u00b7fes\u00b7so\u00b7ren", "wer\u00b7den", "hei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "NN", "VAFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und werden wild in ihrer Brunft.", "tokens": ["Und", "wer\u00b7den", "wild", "in", "ih\u00b7rer", "Brunft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Nach so viel Sekt und S\u00fc\u00dfigkeiten", "tokens": ["Nach", "so", "viel", "Sekt", "und", "S\u00fc\u00b7\u00dfig\u00b7kei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "PIAT", "NN", "KON", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schmeckt uns die Wei\u00dfwurst und das Bier.", "tokens": ["Schmeckt", "uns", "die", "Wei\u00df\u00b7wurst", "und", "das", "Bier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Abschlu\u00df ist das Heimbegleiten", "tokens": ["Der", "Ab\u00b7schlu\u00df", "ist", "das", "Heim\u00b7be\u00b7glei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr jedes Paar. Warum nicht hier?", "tokens": ["F\u00fcr", "je\u00b7des", "Paar", ".", "Wa\u00b7rum", "nicht", "hier", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$.", "PWAV", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "Auch Josef sa\u00df in einem Wagen", "tokens": ["Auch", "Jo\u00b7sef", "sa\u00df", "in", "ei\u00b7nem", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlte, wie an ihn sich pre\u00dft',", "tokens": ["Und", "f\u00fchl\u00b7te", ",", "wie", "an", "ihn", "sich", "pre\u00dft'", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PWAV", "APPR", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was hier nicht unbefangen sagen,", "tokens": ["Was", "hier", "nicht", "un\u00b7be\u00b7fan\u00b7gen", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch sich sehr einfach denken l\u00e4\u00dft.", "tokens": ["Doch", "sich", "sehr", "ein\u00b7fach", "den\u00b7ken", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADV", "ADJD", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Er f\u00fchlte seine Pulse h\u00e4mmern,", "tokens": ["Er", "f\u00fchl\u00b7te", "sei\u00b7ne", "Pul\u00b7se", "h\u00e4m\u00b7mern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch wu\u00dft' er nicht, was sonst geschah;", "tokens": ["Doch", "wu\u00dft'", "er", "nicht", ",", "was", "sonst", "ge\u00b7schah", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "$,", "PRELS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn seinen Sinn umfing ein D\u00e4mmern,", "tokens": ["Denn", "sei\u00b7nen", "Sinn", "um\u00b7fing", "ein", "D\u00e4m\u00b7mern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df er nichts mehr Genaues sah.", "tokens": ["Da\u00df", "er", "nichts", "mehr", "Ge\u00b7nau\u00b7es", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "PIAT", "NN", "VVFIN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.69": {"line.1": {"text": "Er stolpert hastig \u00fcber Stiegen", "tokens": ["Er", "stol\u00b7pert", "has\u00b7tig", "\u00fc\u00b7ber", "Stie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00e4llt auch irgendwo ins Bett,", "tokens": ["Und", "f\u00e4llt", "auch", "ir\u00b7gend\u00b7wo", "ins", "Bett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und mu\u00df sehr lang darinnen liegen \u2013", "tokens": ["Und", "mu\u00df", "sehr", "lang", "da\u00b7rin\u00b7nen", "lie\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "ADJD", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das \u00fcbrige war wundernett.", "tokens": ["Das", "\u00fcb\u00b7ri\u00b7ge", "war", "wun\u00b7der\u00b7nett", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.70": {"line.1": {"text": "Er hat die Zeit bis abends sieben", "tokens": ["Er", "hat", "die", "Zeit", "bis", "a\u00b7bends", "sie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "ADV", "CARD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei diesem M\u00e4dchen zugebracht,", "tokens": ["Bei", "die\u00b7sem", "M\u00e4d\u00b7chen", "zu\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und fuhr alsdann zu seinen Lieben", "tokens": ["Und", "fuhr", "als\u00b7dann", "zu", "sei\u00b7nen", "Lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nach Freising etwa um halb acht.", "tokens": ["Nach", "Frei\u00b7sing", "et\u00b7wa", "um", "halb", "acht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "APPR", "ADJD", "CARD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.71": {"line.1": {"text": "Als er daheim nun angelangte,", "tokens": ["Als", "er", "da\u00b7heim", "nun", "an\u00b7ge\u00b7lang\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War er von solcher M\u00fcdigkeit,", "tokens": ["War", "er", "von", "sol\u00b7cher", "M\u00fc\u00b7dig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df seine Frau um ihn sich bangte;", "tokens": ["Da\u00df", "sei\u00b7ne", "Frau", "um", "ihn", "sich", "bang\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie macht' das Bett f\u00fcr ihn bereit.", "tokens": ["Sie", "macht'", "das", "Bett", "f\u00fcr", "ihn", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Und Josef hat sich ausgezogen", "tokens": ["Und", "Jo\u00b7sef", "hat", "sich", "aus\u00b7ge\u00b7zo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "PRF", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sprach, da\u00df er erk\u00e4ltet sei,", "tokens": ["Und", "sprach", ",", "da\u00df", "er", "er\u00b7k\u00e4l\u00b7tet", "sei", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PPER", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und hat noch dies und das gelogen,", "tokens": ["Und", "hat", "noch", "dies", "und", "das", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PDS", "KON", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn eine Frau fr\u00e4gt vielerlei.", "tokens": ["Denn", "ei\u00b7ne", "Frau", "fr\u00e4gt", "vie\u00b7ler\u00b7lei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "Da\u00df L\u00fcgen kurze Beine tragen,", "tokens": ["Da\u00df", "L\u00fc\u00b7gen", "kur\u00b7ze", "Bei\u00b7ne", "tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das zeigte sich hier wunderbar;", "tokens": ["Das", "zeig\u00b7te", "sich", "hier", "wun\u00b7der\u00b7bar", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn Josef ward so ganz geschlagen,", "tokens": ["Denn", "Jo\u00b7sef", "ward", "so", "ganz", "ge\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df hier f\u00fcr ihn kein Ausweg war.", "tokens": ["Da\u00df", "hier", "f\u00fcr", "ihn", "kein", "Aus\u00b7weg", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "PPER", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Er trug \u2013 da gibt es kein Entrinnen", "tokens": ["Er", "trug", "\u2013", "da", "gibt", "es", "kein", "Ent\u00b7rin\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "ADV", "VVFIN", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und kein Erkl\u00e4ren so und so \u2013", "tokens": ["Und", "kein", "Er\u00b7kl\u00e4\u00b7ren", "so", "und", "so", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "KON", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er trug aus duftig wei\u00dfem Linnen", "tokens": ["Er", "trug", "aus", "duf\u00b7tig", "wei\u00b7\u00dfem", "Lin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "\u2013 \u2013 Das H\u00f6schen seines Domino \u2013 \u2013!", "tokens": ["\u2013", "\u2013", "Das", "H\u00f6\u00b7schen", "sei\u00b7nes", "Do\u00b7mi\u00b7no", "\u2013", "\u2013", "!"], "token_info": ["punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "$(", "ART", "NN", "PPOSAT", "NN", "$(", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}