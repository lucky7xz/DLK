{"textgrid.poem.31230": {"metadata": {"author": {"name": "Holz, Arno", "birth": "N.A.", "death": "N.A."}, "title": "1L: Kleine Bluhmen wie au\u00df Gla\u00df", "genre": "verse", "period": "N.A.", "pub_year": 1896, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Kleine Bluhmen wie au\u00df Gla\u00df", "tokens": ["Klei\u00b7ne", "Bluh\u00b7men", "wie", "au\u00df", "Gla\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "KOKOM", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "seh ich gar zu gerne/", "tokens": ["seh", "ich", "gar", "zu", "ger\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKA", "ADV", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "durch das tunckel-gr\u00fcne Gra\u00df", "tokens": ["durch", "das", "tun\u00b7ckel\u00b7gr\u00fc\u00b7ne", "Gra\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "kukken sie wie Sterne.", "tokens": ["kuk\u00b7ken", "sie", "wie", "Ster\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Gelb und rosa/ roht und blau/", "tokens": ["Gelb", "und", "ro\u00b7sa", "/", "roht", "und", "blau", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "NE", "$(", "ADJD", "KON", "ADJD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "sch\u00f6n sind auch die weissen;", "tokens": ["sch\u00f6n", "sind", "auch", "die", "weis\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "ART", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Trittmadam und Himmelstau/", "tokens": ["Tritt\u00b7ma\u00b7dam", "und", "Him\u00b7mel\u00b7stau", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "wie sie alle heissen.", "tokens": ["wie", "sie", "al\u00b7le", "heis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Kom und gib mir mitten-drin", "tokens": ["Kom", "und", "gib", "mir", "mit\u00b7ten\u00b7drin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVIMP", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "K\u00fc\u00dfgens ohnbemessen.", "tokens": ["K\u00fc\u00df\u00b7gens", "ohn\u00b7be\u00b7mes\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Morgen sind sie lengst dahin", "tokens": ["Mor\u00b7gen", "sind", "sie", "lengst", "da\u00b7hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "VVFIN", "PAV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "und wir s\u00e4lbst \u2013 vergessen!", "tokens": ["und", "wir", "s\u00e4lbst", "\u2013", "ver\u00b7ges\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$(", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Kleine Bluhmen wie au\u00df Gla\u00df", "tokens": ["Klei\u00b7ne", "Bluh\u00b7men", "wie", "au\u00df", "Gla\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "KOKOM", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "seh ich gar zu gerne/", "tokens": ["seh", "ich", "gar", "zu", "ger\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKA", "ADV", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "durch das tunckel-gr\u00fcne Gra\u00df", "tokens": ["durch", "das", "tun\u00b7ckel\u00b7gr\u00fc\u00b7ne", "Gra\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "kukken sie wie Sterne.", "tokens": ["kuk\u00b7ken", "sie", "wie", "Ster\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.5": {"line.1": {"text": "Gelb und rosa/ roht und blau/", "tokens": ["Gelb", "und", "ro\u00b7sa", "/", "roht", "und", "blau", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "NE", "$(", "ADJD", "KON", "ADJD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "sch\u00f6n sind auch die weissen;", "tokens": ["sch\u00f6n", "sind", "auch", "die", "weis\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "ART", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Trittmadam und Himmelstau/", "tokens": ["Tritt\u00b7ma\u00b7dam", "und", "Him\u00b7mel\u00b7stau", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "wie sie alle heissen.", "tokens": ["wie", "sie", "al\u00b7le", "heis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "Kom und gib mir mitten-drin", "tokens": ["Kom", "und", "gib", "mir", "mit\u00b7ten\u00b7drin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVIMP", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "K\u00fc\u00dfgens ohnbemessen.", "tokens": ["K\u00fc\u00df\u00b7gens", "ohn\u00b7be\u00b7mes\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Morgen sind sie lengst dahin", "tokens": ["Mor\u00b7gen", "sind", "sie", "lengst", "da\u00b7hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "VVFIN", "PAV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "und wir s\u00e4lbst \u2013 vergessen!", "tokens": ["und", "wir", "s\u00e4lbst", "\u2013", "ver\u00b7ges\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$(", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}}}}