{"textgrid.poem.33342": {"metadata": {"author": {"name": "Blumauer, Aloys", "birth": "N.A.", "death": "N.A."}, "title": "Lob der Gans", "genre": "verse", "period": "N.A.", "pub_year": 1776, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Gro\u00dfm\u00e4chtige, zu Wasser und zu Lande", "tokens": ["Gro\u00df\u00b7m\u00e4ch\u00b7ti\u00b7ge", ",", "zu", "Was\u00b7ser", "und", "zu", "Lan\u00b7de"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJA", "$,", "APPR", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Gleich wohl behauste Frau!", "tokens": ["Gleich", "wohl", "be\u00b7haus\u00b7te", "Frau", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dir bring' ich hier im festlichen Gewande", "tokens": ["Dir", "bring'", "ich", "hier", "im", "fest\u00b7li\u00b7chen", "Ge\u00b7wan\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Mein Lobgedicht zur Schau.", "tokens": ["Mein", "Lob\u00b7ge\u00b7dicht", "zur", "Schau", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Man stellt uns in der eselfarb'nen Eule", "tokens": ["Man", "stellt", "uns", "in", "der", "e\u00b7sel\u00b7fa\u00b7rb'\u00b7nen", "Eu\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der Weisheit Sinnbild dar,", "tokens": ["Der", "Weis\u00b7heit", "Sinn\u00b7bild", "dar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und dir ward dieser Vorzug nicht zu Theile,", "tokens": ["Und", "dir", "ward", "die\u00b7ser", "Vor\u00b7zug", "nicht", "zu", "Thei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PDAT", "NN", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die zehnmal weiser war.", "tokens": ["Die", "zehn\u00b7mal", "wei\u00b7ser", "war", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Nur du lehrst wahre Weisheit uns auf Erden;", "tokens": ["Nur", "du", "lehrst", "wah\u00b7re", "Weis\u00b7heit", "uns", "auf", "Er\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ADV", "ADJA", "NN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Denn wo sonst lernten wir", "tokens": ["Denn", "wo", "sonst", "lern\u00b7ten", "wir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Kunst, mit leichter M\u00fche fett zu werden,", "tokens": ["Die", "Kunst", ",", "mit", "leich\u00b7ter", "M\u00fc\u00b7he", "fett", "zu", "wer\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ADJA", "NN", "ADJD", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "So gut, als wie von dir?", "tokens": ["So", "gut", ",", "als", "wie", "von", "dir", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "KOKOM", "APPR", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Du warst so gl\u00fccklich, Rom einst zu salviren", "tokens": ["Du", "warst", "so", "gl\u00fcck\u00b7lich", ",", "Rom", "einst", "zu", "sal\u00b7vi\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "NE", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Durch deine Schnatterey'n,", "tokens": ["Durch", "dei\u00b7ne", "Schnat\u00b7te\u00b7rey'n", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und f\u00fchrtest dadurch auch das Denunciren", "tokens": ["Und", "f\u00fchr\u00b7test", "da\u00b7durch", "auch", "das", "De\u00b7nun\u00b7ci\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PAV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In unsern Staaten ein.", "tokens": ["In", "un\u00b7sern", "Staa\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Und seit der Mutter Gans, so reich an Worten,", "tokens": ["Und", "seit", "der", "Mut\u00b7ter", "Gans", ",", "so", "reich", "an", "Wor\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "NN", "$,", "ADV", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Vermehrt die G\u00e4nschenschaar", "tokens": ["Ver\u00b7mehrt", "die", "G\u00e4n\u00b7schen\u00b7schaar"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bei unserm Fr\u00e4uleinvolk sich aller Orten", "tokens": ["Bei", "un\u00b7serm", "Fr\u00e4u\u00b7lein\u00b7volk", "sich", "al\u00b7ler", "Or\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PRF", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Mit jedem neuen Jahr.", "tokens": ["Mit", "je\u00b7dem", "neu\u00b7en", "Jahr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Ist gleich dein Kopf dumm, wie ein Steyrerst\u00fcckel,", "tokens": ["Ist", "gleich", "dein", "Kopf", "dumm", ",", "wie", "ein", "Stey\u00b7rer\u00b7st\u00fc\u00b7ckel", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "ADJD", "$,", "PWAV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "So gleicht im Hintergrund", "tokens": ["So", "gleicht", "im", "Hin\u00b7ter\u00b7grund"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dein Schweif doch auf ein Haar dem Perpendikel", "tokens": ["Dein", "Schweif", "doch", "auf", "ein", "Haar", "dem", "Per\u00b7pen\u00b7di\u00b7kel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In vieler Weiber Mund.", "tokens": ["In", "vie\u00b7ler", "Wei\u00b7ber", "Mund", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Dein langer Hals hat uns das Gl\u00fcck verliehen,", "tokens": ["Dein", "lan\u00b7ger", "Hals", "hat", "uns", "das", "Gl\u00fcck", "ver\u00b7lie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df der geplagte Mann", "tokens": ["Da\u00df", "der", "ge\u00b7plag\u00b7te", "Mann"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sein Haupt nach eines schweren Tages M\u00fchen", "tokens": ["Sein", "Haupt", "nach", "ei\u00b7nes", "schwe\u00b7ren", "Ta\u00b7ges", "M\u00fc\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sanft niederlegen kann.", "tokens": ["Sanft", "nie\u00b7der\u00b7le\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Und ohne deine weisheitsvollen Spulen,", "tokens": ["Und", "oh\u00b7ne", "dei\u00b7ne", "weis\u00b7heits\u00b7vol\u00b7len", "Spu\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo w\u00e4re Wissenschaft,", "tokens": ["Wo", "w\u00e4\u00b7re", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo uns're Kanzeleien, hohe Schulen,", "tokens": ["Wo", "un\u00b7s'\u00b7re", "Kan\u00b7ze\u00b7lei\u00b7en", ",", "ho\u00b7he", "Schu\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und uns're Autorschaft?", "tokens": ["Und", "un\u00b7s'\u00b7re", "Au\u00b7tor\u00b7schaft", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "Man macht sogar aus deinen Beinen Fl\u00f6ten,", "tokens": ["Man", "macht", "so\u00b7gar", "aus", "dei\u00b7nen", "Bei\u00b7nen", "Fl\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und zeiget damit an,", "tokens": ["Und", "zei\u00b7get", "da\u00b7mit", "an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df oft auch einem Hohlkopf von Poeten", "tokens": ["Da\u00df", "oft", "auch", "ei\u00b7nem", "Hohl\u00b7kopf", "von", "Po\u00b7et\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ein Lied gelingen kann.", "tokens": ["Ein", "Lied", "ge\u00b7lin\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Doch schlecht wirst du f\u00fcr alle diese grossen", "tokens": ["Doch", "schlecht", "wirst", "du", "f\u00fcr", "al\u00b7le", "die\u00b7se", "gros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VAFIN", "PPER", "APPR", "PIS", "PDAT", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Verdienste regalirt,", "tokens": ["Ver\u00b7diens\u00b7te", "re\u00b7ga\u00b7lirt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Am Martinstag zur Martyrin geschossen", "tokens": ["Am", "Mar\u00b7tins\u00b7tag", "zur", "Mar\u00b7ty\u00b7rin", "ge\u00b7schos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPRART", "NN", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und nicht kanonisirt!", "tokens": ["Und", "nicht", "ka\u00b7no\u00b7ni\u00b7sirt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Gro\u00dfm\u00e4chtige, zu Wasser und zu Lande", "tokens": ["Gro\u00df\u00b7m\u00e4ch\u00b7ti\u00b7ge", ",", "zu", "Was\u00b7ser", "und", "zu", "Lan\u00b7de"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJA", "$,", "APPR", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Gleich wohl behauste Frau!", "tokens": ["Gleich", "wohl", "be\u00b7haus\u00b7te", "Frau", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dir bring' ich hier im festlichen Gewande", "tokens": ["Dir", "bring'", "ich", "hier", "im", "fest\u00b7li\u00b7chen", "Ge\u00b7wan\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Mein Lobgedicht zur Schau.", "tokens": ["Mein", "Lob\u00b7ge\u00b7dicht", "zur", "Schau", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Man stellt uns in der eselfarb'nen Eule", "tokens": ["Man", "stellt", "uns", "in", "der", "e\u00b7sel\u00b7fa\u00b7rb'\u00b7nen", "Eu\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der Weisheit Sinnbild dar,", "tokens": ["Der", "Weis\u00b7heit", "Sinn\u00b7bild", "dar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und dir ward dieser Vorzug nicht zu Theile,", "tokens": ["Und", "dir", "ward", "die\u00b7ser", "Vor\u00b7zug", "nicht", "zu", "Thei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PDAT", "NN", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die zehnmal weiser war.", "tokens": ["Die", "zehn\u00b7mal", "wei\u00b7ser", "war", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Nur du lehrst wahre Weisheit uns auf Erden;", "tokens": ["Nur", "du", "lehrst", "wah\u00b7re", "Weis\u00b7heit", "uns", "auf", "Er\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ADV", "ADJA", "NN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Denn wo sonst lernten wir", "tokens": ["Denn", "wo", "sonst", "lern\u00b7ten", "wir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Kunst, mit leichter M\u00fche fett zu werden,", "tokens": ["Die", "Kunst", ",", "mit", "leich\u00b7ter", "M\u00fc\u00b7he", "fett", "zu", "wer\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ADJA", "NN", "ADJD", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "So gut, als wie von dir?", "tokens": ["So", "gut", ",", "als", "wie", "von", "dir", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "KOKOM", "APPR", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Du warst so gl\u00fccklich, Rom einst zu salviren", "tokens": ["Du", "warst", "so", "gl\u00fcck\u00b7lich", ",", "Rom", "einst", "zu", "sal\u00b7vi\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "NE", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Durch deine Schnatterey'n,", "tokens": ["Durch", "dei\u00b7ne", "Schnat\u00b7te\u00b7rey'n", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und f\u00fchrtest dadurch auch das Denunciren", "tokens": ["Und", "f\u00fchr\u00b7test", "da\u00b7durch", "auch", "das", "De\u00b7nun\u00b7ci\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PAV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In unsern Staaten ein.", "tokens": ["In", "un\u00b7sern", "Staa\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Und seit der Mutter Gans, so reich an Worten,", "tokens": ["Und", "seit", "der", "Mut\u00b7ter", "Gans", ",", "so", "reich", "an", "Wor\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "NN", "$,", "ADV", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Vermehrt die G\u00e4nschenschaar", "tokens": ["Ver\u00b7mehrt", "die", "G\u00e4n\u00b7schen\u00b7schaar"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bei unserm Fr\u00e4uleinvolk sich aller Orten", "tokens": ["Bei", "un\u00b7serm", "Fr\u00e4u\u00b7lein\u00b7volk", "sich", "al\u00b7ler", "Or\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PRF", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Mit jedem neuen Jahr.", "tokens": ["Mit", "je\u00b7dem", "neu\u00b7en", "Jahr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Ist gleich dein Kopf dumm, wie ein Steyrerst\u00fcckel,", "tokens": ["Ist", "gleich", "dein", "Kopf", "dumm", ",", "wie", "ein", "Stey\u00b7rer\u00b7st\u00fc\u00b7ckel", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "ADJD", "$,", "PWAV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "So gleicht im Hintergrund", "tokens": ["So", "gleicht", "im", "Hin\u00b7ter\u00b7grund"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dein Schweif doch auf ein Haar dem Perpendikel", "tokens": ["Dein", "Schweif", "doch", "auf", "ein", "Haar", "dem", "Per\u00b7pen\u00b7di\u00b7kel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In vieler Weiber Mund.", "tokens": ["In", "vie\u00b7ler", "Wei\u00b7ber", "Mund", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Dein langer Hals hat uns das Gl\u00fcck verliehen,", "tokens": ["Dein", "lan\u00b7ger", "Hals", "hat", "uns", "das", "Gl\u00fcck", "ver\u00b7lie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df der geplagte Mann", "tokens": ["Da\u00df", "der", "ge\u00b7plag\u00b7te", "Mann"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sein Haupt nach eines schweren Tages M\u00fchen", "tokens": ["Sein", "Haupt", "nach", "ei\u00b7nes", "schwe\u00b7ren", "Ta\u00b7ges", "M\u00fc\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sanft niederlegen kann.", "tokens": ["Sanft", "nie\u00b7der\u00b7le\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Und ohne deine weisheitsvollen Spulen,", "tokens": ["Und", "oh\u00b7ne", "dei\u00b7ne", "weis\u00b7heits\u00b7vol\u00b7len", "Spu\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo w\u00e4re Wissenschaft,", "tokens": ["Wo", "w\u00e4\u00b7re", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo uns're Kanzeleien, hohe Schulen,", "tokens": ["Wo", "un\u00b7s'\u00b7re", "Kan\u00b7ze\u00b7lei\u00b7en", ",", "ho\u00b7he", "Schu\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und uns're Autorschaft?", "tokens": ["Und", "un\u00b7s'\u00b7re", "Au\u00b7tor\u00b7schaft", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.19": {"line.1": {"text": "Man macht sogar aus deinen Beinen Fl\u00f6ten,", "tokens": ["Man", "macht", "so\u00b7gar", "aus", "dei\u00b7nen", "Bei\u00b7nen", "Fl\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und zeiget damit an,", "tokens": ["Und", "zei\u00b7get", "da\u00b7mit", "an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df oft auch einem Hohlkopf von Poeten", "tokens": ["Da\u00df", "oft", "auch", "ei\u00b7nem", "Hohl\u00b7kopf", "von", "Po\u00b7et\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ein Lied gelingen kann.", "tokens": ["Ein", "Lied", "ge\u00b7lin\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Doch schlecht wirst du f\u00fcr alle diese grossen", "tokens": ["Doch", "schlecht", "wirst", "du", "f\u00fcr", "al\u00b7le", "die\u00b7se", "gros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VAFIN", "PPER", "APPR", "PIS", "PDAT", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Verdienste regalirt,", "tokens": ["Ver\u00b7diens\u00b7te", "re\u00b7ga\u00b7lirt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Am Martinstag zur Martyrin geschossen", "tokens": ["Am", "Mar\u00b7tins\u00b7tag", "zur", "Mar\u00b7ty\u00b7rin", "ge\u00b7schos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPRART", "NN", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und nicht kanonisirt!", "tokens": ["Und", "nicht", "ka\u00b7no\u00b7ni\u00b7sirt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}