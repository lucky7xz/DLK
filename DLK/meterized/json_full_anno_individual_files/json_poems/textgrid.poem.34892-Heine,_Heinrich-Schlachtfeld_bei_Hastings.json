{"textgrid.poem.34892": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Schlachtfeld bei Hastings", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Abt von Waltham seufzte tief,", "tokens": ["Der", "Abt", "von", "Walt\u00b7ham", "seufz\u00b7te", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als er die Kunde vernommen,", "tokens": ["Als", "er", "die", "Kun\u00b7de", "ver\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df K\u00f6nig Harold elendiglich", "tokens": ["Da\u00df", "K\u00f6\u00b7nig", "Ha\u00b7rold", "e\u00b7len\u00b7dig\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "NE", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Hastings umgekommen.", "tokens": ["Bei", "Has\u00b7tings", "um\u00b7ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Zwei M\u00f6nche, Asgod und Ailrik genannt,", "tokens": ["Zwei", "M\u00f6n\u00b7che", ",", "As\u00b7god", "und", "Ail\u00b7rik", "ge\u00b7nannt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die schickt' er aus als Boten,", "tokens": ["Die", "schickt'", "er", "aus", "als", "Bo\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKVZ", "KOKOM", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie sollten suchen die Leiche Harolds", "tokens": ["Sie", "soll\u00b7ten", "su\u00b7chen", "die", "Lei\u00b7che", "Ha\u00b7rolds"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "VVFIN", "ART", "NN", "NE"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bei Hastings unter den Toten.", "tokens": ["Bei", "Has\u00b7tings", "un\u00b7ter", "den", "To\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Die M\u00f6nche gingen traurig fort", "tokens": ["Die", "M\u00f6n\u00b7che", "gin\u00b7gen", "trau\u00b7rig", "fort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kehrten traurig zur\u00fccke:", "tokens": ["Und", "kehr\u00b7ten", "trau\u00b7rig", "zu\u00b7r\u00fc\u00b7cke", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbhochw\u00fcrdiger Vater, die Welt ist uns gram,", "tokens": ["\u00bb", "hoch\u00b7w\u00fcr\u00b7di\u00b7ger", "Va\u00b7ter", ",", "die", "Welt", "ist", "uns", "gram", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "ART", "NN", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wir sind verlassen vom Gl\u00fccke.", "tokens": ["Wir", "sind", "ver\u00b7las\u00b7sen", "vom", "Gl\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Gefallen ist der be\u00dfre Mann,", "tokens": ["Ge\u00b7fal\u00b7len", "ist", "der", "be\u00df\u00b7re", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es siegte der Bankert, der schlechte,", "tokens": ["Es", "sieg\u00b7te", "der", "Ban\u00b7kert", ",", "der", "schlech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Gewappnete Diebe verteilen das Land", "tokens": ["Ge\u00b7wapp\u00b7ne\u00b7te", "Die\u00b7be", "ver\u00b7tei\u00b7len", "das", "Land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "VVFIN", "ART", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und machen den Freiling zum Knechte.", "tokens": ["Und", "ma\u00b7chen", "den", "Frei\u00b7ling", "zum", "Knech\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Der lausigste Lump aus der Normandie", "tokens": ["Der", "lau\u00b7sigs\u00b7te", "Lump", "aus", "der", "Nor\u00b7man\u00b7die"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Wird Lord auf der Insel der Briten;", "tokens": ["Wird", "Lord", "auf", "der", "In\u00b7sel", "der", "Bri\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ich sah einen Schneider aus Bayeux, er kam", "tokens": ["Ich", "sah", "ei\u00b7nen", "Schnei\u00b7der", "aus", "Ba\u00b7yeux", ",", "er", "kam"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NE", "APPR", "NE", "$,", "PPER", "VVFIN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Mit goldnen Sporen geritten.", "tokens": ["Mit", "gold\u00b7nen", "Spo\u00b7ren", "ge\u00b7rit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Weh dem, der jetzt ein Sachse ist!", "tokens": ["Weh", "dem", ",", "der", "jetzt", "ein", "Sach\u00b7se", "ist", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "$,", "PRELS", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Sachsenheilige droben", "tokens": ["Ihr", "Sach\u00b7sen\u00b7hei\u00b7li\u00b7ge", "dro\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Im Himmelreich, nehmt euch in acht,", "tokens": ["Im", "Him\u00b7mel\u00b7reich", ",", "nehmt", "euch", "in", "acht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "VVFIN", "PPER", "APPR", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ihr seid der Schmach nicht enthoben.", "tokens": ["Ihr", "seid", "der", "Schmach", "nicht", "ent\u00b7ho\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Jetzt wissen wir, was bedeutet hat", "tokens": ["Jetzt", "wis\u00b7sen", "wir", ",", "was", "be\u00b7deu\u00b7tet", "hat"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PWS", "VVFIN", "VAFIN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der gro\u00dfe Komet, der heuer", "tokens": ["Der", "gro\u00b7\u00dfe", "Ko\u00b7met", ",", "der", "heu\u00b7er"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Blutrot am n\u00e4chtlichen Himmel ritt", "tokens": ["Blut\u00b7rot", "am", "n\u00e4cht\u00b7li\u00b7chen", "Him\u00b7mel", "ritt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "ADJA", "NN", "VVFIN"], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Auf einem Besen von Feuer.", "tokens": ["Auf", "ei\u00b7nem", "Be\u00b7sen", "von", "Feu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Bei Hastings in Erf\u00fcllung ging", "tokens": ["Bei", "Has\u00b7tings", "in", "Er\u00b7f\u00fcl\u00b7lung", "ging"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Unsterns b\u00f6ses Zeichen,", "tokens": ["Des", "Uns\u00b7terns", "b\u00f6\u00b7ses", "Zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir waren auf dem Schlachtfeld dort", "tokens": ["Wir", "wa\u00b7ren", "auf", "dem", "Schlacht\u00b7feld", "dort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und suchten unter den Leichen.", "tokens": ["Und", "such\u00b7ten", "un\u00b7ter", "den", "Lei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "Wir suchten hin, wir suchten her,", "tokens": ["Wir", "such\u00b7ten", "hin", ",", "wir", "such\u00b7ten", "her", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bis alle Hoffnung verschwunden \u2013", "tokens": ["Bis", "al\u00b7le", "Hoff\u00b7nung", "ver\u00b7schwun\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Leichnam des toten K\u00f6nigs Harold,", "tokens": ["Den", "Leich\u00b7nam", "des", "to\u00b7ten", "K\u00f6\u00b7nigs", "Ha\u00b7rold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "NE", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir haben ihn nicht gefunden.\u00ab", "tokens": ["Wir", "ha\u00b7ben", "ihn", "nicht", "ge\u00b7fun\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "VVPP", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.10": {"line.1": {"text": "Asgod und Ailrik sprachen also;", "tokens": ["As\u00b7god", "und", "Ail\u00b7rik", "spra\u00b7chen", "al\u00b7so", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Abt rang jammernd die H\u00e4nde,", "tokens": ["Der", "Abt", "rang", "jam\u00b7mernd", "die", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Versank in tiefe Nachdenklichkeit", "tokens": ["Ver\u00b7sank", "in", "tie\u00b7fe", "Nach\u00b7denk\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und sprach mit Seufzen am Ende:", "tokens": ["Und", "sprach", "mit", "Seuf\u00b7zen", "am", "En\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.11": {"line.1": {"text": "\u00bbzu Grendelfield am Bardenstein,", "tokens": ["\u00bb", "zu", "Gren\u00b7del\u00b7field", "am", "Bar\u00b7den\u00b7stein", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Just in des Waldes Mitte,", "tokens": ["Just", "in", "des", "Wal\u00b7des", "Mit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da wohnet Edith Schwanenhals", "tokens": ["Da", "woh\u00b7net", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In einer d\u00fcrft'gen H\u00fctte.", "tokens": ["In", "ei\u00b7ner", "d\u00fcrft'\u00b7gen", "H\u00fct\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Man hie\u00df sie Edith Schwanenhals,", "tokens": ["Man", "hie\u00df", "sie", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil wie der Hals der Schw\u00e4ne", "tokens": ["Weil", "wie", "der", "Hals", "der", "Schw\u00e4\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ihr Nacken war; der K\u00f6nig Harold,", "tokens": ["Ihr", "Na\u00b7cken", "war", ";", "der", "K\u00f6\u00b7nig", "Ha\u00b7rold", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "$.", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er liebte die junge Sch\u00f6ne.", "tokens": ["Er", "lieb\u00b7te", "die", "jun\u00b7ge", "Sch\u00f6\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.13": {"line.1": {"text": "Er hat sie geliebt, gek\u00fc\u00dft und geherzt,", "tokens": ["Er", "hat", "sie", "ge\u00b7liebt", ",", "ge\u00b7k\u00fc\u00dft", "und", "ge\u00b7herzt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVPP", "$,", "VVPP", "KON", "VVPP", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und endlich verlassen, vergessen.", "tokens": ["Und", "end\u00b7lich", "ver\u00b7las\u00b7sen", ",", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "VVINF", "$,", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Die Zeit verflie\u00dft; wohl sechzehn Jahr'", "tokens": ["Die", "Zeit", "ver\u00b7flie\u00dft", ";", "wohl", "sech\u00b7zehn", "Jahr'"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "ADV", "CARD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verflossen unterdessen.", "tokens": ["Ver\u00b7flos\u00b7sen", "un\u00b7ter\u00b7des\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Begebt euch, Br\u00fcder, zu diesem Weib", "tokens": ["Be\u00b7gebt", "euch", ",", "Br\u00fc\u00b7der", ",", "zu", "die\u00b7sem", "Weib"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "NN", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und la\u00dft sie mit euch gehen", "tokens": ["Und", "la\u00dft", "sie", "mit", "euch", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "PPER", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zur\u00fcck nach Hastings, der Blick des Weibs", "tokens": ["Zu\u00b7r\u00fcck", "nach", "Has\u00b7tings", ",", "der", "Blick", "des", "Weibs"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wird dort den K\u00f6nig ersp\u00e4hen.", "tokens": ["Wird", "dort", "den", "K\u00f6\u00b7nig", "er\u00b7sp\u00e4\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Nach Waltham-Abtei hierher alsdann", "tokens": ["Nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", "hier\u00b7her", "als\u00b7dann"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "PAV", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sollt ihr die Leiche bringen,", "tokens": ["Sollt", "ihr", "die", "Lei\u00b7che", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Damit wir christlich bestatten den Leib", "tokens": ["Da\u00b7mit", "wir", "christ\u00b7lich", "be\u00b7stat\u00b7ten", "den", "Leib"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "VVFIN", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und f\u00fcr die Seele singen.\u00ab", "tokens": ["Und", "f\u00fcr", "die", "See\u00b7le", "sin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Um Mitternacht gelangten schon", "tokens": ["Um", "Mit\u00b7ter\u00b7nacht", "ge\u00b7lang\u00b7ten", "schon"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUI", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Boten zur H\u00fctte im Walde:", "tokens": ["Die", "Bo\u00b7ten", "zur", "H\u00fct\u00b7te", "im", "Wal\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "\u00bberwache, Edith Schwanenhals,", "tokens": ["\u00bb", "er\u00b7wa\u00b7che", ",", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und folge uns alsbalde.", "tokens": ["Und", "fol\u00b7ge", "uns", "als\u00b7bal\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Der Herzog der Normannen hat", "tokens": ["Der", "Her\u00b7zog", "der", "Nor\u00b7man\u00b7nen", "hat"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "VAFIN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Den Sieg davongetragen,", "tokens": ["Den", "Sieg", "da\u00b7von\u00b7ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und auf dem Feld bei Hastings liegt", "tokens": ["Und", "auf", "dem", "Feld", "bei", "Has\u00b7tings", "liegt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der K\u00f6nig Harold erschlagen.", "tokens": ["Der", "K\u00f6\u00b7nig", "Ha\u00b7rold", "er\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.18": {"line.1": {"text": "Komm mit nach Hastings, wir suchen dort", "tokens": ["Komm", "mit", "nach", "Has\u00b7tings", ",", "wir", "su\u00b7chen", "dort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "APPR", "APPR", "NE", "$,", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Den Leichnam unter den Toten,", "tokens": ["Den", "Leich\u00b7nam", "un\u00b7ter", "den", "To\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und bringen ihn nach Waltham-Abtei,", "tokens": ["Und", "brin\u00b7gen", "ihn", "nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie uns der Abt geboten.\u00ab", "tokens": ["Wie", "uns", "der", "Abt", "ge\u00b7bo\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Kein Wort sprach Edith Schwanenhals,", "tokens": ["Kein", "Wort", "sprach", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie sch\u00fcrzte sich geschwinde", "tokens": ["Sie", "sch\u00fcrz\u00b7te", "sich", "ge\u00b7schwin\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und folgte den M\u00f6nchen; ihr greisendes Haar,", "tokens": ["Und", "folg\u00b7te", "den", "M\u00f6n\u00b7chen", ";", "ihr", "grei\u00b7sen\u00b7des", "Haar", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Das flatterte wild im Winde.", "tokens": ["Das", "flat\u00b7ter\u00b7te", "wild", "im", "Win\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJD", "APPRART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.20": {"line.1": {"text": "Es folgte barfu\u00df das arme Weib", "tokens": ["Es", "folg\u00b7te", "bar\u00b7fu\u00df", "das", "ar\u00b7me", "Weib"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch S\u00fcmpfe und Baumgestr\u00fcppe.", "tokens": ["Durch", "S\u00fcmp\u00b7fe", "und", "Baum\u00b7ge\u00b7str\u00fcp\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bei Tagesanbruch gewahrten sie schon", "tokens": ["Bei", "Ta\u00b7ge\u00b7san\u00b7bruch", "ge\u00b7wahr\u00b7ten", "sie", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Zu Hastings die kreidige Klippe.", "tokens": ["Zu", "Has\u00b7tings", "die", "krei\u00b7di\u00b7ge", "Klip\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.21": {"line.1": {"text": "Der Nebel, der das Schlachtfeld bedeckt", "tokens": ["Der", "Ne\u00b7bel", ",", "der", "das", "Schlacht\u00b7feld", "be\u00b7deckt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVPP"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Als wie ein wei\u00dfes Leilich,", "tokens": ["Als", "wie", "ein", "wei\u00b7\u00dfes", "Lei\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zerflo\u00df allm\u00e4hlich; es flatterten auf", "tokens": ["Zer\u00b7flo\u00df", "all\u00b7m\u00e4h\u00b7lich", ";", "es", "flat\u00b7ter\u00b7ten", "auf"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "$.", "PPER", "VVFIN", "APPR"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Dohlen und kr\u00e4chzten abscheulich.", "tokens": ["Die", "Doh\u00b7len", "und", "kr\u00e4chz\u00b7ten", "ab\u00b7scheu\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "ADJD", "$."], "meter": "-+--+-++-", "measure": "iambic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Viel tausend Leichen lagen dort", "tokens": ["Viel", "tau\u00b7send", "Lei\u00b7chen", "la\u00b7gen", "dort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "CARD", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erb\u00e4rmlich auf blutiger Erde,", "tokens": ["Er\u00b7b\u00e4rm\u00b7lich", "auf", "blu\u00b7ti\u00b7ger", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Nackt ausgepl\u00fcndert, verst\u00fcmmelt, zerfleischt,", "tokens": ["Nackt", "aus\u00b7ge\u00b7pl\u00fcn\u00b7dert", ",", "ver\u00b7st\u00fcm\u00b7melt", ",", "zer\u00b7fleischt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Daneben die \u00c4ser der Pferde.", "tokens": ["Da\u00b7ne\u00b7ben", "die", "\u00c4\u00b7ser", "der", "Pfer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.23": {"line.1": {"text": "Es wadete Edith Schwanenhals", "tokens": ["Es", "wa\u00b7de\u00b7te", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Im Blute mit nackten F\u00fc\u00dfen;", "tokens": ["Im", "Blu\u00b7te", "mit", "nack\u00b7ten", "F\u00fc\u00b7\u00dfen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wie Pfeile aus ihrem stieren Aug'", "tokens": ["Wie", "Pfei\u00b7le", "aus", "ih\u00b7rem", "stie\u00b7ren", "Aug'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die forschenden Blicke schie\u00dfen.", "tokens": ["Die", "for\u00b7schen\u00b7den", "Bli\u00b7cke", "schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "Sie suchte hin, sie suchte her,", "tokens": ["Sie", "such\u00b7te", "hin", ",", "sie", "such\u00b7te", "her", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Oft mu\u00dfte sie m\u00fchsam verscheuchen", "tokens": ["Oft", "mu\u00df\u00b7te", "sie", "m\u00fch\u00b7sam", "ver\u00b7scheu\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "ADJD", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Die fra\u00dfbegierige Rabenschar;", "tokens": ["Die", "fra\u00df\u00b7be\u00b7gie\u00b7ri\u00b7ge", "Ra\u00b7ben\u00b7schar", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die M\u00f6nche hinter ihr keuchen.", "tokens": ["Die", "M\u00f6n\u00b7che", "hin\u00b7ter", "ihr", "keu\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.25": {"line.1": {"text": "Sie suchte schon den ganzen Tag,", "tokens": ["Sie", "such\u00b7te", "schon", "den", "gan\u00b7zen", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es ward schon Abend \u2013 pl\u00f6tzlich", "tokens": ["Es", "ward", "schon", "A\u00b7bend", "\u2013", "pl\u00f6tz\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$(", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bricht aus der Brust des armen Weibs", "tokens": ["Bricht", "aus", "der", "Brust", "des", "ar\u00b7men", "Weibs"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Ein geller Schrei, entsetzlich.", "tokens": ["Ein", "gel\u00b7ler", "Schrei", ",", "ent\u00b7setz\u00b7lich", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.26": {"line.1": {"text": "Gefunden hat Edith Schwanenhals", "tokens": ["Ge\u00b7fun\u00b7den", "hat", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "NE", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Des toten K\u00f6nigs Leiche.", "tokens": ["Des", "to\u00b7ten", "K\u00f6\u00b7nigs", "Lei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie sprach kein Wort, sie weinte nicht,", "tokens": ["Sie", "sprach", "kein", "Wort", ",", "sie", "wein\u00b7te", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "PPER", "VVFIN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie k\u00fc\u00dfte das Antlitz, das bleiche.", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "das", "Ant\u00b7litz", ",", "das", "blei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ART", "ADJA", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.27": {"line.1": {"text": "Sie k\u00fc\u00dfte die Stirne, sie k\u00fc\u00dfte den Mund,", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "die", "Stir\u00b7ne", ",", "sie", "k\u00fc\u00df\u00b7te", "den", "Mund", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Sie hielt ihn fest umschlossen;", "tokens": ["Sie", "hielt", "ihn", "fest", "um\u00b7schlos\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie k\u00fc\u00dfte auf des K\u00f6nigs Brust", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "auf", "des", "K\u00f6\u00b7nigs", "Brust"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Wunde blutumflossen.", "tokens": ["Die", "Wun\u00b7de", "blu\u00b7tum\u00b7flos\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.28": {"line.1": {"text": "Auf seiner Schulter erblickt sie auch \u2013", "tokens": ["Auf", "sei\u00b7ner", "Schul\u00b7ter", "er\u00b7blickt", "sie", "auch", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und sie bedeckt sie mit K\u00fcssen \u2013", "tokens": ["Und", "sie", "be\u00b7deckt", "sie", "mit", "K\u00fcs\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "NN", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Drei kleine Narben, Denkm\u00e4ler der Lust,", "tokens": ["Drei", "klei\u00b7ne", "Nar\u00b7ben", ",", "Denk\u00b7m\u00e4\u00b7ler", "der", "Lust", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "$,", "NN", "ART", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Die sie einst hineingebissen.", "tokens": ["Die", "sie", "einst", "hin\u00b7ein\u00b7ge\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Die M\u00f6nche konnten mittlerweil'", "tokens": ["Die", "M\u00f6n\u00b7che", "konn\u00b7ten", "mitt\u00b7ler\u00b7weil'"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Baumst\u00e4mme zusammenfugen;", "tokens": ["Baum\u00b7st\u00e4m\u00b7me", "zu\u00b7sam\u00b7men\u00b7fu\u00b7gen", ";"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Das war die Bahre, worauf sie alsdann", "tokens": ["Das", "war", "die", "Bah\u00b7re", ",", "wo\u00b7rauf", "sie", "als\u00b7dann"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Den toten K\u00f6nig trugen.", "tokens": ["Den", "to\u00b7ten", "K\u00f6\u00b7nig", "tru\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.30": {"line.1": {"text": "Sie trugen ihn nach Waltham-Abtei,", "tokens": ["Sie", "tru\u00b7gen", "ihn", "nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man ihn dort begr\u00fcbe;", "tokens": ["Da\u00df", "man", "ihn", "dort", "be\u00b7gr\u00fc\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es folgte Edith Schwanenhals", "tokens": ["Es", "folg\u00b7te", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Leiche ihrer Liebe.", "tokens": ["Der", "Lei\u00b7che", "ih\u00b7rer", "Lie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.31": {"line.1": {"text": "Sie sang die Totenlitanei'n", "tokens": ["Sie", "sang", "die", "To\u00b7ten\u00b7li\u00b7ta\u00b7nei'n"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In kindisch frommer Weise;", "tokens": ["In", "kin\u00b7disch", "from\u00b7mer", "Wei\u00b7se", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das klang so schauerlich in der Nacht \u2013", "tokens": ["Das", "klang", "so", "schau\u00b7er\u00b7lich", "in", "der", "Nacht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-++-+", "measure": "unknown.measure.penta"}, "line.4": {"text": "Die M\u00f6nche beteten leise. \u2013", "tokens": ["Die", "M\u00f6n\u00b7che", "be\u00b7te\u00b7ten", "lei\u00b7se", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.32": {"line.1": {"text": "Der Abt von Waltham seufzte tief,", "tokens": ["Der", "Abt", "von", "Walt\u00b7ham", "seufz\u00b7te", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als er die Kunde vernommen,", "tokens": ["Als", "er", "die", "Kun\u00b7de", "ver\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df K\u00f6nig Harold elendiglich", "tokens": ["Da\u00df", "K\u00f6\u00b7nig", "Ha\u00b7rold", "e\u00b7len\u00b7dig\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "NE", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Hastings umgekommen.", "tokens": ["Bei", "Has\u00b7tings", "um\u00b7ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "Zwei M\u00f6nche, Asgod und Ailrik genannt,", "tokens": ["Zwei", "M\u00f6n\u00b7che", ",", "As\u00b7god", "und", "Ail\u00b7rik", "ge\u00b7nannt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die schickt' er aus als Boten,", "tokens": ["Die", "schickt'", "er", "aus", "als", "Bo\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKVZ", "KOKOM", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie sollten suchen die Leiche Harolds", "tokens": ["Sie", "soll\u00b7ten", "su\u00b7chen", "die", "Lei\u00b7che", "Ha\u00b7rolds"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "VVFIN", "ART", "NN", "NE"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bei Hastings unter den Toten.", "tokens": ["Bei", "Has\u00b7tings", "un\u00b7ter", "den", "To\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.34": {"line.1": {"text": "Die M\u00f6nche gingen traurig fort", "tokens": ["Die", "M\u00f6n\u00b7che", "gin\u00b7gen", "trau\u00b7rig", "fort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kehrten traurig zur\u00fccke:", "tokens": ["Und", "kehr\u00b7ten", "trau\u00b7rig", "zu\u00b7r\u00fc\u00b7cke", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbhochw\u00fcrdiger Vater, die Welt ist uns gram,", "tokens": ["\u00bb", "hoch\u00b7w\u00fcr\u00b7di\u00b7ger", "Va\u00b7ter", ",", "die", "Welt", "ist", "uns", "gram", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "ART", "NN", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wir sind verlassen vom Gl\u00fccke.", "tokens": ["Wir", "sind", "ver\u00b7las\u00b7sen", "vom", "Gl\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.35": {"line.1": {"text": "Gefallen ist der be\u00dfre Mann,", "tokens": ["Ge\u00b7fal\u00b7len", "ist", "der", "be\u00df\u00b7re", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es siegte der Bankert, der schlechte,", "tokens": ["Es", "sieg\u00b7te", "der", "Ban\u00b7kert", ",", "der", "schlech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Gewappnete Diebe verteilen das Land", "tokens": ["Ge\u00b7wapp\u00b7ne\u00b7te", "Die\u00b7be", "ver\u00b7tei\u00b7len", "das", "Land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "VVFIN", "ART", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und machen den Freiling zum Knechte.", "tokens": ["Und", "ma\u00b7chen", "den", "Frei\u00b7ling", "zum", "Knech\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.36": {"line.1": {"text": "Der lausigste Lump aus der Normandie", "tokens": ["Der", "lau\u00b7sigs\u00b7te", "Lump", "aus", "der", "Nor\u00b7man\u00b7die"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Wird Lord auf der Insel der Briten;", "tokens": ["Wird", "Lord", "auf", "der", "In\u00b7sel", "der", "Bri\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ich sah einen Schneider aus Bayeux, er kam", "tokens": ["Ich", "sah", "ei\u00b7nen", "Schnei\u00b7der", "aus", "Ba\u00b7yeux", ",", "er", "kam"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NE", "APPR", "NE", "$,", "PPER", "VVFIN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Mit goldnen Sporen geritten.", "tokens": ["Mit", "gold\u00b7nen", "Spo\u00b7ren", "ge\u00b7rit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.37": {"line.1": {"text": "Weh dem, der jetzt ein Sachse ist!", "tokens": ["Weh", "dem", ",", "der", "jetzt", "ein", "Sach\u00b7se", "ist", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "$,", "PRELS", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Sachsenheilige droben", "tokens": ["Ihr", "Sach\u00b7sen\u00b7hei\u00b7li\u00b7ge", "dro\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Im Himmelreich, nehmt euch in acht,", "tokens": ["Im", "Him\u00b7mel\u00b7reich", ",", "nehmt", "euch", "in", "acht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "VVFIN", "PPER", "APPR", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ihr seid der Schmach nicht enthoben.", "tokens": ["Ihr", "seid", "der", "Schmach", "nicht", "ent\u00b7ho\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.38": {"line.1": {"text": "Jetzt wissen wir, was bedeutet hat", "tokens": ["Jetzt", "wis\u00b7sen", "wir", ",", "was", "be\u00b7deu\u00b7tet", "hat"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PWS", "VVFIN", "VAFIN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der gro\u00dfe Komet, der heuer", "tokens": ["Der", "gro\u00b7\u00dfe", "Ko\u00b7met", ",", "der", "heu\u00b7er"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Blutrot am n\u00e4chtlichen Himmel ritt", "tokens": ["Blut\u00b7rot", "am", "n\u00e4cht\u00b7li\u00b7chen", "Him\u00b7mel", "ritt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "ADJA", "NN", "VVFIN"], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Auf einem Besen von Feuer.", "tokens": ["Auf", "ei\u00b7nem", "Be\u00b7sen", "von", "Feu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.39": {"line.1": {"text": "Bei Hastings in Erf\u00fcllung ging", "tokens": ["Bei", "Has\u00b7tings", "in", "Er\u00b7f\u00fcl\u00b7lung", "ging"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Unsterns b\u00f6ses Zeichen,", "tokens": ["Des", "Uns\u00b7terns", "b\u00f6\u00b7ses", "Zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir waren auf dem Schlachtfeld dort", "tokens": ["Wir", "wa\u00b7ren", "auf", "dem", "Schlacht\u00b7feld", "dort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und suchten unter den Leichen.", "tokens": ["Und", "such\u00b7ten", "un\u00b7ter", "den", "Lei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.40": {"line.1": {"text": "Wir suchten hin, wir suchten her,", "tokens": ["Wir", "such\u00b7ten", "hin", ",", "wir", "such\u00b7ten", "her", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bis alle Hoffnung verschwunden \u2013", "tokens": ["Bis", "al\u00b7le", "Hoff\u00b7nung", "ver\u00b7schwun\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Leichnam des toten K\u00f6nigs Harold,", "tokens": ["Den", "Leich\u00b7nam", "des", "to\u00b7ten", "K\u00f6\u00b7nigs", "Ha\u00b7rold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "NE", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir haben ihn nicht gefunden.\u00ab", "tokens": ["Wir", "ha\u00b7ben", "ihn", "nicht", "ge\u00b7fun\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "VVPP", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.41": {"line.1": {"text": "Asgod und Ailrik sprachen also;", "tokens": ["As\u00b7god", "und", "Ail\u00b7rik", "spra\u00b7chen", "al\u00b7so", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Abt rang jammernd die H\u00e4nde,", "tokens": ["Der", "Abt", "rang", "jam\u00b7mernd", "die", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Versank in tiefe Nachdenklichkeit", "tokens": ["Ver\u00b7sank", "in", "tie\u00b7fe", "Nach\u00b7denk\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und sprach mit Seufzen am Ende:", "tokens": ["Und", "sprach", "mit", "Seuf\u00b7zen", "am", "En\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.42": {"line.1": {"text": "\u00bbzu Grendelfield am Bardenstein,", "tokens": ["\u00bb", "zu", "Gren\u00b7del\u00b7field", "am", "Bar\u00b7den\u00b7stein", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Just in des Waldes Mitte,", "tokens": ["Just", "in", "des", "Wal\u00b7des", "Mit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da wohnet Edith Schwanenhals", "tokens": ["Da", "woh\u00b7net", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In einer d\u00fcrft'gen H\u00fctte.", "tokens": ["In", "ei\u00b7ner", "d\u00fcrft'\u00b7gen", "H\u00fct\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.43": {"line.1": {"text": "Man hie\u00df sie Edith Schwanenhals,", "tokens": ["Man", "hie\u00df", "sie", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil wie der Hals der Schw\u00e4ne", "tokens": ["Weil", "wie", "der", "Hals", "der", "Schw\u00e4\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ihr Nacken war; der K\u00f6nig Harold,", "tokens": ["Ihr", "Na\u00b7cken", "war", ";", "der", "K\u00f6\u00b7nig", "Ha\u00b7rold", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "$.", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er liebte die junge Sch\u00f6ne.", "tokens": ["Er", "lieb\u00b7te", "die", "jun\u00b7ge", "Sch\u00f6\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.44": {"line.1": {"text": "Er hat sie geliebt, gek\u00fc\u00dft und geherzt,", "tokens": ["Er", "hat", "sie", "ge\u00b7liebt", ",", "ge\u00b7k\u00fc\u00dft", "und", "ge\u00b7herzt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVPP", "$,", "VVPP", "KON", "VVPP", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und endlich verlassen, vergessen.", "tokens": ["Und", "end\u00b7lich", "ver\u00b7las\u00b7sen", ",", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "VVINF", "$,", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Die Zeit verflie\u00dft; wohl sechzehn Jahr'", "tokens": ["Die", "Zeit", "ver\u00b7flie\u00dft", ";", "wohl", "sech\u00b7zehn", "Jahr'"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "ADV", "CARD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verflossen unterdessen.", "tokens": ["Ver\u00b7flos\u00b7sen", "un\u00b7ter\u00b7des\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.45": {"line.1": {"text": "Begebt euch, Br\u00fcder, zu diesem Weib", "tokens": ["Be\u00b7gebt", "euch", ",", "Br\u00fc\u00b7der", ",", "zu", "die\u00b7sem", "Weib"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "NN", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und la\u00dft sie mit euch gehen", "tokens": ["Und", "la\u00dft", "sie", "mit", "euch", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "PPER", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zur\u00fcck nach Hastings, der Blick des Weibs", "tokens": ["Zu\u00b7r\u00fcck", "nach", "Has\u00b7tings", ",", "der", "Blick", "des", "Weibs"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wird dort den K\u00f6nig ersp\u00e4hen.", "tokens": ["Wird", "dort", "den", "K\u00f6\u00b7nig", "er\u00b7sp\u00e4\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.46": {"line.1": {"text": "Nach Waltham-Abtei hierher alsdann", "tokens": ["Nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", "hier\u00b7her", "als\u00b7dann"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "PAV", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sollt ihr die Leiche bringen,", "tokens": ["Sollt", "ihr", "die", "Lei\u00b7che", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Damit wir christlich bestatten den Leib", "tokens": ["Da\u00b7mit", "wir", "christ\u00b7lich", "be\u00b7stat\u00b7ten", "den", "Leib"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "VVFIN", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und f\u00fcr die Seele singen.\u00ab", "tokens": ["Und", "f\u00fcr", "die", "See\u00b7le", "sin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.47": {"line.1": {"text": "Um Mitternacht gelangten schon", "tokens": ["Um", "Mit\u00b7ter\u00b7nacht", "ge\u00b7lang\u00b7ten", "schon"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUI", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Boten zur H\u00fctte im Walde:", "tokens": ["Die", "Bo\u00b7ten", "zur", "H\u00fct\u00b7te", "im", "Wal\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "\u00bberwache, Edith Schwanenhals,", "tokens": ["\u00bb", "er\u00b7wa\u00b7che", ",", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und folge uns alsbalde.", "tokens": ["Und", "fol\u00b7ge", "uns", "als\u00b7bal\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.48": {"line.1": {"text": "Der Herzog der Normannen hat", "tokens": ["Der", "Her\u00b7zog", "der", "Nor\u00b7man\u00b7nen", "hat"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "VAFIN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Den Sieg davongetragen,", "tokens": ["Den", "Sieg", "da\u00b7von\u00b7ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und auf dem Feld bei Hastings liegt", "tokens": ["Und", "auf", "dem", "Feld", "bei", "Has\u00b7tings", "liegt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der K\u00f6nig Harold erschlagen.", "tokens": ["Der", "K\u00f6\u00b7nig", "Ha\u00b7rold", "er\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.49": {"line.1": {"text": "Komm mit nach Hastings, wir suchen dort", "tokens": ["Komm", "mit", "nach", "Has\u00b7tings", ",", "wir", "su\u00b7chen", "dort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "APPR", "APPR", "NE", "$,", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Den Leichnam unter den Toten,", "tokens": ["Den", "Leich\u00b7nam", "un\u00b7ter", "den", "To\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und bringen ihn nach Waltham-Abtei,", "tokens": ["Und", "brin\u00b7gen", "ihn", "nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie uns der Abt geboten.\u00ab", "tokens": ["Wie", "uns", "der", "Abt", "ge\u00b7bo\u00b7ten", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.50": {"line.1": {"text": "Kein Wort sprach Edith Schwanenhals,", "tokens": ["Kein", "Wort", "sprach", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie sch\u00fcrzte sich geschwinde", "tokens": ["Sie", "sch\u00fcrz\u00b7te", "sich", "ge\u00b7schwin\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und folgte den M\u00f6nchen; ihr greisendes Haar,", "tokens": ["Und", "folg\u00b7te", "den", "M\u00f6n\u00b7chen", ";", "ihr", "grei\u00b7sen\u00b7des", "Haar", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Das flatterte wild im Winde.", "tokens": ["Das", "flat\u00b7ter\u00b7te", "wild", "im", "Win\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJD", "APPRART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.51": {"line.1": {"text": "Es folgte barfu\u00df das arme Weib", "tokens": ["Es", "folg\u00b7te", "bar\u00b7fu\u00df", "das", "ar\u00b7me", "Weib"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch S\u00fcmpfe und Baumgestr\u00fcppe.", "tokens": ["Durch", "S\u00fcmp\u00b7fe", "und", "Baum\u00b7ge\u00b7str\u00fcp\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bei Tagesanbruch gewahrten sie schon", "tokens": ["Bei", "Ta\u00b7ge\u00b7san\u00b7bruch", "ge\u00b7wahr\u00b7ten", "sie", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Zu Hastings die kreidige Klippe.", "tokens": ["Zu", "Has\u00b7tings", "die", "krei\u00b7di\u00b7ge", "Klip\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.52": {"line.1": {"text": "Der Nebel, der das Schlachtfeld bedeckt", "tokens": ["Der", "Ne\u00b7bel", ",", "der", "das", "Schlacht\u00b7feld", "be\u00b7deckt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVPP"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Als wie ein wei\u00dfes Leilich,", "tokens": ["Als", "wie", "ein", "wei\u00b7\u00dfes", "Lei\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zerflo\u00df allm\u00e4hlich; es flatterten auf", "tokens": ["Zer\u00b7flo\u00df", "all\u00b7m\u00e4h\u00b7lich", ";", "es", "flat\u00b7ter\u00b7ten", "auf"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "$.", "PPER", "VVFIN", "APPR"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Dohlen und kr\u00e4chzten abscheulich.", "tokens": ["Die", "Doh\u00b7len", "und", "kr\u00e4chz\u00b7ten", "ab\u00b7scheu\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "ADJD", "$."], "meter": "-+--+-++-", "measure": "iambic.tetra.relaxed"}}, "stanza.53": {"line.1": {"text": "Viel tausend Leichen lagen dort", "tokens": ["Viel", "tau\u00b7send", "Lei\u00b7chen", "la\u00b7gen", "dort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "CARD", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erb\u00e4rmlich auf blutiger Erde,", "tokens": ["Er\u00b7b\u00e4rm\u00b7lich", "auf", "blu\u00b7ti\u00b7ger", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Nackt ausgepl\u00fcndert, verst\u00fcmmelt, zerfleischt,", "tokens": ["Nackt", "aus\u00b7ge\u00b7pl\u00fcn\u00b7dert", ",", "ver\u00b7st\u00fcm\u00b7melt", ",", "zer\u00b7fleischt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Daneben die \u00c4ser der Pferde.", "tokens": ["Da\u00b7ne\u00b7ben", "die", "\u00c4\u00b7ser", "der", "Pfer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.54": {"line.1": {"text": "Es wadete Edith Schwanenhals", "tokens": ["Es", "wa\u00b7de\u00b7te", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Im Blute mit nackten F\u00fc\u00dfen;", "tokens": ["Im", "Blu\u00b7te", "mit", "nack\u00b7ten", "F\u00fc\u00b7\u00dfen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wie Pfeile aus ihrem stieren Aug'", "tokens": ["Wie", "Pfei\u00b7le", "aus", "ih\u00b7rem", "stie\u00b7ren", "Aug'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die forschenden Blicke schie\u00dfen.", "tokens": ["Die", "for\u00b7schen\u00b7den", "Bli\u00b7cke", "schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.55": {"line.1": {"text": "Sie suchte hin, sie suchte her,", "tokens": ["Sie", "such\u00b7te", "hin", ",", "sie", "such\u00b7te", "her", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Oft mu\u00dfte sie m\u00fchsam verscheuchen", "tokens": ["Oft", "mu\u00df\u00b7te", "sie", "m\u00fch\u00b7sam", "ver\u00b7scheu\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "ADJD", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Die fra\u00dfbegierige Rabenschar;", "tokens": ["Die", "fra\u00df\u00b7be\u00b7gie\u00b7ri\u00b7ge", "Ra\u00b7ben\u00b7schar", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die M\u00f6nche hinter ihr keuchen.", "tokens": ["Die", "M\u00f6n\u00b7che", "hin\u00b7ter", "ihr", "keu\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.56": {"line.1": {"text": "Sie suchte schon den ganzen Tag,", "tokens": ["Sie", "such\u00b7te", "schon", "den", "gan\u00b7zen", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es ward schon Abend \u2013 pl\u00f6tzlich", "tokens": ["Es", "ward", "schon", "A\u00b7bend", "\u2013", "pl\u00f6tz\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$(", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bricht aus der Brust des armen Weibs", "tokens": ["Bricht", "aus", "der", "Brust", "des", "ar\u00b7men", "Weibs"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Ein geller Schrei, entsetzlich.", "tokens": ["Ein", "gel\u00b7ler", "Schrei", ",", "ent\u00b7setz\u00b7lich", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.57": {"line.1": {"text": "Gefunden hat Edith Schwanenhals", "tokens": ["Ge\u00b7fun\u00b7den", "hat", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "NE", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Des toten K\u00f6nigs Leiche.", "tokens": ["Des", "to\u00b7ten", "K\u00f6\u00b7nigs", "Lei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie sprach kein Wort, sie weinte nicht,", "tokens": ["Sie", "sprach", "kein", "Wort", ",", "sie", "wein\u00b7te", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "PPER", "VVFIN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie k\u00fc\u00dfte das Antlitz, das bleiche.", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "das", "Ant\u00b7litz", ",", "das", "blei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ART", "ADJA", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.58": {"line.1": {"text": "Sie k\u00fc\u00dfte die Stirne, sie k\u00fc\u00dfte den Mund,", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "die", "Stir\u00b7ne", ",", "sie", "k\u00fc\u00df\u00b7te", "den", "Mund", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Sie hielt ihn fest umschlossen;", "tokens": ["Sie", "hielt", "ihn", "fest", "um\u00b7schlos\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie k\u00fc\u00dfte auf des K\u00f6nigs Brust", "tokens": ["Sie", "k\u00fc\u00df\u00b7te", "auf", "des", "K\u00f6\u00b7nigs", "Brust"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Wunde blutumflossen.", "tokens": ["Die", "Wun\u00b7de", "blu\u00b7tum\u00b7flos\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.59": {"line.1": {"text": "Auf seiner Schulter erblickt sie auch \u2013", "tokens": ["Auf", "sei\u00b7ner", "Schul\u00b7ter", "er\u00b7blickt", "sie", "auch", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und sie bedeckt sie mit K\u00fcssen \u2013", "tokens": ["Und", "sie", "be\u00b7deckt", "sie", "mit", "K\u00fcs\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "NN", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Drei kleine Narben, Denkm\u00e4ler der Lust,", "tokens": ["Drei", "klei\u00b7ne", "Nar\u00b7ben", ",", "Denk\u00b7m\u00e4\u00b7ler", "der", "Lust", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "$,", "NN", "ART", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Die sie einst hineingebissen.", "tokens": ["Die", "sie", "einst", "hin\u00b7ein\u00b7ge\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.60": {"line.1": {"text": "Die M\u00f6nche konnten mittlerweil'", "tokens": ["Die", "M\u00f6n\u00b7che", "konn\u00b7ten", "mitt\u00b7ler\u00b7weil'"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Baumst\u00e4mme zusammenfugen;", "tokens": ["Baum\u00b7st\u00e4m\u00b7me", "zu\u00b7sam\u00b7men\u00b7fu\u00b7gen", ";"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Das war die Bahre, worauf sie alsdann", "tokens": ["Das", "war", "die", "Bah\u00b7re", ",", "wo\u00b7rauf", "sie", "als\u00b7dann"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Den toten K\u00f6nig trugen.", "tokens": ["Den", "to\u00b7ten", "K\u00f6\u00b7nig", "tru\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.61": {"line.1": {"text": "Sie trugen ihn nach Waltham-Abtei,", "tokens": ["Sie", "tru\u00b7gen", "ihn", "nach", "Wal\u00b7tham\u00b7Ab\u00b7tei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man ihn dort begr\u00fcbe;", "tokens": ["Da\u00df", "man", "ihn", "dort", "be\u00b7gr\u00fc\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es folgte Edith Schwanenhals", "tokens": ["Es", "folg\u00b7te", "E\u00b7dith", "Schwa\u00b7nen\u00b7hals"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Leiche ihrer Liebe.", "tokens": ["Der", "Lei\u00b7che", "ih\u00b7rer", "Lie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.62": {"line.1": {"text": "Sie sang die Totenlitanei'n", "tokens": ["Sie", "sang", "die", "To\u00b7ten\u00b7li\u00b7ta\u00b7nei'n"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In kindisch frommer Weise;", "tokens": ["In", "kin\u00b7disch", "from\u00b7mer", "Wei\u00b7se", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das klang so schauerlich in der Nacht \u2013", "tokens": ["Das", "klang", "so", "schau\u00b7er\u00b7lich", "in", "der", "Nacht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-++-+", "measure": "unknown.measure.penta"}, "line.4": {"text": "Die M\u00f6nche beteten leise. \u2013", "tokens": ["Die", "M\u00f6n\u00b7che", "be\u00b7te\u00b7ten", "lei\u00b7se", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}