{"textgrid.poem.41462": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die meisten h\u00fcten nur die Sch\u00e4tze, die sie erben,", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die meisten h\u00fcten nur die Sch\u00e4tze, die sie erben,", "tokens": ["Die", "meis\u00b7ten", "h\u00fc\u00b7ten", "nur", "die", "Sch\u00e4t\u00b7ze", ",", "die", "sie", "er\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie einen todten Schatz, den niemand gr\u00f6\u00dfer macht.", "tokens": ["Wie", "ei\u00b7nen", "tod\u00b7ten", "Schatz", ",", "den", "nie\u00b7mand", "gr\u00f6\u00b7\u00dfer", "macht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "$,", "PRELS", "PIS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sie sammeln, was man meint, und bl\u00e4ttern Tag und Nacht,", "tokens": ["Sie", "sam\u00b7meln", ",", "was", "man", "meint", ",", "und", "bl\u00e4t\u00b7tern", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "PIS", "VVFIN", "$,", "KON", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bis sie, sich unbekannt und unentwickelt, sterben,", "tokens": ["Bis", "sie", ",", "sich", "un\u00b7be\u00b7kannt", "und", "un\u00b7ent\u00b7wi\u00b7ckelt", ",", "ster\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PRF", "ADJD", "KON", "ADJD", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ihr unfruchtbarer Witz hat nichts hervorgebracht.", "tokens": ["Ihr", "un\u00b7frucht\u00b7ba\u00b7rer", "Witz", "hat", "nichts", "her\u00b7vor\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "So ist ein Hobbes nicht erfahren.", "tokens": ["So", "ist", "ein", "Hob\u00b7bes", "nicht", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er irrt zwar oft, doch hat er selbst gedacht.", "tokens": ["Er", "irrt", "zwar", "oft", ",", "doch", "hat", "er", "selbst", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Des stolzen Britten Lehrer waren", "tokens": ["Des", "stol\u00b7zen", "Brit\u00b7ten", "Leh\u00b7rer", "wa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Homer, Virgil, Thucydides, Euclid,", "tokens": ["Ho\u00b7mer", ",", "Vir\u00b7gil", ",", "Thu\u00b7cy\u00b7di\u00b7des", ",", "Eu\u00b7clid", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.5": {"text": "Die las er stets mit Wahl und Unterschied.", "tokens": ["Die", "las", "er", "stets", "mit", "Wahl", "und", "Un\u00b7ter\u00b7schied", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Er w\u00e4re, sagt' er oft, wohl nie geschickt gewesen,", "tokens": ["Er", "w\u00e4\u00b7re", ",", "sagt'", "er", "oft", ",", "wohl", "nie", "ge\u00b7schickt", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$,", "ADV", "ADV", "VVPP", "VAPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Dinge tiefer einzusehn,", "tokens": ["Die", "Din\u00b7ge", "tie\u00b7fer", "ein\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Die Schulgelehrte halb verstehn,", "tokens": ["Die", "Schul\u00b7ge\u00b7lehr\u00b7te", "halb", "ver\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "H\u00e4tt' er so viel, wie sie, gelesen.", "tokens": ["H\u00e4tt'", "er", "so", "viel", ",", "wie", "sie", ",", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "$,", "PWAV", "PPER", "$,", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Die meisten h\u00fcten nur die Sch\u00e4tze, die sie erben,", "tokens": ["Die", "meis\u00b7ten", "h\u00fc\u00b7ten", "nur", "die", "Sch\u00e4t\u00b7ze", ",", "die", "sie", "er\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie einen todten Schatz, den niemand gr\u00f6\u00dfer macht.", "tokens": ["Wie", "ei\u00b7nen", "tod\u00b7ten", "Schatz", ",", "den", "nie\u00b7mand", "gr\u00f6\u00b7\u00dfer", "macht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "$,", "PRELS", "PIS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sie sammeln, was man meint, und bl\u00e4ttern Tag und Nacht,", "tokens": ["Sie", "sam\u00b7meln", ",", "was", "man", "meint", ",", "und", "bl\u00e4t\u00b7tern", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "PIS", "VVFIN", "$,", "KON", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bis sie, sich unbekannt und unentwickelt, sterben,", "tokens": ["Bis", "sie", ",", "sich", "un\u00b7be\u00b7kannt", "und", "un\u00b7ent\u00b7wi\u00b7ckelt", ",", "ster\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PRF", "ADJD", "KON", "ADJD", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ihr unfruchtbarer Witz hat nichts hervorgebracht.", "tokens": ["Ihr", "un\u00b7frucht\u00b7ba\u00b7rer", "Witz", "hat", "nichts", "her\u00b7vor\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "So ist ein Hobbes nicht erfahren.", "tokens": ["So", "ist", "ein", "Hob\u00b7bes", "nicht", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er irrt zwar oft, doch hat er selbst gedacht.", "tokens": ["Er", "irrt", "zwar", "oft", ",", "doch", "hat", "er", "selbst", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Des stolzen Britten Lehrer waren", "tokens": ["Des", "stol\u00b7zen", "Brit\u00b7ten", "Leh\u00b7rer", "wa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Homer, Virgil, Thucydides, Euclid,", "tokens": ["Ho\u00b7mer", ",", "Vir\u00b7gil", ",", "Thu\u00b7cy\u00b7di\u00b7des", ",", "Eu\u00b7clid", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.5": {"text": "Die las er stets mit Wahl und Unterschied.", "tokens": ["Die", "las", "er", "stets", "mit", "Wahl", "und", "Un\u00b7ter\u00b7schied", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Er w\u00e4re, sagt' er oft, wohl nie geschickt gewesen,", "tokens": ["Er", "w\u00e4\u00b7re", ",", "sagt'", "er", "oft", ",", "wohl", "nie", "ge\u00b7schickt", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$,", "ADV", "ADV", "VVPP", "VAPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Dinge tiefer einzusehn,", "tokens": ["Die", "Din\u00b7ge", "tie\u00b7fer", "ein\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Die Schulgelehrte halb verstehn,", "tokens": ["Die", "Schul\u00b7ge\u00b7lehr\u00b7te", "halb", "ver\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "H\u00e4tt' er so viel, wie sie, gelesen.", "tokens": ["H\u00e4tt'", "er", "so", "viel", ",", "wie", "sie", ",", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "$,", "PWAV", "PPER", "$,", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}