{"textgrid.poem.49264": {"metadata": {"author": {"name": "Opitz, Martin", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die Thr\u00e4nen voller Angst, die Seufftzer manigfalt,", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die Thr\u00e4nen voller Angst, die Seufftzer manigfalt,", "tokens": ["Die", "Thr\u00e4\u00b7nen", "vol\u00b7ler", "Angst", ",", "die", "Seufft\u00b7zer", "ma\u00b7nig\u00b7falt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Augen roth als Blut, die traurige Gestalt,", "tokens": ["Die", "Au\u00b7gen", "roth", "als", "Blut", ",", "die", "trau\u00b7ri\u00b7ge", "Ge\u00b7stalt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr Eltern und die Klagen,", "tokens": ["Ihr", "El\u00b7tern", "und", "die", "Kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Vor euer treues Kind,", "tokens": ["Vor", "eu\u00b7er", "treu\u00b7es", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Mu\u00df Jederman nur sagen,", "tokens": ["Mu\u00df", "Je\u00b7der\u00b7man", "nur", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Da\u00df sie nicht unrecht sind.", "tokens": ["Da\u00df", "sie", "nicht", "un\u00b7recht", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "NN", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Wir armen Sterblichen, wie gar ein nichtigs Ding", "tokens": ["Wir", "ar\u00b7men", "Sterb\u00b7li\u00b7chen", ",", "wie", "gar", "ein", "nich\u00b7tigs", "Ding"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "NN", "$,", "PWAV", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist dieses, was wir sind? So bald die Th\u00fcr auffgieng,", "tokens": ["Ist", "die\u00b7ses", ",", "was", "wir", "sind", "?", "So", "bald", "die", "Th\u00fcr", "auff\u00b7gieng", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "$,", "PRELS", "PPER", "VAFIN", "$.", "ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "War Freud' an allen Enden,", "tokens": ["War", "Freud'", "an", "al\u00b7len", "En\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Au\u00df Hoffnung, euer Sohn", "tokens": ["Au\u00df", "Hoff\u00b7nung", ",", "eu\u00b7er", "Sohn"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$,", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Der w\u00fcrd' ein Schreiben senden;", "tokens": ["Der", "w\u00fcrd'", "ein", "Schrei\u00b7ben", "sen\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "O nein, er ist darvon.", "tokens": ["O", "nein", ",", "er", "ist", "dar\u00b7von", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$,", "PPER", "VAFIN", "PAV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Er ist hinweg gerafft, der noch vor kurtzer Zeit", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ",", "der", "noch", "vor", "kurt\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "PRELS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bedacht war spat und fr\u00fc mit h\u00f6chster Embsigkeit,", "tokens": ["Be\u00b7dacht", "war", "spat", "und", "fr\u00fc", "mit", "h\u00f6chs\u00b7ter", "Emb\u00b7sig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "KON", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie seine gantze Jugend", "tokens": ["Wie", "sei\u00b7ne", "gant\u00b7ze", "Ju\u00b7gend"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Recht wol w\u00fcrd' angewand,", "tokens": ["Recht", "wol", "w\u00fcrd'", "an\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Damit er K\u00fcnst' und Tugend", "tokens": ["Da\u00b7mit", "er", "K\u00fcnst'", "und", "Tu\u00b7gend"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Br\u00e4cht' in sein Vatterland.", "tokens": ["Br\u00e4cht'", "in", "sein", "Vat\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "++-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Er ist hinweg gerafft: ihr hattet schon gedacht,", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ":", "ihr", "hat\u00b7tet", "schon", "ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihr w\u00fcrdet, wann ihn Gott gesund anheim gebracht,", "tokens": ["Ihr", "w\u00fcr\u00b7det", ",", "wann", "ihn", "Gott", "ge\u00b7sund", "an\u00b7heim", "ge\u00b7bracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "PWAV", "PPER", "NN", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Augen k\u00f6nnen weyden", "tokens": ["Die", "Au\u00b7gen", "k\u00f6n\u00b7nen", "wey\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und ihm entgegen ziehn", "tokens": ["Und", "ihm", "ent\u00b7ge\u00b7gen", "ziehn"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "PTKVZ", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "In voller Lust und Freuden;", "tokens": ["In", "vol\u00b7ler", "Lust", "und", "Freu\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nein, nein, er ist dahin.", "tokens": ["Nein", ",", "nein", ",", "er", "ist", "da\u00b7hin", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PTKANT", "$,", "PPER", "VAFIN", "PAV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "O ungl\u00fcckhaffter Bott', ist di\u00df nun seine M\u00fch?", "tokens": ["O", "un\u00b7gl\u00fcck\u00b7haff\u00b7ter", "Bott'", ",", "ist", "di\u00df", "nun", "sei\u00b7ne", "M\u00fch", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "VAFIN", "PDS", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist di\u00df sein Schwei\u00df und Frost, sein Lesen spat unnd fr\u00fch?", "tokens": ["Ist", "di\u00df", "sein", "Schwei\u00df", "und", "Frost", ",", "sein", "Le\u00b7sen", "spat", "unnd", "fr\u00fch", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PPOSAT", "NN", "KON", "NN", "$,", "PPOSAT", "NN", "VVFIN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein einiges Gesetze", "tokens": ["Kein", "ei\u00b7ni\u00b7ges", "Ge\u00b7set\u00b7ze"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Steht im Justinian,", "tokens": ["Steht", "im", "Jus\u00b7ti\u00b7ni\u00b7an", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Das durch das schwartze Netze", "tokens": ["Das", "durch", "das", "schwart\u00b7ze", "Net\u00b7ze"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "De\u00df Todes reissen kan.", "tokens": ["De\u00df", "To\u00b7des", "reis\u00b7sen", "kan", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Er ist hinweg gerafft, der Mutter beste Zier,", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ",", "der", "Mut\u00b7ter", "bes\u00b7te", "Zier", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "De\u00df Vatters grosser Trost, und zwar sehr weit von hier,", "tokens": ["De\u00df", "Vat\u00b7ters", "gros\u00b7ser", "Trost", ",", "und", "zwar", "sehr", "weit", "von", "hier", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "KON", "ADV", "ADV", "ADJD", "APPR", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So da\u00df sie ihn mit Pflegen", "tokens": ["So", "da\u00df", "sie", "ihn", "mit", "Pfle\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und Warten nie gelabt,", "tokens": ["Und", "War\u00b7ten", "nie", "ge\u00b7labt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Auch in den Sarg zu legen", "tokens": ["Auch", "in", "den", "Sarg", "zu", "le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nicht haben Fug gehabt.", "tokens": ["Nicht", "ha\u00b7ben", "Fug", "ge\u00b7habt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VAFIN", "NN", "VAPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Die Schwester hett' ihm doch noch einen treuen Ku\u00df", "tokens": ["Die", "Schwes\u00b7ter", "hett'", "ihm", "doch", "noch", "ei\u00b7nen", "treu\u00b7en", "Ku\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gegeben auff den Weg und letzten Abschiedgru\u00df,", "tokens": ["Ge\u00b7ge\u00b7ben", "auff", "den", "Weg", "und", "letz\u00b7ten", "Ab\u00b7schied\u00b7gru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hett' ihm zum Liebeszeichen", "tokens": ["Hett'", "ihm", "zum", "Lie\u00b7bes\u00b7zei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die Augen zugedr\u00fcckt", "tokens": ["Die", "Au\u00b7gen", "zu\u00b7ge\u00b7dr\u00fcckt"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "VVPP"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und seine blasse Leichen", "tokens": ["Und", "sei\u00b7ne", "blas\u00b7se", "Lei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Mit Blumen au\u00dfgeschm\u00fcckt.", "tokens": ["Mit", "Blu\u00b7men", "au\u00df\u00b7ge\u00b7schm\u00fcckt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Es lege sein Gebein' in dieser sch\u00f6nen Statt,", "tokens": ["Es", "le\u00b7ge", "sein", "Ge\u00b7bein'", "in", "die\u00b7ser", "sch\u00f6\u00b7nen", "Statt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dergleichen weit unnd breit gantz Teutschland nicht mehr hat,", "tokens": ["Derg\u00b7lei\u00b7chen", "weit", "unnd", "breit", "gantz", "Teutschland", "nicht", "mehr", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "KON", "ADJD", "ADV", "NN", "PTKNEG", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Es hette sampt den Seinen", "tokens": ["Es", "het\u00b7te", "sampt", "den", "Sei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und die ihm sonst bekand,", "tokens": ["Und", "die", "ihm", "sonst", "be\u00b7kand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Auch helffen umb ihn weinen", "tokens": ["Auch", "helf\u00b7fen", "umb", "ihn", "wei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVINF", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Das grosse Vatterland.", "tokens": ["Das", "gros\u00b7se", "Vat\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Nein, der, der \u00fcber uns sitzt, aller Wei\u00dfheit voll,", "tokens": ["Nein", ",", "der", ",", "der", "\u00fc\u00b7ber", "uns", "sitzt", ",", "al\u00b7ler", "Wei\u00df\u00b7heit", "voll", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "$,", "PRELS", "APPR", "PPER", "VVFIN", "$,", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wei\u00df, wenn er uns hier seyn und nicht seyn lassen soll;", "tokens": ["Wei\u00df", ",", "wenn", "er", "uns", "hier", "seyn", "und", "nicht", "seyn", "las\u00b7sen", "soll", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "PPER", "PPER", "ADV", "VAINF", "KON", "PTKNEG", "VAINF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein Mensch vermag zu kommen", "tokens": ["Kein", "Mensch", "ver\u00b7mag", "zu", "kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ohn ihn auff diese Welt,", "tokens": ["Ohn", "ihn", "auff", "die\u00b7se", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Wird auch nicht weggenommen", "tokens": ["Wird", "auch", "nicht", "weg\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PTKNEG", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Als wann es ihm gefellt.", "tokens": ["Als", "wann", "es", "ihm", "ge\u00b7fellt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Was hilfft das Weinen dann? Ja, wof\u00fcr Creutz unnd Pein", "tokens": ["Was", "hilfft", "das", "Wei\u00b7nen", "dann", "?", "Ja", ",", "wo\u00b7f\u00fcr", "Creutz", "unnd", "Pein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "ART", "NN", "ADV", "$.", "PTKANT", "$,", "PWAV", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Leyd und Kl\u00e4glich-Thun uns kan beh\u00fclfflich seyn,", "tokens": ["Das", "Leyd", "und", "Kl\u00e4g\u00b7lich\u00b7Thun", "uns", "kan", "be\u00b7h\u00fclf\u00b7flich", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "PPER", "VMFIN", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So sind die tr\u00fcben Zehren", "tokens": ["So", "sind", "die", "tr\u00fc\u00b7ben", "Zeh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Viel werther noch als Gold,", "tokens": ["Viel", "wert\u00b7her", "noch", "als", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "KOUS", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Jetzt, nun sie ihm nicht wehren,", "tokens": ["Jetzt", ",", "nun", "sie", "ihm", "nicht", "weh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "PPER", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "So hei\u00dft es nur Gedult.", "tokens": ["So", "hei\u00dft", "es", "nur", "Ge\u00b7dult", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Er ist der Welt entw\u00fcscht, da nichts als Krieg unnd Streit,", "tokens": ["Er", "ist", "der", "Welt", "ent\u00b7w\u00fcscht", ",", "da", "nichts", "als", "Krieg", "unnd", "Streit", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$,", "KOUS", "PIS", "KOKOM", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Als angeschminckte Lieb', als Ha\u00df und grimmer Neyd,", "tokens": ["Als", "an\u00b7ge\u00b7schminck\u00b7te", "Lieb'", ",", "als", "Ha\u00df", "und", "grim\u00b7mer", "Neyd", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$,", "KOUS", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als Schand' und b\u00f6se L\u00fcsten", "tokens": ["Als", "Schand'", "und", "b\u00f6\u00b7se", "L\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "In vollem Schwange gehn", "tokens": ["In", "vol\u00b7lem", "Schwan\u00b7ge", "gehn"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und Laster, die uns Cristen", "tokens": ["Und", "Las\u00b7ter", ",", "die", "uns", "Cris\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NN", "$,", "PRELS", "PPER", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nur nicht zu nennen stehn.", "tokens": ["Nur", "nicht", "zu", "nen\u00b7nen", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "PTKZU", "VVINF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Er schl\u00e4fft bey Vielen auch, die ewig sind bekand,", "tokens": ["Er", "schl\u00e4fft", "bey", "Vie\u00b7len", "auch", ",", "die", "e\u00b7wig", "sind", "be\u00b7kand", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "ADV", "$,", "PRELS", "ADJD", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch Zuthun der Vernunfft und die gelehrte Hand,", "tokens": ["Durch", "Zu\u00b7thun", "der", "Ver\u00b7nunfft", "und", "die", "ge\u00b7lehr\u00b7te", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "KON", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Hier wo die Ill' und Breusche", "tokens": ["Hier", "wo", "die", "Ill'", "und", "Breu\u00b7sche"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PWAV", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sich mengen in den Rein", "tokens": ["Sich", "men\u00b7gen", "in", "den", "Rein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Mit lieblichem Ger\u00e4usche", "tokens": ["Mit", "lieb\u00b7li\u00b7chem", "Ge\u00b7r\u00e4u\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und reich an Fr\u00fcchten seyn.", "tokens": ["Und", "reich", "an", "Fr\u00fcch\u00b7ten", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Er ist auff eine Schul, in der Gott selber lehrt", "tokens": ["Er", "ist", "auff", "ei\u00b7ne", "Schul", ",", "in", "der", "Gott", "sel\u00b7ber", "lehrt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "$,", "APPR", "ART", "NN", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Solch unerforschlichs Ding, das noch kein Ohr geh\u00f6rt.", "tokens": ["Solch", "un\u00b7er\u00b7for\u00b7schlichs", "Ding", ",", "das", "noch", "kein", "Ohr", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "PRELS", "ADV", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was wir an jetzund kennen", "tokens": ["Was", "wir", "an", "je\u00b7tzund", "ken\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "ADV", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Hat weder Art noch Krafft", "tokens": ["Hat", "we\u00b7der", "Art", "noch", "Krafft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "KON", "NN", "ADV", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und ist ein Traum zu nennen", "tokens": ["Und", "ist", "ein", "Traum", "zu", "nen\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Der rechten Wissenschafft.", "tokens": ["Der", "rech\u00b7ten", "Wis\u00b7sen\u00b7schafft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Wol dir, du liebe Seel', empfinde deine Lust,", "tokens": ["Wol", "dir", ",", "du", "lie\u00b7be", "Seel'", ",", "emp\u00b7fin\u00b7de", "dei\u00b7ne", "Lust", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$,", "PPER", "ADJA", "NN", "$,", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Geneu\u00df der Fr\u00f6ligkeit, die uns noch unbewust,", "tokens": ["Ge\u00b7neu\u00df", "der", "Fr\u00f6\u00b7lig\u00b7keit", ",", "die", "uns", "noch", "un\u00b7be\u00b7wust", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In die du auffgenommen", "tokens": ["In", "die", "du", "auff\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nun bist ohn End und Ziel;", "tokens": ["Nun", "bist", "ohn", "End", "und", "Ziel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Wir wollen zu dir kommen", "tokens": ["Wir", "wol\u00b7len", "zu", "dir", "kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wo, wie und wann Gott wil.", "tokens": ["Wo", ",", "wie", "und", "wann", "Gott", "wil", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "PWAV", "KON", "PWAV", "NN", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Die Thr\u00e4nen voller Angst, die Seufftzer manigfalt,", "tokens": ["Die", "Thr\u00e4\u00b7nen", "vol\u00b7ler", "Angst", ",", "die", "Seufft\u00b7zer", "ma\u00b7nig\u00b7falt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Augen roth als Blut, die traurige Gestalt,", "tokens": ["Die", "Au\u00b7gen", "roth", "als", "Blut", ",", "die", "trau\u00b7ri\u00b7ge", "Ge\u00b7stalt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr Eltern und die Klagen,", "tokens": ["Ihr", "El\u00b7tern", "und", "die", "Kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Vor euer treues Kind,", "tokens": ["Vor", "eu\u00b7er", "treu\u00b7es", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Mu\u00df Jederman nur sagen,", "tokens": ["Mu\u00df", "Je\u00b7der\u00b7man", "nur", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Da\u00df sie nicht unrecht sind.", "tokens": ["Da\u00df", "sie", "nicht", "un\u00b7recht", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "NN", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Wir armen Sterblichen, wie gar ein nichtigs Ding", "tokens": ["Wir", "ar\u00b7men", "Sterb\u00b7li\u00b7chen", ",", "wie", "gar", "ein", "nich\u00b7tigs", "Ding"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "NN", "$,", "PWAV", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist dieses, was wir sind? So bald die Th\u00fcr auffgieng,", "tokens": ["Ist", "die\u00b7ses", ",", "was", "wir", "sind", "?", "So", "bald", "die", "Th\u00fcr", "auff\u00b7gieng", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "$,", "PRELS", "PPER", "VAFIN", "$.", "ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "War Freud' an allen Enden,", "tokens": ["War", "Freud'", "an", "al\u00b7len", "En\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Au\u00df Hoffnung, euer Sohn", "tokens": ["Au\u00df", "Hoff\u00b7nung", ",", "eu\u00b7er", "Sohn"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$,", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Der w\u00fcrd' ein Schreiben senden;", "tokens": ["Der", "w\u00fcrd'", "ein", "Schrei\u00b7ben", "sen\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "O nein, er ist darvon.", "tokens": ["O", "nein", ",", "er", "ist", "dar\u00b7von", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$,", "PPER", "VAFIN", "PAV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Er ist hinweg gerafft, der noch vor kurtzer Zeit", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ",", "der", "noch", "vor", "kurt\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "PRELS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bedacht war spat und fr\u00fc mit h\u00f6chster Embsigkeit,", "tokens": ["Be\u00b7dacht", "war", "spat", "und", "fr\u00fc", "mit", "h\u00f6chs\u00b7ter", "Emb\u00b7sig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "KON", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie seine gantze Jugend", "tokens": ["Wie", "sei\u00b7ne", "gant\u00b7ze", "Ju\u00b7gend"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Recht wol w\u00fcrd' angewand,", "tokens": ["Recht", "wol", "w\u00fcrd'", "an\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Damit er K\u00fcnst' und Tugend", "tokens": ["Da\u00b7mit", "er", "K\u00fcnst'", "und", "Tu\u00b7gend"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Br\u00e4cht' in sein Vatterland.", "tokens": ["Br\u00e4cht'", "in", "sein", "Vat\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "++-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Er ist hinweg gerafft: ihr hattet schon gedacht,", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ":", "ihr", "hat\u00b7tet", "schon", "ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihr w\u00fcrdet, wann ihn Gott gesund anheim gebracht,", "tokens": ["Ihr", "w\u00fcr\u00b7det", ",", "wann", "ihn", "Gott", "ge\u00b7sund", "an\u00b7heim", "ge\u00b7bracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "PWAV", "PPER", "NN", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Augen k\u00f6nnen weyden", "tokens": ["Die", "Au\u00b7gen", "k\u00f6n\u00b7nen", "wey\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und ihm entgegen ziehn", "tokens": ["Und", "ihm", "ent\u00b7ge\u00b7gen", "ziehn"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "PTKVZ", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "In voller Lust und Freuden;", "tokens": ["In", "vol\u00b7ler", "Lust", "und", "Freu\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nein, nein, er ist dahin.", "tokens": ["Nein", ",", "nein", ",", "er", "ist", "da\u00b7hin", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PTKANT", "$,", "PPER", "VAFIN", "PAV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "O ungl\u00fcckhaffter Bott', ist di\u00df nun seine M\u00fch?", "tokens": ["O", "un\u00b7gl\u00fcck\u00b7haff\u00b7ter", "Bott'", ",", "ist", "di\u00df", "nun", "sei\u00b7ne", "M\u00fch", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "VAFIN", "PDS", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist di\u00df sein Schwei\u00df und Frost, sein Lesen spat unnd fr\u00fch?", "tokens": ["Ist", "di\u00df", "sein", "Schwei\u00df", "und", "Frost", ",", "sein", "Le\u00b7sen", "spat", "unnd", "fr\u00fch", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PPOSAT", "NN", "KON", "NN", "$,", "PPOSAT", "NN", "VVFIN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein einiges Gesetze", "tokens": ["Kein", "ei\u00b7ni\u00b7ges", "Ge\u00b7set\u00b7ze"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Steht im Justinian,", "tokens": ["Steht", "im", "Jus\u00b7ti\u00b7ni\u00b7an", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Das durch das schwartze Netze", "tokens": ["Das", "durch", "das", "schwart\u00b7ze", "Net\u00b7ze"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "De\u00df Todes reissen kan.", "tokens": ["De\u00df", "To\u00b7des", "reis\u00b7sen", "kan", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Er ist hinweg gerafft, der Mutter beste Zier,", "tokens": ["Er", "ist", "hin\u00b7weg", "ge\u00b7rafft", ",", "der", "Mut\u00b7ter", "bes\u00b7te", "Zier", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "De\u00df Vatters grosser Trost, und zwar sehr weit von hier,", "tokens": ["De\u00df", "Vat\u00b7ters", "gros\u00b7ser", "Trost", ",", "und", "zwar", "sehr", "weit", "von", "hier", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "KON", "ADV", "ADV", "ADJD", "APPR", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So da\u00df sie ihn mit Pflegen", "tokens": ["So", "da\u00df", "sie", "ihn", "mit", "Pfle\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und Warten nie gelabt,", "tokens": ["Und", "War\u00b7ten", "nie", "ge\u00b7labt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Auch in den Sarg zu legen", "tokens": ["Auch", "in", "den", "Sarg", "zu", "le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nicht haben Fug gehabt.", "tokens": ["Nicht", "ha\u00b7ben", "Fug", "ge\u00b7habt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VAFIN", "NN", "VAPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Die Schwester hett' ihm doch noch einen treuen Ku\u00df", "tokens": ["Die", "Schwes\u00b7ter", "hett'", "ihm", "doch", "noch", "ei\u00b7nen", "treu\u00b7en", "Ku\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gegeben auff den Weg und letzten Abschiedgru\u00df,", "tokens": ["Ge\u00b7ge\u00b7ben", "auff", "den", "Weg", "und", "letz\u00b7ten", "Ab\u00b7schied\u00b7gru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hett' ihm zum Liebeszeichen", "tokens": ["Hett'", "ihm", "zum", "Lie\u00b7bes\u00b7zei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die Augen zugedr\u00fcckt", "tokens": ["Die", "Au\u00b7gen", "zu\u00b7ge\u00b7dr\u00fcckt"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "VVPP"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und seine blasse Leichen", "tokens": ["Und", "sei\u00b7ne", "blas\u00b7se", "Lei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Mit Blumen au\u00dfgeschm\u00fcckt.", "tokens": ["Mit", "Blu\u00b7men", "au\u00df\u00b7ge\u00b7schm\u00fcckt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "Es lege sein Gebein' in dieser sch\u00f6nen Statt,", "tokens": ["Es", "le\u00b7ge", "sein", "Ge\u00b7bein'", "in", "die\u00b7ser", "sch\u00f6\u00b7nen", "Statt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dergleichen weit unnd breit gantz Teutschland nicht mehr hat,", "tokens": ["Derg\u00b7lei\u00b7chen", "weit", "unnd", "breit", "gantz", "Teutschland", "nicht", "mehr", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "KON", "ADJD", "ADV", "NN", "PTKNEG", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Es hette sampt den Seinen", "tokens": ["Es", "het\u00b7te", "sampt", "den", "Sei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und die ihm sonst bekand,", "tokens": ["Und", "die", "ihm", "sonst", "be\u00b7kand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Auch helffen umb ihn weinen", "tokens": ["Auch", "helf\u00b7fen", "umb", "ihn", "wei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVINF", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Das grosse Vatterland.", "tokens": ["Das", "gros\u00b7se", "Vat\u00b7ter\u00b7land", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.23": {"line.1": {"text": "Nein, der, der \u00fcber uns sitzt, aller Wei\u00dfheit voll,", "tokens": ["Nein", ",", "der", ",", "der", "\u00fc\u00b7ber", "uns", "sitzt", ",", "al\u00b7ler", "Wei\u00df\u00b7heit", "voll", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "$,", "PRELS", "APPR", "PPER", "VVFIN", "$,", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wei\u00df, wenn er uns hier seyn und nicht seyn lassen soll;", "tokens": ["Wei\u00df", ",", "wenn", "er", "uns", "hier", "seyn", "und", "nicht", "seyn", "las\u00b7sen", "soll", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "PPER", "PPER", "ADV", "VAINF", "KON", "PTKNEG", "VAINF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein Mensch vermag zu kommen", "tokens": ["Kein", "Mensch", "ver\u00b7mag", "zu", "kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ohn ihn auff diese Welt,", "tokens": ["Ohn", "ihn", "auff", "die\u00b7se", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Wird auch nicht weggenommen", "tokens": ["Wird", "auch", "nicht", "weg\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PTKNEG", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Als wann es ihm gefellt.", "tokens": ["Als", "wann", "es", "ihm", "ge\u00b7fellt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.24": {"line.1": {"text": "Was hilfft das Weinen dann? Ja, wof\u00fcr Creutz unnd Pein", "tokens": ["Was", "hilfft", "das", "Wei\u00b7nen", "dann", "?", "Ja", ",", "wo\u00b7f\u00fcr", "Creutz", "unnd", "Pein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "ART", "NN", "ADV", "$.", "PTKANT", "$,", "PWAV", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Leyd und Kl\u00e4glich-Thun uns kan beh\u00fclfflich seyn,", "tokens": ["Das", "Leyd", "und", "Kl\u00e4g\u00b7lich\u00b7Thun", "uns", "kan", "be\u00b7h\u00fclf\u00b7flich", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "PPER", "VMFIN", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So sind die tr\u00fcben Zehren", "tokens": ["So", "sind", "die", "tr\u00fc\u00b7ben", "Zeh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Viel werther noch als Gold,", "tokens": ["Viel", "wert\u00b7her", "noch", "als", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "KOUS", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Jetzt, nun sie ihm nicht wehren,", "tokens": ["Jetzt", ",", "nun", "sie", "ihm", "nicht", "weh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "PPER", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "So hei\u00dft es nur Gedult.", "tokens": ["So", "hei\u00dft", "es", "nur", "Ge\u00b7dult", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Er ist der Welt entw\u00fcscht, da nichts als Krieg unnd Streit,", "tokens": ["Er", "ist", "der", "Welt", "ent\u00b7w\u00fcscht", ",", "da", "nichts", "als", "Krieg", "unnd", "Streit", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$,", "KOUS", "PIS", "KOKOM", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Als angeschminckte Lieb', als Ha\u00df und grimmer Neyd,", "tokens": ["Als", "an\u00b7ge\u00b7schminck\u00b7te", "Lieb'", ",", "als", "Ha\u00df", "und", "grim\u00b7mer", "Neyd", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$,", "KOUS", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als Schand' und b\u00f6se L\u00fcsten", "tokens": ["Als", "Schand'", "und", "b\u00f6\u00b7se", "L\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "In vollem Schwange gehn", "tokens": ["In", "vol\u00b7lem", "Schwan\u00b7ge", "gehn"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und Laster, die uns Cristen", "tokens": ["Und", "Las\u00b7ter", ",", "die", "uns", "Cris\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NN", "$,", "PRELS", "PPER", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Nur nicht zu nennen stehn.", "tokens": ["Nur", "nicht", "zu", "nen\u00b7nen", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "PTKZU", "VVINF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.26": {"line.1": {"text": "Er schl\u00e4fft bey Vielen auch, die ewig sind bekand,", "tokens": ["Er", "schl\u00e4fft", "bey", "Vie\u00b7len", "auch", ",", "die", "e\u00b7wig", "sind", "be\u00b7kand", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "ADV", "$,", "PRELS", "ADJD", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch Zuthun der Vernunfft und die gelehrte Hand,", "tokens": ["Durch", "Zu\u00b7thun", "der", "Ver\u00b7nunfft", "und", "die", "ge\u00b7lehr\u00b7te", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "KON", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Hier wo die Ill' und Breusche", "tokens": ["Hier", "wo", "die", "Ill'", "und", "Breu\u00b7sche"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PWAV", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sich mengen in den Rein", "tokens": ["Sich", "men\u00b7gen", "in", "den", "Rein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Mit lieblichem Ger\u00e4usche", "tokens": ["Mit", "lieb\u00b7li\u00b7chem", "Ge\u00b7r\u00e4u\u00b7sche"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und reich an Fr\u00fcchten seyn.", "tokens": ["Und", "reich", "an", "Fr\u00fcch\u00b7ten", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Er ist auff eine Schul, in der Gott selber lehrt", "tokens": ["Er", "ist", "auff", "ei\u00b7ne", "Schul", ",", "in", "der", "Gott", "sel\u00b7ber", "lehrt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "$,", "APPR", "ART", "NN", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Solch unerforschlichs Ding, das noch kein Ohr geh\u00f6rt.", "tokens": ["Solch", "un\u00b7er\u00b7for\u00b7schlichs", "Ding", ",", "das", "noch", "kein", "Ohr", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "PRELS", "ADV", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was wir an jetzund kennen", "tokens": ["Was", "wir", "an", "je\u00b7tzund", "ken\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "ADV", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Hat weder Art noch Krafft", "tokens": ["Hat", "we\u00b7der", "Art", "noch", "Krafft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "KON", "NN", "ADV", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und ist ein Traum zu nennen", "tokens": ["Und", "ist", "ein", "Traum", "zu", "nen\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Der rechten Wissenschafft.", "tokens": ["Der", "rech\u00b7ten", "Wis\u00b7sen\u00b7schafft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.28": {"line.1": {"text": "Wol dir, du liebe Seel', empfinde deine Lust,", "tokens": ["Wol", "dir", ",", "du", "lie\u00b7be", "Seel'", ",", "emp\u00b7fin\u00b7de", "dei\u00b7ne", "Lust", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$,", "PPER", "ADJA", "NN", "$,", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Geneu\u00df der Fr\u00f6ligkeit, die uns noch unbewust,", "tokens": ["Ge\u00b7neu\u00df", "der", "Fr\u00f6\u00b7lig\u00b7keit", ",", "die", "uns", "noch", "un\u00b7be\u00b7wust", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In die du auffgenommen", "tokens": ["In", "die", "du", "auff\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nun bist ohn End und Ziel;", "tokens": ["Nun", "bist", "ohn", "End", "und", "Ziel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Wir wollen zu dir kommen", "tokens": ["Wir", "wol\u00b7len", "zu", "dir", "kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wo, wie und wann Gott wil.", "tokens": ["Wo", ",", "wie", "und", "wann", "Gott", "wil", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "PWAV", "KON", "PWAV", "NN", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}