{"textgrid.poem.42843": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Ein St\u00fcck Rheinfahrt", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich habe nach dem langweiligen Rhein", "tokens": ["Ich", "ha\u00b7be", "nach", "dem", "lang\u00b7wei\u00b7li\u00b7gen", "Rhein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NE"], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Und den kitschigen Burgschutthaufen", "tokens": ["Und", "den", "kit\u00b7schi\u00b7gen", "Burg\u00b7schutt\u00b7hau\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Gar nicht gesehn, zog es vor, zu saufen \u2013", "tokens": ["Gar", "nicht", "ge\u00b7sehn", ",", "zog", "es", "vor", ",", "zu", "sau\u00b7fen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "VVPP", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "PTKZU", "VVINF", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Nein: Wir tranken einen vorz\u00fcglichen Wein.", "tokens": ["Nein", ":", "Wir", "tran\u00b7ken", "ei\u00b7nen", "vor\u00b7z\u00fcg\u00b7li\u00b7chen", "Wein", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Wir benahmen uns auf jeder Station", "tokens": ["Wir", "be\u00b7nah\u00b7men", "uns", "auf", "je\u00b7der", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Am Fenster wie Gesindel,", "tokens": ["Am", "Fens\u00b7ter", "wie", "Ge\u00b7sin\u00b7del", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Schimpften in ordin\u00e4rem Ton", "tokens": ["Schimpf\u00b7ten", "in", "or\u00b7di\u00b7n\u00e4\u00b7rem", "Ton"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "\u00dcber angebliches Kindergewindel.", "tokens": ["\u00dc\u00b7ber", "an\u00b7ge\u00b7bli\u00b7ches", "Kin\u00b7der\u00b7ge\u00b7win\u00b7del", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und infolgedessen", "tokens": ["Und", "in\u00b7fol\u00b7ge\u00b7des\u00b7sen"], "token_info": ["word", "word"], "pos": ["KON", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und berechnenderweise", "tokens": ["Und", "be\u00b7rech\u00b7nen\u00b7der\u00b7wei\u00b7se"], "token_info": ["word", "word"], "pos": ["KON", "VVFIN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.7": {"text": "Haben wir w\u00e4hrend der ganzen Reise", "tokens": ["Ha\u00b7ben", "wir", "w\u00e4h\u00b7rend", "der", "gan\u00b7zen", "Rei\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.8": {"text": "Allein im Kupee gesessen.", "tokens": ["Al\u00b7lein", "im", "Ku\u00b7pee", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "VVPP", "$."], "meter": "---++-+-", "measure": "unknown.measure.tri"}}, "stanza.3": {"line.1": {"text": "Und was ergibt dann sich?", "tokens": ["Und", "was", "er\u00b7gibt", "dann", "sich", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ADV", "PRF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ach, ein Loch im Strumpf kann sich", "tokens": ["Ach", ",", "ein", "Loch", "im", "Strumpf", "kann", "sich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "ART", "NN", "APPRART", "NN", "VMFIN", "PRF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Durch alle Gr\u00f6\u00dfen", "tokens": ["Durch", "al\u00b7le", "Gr\u00f6\u00b7\u00dfen"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PIAT", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Bis in ein randloses Gl\u00fcck aufl\u00f6sen.", "tokens": ["Bis", "in", "ein", "rand\u00b7lo\u00b7ses", "Gl\u00fcck", "auf\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.4": {"line.1": {"text": "Das Gl\u00fcck schl\u00e4gt manchen Kegelpurz.", "tokens": ["Das", "Gl\u00fcck", "schl\u00e4gt", "man\u00b7chen", "Ke\u00b7gel\u00b7purz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Reise war zu kurz.", "tokens": ["Die", "Rei\u00b7se", "war", "zu", "kurz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKA", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der Rhein und die Burgen g\u00e4hnten.", "tokens": ["Der", "Rhein", "und", "die", "Bur\u00b7gen", "g\u00e4hn\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "KON", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wir w\u00e4hnten", "tokens": ["Wir", "w\u00e4hn\u00b7ten"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Beide Prinzen zu sein.", "tokens": ["Bei\u00b7de", "Prin\u00b7zen", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.5": {"line.1": {"text": "Unbestreitbar ausgezeichnet ist der Wein.", "tokens": ["Un\u00b7be\u00b7streit\u00b7bar", "aus\u00b7ge\u00b7zeich\u00b7net", "ist", "der", "Wein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "VAFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.6": {"line.1": {"text": "Ich habe nach dem langweiligen Rhein", "tokens": ["Ich", "ha\u00b7be", "nach", "dem", "lang\u00b7wei\u00b7li\u00b7gen", "Rhein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NE"], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Und den kitschigen Burgschutthaufen", "tokens": ["Und", "den", "kit\u00b7schi\u00b7gen", "Burg\u00b7schutt\u00b7hau\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Gar nicht gesehn, zog es vor, zu saufen \u2013", "tokens": ["Gar", "nicht", "ge\u00b7sehn", ",", "zog", "es", "vor", ",", "zu", "sau\u00b7fen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "VVPP", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "PTKZU", "VVINF", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Nein: Wir tranken einen vorz\u00fcglichen Wein.", "tokens": ["Nein", ":", "Wir", "tran\u00b7ken", "ei\u00b7nen", "vor\u00b7z\u00fcg\u00b7li\u00b7chen", "Wein", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Wir benahmen uns auf jeder Station", "tokens": ["Wir", "be\u00b7nah\u00b7men", "uns", "auf", "je\u00b7der", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Am Fenster wie Gesindel,", "tokens": ["Am", "Fens\u00b7ter", "wie", "Ge\u00b7sin\u00b7del", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Schimpften in ordin\u00e4rem Ton", "tokens": ["Schimpf\u00b7ten", "in", "or\u00b7di\u00b7n\u00e4\u00b7rem", "Ton"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "\u00dcber angebliches Kindergewindel.", "tokens": ["\u00dc\u00b7ber", "an\u00b7ge\u00b7bli\u00b7ches", "Kin\u00b7der\u00b7ge\u00b7win\u00b7del", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und infolgedessen", "tokens": ["Und", "in\u00b7fol\u00b7ge\u00b7des\u00b7sen"], "token_info": ["word", "word"], "pos": ["KON", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und berechnenderweise", "tokens": ["Und", "be\u00b7rech\u00b7nen\u00b7der\u00b7wei\u00b7se"], "token_info": ["word", "word"], "pos": ["KON", "VVFIN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.7": {"text": "Haben wir w\u00e4hrend der ganzen Reise", "tokens": ["Ha\u00b7ben", "wir", "w\u00e4h\u00b7rend", "der", "gan\u00b7zen", "Rei\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.8": {"text": "Allein im Kupee gesessen.", "tokens": ["Al\u00b7lein", "im", "Ku\u00b7pee", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "VVPP", "$."], "meter": "---++-+-", "measure": "unknown.measure.tri"}}, "stanza.8": {"line.1": {"text": "Und was ergibt dann sich?", "tokens": ["Und", "was", "er\u00b7gibt", "dann", "sich", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ADV", "PRF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ach, ein Loch im Strumpf kann sich", "tokens": ["Ach", ",", "ein", "Loch", "im", "Strumpf", "kann", "sich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "ART", "NN", "APPRART", "NN", "VMFIN", "PRF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Durch alle Gr\u00f6\u00dfen", "tokens": ["Durch", "al\u00b7le", "Gr\u00f6\u00b7\u00dfen"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PIAT", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Bis in ein randloses Gl\u00fcck aufl\u00f6sen.", "tokens": ["Bis", "in", "ein", "rand\u00b7lo\u00b7ses", "Gl\u00fcck", "auf\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.9": {"line.1": {"text": "Das Gl\u00fcck schl\u00e4gt manchen Kegelpurz.", "tokens": ["Das", "Gl\u00fcck", "schl\u00e4gt", "man\u00b7chen", "Ke\u00b7gel\u00b7purz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Reise war zu kurz.", "tokens": ["Die", "Rei\u00b7se", "war", "zu", "kurz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKA", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der Rhein und die Burgen g\u00e4hnten.", "tokens": ["Der", "Rhein", "und", "die", "Bur\u00b7gen", "g\u00e4hn\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "KON", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wir w\u00e4hnten", "tokens": ["Wir", "w\u00e4hn\u00b7ten"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Beide Prinzen zu sein.", "tokens": ["Bei\u00b7de", "Prin\u00b7zen", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Unbestreitbar ausgezeichnet ist der Wein.", "tokens": ["Un\u00b7be\u00b7streit\u00b7bar", "aus\u00b7ge\u00b7zeich\u00b7net", "ist", "der", "Wein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "VAFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}}}}