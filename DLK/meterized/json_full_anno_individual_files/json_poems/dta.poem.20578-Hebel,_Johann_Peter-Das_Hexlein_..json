{"dta.poem.20578": {"metadata": {"author": {"name": "Hebel, Johann Peter", "birth": "N.A.", "death": "N.A."}, "title": "Das Hexlein .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1803", "urn": "urn:nbn:de:kobv:b4-200905192133", "language": ["de:0.99"], "booktitle": "[Hebel, Johann Peter]: Allemannische Gedichte. Karlsruhe, 1803."}, "poem": {"stanza.1": {"line.1": {"text": "Und woni uffem Schnid-Stuhl sitz               ", "tokens": ["Und", "wo\u00b7ni", "uf\u00b7fem", "Schni\u00b7dStuhl", "sitz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ADJA", "NN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "f\u00fcr Basseltang, und Liechtsp\u00f6h schnitz,", "tokens": ["f\u00fcr", "Bas\u00b7sel\u00b7tang", ",", "und", "Liechts\u00b7p\u00f6h", "schnitz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "KON", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "se chunnt e Hexli wohlgimuth,", "tokens": ["se", "chunnt", "e", "Hex\u00b7li", "wohl\u00b7gi\u00b7muth", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM", "FM", "FM", "FM", "FM", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und frogt no frey: \u201eHaut\u2019s Messer gut?\u201c", "tokens": ["und", "frogt", "no", "frey", ":", "\u201e", "Haut's", "Mes\u00b7ser", "gut", "?", "\u201c"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "NE", "PTKVZ", "$.", "$(", "NE", "NE", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Und seit mer frey no Gute Tag!", "tokens": ["Und", "seit", "mer", "frey", "no", "Gu\u00b7te", "Tag", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und woni lueg, und woni sag:", "tokens": ["und", "wo\u00b7ni", "lu\u00b7eg", ",", "und", "wo\u00b7ni", "sag", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "VVFIN", "$,", "KON", "PWAV", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u201e\u2019s ch\u00f6nnt besser go, und Gro\u00dfe Dank!\u201c", "tokens": ["\u201e", "'s", "ch\u00f6nnt", "bes\u00b7ser", "go", ",", "und", "Gro\u00b7\u00dfe", "Dank", "!", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "ADJD", "ADJD", "FM", "$,", "KON", "ADJA", "NN", "$.", "$("], "meter": "+---+-+-+", "measure": "dactylic.init"}, "line.4": {"text": "se wird mer \u2019s Herz uf ei mol chrank.", "tokens": ["se", "wird", "mer", "'s", "Herz", "uf", "ei", "mol", "chrank", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PPER", "NN", "APPR", "FM", "FM", "FM", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Und uf, und furt enanderno,", "tokens": ["Und", "uf", ",", "und", "furt", "en\u00b7an\u00b7der\u00b7no", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PTKVZ", "$,", "KON", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und woni lueg, ischs n\u00fcmme do,", "tokens": ["und", "wo\u00b7ni", "lu\u00b7eg", ",", "ischs", "n\u00fcm\u00b7me", "do", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "VVFIN", "$,", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "und woni r\u00fcef: \u201eDu Hexli he!\u201c", "tokens": ["und", "wo\u00b7ni", "r\u00fcef", ":", "\u201e", "Du", "Hex\u00b7li", "he", "!", "\u201c"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWAV", "VVFIN", "$.", "$(", "PPER", "NE", "NE", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "se gits mer scho kei Antwort meh.", "tokens": ["se", "gits", "mer", "scho", "kei", "Ant\u00b7wort", "meh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.it", "FM.it", "FM.it", "FM.it", "PIAT", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und sieder schmekt mer \u2019s Esse nit;", "tokens": ["Und", "sie\u00b7der", "schmekt", "mer", "'s", "Es\u00b7se", "nit", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PPER", "NN", "PTKNEG", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "stell umme, was de hesch und witt,", "tokens": ["stell", "um\u00b7me", ",", "was", "de", "hesch", "und", "witt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$,", "PRELS", "NE", "ADJD", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "und wenn en anders schlofe cha,", "tokens": ["und", "wenn", "en", "an\u00b7ders", "schlo\u00b7fe", "cha", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NE", "ADV", "VVFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "se h\u00f6ri alli Stunde schla.", "tokens": ["se", "h\u00f6\u00b7ri", "al\u00b7li", "Stun\u00b7de", "schla", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Und was i schaff das g\u2019rothet nit,", "tokens": ["Und", "was", "i", "schaff", "das", "g'\u00b7ro\u00b7thet", "nit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "NE", "VVFIN", "PDS", "VVFIN", "PTKNEG", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "und alli Schritt und alli Tritt,", "tokens": ["und", "al\u00b7li", "Schritt", "und", "al\u00b7li", "Tritt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "se chunnt mer ebe das Hexli f\u00fcr,", "tokens": ["se", "chunnt", "mer", "e\u00b7be", "das", "Hex\u00b7li", "f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.fr", "FM.fr", "FM.fr", "FM.fr", "ART", "NN", "APPR", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "und was i schwetz, isch hinterf\u00fcr.", "tokens": ["und", "was", "i", "schwetz", ",", "isch", "hin\u00b7ter\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "NE", "VVFIN", "$,", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "\u2019s isch wohr, es het e Gsichtli gha,", "tokens": ["'s", "isch", "wohr", ",", "es", "het", "e", "Gsicht\u00b7li", "gha", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "$,", "PPER", "VAFIN", "NE", "NE", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u2019s verluegti si en Engel dra;", "tokens": ["'s", "ver\u00b7lu\u00b7eg\u00b7ti", "si", "en", "En\u00b7gel", "dra", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "FM", "FM", "FM", "NE", "NE", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "und \u2019s seit mit so \u2019me freie Muth,", "tokens": ["und", "'s", "seit", "mit", "so", "'me", "frei\u00b7e", "Muth", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPR", "APPR", "ADV", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "so lieb und s\u00fc\u00df: \u201eHaut \u2019s Messer gut?\u201c", "tokens": ["so", "lieb", "und", "s\u00fc\u00df", ":", "\u201e", "Haut", "'s", "Mes\u00b7ser", "gut", "?", "\u201c"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$.", "$(", "NN", "PPER", "NN", "ADJD", "$.", "$("], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}}, "stanza.7": {"line.1": {"text": "Und leider hani\u2019s gh\u00f6rt und gseh,", "tokens": ["Und", "lei\u00b7der", "ha\u00b7ni's", "gh\u00f6rt", "und", "gseh", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADJD", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und sellemols und n\u00fcmme meh;", "tokens": ["und", "sel\u00b7le\u00b7mols", "und", "n\u00fcm\u00b7me", "meh", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KON", "VVFIN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "d\u00f6rt ischs an Hag und Hurst verbey,", "tokens": ["d\u00f6rt", "ischs", "an", "Hag", "und", "Hurst", "ver\u00b7bey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und witers \u00fcber Stock und Stei.", "tokens": ["und", "wi\u00b7ters", "\u00fc\u00b7ber", "Stock", "und", "Stei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wer sp\u00f6chtet mer mi Hexli us,", "tokens": ["Wer", "sp\u00f6ch\u00b7tet", "mer", "mi", "Hex\u00b7li", "us", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "FM", "FM", "FM", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wer zeigtmer siner Mutter Hus?", "tokens": ["wer", "zeigt\u00b7mer", "si\u00b7ner", "Mut\u00b7ter", "Hus", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJA", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "J lauf no, was i laufe cha,", "tokens": ["J", "lauf", "no", ",", "was", "i", "lau\u00b7fe", "cha", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$,", "PWS", "FM", "FM", "FM", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wer wei\u00df, se triffi\u2019s doch no a!", "tokens": ["wer", "wei\u00df", ",", "se", "trif\u00b7fi's", "doch", "no", "a", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "FM", "FM", "FM", "FM", "FM", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "J lauf no alli D\u00f6rfer us,", "tokens": ["J", "lauf", "no", "al\u00b7li", "D\u00f6r\u00b7fer", "us", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "i such und frog vo Hus zu Hus,", "tokens": ["i", "such", "und", "frog", "vo", "Hus", "zu", "Hus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "und w\u00fcrd mer nit mi Hexli chund,", "tokens": ["und", "w\u00fcrd", "mer", "nit", "mi", "Hex\u00b7li", "chund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PTKNEG", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "se w\u00fcrdi ebe n\u00fcmme gsund.", "tokens": ["se", "w\u00fcr\u00b7di", "e\u00b7be", "n\u00fcm\u00b7me", "gsund", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}