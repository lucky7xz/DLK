{"textgrid.poem.49681": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "1L: Neulich waren wir in Deutschland Zeugen", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Neulich waren wir in Deutschland Zeugen", "tokens": ["Neu\u00b7lich", "wa\u00b7ren", "wir", "in", "Deutschland", "Zeu\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NE", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Von dem ausgepr\u00e4gten Ehrgef\u00fchl.", "tokens": ["Von", "dem", "aus\u00b7ge\u00b7pr\u00e4g\u00b7ten", "Ehr\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Vielen Menschen ist es gar nicht eigen,", "tokens": ["Vie\u00b7len", "Men\u00b7schen", "ist", "es", "gar", "nicht", "ei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und ein F\u00e4hnrich hat es gleich zuviel.", "tokens": ["Und", "ein", "F\u00e4hn\u00b7rich", "hat", "es", "gleich", "zu\u00b7viel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "ADV", "PIS", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "H\u00fcssener mit Namen hat in Essen", "tokens": ["H\u00fcs\u00b7se\u00b7ner", "mit", "Na\u00b7men", "hat", "in", "Es\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "NN", "VAFIN", "APPR", "NN"], "meter": "+---+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Einen Menschen durch und durch gespie\u00dft,", "tokens": ["Ei\u00b7nen", "Men\u00b7schen", "durch", "und", "durch", "ge\u00b7spie\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "KON", "APPR", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Dieser hatte n\u00e4mlich ganz vergessen", "tokens": ["Die\u00b7ser", "hat\u00b7te", "n\u00e4m\u00b7lich", "ganz", "ver\u00b7ges\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "VVPP"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und hat bei der Nacht ihn nicht gegr\u00fc\u00dft.", "tokens": ["Und", "hat", "bei", "der", "Nacht", "ihn", "nicht", "ge\u00b7gr\u00fc\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "ART", "NN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Einen F\u00e4hnrich mu\u00df es stark erbosen,", "tokens": ["Ei\u00b7nen", "F\u00e4hn\u00b7rich", "mu\u00df", "es", "stark", "er\u00b7bo\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weil sie beinah Offiziere sind,", "tokens": ["Weil", "sie", "bei\u00b7nah", "Of\u00b7fi\u00b7zie\u00b7re", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Darum hat er w\u00fctend zugesto\u00dfen", "tokens": ["Da\u00b7rum", "hat", "er", "w\u00fc\u00b7tend", "zu\u00b7ge\u00b7sto\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ADJD", "VVINF"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Und die Zigarette angez\u00fcndt.", "tokens": ["Und", "die", "Zi\u00b7ga\u00b7ret\u00b7te", "an\u00b7ge\u00b7z\u00fcndt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Auch in Friedrichsort, in einem Hafen,", "tokens": ["Auch", "in", "Fried\u00b7rich\u00b7sort", ",", "in", "ei\u00b7nem", "Ha\u00b7fen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "War die \u00e4hnliche Begebenheit.", "tokens": ["War", "die", "\u00e4hn\u00b7li\u00b7che", "Be\u00b7ge\u00b7ben\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Einem F\u00e4hnrich machten hier zu schaffen", "tokens": ["Ei\u00b7nem", "F\u00e4hn\u00b7rich", "mach\u00b7ten", "hier", "zu", "schaf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PTKZU", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Zwei Matrosen durch Betrunkenheit.", "tokens": ["Zwei", "Mat\u00b7ro\u00b7sen", "durch", "Be\u00b7trun\u00b7ken\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "Und sie schlugen ihn mit Namen Abel", "tokens": ["Und", "sie", "schlu\u00b7gen", "ihn", "mit", "Na\u00b7men", "A\u00b7bel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "NN", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Voller Roheit und in das Genick.", "tokens": ["Vol\u00b7ler", "Ro\u00b7heit", "und", "in", "das", "Ge\u00b7nick", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Als er ziehen wollte seinen Sabel,", "tokens": ["Als", "er", "zie\u00b7hen", "woll\u00b7te", "sei\u00b7nen", "Sa\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "VMFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Waren sie schon ziemlich weit zur\u00fcck.", "tokens": ["Wa\u00b7ren", "sie", "schon", "ziem\u00b7lich", "weit", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "Die Matrosen haben sich geborgen", "tokens": ["Die", "Mat\u00b7ro\u00b7sen", "ha\u00b7ben", "sich", "ge\u00b7bor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PRF", "VVPP"], "meter": "-++-+-+-+-", "measure": "unknown.measure.penta"}, "line.2": {"text": "Vor dem Vorgesetzten durch die Flucht.", "tokens": ["Vor", "dem", "Vor\u00b7ge\u00b7setz\u00b7ten", "durch", "die", "Flucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Dieser hat aus Zorn am andern Morgen", "tokens": ["Die\u00b7ser", "hat", "aus", "Zorn", "am", "an\u00b7dern", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Einen Selbstmord mit Erfolg versucht.", "tokens": ["Ei\u00b7nen", "Selbst\u00b7mord", "mit", "Er\u00b7folg", "ver\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "\u00dcber diese beiden Schreckenstaten", "tokens": ["\u00dc\u00b7ber", "die\u00b7se", "bei\u00b7den", "Schre\u00b7cken\u00b7sta\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "PIAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Hat vermutlich jeder nachgedacht,", "tokens": ["Hat", "ver\u00b7mut\u00b7lich", "je\u00b7der", "nach\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und fast alle Zeitungsbl\u00e4tter hatten", "tokens": ["Und", "fast", "al\u00b7le", "Zei\u00b7tungs\u00b7bl\u00e4t\u00b7ter", "hat\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PIAT", "NN", "VAFIN"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Ihrerseits Verschiednes beigebracht.", "tokens": ["Ih\u00b7rer\u00b7seits", "Ver\u00b7schied\u00b7nes", "bei\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.8": {"line.1": {"text": "Und so wissen wir auf diese Weise,", "tokens": ["Und", "so", "wis\u00b7sen", "wir", "auf", "die\u00b7se", "Wei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Was nicht jedermann vorher gewi\u00dft,", "tokens": ["Was", "nicht", "je\u00b7der\u00b7mann", "vor\u00b7her", "ge\u00b7wi\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "PIS", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da\u00df zur Zeit es nichts Geringes hei\u00dfe,", "tokens": ["Da\u00df", "zur", "Zeit", "es", "nichts", "Ge\u00b7rin\u00b7ges", "hei\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPRART", "NN", "PPER", "PIS", "ADJA", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Wenn der junge Mensch ein F\u00e4hnrich ist.", "tokens": ["Wenn", "der", "jun\u00b7ge", "Mensch", "ein", "F\u00e4hn\u00b7rich", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "Neulich waren wir in Deutschland Zeugen", "tokens": ["Neu\u00b7lich", "wa\u00b7ren", "wir", "in", "Deutschland", "Zeu\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NE", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Von dem ausgepr\u00e4gten Ehrgef\u00fchl.", "tokens": ["Von", "dem", "aus\u00b7ge\u00b7pr\u00e4g\u00b7ten", "Ehr\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Vielen Menschen ist es gar nicht eigen,", "tokens": ["Vie\u00b7len", "Men\u00b7schen", "ist", "es", "gar", "nicht", "ei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und ein F\u00e4hnrich hat es gleich zuviel.", "tokens": ["Und", "ein", "F\u00e4hn\u00b7rich", "hat", "es", "gleich", "zu\u00b7viel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "ADV", "PIS", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.10": {"line.1": {"text": "H\u00fcssener mit Namen hat in Essen", "tokens": ["H\u00fcs\u00b7se\u00b7ner", "mit", "Na\u00b7men", "hat", "in", "Es\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "NN", "VAFIN", "APPR", "NN"], "meter": "+---+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Einen Menschen durch und durch gespie\u00dft,", "tokens": ["Ei\u00b7nen", "Men\u00b7schen", "durch", "und", "durch", "ge\u00b7spie\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "KON", "APPR", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Dieser hatte n\u00e4mlich ganz vergessen", "tokens": ["Die\u00b7ser", "hat\u00b7te", "n\u00e4m\u00b7lich", "ganz", "ver\u00b7ges\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "VVPP"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und hat bei der Nacht ihn nicht gegr\u00fc\u00dft.", "tokens": ["Und", "hat", "bei", "der", "Nacht", "ihn", "nicht", "ge\u00b7gr\u00fc\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "ART", "NN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Einen F\u00e4hnrich mu\u00df es stark erbosen,", "tokens": ["Ei\u00b7nen", "F\u00e4hn\u00b7rich", "mu\u00df", "es", "stark", "er\u00b7bo\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weil sie beinah Offiziere sind,", "tokens": ["Weil", "sie", "bei\u00b7nah", "Of\u00b7fi\u00b7zie\u00b7re", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Darum hat er w\u00fctend zugesto\u00dfen", "tokens": ["Da\u00b7rum", "hat", "er", "w\u00fc\u00b7tend", "zu\u00b7ge\u00b7sto\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ADJD", "VVINF"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Und die Zigarette angez\u00fcndt.", "tokens": ["Und", "die", "Zi\u00b7ga\u00b7ret\u00b7te", "an\u00b7ge\u00b7z\u00fcndt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.12": {"line.1": {"text": "Auch in Friedrichsort, in einem Hafen,", "tokens": ["Auch", "in", "Fried\u00b7rich\u00b7sort", ",", "in", "ei\u00b7nem", "Ha\u00b7fen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "War die \u00e4hnliche Begebenheit.", "tokens": ["War", "die", "\u00e4hn\u00b7li\u00b7che", "Be\u00b7ge\u00b7ben\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Einem F\u00e4hnrich machten hier zu schaffen", "tokens": ["Ei\u00b7nem", "F\u00e4hn\u00b7rich", "mach\u00b7ten", "hier", "zu", "schaf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PTKZU", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Zwei Matrosen durch Betrunkenheit.", "tokens": ["Zwei", "Mat\u00b7ro\u00b7sen", "durch", "Be\u00b7trun\u00b7ken\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "Und sie schlugen ihn mit Namen Abel", "tokens": ["Und", "sie", "schlu\u00b7gen", "ihn", "mit", "Na\u00b7men", "A\u00b7bel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "NN", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Voller Roheit und in das Genick.", "tokens": ["Vol\u00b7ler", "Ro\u00b7heit", "und", "in", "das", "Ge\u00b7nick", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Als er ziehen wollte seinen Sabel,", "tokens": ["Als", "er", "zie\u00b7hen", "woll\u00b7te", "sei\u00b7nen", "Sa\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "VMFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Waren sie schon ziemlich weit zur\u00fcck.", "tokens": ["Wa\u00b7ren", "sie", "schon", "ziem\u00b7lich", "weit", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.14": {"line.1": {"text": "Die Matrosen haben sich geborgen", "tokens": ["Die", "Mat\u00b7ro\u00b7sen", "ha\u00b7ben", "sich", "ge\u00b7bor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PRF", "VVPP"], "meter": "-++-+-+-+-", "measure": "unknown.measure.penta"}, "line.2": {"text": "Vor dem Vorgesetzten durch die Flucht.", "tokens": ["Vor", "dem", "Vor\u00b7ge\u00b7setz\u00b7ten", "durch", "die", "Flucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Dieser hat aus Zorn am andern Morgen", "tokens": ["Die\u00b7ser", "hat", "aus", "Zorn", "am", "an\u00b7dern", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Einen Selbstmord mit Erfolg versucht.", "tokens": ["Ei\u00b7nen", "Selbst\u00b7mord", "mit", "Er\u00b7folg", "ver\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.15": {"line.1": {"text": "\u00dcber diese beiden Schreckenstaten", "tokens": ["\u00dc\u00b7ber", "die\u00b7se", "bei\u00b7den", "Schre\u00b7cken\u00b7sta\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "PIAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Hat vermutlich jeder nachgedacht,", "tokens": ["Hat", "ver\u00b7mut\u00b7lich", "je\u00b7der", "nach\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und fast alle Zeitungsbl\u00e4tter hatten", "tokens": ["Und", "fast", "al\u00b7le", "Zei\u00b7tungs\u00b7bl\u00e4t\u00b7ter", "hat\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PIAT", "NN", "VAFIN"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Ihrerseits Verschiednes beigebracht.", "tokens": ["Ih\u00b7rer\u00b7seits", "Ver\u00b7schied\u00b7nes", "bei\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.16": {"line.1": {"text": "Und so wissen wir auf diese Weise,", "tokens": ["Und", "so", "wis\u00b7sen", "wir", "auf", "die\u00b7se", "Wei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Was nicht jedermann vorher gewi\u00dft,", "tokens": ["Was", "nicht", "je\u00b7der\u00b7mann", "vor\u00b7her", "ge\u00b7wi\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "PIS", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da\u00df zur Zeit es nichts Geringes hei\u00dfe,", "tokens": ["Da\u00df", "zur", "Zeit", "es", "nichts", "Ge\u00b7rin\u00b7ges", "hei\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPRART", "NN", "PPER", "PIS", "ADJA", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Wenn der junge Mensch ein F\u00e4hnrich ist.", "tokens": ["Wenn", "der", "jun\u00b7ge", "Mensch", "ein", "F\u00e4hn\u00b7rich", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}}}}