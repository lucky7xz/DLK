{"textgrid.poem.57478": {"metadata": {"author": {"name": "Gottsched, Johann Christoph", "birth": "N.A.", "death": "N.A."}, "title": "1L: O Geist der Weisheit! dessen ZugDen Sinn der Sterblichen von wilder Thiere Toben,", "genre": "verse", "period": "N.A.", "pub_year": 1733, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "O Geist der Weisheit! dessen ZugDen Sinn der Sterblichen von wilder Thiere Toben,", "tokens": ["O", "Geist", "der", "Weis\u00b7heit", "!", "des\u00b7sen", "Zug", "Den", "Sinn", "der", "Sterb\u00b7li\u00b7chen", "von", "wil\u00b7der", "Thie\u00b7re", "To\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ART", "NN", "$.", "PRELAT", "NN", "ART", "NN", "ART", "NN", "APPR", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.2": {"text": "Zur Einsicht und Vernunft erhoben,", "tokens": ["Zur", "Ein\u00b7sicht", "und", "Ver\u00b7nunft", "er\u00b7ho\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Wahn und Einfalt niederschlug.", "tokens": ["Die", "Wahn", "und", "Ein\u00b7falt", "nie\u00b7der\u00b7schlug", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Du Geist der Wissenschaft und Kunst!", "tokens": ["Du", "Geist", "der", "Wis\u00b7sen\u00b7schaft", "und", "Kunst", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der durch ein h\u00f6her Licht die Barberey gest\u00f6ret,", "tokens": ["Der", "durch", "ein", "h\u00f6\u00b7her", "Licht", "die", "Bar\u00b7be\u00b7rey", "ge\u00b7st\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und Menschen Menschen seyn gelehret;", "tokens": ["Und", "Men\u00b7schen", "Men\u00b7schen", "seyn", "ge\u00b7leh\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Belebe mich vorjetzt mit deines Triebes Gunst,", "tokens": ["Be\u00b7le\u00b7be", "mich", "vor\u00b7jetzt", "mit", "dei\u00b7nes", "Trie\u00b7bes", "Gunst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und la\u00df es die\u00dfmal mir gelingen", "tokens": ["Und", "la\u00df", "es", "die\u00df\u00b7mal", "mir", "ge\u00b7lin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "PPER", "ADV", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Von deinem Heiligthum und liebsten Sohn zu singen.", "tokens": ["Von", "dei\u00b7nem", "Hei\u00b7lig\u00b7thum", "und", "liebs\u00b7ten", "Sohn", "zu", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Es h\u00f6rt mich ein ", "tokens": ["Es", "h\u00f6rt", "mich", "ein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Des ", "tokens": ["Des"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Das l\u00e4ngst den K\u00fcnsten gn\u00e4dig war.", "tokens": ["Das", "l\u00e4ngst", "den", "K\u00fcns\u00b7ten", "gn\u00e4\u00b7dig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ihr heitres Antlitz st\u00e4rkt die Kraft", "tokens": ["Ihr", "heit\u00b7res", "Ant\u00b7litz", "st\u00e4rkt", "die", "Kraft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Der Musen, die sonst leicht bey Furcht und Gram erliegen:", "tokens": ["Der", "Mu\u00b7sen", ",", "die", "sonst", "leicht", "bey", "Furcht", "und", "Gram", "er\u00b7lie\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ADJD", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "In dem was andre schreckt, in Kunst und Wissenschaft.", "tokens": ["In", "dem", "was", "and\u00b7re", "schreckt", ",", "in", "Kunst", "und", "Wis\u00b7sen\u00b7schaft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PWS", "PIS", "VVFIN", "$,", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "O m\u00f6cht ein Stral von Ihren Blicken,", "tokens": ["O", "m\u00f6cht", "ein", "Stral", "von", "Ih\u00b7ren", "Bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Nach oft gesp\u00fcrter Huld, mich selber mir entr\u00fccken!", "tokens": ["Nach", "oft", "ge\u00b7sp\u00fcr\u00b7ter", "Huld", ",", "mich", "sel\u00b7ber", "mir", "ent\u00b7r\u00fc\u00b7cken", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJA", "NN", "$,", "PPER", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Da, wo der Plei\u00dfe feuchter Rand,", "tokens": ["Da", ",", "wo", "der", "Plei\u00b7\u00dfe", "feuch\u00b7ter", "Rand", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die fette Mei\u00dfnerflur mit sanfter Fluth erfrischet,", "tokens": ["Die", "fet\u00b7te", "Mei\u00df\u00b7ner\u00b7flur", "mit", "sanf\u00b7ter", "Fluth", "er\u00b7fri\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da wo sie sich mit Wellen mischet,", "tokens": ["Da", "wo", "sie", "sich", "mit", "Wel\u00b7len", "mi\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWAV", "PPER", "PRF", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die ihr die Baare zugesandt;", "tokens": ["Die", "ihr", "die", "Baa\u00b7re", "zu\u00b7ge\u00b7sandt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wo sonst ein slavisches Geschlecht,", "tokens": ["Wo", "sonst", "ein", "sla\u00b7vi\u00b7sches", "Ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Ja bis in Th\u00fcringen gedrungen,", "tokens": ["Ja", "bis", "in", "Th\u00fc\u00b7rin\u00b7gen", "ge\u00b7drun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "KON", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "Bis ihn der gro\u00dfe Karl durch Tapferkeit geschw\u00e4cht:", "tokens": ["Bis", "ihn", "der", "gro\u00b7\u00dfe", "Karl", "durch", "Tap\u00b7fer\u00b7keit", "ge\u00b7schw\u00e4cht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NE", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "In wilden ", "tokens": ["In", "wil\u00b7den"], "token_info": ["word", "word"], "pos": ["APPR", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Ist ", "tokens": ["Ist"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "+", "measure": "single.up"}}, "stanza.4": {"line.1": {"text": "Wer will im dunkeln Alterthum", "tokens": ["Wer", "will", "im", "dun\u00b7keln", "Al\u00b7ter\u00b7thum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der gr\u00f6\u00dften St\u00e4dte Grund und Stiftung recht erfahren?", "tokens": ["Der", "gr\u00f6\u00df\u00b7ten", "St\u00e4d\u00b7te", "Grund", "und", "Stif\u00b7tung", "recht", "er\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "KON", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wuchs doch ", "tokens": ["Wuchs", "doch"], "token_info": ["word", "word"], "pos": ["NN", "ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Zu dem erlangten Flor und Ruhm.", "tokens": ["Zu", "dem", "er\u00b7lang\u00b7ten", "Flor", "und", "Ruhm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der Berge Moo\u00df und tiefer Schacht,", "tokens": ["Der", "Ber\u00b7ge", "Moo\u00df", "und", "tie\u00b7fer", "Schacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Versteckt den ersten Keim, die Wurzeln junger Eichen;", "tokens": ["Ver\u00b7steckt", "den", "ers\u00b7ten", "Keim", ",", "die", "Wur\u00b7zeln", "jun\u00b7ger", "Ei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$,", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch wenn sie an die Wolken reichen,", "tokens": ["Doch", "wenn", "sie", "an", "die", "Wol\u00b7ken", "rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Erstaunt ein Wandersmann vor ihrer Zweige Pracht.", "tokens": ["Er\u00b7staunt", "ein", "Wan\u00b7ders\u00b7mann", "vor", "ih\u00b7rer", "Zwei\u00b7ge", "Pracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kein Wunder, wenn wir gleichfalls lesen,", "tokens": ["Kein", "Wun\u00b7der", ",", "wenn", "wir", "gleich\u00b7falls", "le\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}}, "stanza.5": {"line.1": {"text": "Kein Schimpf f\u00fcr dich, ber\u00fchmte Stadt!", "tokens": ["Kein", "Schimpf", "f\u00fcr", "dich", ",", "be\u00b7r\u00fchm\u00b7te", "Stadt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "PPER", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Vorsicht hatte dich schon damals ausersehen", "tokens": ["Die", "Vor\u00b7sicht", "hat\u00b7te", "dich", "schon", "da\u00b7mals", "au\u00b7ser\u00b7se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Zu allem was hernach geschehen,", "tokens": ["Zu", "al\u00b7lem", "was", "her\u00b7nach", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "PWS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dich empor gehoben hat.", "tokens": ["Und", "dich", "em\u00b7por", "ge\u00b7ho\u00b7ben", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PTKVZ", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So weit der Saal und Muldenflu\u00df,", "tokens": ["So", "weit", "der", "Saal", "und", "Mul\u00b7den\u00b7flu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So weit die Elster sich in krummen Ufern schleichet,", "tokens": ["So", "weit", "die", "Els\u00b7ter", "sich", "in", "krum\u00b7men", "U\u00b7fern", "schlei\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "PRF", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Bl\u00fcht keine Stadt die dir nicht weichet,", "tokens": ["Bl\u00fcht", "kei\u00b7ne", "Stadt", "die", "dir", "nicht", "wei\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ART", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Dir nicht in Demuth selbst den Vorzug geben mu\u00df.", "tokens": ["Dir", "nicht", "in", "De\u00b7muth", "selbst", "den", "Vor\u00b7zug", "ge\u00b7ben", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "APPR", "NN", "ADV", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So hoch hast du durch tausend Proben,", "tokens": ["So", "hoch", "hast", "du", "durch", "tau\u00b7send", "Pro\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Von Witz und Wissenschaft und Handel dich erhoben!", "tokens": ["Von", "Witz", "und", "Wis\u00b7sen\u00b7schaft", "und", "Han\u00b7del", "dich", "er\u00b7ho\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Wodurch ", "tokens": ["Wo\u00b7durch"], "token_info": ["word"], "pos": ["PWAV"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "An Reichthum ", "tokens": ["An", "Reicht\u00b7hum"], "token_info": ["word", "word"], "pos": ["APPR", "NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Dadurch kannst du, o ", "tokens": ["Da\u00b7durch", "kannst", "du", ",", "o"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PAV", "VMFIN", "PPER", "$,", "FM"], "meter": "--+-+", "measure": "anapaest.init"}, "line.4": {"text": "Das alles gr\u00fcndet auch dein Lob!", "tokens": ["Das", "al\u00b7les", "gr\u00fcn\u00b7det", "auch", "dein", "Lob", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat sich im ", "tokens": ["Hat", "sich", "im"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PRF", "APPRART"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Den Preis der sch\u00f6nsten Stadt erstritten;", "tokens": ["Den", "Preis", "der", "sch\u00f6ns\u00b7ten", "Stadt", "er\u00b7strit\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Stammt ", "tokens": ["Stammt"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Was Wunder? da\u00df auch deine Mauren", "tokens": ["Was", "Wun\u00b7der", "?", "da\u00df", "auch", "dei\u00b7ne", "Mau\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "NN", "$.", "KOUS", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Durch kluger B\u00fcrger Flei\u00df, erwachsen, stehn und dauren.", "tokens": ["Durch", "klu\u00b7ger", "B\u00fcr\u00b7ger", "Flei\u00df", ",", "er\u00b7wach\u00b7sen", ",", "stehn", "und", "dau\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "$,", "VVPP", "$,", "VVFIN", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Zinst dir kein weiter ", "tokens": ["Zinst", "dir", "kein", "wei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PPER", "PIAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Kein tief und breiter Strom durch Segel, Flagg und Masten,", "tokens": ["Kein", "tief", "und", "brei\u00b7ter", "Strom", "durch", "Se\u00b7gel", ",", "Flagg", "und", "Mas\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "KON", "ADJA", "NN", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.4": {"text": "Und Kostbarkeiten aus ", "tokens": ["Und", "Kost\u00b7bar\u00b7kei\u00b7ten", "aus"], "token_info": ["word", "word", "word"], "pos": ["KON", "NN", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Siehst du hier keine Wimpel wehn,", "tokens": ["Siehst", "du", "hier", "kei\u00b7ne", "Wim\u00b7pel", "wehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und sinkt kein Anker gleich in deinem Hafen nieder;", "tokens": ["Und", "sinkt", "kein", "An\u00b7ker", "gleich", "in", "dei\u00b7nem", "Ha\u00b7fen", "nie\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "ADV", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ja l\u00e4\u00dft dein Flu\u00df gleich hin und wieder,", "tokens": ["Ja", "l\u00e4\u00dft", "dein", "Flu\u00df", "gleich", "hin", "und", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VVFIN", "PPOSAT", "NN", "ADV", "PTKVZ", "KON", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kaum einen schmalen Kahn bey zwanzig M\u00fchlen sehn:", "tokens": ["Kaum", "ei\u00b7nen", "schma\u00b7len", "Kahn", "bey", "zwan\u00b7zig", "M\u00fch\u00b7len", "sehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So ward dir doch ", "tokens": ["So", "ward", "dir", "doch"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Denn Kunst und Witz ersetzt, was die Natur entzogen.", "tokens": ["Denn", "Kunst", "und", "Witz", "er\u00b7setzt", ",", "was", "die", "Na\u00b7tur", "ent\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "VVPP", "$,", "PRELS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ihr Pl\u00e4tze! die der Stifter Witz,", "tokens": ["Ihr", "Pl\u00e4t\u00b7ze", "!", "die", "der", "Stif\u00b7ter", "Witz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "ART", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Vieleicht der Zufall blo\u00df, an Strom und See gebauet;", "tokens": ["Vie\u00b7leicht", "der", "Zu\u00b7fall", "blo\u00df", ",", "an", "Strom", "und", "See", "ge\u00b7bau\u00b7et", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "$,", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wo ihr in stolzer N\u00e4he schauet", "tokens": ["Wo", "ihr", "in", "stol\u00b7zer", "N\u00e4\u00b7he", "schau\u00b7et"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Seyd nicht zu frech auf euer Gl\u00fcck!", "tokens": ["Seyd", "nicht", "zu", "frech", "auf", "eu\u00b7er", "Gl\u00fcck", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PTKNEG", "PTKA", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Das Meer scheint freylich euch den Reichthum aufzuth\u00fcrmen:", "tokens": ["Das", "Meer", "scheint", "frey\u00b7lich", "euch", "den", "Reicht\u00b7hum", "auf\u00b7zut\u00b7h\u00fcr\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Doch \u00f6fters schreckt es auch mit St\u00fcrmen,", "tokens": ["Doch", "\u00f6f\u00b7ters", "schreckt", "es", "auch", "mit", "St\u00fcr\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Und schickt die Flotten krank, zerlechzt und leer zur\u00fcck.", "tokens": ["Und", "schickt", "die", "Flot\u00b7ten", "krank", ",", "zer\u00b7lechzt", "und", "leer", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADJD", "$,", "VVFIN", "KON", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Wo nicht der Schatz von vielen Jahren,", "tokens": ["Wo", "nicht", "der", "Schatz", "von", "vie\u00b7len", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Durch ein zerscheitert Schiff dem Abgrund zugefahren.", "tokens": ["Durch", "ein", "zer\u00b7schei\u00b7tert", "Schiff", "dem", "Ab\u00b7grund", "zu\u00b7ge\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VVPP", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Das alles schrecket ", "tokens": ["Das", "al\u00b7les", "schre\u00b7cket"], "token_info": ["word", "word", "word"], "pos": ["PDS", "PIS", "VVFIN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Das seine Frachten nicht den Wellen anvertrauet;", "tokens": ["Das", "sei\u00b7ne", "Frach\u00b7ten", "nicht", "den", "Wel\u00b7len", "an\u00b7ver\u00b7trau\u00b7et", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dem nie vor Sturm und Wetter grauet,", "tokens": ["Dem", "nie", "vor", "Sturm", "und", "Wet\u00b7ter", "grau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Davon oft Mast und Ruder bricht.", "tokens": ["Da\u00b7von", "oft", "Mast", "und", "Ru\u00b7der", "bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hier bebt kein Mensch vor Syrt und Strand,", "tokens": ["Hier", "bebt", "kein", "Mensch", "vor", "Syrt", "und", "Strand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kein ", "tokens": ["Kein"], "token_info": ["word"], "pos": ["PIAT"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Er liegt in unbesorgtem Schlummer,", "tokens": ["Er", "liegt", "in", "un\u00b7be\u00b7sorg\u00b7tem", "Schlum\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die G\u00fcter, die er hofft, bringt ihm das sichre Land.", "tokens": ["Die", "G\u00fc\u00b7ter", ",", "die", "er", "hofft", ",", "bringt", "ihm", "das", "sich\u00b7re", "Land", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Bey zehnfach leidlichern Gefahren,", "tokens": ["Bey", "zehn\u00b7fach", "leid\u00b7li\u00b7chern", "Ge\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Versorgt der Rosse Kraft ihn mit den sch\u00f6nsten Waaren.", "tokens": ["Ver\u00b7sorgt", "der", "Ros\u00b7se", "Kraft", "ihn", "mit", "den", "sch\u00f6ns\u00b7ten", "Waa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "PPER", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Wie sich bey voller Fr\u00fchlingszeit", "tokens": ["Wie", "sich", "bey", "vol\u00b7ler", "Fr\u00fch\u00b7lings\u00b7zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein arbeitsamer Stock voll junger Bienen reget;", "tokens": ["Ein", "ar\u00b7beit\u00b7sa\u00b7mer", "Stock", "voll", "jun\u00b7ger", "Bie\u00b7nen", "re\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie alles sich vor Flei\u00df beweget,", "tokens": ["Wie", "al\u00b7les", "sich", "vor", "Flei\u00df", "be\u00b7we\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn Sonn und Luft die Kraft verleiht:", "tokens": ["Wenn", "Sonn", "und", "Luft", "die", "Kraft", "ver\u00b7leiht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die\u00df muntre Volk durchfliegt das Feld,", "tokens": ["Die\u00df", "mun\u00b7tre", "Volk", "durch\u00b7fliegt", "das", "Feld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und k\u00f6mmt durchaus beschwert mit s\u00fc\u00dfer Beute wieder;", "tokens": ["Und", "k\u00f6mmt", "durc\u00b7haus", "be\u00b7schwert", "mit", "s\u00fc\u00b7\u00dfer", "Beu\u00b7te", "wie\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "VVFIN", "APPR", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Es legt der Blumen Balsam nieder,", "tokens": ["Es", "legt", "der", "Blu\u00b7men", "Bal\u00b7sam", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und f\u00fcllt die Zellen an, die es dazu bestellt:", "tokens": ["Und", "f\u00fcllt", "die", "Zel\u00b7len", "an", ",", "die", "es", "da\u00b7zu", "be\u00b7stellt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$,", "PRELS", "PPER", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So pflegen ", "tokens": ["So", "pfle\u00b7gen"], "token_info": ["word", "word"], "pos": ["ADV", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Dreymal im Jahre sich besch\u00e4fftigt sehn zu lassen.", "tokens": ["Drey\u00b7mal", "im", "Jah\u00b7re", "sich", "be\u00b7sch\u00e4ff\u00b7tigt", "sehn", "zu", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PRF", "ADJD", "VVINF", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.11": {"text": "Wo bin ich? zeigt sich ", "tokens": ["Wo", "bin", "ich", "?", "zeigt", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "$.", "VVFIN", "PRF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.12": {"text": "Seh ich ", "tokens": ["Seh", "ich"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "Ja! ", "tokens": ["Ja", "!"], "token_info": ["word", "punct"], "pos": ["PTKANT", "$."], "meter": "+", "measure": "single.up"}, "line.14": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Die aus dem ", "tokens": ["Die", "aus", "dem"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.16": {"text": "Erscheinen auf der Kr\u00e4mer Winken;", "tokens": ["Er\u00b7schei\u00b7nen", "auf", "der", "Kr\u00e4\u00b7mer", "Win\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Ihr weiter Wagen wird von tausend Lasten schwer.", "tokens": ["Ihr", "wei\u00b7ter", "Wa\u00b7gen", "wird", "von", "tau\u00b7send", "Las\u00b7ten", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "APPR", "CARD", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.19": {"text": "Ja Donau, Rhein und Mayn, sind Leipzig zinsbar worden.", "tokens": ["Ja", "Do\u00b7nau", ",", "Rhein", "und", "Mayn", ",", "sind", "Leip\u00b7zig", "zins\u00b7bar", "wor\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "NE", "$,", "NE", "KON", "NN", "$,", "VAFIN", "NE", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Noch mehr! auch Weisheit steht hier feil,", "tokens": ["Noch", "mehr", "!", "auch", "Weis\u00b7heit", "steht", "hier", "feil", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$.", "ADV", "NN", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Was ihrer Priester wacher Flei\u00df,", "tokens": ["Was", "ih\u00b7rer", "Pries\u00b7ter", "wa\u00b7cher", "Flei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So weit Europa geht, ersonnen und geschrieben,", "tokens": ["So", "weit", "Eu\u00b7ro\u00b7pa", "geht", ",", "er\u00b7son\u00b7nen", "und", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NE", "VVFIN", "$,", "ADJA", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das alles wird hieher getrieben,", "tokens": ["Das", "al\u00b7les", "wird", "hie\u00b7her", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wo kluger K\u00e4ufer Blick es auszusp\u00e4hen weis.", "tokens": ["Wo", "klu\u00b7ger", "K\u00e4u\u00b7fer", "Blick", "es", "aus\u00b7zu\u00b7sp\u00e4\u00b7hen", "weis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "NN", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der W\u00e4lschen Geist, der Franzen K\u00fcnste,", "tokens": ["Der", "W\u00e4l\u00b7schen", "Geist", ",", "der", "Fran\u00b7zen", "K\u00fcns\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Der Britten tiefer Sinn, dient Leipzig zum Gewinnste.", "tokens": ["Der", "Brit\u00b7ten", "tie\u00b7fer", "Sinn", ",", "dient", "Leip\u00b7zig", "zum", "Ge\u00b7winns\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "VVFIN", "NE", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Was sag ich? Salems Wissenschaft,", "tokens": ["Was", "sag", "ich", "?", "Sa\u00b7lems", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$.", "NE", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ph\u00f6niciens Verstand, Aegyptens Wunderwerke,", "tokens": ["Ph\u00f6\u00b7ni\u00b7ci\u00b7ens", "Ver\u00b7stand", ",", "A\u00b7e\u00b7gyp\u00b7tens", "Wun\u00b7der\u00b7wer\u00b7ke", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "NE", "NN", "$,"], "meter": "+-+--+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Erblickt man hier in voller St\u00e4rke,", "tokens": ["Er\u00b7blickt", "man", "hier", "in", "vol\u00b7ler", "St\u00e4r\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit j\u00e4hrlich neu verj\u00fcngter Kraft.", "tokens": ["Mit", "j\u00e4hr\u00b7lich", "neu", "ver\u00b7j\u00fcng\u00b7ter", "Kraft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Was sonst Ionien erfand,", "tokens": ["Was", "sonst", "I\u00b7o\u00b7ni\u00b7en", "er\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Arabien getr\u00e4umt, und Indien gelehret,", "tokens": ["A\u00b7ra\u00b7bi\u00b7en", "ge\u00b7tr\u00e4umt", ",", "und", "In\u00b7di\u00b7en", "ge\u00b7leh\u00b7ret", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$,", "KON", "NE", "VVPP", "$,"], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "Was Peking vom Confuz geh\u00f6ret,", "tokens": ["Was", "Pe\u00b7king", "vom", "Con\u00b7fuz", "ge\u00b7h\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "APPRART", "NN", "VVFIN", "$,"], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Der Perser Sonnendienst, und der Mogollen Tand;", "tokens": ["Der", "Per\u00b7ser", "Son\u00b7nen\u00b7dienst", ",", "und", "der", "Mo\u00b7gol\u00b7len", "Tand", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "KON", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Womit sich Mandarinen \u00e4ffen,", "tokens": ["Wo\u00b7mit", "sich", "Man\u00b7da\u00b7ri\u00b7nen", "\u00e4f\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und B\u00fccher aus Byzanz, die sind hier anzutreffen.", "tokens": ["Und", "B\u00fc\u00b7cher", "aus", "By\u00b7zanz", ",", "die", "sind", "hier", "an\u00b7zu\u00b7tref\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$,", "PRELS", "VAFIN", "ADV", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wo bleibt Athens Vernunft und Geist?", "tokens": ["Wo", "bleibt", "A\u00b7thens", "Ver\u00b7nunft", "und", "Geist", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "NE", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Bew\u00e4hrter Dichter Witz, der Redner Zauberworte;", "tokens": ["Be\u00b7w\u00e4hr\u00b7ter", "Dich\u00b7ter", "Witz", ",", "der", "Red\u00b7ner", "Zau\u00b7ber\u00b7wor\u00b7te", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Davon die Kraft an diesem Orte,", "tokens": ["Da\u00b7von", "die", "Kraft", "an", "die\u00b7sem", "Or\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Sich \u00f6fters noch lebendig weist.", "tokens": ["Sich", "\u00f6f\u00b7ters", "noch", "le\u00b7ben\u00b7dig", "weist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+++-+", "measure": "unknown.measure.penta"}, "line.15": {"text": "Wo bleibt der alten Weisen Mund;", "tokens": ["Wo", "bleibt", "der", "al\u00b7ten", "Wei\u00b7sen", "Mund", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Was Sokrates gelehrt, was Plato aufgeschrieben;", "tokens": ["Was", "Sok\u00b7ra\u00b7tes", "ge\u00b7lehrt", ",", "was", "Pla\u00b7to", "auf\u00b7ge\u00b7schrie\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VVPP", "$,", "PRELS", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Was uns vom Zeno noch geblieben;", "tokens": ["Was", "uns", "vom", "Ze\u00b7no", "noch", "ge\u00b7blie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPRART", "NE", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Was jener Stagirit, und Theophrast verstund;", "tokens": ["Was", "je\u00b7ner", "Sta\u00b7gi\u00b7rit", ",", "und", "Theo\u00b7ph\u00b7rast", "ver\u00b7stund", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "NN", "$,", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Was Rom im Tullius gebohren,", "tokens": ["Was", "Rom", "im", "Tul\u00b7li\u00b7us", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Am Antonin verehrt, im Seneca verlohren?", "tokens": ["Am", "An\u00b7to\u00b7nin", "ver\u00b7ehrt", ",", "im", "Se\u00b7ne\u00b7ca", "ver\u00b7loh\u00b7ren", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$,", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Das alles, und was Flaccus war,", "tokens": ["Das", "al\u00b7les", ",", "und", "was", "Flac\u00b7cus", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "$,", "KON", "PWS", "NE", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was Maro und Ovid und Livius gewesen,", "tokens": ["Was", "Ma\u00b7ro", "und", "O\u00b7vid", "und", "Li\u00b7vius", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "KON", "NE", "KON", "NE", "VAPP", "$,"], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das bl\u00fcht allhier, das h\u00f6rt man lesen,", "tokens": ["Das", "bl\u00fcht", "all\u00b7hier", ",", "das", "h\u00f6rt", "man", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "$,", "PDS", "VVFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das stellt uns Leipzig sch\u00f6ner dar.", "tokens": ["Das", "stellt", "uns", "Leip\u00b7zig", "sch\u00f6\u00b7ner", "dar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "NE", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der B\u00fcchers\u00e4le gro\u00dfe Zahl", "tokens": ["Der", "B\u00fc\u00b7cher\u00b7s\u00e4\u00b7le", "gro\u00b7\u00dfe", "Zahl"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hebt Seltenheiten auf, die in verflo\u00dfnen Jahren,", "tokens": ["Hebt", "Sel\u00b7ten\u00b7hei\u00b7ten", "auf", ",", "die", "in", "ver\u00b7flo\u00df\u00b7nen", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PTKVZ", "$,", "PRELS", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Bey fernen V\u00f6lkern heilig waren;", "tokens": ["Bey", "fer\u00b7nen", "V\u00f6l\u00b7kern", "hei\u00b7lig", "wa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Besonders von Geschmack, und ungemein an Wahl.", "tokens": ["Be\u00b7son\u00b7ders", "von", "Ge\u00b7schmack", ",", "und", "un\u00b7ge\u00b7mein", "an", "Wahl", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$,", "KON", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Hier leben gro\u00dfer K\u00fcnstler Werke,", "tokens": ["Hier", "le\u00b7ben", "gro\u00b7\u00dfer", "K\u00fcnst\u00b7ler", "Wer\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ja Sachsens F\u00fcrsten selbst, in Bildern voller St\u00e4rke.", "tokens": ["Ja", "Sach\u00b7sens", "F\u00fcrs\u00b7ten", "selbst", ",", "in", "Bil\u00b7dern", "vol\u00b7ler", "St\u00e4r\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "NN", "NN", "ADV", "$,", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Verkl\u00e4rter ", "tokens": ["Ver\u00b7kl\u00e4r\u00b7ter"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Der Du den Musensitz am Plei\u00dfenstrom erbauet,", "tokens": ["Der", "Du", "den", "Mu\u00b7sen\u00b7sitz", "am", "Plei\u00b7\u00dfen\u00b7strom", "er\u00b7bau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Auch Dein Gem\u00e4ld wird hier geschauet,", "tokens": ["Auch", "Dein", "Ge\u00b7m\u00e4ld", "wird", "hier", "ge\u00b7schau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wo es die Ehrfurcht aufgestellt.", "tokens": ["Wo", "es", "die", "Ehr\u00b7furcht", "auf\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dir weis es Leipzig ewig Dank,", "tokens": ["Dir", "weis", "es", "Leip\u00b7zig", "e\u00b7wig", "Dank", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "PPER", "NE", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da\u00df Du der Wissenschaft den Aufenthalt gegr\u00fcndet:", "tokens": ["Da\u00df", "Du", "der", "Wis\u00b7sen\u00b7schaft", "den", "Auf\u00b7ent\u00b7halt", "ge\u00b7gr\u00fcn\u00b7det", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So lange sich der Witz hier findet,", "tokens": ["So", "lan\u00b7ge", "sich", "der", "Witz", "hier", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PRF", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Verehrt, o Churf\u00fcrst! Dich der Musen Lobgesang.", "tokens": ["Ver\u00b7ehrt", ",", "o", "Chur\u00b7f\u00fcrst", "!", "Dich", "der", "Mu\u00b7sen", "Lob\u00b7ge\u00b7sang", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "FM", "FM", "$.", "PPER", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du warest streitbar in den Kriegen;", "tokens": ["Du", "wa\u00b7rest", "streit\u00b7bar", "in", "den", "Krie\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und gleichwohl ist durch Dich die Wissenschaft gestiegen.", "tokens": ["Und", "gleich\u00b7wohl", "ist", "durch", "Dich", "die", "Wis\u00b7sen\u00b7schaft", "ge\u00b7stie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "APPR", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Dir folgt der Helden ganze Reih,", "tokens": ["Dir", "folgt", "der", "Hel\u00b7den", "gan\u00b7ze", "Reih", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Deinen Zweck erf\u00fcllt, der Weisheit Flor geheget,", "tokens": ["Die", "Dei\u00b7nen", "Zweck", "er\u00b7f\u00fcllt", ",", "der", "Weis\u00b7heit", "Flor", "ge\u00b7he\u00b7get", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVPP", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und jede Wissenschaft verpfleget;", "tokens": ["Und", "je\u00b7de", "Wis\u00b7sen\u00b7schaft", "ver\u00b7pfle\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die alle sind vom Tode frey!", "tokens": ["Die", "al\u00b7le", "sind", "vom", "To\u00b7de", "frey", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Vor andern prangen au\u00dfer Dir,", "tokens": ["Vor", "an\u00b7dern", "pran\u00b7gen", "au\u00b7\u00dfer", "Dir", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Von welchen Pfllicht und Wahrheit melden:", "tokens": ["Von", "wel\u00b7chen", "Pfllicht", "und", "Wahr\u00b7heit", "mel\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Sie mehrten Leipzigs Flor, der freyen K\u00fcnste Zier.", "tokens": ["Sie", "mehr\u00b7ten", "Leip\u00b7zigs", "Flor", ",", "der", "frey\u00b7en", "K\u00fcns\u00b7te", "Zier", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "NN", "$,", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch ihre Sorgfalt ists geschehen,", "tokens": ["Durch", "ih\u00b7re", "Sorg\u00b7falt", "ists", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Da\u00df wir noch Priester gnug in Pallas Tempeln sehen.", "tokens": ["Da\u00df", "wir", "noch", "Pries\u00b7ter", "gnug", "in", "Pal\u00b7las", "Tem\u00b7peln", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "ADV", "APPR", "NE", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Hier steht im sch\u00f6nsten Purpurschmuck,", "tokens": ["Hier", "steht", "im", "sch\u00f6ns\u00b7ten", "Pur\u00b7pursc\u00b7hmuck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Lehrer kleine Zahl, die solchen gleich getragen,", "tokens": ["Der", "Leh\u00b7rer", "klei\u00b7ne", "Zahl", ",", "die", "sol\u00b7chen", "gleich", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "PRELS", "PIAT", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als sie in ihren letzten Tagen", "tokens": ["Als", "sie", "in", "ih\u00b7ren", "letz\u00b7ten", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Des Todes Sichel niederschlug.", "tokens": ["Des", "To\u00b7des", "Si\u00b7chel", "nie\u00b7der\u00b7schlug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Nachwelt ehrt noch ihre Gruft,", "tokens": ["Die", "Nach\u00b7welt", "ehrt", "noch", "ih\u00b7re", "Gruft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und Leipzig wird ihr Lob, so lang es steht, bekr\u00f6nen;", "tokens": ["Und", "Leip\u00b7zig", "wird", "ihr", "Lob", ",", "so", "lang", "es", "steht", ",", "be\u00b7kr\u00f6\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "PPOSAT", "NN", "$,", "ADV", "ADJD", "PPER", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Man zeigt ihr Beyspiel muntern S\u00f6hnen,", "tokens": ["Man", "zeigt", "ihr", "Bey\u00b7spiel", "mun\u00b7tern", "S\u00f6h\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Indem man ihren Fu\u00df zum Weisheitpfade ruft.", "tokens": ["In\u00b7dem", "man", "ih\u00b7ren", "Fu\u00df", "zum", "Weis\u00b7heit\u00b7pfa\u00b7de", "ruft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn nichts entz\u00fcndet mehr die Jugend,", "tokens": ["Denn", "nichts", "ent\u00b7z\u00fcn\u00b7det", "mehr", "die", "Ju\u00b7gend", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Als Muster edler Art an Wissenschaft und Tugend.", "tokens": ["Als", "Mus\u00b7ter", "ed\u00b7ler", "Art", "an", "Wis\u00b7sen\u00b7schaft", "und", "Tu\u00b7gend", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Was prangt nicht dort f\u00fcr manches Licht,", "tokens": ["Was", "prangt", "nicht", "dort", "f\u00fcr", "man\u00b7ches", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PTKNEG", "ADV", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das die gelehrte Welt, gleich hellen Sternen schm\u00fccket,", "tokens": ["Das", "die", "ge\u00b7lehr\u00b7te", "Welt", ",", "gleich", "hel\u00b7len", "Ster\u00b7nen", "schm\u00fc\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "ADJA", "NN", "$,", "ADV", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wird nicht ", "tokens": ["Wird", "nicht"], "token_info": ["word", "word"], "pos": ["VAFIN", "PTKNEG"], "meter": "-+", "measure": "iambic.single"}, "line.4": {"text": "Seh ich den gro\u00dfen ", "tokens": ["Seh", "ich", "den", "gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Da stralt ein kluger ", "tokens": ["Da", "stralt", "ein", "klu\u00b7ger"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Auch ", "tokens": ["Auch"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Wie Preu\u00dfens Archimed und Schmuck,", "tokens": ["Wie", "Preu\u00b7\u00dfens", "Ar\u00b7chi\u00b7med", "und", "Schmuck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Noch funfzig andre sieht man prangen,", "tokens": ["Noch", "funf\u00b7zig", "and\u00b7re", "sieht", "man", "pran\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "PIS", "VVFIN", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die uns, wie ", "tokens": ["Die", "uns", ",", "wie"], "token_info": ["word", "word", "punct", "word"], "pos": ["ART", "PPER", "$,", "PWAV"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.17": {"line.1": {"text": "Nur einer fehlt, der hier nicht steht!", "tokens": ["Nur", "ei\u00b7ner", "fehlt", ",", "der", "hier", "nicht", "steht", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$,", "PRELS", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und doch an Ruhm und Glanz und Gr\u00f6\u00dfe keinem weichet;", "tokens": ["Und", "doch", "an", "Ruhm", "und", "Glanz", "und", "Gr\u00f6\u00b7\u00dfe", "kei\u00b7nem", "wei\u00b7chet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "KON", "NN", "KON", "NN", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein Mann, der alles l\u00e4ngst erreichet,", "tokens": ["Ein", "Mann", ",", "der", "al\u00b7les", "l\u00e4ngst", "er\u00b7rei\u00b7chet", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wodurch man ewig sich erh\u00f6ht.", "tokens": ["Wo\u00b7durch", "man", "e\u00b7wig", "sich", "er\u00b7h\u00f6ht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADJD", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Wunder tiefer Wissenschaft,", "tokens": ["Ein", "Wun\u00b7der", "tie\u00b7fer", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Durchdringend an Vernunft, an Einsicht auserlesen,", "tokens": ["Durch\u00b7drin\u00b7gend", "an", "Ver\u00b7nunft", ",", "an", "Ein\u00b7sicht", "au\u00b7ser\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NN", "$,", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ein Geist von allgemeinem Wesen,", "tokens": ["Ein", "Geist", "von", "all\u00b7ge\u00b7mei\u00b7nem", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Von unumschr\u00e4nktem Witz und unersch\u00f6pfter Kraft;", "tokens": ["Von", "un\u00b7um\u00b7schr\u00e4nk\u00b7tem", "Witz", "und", "un\u00b7er\u00b7sch\u00f6pf\u00b7ter", "Kraft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Der alles das in eins gebunden,", "tokens": ["Der", "al\u00b7les", "das", "in", "eins", "ge\u00b7bun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ART", "APPR", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Was je der Mensch erfand; doch selbst noch mehr erfunden.", "tokens": ["Was", "je", "der", "Mensch", "er\u00b7fand", ";", "doch", "selbst", "noch", "mehr", "er\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVFIN", "$.", "ADV", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Wer ists? O Leipzig! sollte man", "tokens": ["Wer", "ists", "?", "O", "Leip\u00b7zig", "!", "soll\u00b7te", "man"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "$.", "NE", "NE", "$.", "VMFIN", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bey dir noch allererst nach dessen Namen fragen?", "tokens": ["Bey", "dir", "noch", "al\u00b7le\u00b7rerst", "nach", "des\u00b7sen", "Na\u00b7men", "fra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "ADV", "APPR", "PRELAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Den doch dein eigner Schoo\u00df getragen,", "tokens": ["Den", "doch", "dein", "eig\u00b7ner", "Schoo\u00df", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PPOSAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als er das erste Licht gewann?", "tokens": ["Als", "er", "das", "ers\u00b7te", "Licht", "ge\u00b7wann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ist dir dein Sohn so schlecht bekannt,", "tokens": ["Ist", "dir", "dein", "Sohn", "so", "schlecht", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPOSAT", "NN", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Den halb Europa so, wie Deutschland, hochgeachtet,", "tokens": ["Den", "halb", "Eu\u00b7ro\u00b7pa", "so", ",", "wie", "Deutschland", ",", "hoch\u00b7ge\u00b7ach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJD", "NE", "ADV", "$,", "PWAV", "NE", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Den Albion voll Neid betrachtet,", "tokens": ["Den", "Al\u00b7bi\u00b7on", "voll", "Neid", "be\u00b7trach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den Frankreich uns misg\u00f6nnt, so wie das w\u00e4lsche Land?", "tokens": ["Den", "Fran\u00b7kreich", "uns", "mis\u00b7g\u00f6nnt", ",", "so", "wie", "das", "w\u00e4l\u00b7sche", "Land", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "PPER", "VVFIN", "$,", "ADV", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie? Leipzig, kannst du den verkennen,", "tokens": ["Wie", "?", "Leip\u00b7zig", ",", "kannst", "du", "den", "ver\u00b7ken\u00b7nen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "NE", "$,", "VMFIN", "PPER", "ART", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Um den die V\u00f6lker dich begl\u00fcckt und selig nennen?", "tokens": ["Um", "den", "die", "V\u00f6l\u00b7ker", "dich", "be\u00b7gl\u00fcckt", "und", "se\u00b7lig", "nen\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "ART", "NN", "PPER", "VVPP", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Dein ", "tokens": ["Dein"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Der deine gleichfalls wuchs, dieweil du ihn gebohren!", "tokens": ["Der", "dei\u00b7ne", "gleich\u00b7falls", "wuchs", ",", "die\u00b7weil", "du", "ihn", "ge\u00b7boh\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "ADV", "VVFIN", "$,", "KOUS", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Denn hast du ihn gleich jung verlohren;", "tokens": ["Denn", "hast", "du", "ihn", "gleich", "jung", "ver\u00b7loh\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So blieb er doch dein Eigenthum.", "tokens": ["So", "blieb", "er", "doch", "dein", "Ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "Warum? des ", "tokens": ["Wa\u00b7rum", "?", "des"], "token_info": ["word", "punct", "word"], "pos": ["PWAV", "$.", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "So lang ein ", "tokens": ["So", "lang", "ein"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADJD", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "R\u00fchmt sichs des ", "tokens": ["R\u00fchmt", "sichs", "des"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PIS", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.9": {"text": "So lange Rotterdam wird stehen,", "tokens": ["So", "lan\u00b7ge", "Rot\u00b7ter\u00b7dam", "wird", "ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NE", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Wird auch dein Ehrenmaal, ", "tokens": ["Wird", "auch", "dein", "Eh\u00b7ren\u00b7maal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Wenn sieben St\u00e4dte den ", "tokens": ["Wenn", "sie\u00b7ben", "St\u00e4d\u00b7te", "den"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "CARD", "NN", "ART"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Aus reger Eifersucht einander abgestritten:", "tokens": ["Aus", "re\u00b7ger", "Ei\u00b7fer\u00b7sucht", "ein\u00b7an\u00b7der", "ab\u00b7ge\u00b7strit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "VVPP", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Was h\u00e4tte ", "tokens": ["Was", "h\u00e4t\u00b7te"], "token_info": ["word", "word"], "pos": ["PWS", "VAFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Wenn hier ein Zweifel m\u00f6glich w\u00e4r?", "tokens": ["Wenn", "hier", "ein", "Zwei\u00b7fel", "m\u00f6g\u00b7lich", "w\u00e4r", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der stolzen Tyber breiter Rand", "tokens": ["Der", "stol\u00b7zen", "Ty\u00b7ber", "brei\u00b7ter", "Rand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "W\u00fcrd eifrig um den Ruhm von dieser Wiege k\u00e4mpfen.", "tokens": ["W\u00fcrd", "eif\u00b7rig", "um", "den", "Ruhm", "von", "die\u00b7ser", "Wie\u00b7ge", "k\u00e4mp\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "ART", "NN", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Seyne, solchen Stolz zu d\u00e4mpfen,", "tokens": ["Die", "Sey\u00b7ne", ",", "sol\u00b7chen", "Stolz", "zu", "d\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "W\u00fcrd streiten, da\u00df man ihr die\u00df hohe Lob entwandt.", "tokens": ["W\u00fcrd", "strei\u00b7ten", ",", "da\u00df", "man", "ihr", "die\u00df", "ho\u00b7he", "Lob", "ent\u00b7wandt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVFIN", "$,", "KOUS", "PIS", "PPER", "PDS", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und an der Themse feuchten Fl\u00e4chen,", "tokens": ["Und", "an", "der", "Them\u00b7se", "feuch\u00b7ten", "Fl\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "W\u00fcrd London eifern, sich den Vorzug zuzusprechen.", "tokens": ["W\u00fcrd", "Lon\u00b7don", "ei\u00b7fern", ",", "sich", "den", "Vor\u00b7zug", "zu\u00b7zu\u00b7spre\u00b7chen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "VVFIN", "$,", "PRF", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Da ihn in seinen Lebensjahren", "tokens": ["Da", "ihn", "in", "sei\u00b7nen", "Le\u00b7bens\u00b7jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kein Reich und keine Stadt begehrt.", "tokens": ["Kein", "Reich", "und", "kei\u00b7ne", "Stadt", "be\u00b7gehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um des von ", "tokens": ["Um", "des", "von"], "token_info": ["word", "word", "word"], "pos": ["KOUI", "ART", "APPR"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Hat manch gekr\u00f6ntes Haupt, vorl\u00e4ngst eh er gestorben,", "tokens": ["Hat", "manch", "ge\u00b7kr\u00f6n\u00b7tes", "Haupt", ",", "vor\u00b7l\u00e4ngst", "eh", "er", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "$,", "ADV", "KOUS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Durch Gnad und Wohlthun sich beworben,", "tokens": ["Durch", "Gnad", "und", "Wohl\u00b7thun", "sich", "be\u00b7wor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "In L\u00e4ndern, wo er noch verehrungsw\u00fcrdig hei\u00dft;", "tokens": ["In", "L\u00e4n\u00b7dern", ",", "wo", "er", "noch", "ver\u00b7eh\u00b7rungs\u00b7w\u00fcr\u00b7dig", "hei\u00dft", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PWAV", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Wo sein Verdienst und Rath und Schriften,", "tokens": ["Wo", "sein", "Ver\u00b7dienst", "und", "Rath", "und", "Schrif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "KON", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Ihn lebend gro\u00df gemacht, ihm todt manch Denkmaal stiften.", "tokens": ["Ihn", "le\u00b7bend", "gro\u00df", "ge\u00b7macht", ",", "ihm", "todt", "manch", "Denk\u00b7maal", "stif\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "VVPP", "$,", "PPER", "ADJD", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Der Britten Haupt hat ihn erh\u00f6ht,", "tokens": ["Der", "Brit\u00b7ten", "Haupt", "hat", "ihn", "er\u00b7h\u00f6ht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Barbarey aus seinen Staaten;", "tokens": ["Die", "Bar\u00b7ba\u00b7rey", "aus", "sei\u00b7nen", "Staa\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wo noch sein Ruhm im Segen steht.", "tokens": ["Wo", "noch", "sein", "Ruhm", "im", "Se\u00b7gen", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der sechste ", "tokens": ["Der", "sechs\u00b7te"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Sein Feldherr, ", "tokens": ["Sein", "Feld\u00b7herr", ","], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Vernahmen kaum was er begehrte;", "tokens": ["Ver\u00b7nah\u00b7men", "kaum", "was", "er", "be\u00b7gehr\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PWS", "PPER", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So ward ihm selbst in Wien der Zutritt bald erlaubt.", "tokens": ["So", "ward", "ihm", "selbst", "in", "Wi\u00b7en", "der", "Zu\u00b7tritt", "bald", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPR", "NE", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Lutetien war ihm gewogen,", "tokens": ["Lu\u00b7te\u00b7ti\u00b7en", "war", "ihm", "ge\u00b7wo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.9": {"text": "Und h\u00e4tt auf Lebenslang ihn gern zu sich gezogen.", "tokens": ["Und", "h\u00e4tt", "auf", "Le\u00b7bens\u00b7lang", "ihn", "gern", "zu", "sich", "ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "NN", "PPER", "ADV", "APPR", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.23": {"line.1": {"text": "Besoldung, Aemter, suchten ihn,", "tokens": ["Be\u00b7sol\u00b7dung", ",", "A\u00b7em\u00b7ter", ",", "such\u00b7ten", "ihn", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Des Reiches Freyherrnstand, (ein seltner Lohn vom Wissen,", "tokens": ["Des", "Rei\u00b7ches", "Frey\u00b7herrns\u00b7tand", ",", "(", "ein", "selt\u00b7ner", "Lohn", "vom", "Wis\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "$(", "ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Seit ihn das Gold zu sich gerissen!)", "tokens": ["Seit", "ihn", "das", "Gold", "zu", "sich", "ge\u00b7ris\u00b7sen", "!", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "APPR", "PRF", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vergalten sein gelehrt Bem\u00fchn.", "tokens": ["Ver\u00b7gal\u00b7ten", "sein", "ge\u00b7lehrt", "Be\u00b7m\u00fchn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "VVPP", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Bey zweenen Kaisern Rath zu seyn,", "tokens": ["Bey", "zwee\u00b7nen", "Kai\u00b7sern", "Rath", "zu", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und so viel K\u00f6nigen mit Werk und That zu dienen,", "tokens": ["Und", "so", "viel", "K\u00f6\u00b7ni\u00b7gen", "mit", "Werk", "und", "That", "zu", "die\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIAT", "NN", "APPR", "NN", "KON", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Hat billig jedem viel geschienen,", "tokens": ["Hat", "bil\u00b7lig", "je\u00b7dem", "viel", "ge\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "PIAT", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Hei\u00dft wirklich ehrenvoll, bleibt ewig ungemein;", "tokens": ["Hei\u00dft", "wirk\u00b7lich", "eh\u00b7ren\u00b7voll", ",", "bleibt", "e\u00b7wig", "un\u00b7ge\u00b7mein", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADJD", "$,", "VVFIN", "ADJD", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Seit Gattungen geringrer Gaben,", "tokens": ["Seit", "Gat\u00b7tun\u00b7gen", "ge\u00b7ring\u00b7rer", "Ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Die strenge Wissenschaft vom Hof entfernet haben.", "tokens": ["Die", "stren\u00b7ge", "Wis\u00b7sen\u00b7schaft", "vom", "Hof", "ent\u00b7fer\u00b7net", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.24": {"line.1": {"text": "An Witz und Einsicht reich und satt,", "tokens": ["An", "Witz", "und", "Ein\u00b7sicht", "reich", "und", "satt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat er der Wahrheit sich zum Priester eingeweihet:", "tokens": ["Hat", "er", "der", "Wahr\u00b7heit", "sich", "zum", "Pries\u00b7ter", "ein\u00b7ge\u00b7wei\u00b7het", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "PRF", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hier hat er keine M\u00fch gescheuet,", "tokens": ["Hier", "hat", "er", "kei\u00b7ne", "M\u00fch", "ge\u00b7scheu\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Davor ein Tr\u00e4ger Abscheu hat.", "tokens": ["Da\u00b7vor", "ein", "Tr\u00e4\u00b7ger", "Ab\u00b7scheu", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der tiefsten Weisheit ersten Grund,", "tokens": ["Der", "tiefs\u00b7ten", "Weis\u00b7heit", "ers\u00b7ten", "Grund", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Sch\u00e4tze der Natur, der Zahlen Seltenheiten,", "tokens": ["Die", "Sch\u00e4t\u00b7ze", "der", "Na\u00b7tur", ",", "der", "Zah\u00b7len", "Sel\u00b7ten\u00b7hei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Me\u00dfkunst hohe Trefflichkeiten,", "tokens": ["Der", "Me\u00df\u00b7kunst", "ho\u00b7he", "Treff\u00b7lich\u00b7kei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Das alles sah er ein; das that er andern kund.", "tokens": ["Das", "al\u00b7les", "sah", "er", "ein", ";", "das", "that", "er", "an\u00b7dern", "kund", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "PPER", "PTKVZ", "$.", "PDS", "VVFIN", "PPER", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er war ein Meister in Geschichten,", "tokens": ["Er", "war", "ein", "Meis\u00b7ter", "in", "Ge\u00b7schich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Im Alterthume stark, und ein Lucrez im Dichten.", "tokens": ["Im", "Al\u00b7ter\u00b7thu\u00b7me", "stark", ",", "und", "ein", "Lu\u00b7crez", "im", "Dich\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "$,", "KON", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.25": {"line.1": {"text": "Wer kennt die Wunderrechnung nicht,", "tokens": ["Wer", "kennt", "die", "Wun\u00b7der\u00b7rech\u00b7nung", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Archimed ersann, den Weltraum zu ergr\u00fcnden?", "tokens": ["Die", "Ar\u00b7chi\u00b7med", "er\u00b7sann", ",", "den", "Welt\u00b7raum", "zu", "er\u00b7gr\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was gr\u00f6\u00dfers war kaum auszufinden,", "tokens": ["Was", "gr\u00f6\u00b7\u00dfers", "war", "kaum", "aus\u00b7zu\u00b7fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VAFIN", "ADV", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In dem, was Menschenwitz verspricht.", "tokens": ["In", "dem", ",", "was", "Men\u00b7schen\u00b7witz", "ver\u00b7spricht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PWS", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nur ", "tokens": ["Nur"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Er fand die Rechenkunst in dem unendlich Kleinen:", "tokens": ["Er", "fand", "die", "Re\u00b7chen\u00b7kunst", "in", "dem", "un\u00b7end\u00b7lich", "Klei\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Hier konnt er doppelt gro\u00df erscheinen,", "tokens": ["Hier", "konnt", "er", "dop\u00b7pelt", "gro\u00df", "er\u00b7schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADJD", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und ganzer V\u00f6lker Neid war seines Witzes Frucht.", "tokens": ["Und", "gan\u00b7zer", "V\u00f6l\u00b7ker", "Neid", "war", "sei\u00b7nes", "Wit\u00b7zes", "Frucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "NN", "VAFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Die Eifersucht der stolzen Britten", "tokens": ["Die", "Ei\u00b7fer\u00b7sucht", "der", "stol\u00b7zen", "Brit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Hat die Erfindung ihm aufs heftigste bestritten.", "tokens": ["Hat", "die", "Er\u00b7fin\u00b7dung", "ihm", "aufs", "hef\u00b7tigs\u00b7te", "be\u00b7strit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.26": {"line.1": {"text": "Wie dort den neuen Theil der Welt,", "tokens": ["Wie", "dort", "den", "neu\u00b7en", "Theil", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df beyder Ruhm zwar nicht verschwunden;", "tokens": ["Da\u00df", "bey\u00b7der", "Ruhm", "zwar", "nicht", "ver\u00b7schwun\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ob jener gleich den Preis beh\u00e4lt.", "tokens": ["Ob", "je\u00b7ner", "gleich", "den", "Preis", "be\u00b7h\u00e4lt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "H\u00e4tt kein Columbus sich gewagt,", "tokens": ["H\u00e4tt", "kein", "Co\u00b7lum\u00b7bus", "sich", "ge\u00b7wagt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und seinen k\u00fchnen Mast dem Ocean vertrauet,", "tokens": ["Und", "sei\u00b7nen", "k\u00fch\u00b7nen", "Mast", "dem", "O\u00b7cean", "ver\u00b7trau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Den noch kein Schiffer je geschauet:", "tokens": ["Den", "noch", "kein", "Schif\u00b7fer", "je", "ge\u00b7schau\u00b7et", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Wem h\u00e4tt Americus so herzhaft nachgejagt?", "tokens": ["Wem", "h\u00e4tt", "A\u00b7me\u00b7ri\u00b7cus", "so", "herz\u00b7haft", "nach\u00b7ge\u00b7jagt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NE", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So w\u00e4r auch ", "tokens": ["So", "w\u00e4r", "auch"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "W\u00e4r unsers ", "tokens": ["W\u00e4r", "un\u00b7sers"], "token_info": ["word", "word"], "pos": ["VAFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.27": {"line.1": {"text": "Gebrauchte sonst ", "tokens": ["Ge\u00b7brauch\u00b7te", "sonst"], "token_info": ["word", "word"], "pos": ["NN", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Die Kunst, zehn Ziffern nur im Rechnen anzuwenden;", "tokens": ["Die", "Kunst", ",", "zehn", "Zif\u00b7fern", "nur", "im", "Rech\u00b7nen", "an\u00b7zu\u00b7wen\u00b7den", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "CARD", "NN", "ADV", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und doch das schwerste zu vollenden;", "tokens": ["Und", "doch", "das", "schwers\u00b7te", "zu", "voll\u00b7en\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So that zwar ", "tokens": ["So", "that", "zwar"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Vier Ziffern langten v\u00f6llig hin,", "tokens": ["Vier", "Zif\u00b7fern", "lang\u00b7ten", "v\u00f6l\u00b7lig", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die unerme\u00dfne Reih der Gr\u00f6\u00dfen zu erreichen:", "tokens": ["Die", "un\u00b7er\u00b7me\u00df\u00b7ne", "Reih", "der", "Gr\u00f6\u00b7\u00dfen", "zu", "er\u00b7rei\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch dieser Kunstgriff selbst mu\u00df weichen,", "tokens": ["Doch", "die\u00b7ser", "Kunst\u00b7griff", "selbst", "mu\u00df", "wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "NN", "ADV", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Was gr\u00f6\u00dfers noch erfand des ", "tokens": ["Was", "gr\u00f6\u00b7\u00dfers", "noch", "er\u00b7fand", "des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ADV", "VVFIN", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Das ungeheure Heer der Zahlen", "tokens": ["Das", "un\u00b7ge\u00b7heu\u00b7re", "Heer", "der", "Zah\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "L\u00e4\u00dft durch zwo Ziffern sich, durch Null und Eins schon malen.", "tokens": ["L\u00e4\u00dft", "durch", "zwo", "Zif\u00b7fern", "sich", ",", "durch", "Null", "und", "Eins", "schon", "ma\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "CARD", "NN", "PRF", "$,", "APPR", "NE", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.28": {"line.1": {"text": "Ihr V\u00f6lker! deren letzten Strand,", "tokens": ["Ihr", "V\u00f6l\u00b7ker", "!", "de\u00b7ren", "letz\u00b7ten", "Strand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "PRELAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Japonesermeer durch seine Fluth benetzet,", "tokens": ["Das", "Ja\u00b7po\u00b7nes\u00b7er\u00b7meer", "durch", "sei\u00b7ne", "Fluth", "be\u00b7net\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Die ihr nur euch f\u00fcr weise sch\u00e4tzet,", "tokens": ["Die", "ihr", "nur", "euch", "f\u00fcr", "wei\u00b7se", "sch\u00e4t\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "PPER", "APPR", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bewundert dieses Manns Verstand!", "tokens": ["Be\u00b7wun\u00b7dert", "die\u00b7ses", "Manns", "Ver\u00b7stand", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ihr, die ihr sonst Europen kaum", "tokens": ["Ihr", ",", "die", "ihr", "sonst", "Eu\u00b7ro\u00b7pen", "kaum"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "PPER", "ADV", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Auge zugesteht, die Wahrheit zu erkennen:", "tokens": ["Ein", "Au\u00b7ge", "zu\u00b7ge\u00b7steht", ",", "die", "Wahr\u00b7heit", "zu", "er\u00b7ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "H\u00f6rt auf, euch noch so klug zu nennen,", "tokens": ["H\u00f6rt", "auf", ",", "euch", "noch", "so", "klug", "zu", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "$,", "PPER", "ADV", "ADV", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und gebt hinfort nicht mehr dem alten Stolze Raum:", "tokens": ["Und", "gebt", "hin\u00b7fort", "nicht", "mehr", "dem", "al\u00b7ten", "Stol\u00b7ze", "Raum", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PTKNEG", "ADV", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Seitdem ein Deutscher euch erkl\u00e4ret,", "tokens": ["Seit\u00b7dem", "ein", "Deut\u00b7scher", "euch", "er\u00b7kl\u00e4\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Was eures Stifters Witz euch r\u00e4thselhaft gelehret.", "tokens": ["Was", "eu\u00b7res", "Stif\u00b7ters", "Witz", "euch", "r\u00e4th\u00b7sel\u00b7haft", "ge\u00b7leh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "NN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.29": {"line.1": {"text": "Des gro\u00dfen ", "tokens": ["Des", "gro\u00b7\u00dfen"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Vertraute seine Kunst geheimnisvollen Strichen;", "tokens": ["Ver\u00b7trau\u00b7te", "sei\u00b7ne", "Kunst", "ge\u00b7heim\u00b7nis\u00b7vol\u00b7len", "Stri\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Kraft davon war euch entwichen,", "tokens": ["Die", "Kraft", "da\u00b7von", "war", "euch", "ent\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und was man vorgab, fiel dahin.", "tokens": ["Und", "was", "man", "vor\u00b7gab", ",", "fiel", "da\u00b7hin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PIS", "VVFIN", "$,", "VVFIN", "PAV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "In mancher lockenden Figur", "tokens": ["In", "man\u00b7cher", "lo\u00b7cken\u00b7den", "Fi\u00b7gur"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Gebrochner Linien mit ganzen untermenget,", "tokens": ["Ge\u00b7broch\u00b7ner", "Li\u00b7ni\u00b7en", "mit", "gan\u00b7zen", "un\u00b7ter\u00b7men\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Lag ein verborgner Sinn gedr\u00e4nget,", "tokens": ["Lag", "ein", "ver\u00b7borg\u00b7ner", "Sinn", "ge\u00b7dr\u00e4n\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und die\u00df versteckte Werk erreichte ", "tokens": ["Und", "die\u00df", "ver\u00b7steck\u00b7te", "Werk", "er\u00b7reich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "VVFIN", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Was China seit vier tausend Jahren", "tokens": ["Was", "Chi\u00b7na", "seit", "vier", "tau\u00b7send", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "NE", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Gesucht und nicht entdeckt, hat es durch ihn erfahren.", "tokens": ["Ge\u00b7sucht", "und", "nicht", "ent\u00b7deckt", ",", "hat", "es", "durch", "ihn", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "PTKNEG", "VVPP", "$,", "VAFIN", "PPER", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.30": {"line.1": {"text": "Der Preu\u00dfen erster ", "tokens": ["Der", "Preu\u00b7\u00dfen", "ers\u00b7ter"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Der jede Wissenschaft auf seinen Thron erhoben,", "tokens": ["Der", "je\u00b7de", "Wis\u00b7sen\u00b7schaft", "auf", "sei\u00b7nen", "Thron", "er\u00b7ho\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "APPR", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und den noch alle Musen loben,", "tokens": ["Und", "den", "noch", "al\u00b7le", "Mu\u00b7sen", "lo\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil unter Ihm ihr Kummer wich;", "tokens": ["Weil", "un\u00b7ter", "Ihm", "ihr", "Kum\u00b7mer", "wich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der weise Held empfand den Trieb,", "tokens": ["Der", "wei\u00b7se", "Held", "emp\u00b7fand", "den", "Trieb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der Weisheit in Berlin ein eignes Haus zu gr\u00fcnden.", "tokens": ["Der", "Weis\u00b7heit", "in", "Ber\u00b7lin", "ein", "eig\u00b7nes", "Haus", "zu", "gr\u00fcn\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+++-+-+-+-", "measure": "unknown.measure.septa"}, "line.7": {"text": "Hier war ein ", "tokens": ["Hier", "war", "ein"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VAFIN", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "Der dieser neuen Zunft die ersten Regeln schrieb.", "tokens": ["Der", "die\u00b7ser", "neu\u00b7en", "Zunft", "die", "ers\u00b7ten", "Re\u00b7geln", "schrieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "ADJA", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und der Gesellschaft Grund geleget,", "tokens": ["Und", "der", "Ge\u00b7sell\u00b7schaft", "Grund", "ge\u00b7le\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Die Deutschland itzt noch ziert und reichlich Fr\u00fcchte tr\u00e4get.", "tokens": ["Die", "Deutschland", "itzt", "noch", "ziert", "und", "reich\u00b7lich", "Fr\u00fcch\u00b7te", "tr\u00e4\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "VVFIN", "KON", "ADJD", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.31": {"line.1": {"text": "Wie hoch erhob die Weisheit dich,", "tokens": ["Wie", "hoch", "er\u00b7hob", "die", "Weis\u00b7heit", "dich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "ART", "NN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Minerva Deiner Zeit, verkl\u00e4rte ", "tokens": ["Mi\u00b7ner\u00b7va", "Dei\u00b7ner", "Zeit", ",", "ver\u00b7kl\u00e4r\u00b7te"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "PPOSAT", "NN", "$,", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Du prangst zwar an der Sternenb\u00fchne;", "tokens": ["Du", "prangst", "zwar", "an", "der", "Ster\u00b7nen\u00b7b\u00fch\u00b7ne", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch auch Dein Ruhm verewigt sich.", "tokens": ["Doch", "auch", "Dein", "Ruhm", "ve\u00b7re\u00b7wigt", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "VVFIN", "PRF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Hat Leibnitz nicht durch Deine Hand", "tokens": ["Hat", "Leib\u00b7nitz", "nicht", "durch", "Dei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "NE", "PTKNEG", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit ", "tokens": ["Mit"], "token_info": ["word"], "pos": ["APPR"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Davon das Lob nur Dir geb\u00fchret;", "tokens": ["Da\u00b7von", "das", "Lob", "nur", "Dir", "ge\u00b7b\u00fch\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ob Deutsch- und England gleich den Nutz davon empfand?", "tokens": ["Ob", "Deut\u00b7sch", "und", "En\u00b7gland", "gleich", "den", "Nutz", "da\u00b7von", "emp\u00b7fand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "TRUNC", "KON", "NE", "ADV", "ART", "NN", "PAV", "VVFIN", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Wie bey Turnieren alter Zeiten,", "tokens": ["Wie", "bey", "Tur\u00b7nie\u00b7ren", "al\u00b7ter", "Zei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Warst Du die Richterinn gelehrter Zwistigkeiten.", "tokens": ["Warst", "Du", "die", "Rich\u00b7te\u00b7rinn", "ge\u00b7lehr\u00b7ter", "Zwis\u00b7tig\u00b7kei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.32": {"line.1": {"text": "Es regte sich der Sp\u00f6tter Wuth", "tokens": ["Es", "reg\u00b7te", "sich", "der", "Sp\u00f6t\u00b7ter", "Wuth"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Durch Schl\u00fcsse voller Trug den Glauben zu bek\u00e4mpfen,", "tokens": ["Durch", "Schl\u00fcs\u00b7se", "vol\u00b7ler", "Trug", "den", "Glau\u00b7ben", "zu", "be\u00b7k\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vernunft und Schrift durch das zu d\u00e4mpfen,", "tokens": ["Ver\u00b7nunft", "und", "Schrift", "durch", "das", "zu", "d\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "PDS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Was beyden Lichtern Eintrag thut.", "tokens": ["Was", "bey\u00b7den", "Lich\u00b7tern", "Ein\u00b7trag", "thut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Man sch\u00e4rft des ", "tokens": ["Man", "sch\u00e4rft", "des"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Was ", "tokens": ["Was"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Wird noch verschmitzter angesponnen;", "tokens": ["Wird", "noch", "ver\u00b7schmitz\u00b7ter", "an\u00b7ge\u00b7spon\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ein neuer Firni\u00df giebt verlegner Waare Lauf.", "tokens": ["Ein", "neu\u00b7er", "Fir\u00b7ni\u00df", "giebt", "ver\u00b7leg\u00b7ner", "Waa\u00b7re", "Lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Man glaubt in ", "tokens": ["Man", "glaubt", "in"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Mehr Nachdruck, St\u00e4rk und Kraft als in der Schrift zu finden.", "tokens": ["Mehr", "Nach\u00b7druck", ",", "St\u00e4rk", "und", "Kraft", "als", "in", "der", "Schrift", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "NN", "KON", "NN", "KOKOM", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.33": {"line.1": {"text": "Die\u00df wirkte ", "tokens": ["Die\u00df", "wirk\u00b7te"], "token_info": ["word", "word"], "pos": ["PDS", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Der Glauben und Vernunft mit Zweifeln \u00fcberh\u00e4ufte,", "tokens": ["Der", "Glau\u00b7ben", "und", "Ver\u00b7nunft", "mit", "Zwei\u00b7feln", "\u00fc\u00b7berh\u00b7\u00e4uf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und sich auf lauter Blendwerk steifte,", "tokens": ["Und", "sich", "auf", "lau\u00b7ter", "Blend\u00b7werk", "steif\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Bl\u00f6den sehr ins Auge fiel.", "tokens": ["Das", "Bl\u00f6\u00b7den", "sehr", "ins", "Au\u00b7ge", "fiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der wilden Jugend rohe Brust", "tokens": ["Der", "wil\u00b7den", "Ju\u00b7gend", "ro\u00b7he", "Brust"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ergreift mit voller Lust den Scheingrund, nichts zu glauben;", "tokens": ["Er\u00b7greift", "mit", "vol\u00b7ler", "Lust", "den", "Schein\u00b7grund", ",", "nichts", "zu", "glau\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "ART", "NN", "$,", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "L\u00e4\u00dft sich Verstand und Sinne rauben", "tokens": ["L\u00e4\u00dft", "sich", "Ver\u00b7stand", "und", "Sin\u00b7ne", "rau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "NN", "KON", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und braucht der Zweifler Traum zum Vorwand arger Lust.", "tokens": ["Und", "braucht", "der", "Zweif\u00b7ler", "Traum", "zum", "Vor\u00b7wand", "ar\u00b7ger", "Lust", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPRART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kein Wunder da\u00df dergleichen Schriften,", "tokens": ["Kein", "Wun\u00b7der", "da\u00df", "derg\u00b7lei\u00b7chen", "Schrif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KOUS", "PIS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Mehr Schaden, als ", "tokens": ["Mehr", "Scha\u00b7den", ",", "als"], "token_info": ["word", "word", "punct", "word"], "pos": ["PIAT", "NN", "$,", "KOUS"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.34": {"line.1": {"text": "Wer hat nun dieser ", "tokens": ["Wer", "hat", "nun", "die\u00b7ser"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "PDAT"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Mit gl\u00fccklichem Erfolg am st\u00e4rksten widersetzet?", "tokens": ["Mit", "gl\u00fcck\u00b7li\u00b7chem", "Er\u00b7folg", "am", "st\u00e4rks\u00b7ten", "wi\u00b7der\u00b7set\u00b7zet", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wer hat sie auf den Tod verletzet,", "tokens": ["Wer", "hat", "sie", "auf", "den", "Tod", "ver\u00b7let\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df sie, wie jene ", "tokens": ["Da\u00df", "sie", ",", "wie", "je\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "$,", "PWAV", "PDS"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Viel gro\u00dfe M\u00e4nner stritten hier,", "tokens": ["Viel", "gro\u00b7\u00dfe", "M\u00e4n\u00b7ner", "strit\u00b7ten", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Glauben und Vernunft geschickt und scharf verfochten:", "tokens": ["Die", "Glau\u00b7ben", "und", "Ver\u00b7nunft", "ge\u00b7schickt", "und", "scharf", "ver\u00b7foch\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch keinem ward der Kranz geflochten;", "tokens": ["Doch", "kei\u00b7nem", "ward", "der", "Kranz", "ge\u00b7floch\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Der Sieg in diesem Kampf, geb\u00fchrt, o ", "tokens": ["Der", "Sieg", "in", "die\u00b7sem", "Kampf", ",", "ge\u00b7b\u00fchrt", ",", "o"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "$,", "VVPP", "$,", "FM"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Das Buch so man von dir gelesen,", "tokens": ["Das", "Buch", "so", "man", "von", "dir", "ge\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PIS", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ist ein Triumph der Schrift und der Vernunft gewesen.", "tokens": ["Ist", "ein", "Tri\u00b7umph", "der", "Schrift", "und", "der", "Ver\u00b7nunft", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "KON", "ART", "NN", "VAPP", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.35": {"line.1": {"text": "Tr\u00e4gt nicht der Pallas Helm dein Bild,", "tokens": ["Tr\u00e4gt", "nicht", "der", "Pal\u00b7las", "Helm", "dein", "Bild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "ART", "NN", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die unl\u00e4ngst das Panier von dem ber\u00fchmten Orden,", "tokens": ["Die", "un\u00b7l\u00e4ngst", "das", "Pa\u00b7nier", "von", "dem", "be\u00b7r\u00fchm\u00b7ten", "Or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der Wahrheitliebenden geworden,", "tokens": ["Der", "Wahr\u00b7heit\u00b7lie\u00b7ben\u00b7den", "ge\u00b7wor\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und jedes Glied mit Muth erf\u00fcllt?", "tokens": ["Und", "je\u00b7des", "Glied", "mit", "Muth", "er\u00b7f\u00fcllt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "O mehr als g\u00fcldnes Wort, das vom Horaz entsprungen,", "tokens": ["O", "mehr", "als", "g\u00fcld\u00b7nes", "Wort", ",", "das", "vom", "Ho\u00b7raz", "ent\u00b7sprun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "KOKOM", "ADJA", "NN", "$,", "PRELS", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Doch itzt noch tiefer eingedrungen,", "tokens": ["Doch", "itzt", "noch", "tie\u00b7fer", "ein\u00b7ge\u00b7drun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Seit edle Geister sich der Wahrheitliebe weihn;", "tokens": ["Seit", "ed\u00b7le", "Geis\u00b7ter", "sich", "der", "Wahr\u00b7heit\u00b7lie\u00b7be", "weihn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PRF", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Seit uns ein gro\u00dfer Graf will treiben,", "tokens": ["Seit", "uns", "ein", "gro\u00b7\u00dfer", "Graf", "will", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Mit Eifer nachzusehn, was ", "tokens": ["Mit", "Ei\u00b7fer", "nach\u00b7zu\u00b7sehn", ",", "was"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["APPR", "NN", "VVINF", "$,", "PWS"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.36": {"line.1": {"text": "Begl\u00fccktes Leipzig! sey erfreut,", "tokens": ["Be\u00b7gl\u00fcck\u00b7tes", "Leip\u00b7zig", "!", "sey", "er\u00b7freut", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$.", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df deinem Sohne nur die\u00df gro\u00dfe Werk gelungen;", "tokens": ["Da\u00df", "dei\u00b7nem", "Soh\u00b7ne", "nur", "die\u00df", "gro\u00b7\u00dfe", "Werk", "ge\u00b7lun\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PDS", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der hier ein st\u00e4rker Heer bezwungen,", "tokens": ["Der", "hier", "ein", "st\u00e4r\u00b7ker", "Heer", "be\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als der des Xerxes Macht zerstreut.", "tokens": ["Als", "der", "des", "Xe\u00b7rxes", "Macht", "zer\u00b7streut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Als ", "tokens": ["Als"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Erfocht ein tapfrer Held, nach zweener Br\u00fcder Leichen,", "tokens": ["Er\u00b7focht", "ein", "tapf\u00b7rer", "Held", ",", "nach", "zwee\u00b7ner", "Br\u00fc\u00b7der", "Lei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Vaterstadt die Siegeszeichen,", "tokens": ["Der", "Va\u00b7ter\u00b7stadt", "die", "Sie\u00b7ges\u00b7zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und Rom gewann dadurch die Oberherrschaft gar.", "tokens": ["Und", "Rom", "ge\u00b7wann", "da\u00b7durch", "die", "O\u00b7ber\u00b7herr\u00b7schaft", "gar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PAV", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch das, was ", "tokens": ["Durch", "das", ",", "was"], "token_info": ["word", "word", "punct", "word"], "pos": ["APPR", "PDS", "$,", "PWS"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "Hat Glaub und Wahrheit mehr, als vormals Rom gewonnen.", "tokens": ["Hat", "Glaub", "und", "Wahr\u00b7heit", "mehr", ",", "als", "vor\u00b7mals", "Rom", "ge\u00b7won\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "ADV", "$,", "KOUS", "ADV", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.37": {"line.1": {"text": "Sey stolz auf deines B\u00fcrgers Preis!", "tokens": ["Sey", "stolz", "auf", "dei\u00b7nes", "B\u00fcr\u00b7gers", "Preis", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ber\u00fchmtes Plei\u00dfathen, sey stolz auf seine Werke!", "tokens": ["Be\u00b7r\u00fchm\u00b7tes", "Plei\u00b7\u00df\u00b7a\u00b7then", ",", "sey", "stolz", "auf", "sei\u00b7ne", "Wer\u00b7ke", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VAFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Weil seines Kiels bew\u00e4hrte St\u00e4rke", "tokens": ["Weil", "sei\u00b7nes", "Kiels", "be\u00b7w\u00e4hr\u00b7te", "St\u00e4r\u00b7ke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Kaum irgend ihres gleichen weis.", "tokens": ["Kaum", "ir\u00b7gend", "ih\u00b7res", "glei\u00b7chen", "weis", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPOSAT", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "La\u00df dieses Jahr dir heilig seyn,", "tokens": ["La\u00df", "die\u00b7ses", "Jahr", "dir", "hei\u00b7lig", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PDAT", "NN", "PPER", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das hundertste nach dem, daran du den gebohren,", "tokens": ["Das", "hun\u00b7derts\u00b7te", "nach", "dem", ",", "da\u00b7ran", "du", "den", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "APPR", "ART", "$,", "PAV", "PPER", "ART", "VVPP", "$,"], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Den selbst die Vorsicht auserkohren,", "tokens": ["Den", "selbst", "die", "Vor\u00b7sicht", "au\u00b7ser\u00b7koh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zu ihrer Rechte Schutz, Verstand und Kiel zu weihn.", "tokens": ["Zu", "ih\u00b7rer", "Rech\u00b7te", "Schutz", ",", "Ver\u00b7stand", "und", "Kiel", "zu", "weihn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "$,", "NN", "KON", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Sey stolz, und la\u00df in deinen Mauren", "tokens": ["Sey", "stolz", ",", "und", "la\u00df", "in", "dei\u00b7nen", "Mau\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAIMP", "ADJD", "$,", "KON", "VVIMP", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ein Denkmaal deiner Pflicht aus Dank und Ehrfurcht dauren.", "tokens": ["Ein", "Denk\u00b7maal", "dei\u00b7ner", "Pflicht", "aus", "Dank", "und", "Ehr\u00b7furcht", "dau\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.38": {"line.1": {"text": "Dir fehlts gewi\u00df an Marmor nicht,", "tokens": ["Dir", "fehlts", "ge\u00b7wi\u00df", "an", "Mar\u00b7mor", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sonst Athen gethan, die Weisen zu verehren:", "tokens": ["Wie", "sonst", "A\u00b7then", "ge\u00b7than", ",", "die", "Wei\u00b7sen", "zu", "ver\u00b7eh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "NE", "VVPP", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Versuchs an dem, von dessen Lehren", "tokens": ["Ver\u00b7suchs", "an", "dem", ",", "von", "des\u00b7sen", "Leh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "$,", "APPR", "PRELAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Wahrheit dir viel Glanz verspricht.", "tokens": ["Die", "Wahr\u00b7heit", "dir", "viel", "Glanz", "ver\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie kr\u00e4ftig wird sein Ehrenbild", "tokens": ["Wie", "kr\u00e4f\u00b7tig", "wird", "sein", "Eh\u00b7ren\u00b7bild"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "In deiner S\u00f6hne Brust den Weisheittrieb erhitzen!", "tokens": ["In", "dei\u00b7ner", "S\u00f6h\u00b7ne", "Brust", "den", "Weis\u00b7heit\u00b7trieb", "er\u00b7hit\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wie mancher Kopf wird dir noch n\u00fctzen.", "tokens": ["Wie", "man\u00b7cher", "Kopf", "wird", "dir", "noch", "n\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den ", "tokens": ["Den"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.9": {"text": "Du selber wirst dadurch auf Erden,", "tokens": ["Du", "sel\u00b7ber", "wirst", "da\u00b7durch", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "PAV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "In aller V\u00f6lker Mund der Weisheit Mutter werden.", "tokens": ["In", "al\u00b7ler", "V\u00f6l\u00b7ker", "Mund", "der", "Weis\u00b7heit", "Mut\u00b7ter", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "ART", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.39": {"line.1": {"text": "Nicht seit zwey Jahren schon auch Deinen Schutz erlanget?", "tokens": ["Nicht", "seit", "zwey", "Jah\u00b7ren", "schon", "auch", "Dei\u00b7nen", "Schutz", "er\u00b7lan\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "CARD", "NN", "ADV", "ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Seit es mit Deinem Namen pranget,", "tokens": ["Seit", "es", "mit", "Dei\u00b7nem", "Na\u00b7men", "pran\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ward es der kl\u00fcgsten Augenmerk.", "tokens": ["Ward", "es", "der", "kl\u00fcgs\u00b7ten", "Au\u00b7gen\u00b7merk", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Gnad und Huld so ich empfand,", "tokens": ["Die", "Gnad", "und", "Huld", "so", "ich", "emp\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat Leibnitz zehnfach mehr, als mein Bem\u00fchn verdienet:", "tokens": ["Hat", "Leib\u00b7nitz", "zehn\u00b7fach", "mehr", ",", "als", "mein", "Be\u00b7m\u00fchn", "ver\u00b7die\u00b7net", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "VVFIN", "ADV", "$,", "KOUS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Drum hab ich mir die\u00df Lob erk\u00fchnet,", "tokens": ["Drum", "hab", "ich", "mir", "die\u00df", "Lob", "er\u00b7k\u00fch\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPER", "PDS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "O tr\u00e4f ein gleiches Gl\u00fcck des Dichters Gegenstand?", "tokens": ["O", "tr\u00e4f", "ein", "glei\u00b7ches", "Gl\u00fcck", "des", "Dich\u00b7ters", "Ge\u00b7gen\u00b7stand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "ADJA", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So w\u00fcrd einmal die Nachwelt lesen,", "tokens": ["So", "w\u00fcrd", "ein\u00b7mal", "die", "Nach\u00b7welt", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Wie hold ", "tokens": ["Wie", "hold"], "token_info": ["word", "word"], "pos": ["PWAV", "ADJD"], "meter": "-+", "measure": "iambic.single"}}, "stanza.40": {"line.1": {"text": "Die Welt erkennts, ", "tokens": ["Die", "Welt", "er\u00b7kennts", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Wie sanft das Musenvolk bey Sachsens Schwertern sitzet;", "tokens": ["Wie", "sanft", "das", "Mu\u00b7sen\u00b7volk", "bey", "Sach\u00b7sens", "Schwer\u00b7tern", "sit\u00b7zet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ART", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wenn Mars gleich auch allhier geblitzet,", "tokens": ["Wenn", "Mars", "gleich", "auch", "all\u00b7hier", "ge\u00b7blit\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "ADV", "ADV", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und selbst dem Pindus schrecklich war.", "tokens": ["Und", "selbst", "dem", "Pin\u00b7dus", "schreck\u00b7lich", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Des Himmels Schild beschirm forthin", "tokens": ["Des", "Him\u00b7mels", "Schild", "be\u00b7schirm", "for\u00b7thin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Sein Wohlstand wird uns allen eigen;", "tokens": ["Sein", "Wohl\u00b7stand", "wird", "uns", "al\u00b7len", "ei\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ihr unverr\u00fcckter Flor ist unsers Chors Gewinn.", "tokens": ["Ihr", "un\u00b7ver\u00b7r\u00fcck\u00b7ter", "Flor", "ist", "un\u00b7sers", "Chors", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.41": {"line.1": {"text": "Wo kann das Wissen sch\u00f6ner bl\u00fchen,", "tokens": ["Wo", "kann", "das", "Wis\u00b7sen", "sch\u00f6\u00b7ner", "bl\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als wo die F\u00fcrsten selbst sich um sein Wohl bem\u00fchen?", "tokens": ["Als", "wo", "die", "F\u00fcrs\u00b7ten", "selbst", "sich", "um", "sein", "Wohl", "be\u00b7m\u00fc\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "ART", "NN", "ADV", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.42": {"line.1": {"text": "O Geist der Weisheit! dessen ZugDen Sinn der Sterblichen von wilder Thiere Toben,", "tokens": ["O", "Geist", "der", "Weis\u00b7heit", "!", "des\u00b7sen", "Zug", "Den", "Sinn", "der", "Sterb\u00b7li\u00b7chen", "von", "wil\u00b7der", "Thie\u00b7re", "To\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ART", "NN", "$.", "PRELAT", "NN", "ART", "NN", "ART", "NN", "APPR", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.2": {"text": "Zur Einsicht und Vernunft erhoben,", "tokens": ["Zur", "Ein\u00b7sicht", "und", "Ver\u00b7nunft", "er\u00b7ho\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Wahn und Einfalt niederschlug.", "tokens": ["Die", "Wahn", "und", "Ein\u00b7falt", "nie\u00b7der\u00b7schlug", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Du Geist der Wissenschaft und Kunst!", "tokens": ["Du", "Geist", "der", "Wis\u00b7sen\u00b7schaft", "und", "Kunst", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der durch ein h\u00f6her Licht die Barberey gest\u00f6ret,", "tokens": ["Der", "durch", "ein", "h\u00f6\u00b7her", "Licht", "die", "Bar\u00b7be\u00b7rey", "ge\u00b7st\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und Menschen Menschen seyn gelehret;", "tokens": ["Und", "Men\u00b7schen", "Men\u00b7schen", "seyn", "ge\u00b7leh\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Belebe mich vorjetzt mit deines Triebes Gunst,", "tokens": ["Be\u00b7le\u00b7be", "mich", "vor\u00b7jetzt", "mit", "dei\u00b7nes", "Trie\u00b7bes", "Gunst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und la\u00df es die\u00dfmal mir gelingen", "tokens": ["Und", "la\u00df", "es", "die\u00df\u00b7mal", "mir", "ge\u00b7lin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "PPER", "ADV", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Von deinem Heiligthum und liebsten Sohn zu singen.", "tokens": ["Von", "dei\u00b7nem", "Hei\u00b7lig\u00b7thum", "und", "liebs\u00b7ten", "Sohn", "zu", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.43": {"line.1": {"text": "Es h\u00f6rt mich ein ", "tokens": ["Es", "h\u00f6rt", "mich", "ein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Des ", "tokens": ["Des"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Das l\u00e4ngst den K\u00fcnsten gn\u00e4dig war.", "tokens": ["Das", "l\u00e4ngst", "den", "K\u00fcns\u00b7ten", "gn\u00e4\u00b7dig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ihr heitres Antlitz st\u00e4rkt die Kraft", "tokens": ["Ihr", "heit\u00b7res", "Ant\u00b7litz", "st\u00e4rkt", "die", "Kraft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Der Musen, die sonst leicht bey Furcht und Gram erliegen:", "tokens": ["Der", "Mu\u00b7sen", ",", "die", "sonst", "leicht", "bey", "Furcht", "und", "Gram", "er\u00b7lie\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ADJD", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "In dem was andre schreckt, in Kunst und Wissenschaft.", "tokens": ["In", "dem", "was", "and\u00b7re", "schreckt", ",", "in", "Kunst", "und", "Wis\u00b7sen\u00b7schaft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PWS", "PIS", "VVFIN", "$,", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "O m\u00f6cht ein Stral von Ihren Blicken,", "tokens": ["O", "m\u00f6cht", "ein", "Stral", "von", "Ih\u00b7ren", "Bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Nach oft gesp\u00fcrter Huld, mich selber mir entr\u00fccken!", "tokens": ["Nach", "oft", "ge\u00b7sp\u00fcr\u00b7ter", "Huld", ",", "mich", "sel\u00b7ber", "mir", "ent\u00b7r\u00fc\u00b7cken", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJA", "NN", "$,", "PPER", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.44": {"line.1": {"text": "Da, wo der Plei\u00dfe feuchter Rand,", "tokens": ["Da", ",", "wo", "der", "Plei\u00b7\u00dfe", "feuch\u00b7ter", "Rand", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die fette Mei\u00dfnerflur mit sanfter Fluth erfrischet,", "tokens": ["Die", "fet\u00b7te", "Mei\u00df\u00b7ner\u00b7flur", "mit", "sanf\u00b7ter", "Fluth", "er\u00b7fri\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da wo sie sich mit Wellen mischet,", "tokens": ["Da", "wo", "sie", "sich", "mit", "Wel\u00b7len", "mi\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWAV", "PPER", "PRF", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die ihr die Baare zugesandt;", "tokens": ["Die", "ihr", "die", "Baa\u00b7re", "zu\u00b7ge\u00b7sandt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wo sonst ein slavisches Geschlecht,", "tokens": ["Wo", "sonst", "ein", "sla\u00b7vi\u00b7sches", "Ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Ja bis in Th\u00fcringen gedrungen,", "tokens": ["Ja", "bis", "in", "Th\u00fc\u00b7rin\u00b7gen", "ge\u00b7drun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "KON", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "Bis ihn der gro\u00dfe Karl durch Tapferkeit geschw\u00e4cht:", "tokens": ["Bis", "ihn", "der", "gro\u00b7\u00dfe", "Karl", "durch", "Tap\u00b7fer\u00b7keit", "ge\u00b7schw\u00e4cht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NE", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "In wilden ", "tokens": ["In", "wil\u00b7den"], "token_info": ["word", "word"], "pos": ["APPR", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Ist ", "tokens": ["Ist"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "+", "measure": "single.up"}}, "stanza.45": {"line.1": {"text": "Wer will im dunkeln Alterthum", "tokens": ["Wer", "will", "im", "dun\u00b7keln", "Al\u00b7ter\u00b7thum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der gr\u00f6\u00dften St\u00e4dte Grund und Stiftung recht erfahren?", "tokens": ["Der", "gr\u00f6\u00df\u00b7ten", "St\u00e4d\u00b7te", "Grund", "und", "Stif\u00b7tung", "recht", "er\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "KON", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wuchs doch ", "tokens": ["Wuchs", "doch"], "token_info": ["word", "word"], "pos": ["NN", "ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Zu dem erlangten Flor und Ruhm.", "tokens": ["Zu", "dem", "er\u00b7lang\u00b7ten", "Flor", "und", "Ruhm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der Berge Moo\u00df und tiefer Schacht,", "tokens": ["Der", "Ber\u00b7ge", "Moo\u00df", "und", "tie\u00b7fer", "Schacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Versteckt den ersten Keim, die Wurzeln junger Eichen;", "tokens": ["Ver\u00b7steckt", "den", "ers\u00b7ten", "Keim", ",", "die", "Wur\u00b7zeln", "jun\u00b7ger", "Ei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$,", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch wenn sie an die Wolken reichen,", "tokens": ["Doch", "wenn", "sie", "an", "die", "Wol\u00b7ken", "rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Erstaunt ein Wandersmann vor ihrer Zweige Pracht.", "tokens": ["Er\u00b7staunt", "ein", "Wan\u00b7ders\u00b7mann", "vor", "ih\u00b7rer", "Zwei\u00b7ge", "Pracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kein Wunder, wenn wir gleichfalls lesen,", "tokens": ["Kein", "Wun\u00b7der", ",", "wenn", "wir", "gleich\u00b7falls", "le\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}}, "stanza.46": {"line.1": {"text": "Kein Schimpf f\u00fcr dich, ber\u00fchmte Stadt!", "tokens": ["Kein", "Schimpf", "f\u00fcr", "dich", ",", "be\u00b7r\u00fchm\u00b7te", "Stadt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "PPER", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Vorsicht hatte dich schon damals ausersehen", "tokens": ["Die", "Vor\u00b7sicht", "hat\u00b7te", "dich", "schon", "da\u00b7mals", "au\u00b7ser\u00b7se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Zu allem was hernach geschehen,", "tokens": ["Zu", "al\u00b7lem", "was", "her\u00b7nach", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "PWS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dich empor gehoben hat.", "tokens": ["Und", "dich", "em\u00b7por", "ge\u00b7ho\u00b7ben", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PTKVZ", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So weit der Saal und Muldenflu\u00df,", "tokens": ["So", "weit", "der", "Saal", "und", "Mul\u00b7den\u00b7flu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So weit die Elster sich in krummen Ufern schleichet,", "tokens": ["So", "weit", "die", "Els\u00b7ter", "sich", "in", "krum\u00b7men", "U\u00b7fern", "schlei\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "PRF", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Bl\u00fcht keine Stadt die dir nicht weichet,", "tokens": ["Bl\u00fcht", "kei\u00b7ne", "Stadt", "die", "dir", "nicht", "wei\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ART", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Dir nicht in Demuth selbst den Vorzug geben mu\u00df.", "tokens": ["Dir", "nicht", "in", "De\u00b7muth", "selbst", "den", "Vor\u00b7zug", "ge\u00b7ben", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "APPR", "NN", "ADV", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So hoch hast du durch tausend Proben,", "tokens": ["So", "hoch", "hast", "du", "durch", "tau\u00b7send", "Pro\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Von Witz und Wissenschaft und Handel dich erhoben!", "tokens": ["Von", "Witz", "und", "Wis\u00b7sen\u00b7schaft", "und", "Han\u00b7del", "dich", "er\u00b7ho\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.47": {"line.1": {"text": "Wodurch ", "tokens": ["Wo\u00b7durch"], "token_info": ["word"], "pos": ["PWAV"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "An Reichthum ", "tokens": ["An", "Reicht\u00b7hum"], "token_info": ["word", "word"], "pos": ["APPR", "NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Dadurch kannst du, o ", "tokens": ["Da\u00b7durch", "kannst", "du", ",", "o"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PAV", "VMFIN", "PPER", "$,", "FM"], "meter": "--+-+", "measure": "anapaest.init"}, "line.4": {"text": "Das alles gr\u00fcndet auch dein Lob!", "tokens": ["Das", "al\u00b7les", "gr\u00fcn\u00b7det", "auch", "dein", "Lob", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat sich im ", "tokens": ["Hat", "sich", "im"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PRF", "APPRART"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Den Preis der sch\u00f6nsten Stadt erstritten;", "tokens": ["Den", "Preis", "der", "sch\u00f6ns\u00b7ten", "Stadt", "er\u00b7strit\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Stammt ", "tokens": ["Stammt"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Was Wunder? da\u00df auch deine Mauren", "tokens": ["Was", "Wun\u00b7der", "?", "da\u00df", "auch", "dei\u00b7ne", "Mau\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "NN", "$.", "KOUS", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Durch kluger B\u00fcrger Flei\u00df, erwachsen, stehn und dauren.", "tokens": ["Durch", "klu\u00b7ger", "B\u00fcr\u00b7ger", "Flei\u00df", ",", "er\u00b7wach\u00b7sen", ",", "stehn", "und", "dau\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "$,", "VVPP", "$,", "VVFIN", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.48": {"line.1": {"text": "Zinst dir kein weiter ", "tokens": ["Zinst", "dir", "kein", "wei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PPER", "PIAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Kein tief und breiter Strom durch Segel, Flagg und Masten,", "tokens": ["Kein", "tief", "und", "brei\u00b7ter", "Strom", "durch", "Se\u00b7gel", ",", "Flagg", "und", "Mas\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "KON", "ADJA", "NN", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.4": {"text": "Und Kostbarkeiten aus ", "tokens": ["Und", "Kost\u00b7bar\u00b7kei\u00b7ten", "aus"], "token_info": ["word", "word", "word"], "pos": ["KON", "NN", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Siehst du hier keine Wimpel wehn,", "tokens": ["Siehst", "du", "hier", "kei\u00b7ne", "Wim\u00b7pel", "wehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und sinkt kein Anker gleich in deinem Hafen nieder;", "tokens": ["Und", "sinkt", "kein", "An\u00b7ker", "gleich", "in", "dei\u00b7nem", "Ha\u00b7fen", "nie\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "ADV", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ja l\u00e4\u00dft dein Flu\u00df gleich hin und wieder,", "tokens": ["Ja", "l\u00e4\u00dft", "dein", "Flu\u00df", "gleich", "hin", "und", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VVFIN", "PPOSAT", "NN", "ADV", "PTKVZ", "KON", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kaum einen schmalen Kahn bey zwanzig M\u00fchlen sehn:", "tokens": ["Kaum", "ei\u00b7nen", "schma\u00b7len", "Kahn", "bey", "zwan\u00b7zig", "M\u00fch\u00b7len", "sehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So ward dir doch ", "tokens": ["So", "ward", "dir", "doch"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Denn Kunst und Witz ersetzt, was die Natur entzogen.", "tokens": ["Denn", "Kunst", "und", "Witz", "er\u00b7setzt", ",", "was", "die", "Na\u00b7tur", "ent\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "VVPP", "$,", "PRELS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ihr Pl\u00e4tze! die der Stifter Witz,", "tokens": ["Ihr", "Pl\u00e4t\u00b7ze", "!", "die", "der", "Stif\u00b7ter", "Witz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "ART", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Vieleicht der Zufall blo\u00df, an Strom und See gebauet;", "tokens": ["Vie\u00b7leicht", "der", "Zu\u00b7fall", "blo\u00df", ",", "an", "Strom", "und", "See", "ge\u00b7bau\u00b7et", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "$,", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wo ihr in stolzer N\u00e4he schauet", "tokens": ["Wo", "ihr", "in", "stol\u00b7zer", "N\u00e4\u00b7he", "schau\u00b7et"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Seyd nicht zu frech auf euer Gl\u00fcck!", "tokens": ["Seyd", "nicht", "zu", "frech", "auf", "eu\u00b7er", "Gl\u00fcck", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PTKNEG", "PTKA", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Das Meer scheint freylich euch den Reichthum aufzuth\u00fcrmen:", "tokens": ["Das", "Meer", "scheint", "frey\u00b7lich", "euch", "den", "Reicht\u00b7hum", "auf\u00b7zut\u00b7h\u00fcr\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Doch \u00f6fters schreckt es auch mit St\u00fcrmen,", "tokens": ["Doch", "\u00f6f\u00b7ters", "schreckt", "es", "auch", "mit", "St\u00fcr\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Und schickt die Flotten krank, zerlechzt und leer zur\u00fcck.", "tokens": ["Und", "schickt", "die", "Flot\u00b7ten", "krank", ",", "zer\u00b7lechzt", "und", "leer", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADJD", "$,", "VVFIN", "KON", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Wo nicht der Schatz von vielen Jahren,", "tokens": ["Wo", "nicht", "der", "Schatz", "von", "vie\u00b7len", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Durch ein zerscheitert Schiff dem Abgrund zugefahren.", "tokens": ["Durch", "ein", "zer\u00b7schei\u00b7tert", "Schiff", "dem", "Ab\u00b7grund", "zu\u00b7ge\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VVPP", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.49": {"line.1": {"text": "Das alles schrecket ", "tokens": ["Das", "al\u00b7les", "schre\u00b7cket"], "token_info": ["word", "word", "word"], "pos": ["PDS", "PIS", "VVFIN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Das seine Frachten nicht den Wellen anvertrauet;", "tokens": ["Das", "sei\u00b7ne", "Frach\u00b7ten", "nicht", "den", "Wel\u00b7len", "an\u00b7ver\u00b7trau\u00b7et", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dem nie vor Sturm und Wetter grauet,", "tokens": ["Dem", "nie", "vor", "Sturm", "und", "Wet\u00b7ter", "grau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Davon oft Mast und Ruder bricht.", "tokens": ["Da\u00b7von", "oft", "Mast", "und", "Ru\u00b7der", "bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hier bebt kein Mensch vor Syrt und Strand,", "tokens": ["Hier", "bebt", "kein", "Mensch", "vor", "Syrt", "und", "Strand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kein ", "tokens": ["Kein"], "token_info": ["word"], "pos": ["PIAT"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Er liegt in unbesorgtem Schlummer,", "tokens": ["Er", "liegt", "in", "un\u00b7be\u00b7sorg\u00b7tem", "Schlum\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die G\u00fcter, die er hofft, bringt ihm das sichre Land.", "tokens": ["Die", "G\u00fc\u00b7ter", ",", "die", "er", "hofft", ",", "bringt", "ihm", "das", "sich\u00b7re", "Land", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Bey zehnfach leidlichern Gefahren,", "tokens": ["Bey", "zehn\u00b7fach", "leid\u00b7li\u00b7chern", "Ge\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Versorgt der Rosse Kraft ihn mit den sch\u00f6nsten Waaren.", "tokens": ["Ver\u00b7sorgt", "der", "Ros\u00b7se", "Kraft", "ihn", "mit", "den", "sch\u00f6ns\u00b7ten", "Waa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "PPER", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.50": {"line.1": {"text": "Wie sich bey voller Fr\u00fchlingszeit", "tokens": ["Wie", "sich", "bey", "vol\u00b7ler", "Fr\u00fch\u00b7lings\u00b7zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein arbeitsamer Stock voll junger Bienen reget;", "tokens": ["Ein", "ar\u00b7beit\u00b7sa\u00b7mer", "Stock", "voll", "jun\u00b7ger", "Bie\u00b7nen", "re\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie alles sich vor Flei\u00df beweget,", "tokens": ["Wie", "al\u00b7les", "sich", "vor", "Flei\u00df", "be\u00b7we\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn Sonn und Luft die Kraft verleiht:", "tokens": ["Wenn", "Sonn", "und", "Luft", "die", "Kraft", "ver\u00b7leiht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die\u00df muntre Volk durchfliegt das Feld,", "tokens": ["Die\u00df", "mun\u00b7tre", "Volk", "durch\u00b7fliegt", "das", "Feld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und k\u00f6mmt durchaus beschwert mit s\u00fc\u00dfer Beute wieder;", "tokens": ["Und", "k\u00f6mmt", "durc\u00b7haus", "be\u00b7schwert", "mit", "s\u00fc\u00b7\u00dfer", "Beu\u00b7te", "wie\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "VVFIN", "APPR", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Es legt der Blumen Balsam nieder,", "tokens": ["Es", "legt", "der", "Blu\u00b7men", "Bal\u00b7sam", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und f\u00fcllt die Zellen an, die es dazu bestellt:", "tokens": ["Und", "f\u00fcllt", "die", "Zel\u00b7len", "an", ",", "die", "es", "da\u00b7zu", "be\u00b7stellt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$,", "PRELS", "PPER", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "So pflegen ", "tokens": ["So", "pfle\u00b7gen"], "token_info": ["word", "word"], "pos": ["ADV", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Dreymal im Jahre sich besch\u00e4fftigt sehn zu lassen.", "tokens": ["Drey\u00b7mal", "im", "Jah\u00b7re", "sich", "be\u00b7sch\u00e4ff\u00b7tigt", "sehn", "zu", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PRF", "ADJD", "VVINF", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.11": {"text": "Wo bin ich? zeigt sich ", "tokens": ["Wo", "bin", "ich", "?", "zeigt", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "$.", "VVFIN", "PRF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.12": {"text": "Seh ich ", "tokens": ["Seh", "ich"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "Ja! ", "tokens": ["Ja", "!"], "token_info": ["word", "punct"], "pos": ["PTKANT", "$."], "meter": "+", "measure": "single.up"}, "line.14": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Die aus dem ", "tokens": ["Die", "aus", "dem"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.16": {"text": "Erscheinen auf der Kr\u00e4mer Winken;", "tokens": ["Er\u00b7schei\u00b7nen", "auf", "der", "Kr\u00e4\u00b7mer", "Win\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Ihr weiter Wagen wird von tausend Lasten schwer.", "tokens": ["Ihr", "wei\u00b7ter", "Wa\u00b7gen", "wird", "von", "tau\u00b7send", "Las\u00b7ten", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "APPR", "CARD", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.19": {"text": "Ja Donau, Rhein und Mayn, sind Leipzig zinsbar worden.", "tokens": ["Ja", "Do\u00b7nau", ",", "Rhein", "und", "Mayn", ",", "sind", "Leip\u00b7zig", "zins\u00b7bar", "wor\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "NE", "$,", "NE", "KON", "NN", "$,", "VAFIN", "NE", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.51": {"line.1": {"text": "Noch mehr! auch Weisheit steht hier feil,", "tokens": ["Noch", "mehr", "!", "auch", "Weis\u00b7heit", "steht", "hier", "feil", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$.", "ADV", "NN", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Was ihrer Priester wacher Flei\u00df,", "tokens": ["Was", "ih\u00b7rer", "Pries\u00b7ter", "wa\u00b7cher", "Flei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So weit Europa geht, ersonnen und geschrieben,", "tokens": ["So", "weit", "Eu\u00b7ro\u00b7pa", "geht", ",", "er\u00b7son\u00b7nen", "und", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NE", "VVFIN", "$,", "ADJA", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das alles wird hieher getrieben,", "tokens": ["Das", "al\u00b7les", "wird", "hie\u00b7her", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wo kluger K\u00e4ufer Blick es auszusp\u00e4hen weis.", "tokens": ["Wo", "klu\u00b7ger", "K\u00e4u\u00b7fer", "Blick", "es", "aus\u00b7zu\u00b7sp\u00e4\u00b7hen", "weis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "NN", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der W\u00e4lschen Geist, der Franzen K\u00fcnste,", "tokens": ["Der", "W\u00e4l\u00b7schen", "Geist", ",", "der", "Fran\u00b7zen", "K\u00fcns\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Der Britten tiefer Sinn, dient Leipzig zum Gewinnste.", "tokens": ["Der", "Brit\u00b7ten", "tie\u00b7fer", "Sinn", ",", "dient", "Leip\u00b7zig", "zum", "Ge\u00b7winns\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "VVFIN", "NE", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.52": {"line.1": {"text": "Was sag ich? Salems Wissenschaft,", "tokens": ["Was", "sag", "ich", "?", "Sa\u00b7lems", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$.", "NE", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ph\u00f6niciens Verstand, Aegyptens Wunderwerke,", "tokens": ["Ph\u00f6\u00b7ni\u00b7ci\u00b7ens", "Ver\u00b7stand", ",", "A\u00b7e\u00b7gyp\u00b7tens", "Wun\u00b7der\u00b7wer\u00b7ke", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "NE", "NN", "$,"], "meter": "+-+--+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Erblickt man hier in voller St\u00e4rke,", "tokens": ["Er\u00b7blickt", "man", "hier", "in", "vol\u00b7ler", "St\u00e4r\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit j\u00e4hrlich neu verj\u00fcngter Kraft.", "tokens": ["Mit", "j\u00e4hr\u00b7lich", "neu", "ver\u00b7j\u00fcng\u00b7ter", "Kraft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Was sonst Ionien erfand,", "tokens": ["Was", "sonst", "I\u00b7o\u00b7ni\u00b7en", "er\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Arabien getr\u00e4umt, und Indien gelehret,", "tokens": ["A\u00b7ra\u00b7bi\u00b7en", "ge\u00b7tr\u00e4umt", ",", "und", "In\u00b7di\u00b7en", "ge\u00b7leh\u00b7ret", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$,", "KON", "NE", "VVPP", "$,"], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "Was Peking vom Confuz geh\u00f6ret,", "tokens": ["Was", "Pe\u00b7king", "vom", "Con\u00b7fuz", "ge\u00b7h\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "APPRART", "NN", "VVFIN", "$,"], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Der Perser Sonnendienst, und der Mogollen Tand;", "tokens": ["Der", "Per\u00b7ser", "Son\u00b7nen\u00b7dienst", ",", "und", "der", "Mo\u00b7gol\u00b7len", "Tand", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "KON", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Womit sich Mandarinen \u00e4ffen,", "tokens": ["Wo\u00b7mit", "sich", "Man\u00b7da\u00b7ri\u00b7nen", "\u00e4f\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und B\u00fccher aus Byzanz, die sind hier anzutreffen.", "tokens": ["Und", "B\u00fc\u00b7cher", "aus", "By\u00b7zanz", ",", "die", "sind", "hier", "an\u00b7zu\u00b7tref\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$,", "PRELS", "VAFIN", "ADV", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wo bleibt Athens Vernunft und Geist?", "tokens": ["Wo", "bleibt", "A\u00b7thens", "Ver\u00b7nunft", "und", "Geist", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "NE", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Bew\u00e4hrter Dichter Witz, der Redner Zauberworte;", "tokens": ["Be\u00b7w\u00e4hr\u00b7ter", "Dich\u00b7ter", "Witz", ",", "der", "Red\u00b7ner", "Zau\u00b7ber\u00b7wor\u00b7te", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Davon die Kraft an diesem Orte,", "tokens": ["Da\u00b7von", "die", "Kraft", "an", "die\u00b7sem", "Or\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Sich \u00f6fters noch lebendig weist.", "tokens": ["Sich", "\u00f6f\u00b7ters", "noch", "le\u00b7ben\u00b7dig", "weist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+++-+", "measure": "unknown.measure.penta"}, "line.15": {"text": "Wo bleibt der alten Weisen Mund;", "tokens": ["Wo", "bleibt", "der", "al\u00b7ten", "Wei\u00b7sen", "Mund", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Was Sokrates gelehrt, was Plato aufgeschrieben;", "tokens": ["Was", "Sok\u00b7ra\u00b7tes", "ge\u00b7lehrt", ",", "was", "Pla\u00b7to", "auf\u00b7ge\u00b7schrie\u00b7ben", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VVPP", "$,", "PRELS", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Was uns vom Zeno noch geblieben;", "tokens": ["Was", "uns", "vom", "Ze\u00b7no", "noch", "ge\u00b7blie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPRART", "NE", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Was jener Stagirit, und Theophrast verstund;", "tokens": ["Was", "je\u00b7ner", "Sta\u00b7gi\u00b7rit", ",", "und", "Theo\u00b7ph\u00b7rast", "ver\u00b7stund", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "NN", "$,", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Was Rom im Tullius gebohren,", "tokens": ["Was", "Rom", "im", "Tul\u00b7li\u00b7us", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Am Antonin verehrt, im Seneca verlohren?", "tokens": ["Am", "An\u00b7to\u00b7nin", "ver\u00b7ehrt", ",", "im", "Se\u00b7ne\u00b7ca", "ver\u00b7loh\u00b7ren", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$,", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.53": {"line.1": {"text": "Das alles, und was Flaccus war,", "tokens": ["Das", "al\u00b7les", ",", "und", "was", "Flac\u00b7cus", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "$,", "KON", "PWS", "NE", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was Maro und Ovid und Livius gewesen,", "tokens": ["Was", "Ma\u00b7ro", "und", "O\u00b7vid", "und", "Li\u00b7vius", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "KON", "NE", "KON", "NE", "VAPP", "$,"], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das bl\u00fcht allhier, das h\u00f6rt man lesen,", "tokens": ["Das", "bl\u00fcht", "all\u00b7hier", ",", "das", "h\u00f6rt", "man", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "$,", "PDS", "VVFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das stellt uns Leipzig sch\u00f6ner dar.", "tokens": ["Das", "stellt", "uns", "Leip\u00b7zig", "sch\u00f6\u00b7ner", "dar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "NE", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der B\u00fcchers\u00e4le gro\u00dfe Zahl", "tokens": ["Der", "B\u00fc\u00b7cher\u00b7s\u00e4\u00b7le", "gro\u00b7\u00dfe", "Zahl"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hebt Seltenheiten auf, die in verflo\u00dfnen Jahren,", "tokens": ["Hebt", "Sel\u00b7ten\u00b7hei\u00b7ten", "auf", ",", "die", "in", "ver\u00b7flo\u00df\u00b7nen", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PTKVZ", "$,", "PRELS", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Bey fernen V\u00f6lkern heilig waren;", "tokens": ["Bey", "fer\u00b7nen", "V\u00f6l\u00b7kern", "hei\u00b7lig", "wa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Besonders von Geschmack, und ungemein an Wahl.", "tokens": ["Be\u00b7son\u00b7ders", "von", "Ge\u00b7schmack", ",", "und", "un\u00b7ge\u00b7mein", "an", "Wahl", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$,", "KON", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Hier leben gro\u00dfer K\u00fcnstler Werke,", "tokens": ["Hier", "le\u00b7ben", "gro\u00b7\u00dfer", "K\u00fcnst\u00b7ler", "Wer\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ja Sachsens F\u00fcrsten selbst, in Bildern voller St\u00e4rke.", "tokens": ["Ja", "Sach\u00b7sens", "F\u00fcrs\u00b7ten", "selbst", ",", "in", "Bil\u00b7dern", "vol\u00b7ler", "St\u00e4r\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "NN", "NN", "ADV", "$,", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.54": {"line.1": {"text": "Verkl\u00e4rter ", "tokens": ["Ver\u00b7kl\u00e4r\u00b7ter"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Der Du den Musensitz am Plei\u00dfenstrom erbauet,", "tokens": ["Der", "Du", "den", "Mu\u00b7sen\u00b7sitz", "am", "Plei\u00b7\u00dfen\u00b7strom", "er\u00b7bau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Auch Dein Gem\u00e4ld wird hier geschauet,", "tokens": ["Auch", "Dein", "Ge\u00b7m\u00e4ld", "wird", "hier", "ge\u00b7schau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wo es die Ehrfurcht aufgestellt.", "tokens": ["Wo", "es", "die", "Ehr\u00b7furcht", "auf\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dir weis es Leipzig ewig Dank,", "tokens": ["Dir", "weis", "es", "Leip\u00b7zig", "e\u00b7wig", "Dank", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "PPER", "NE", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da\u00df Du der Wissenschaft den Aufenthalt gegr\u00fcndet:", "tokens": ["Da\u00df", "Du", "der", "Wis\u00b7sen\u00b7schaft", "den", "Auf\u00b7ent\u00b7halt", "ge\u00b7gr\u00fcn\u00b7det", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So lange sich der Witz hier findet,", "tokens": ["So", "lan\u00b7ge", "sich", "der", "Witz", "hier", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PRF", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Verehrt, o Churf\u00fcrst! Dich der Musen Lobgesang.", "tokens": ["Ver\u00b7ehrt", ",", "o", "Chur\u00b7f\u00fcrst", "!", "Dich", "der", "Mu\u00b7sen", "Lob\u00b7ge\u00b7sang", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "FM", "FM", "$.", "PPER", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du warest streitbar in den Kriegen;", "tokens": ["Du", "wa\u00b7rest", "streit\u00b7bar", "in", "den", "Krie\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und gleichwohl ist durch Dich die Wissenschaft gestiegen.", "tokens": ["Und", "gleich\u00b7wohl", "ist", "durch", "Dich", "die", "Wis\u00b7sen\u00b7schaft", "ge\u00b7stie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "APPR", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.55": {"line.1": {"text": "Dir folgt der Helden ganze Reih,", "tokens": ["Dir", "folgt", "der", "Hel\u00b7den", "gan\u00b7ze", "Reih", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Deinen Zweck erf\u00fcllt, der Weisheit Flor geheget,", "tokens": ["Die", "Dei\u00b7nen", "Zweck", "er\u00b7f\u00fcllt", ",", "der", "Weis\u00b7heit", "Flor", "ge\u00b7he\u00b7get", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVPP", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und jede Wissenschaft verpfleget;", "tokens": ["Und", "je\u00b7de", "Wis\u00b7sen\u00b7schaft", "ver\u00b7pfle\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die alle sind vom Tode frey!", "tokens": ["Die", "al\u00b7le", "sind", "vom", "To\u00b7de", "frey", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Vor andern prangen au\u00dfer Dir,", "tokens": ["Vor", "an\u00b7dern", "pran\u00b7gen", "au\u00b7\u00dfer", "Dir", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Von welchen Pfllicht und Wahrheit melden:", "tokens": ["Von", "wel\u00b7chen", "Pfllicht", "und", "Wahr\u00b7heit", "mel\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Sie mehrten Leipzigs Flor, der freyen K\u00fcnste Zier.", "tokens": ["Sie", "mehr\u00b7ten", "Leip\u00b7zigs", "Flor", ",", "der", "frey\u00b7en", "K\u00fcns\u00b7te", "Zier", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "NN", "$,", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch ihre Sorgfalt ists geschehen,", "tokens": ["Durch", "ih\u00b7re", "Sorg\u00b7falt", "ists", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Da\u00df wir noch Priester gnug in Pallas Tempeln sehen.", "tokens": ["Da\u00df", "wir", "noch", "Pries\u00b7ter", "gnug", "in", "Pal\u00b7las", "Tem\u00b7peln", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "ADV", "APPR", "NE", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.56": {"line.1": {"text": "Hier steht im sch\u00f6nsten Purpurschmuck,", "tokens": ["Hier", "steht", "im", "sch\u00f6ns\u00b7ten", "Pur\u00b7pursc\u00b7hmuck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Lehrer kleine Zahl, die solchen gleich getragen,", "tokens": ["Der", "Leh\u00b7rer", "klei\u00b7ne", "Zahl", ",", "die", "sol\u00b7chen", "gleich", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "PRELS", "PIAT", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als sie in ihren letzten Tagen", "tokens": ["Als", "sie", "in", "ih\u00b7ren", "letz\u00b7ten", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Des Todes Sichel niederschlug.", "tokens": ["Des", "To\u00b7des", "Si\u00b7chel", "nie\u00b7der\u00b7schlug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Nachwelt ehrt noch ihre Gruft,", "tokens": ["Die", "Nach\u00b7welt", "ehrt", "noch", "ih\u00b7re", "Gruft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und Leipzig wird ihr Lob, so lang es steht, bekr\u00f6nen;", "tokens": ["Und", "Leip\u00b7zig", "wird", "ihr", "Lob", ",", "so", "lang", "es", "steht", ",", "be\u00b7kr\u00f6\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "PPOSAT", "NN", "$,", "ADV", "ADJD", "PPER", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Man zeigt ihr Beyspiel muntern S\u00f6hnen,", "tokens": ["Man", "zeigt", "ihr", "Bey\u00b7spiel", "mun\u00b7tern", "S\u00f6h\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Indem man ihren Fu\u00df zum Weisheitpfade ruft.", "tokens": ["In\u00b7dem", "man", "ih\u00b7ren", "Fu\u00df", "zum", "Weis\u00b7heit\u00b7pfa\u00b7de", "ruft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn nichts entz\u00fcndet mehr die Jugend,", "tokens": ["Denn", "nichts", "ent\u00b7z\u00fcn\u00b7det", "mehr", "die", "Ju\u00b7gend", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Als Muster edler Art an Wissenschaft und Tugend.", "tokens": ["Als", "Mus\u00b7ter", "ed\u00b7ler", "Art", "an", "Wis\u00b7sen\u00b7schaft", "und", "Tu\u00b7gend", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.57": {"line.1": {"text": "Was prangt nicht dort f\u00fcr manches Licht,", "tokens": ["Was", "prangt", "nicht", "dort", "f\u00fcr", "man\u00b7ches", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PTKNEG", "ADV", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das die gelehrte Welt, gleich hellen Sternen schm\u00fccket,", "tokens": ["Das", "die", "ge\u00b7lehr\u00b7te", "Welt", ",", "gleich", "hel\u00b7len", "Ster\u00b7nen", "schm\u00fc\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "ADJA", "NN", "$,", "ADV", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wird nicht ", "tokens": ["Wird", "nicht"], "token_info": ["word", "word"], "pos": ["VAFIN", "PTKNEG"], "meter": "-+", "measure": "iambic.single"}, "line.4": {"text": "Seh ich den gro\u00dfen ", "tokens": ["Seh", "ich", "den", "gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Da stralt ein kluger ", "tokens": ["Da", "stralt", "ein", "klu\u00b7ger"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Auch ", "tokens": ["Auch"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Wie Preu\u00dfens Archimed und Schmuck,", "tokens": ["Wie", "Preu\u00b7\u00dfens", "Ar\u00b7chi\u00b7med", "und", "Schmuck", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Noch funfzig andre sieht man prangen,", "tokens": ["Noch", "funf\u00b7zig", "and\u00b7re", "sieht", "man", "pran\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "PIS", "VVFIN", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die uns, wie ", "tokens": ["Die", "uns", ",", "wie"], "token_info": ["word", "word", "punct", "word"], "pos": ["ART", "PPER", "$,", "PWAV"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.58": {"line.1": {"text": "Nur einer fehlt, der hier nicht steht!", "tokens": ["Nur", "ei\u00b7ner", "fehlt", ",", "der", "hier", "nicht", "steht", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$,", "PRELS", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und doch an Ruhm und Glanz und Gr\u00f6\u00dfe keinem weichet;", "tokens": ["Und", "doch", "an", "Ruhm", "und", "Glanz", "und", "Gr\u00f6\u00b7\u00dfe", "kei\u00b7nem", "wei\u00b7chet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "KON", "NN", "KON", "NN", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein Mann, der alles l\u00e4ngst erreichet,", "tokens": ["Ein", "Mann", ",", "der", "al\u00b7les", "l\u00e4ngst", "er\u00b7rei\u00b7chet", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wodurch man ewig sich erh\u00f6ht.", "tokens": ["Wo\u00b7durch", "man", "e\u00b7wig", "sich", "er\u00b7h\u00f6ht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADJD", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Wunder tiefer Wissenschaft,", "tokens": ["Ein", "Wun\u00b7der", "tie\u00b7fer", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Durchdringend an Vernunft, an Einsicht auserlesen,", "tokens": ["Durch\u00b7drin\u00b7gend", "an", "Ver\u00b7nunft", ",", "an", "Ein\u00b7sicht", "au\u00b7ser\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NN", "$,", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ein Geist von allgemeinem Wesen,", "tokens": ["Ein", "Geist", "von", "all\u00b7ge\u00b7mei\u00b7nem", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Von unumschr\u00e4nktem Witz und unersch\u00f6pfter Kraft;", "tokens": ["Von", "un\u00b7um\u00b7schr\u00e4nk\u00b7tem", "Witz", "und", "un\u00b7er\u00b7sch\u00f6pf\u00b7ter", "Kraft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Der alles das in eins gebunden,", "tokens": ["Der", "al\u00b7les", "das", "in", "eins", "ge\u00b7bun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ART", "APPR", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Was je der Mensch erfand; doch selbst noch mehr erfunden.", "tokens": ["Was", "je", "der", "Mensch", "er\u00b7fand", ";", "doch", "selbst", "noch", "mehr", "er\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVFIN", "$.", "ADV", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.59": {"line.1": {"text": "Wer ists? O Leipzig! sollte man", "tokens": ["Wer", "ists", "?", "O", "Leip\u00b7zig", "!", "soll\u00b7te", "man"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "$.", "NE", "NE", "$.", "VMFIN", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bey dir noch allererst nach dessen Namen fragen?", "tokens": ["Bey", "dir", "noch", "al\u00b7le\u00b7rerst", "nach", "des\u00b7sen", "Na\u00b7men", "fra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "ADV", "APPR", "PRELAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Den doch dein eigner Schoo\u00df getragen,", "tokens": ["Den", "doch", "dein", "eig\u00b7ner", "Schoo\u00df", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PPOSAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als er das erste Licht gewann?", "tokens": ["Als", "er", "das", "ers\u00b7te", "Licht", "ge\u00b7wann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ist dir dein Sohn so schlecht bekannt,", "tokens": ["Ist", "dir", "dein", "Sohn", "so", "schlecht", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPOSAT", "NN", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Den halb Europa so, wie Deutschland, hochgeachtet,", "tokens": ["Den", "halb", "Eu\u00b7ro\u00b7pa", "so", ",", "wie", "Deutschland", ",", "hoch\u00b7ge\u00b7ach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJD", "NE", "ADV", "$,", "PWAV", "NE", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Den Albion voll Neid betrachtet,", "tokens": ["Den", "Al\u00b7bi\u00b7on", "voll", "Neid", "be\u00b7trach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den Frankreich uns misg\u00f6nnt, so wie das w\u00e4lsche Land?", "tokens": ["Den", "Fran\u00b7kreich", "uns", "mis\u00b7g\u00f6nnt", ",", "so", "wie", "das", "w\u00e4l\u00b7sche", "Land", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "PPER", "VVFIN", "$,", "ADV", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie? Leipzig, kannst du den verkennen,", "tokens": ["Wie", "?", "Leip\u00b7zig", ",", "kannst", "du", "den", "ver\u00b7ken\u00b7nen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "NE", "$,", "VMFIN", "PPER", "ART", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Um den die V\u00f6lker dich begl\u00fcckt und selig nennen?", "tokens": ["Um", "den", "die", "V\u00f6l\u00b7ker", "dich", "be\u00b7gl\u00fcckt", "und", "se\u00b7lig", "nen\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "ART", "NN", "PPER", "VVPP", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.60": {"line.1": {"text": "Dein ", "tokens": ["Dein"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Der deine gleichfalls wuchs, dieweil du ihn gebohren!", "tokens": ["Der", "dei\u00b7ne", "gleich\u00b7falls", "wuchs", ",", "die\u00b7weil", "du", "ihn", "ge\u00b7boh\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "ADV", "VVFIN", "$,", "KOUS", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Denn hast du ihn gleich jung verlohren;", "tokens": ["Denn", "hast", "du", "ihn", "gleich", "jung", "ver\u00b7loh\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So blieb er doch dein Eigenthum.", "tokens": ["So", "blieb", "er", "doch", "dein", "Ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "Warum? des ", "tokens": ["Wa\u00b7rum", "?", "des"], "token_info": ["word", "punct", "word"], "pos": ["PWAV", "$.", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "So lang ein ", "tokens": ["So", "lang", "ein"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADJD", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "R\u00fchmt sichs des ", "tokens": ["R\u00fchmt", "sichs", "des"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PIS", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.9": {"text": "So lange Rotterdam wird stehen,", "tokens": ["So", "lan\u00b7ge", "Rot\u00b7ter\u00b7dam", "wird", "ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NE", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Wird auch dein Ehrenmaal, ", "tokens": ["Wird", "auch", "dein", "Eh\u00b7ren\u00b7maal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.61": {"line.1": {"text": "Wenn sieben St\u00e4dte den ", "tokens": ["Wenn", "sie\u00b7ben", "St\u00e4d\u00b7te", "den"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "CARD", "NN", "ART"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Aus reger Eifersucht einander abgestritten:", "tokens": ["Aus", "re\u00b7ger", "Ei\u00b7fer\u00b7sucht", "ein\u00b7an\u00b7der", "ab\u00b7ge\u00b7strit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "VVPP", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Was h\u00e4tte ", "tokens": ["Was", "h\u00e4t\u00b7te"], "token_info": ["word", "word"], "pos": ["PWS", "VAFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Wenn hier ein Zweifel m\u00f6glich w\u00e4r?", "tokens": ["Wenn", "hier", "ein", "Zwei\u00b7fel", "m\u00f6g\u00b7lich", "w\u00e4r", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der stolzen Tyber breiter Rand", "tokens": ["Der", "stol\u00b7zen", "Ty\u00b7ber", "brei\u00b7ter", "Rand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "W\u00fcrd eifrig um den Ruhm von dieser Wiege k\u00e4mpfen.", "tokens": ["W\u00fcrd", "eif\u00b7rig", "um", "den", "Ruhm", "von", "die\u00b7ser", "Wie\u00b7ge", "k\u00e4mp\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "ART", "NN", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Seyne, solchen Stolz zu d\u00e4mpfen,", "tokens": ["Die", "Sey\u00b7ne", ",", "sol\u00b7chen", "Stolz", "zu", "d\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "W\u00fcrd streiten, da\u00df man ihr die\u00df hohe Lob entwandt.", "tokens": ["W\u00fcrd", "strei\u00b7ten", ",", "da\u00df", "man", "ihr", "die\u00df", "ho\u00b7he", "Lob", "ent\u00b7wandt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVFIN", "$,", "KOUS", "PIS", "PPER", "PDS", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und an der Themse feuchten Fl\u00e4chen,", "tokens": ["Und", "an", "der", "Them\u00b7se", "feuch\u00b7ten", "Fl\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "W\u00fcrd London eifern, sich den Vorzug zuzusprechen.", "tokens": ["W\u00fcrd", "Lon\u00b7don", "ei\u00b7fern", ",", "sich", "den", "Vor\u00b7zug", "zu\u00b7zu\u00b7spre\u00b7chen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "VVFIN", "$,", "PRF", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.62": {"line.1": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Da ihn in seinen Lebensjahren", "tokens": ["Da", "ihn", "in", "sei\u00b7nen", "Le\u00b7bens\u00b7jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kein Reich und keine Stadt begehrt.", "tokens": ["Kein", "Reich", "und", "kei\u00b7ne", "Stadt", "be\u00b7gehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um des von ", "tokens": ["Um", "des", "von"], "token_info": ["word", "word", "word"], "pos": ["KOUI", "ART", "APPR"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Hat manch gekr\u00f6ntes Haupt, vorl\u00e4ngst eh er gestorben,", "tokens": ["Hat", "manch", "ge\u00b7kr\u00f6n\u00b7tes", "Haupt", ",", "vor\u00b7l\u00e4ngst", "eh", "er", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "$,", "ADV", "KOUS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Durch Gnad und Wohlthun sich beworben,", "tokens": ["Durch", "Gnad", "und", "Wohl\u00b7thun", "sich", "be\u00b7wor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "In L\u00e4ndern, wo er noch verehrungsw\u00fcrdig hei\u00dft;", "tokens": ["In", "L\u00e4n\u00b7dern", ",", "wo", "er", "noch", "ver\u00b7eh\u00b7rungs\u00b7w\u00fcr\u00b7dig", "hei\u00dft", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PWAV", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Wo sein Verdienst und Rath und Schriften,", "tokens": ["Wo", "sein", "Ver\u00b7dienst", "und", "Rath", "und", "Schrif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "KON", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Ihn lebend gro\u00df gemacht, ihm todt manch Denkmaal stiften.", "tokens": ["Ihn", "le\u00b7bend", "gro\u00df", "ge\u00b7macht", ",", "ihm", "todt", "manch", "Denk\u00b7maal", "stif\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "VVPP", "$,", "PPER", "ADJD", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.63": {"line.1": {"text": "Der Britten Haupt hat ihn erh\u00f6ht,", "tokens": ["Der", "Brit\u00b7ten", "Haupt", "hat", "ihn", "er\u00b7h\u00f6ht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Barbarey aus seinen Staaten;", "tokens": ["Die", "Bar\u00b7ba\u00b7rey", "aus", "sei\u00b7nen", "Staa\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wo noch sein Ruhm im Segen steht.", "tokens": ["Wo", "noch", "sein", "Ruhm", "im", "Se\u00b7gen", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der sechste ", "tokens": ["Der", "sechs\u00b7te"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Sein Feldherr, ", "tokens": ["Sein", "Feld\u00b7herr", ","], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Vernahmen kaum was er begehrte;", "tokens": ["Ver\u00b7nah\u00b7men", "kaum", "was", "er", "be\u00b7gehr\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PWS", "PPER", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So ward ihm selbst in Wien der Zutritt bald erlaubt.", "tokens": ["So", "ward", "ihm", "selbst", "in", "Wi\u00b7en", "der", "Zu\u00b7tritt", "bald", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPR", "NE", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Lutetien war ihm gewogen,", "tokens": ["Lu\u00b7te\u00b7ti\u00b7en", "war", "ihm", "ge\u00b7wo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.9": {"text": "Und h\u00e4tt auf Lebenslang ihn gern zu sich gezogen.", "tokens": ["Und", "h\u00e4tt", "auf", "Le\u00b7bens\u00b7lang", "ihn", "gern", "zu", "sich", "ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "NN", "PPER", "ADV", "APPR", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.64": {"line.1": {"text": "Besoldung, Aemter, suchten ihn,", "tokens": ["Be\u00b7sol\u00b7dung", ",", "A\u00b7em\u00b7ter", ",", "such\u00b7ten", "ihn", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Des Reiches Freyherrnstand, (ein seltner Lohn vom Wissen,", "tokens": ["Des", "Rei\u00b7ches", "Frey\u00b7herrns\u00b7tand", ",", "(", "ein", "selt\u00b7ner", "Lohn", "vom", "Wis\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "$(", "ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Seit ihn das Gold zu sich gerissen!)", "tokens": ["Seit", "ihn", "das", "Gold", "zu", "sich", "ge\u00b7ris\u00b7sen", "!", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "APPR", "PRF", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vergalten sein gelehrt Bem\u00fchn.", "tokens": ["Ver\u00b7gal\u00b7ten", "sein", "ge\u00b7lehrt", "Be\u00b7m\u00fchn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "VVPP", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Bey zweenen Kaisern Rath zu seyn,", "tokens": ["Bey", "zwee\u00b7nen", "Kai\u00b7sern", "Rath", "zu", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und so viel K\u00f6nigen mit Werk und That zu dienen,", "tokens": ["Und", "so", "viel", "K\u00f6\u00b7ni\u00b7gen", "mit", "Werk", "und", "That", "zu", "die\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIAT", "NN", "APPR", "NN", "KON", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Hat billig jedem viel geschienen,", "tokens": ["Hat", "bil\u00b7lig", "je\u00b7dem", "viel", "ge\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "PIAT", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Hei\u00dft wirklich ehrenvoll, bleibt ewig ungemein;", "tokens": ["Hei\u00dft", "wirk\u00b7lich", "eh\u00b7ren\u00b7voll", ",", "bleibt", "e\u00b7wig", "un\u00b7ge\u00b7mein", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADJD", "$,", "VVFIN", "ADJD", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Seit Gattungen geringrer Gaben,", "tokens": ["Seit", "Gat\u00b7tun\u00b7gen", "ge\u00b7ring\u00b7rer", "Ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Die strenge Wissenschaft vom Hof entfernet haben.", "tokens": ["Die", "stren\u00b7ge", "Wis\u00b7sen\u00b7schaft", "vom", "Hof", "ent\u00b7fer\u00b7net", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.65": {"line.1": {"text": "An Witz und Einsicht reich und satt,", "tokens": ["An", "Witz", "und", "Ein\u00b7sicht", "reich", "und", "satt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat er der Wahrheit sich zum Priester eingeweihet:", "tokens": ["Hat", "er", "der", "Wahr\u00b7heit", "sich", "zum", "Pries\u00b7ter", "ein\u00b7ge\u00b7wei\u00b7het", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "PRF", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hier hat er keine M\u00fch gescheuet,", "tokens": ["Hier", "hat", "er", "kei\u00b7ne", "M\u00fch", "ge\u00b7scheu\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Davor ein Tr\u00e4ger Abscheu hat.", "tokens": ["Da\u00b7vor", "ein", "Tr\u00e4\u00b7ger", "Ab\u00b7scheu", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der tiefsten Weisheit ersten Grund,", "tokens": ["Der", "tiefs\u00b7ten", "Weis\u00b7heit", "ers\u00b7ten", "Grund", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Sch\u00e4tze der Natur, der Zahlen Seltenheiten,", "tokens": ["Die", "Sch\u00e4t\u00b7ze", "der", "Na\u00b7tur", ",", "der", "Zah\u00b7len", "Sel\u00b7ten\u00b7hei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Me\u00dfkunst hohe Trefflichkeiten,", "tokens": ["Der", "Me\u00df\u00b7kunst", "ho\u00b7he", "Treff\u00b7lich\u00b7kei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Das alles sah er ein; das that er andern kund.", "tokens": ["Das", "al\u00b7les", "sah", "er", "ein", ";", "das", "that", "er", "an\u00b7dern", "kund", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "PPER", "PTKVZ", "$.", "PDS", "VVFIN", "PPER", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er war ein Meister in Geschichten,", "tokens": ["Er", "war", "ein", "Meis\u00b7ter", "in", "Ge\u00b7schich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Im Alterthume stark, und ein Lucrez im Dichten.", "tokens": ["Im", "Al\u00b7ter\u00b7thu\u00b7me", "stark", ",", "und", "ein", "Lu\u00b7crez", "im", "Dich\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "$,", "KON", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.66": {"line.1": {"text": "Wer kennt die Wunderrechnung nicht,", "tokens": ["Wer", "kennt", "die", "Wun\u00b7der\u00b7rech\u00b7nung", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Archimed ersann, den Weltraum zu ergr\u00fcnden?", "tokens": ["Die", "Ar\u00b7chi\u00b7med", "er\u00b7sann", ",", "den", "Welt\u00b7raum", "zu", "er\u00b7gr\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was gr\u00f6\u00dfers war kaum auszufinden,", "tokens": ["Was", "gr\u00f6\u00b7\u00dfers", "war", "kaum", "aus\u00b7zu\u00b7fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VAFIN", "ADV", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In dem, was Menschenwitz verspricht.", "tokens": ["In", "dem", ",", "was", "Men\u00b7schen\u00b7witz", "ver\u00b7spricht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PWS", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nur ", "tokens": ["Nur"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Er fand die Rechenkunst in dem unendlich Kleinen:", "tokens": ["Er", "fand", "die", "Re\u00b7chen\u00b7kunst", "in", "dem", "un\u00b7end\u00b7lich", "Klei\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Hier konnt er doppelt gro\u00df erscheinen,", "tokens": ["Hier", "konnt", "er", "dop\u00b7pelt", "gro\u00df", "er\u00b7schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADJD", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und ganzer V\u00f6lker Neid war seines Witzes Frucht.", "tokens": ["Und", "gan\u00b7zer", "V\u00f6l\u00b7ker", "Neid", "war", "sei\u00b7nes", "Wit\u00b7zes", "Frucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "NN", "VAFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Die Eifersucht der stolzen Britten", "tokens": ["Die", "Ei\u00b7fer\u00b7sucht", "der", "stol\u00b7zen", "Brit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Hat die Erfindung ihm aufs heftigste bestritten.", "tokens": ["Hat", "die", "Er\u00b7fin\u00b7dung", "ihm", "aufs", "hef\u00b7tigs\u00b7te", "be\u00b7strit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.67": {"line.1": {"text": "Wie dort den neuen Theil der Welt,", "tokens": ["Wie", "dort", "den", "neu\u00b7en", "Theil", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df beyder Ruhm zwar nicht verschwunden;", "tokens": ["Da\u00df", "bey\u00b7der", "Ruhm", "zwar", "nicht", "ver\u00b7schwun\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ob jener gleich den Preis beh\u00e4lt.", "tokens": ["Ob", "je\u00b7ner", "gleich", "den", "Preis", "be\u00b7h\u00e4lt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "H\u00e4tt kein Columbus sich gewagt,", "tokens": ["H\u00e4tt", "kein", "Co\u00b7lum\u00b7bus", "sich", "ge\u00b7wagt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und seinen k\u00fchnen Mast dem Ocean vertrauet,", "tokens": ["Und", "sei\u00b7nen", "k\u00fch\u00b7nen", "Mast", "dem", "O\u00b7cean", "ver\u00b7trau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Den noch kein Schiffer je geschauet:", "tokens": ["Den", "noch", "kein", "Schif\u00b7fer", "je", "ge\u00b7schau\u00b7et", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Wem h\u00e4tt Americus so herzhaft nachgejagt?", "tokens": ["Wem", "h\u00e4tt", "A\u00b7me\u00b7ri\u00b7cus", "so", "herz\u00b7haft", "nach\u00b7ge\u00b7jagt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NE", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So w\u00e4r auch ", "tokens": ["So", "w\u00e4r", "auch"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "W\u00e4r unsers ", "tokens": ["W\u00e4r", "un\u00b7sers"], "token_info": ["word", "word"], "pos": ["VAFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.68": {"line.1": {"text": "Gebrauchte sonst ", "tokens": ["Ge\u00b7brauch\u00b7te", "sonst"], "token_info": ["word", "word"], "pos": ["NN", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Die Kunst, zehn Ziffern nur im Rechnen anzuwenden;", "tokens": ["Die", "Kunst", ",", "zehn", "Zif\u00b7fern", "nur", "im", "Rech\u00b7nen", "an\u00b7zu\u00b7wen\u00b7den", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "CARD", "NN", "ADV", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und doch das schwerste zu vollenden;", "tokens": ["Und", "doch", "das", "schwers\u00b7te", "zu", "voll\u00b7en\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So that zwar ", "tokens": ["So", "that", "zwar"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Vier Ziffern langten v\u00f6llig hin,", "tokens": ["Vier", "Zif\u00b7fern", "lang\u00b7ten", "v\u00f6l\u00b7lig", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die unerme\u00dfne Reih der Gr\u00f6\u00dfen zu erreichen:", "tokens": ["Die", "un\u00b7er\u00b7me\u00df\u00b7ne", "Reih", "der", "Gr\u00f6\u00b7\u00dfen", "zu", "er\u00b7rei\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch dieser Kunstgriff selbst mu\u00df weichen,", "tokens": ["Doch", "die\u00b7ser", "Kunst\u00b7griff", "selbst", "mu\u00df", "wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "NN", "ADV", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Was gr\u00f6\u00dfers noch erfand des ", "tokens": ["Was", "gr\u00f6\u00b7\u00dfers", "noch", "er\u00b7fand", "des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ADV", "VVFIN", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Das ungeheure Heer der Zahlen", "tokens": ["Das", "un\u00b7ge\u00b7heu\u00b7re", "Heer", "der", "Zah\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "L\u00e4\u00dft durch zwo Ziffern sich, durch Null und Eins schon malen.", "tokens": ["L\u00e4\u00dft", "durch", "zwo", "Zif\u00b7fern", "sich", ",", "durch", "Null", "und", "Eins", "schon", "ma\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "CARD", "NN", "PRF", "$,", "APPR", "NE", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.69": {"line.1": {"text": "Ihr V\u00f6lker! deren letzten Strand,", "tokens": ["Ihr", "V\u00f6l\u00b7ker", "!", "de\u00b7ren", "letz\u00b7ten", "Strand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "PRELAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Japonesermeer durch seine Fluth benetzet,", "tokens": ["Das", "Ja\u00b7po\u00b7nes\u00b7er\u00b7meer", "durch", "sei\u00b7ne", "Fluth", "be\u00b7net\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Die ihr nur euch f\u00fcr weise sch\u00e4tzet,", "tokens": ["Die", "ihr", "nur", "euch", "f\u00fcr", "wei\u00b7se", "sch\u00e4t\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "PPER", "APPR", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bewundert dieses Manns Verstand!", "tokens": ["Be\u00b7wun\u00b7dert", "die\u00b7ses", "Manns", "Ver\u00b7stand", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ihr, die ihr sonst Europen kaum", "tokens": ["Ihr", ",", "die", "ihr", "sonst", "Eu\u00b7ro\u00b7pen", "kaum"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "PPER", "ADV", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Auge zugesteht, die Wahrheit zu erkennen:", "tokens": ["Ein", "Au\u00b7ge", "zu\u00b7ge\u00b7steht", ",", "die", "Wahr\u00b7heit", "zu", "er\u00b7ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "H\u00f6rt auf, euch noch so klug zu nennen,", "tokens": ["H\u00f6rt", "auf", ",", "euch", "noch", "so", "klug", "zu", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "$,", "PPER", "ADV", "ADV", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und gebt hinfort nicht mehr dem alten Stolze Raum:", "tokens": ["Und", "gebt", "hin\u00b7fort", "nicht", "mehr", "dem", "al\u00b7ten", "Stol\u00b7ze", "Raum", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PTKNEG", "ADV", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Seitdem ein Deutscher euch erkl\u00e4ret,", "tokens": ["Seit\u00b7dem", "ein", "Deut\u00b7scher", "euch", "er\u00b7kl\u00e4\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Was eures Stifters Witz euch r\u00e4thselhaft gelehret.", "tokens": ["Was", "eu\u00b7res", "Stif\u00b7ters", "Witz", "euch", "r\u00e4th\u00b7sel\u00b7haft", "ge\u00b7leh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "NN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.70": {"line.1": {"text": "Des gro\u00dfen ", "tokens": ["Des", "gro\u00b7\u00dfen"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Vertraute seine Kunst geheimnisvollen Strichen;", "tokens": ["Ver\u00b7trau\u00b7te", "sei\u00b7ne", "Kunst", "ge\u00b7heim\u00b7nis\u00b7vol\u00b7len", "Stri\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Kraft davon war euch entwichen,", "tokens": ["Die", "Kraft", "da\u00b7von", "war", "euch", "ent\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und was man vorgab, fiel dahin.", "tokens": ["Und", "was", "man", "vor\u00b7gab", ",", "fiel", "da\u00b7hin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PIS", "VVFIN", "$,", "VVFIN", "PAV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "In mancher lockenden Figur", "tokens": ["In", "man\u00b7cher", "lo\u00b7cken\u00b7den", "Fi\u00b7gur"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Gebrochner Linien mit ganzen untermenget,", "tokens": ["Ge\u00b7broch\u00b7ner", "Li\u00b7ni\u00b7en", "mit", "gan\u00b7zen", "un\u00b7ter\u00b7men\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Lag ein verborgner Sinn gedr\u00e4nget,", "tokens": ["Lag", "ein", "ver\u00b7borg\u00b7ner", "Sinn", "ge\u00b7dr\u00e4n\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und die\u00df versteckte Werk erreichte ", "tokens": ["Und", "die\u00df", "ver\u00b7steck\u00b7te", "Werk", "er\u00b7reich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "VVFIN", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Was China seit vier tausend Jahren", "tokens": ["Was", "Chi\u00b7na", "seit", "vier", "tau\u00b7send", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "NE", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Gesucht und nicht entdeckt, hat es durch ihn erfahren.", "tokens": ["Ge\u00b7sucht", "und", "nicht", "ent\u00b7deckt", ",", "hat", "es", "durch", "ihn", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "PTKNEG", "VVPP", "$,", "VAFIN", "PPER", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.71": {"line.1": {"text": "Der Preu\u00dfen erster ", "tokens": ["Der", "Preu\u00b7\u00dfen", "ers\u00b7ter"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Der jede Wissenschaft auf seinen Thron erhoben,", "tokens": ["Der", "je\u00b7de", "Wis\u00b7sen\u00b7schaft", "auf", "sei\u00b7nen", "Thron", "er\u00b7ho\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "APPR", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und den noch alle Musen loben,", "tokens": ["Und", "den", "noch", "al\u00b7le", "Mu\u00b7sen", "lo\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil unter Ihm ihr Kummer wich;", "tokens": ["Weil", "un\u00b7ter", "Ihm", "ihr", "Kum\u00b7mer", "wich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der weise Held empfand den Trieb,", "tokens": ["Der", "wei\u00b7se", "Held", "emp\u00b7fand", "den", "Trieb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der Weisheit in Berlin ein eignes Haus zu gr\u00fcnden.", "tokens": ["Der", "Weis\u00b7heit", "in", "Ber\u00b7lin", "ein", "eig\u00b7nes", "Haus", "zu", "gr\u00fcn\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+++-+-+-+-", "measure": "unknown.measure.septa"}, "line.7": {"text": "Hier war ein ", "tokens": ["Hier", "war", "ein"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VAFIN", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "Der dieser neuen Zunft die ersten Regeln schrieb.", "tokens": ["Der", "die\u00b7ser", "neu\u00b7en", "Zunft", "die", "ers\u00b7ten", "Re\u00b7geln", "schrieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "ADJA", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und der Gesellschaft Grund geleget,", "tokens": ["Und", "der", "Ge\u00b7sell\u00b7schaft", "Grund", "ge\u00b7le\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Die Deutschland itzt noch ziert und reichlich Fr\u00fcchte tr\u00e4get.", "tokens": ["Die", "Deutschland", "itzt", "noch", "ziert", "und", "reich\u00b7lich", "Fr\u00fcch\u00b7te", "tr\u00e4\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "VVFIN", "KON", "ADJD", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.72": {"line.1": {"text": "Wie hoch erhob die Weisheit dich,", "tokens": ["Wie", "hoch", "er\u00b7hob", "die", "Weis\u00b7heit", "dich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "ART", "NN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Minerva Deiner Zeit, verkl\u00e4rte ", "tokens": ["Mi\u00b7ner\u00b7va", "Dei\u00b7ner", "Zeit", ",", "ver\u00b7kl\u00e4r\u00b7te"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "PPOSAT", "NN", "$,", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Du prangst zwar an der Sternenb\u00fchne;", "tokens": ["Du", "prangst", "zwar", "an", "der", "Ster\u00b7nen\u00b7b\u00fch\u00b7ne", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch auch Dein Ruhm verewigt sich.", "tokens": ["Doch", "auch", "Dein", "Ruhm", "ve\u00b7re\u00b7wigt", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "VVFIN", "PRF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Hat Leibnitz nicht durch Deine Hand", "tokens": ["Hat", "Leib\u00b7nitz", "nicht", "durch", "Dei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "NE", "PTKNEG", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit ", "tokens": ["Mit"], "token_info": ["word"], "pos": ["APPR"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Davon das Lob nur Dir geb\u00fchret;", "tokens": ["Da\u00b7von", "das", "Lob", "nur", "Dir", "ge\u00b7b\u00fch\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ob Deutsch- und England gleich den Nutz davon empfand?", "tokens": ["Ob", "Deut\u00b7sch", "und", "En\u00b7gland", "gleich", "den", "Nutz", "da\u00b7von", "emp\u00b7fand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "TRUNC", "KON", "NE", "ADV", "ART", "NN", "PAV", "VVFIN", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Wie bey Turnieren alter Zeiten,", "tokens": ["Wie", "bey", "Tur\u00b7nie\u00b7ren", "al\u00b7ter", "Zei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Warst Du die Richterinn gelehrter Zwistigkeiten.", "tokens": ["Warst", "Du", "die", "Rich\u00b7te\u00b7rinn", "ge\u00b7lehr\u00b7ter", "Zwis\u00b7tig\u00b7kei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.73": {"line.1": {"text": "Es regte sich der Sp\u00f6tter Wuth", "tokens": ["Es", "reg\u00b7te", "sich", "der", "Sp\u00f6t\u00b7ter", "Wuth"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Durch Schl\u00fcsse voller Trug den Glauben zu bek\u00e4mpfen,", "tokens": ["Durch", "Schl\u00fcs\u00b7se", "vol\u00b7ler", "Trug", "den", "Glau\u00b7ben", "zu", "be\u00b7k\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vernunft und Schrift durch das zu d\u00e4mpfen,", "tokens": ["Ver\u00b7nunft", "und", "Schrift", "durch", "das", "zu", "d\u00e4mp\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "PDS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Was beyden Lichtern Eintrag thut.", "tokens": ["Was", "bey\u00b7den", "Lich\u00b7tern", "Ein\u00b7trag", "thut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Man sch\u00e4rft des ", "tokens": ["Man", "sch\u00e4rft", "des"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Was ", "tokens": ["Was"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Wird noch verschmitzter angesponnen;", "tokens": ["Wird", "noch", "ver\u00b7schmitz\u00b7ter", "an\u00b7ge\u00b7spon\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ein neuer Firni\u00df giebt verlegner Waare Lauf.", "tokens": ["Ein", "neu\u00b7er", "Fir\u00b7ni\u00df", "giebt", "ver\u00b7leg\u00b7ner", "Waa\u00b7re", "Lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Man glaubt in ", "tokens": ["Man", "glaubt", "in"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Mehr Nachdruck, St\u00e4rk und Kraft als in der Schrift zu finden.", "tokens": ["Mehr", "Nach\u00b7druck", ",", "St\u00e4rk", "und", "Kraft", "als", "in", "der", "Schrift", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "NN", "KON", "NN", "KOKOM", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.74": {"line.1": {"text": "Die\u00df wirkte ", "tokens": ["Die\u00df", "wirk\u00b7te"], "token_info": ["word", "word"], "pos": ["PDS", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Der Glauben und Vernunft mit Zweifeln \u00fcberh\u00e4ufte,", "tokens": ["Der", "Glau\u00b7ben", "und", "Ver\u00b7nunft", "mit", "Zwei\u00b7feln", "\u00fc\u00b7berh\u00b7\u00e4uf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und sich auf lauter Blendwerk steifte,", "tokens": ["Und", "sich", "auf", "lau\u00b7ter", "Blend\u00b7werk", "steif\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Bl\u00f6den sehr ins Auge fiel.", "tokens": ["Das", "Bl\u00f6\u00b7den", "sehr", "ins", "Au\u00b7ge", "fiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der wilden Jugend rohe Brust", "tokens": ["Der", "wil\u00b7den", "Ju\u00b7gend", "ro\u00b7he", "Brust"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ergreift mit voller Lust den Scheingrund, nichts zu glauben;", "tokens": ["Er\u00b7greift", "mit", "vol\u00b7ler", "Lust", "den", "Schein\u00b7grund", ",", "nichts", "zu", "glau\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "ART", "NN", "$,", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "L\u00e4\u00dft sich Verstand und Sinne rauben", "tokens": ["L\u00e4\u00dft", "sich", "Ver\u00b7stand", "und", "Sin\u00b7ne", "rau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "NN", "KON", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und braucht der Zweifler Traum zum Vorwand arger Lust.", "tokens": ["Und", "braucht", "der", "Zweif\u00b7ler", "Traum", "zum", "Vor\u00b7wand", "ar\u00b7ger", "Lust", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPRART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kein Wunder da\u00df dergleichen Schriften,", "tokens": ["Kein", "Wun\u00b7der", "da\u00df", "derg\u00b7lei\u00b7chen", "Schrif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KOUS", "PIS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Mehr Schaden, als ", "tokens": ["Mehr", "Scha\u00b7den", ",", "als"], "token_info": ["word", "word", "punct", "word"], "pos": ["PIAT", "NN", "$,", "KOUS"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.75": {"line.1": {"text": "Wer hat nun dieser ", "tokens": ["Wer", "hat", "nun", "die\u00b7ser"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "PDAT"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Mit gl\u00fccklichem Erfolg am st\u00e4rksten widersetzet?", "tokens": ["Mit", "gl\u00fcck\u00b7li\u00b7chem", "Er\u00b7folg", "am", "st\u00e4rks\u00b7ten", "wi\u00b7der\u00b7set\u00b7zet", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wer hat sie auf den Tod verletzet,", "tokens": ["Wer", "hat", "sie", "auf", "den", "Tod", "ver\u00b7let\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df sie, wie jene ", "tokens": ["Da\u00df", "sie", ",", "wie", "je\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "$,", "PWAV", "PDS"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Viel gro\u00dfe M\u00e4nner stritten hier,", "tokens": ["Viel", "gro\u00b7\u00dfe", "M\u00e4n\u00b7ner", "strit\u00b7ten", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Glauben und Vernunft geschickt und scharf verfochten:", "tokens": ["Die", "Glau\u00b7ben", "und", "Ver\u00b7nunft", "ge\u00b7schickt", "und", "scharf", "ver\u00b7foch\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch keinem ward der Kranz geflochten;", "tokens": ["Doch", "kei\u00b7nem", "ward", "der", "Kranz", "ge\u00b7floch\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Der Sieg in diesem Kampf, geb\u00fchrt, o ", "tokens": ["Der", "Sieg", "in", "die\u00b7sem", "Kampf", ",", "ge\u00b7b\u00fchrt", ",", "o"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "$,", "VVPP", "$,", "FM"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Das Buch so man von dir gelesen,", "tokens": ["Das", "Buch", "so", "man", "von", "dir", "ge\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PIS", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ist ein Triumph der Schrift und der Vernunft gewesen.", "tokens": ["Ist", "ein", "Tri\u00b7umph", "der", "Schrift", "und", "der", "Ver\u00b7nunft", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "KON", "ART", "NN", "VAPP", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.76": {"line.1": {"text": "Tr\u00e4gt nicht der Pallas Helm dein Bild,", "tokens": ["Tr\u00e4gt", "nicht", "der", "Pal\u00b7las", "Helm", "dein", "Bild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "ART", "NN", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die unl\u00e4ngst das Panier von dem ber\u00fchmten Orden,", "tokens": ["Die", "un\u00b7l\u00e4ngst", "das", "Pa\u00b7nier", "von", "dem", "be\u00b7r\u00fchm\u00b7ten", "Or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der Wahrheitliebenden geworden,", "tokens": ["Der", "Wahr\u00b7heit\u00b7lie\u00b7ben\u00b7den", "ge\u00b7wor\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und jedes Glied mit Muth erf\u00fcllt?", "tokens": ["Und", "je\u00b7des", "Glied", "mit", "Muth", "er\u00b7f\u00fcllt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "O mehr als g\u00fcldnes Wort, das vom Horaz entsprungen,", "tokens": ["O", "mehr", "als", "g\u00fcld\u00b7nes", "Wort", ",", "das", "vom", "Ho\u00b7raz", "ent\u00b7sprun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "KOKOM", "ADJA", "NN", "$,", "PRELS", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Doch itzt noch tiefer eingedrungen,", "tokens": ["Doch", "itzt", "noch", "tie\u00b7fer", "ein\u00b7ge\u00b7drun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Seit edle Geister sich der Wahrheitliebe weihn;", "tokens": ["Seit", "ed\u00b7le", "Geis\u00b7ter", "sich", "der", "Wahr\u00b7heit\u00b7lie\u00b7be", "weihn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PRF", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Seit uns ein gro\u00dfer Graf will treiben,", "tokens": ["Seit", "uns", "ein", "gro\u00b7\u00dfer", "Graf", "will", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Mit Eifer nachzusehn, was ", "tokens": ["Mit", "Ei\u00b7fer", "nach\u00b7zu\u00b7sehn", ",", "was"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["APPR", "NN", "VVINF", "$,", "PWS"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.77": {"line.1": {"text": "Begl\u00fccktes Leipzig! sey erfreut,", "tokens": ["Be\u00b7gl\u00fcck\u00b7tes", "Leip\u00b7zig", "!", "sey", "er\u00b7freut", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$.", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df deinem Sohne nur die\u00df gro\u00dfe Werk gelungen;", "tokens": ["Da\u00df", "dei\u00b7nem", "Soh\u00b7ne", "nur", "die\u00df", "gro\u00b7\u00dfe", "Werk", "ge\u00b7lun\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PDS", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der hier ein st\u00e4rker Heer bezwungen,", "tokens": ["Der", "hier", "ein", "st\u00e4r\u00b7ker", "Heer", "be\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als der des Xerxes Macht zerstreut.", "tokens": ["Als", "der", "des", "Xe\u00b7rxes", "Macht", "zer\u00b7streut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Als ", "tokens": ["Als"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Erfocht ein tapfrer Held, nach zweener Br\u00fcder Leichen,", "tokens": ["Er\u00b7focht", "ein", "tapf\u00b7rer", "Held", ",", "nach", "zwee\u00b7ner", "Br\u00fc\u00b7der", "Lei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Vaterstadt die Siegeszeichen,", "tokens": ["Der", "Va\u00b7ter\u00b7stadt", "die", "Sie\u00b7ges\u00b7zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und Rom gewann dadurch die Oberherrschaft gar.", "tokens": ["Und", "Rom", "ge\u00b7wann", "da\u00b7durch", "die", "O\u00b7ber\u00b7herr\u00b7schaft", "gar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PAV", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch das, was ", "tokens": ["Durch", "das", ",", "was"], "token_info": ["word", "word", "punct", "word"], "pos": ["APPR", "PDS", "$,", "PWS"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "Hat Glaub und Wahrheit mehr, als vormals Rom gewonnen.", "tokens": ["Hat", "Glaub", "und", "Wahr\u00b7heit", "mehr", ",", "als", "vor\u00b7mals", "Rom", "ge\u00b7won\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "ADV", "$,", "KOUS", "ADV", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.78": {"line.1": {"text": "Sey stolz auf deines B\u00fcrgers Preis!", "tokens": ["Sey", "stolz", "auf", "dei\u00b7nes", "B\u00fcr\u00b7gers", "Preis", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ber\u00fchmtes Plei\u00dfathen, sey stolz auf seine Werke!", "tokens": ["Be\u00b7r\u00fchm\u00b7tes", "Plei\u00b7\u00df\u00b7a\u00b7then", ",", "sey", "stolz", "auf", "sei\u00b7ne", "Wer\u00b7ke", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VAFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Weil seines Kiels bew\u00e4hrte St\u00e4rke", "tokens": ["Weil", "sei\u00b7nes", "Kiels", "be\u00b7w\u00e4hr\u00b7te", "St\u00e4r\u00b7ke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Kaum irgend ihres gleichen weis.", "tokens": ["Kaum", "ir\u00b7gend", "ih\u00b7res", "glei\u00b7chen", "weis", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPOSAT", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "La\u00df dieses Jahr dir heilig seyn,", "tokens": ["La\u00df", "die\u00b7ses", "Jahr", "dir", "hei\u00b7lig", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PDAT", "NN", "PPER", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das hundertste nach dem, daran du den gebohren,", "tokens": ["Das", "hun\u00b7derts\u00b7te", "nach", "dem", ",", "da\u00b7ran", "du", "den", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "APPR", "ART", "$,", "PAV", "PPER", "ART", "VVPP", "$,"], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Den selbst die Vorsicht auserkohren,", "tokens": ["Den", "selbst", "die", "Vor\u00b7sicht", "au\u00b7ser\u00b7koh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zu ihrer Rechte Schutz, Verstand und Kiel zu weihn.", "tokens": ["Zu", "ih\u00b7rer", "Rech\u00b7te", "Schutz", ",", "Ver\u00b7stand", "und", "Kiel", "zu", "weihn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "$,", "NN", "KON", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Sey stolz, und la\u00df in deinen Mauren", "tokens": ["Sey", "stolz", ",", "und", "la\u00df", "in", "dei\u00b7nen", "Mau\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAIMP", "ADJD", "$,", "KON", "VVIMP", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ein Denkmaal deiner Pflicht aus Dank und Ehrfurcht dauren.", "tokens": ["Ein", "Denk\u00b7maal", "dei\u00b7ner", "Pflicht", "aus", "Dank", "und", "Ehr\u00b7furcht", "dau\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.79": {"line.1": {"text": "Dir fehlts gewi\u00df an Marmor nicht,", "tokens": ["Dir", "fehlts", "ge\u00b7wi\u00df", "an", "Mar\u00b7mor", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sonst Athen gethan, die Weisen zu verehren:", "tokens": ["Wie", "sonst", "A\u00b7then", "ge\u00b7than", ",", "die", "Wei\u00b7sen", "zu", "ver\u00b7eh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "NE", "VVPP", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Versuchs an dem, von dessen Lehren", "tokens": ["Ver\u00b7suchs", "an", "dem", ",", "von", "des\u00b7sen", "Leh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "$,", "APPR", "PRELAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Wahrheit dir viel Glanz verspricht.", "tokens": ["Die", "Wahr\u00b7heit", "dir", "viel", "Glanz", "ver\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie kr\u00e4ftig wird sein Ehrenbild", "tokens": ["Wie", "kr\u00e4f\u00b7tig", "wird", "sein", "Eh\u00b7ren\u00b7bild"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "In deiner S\u00f6hne Brust den Weisheittrieb erhitzen!", "tokens": ["In", "dei\u00b7ner", "S\u00f6h\u00b7ne", "Brust", "den", "Weis\u00b7heit\u00b7trieb", "er\u00b7hit\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wie mancher Kopf wird dir noch n\u00fctzen.", "tokens": ["Wie", "man\u00b7cher", "Kopf", "wird", "dir", "noch", "n\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den ", "tokens": ["Den"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.9": {"text": "Du selber wirst dadurch auf Erden,", "tokens": ["Du", "sel\u00b7ber", "wirst", "da\u00b7durch", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "PAV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "In aller V\u00f6lker Mund der Weisheit Mutter werden.", "tokens": ["In", "al\u00b7ler", "V\u00f6l\u00b7ker", "Mund", "der", "Weis\u00b7heit", "Mut\u00b7ter", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "ART", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.80": {"line.1": {"text": "Nicht seit zwey Jahren schon auch Deinen Schutz erlanget?", "tokens": ["Nicht", "seit", "zwey", "Jah\u00b7ren", "schon", "auch", "Dei\u00b7nen", "Schutz", "er\u00b7lan\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "CARD", "NN", "ADV", "ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Seit es mit Deinem Namen pranget,", "tokens": ["Seit", "es", "mit", "Dei\u00b7nem", "Na\u00b7men", "pran\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ward es der kl\u00fcgsten Augenmerk.", "tokens": ["Ward", "es", "der", "kl\u00fcgs\u00b7ten", "Au\u00b7gen\u00b7merk", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Gnad und Huld so ich empfand,", "tokens": ["Die", "Gnad", "und", "Huld", "so", "ich", "emp\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat Leibnitz zehnfach mehr, als mein Bem\u00fchn verdienet:", "tokens": ["Hat", "Leib\u00b7nitz", "zehn\u00b7fach", "mehr", ",", "als", "mein", "Be\u00b7m\u00fchn", "ver\u00b7die\u00b7net", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "VVFIN", "ADV", "$,", "KOUS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Drum hab ich mir die\u00df Lob erk\u00fchnet,", "tokens": ["Drum", "hab", "ich", "mir", "die\u00df", "Lob", "er\u00b7k\u00fch\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPER", "PDS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "O tr\u00e4f ein gleiches Gl\u00fcck des Dichters Gegenstand?", "tokens": ["O", "tr\u00e4f", "ein", "glei\u00b7ches", "Gl\u00fcck", "des", "Dich\u00b7ters", "Ge\u00b7gen\u00b7stand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "ADJA", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So w\u00fcrd einmal die Nachwelt lesen,", "tokens": ["So", "w\u00fcrd", "ein\u00b7mal", "die", "Nach\u00b7welt", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Wie hold ", "tokens": ["Wie", "hold"], "token_info": ["word", "word"], "pos": ["PWAV", "ADJD"], "meter": "-+", "measure": "iambic.single"}}, "stanza.81": {"line.1": {"text": "Die Welt erkennts, ", "tokens": ["Die", "Welt", "er\u00b7kennts", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Wie sanft das Musenvolk bey Sachsens Schwertern sitzet;", "tokens": ["Wie", "sanft", "das", "Mu\u00b7sen\u00b7volk", "bey", "Sach\u00b7sens", "Schwer\u00b7tern", "sit\u00b7zet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ART", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wenn Mars gleich auch allhier geblitzet,", "tokens": ["Wenn", "Mars", "gleich", "auch", "all\u00b7hier", "ge\u00b7blit\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "ADV", "ADV", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und selbst dem Pindus schrecklich war.", "tokens": ["Und", "selbst", "dem", "Pin\u00b7dus", "schreck\u00b7lich", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Des Himmels Schild beschirm forthin", "tokens": ["Des", "Him\u00b7mels", "Schild", "be\u00b7schirm", "for\u00b7thin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Sein Wohlstand wird uns allen eigen;", "tokens": ["Sein", "Wohl\u00b7stand", "wird", "uns", "al\u00b7len", "ei\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ihr unverr\u00fcckter Flor ist unsers Chors Gewinn.", "tokens": ["Ihr", "un\u00b7ver\u00b7r\u00fcck\u00b7ter", "Flor", "ist", "un\u00b7sers", "Chors", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.82": {"line.1": {"text": "Wo kann das Wissen sch\u00f6ner bl\u00fchen,", "tokens": ["Wo", "kann", "das", "Wis\u00b7sen", "sch\u00f6\u00b7ner", "bl\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als wo die F\u00fcrsten selbst sich um sein Wohl bem\u00fchen?", "tokens": ["Als", "wo", "die", "F\u00fcrs\u00b7ten", "selbst", "sich", "um", "sein", "Wohl", "be\u00b7m\u00fc\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "ART", "NN", "ADV", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}