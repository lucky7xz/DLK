{"dta.poem.280": {"metadata": {"author": {"name": "Gressel, Johann Georg", "birth": "N.A.", "death": "N.A."}, "title": "An den Wider-Hall.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1716", "urn": "urn:nbn:de:kobv:b4-200905199041", "language": ["de:0.99"], "booktitle": "Celander [i. e. Gressel, Johann Georg]: Verliebte-Galante/ Sinn-Vermischte und Grab-Gedichte. Hamburg u. a., 1716."}, "poem": {"stanza.1": {"line.1": {"text": "Stiller Ort/ begl\u00fcckte Schatten/", "tokens": ["Stil\u00b7ler", "Ort", "/", "be\u00b7gl\u00fcck\u00b7te", "Schat\u00b7ten", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "NN", "$(", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Echo Sitz und Wunder-Haus", "tokens": ["E\u00b7cho", "Sitz", "und", "Wun\u00b7der\u00b7Haus"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "G\u00f6nnt/ da\u00df ich auf euren Matten", "tokens": ["G\u00f6nnt", "/", "da\u00df", "ich", "auf", "eu\u00b7ren", "Mat\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "KOUS", "PPER", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sch\u00fctte meine Seuffzer aus;", "tokens": ["Sch\u00fct\u00b7te", "mei\u00b7ne", "Seuff\u00b7zer", "aus", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn meine Seuffzer/ meine Z\u00e4hren", "tokens": ["Denn", "mei\u00b7ne", "Seuff\u00b7zer", "/", "mei\u00b7ne", "Z\u00e4h\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "$(", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Will/ mir zum Troste/ keiner h\u00f6ren.", "tokens": ["Will", "/", "mir", "zum", "Tros\u00b7te", "/", "kei\u00b7ner", "h\u00f6\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "$(", "PPER", "APPRART", "NN", "$(", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Echo. Erh\u00f6ren.", "tokens": ["E\u00b7cho", ".", "Er\u00b7h\u00f6\u00b7ren", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.2": {"line.1": {"text": "Das Erh\u00f6ren ist verschwunden/", "tokens": ["Das", "Er\u00b7h\u00f6\u00b7ren", "ist", "ver\u00b7schwun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mein Schatz ist in jener Welt/", "tokens": ["Mein", "Schatz", "ist", "in", "je\u00b7ner", "Welt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "PDAT", "NN", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und der Schmertz vonmeinen Wunden", "tokens": ["Und", "der", "Schmertz", "von\u00b7mei\u00b7nen", "Wun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat mein Hertz schon halb gef\u00e4llt.", "tokens": ["Hat", "mein", "Hertz", "schon", "halb", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ach dein Erh\u00f6ren ist ein Meynen/", "tokens": ["Ach", "dein", "Er\u00b7h\u00f6\u00b7ren", "ist", "ein", "Mey\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PPOSAT", "NN", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mir wird die Sonne nimmer scheinen.", "tokens": ["Mir", "wird", "die", "Son\u00b7ne", "nim\u00b7mer", "schei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Echo. Erscheinen.", "tokens": ["E\u00b7cho", ".", "Er\u00b7schei\u00b7nen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.3": {"line.1": {"text": "Wird des Gl\u00fcckes froher Morgen", "tokens": ["Wird", "des", "Gl\u00fc\u00b7ckes", "fro\u00b7her", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nach den St\u00fcrmen mir aufgehn?", "tokens": ["Nach", "den", "St\u00fcr\u00b7men", "mir", "auf\u00b7gehn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wird der Himmel vor mich sorgen?", "tokens": ["Wird", "der", "Him\u00b7mel", "vor", "mich", "sor\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "PPER", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Soll ich mich erfreuet sehn?", "tokens": ["Soll", "ich", "mich", "er\u00b7freu\u00b7et", "sehn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "VVFIN", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ach Nein! ach Nein! ich bin verlassen", "tokens": ["Ach", "Nein", "!", "ach", "Nein", "!", "ich", "bin", "ver\u00b7las\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "PTKANT", "$.", "XY", "PTKANT", "$.", "PPER", "VAFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ich kan sie nicht wieder umfassen.", "tokens": ["Ich", "kan", "sie", "nicht", "wie\u00b7der", "um\u00b7fas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.7": {"text": "Ech. Umfassen.", "tokens": ["Ech", ".", "Um\u00b7fas\u00b7sen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Ich umfasse ihre Br\u00fcste!", "tokens": ["Ich", "um\u00b7fas\u00b7se", "ih\u00b7re", "Br\u00fcs\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nein/ ach Nein! das kan nicht seyn", "tokens": ["Nein", "/", "ach", "Nein", "!", "das", "kan", "nicht", "seyn"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$(", "XY", "PTKANT", "$.", "PDS", "VMFIN", "PTKNEG", "VAINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn zu kosten s\u00fcsse L\u00fcste", "tokens": ["Denn", "zu", "kos\u00b7ten", "s\u00fcs\u00b7se", "L\u00fcs\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ADJA", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Geht mit Todten keiner ein;", "tokens": ["Geht", "mit", "Tod\u00b7ten", "kei\u00b7ner", "ein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "PIS", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Den Todt und Grab mag ich umfangen/", "tokens": ["Den", "Todt", "und", "Grab", "mag", "ich", "um\u00b7fan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Echo. Erlangen.", "tokens": ["E\u00b7cho", ".", "Er\u00b7lan\u00b7gen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.5": {"line.1": {"text": "Nimmer und zu keinen Zeiten", "tokens": ["Nim\u00b7mer", "und", "zu", "kei\u00b7nen", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KON", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Echo wird dein Reden wahr/", "tokens": ["E\u00b7cho", "wird", "dein", "Re\u00b7den", "wahr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPOSAT", "NN", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Da mich Augst und Noht bestreiten", "tokens": ["Da", "mich", "Augst", "und", "Noht", "be\u00b7strei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "NN", "KON", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wird mir kein Befreyungs-Jahr;", "tokens": ["Wird", "mir", "kein", "Be\u00b7freyungs\u00b7Jahr", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "NN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Denn von den Banden/ von den Ketten", "tokens": ["Denn", "von", "den", "Ban\u00b7den", "/", "von", "den", "Ket\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "$(", "APPR", "ART", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Wird nun/ sie todt/ mich keiner retten.", "tokens": ["Wird", "nun", "/", "sie", "todt", "/", "mich", "kei\u00b7ner", "ret\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$(", "PPER", "ADJD", "$(", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Echo. Erretten.", "tokens": ["E\u00b7cho", ".", "Er\u00b7ret\u00b7ten", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}}}}