{"textgrid.poem.53888": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Gebet des Zeitungslesers", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Du lieber Gott, so h\u00f6r mein leises Flehen!", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "so", "h\u00f6r", "mein", "lei\u00b7ses", "Fle\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "ADV", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Tu auf den Packen hier heruntersehen!", "tokens": ["Tu", "auf", "den", "Pa\u00b7cken", "hier", "her\u00b7un\u00b7ter\u00b7se\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Du lieber Gott, ich pfeif am letzten Loche:", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "ich", "pfeif", "am", "letz\u00b7ten", "Lo\u00b7che", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "PPER", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "das sind die Zeitungen von einer Woche!", "tokens": ["das", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7ner", "Wo\u00b7che", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Die mu\u00df ich alle, alle lesen:", "tokens": ["Die", "mu\u00df", "ich", "al\u00b7le", ",", "al\u00b7le", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PIS", "$,", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Vom B\u00fcrgerkrieg bei Nord- und S\u00fcdchinesen;", "tokens": ["Vom", "B\u00fcr\u00b7ger\u00b7krieg", "bei", "Nord", "und", "S\u00fcd\u00b7chi\u00b7ne\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "TRUNC", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "vom Turnerfest mit Gr\u00e4tsche und mit Kippe;", "tokens": ["vom", "Tur\u00b7ner\u00b7fest", "mit", "Gr\u00e4t\u00b7sche", "und", "mit", "Kip\u00b7pe", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "vom Flaggenstreit in Schaumburg-Lippe;", "tokens": ["vom", "Flag\u00b7gen\u00b7streit", "in", "Schaum\u00b7bur\u00b7g\u00b7Lip\u00b7pe", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "von Abegg, L\u00fcbeck, Ahlbeck, Becker;", "tokens": ["von", "A\u00b7begg", ",", "L\u00fc\u00b7beck", ",", "Ahl\u00b7beck", ",", "Be\u00b7cker", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NN", "$,", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "von Schnillers Testamentsvollstrecker;", "tokens": ["von", "Schnil\u00b7lers", "Tes\u00b7ta\u00b7ments\u00b7voll\u00b7stre\u00b7cker", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "vom Prinz von Wales und von Richard Strauss \u2013", "tokens": ["vom", "Prinz", "von", "Wa\u00b7les", "und", "von", "Ric\u00b7hard", "Strauss", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NE", "KON", "APPR", "NE", "NE", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "das fliegt mir alles so ins Haus!", "tokens": ["das", "fliegt", "mir", "al\u00b7les", "so", "ins", "Haus", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PIS", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ich kaufs auch noch. Sobald ichs seh,", "tokens": ["Ich", "kaufs", "auch", "noch", ".", "So\u00b7bald", "ichs", "seh", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$.", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "fixe Idee:", "tokens": ["fi\u00b7xe", "I\u00b7dee", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.10": {"text": "\u00bbacht-Uhr-Abendblatt! Acht-Uhr! B.Z.! Die Nachtausgabe!\u00ab", "tokens": ["\u00bb", "acht\u00b7Uhr\u00b7A\u00b7bend\u00b7blatt", "!", "Acht\u00b7Uhr", "!", "B.", "Z.", "!", "Die", "Nach\u00b7taus\u00b7ga\u00b7be", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "abbreviation", "abbreviation", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ITJ", "$.", "NN", "$.", "NN", "NN", "$.", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Wo nur eine Zeitung ist, da trabe", "tokens": ["Wo", "nur", "ei\u00b7ne", "Zei\u00b7tung", "ist", ",", "da", "tra\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "VAFIN", "$,", "ADV", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "ich hin \u2013 aus Gier", "tokens": ["ich", "hin", "\u2013", "aus", "Gier"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "ADV", "$(", "APPR", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "nach Papier \u2013 immer nach Papier \u2013", "tokens": ["nach", "Pa\u00b7pier", "\u2013", "im\u00b7mer", "nach", "Pa\u00b7pier", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "ADV", "APPR", "NN", "$("], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.4": {"text": "bleib auf der Stra\u00dfe stehn und lese hier:", "tokens": ["bleib", "auf", "der", "Stra\u00b7\u00dfe", "stehn", "und", "le\u00b7se", "hier", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "VVINF", "KON", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Die westliche Ostsee ziemlich bewegt;", "tokens": ["Die", "west\u00b7li\u00b7che", "Ost\u00b7see", "ziem\u00b7lich", "be\u00b7wegt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Pola Negri endg\u00fcltig trocken gelegt;", "tokens": ["Po\u00b7la", "Ne\u00b7gri", "end\u00b7g\u00fcl\u00b7tig", "tro\u00b7cken", "ge\u00b7legt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADJD", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Churchill gest\u00fcrzt \u2013 die Kammer tobt;", "tokens": ["Chur\u00b7chill", "ge\u00b7st\u00fcrzt", "\u2013", "die", "Kam\u00b7mer", "tobt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$(", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der Papst mit Mary Wigman verlobt;", "tokens": ["der", "Papst", "mit", "Ma\u00b7ry", "Wig\u00b7man", "ver\u00b7lobt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "NE", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "(das ist ihm recht!) \u2013 Sturm auf den Azoren;", "tokens": ["(", "das", "ist", "ihm", "recht", "!", ")", "\u2013", "Sturm", "auf", "den", "A\u00b7zo\u00b7ren", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "PPER", "ADJD", "$.", "$(", "$(", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ludendorffs Dackel hat seinen Schwanz verloren;", "tokens": ["Lu\u00b7den\u00b7dorffs", "Da\u00b7ckel", "hat", "sei\u00b7nen", "Schwanz", "ver\u00b7lo\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.7": {"text": "in Gr\u00f6nland Badehosenhausse;", "tokens": ["in", "Gr\u00f6n\u00b7land", "Ba\u00b7de\u00b7ho\u00b7sen\u00b7haus\u00b7se", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Pallenberg hundertmal in einer Posse;", "tokens": ["Pal\u00b7len\u00b7berg", "hun\u00b7dert\u00b7mal", "in", "ei\u00b7ner", "Pos\u00b7se", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Verfilmung des Dramas Ain und Kabels;", "tokens": ["Ver\u00b7fil\u00b7mung", "des", "Dra\u00b7mas", "Ain", "und", "Ka\u00b7bels", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NE", "KON", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Pr\u00e4miierung des kleinsten Damennabels;", "tokens": ["Pr\u00e4\u00b7mii\u00b7e\u00b7rung", "des", "kleins\u00b7ten", "Da\u00b7men\u00b7na\u00b7bels", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.11": {"text": "Mussolini und das schwarze Hemd seiner Amme \u2013", "tokens": ["Mus\u00b7so\u00b7li\u00b7ni", "und", "das", "schwar\u00b7ze", "Hemd", "sei\u00b7ner", "Am\u00b7me", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ART", "ADJA", "NN", "PPOSAT", "NN", "$("], "meter": "+---+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.12": {"text": "Nachrichten, Nachrichten, Telegramme, Telegramme, Telegramme \u2013", "tokens": ["Nach\u00b7rich\u00b7ten", ",", "Nach\u00b7rich\u00b7ten", ",", "Te\u00b7le\u00b7gram\u00b7me", ",", "Te\u00b7le\u00b7gram\u00b7me", ",", "Te\u00b7le\u00b7gram\u00b7me", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$("], "meter": "-+--+-+-+-+-+-+-+-", "measure": "iambic.octa.plus.relaxed"}}, "stanza.5": {"line.1": {"text": "Was geht denn mich das an?", "tokens": ["Was", "geht", "denn", "mich", "das", "an", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "KON", "PPER", "PDS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Das geht mich gar nichts an!", "tokens": ["Das", "geht", "mich", "gar", "nichts", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PIS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Das geht mich gar nichts an!", "tokens": ["Das", "geht", "mich", "gar", "nichts", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PIS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "In den Beilagen raschelt und zischelt der Wind \u2013", "tokens": ["In", "den", "Bei\u00b7la\u00b7gen", "ra\u00b7schelt", "und", "zi\u00b7schelt", "der", "Wind", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "KON", "VVFIN", "ART", "NN", "$("], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.2": {"text": "Ich bin ein armes zerlesenes Kind . . .", "tokens": ["Ich", "bin", "ein", "ar\u00b7mes", "zer\u00b7le\u00b7se\u00b7nes", "Kind", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Hat keiner mit mir Armen", "tokens": ["Hat", "kei\u00b7ner", "mit", "mir", "Ar\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "APPR", "PPER", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Erbarmen?", "tokens": ["Er\u00b7bar\u00b7men", "?"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.7": {"line.1": {"text": "Man sagt von IHM, da\u00df ER doch auch nen Sohn hat . . .", "tokens": ["Man", "sagt", "von", "IhM", ",", "da\u00df", "Er", "doch", "auch", "nen", "Sohn", "hat", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NE", "$,", "KOUS", "PPER", "ADV", "ADV", "ADJA", "NN", "VAFIN", "$.", "$.", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das sind die Zeitungen von einem Monat!", "tokens": ["Das", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7nem", "Mo\u00b7nat", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+---+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Wenn ich sie seh: mich schaudert und mich graust \u2013", "tokens": ["Wenn", "ich", "sie", "seh", ":", "mich", "schau\u00b7dert", "und", "mich", "graust", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "KON", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "was kommt da noch auf mich herabgebraust?", "tokens": ["was", "kommt", "da", "noch", "auf", "mich", "her\u00b7ab\u00b7ge\u00b7braust", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ADV", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Befrei mich Du vom irdischen B\u00f6sen.", "tokens": ["Be\u00b7frei", "mich", "Du", "vom", "ir\u00b7di\u00b7schen", "B\u00f6\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Warum mu\u00df ich denn Silbenr\u00e4tsel l\u00f6sen?", "tokens": ["Wa\u00b7rum", "mu\u00df", "ich", "denn", "Sil\u00b7ben\u00b7r\u00e4t\u00b7sel", "l\u00f6\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "ADV", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Was kostets mich f\u00fcr lange Stunden", "tokens": ["Was", "kos\u00b7tets", "mich", "f\u00fcr", "lan\u00b7ge", "Stun\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "bis ich: \u00bbM\u00e4tresse unter Ludwig XVI.\u00ab gefunden \u2013", "tokens": ["bis", "ich", ":", "\u00bb", "M\u00e4\u00b7tres\u00b7se", "un\u00b7ter", "Lud\u00b7wig", "XvI", ".", "\u00ab", "ge\u00b7fun\u00b7den", "\u2013"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$.", "$(", "NN", "APPR", "NE", "NE", "$.", "$(", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Aufl\u00f6sung: \u00bbNichtsw\u00fcrdig ist die Nation.\u00ab", "tokens": ["Auf\u00b7l\u00f6\u00b7sung", ":", "\u00bb", "Nichts\u00b7w\u00fcr\u00b7dig", "ist", "die", "Na\u00b7tion", ".", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "$(", "ADJD", "VAFIN", "ART", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Oder: \u00bbDu sollst nicht t\u00f6ten, spricht der Gottessohn!\u00ab", "tokens": ["O\u00b7der", ":", "\u00bb", "Du", "sollst", "nicht", "t\u00f6\u00b7ten", ",", "spricht", "der", "Got\u00b7tes\u00b7sohn", "!", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$.", "$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Es ist manchmal ein Kreuz mit Deinem Wort!", "tokens": ["Es", "ist", "manch\u00b7mal", "ein", "Kreuz", "mit", "Dei\u00b7nem", "Wort", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "nimm doch die Kreuzwortr\u00e4tsel fort . . .", "tokens": ["nimm", "doch", "die", "Kreuz\u00b7wort\u00b7r\u00e4t\u00b7sel", "fort", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVIMP", "ADV", "ART", "NN", "PTKVZ", "$.", "$.", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.9": {"text": "So pl\u00e4tschert das tagaus, tagein,", "tokens": ["So", "pl\u00e4t\u00b7schert", "das", "ta\u00b7gaus", ",", "ta\u00b7gein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "auf mich, den armen Leser herein \u2013", "tokens": ["auf", "mich", ",", "den", "ar\u00b7men", "Le\u00b7ser", "her\u00b7ein", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.9": {"line.1": {"text": "Papier! Papier! Von welchem Riesenbaume", "tokens": ["Pa\u00b7pier", "!", "Pa\u00b7pier", "!", "Von", "wel\u00b7chem", "Rie\u00b7sen\u00b7bau\u00b7me"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NN", "$.", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "verflattert das in unserm Erdenraume?", "tokens": ["ver\u00b7flat\u00b7tert", "das", "in", "un\u00b7serm", "Er\u00b7den\u00b7rau\u00b7me", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Papier! Papier! Genug! Genug des Segens!", "tokens": ["Pa\u00b7pier", "!", "Pa\u00b7pier", "!", "Ge\u00b7nug", "!", "Ge\u00b7nug", "des", "Se\u00b7gens", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "ADV", "$.", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ertr\u00e4nk mich nicht, du Flut des Zeitungsregens!", "tokens": ["Er\u00b7tr\u00e4nk", "mich", "nicht", ",", "du", "Flut", "des", "Zei\u00b7tungs\u00b7re\u00b7gens", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "$,", "PPER", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Hier sind die Fahnen aller Staaten!", "tokens": ["Hier", "sind", "die", "Fah\u00b7nen", "al\u00b7ler", "Staa\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Allons, journaux de la patrie!", "tokens": ["Al\u00b7lons", ",", "jour\u00b7naux", "de", "la", "pa\u00b7trie", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "FM", "FM", "FM", "FM", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich kann in Zeitungen schwimmen \u2013 in Zeitungen waten \u2013", "tokens": ["Ich", "kann", "in", "Zei\u00b7tun\u00b7gen", "schwim\u00b7men", "\u2013", "in", "Zei\u00b7tun\u00b7gen", "wa\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "NN", "VVINF", "$(", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+--+--+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "aber ohne Zeitungen sein: das kann ich nie!", "tokens": ["a\u00b7ber", "oh\u00b7ne", "Zei\u00b7tun\u00b7gen", "sein", ":", "das", "kann", "ich", "nie", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VAINF", "$.", "PDS", "VMFIN", "PPER", "ADV", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.5": {"text": "Wie sie mich qu\u00e4len,", "tokens": ["Wie", "sie", "mich", "qu\u00e4\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "VVINF", "$,"], "meter": "++-+-", "measure": "iambic.di"}, "line.6": {"text": "t\u00f6ten beinah \u2013", "tokens": ["t\u00f6\u00b7ten", "bei\u00b7nah", "\u2013"], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "ADV", "$("], "meter": "+--+", "measure": "iambic.di.chol"}, "line.7": {"text": "Und wie sie mir fehlen,", "tokens": ["Und", "wie", "sie", "mir", "feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.8": {"text": "wenn sie nicht da . . . !", "tokens": ["wenn", "sie", "nicht", "da", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "$.", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "Was soll mir das? Was hats f\u00fcr einen Sinn?", "tokens": ["Was", "soll", "mir", "das", "?", "Was", "hats", "f\u00fcr", "ei\u00b7nen", "Sinn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PDS", "$.", "PWS", "VAFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Mein ganzes Leben ging in Kleinigkeiten hin . . .", "tokens": ["Mein", "gan\u00b7zes", "Le\u00b7ben", "ging", "in", "Klei\u00b7nig\u00b7kei\u00b7ten", "hin", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Am j\u00fcngsten Tage des Gerichts,", "tokens": ["Am", "j\u00fcng\u00b7sten", "Ta\u00b7ge", "des", "Ge\u00b7richts", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "da werd ich sehn:", "tokens": ["da", "werd", "ich", "sehn", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Ich kam zu nichts.", "tokens": ["Ich", "kam", "zu", "nichts", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Zerteilt. Zerspielt. Zerspellt. Zerzettelt.", "tokens": ["Zer\u00b7teilt", ".", "Zer\u00b7spielt", ".", "Zer\u00b7spellt", ".", "Zer\u00b7zet\u00b7telt", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$.", "VVFIN", "$.", "VVFIN", "$.", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mein Lebtag hab ich nur um eins gebettelt:", "tokens": ["Mein", "Leb\u00b7tag", "hab", "ich", "nur", "um", "eins", "ge\u00b7bet\u00b7telt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "APPR", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "um Ruhe.", "tokens": ["um", "Ru\u00b7he", "."], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Du gabst sie nicht. So mu\u00df ich dienen,", "tokens": ["Du", "gabst", "sie", "nicht", ".", "So", "mu\u00df", "ich", "die\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "als Sklave aller Rotationsmaschinen.", "tokens": ["als", "Skla\u00b7ve", "al\u00b7ler", "Ro\u00b7ta\u00b7ti\u00b7ons\u00b7ma\u00b7schi\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Du lieber Gott, gebleicht ist all mein Haar.", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "ge\u00b7bleicht", "ist", "all", "mein", "Haar", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "VVPP", "VAFIN", "PIAT", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Hier sind die Zeitungen von einem Jahr . . . !", "tokens": ["Hier", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7nem", "Jahr", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Du hast mich ihnen g\u00e4nzlich preisgegeben \u2013", "tokens": ["Du", "hast", "mich", "ih\u00b7nen", "g\u00e4nz\u00b7lich", "preis\u00b7ge\u00b7ge\u00b7ben", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PPER", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "war das ein Leben \u2013 das mein Leben \u2013?", "tokens": ["war", "das", "ein", "Le\u00b7ben", "\u2013", "das", "mein", "Le\u00b7ben", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "$(", "PDS", "PPOSAT", "NN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ich merkte, welche Tageszeit grad war,", "tokens": ["Ich", "merk\u00b7te", ",", "wel\u00b7che", "Ta\u00b7ges\u00b7zeit", "grad", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAT", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "nur am ", "tokens": ["nur", "am"], "token_info": ["word", "word"], "pos": ["ADV", "APPRART"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Bis in die letzten Winkel meines Heims", "tokens": ["Bis", "in", "die", "letz\u00b7ten", "Win\u00b7kel", "mei\u00b7nes", "Heims"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "kam deine ", "tokens": ["kam", "dei\u00b7ne"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.14": {"line.1": {"text": "Verflucht die Bilder, die Plakate!", "tokens": ["Ver\u00b7flucht", "die", "Bil\u00b7der", ",", "die", "Pla\u00b7ka\u00b7te", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die Leitartikel, Inserate!", "tokens": ["die", "Leit\u00b7ar\u00b7ti\u00b7kel", ",", "In\u00b7se\u00b7ra\u00b7te", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "die Neuigkeit, die, kaum geboren, alt!", "tokens": ["die", "Neu\u00b7ig\u00b7keit", ",", "die", ",", "kaum", "ge\u00b7bo\u00b7ren", ",", "alt", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "$,", "ADV", "VVPP", "$,", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "das Blatt am Baum \u2013 der ganze Bl\u00e4tterwald!", "tokens": ["das", "Blatt", "am", "Baum", "\u2013", "der", "gan\u00b7ze", "Bl\u00e4t\u00b7ter\u00b7wald", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Verflucht! Verflucht die Menschenfibel!", "tokens": ["Ver\u00b7flucht", "!", "Ver\u00b7flucht", "die", "Men\u00b7schen\u00b7fi\u00b7bel", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "verflucht die Inseratenbibel!", "tokens": ["ver\u00b7flucht", "die", "In\u00b7se\u00b7ra\u00b7ten\u00b7bi\u00b7bel", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.7": {"text": "Ruhm: Durch die Zeitung. Heirat: durch die Zeitung.", "tokens": ["Ruhm", ":", "Durch", "die", "Zei\u00b7tung", ".", "Hei\u00b7rat", ":", "durch", "die", "Zei\u00b7tung", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "$.", "NN", "$.", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Krieg: Durch die Zeitung. Friede: durch die Zeitung.", "tokens": ["Krieg", ":", "Durch", "die", "Zei\u00b7tung", ".", "Frie\u00b7de", ":", "durch", "die", "Zei\u00b7tung", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "$.", "NN", "$.", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Nimm sie von mir! Die Zeitung triumphiert!", "tokens": ["Nimm", "sie", "von", "mir", "!", "Die", "Zei\u00b7tung", "tri\u00b7um\u00b7phiert", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPR", "PPER", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Es hilft ja nichts.", "tokens": ["Es", "hilft", "ja", "nichts", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Du bist ja sicher", "tokens": ["Du", "bist", "ja", "si\u00b7cher"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "selber", "tokens": ["sel\u00b7ber"], "token_info": ["word"], "pos": ["ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "abonniert . . .", "tokens": ["a\u00b7bon\u00b7niert", ".", ".", "."], "token_info": ["word", "punct", "punct", "punct"], "pos": ["VVFIN", "$.", "$.", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.16": {"line.1": {"text": "Du lieber Gott, so h\u00f6r mein leises Flehen!", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "so", "h\u00f6r", "mein", "lei\u00b7ses", "Fle\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "ADV", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Tu auf den Packen hier heruntersehen!", "tokens": ["Tu", "auf", "den", "Pa\u00b7cken", "hier", "her\u00b7un\u00b7ter\u00b7se\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Du lieber Gott, ich pfeif am letzten Loche:", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "ich", "pfeif", "am", "letz\u00b7ten", "Lo\u00b7che", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "PPER", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "das sind die Zeitungen von einer Woche!", "tokens": ["das", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7ner", "Wo\u00b7che", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Die mu\u00df ich alle, alle lesen:", "tokens": ["Die", "mu\u00df", "ich", "al\u00b7le", ",", "al\u00b7le", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PIS", "$,", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Vom B\u00fcrgerkrieg bei Nord- und S\u00fcdchinesen;", "tokens": ["Vom", "B\u00fcr\u00b7ger\u00b7krieg", "bei", "Nord", "und", "S\u00fcd\u00b7chi\u00b7ne\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "TRUNC", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "vom Turnerfest mit Gr\u00e4tsche und mit Kippe;", "tokens": ["vom", "Tur\u00b7ner\u00b7fest", "mit", "Gr\u00e4t\u00b7sche", "und", "mit", "Kip\u00b7pe", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "vom Flaggenstreit in Schaumburg-Lippe;", "tokens": ["vom", "Flag\u00b7gen\u00b7streit", "in", "Schaum\u00b7bur\u00b7g\u00b7Lip\u00b7pe", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "von Abegg, L\u00fcbeck, Ahlbeck, Becker;", "tokens": ["von", "A\u00b7begg", ",", "L\u00fc\u00b7beck", ",", "Ahl\u00b7beck", ",", "Be\u00b7cker", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NN", "$,", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "von Schnillers Testamentsvollstrecker;", "tokens": ["von", "Schnil\u00b7lers", "Tes\u00b7ta\u00b7ments\u00b7voll\u00b7stre\u00b7cker", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "vom Prinz von Wales und von Richard Strauss \u2013", "tokens": ["vom", "Prinz", "von", "Wa\u00b7les", "und", "von", "Ric\u00b7hard", "Strauss", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NE", "KON", "APPR", "NE", "NE", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "das fliegt mir alles so ins Haus!", "tokens": ["das", "fliegt", "mir", "al\u00b7les", "so", "ins", "Haus", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PIS", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ich kaufs auch noch. Sobald ichs seh,", "tokens": ["Ich", "kaufs", "auch", "noch", ".", "So\u00b7bald", "ichs", "seh", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$.", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "fixe Idee:", "tokens": ["fi\u00b7xe", "I\u00b7dee", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.10": {"text": "\u00bbacht-Uhr-Abendblatt! Acht-Uhr! B.Z.! Die Nachtausgabe!\u00ab", "tokens": ["\u00bb", "acht\u00b7Uhr\u00b7A\u00b7bend\u00b7blatt", "!", "Acht\u00b7Uhr", "!", "B.", "Z.", "!", "Die", "Nach\u00b7taus\u00b7ga\u00b7be", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "abbreviation", "abbreviation", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ITJ", "$.", "NN", "$.", "NN", "NN", "$.", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.18": {"line.1": {"text": "Wo nur eine Zeitung ist, da trabe", "tokens": ["Wo", "nur", "ei\u00b7ne", "Zei\u00b7tung", "ist", ",", "da", "tra\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "VAFIN", "$,", "ADV", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "ich hin \u2013 aus Gier", "tokens": ["ich", "hin", "\u2013", "aus", "Gier"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "ADV", "$(", "APPR", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "nach Papier \u2013 immer nach Papier \u2013", "tokens": ["nach", "Pa\u00b7pier", "\u2013", "im\u00b7mer", "nach", "Pa\u00b7pier", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "ADV", "APPR", "NN", "$("], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.4": {"text": "bleib auf der Stra\u00dfe stehn und lese hier:", "tokens": ["bleib", "auf", "der", "Stra\u00b7\u00dfe", "stehn", "und", "le\u00b7se", "hier", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "VVINF", "KON", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.19": {"line.1": {"text": "Die westliche Ostsee ziemlich bewegt;", "tokens": ["Die", "west\u00b7li\u00b7che", "Ost\u00b7see", "ziem\u00b7lich", "be\u00b7wegt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Pola Negri endg\u00fcltig trocken gelegt;", "tokens": ["Po\u00b7la", "Ne\u00b7gri", "end\u00b7g\u00fcl\u00b7tig", "tro\u00b7cken", "ge\u00b7legt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADJD", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Churchill gest\u00fcrzt \u2013 die Kammer tobt;", "tokens": ["Chur\u00b7chill", "ge\u00b7st\u00fcrzt", "\u2013", "die", "Kam\u00b7mer", "tobt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$(", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der Papst mit Mary Wigman verlobt;", "tokens": ["der", "Papst", "mit", "Ma\u00b7ry", "Wig\u00b7man", "ver\u00b7lobt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "NE", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "(das ist ihm recht!) \u2013 Sturm auf den Azoren;", "tokens": ["(", "das", "ist", "ihm", "recht", "!", ")", "\u2013", "Sturm", "auf", "den", "A\u00b7zo\u00b7ren", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "PPER", "ADJD", "$.", "$(", "$(", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ludendorffs Dackel hat seinen Schwanz verloren;", "tokens": ["Lu\u00b7den\u00b7dorffs", "Da\u00b7ckel", "hat", "sei\u00b7nen", "Schwanz", "ver\u00b7lo\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.7": {"text": "in Gr\u00f6nland Badehosenhausse;", "tokens": ["in", "Gr\u00f6n\u00b7land", "Ba\u00b7de\u00b7ho\u00b7sen\u00b7haus\u00b7se", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Pallenberg hundertmal in einer Posse;", "tokens": ["Pal\u00b7len\u00b7berg", "hun\u00b7dert\u00b7mal", "in", "ei\u00b7ner", "Pos\u00b7se", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Verfilmung des Dramas Ain und Kabels;", "tokens": ["Ver\u00b7fil\u00b7mung", "des", "Dra\u00b7mas", "Ain", "und", "Ka\u00b7bels", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NE", "KON", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Pr\u00e4miierung des kleinsten Damennabels;", "tokens": ["Pr\u00e4\u00b7mii\u00b7e\u00b7rung", "des", "kleins\u00b7ten", "Da\u00b7men\u00b7na\u00b7bels", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.11": {"text": "Mussolini und das schwarze Hemd seiner Amme \u2013", "tokens": ["Mus\u00b7so\u00b7li\u00b7ni", "und", "das", "schwar\u00b7ze", "Hemd", "sei\u00b7ner", "Am\u00b7me", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ART", "ADJA", "NN", "PPOSAT", "NN", "$("], "meter": "+---+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.12": {"text": "Nachrichten, Nachrichten, Telegramme, Telegramme, Telegramme \u2013", "tokens": ["Nach\u00b7rich\u00b7ten", ",", "Nach\u00b7rich\u00b7ten", ",", "Te\u00b7le\u00b7gram\u00b7me", ",", "Te\u00b7le\u00b7gram\u00b7me", ",", "Te\u00b7le\u00b7gram\u00b7me", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$("], "meter": "-+--+-+-+-+-+-+-+-", "measure": "iambic.octa.plus.relaxed"}}, "stanza.20": {"line.1": {"text": "Was geht denn mich das an?", "tokens": ["Was", "geht", "denn", "mich", "das", "an", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "KON", "PPER", "PDS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Das geht mich gar nichts an!", "tokens": ["Das", "geht", "mich", "gar", "nichts", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PIS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Das geht mich gar nichts an!", "tokens": ["Das", "geht", "mich", "gar", "nichts", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PIS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "In den Beilagen raschelt und zischelt der Wind \u2013", "tokens": ["In", "den", "Bei\u00b7la\u00b7gen", "ra\u00b7schelt", "und", "zi\u00b7schelt", "der", "Wind", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "KON", "VVFIN", "ART", "NN", "$("], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.2": {"text": "Ich bin ein armes zerlesenes Kind . . .", "tokens": ["Ich", "bin", "ein", "ar\u00b7mes", "zer\u00b7le\u00b7se\u00b7nes", "Kind", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Hat keiner mit mir Armen", "tokens": ["Hat", "kei\u00b7ner", "mit", "mir", "Ar\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "APPR", "PPER", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Erbarmen?", "tokens": ["Er\u00b7bar\u00b7men", "?"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.22": {"line.1": {"text": "Man sagt von IHM, da\u00df ER doch auch nen Sohn hat . . .", "tokens": ["Man", "sagt", "von", "IhM", ",", "da\u00df", "Er", "doch", "auch", "nen", "Sohn", "hat", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NE", "$,", "KOUS", "PPER", "ADV", "ADV", "ADJA", "NN", "VAFIN", "$.", "$.", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das sind die Zeitungen von einem Monat!", "tokens": ["Das", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7nem", "Mo\u00b7nat", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+---+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Wenn ich sie seh: mich schaudert und mich graust \u2013", "tokens": ["Wenn", "ich", "sie", "seh", ":", "mich", "schau\u00b7dert", "und", "mich", "graust", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "KON", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "was kommt da noch auf mich herabgebraust?", "tokens": ["was", "kommt", "da", "noch", "auf", "mich", "her\u00b7ab\u00b7ge\u00b7braust", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ADV", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.23": {"line.1": {"text": "Befrei mich Du vom irdischen B\u00f6sen.", "tokens": ["Be\u00b7frei", "mich", "Du", "vom", "ir\u00b7di\u00b7schen", "B\u00f6\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Warum mu\u00df ich denn Silbenr\u00e4tsel l\u00f6sen?", "tokens": ["Wa\u00b7rum", "mu\u00df", "ich", "denn", "Sil\u00b7ben\u00b7r\u00e4t\u00b7sel", "l\u00f6\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "ADV", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Was kostets mich f\u00fcr lange Stunden", "tokens": ["Was", "kos\u00b7tets", "mich", "f\u00fcr", "lan\u00b7ge", "Stun\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "bis ich: \u00bbM\u00e4tresse unter Ludwig XVI.\u00ab gefunden \u2013", "tokens": ["bis", "ich", ":", "\u00bb", "M\u00e4\u00b7tres\u00b7se", "un\u00b7ter", "Lud\u00b7wig", "XvI", ".", "\u00ab", "ge\u00b7fun\u00b7den", "\u2013"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$.", "$(", "NN", "APPR", "NE", "NE", "$.", "$(", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Aufl\u00f6sung: \u00bbNichtsw\u00fcrdig ist die Nation.\u00ab", "tokens": ["Auf\u00b7l\u00f6\u00b7sung", ":", "\u00bb", "Nichts\u00b7w\u00fcr\u00b7dig", "ist", "die", "Na\u00b7tion", ".", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "$(", "ADJD", "VAFIN", "ART", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Oder: \u00bbDu sollst nicht t\u00f6ten, spricht der Gottessohn!\u00ab", "tokens": ["O\u00b7der", ":", "\u00bb", "Du", "sollst", "nicht", "t\u00f6\u00b7ten", ",", "spricht", "der", "Got\u00b7tes\u00b7sohn", "!", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$.", "$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Es ist manchmal ein Kreuz mit Deinem Wort!", "tokens": ["Es", "ist", "manch\u00b7mal", "ein", "Kreuz", "mit", "Dei\u00b7nem", "Wort", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "nimm doch die Kreuzwortr\u00e4tsel fort . . .", "tokens": ["nimm", "doch", "die", "Kreuz\u00b7wort\u00b7r\u00e4t\u00b7sel", "fort", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVIMP", "ADV", "ART", "NN", "PTKVZ", "$.", "$.", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.9": {"text": "So pl\u00e4tschert das tagaus, tagein,", "tokens": ["So", "pl\u00e4t\u00b7schert", "das", "ta\u00b7gaus", ",", "ta\u00b7gein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "auf mich, den armen Leser herein \u2013", "tokens": ["auf", "mich", ",", "den", "ar\u00b7men", "Le\u00b7ser", "her\u00b7ein", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.24": {"line.1": {"text": "Papier! Papier! Von welchem Riesenbaume", "tokens": ["Pa\u00b7pier", "!", "Pa\u00b7pier", "!", "Von", "wel\u00b7chem", "Rie\u00b7sen\u00b7bau\u00b7me"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NN", "$.", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "verflattert das in unserm Erdenraume?", "tokens": ["ver\u00b7flat\u00b7tert", "das", "in", "un\u00b7serm", "Er\u00b7den\u00b7rau\u00b7me", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Papier! Papier! Genug! Genug des Segens!", "tokens": ["Pa\u00b7pier", "!", "Pa\u00b7pier", "!", "Ge\u00b7nug", "!", "Ge\u00b7nug", "des", "Se\u00b7gens", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "ADV", "$.", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ertr\u00e4nk mich nicht, du Flut des Zeitungsregens!", "tokens": ["Er\u00b7tr\u00e4nk", "mich", "nicht", ",", "du", "Flut", "des", "Zei\u00b7tungs\u00b7re\u00b7gens", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "$,", "PPER", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.25": {"line.1": {"text": "Hier sind die Fahnen aller Staaten!", "tokens": ["Hier", "sind", "die", "Fah\u00b7nen", "al\u00b7ler", "Staa\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Allons, journaux de la patrie!", "tokens": ["Al\u00b7lons", ",", "jour\u00b7naux", "de", "la", "pa\u00b7trie", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "FM", "FM", "FM", "FM", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich kann in Zeitungen schwimmen \u2013 in Zeitungen waten \u2013", "tokens": ["Ich", "kann", "in", "Zei\u00b7tun\u00b7gen", "schwim\u00b7men", "\u2013", "in", "Zei\u00b7tun\u00b7gen", "wa\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "NN", "VVINF", "$(", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+--+--+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "aber ohne Zeitungen sein: das kann ich nie!", "tokens": ["a\u00b7ber", "oh\u00b7ne", "Zei\u00b7tun\u00b7gen", "sein", ":", "das", "kann", "ich", "nie", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VAINF", "$.", "PDS", "VMFIN", "PPER", "ADV", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.5": {"text": "Wie sie mich qu\u00e4len,", "tokens": ["Wie", "sie", "mich", "qu\u00e4\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "VVINF", "$,"], "meter": "++-+-", "measure": "iambic.di"}, "line.6": {"text": "t\u00f6ten beinah \u2013", "tokens": ["t\u00f6\u00b7ten", "bei\u00b7nah", "\u2013"], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "ADV", "$("], "meter": "+--+", "measure": "iambic.di.chol"}, "line.7": {"text": "Und wie sie mir fehlen,", "tokens": ["Und", "wie", "sie", "mir", "feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.8": {"text": "wenn sie nicht da . . . !", "tokens": ["wenn", "sie", "nicht", "da", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "$.", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "Was soll mir das? Was hats f\u00fcr einen Sinn?", "tokens": ["Was", "soll", "mir", "das", "?", "Was", "hats", "f\u00fcr", "ei\u00b7nen", "Sinn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PDS", "$.", "PWS", "VAFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Mein ganzes Leben ging in Kleinigkeiten hin . . .", "tokens": ["Mein", "gan\u00b7zes", "Le\u00b7ben", "ging", "in", "Klei\u00b7nig\u00b7kei\u00b7ten", "hin", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Am j\u00fcngsten Tage des Gerichts,", "tokens": ["Am", "j\u00fcng\u00b7sten", "Ta\u00b7ge", "des", "Ge\u00b7richts", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "da werd ich sehn:", "tokens": ["da", "werd", "ich", "sehn", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.26": {"line.1": {"text": "Ich kam zu nichts.", "tokens": ["Ich", "kam", "zu", "nichts", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Zerteilt. Zerspielt. Zerspellt. Zerzettelt.", "tokens": ["Zer\u00b7teilt", ".", "Zer\u00b7spielt", ".", "Zer\u00b7spellt", ".", "Zer\u00b7zet\u00b7telt", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$.", "VVFIN", "$.", "VVFIN", "$.", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mein Lebtag hab ich nur um eins gebettelt:", "tokens": ["Mein", "Leb\u00b7tag", "hab", "ich", "nur", "um", "eins", "ge\u00b7bet\u00b7telt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "APPR", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "um Ruhe.", "tokens": ["um", "Ru\u00b7he", "."], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Du gabst sie nicht. So mu\u00df ich dienen,", "tokens": ["Du", "gabst", "sie", "nicht", ".", "So", "mu\u00df", "ich", "die\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "als Sklave aller Rotationsmaschinen.", "tokens": ["als", "Skla\u00b7ve", "al\u00b7ler", "Ro\u00b7ta\u00b7ti\u00b7ons\u00b7ma\u00b7schi\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.27": {"line.1": {"text": "Du lieber Gott, gebleicht ist all mein Haar.", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "ge\u00b7bleicht", "ist", "all", "mein", "Haar", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "$,", "VVPP", "VAFIN", "PIAT", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Hier sind die Zeitungen von einem Jahr . . . !", "tokens": ["Hier", "sind", "die", "Zei\u00b7tun\u00b7gen", "von", "ei\u00b7nem", "Jahr", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Du hast mich ihnen g\u00e4nzlich preisgegeben \u2013", "tokens": ["Du", "hast", "mich", "ih\u00b7nen", "g\u00e4nz\u00b7lich", "preis\u00b7ge\u00b7ge\u00b7ben", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PPER", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "war das ein Leben \u2013 das mein Leben \u2013?", "tokens": ["war", "das", "ein", "Le\u00b7ben", "\u2013", "das", "mein", "Le\u00b7ben", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "$(", "PDS", "PPOSAT", "NN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Ich merkte, welche Tageszeit grad war,", "tokens": ["Ich", "merk\u00b7te", ",", "wel\u00b7che", "Ta\u00b7ges\u00b7zeit", "grad", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAT", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "nur am ", "tokens": ["nur", "am"], "token_info": ["word", "word"], "pos": ["ADV", "APPRART"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Bis in die letzten Winkel meines Heims", "tokens": ["Bis", "in", "die", "letz\u00b7ten", "Win\u00b7kel", "mei\u00b7nes", "Heims"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "kam deine ", "tokens": ["kam", "dei\u00b7ne"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.29": {"line.1": {"text": "Verflucht die Bilder, die Plakate!", "tokens": ["Ver\u00b7flucht", "die", "Bil\u00b7der", ",", "die", "Pla\u00b7ka\u00b7te", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die Leitartikel, Inserate!", "tokens": ["die", "Leit\u00b7ar\u00b7ti\u00b7kel", ",", "In\u00b7se\u00b7ra\u00b7te", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "die Neuigkeit, die, kaum geboren, alt!", "tokens": ["die", "Neu\u00b7ig\u00b7keit", ",", "die", ",", "kaum", "ge\u00b7bo\u00b7ren", ",", "alt", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "$,", "ADV", "VVPP", "$,", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "das Blatt am Baum \u2013 der ganze Bl\u00e4tterwald!", "tokens": ["das", "Blatt", "am", "Baum", "\u2013", "der", "gan\u00b7ze", "Bl\u00e4t\u00b7ter\u00b7wald", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Verflucht! Verflucht die Menschenfibel!", "tokens": ["Ver\u00b7flucht", "!", "Ver\u00b7flucht", "die", "Men\u00b7schen\u00b7fi\u00b7bel", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "verflucht die Inseratenbibel!", "tokens": ["ver\u00b7flucht", "die", "In\u00b7se\u00b7ra\u00b7ten\u00b7bi\u00b7bel", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.7": {"text": "Ruhm: Durch die Zeitung. Heirat: durch die Zeitung.", "tokens": ["Ruhm", ":", "Durch", "die", "Zei\u00b7tung", ".", "Hei\u00b7rat", ":", "durch", "die", "Zei\u00b7tung", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "$.", "NN", "$.", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Krieg: Durch die Zeitung. Friede: durch die Zeitung.", "tokens": ["Krieg", ":", "Durch", "die", "Zei\u00b7tung", ".", "Frie\u00b7de", ":", "durch", "die", "Zei\u00b7tung", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "$.", "NN", "$.", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Nimm sie von mir! Die Zeitung triumphiert!", "tokens": ["Nimm", "sie", "von", "mir", "!", "Die", "Zei\u00b7tung", "tri\u00b7um\u00b7phiert", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPR", "PPER", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.30": {"line.1": {"text": "Es hilft ja nichts.", "tokens": ["Es", "hilft", "ja", "nichts", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Du bist ja sicher", "tokens": ["Du", "bist", "ja", "si\u00b7cher"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "selber", "tokens": ["sel\u00b7ber"], "token_info": ["word"], "pos": ["ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "abonniert . . .", "tokens": ["a\u00b7bon\u00b7niert", ".", ".", "."], "token_info": ["word", "punct", "punct", "punct"], "pos": ["VVFIN", "$.", "$.", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}