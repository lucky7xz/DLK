{"textgrid.poem.39096": {"metadata": {"author": {"name": "Prutz, Robert Eduard", "birth": "N.A.", "death": "N.A."}, "title": "1L: Heinrich Laube, Heinrich Laube,", "genre": "verse", "period": "N.A.", "pub_year": 1844, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Heinrich Laube, Heinrich Laube,", "tokens": ["Hein\u00b7rich", "Lau\u00b7be", ",", "Hein\u00b7rich", "Lau\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Cicero von Ellenbogen,", "tokens": ["Ci\u00b7ce\u00b7ro", "von", "El\u00b7len\u00b7bo\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dank dir, Freund: denn unser Glaube,", "tokens": ["Dank", "dir", ",", "Freund", ":", "denn", "un\u00b7ser", "Glau\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "NN", "$.", "KON", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "diesmal hat er nicht gelogen!", "tokens": ["dies\u00b7mal", "hat", "er", "nicht", "ge\u00b7lo\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Unser Glaube \u2013 dieser n\u00e4mlich,", "tokens": ["Un\u00b7ser", "Glau\u00b7be", "\u2013", "die\u00b7ser", "n\u00e4m\u00b7lich", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PDS", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "da\u00df du dich w\u00fcrdst lau bezeigen,", "tokens": ["da\u00df", "du", "dich", "w\u00fcrdst", "lau", "be\u00b7zei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VAFIN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "weiberhaft, blasiert und gr\u00e4mlich:", "tokens": ["wei\u00b7ber\u00b7haft", ",", "bla\u00b7siert", "und", "gr\u00e4m\u00b7lich", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "VVFIN", "KON", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und das alles, Freund, mit Schweigen!", "tokens": ["und", "das", "al\u00b7les", ",", "Freund", ",", "mit", "Schwei\u00b7gen", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "PIS", "$,", "NN", "$,", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Unser Glaube, da\u00df kein Zweiter", "tokens": ["Un\u00b7ser", "Glau\u00b7be", ",", "da\u00df", "kein", "Zwei\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KOUS", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "in der Paulskirch sei gesessen,", "tokens": ["in", "der", "Pauls\u00b7kirch", "sei", "ge\u00b7ses\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "der zum Knechtesdienst bereiter", "tokens": ["der", "zum", "Knech\u00b7tes\u00b7dienst", "be\u00b7rei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPRART", "NN", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und manierlicher beim Essen;", "tokens": ["und", "ma\u00b7nier\u00b7li\u00b7cher", "beim", "Es\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPRART", "NN", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.4": {"line.1": {"text": "da\u00df von allen sei nicht einer,", "tokens": ["da\u00df", "von", "al\u00b7len", "sei", "nicht", "ei\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PIS", "VAFIN", "PTKNEG", "ART", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "deutscher Freiheit Lampenputzer,", "tokens": ["deut\u00b7scher", "Frei\u00b7heit", "Lam\u00b7pen\u00b7put\u00b7zer", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "weder der als Staatsmann kleiner,", "tokens": ["we\u00b7der", "der", "als", "Staats\u00b7mann", "klei\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "KOUS", "NN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "noch der gr\u00f6\u00dfer sei als Stutzer!", "tokens": ["noch", "der", "gr\u00f6\u00b7\u00dfer", "sei", "als", "Stut\u00b7zer", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJD", "VAFIN", "KOKOM", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Deine Handschuh, das sieht jeder,", "tokens": ["Dei\u00b7ne", "Hand\u00b7schuh", ",", "das", "sieht", "je\u00b7der", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PDS", "VVFIN", "PIS", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "werden alle Tage gelber,", "tokens": ["wer\u00b7den", "al\u00b7le", "Ta\u00b7ge", "gel\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "sind so ziemlich auch von Leder; \u2013", "tokens": ["sind", "so", "ziem\u00b7lich", "auch", "von", "Le\u00b7der", ";", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "doch vom besten bist du selber.", "tokens": ["doch", "vom", "bes\u00b7ten", "bist", "du", "sel\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Heinrich Laube, Heinrich Laube,", "tokens": ["Hein\u00b7rich", "Lau\u00b7be", ",", "Hein\u00b7rich", "Lau\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Cicero von Ellenbogen,", "tokens": ["Ci\u00b7ce\u00b7ro", "von", "El\u00b7len\u00b7bo\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dank dir, Freund: denn unser Glaube,", "tokens": ["Dank", "dir", ",", "Freund", ":", "denn", "un\u00b7ser", "Glau\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "NN", "$.", "KON", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "diesmal hat er nicht gelogen!", "tokens": ["dies\u00b7mal", "hat", "er", "nicht", "ge\u00b7lo\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Unser Glaube \u2013 dieser n\u00e4mlich,", "tokens": ["Un\u00b7ser", "Glau\u00b7be", "\u2013", "die\u00b7ser", "n\u00e4m\u00b7lich", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PDS", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "da\u00df du dich w\u00fcrdst lau bezeigen,", "tokens": ["da\u00df", "du", "dich", "w\u00fcrdst", "lau", "be\u00b7zei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VAFIN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "weiberhaft, blasiert und gr\u00e4mlich:", "tokens": ["wei\u00b7ber\u00b7haft", ",", "bla\u00b7siert", "und", "gr\u00e4m\u00b7lich", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "VVFIN", "KON", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und das alles, Freund, mit Schweigen!", "tokens": ["und", "das", "al\u00b7les", ",", "Freund", ",", "mit", "Schwei\u00b7gen", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "PIS", "$,", "NN", "$,", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Unser Glaube, da\u00df kein Zweiter", "tokens": ["Un\u00b7ser", "Glau\u00b7be", ",", "da\u00df", "kein", "Zwei\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KOUS", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "in der Paulskirch sei gesessen,", "tokens": ["in", "der", "Pauls\u00b7kirch", "sei", "ge\u00b7ses\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "der zum Knechtesdienst bereiter", "tokens": ["der", "zum", "Knech\u00b7tes\u00b7dienst", "be\u00b7rei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPRART", "NN", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "und manierlicher beim Essen;", "tokens": ["und", "ma\u00b7nier\u00b7li\u00b7cher", "beim", "Es\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPRART", "NN", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.9": {"line.1": {"text": "da\u00df von allen sei nicht einer,", "tokens": ["da\u00df", "von", "al\u00b7len", "sei", "nicht", "ei\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PIS", "VAFIN", "PTKNEG", "ART", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "deutscher Freiheit Lampenputzer,", "tokens": ["deut\u00b7scher", "Frei\u00b7heit", "Lam\u00b7pen\u00b7put\u00b7zer", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "weder der als Staatsmann kleiner,", "tokens": ["we\u00b7der", "der", "als", "Staats\u00b7mann", "klei\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "KOUS", "NN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "noch der gr\u00f6\u00dfer sei als Stutzer!", "tokens": ["noch", "der", "gr\u00f6\u00b7\u00dfer", "sei", "als", "Stut\u00b7zer", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJD", "VAFIN", "KOKOM", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Deine Handschuh, das sieht jeder,", "tokens": ["Dei\u00b7ne", "Hand\u00b7schuh", ",", "das", "sieht", "je\u00b7der", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PDS", "VVFIN", "PIS", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "werden alle Tage gelber,", "tokens": ["wer\u00b7den", "al\u00b7le", "Ta\u00b7ge", "gel\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "sind so ziemlich auch von Leder; \u2013", "tokens": ["sind", "so", "ziem\u00b7lich", "auch", "von", "Le\u00b7der", ";", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "doch vom besten bist du selber.", "tokens": ["doch", "vom", "bes\u00b7ten", "bist", "du", "sel\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}