{"textgrid.poem.56748": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die in Wolkenkukuksheim", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die in Wolkenkukuksheim", "tokens": ["Die", "in", "Wol\u00b7ken\u00b7ku\u00b7kuks\u00b7heim"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "zerrei\u00dfen ihre Manuskripte,", "tokens": ["zer\u00b7rei\u00b7\u00dfen", "ih\u00b7re", "Ma\u00b7nus\u00b7krip\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und in unz\u00e4hligen,", "tokens": ["und", "in", "un\u00b7z\u00e4h\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "wei\u00dfen Schnitzelchen", "tokens": ["wei\u00b7\u00dfen", "Schnit\u00b7zel\u00b7chen"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.5": {"text": "flattert und fliegt es mir", "tokens": ["flat\u00b7tert", "und", "fliegt", "es", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVFIN", "PPER", "PPER"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.6": {"text": "um die Schl\u00e4fen.", "tokens": ["um", "die", "Schl\u00e4\u00b7fen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Die Unzufriednen!", "tokens": ["Die", "Un\u00b7zu\u00b7fried\u00b7nen", "!"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.8": {"text": "Nie noch blieben", "tokens": ["Nie", "noch", "blie\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "der Lieder sie froh,", "tokens": ["der", "Lie\u00b7der", "sie", "froh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.10": {"text": "die im Lenz", "tokens": ["die", "im", "Lenz"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPRART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "ihnen knospeten,", "tokens": ["ih\u00b7nen", "knos\u00b7pe\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.12": {"text": "nie noch", "tokens": ["nie", "noch"], "token_info": ["word", "word"], "pos": ["ADV", "ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "der dithyrambischen Ch\u00f6re,", "tokens": ["der", "dit\u00b7hy\u00b7ram\u00b7bi\u00b7schen", "Ch\u00f6\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.14": {"text": "die durch gl\u00fchende Julin\u00e4chte", "tokens": ["die", "durch", "gl\u00fc\u00b7hen\u00b7de", "Ju\u00b7li\u00b7n\u00e4ch\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.15": {"text": "von ihren Munden", "tokens": ["von", "ih\u00b7ren", "Mun\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.16": {"text": "wie Donner brachen.", "tokens": ["wie", "Don\u00b7ner", "bra\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "NN", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.17": {"text": "Immer wieder", "tokens": ["Im\u00b7mer", "wie\u00b7der"], "token_info": ["word", "word"], "pos": ["ADV", "ADV"], "meter": "+-+-", "measure": "trochaic.di"}, "line.18": {"text": "zerst\u00f6ren gleichm\u00fctig sie,", "tokens": ["zer\u00b7st\u00f6\u00b7ren", "gleich\u00b7m\u00fc\u00b7tig", "sie", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PPER", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.19": {"text": "was sie gedichtet:", "tokens": ["was", "sie", "ge\u00b7dich\u00b7tet", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.20": {"text": "und in unz\u00e4hligen,", "tokens": ["und", "in", "un\u00b7z\u00e4h\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.21": {"text": "wei\u00dfen St\u00fcckchen", "tokens": ["wei\u00b7\u00dfen", "St\u00fcck\u00b7chen"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.22": {"text": "flattert es", "tokens": ["flat\u00b7tert", "es"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.23": {"text": "aus dem grauen Papierkorb,", "tokens": ["aus", "dem", "grau\u00b7en", "Pa\u00b7pier\u00b7korb", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.24": {"text": "den sie schelmisch", "tokens": ["den", "sie", "schel\u00b7misch"], "token_info": ["word", "word", "word"], "pos": ["ART", "PPER", "ADJD"], "meter": "--+-", "measure": "anapaest.init"}, "line.25": {"text": "zur Erde kehren.", "tokens": ["zur", "Er\u00b7de", "keh\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.26": {"text": "Gro\u00dfe, redliche Geister!", "tokens": ["Gro\u00b7\u00dfe", ",", "red\u00b7li\u00b7che", "Geis\u00b7ter", "!"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADJA", "$,", "ADJA", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.2": {"line.1": {"text": "Ich, der Erde armer Poet,", "tokens": ["Ich", ",", "der", "Er\u00b7de", "ar\u00b7mer", "Po\u00b7et", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "versteh Euch.", "tokens": ["ver\u00b7steh", "Euch", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "PPER", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Wenn wir ", "tokens": ["Wenn", "wir"], "token_info": ["word", "word"], "pos": ["KOUS", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "gen\u00fcgen wollen,", "tokens": ["ge\u00b7n\u00fc\u00b7gen", "wol\u00b7len", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "ehrlich Schaffende wir,", "tokens": ["ehr\u00b7lich", "Schaf\u00b7fen\u00b7de", "wir", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "NN", "PPER", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "m\u00fcssen wir", "tokens": ["m\u00fcs\u00b7sen", "wir"], "token_info": ["word", "word"], "pos": ["VMFIN", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "unsren Gedanken wieder", "tokens": ["un\u00b7sren", "Ge\u00b7dan\u00b7ken", "wie\u00b7der"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.8": {"text": "all die bunten H\u00fcllen ausziehn.", "tokens": ["all", "die", "bun\u00b7ten", "H\u00fcl\u00b7len", "aus\u00b7ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.9": {"text": "Ach! allein", "tokens": ["Ach", "!", "al\u00b7lein"], "token_info": ["word", "punct", "word"], "pos": ["ITJ", "$.", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "in der Maske des Worts", "tokens": ["in", "der", "Mas\u00b7ke", "des", "Worts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.11": {"text": "wird unser Tiefstes", "tokens": ["wird", "un\u00b7ser", "Tiefs\u00b7tes"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.12": {"text": "dem N\u00e4chsten sichtbar!", "tokens": ["dem", "N\u00e4chs\u00b7ten", "sicht\u00b7bar", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Ihr Stolzen verschm\u00e4ht es,", "tokens": ["Ihr", "Stol\u00b7zen", "ver\u00b7schm\u00e4ht", "es", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "den Wortewerken,", "tokens": ["den", "Wor\u00b7te\u00b7wer\u00b7ken", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "die Ihr erschuft,", "tokens": ["die", "Ihr", "er\u00b7schuft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Dauer zu leihen,", "tokens": ["Dau\u00b7er", "zu", "lei\u00b7hen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "und Ihr k\u00f6nnt es \u2013", "tokens": ["und", "Ihr", "k\u00f6nnt", "es", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "$("], "meter": "+-+-", "measure": "trochaic.di"}, "line.6": {"text": "denn Ihr seid G\u00f6tter!", "tokens": ["denn", "Ihr", "seid", "G\u00f6t\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Keiner von Euch", "tokens": ["Kei\u00b7ner", "von", "Euch"], "token_info": ["word", "word", "word"], "pos": ["PIS", "APPR", "PPER"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.8": {"text": "will Trost, will Erl\u00f6sung,", "tokens": ["will", "Trost", ",", "will", "Er\u00b7l\u00f6\u00b7sung", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "NN", "$,", "VMFIN", "NN", "$,"], "meter": "----+-", "measure": "unknown.measure.single"}, "line.9": {"text": "wei\u00df von dem Wahnsinn", "tokens": ["wei\u00df", "von", "dem", "Wahn\u00b7sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.10": {"text": "Gl\u00fcckes und Leides:", "tokens": ["Gl\u00fc\u00b7ckes", "und", "Lei\u00b7des", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.11": {"text": "in Euch selbst", "tokens": ["in", "Euch", "selbst"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPER", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.12": {"text": "seid Ihr Euch ewig genug!", "tokens": ["seid", "Ihr", "Euch", "e\u00b7wig", "ge\u00b7nug", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADJD", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.4": {"line.1": {"text": "Aber wir Menschen,", "tokens": ["A\u00b7ber", "wir", "Men\u00b7schen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPER", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "wir Selig-Unseligen,", "tokens": ["wir", "Se\u00b7lig\u00b7Un\u00b7se\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "tief in gemeinsame Lose", "tokens": ["tief", "in", "ge\u00b7mein\u00b7sa\u00b7me", "Lo\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ADJA", "NN"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "verstrickten,", "tokens": ["ver\u00b7strick\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "m\u00fcssen einander", "tokens": ["m\u00fcs\u00b7sen", "ein\u00b7an\u00b7der"], "token_info": ["word", "word"], "pos": ["VMFIN", "PRF"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "die Herzen erschlie\u00dfen,", "tokens": ["die", "Her\u00b7zen", "er\u00b7schlie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "m\u00fcssen einander", "tokens": ["m\u00fcs\u00b7sen", "ein\u00b7an\u00b7der"], "token_info": ["word", "word"], "pos": ["VMFIN", "PRF"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.8": {"text": "fragen, belehren,", "tokens": ["fra\u00b7gen", ",", "be\u00b7leh\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVINF", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.9": {"text": "tr\u00f6sten, befreien,", "tokens": ["tr\u00f6s\u00b7ten", ",", "be\u00b7frei\u00b7en", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.10": {"text": "st\u00e4rken, erheitern,", "tokens": ["st\u00e4r\u00b7ken", ",", "er\u00b7hei\u00b7tern", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.11": {"text": "und zu all Dem", "tokens": ["und", "zu", "all", "Dem"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "PDS"], "meter": "+-++", "measure": "unknown.measure.tri"}, "line.12": {"text": "raten und planen,", "tokens": ["ra\u00b7ten", "und", "pla\u00b7nen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.13": {"text": "formen und bauen,", "tokens": ["for\u00b7men", "und", "bau\u00b7en", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.14": {"text": "rastlos, m\u00fchvoll,", "tokens": ["rast\u00b7los", ",", "m\u00fch\u00b7voll", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "an dem Menschheitstempel", "tokens": ["an", "dem", "Menschheits\u00b7tem\u00b7pel"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.16": {"text": "\u00bbkultur\u00ab.", "tokens": ["\u00bb", "kul\u00b7tur", "\u00ab", "."], "token_info": ["punct", "word", "punct", "punct"], "pos": ["$(", "FM.la", "$(", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.5": {"line.1": {"text": "Ich stehe stumm", "tokens": ["Ich", "ste\u00b7he", "stumm"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "in den wirbelnden Flocken", "tokens": ["in", "den", "wir\u00b7beln\u00b7den", "Flo\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "und denke mit Schwermut", "tokens": ["und", "den\u00b7ke", "mit", "Schwer\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "meines St\u00fcckwerks.", "tokens": ["mei\u00b7nes", "St\u00fcck\u00b7werks", "."], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Doch streue ich selbst", "tokens": ["Doch", "streu\u00b7e", "ich", "selbst"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "nichts in den lustigen Tanz.", "tokens": ["nichts", "in", "den", "lus\u00b7ti\u00b7gen", "Tanz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.7": {"text": "Meine Werke, Ihr G\u00f6tter,", "tokens": ["Mei\u00b7ne", "Wer\u00b7ke", ",", "Ihr", "G\u00f6t\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.8": {"text": "st\u00fcrben wie roter Schnee,", "tokens": ["st\u00fcr\u00b7ben", "wie", "ro\u00b7ter", "Schnee", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.9": {"text": "wollt ich sie opfern!", "tokens": ["wollt", "ich", "sie", "op\u00b7fern", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Ich schrieb mit Herzblut ...", "tokens": ["Ich", "schrieb", "mit", "Herz\u00b7blut", "..."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.11": {"text": "Homo sum.", "tokens": ["Ho\u00b7mo", "sum", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Die in Wolkenkukuksheim", "tokens": ["Die", "in", "Wol\u00b7ken\u00b7ku\u00b7kuks\u00b7heim"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "zerrei\u00dfen ihre Manuskripte,", "tokens": ["zer\u00b7rei\u00b7\u00dfen", "ih\u00b7re", "Ma\u00b7nus\u00b7krip\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und in unz\u00e4hligen,", "tokens": ["und", "in", "un\u00b7z\u00e4h\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "wei\u00dfen Schnitzelchen", "tokens": ["wei\u00b7\u00dfen", "Schnit\u00b7zel\u00b7chen"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.5": {"text": "flattert und fliegt es mir", "tokens": ["flat\u00b7tert", "und", "fliegt", "es", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVFIN", "PPER", "PPER"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.6": {"text": "um die Schl\u00e4fen.", "tokens": ["um", "die", "Schl\u00e4\u00b7fen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Die Unzufriednen!", "tokens": ["Die", "Un\u00b7zu\u00b7fried\u00b7nen", "!"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.8": {"text": "Nie noch blieben", "tokens": ["Nie", "noch", "blie\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "der Lieder sie froh,", "tokens": ["der", "Lie\u00b7der", "sie", "froh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.10": {"text": "die im Lenz", "tokens": ["die", "im", "Lenz"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPRART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "ihnen knospeten,", "tokens": ["ih\u00b7nen", "knos\u00b7pe\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.12": {"text": "nie noch", "tokens": ["nie", "noch"], "token_info": ["word", "word"], "pos": ["ADV", "ADV"], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "der dithyrambischen Ch\u00f6re,", "tokens": ["der", "dit\u00b7hy\u00b7ram\u00b7bi\u00b7schen", "Ch\u00f6\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.14": {"text": "die durch gl\u00fchende Julin\u00e4chte", "tokens": ["die", "durch", "gl\u00fc\u00b7hen\u00b7de", "Ju\u00b7li\u00b7n\u00e4ch\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.15": {"text": "von ihren Munden", "tokens": ["von", "ih\u00b7ren", "Mun\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.16": {"text": "wie Donner brachen.", "tokens": ["wie", "Don\u00b7ner", "bra\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "NN", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.17": {"text": "Immer wieder", "tokens": ["Im\u00b7mer", "wie\u00b7der"], "token_info": ["word", "word"], "pos": ["ADV", "ADV"], "meter": "+-+-", "measure": "trochaic.di"}, "line.18": {"text": "zerst\u00f6ren gleichm\u00fctig sie,", "tokens": ["zer\u00b7st\u00f6\u00b7ren", "gleich\u00b7m\u00fc\u00b7tig", "sie", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PPER", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.19": {"text": "was sie gedichtet:", "tokens": ["was", "sie", "ge\u00b7dich\u00b7tet", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.20": {"text": "und in unz\u00e4hligen,", "tokens": ["und", "in", "un\u00b7z\u00e4h\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.21": {"text": "wei\u00dfen St\u00fcckchen", "tokens": ["wei\u00b7\u00dfen", "St\u00fcck\u00b7chen"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.22": {"text": "flattert es", "tokens": ["flat\u00b7tert", "es"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.23": {"text": "aus dem grauen Papierkorb,", "tokens": ["aus", "dem", "grau\u00b7en", "Pa\u00b7pier\u00b7korb", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.24": {"text": "den sie schelmisch", "tokens": ["den", "sie", "schel\u00b7misch"], "token_info": ["word", "word", "word"], "pos": ["ART", "PPER", "ADJD"], "meter": "--+-", "measure": "anapaest.init"}, "line.25": {"text": "zur Erde kehren.", "tokens": ["zur", "Er\u00b7de", "keh\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.26": {"text": "Gro\u00dfe, redliche Geister!", "tokens": ["Gro\u00b7\u00dfe", ",", "red\u00b7li\u00b7che", "Geis\u00b7ter", "!"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADJA", "$,", "ADJA", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.7": {"line.1": {"text": "Ich, der Erde armer Poet,", "tokens": ["Ich", ",", "der", "Er\u00b7de", "ar\u00b7mer", "Po\u00b7et", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "ADJA", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "versteh Euch.", "tokens": ["ver\u00b7steh", "Euch", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "PPER", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Wenn wir ", "tokens": ["Wenn", "wir"], "token_info": ["word", "word"], "pos": ["KOUS", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "gen\u00fcgen wollen,", "tokens": ["ge\u00b7n\u00fc\u00b7gen", "wol\u00b7len", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "ehrlich Schaffende wir,", "tokens": ["ehr\u00b7lich", "Schaf\u00b7fen\u00b7de", "wir", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "NN", "PPER", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "m\u00fcssen wir", "tokens": ["m\u00fcs\u00b7sen", "wir"], "token_info": ["word", "word"], "pos": ["VMFIN", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "unsren Gedanken wieder", "tokens": ["un\u00b7sren", "Ge\u00b7dan\u00b7ken", "wie\u00b7der"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.8": {"text": "all die bunten H\u00fcllen ausziehn.", "tokens": ["all", "die", "bun\u00b7ten", "H\u00fcl\u00b7len", "aus\u00b7ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.9": {"text": "Ach! allein", "tokens": ["Ach", "!", "al\u00b7lein"], "token_info": ["word", "punct", "word"], "pos": ["ITJ", "$.", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "in der Maske des Worts", "tokens": ["in", "der", "Mas\u00b7ke", "des", "Worts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.11": {"text": "wird unser Tiefstes", "tokens": ["wird", "un\u00b7ser", "Tiefs\u00b7tes"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.12": {"text": "dem N\u00e4chsten sichtbar!", "tokens": ["dem", "N\u00e4chs\u00b7ten", "sicht\u00b7bar", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Ihr Stolzen verschm\u00e4ht es,", "tokens": ["Ihr", "Stol\u00b7zen", "ver\u00b7schm\u00e4ht", "es", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "den Wortewerken,", "tokens": ["den", "Wor\u00b7te\u00b7wer\u00b7ken", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "die Ihr erschuft,", "tokens": ["die", "Ihr", "er\u00b7schuft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Dauer zu leihen,", "tokens": ["Dau\u00b7er", "zu", "lei\u00b7hen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "und Ihr k\u00f6nnt es \u2013", "tokens": ["und", "Ihr", "k\u00f6nnt", "es", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "$("], "meter": "+-+-", "measure": "trochaic.di"}, "line.6": {"text": "denn Ihr seid G\u00f6tter!", "tokens": ["denn", "Ihr", "seid", "G\u00f6t\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Keiner von Euch", "tokens": ["Kei\u00b7ner", "von", "Euch"], "token_info": ["word", "word", "word"], "pos": ["PIS", "APPR", "PPER"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.8": {"text": "will Trost, will Erl\u00f6sung,", "tokens": ["will", "Trost", ",", "will", "Er\u00b7l\u00f6\u00b7sung", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "NN", "$,", "VMFIN", "NN", "$,"], "meter": "----+-", "measure": "unknown.measure.single"}, "line.9": {"text": "wei\u00df von dem Wahnsinn", "tokens": ["wei\u00df", "von", "dem", "Wahn\u00b7sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.10": {"text": "Gl\u00fcckes und Leides:", "tokens": ["Gl\u00fc\u00b7ckes", "und", "Lei\u00b7des", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.11": {"text": "in Euch selbst", "tokens": ["in", "Euch", "selbst"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPER", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.12": {"text": "seid Ihr Euch ewig genug!", "tokens": ["seid", "Ihr", "Euch", "e\u00b7wig", "ge\u00b7nug", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADJD", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.9": {"line.1": {"text": "Aber wir Menschen,", "tokens": ["A\u00b7ber", "wir", "Men\u00b7schen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPER", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "wir Selig-Unseligen,", "tokens": ["wir", "Se\u00b7lig\u00b7Un\u00b7se\u00b7li\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "tief in gemeinsame Lose", "tokens": ["tief", "in", "ge\u00b7mein\u00b7sa\u00b7me", "Lo\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ADJA", "NN"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "verstrickten,", "tokens": ["ver\u00b7strick\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "m\u00fcssen einander", "tokens": ["m\u00fcs\u00b7sen", "ein\u00b7an\u00b7der"], "token_info": ["word", "word"], "pos": ["VMFIN", "PRF"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "die Herzen erschlie\u00dfen,", "tokens": ["die", "Her\u00b7zen", "er\u00b7schlie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "m\u00fcssen einander", "tokens": ["m\u00fcs\u00b7sen", "ein\u00b7an\u00b7der"], "token_info": ["word", "word"], "pos": ["VMFIN", "PRF"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.8": {"text": "fragen, belehren,", "tokens": ["fra\u00b7gen", ",", "be\u00b7leh\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVINF", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.9": {"text": "tr\u00f6sten, befreien,", "tokens": ["tr\u00f6s\u00b7ten", ",", "be\u00b7frei\u00b7en", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.10": {"text": "st\u00e4rken, erheitern,", "tokens": ["st\u00e4r\u00b7ken", ",", "er\u00b7hei\u00b7tern", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.11": {"text": "und zu all Dem", "tokens": ["und", "zu", "all", "Dem"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "PDS"], "meter": "+-++", "measure": "unknown.measure.tri"}, "line.12": {"text": "raten und planen,", "tokens": ["ra\u00b7ten", "und", "pla\u00b7nen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.13": {"text": "formen und bauen,", "tokens": ["for\u00b7men", "und", "bau\u00b7en", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.14": {"text": "rastlos, m\u00fchvoll,", "tokens": ["rast\u00b7los", ",", "m\u00fch\u00b7voll", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "an dem Menschheitstempel", "tokens": ["an", "dem", "Menschheits\u00b7tem\u00b7pel"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.16": {"text": "\u00bbkultur\u00ab.", "tokens": ["\u00bb", "kul\u00b7tur", "\u00ab", "."], "token_info": ["punct", "word", "punct", "punct"], "pos": ["$(", "FM.la", "$(", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.10": {"line.1": {"text": "Ich stehe stumm", "tokens": ["Ich", "ste\u00b7he", "stumm"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "in den wirbelnden Flocken", "tokens": ["in", "den", "wir\u00b7beln\u00b7den", "Flo\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "und denke mit Schwermut", "tokens": ["und", "den\u00b7ke", "mit", "Schwer\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "meines St\u00fcckwerks.", "tokens": ["mei\u00b7nes", "St\u00fcck\u00b7werks", "."], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Doch streue ich selbst", "tokens": ["Doch", "streu\u00b7e", "ich", "selbst"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "nichts in den lustigen Tanz.", "tokens": ["nichts", "in", "den", "lus\u00b7ti\u00b7gen", "Tanz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.7": {"text": "Meine Werke, Ihr G\u00f6tter,", "tokens": ["Mei\u00b7ne", "Wer\u00b7ke", ",", "Ihr", "G\u00f6t\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.8": {"text": "st\u00fcrben wie roter Schnee,", "tokens": ["st\u00fcr\u00b7ben", "wie", "ro\u00b7ter", "Schnee", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.9": {"text": "wollt ich sie opfern!", "tokens": ["wollt", "ich", "sie", "op\u00b7fern", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Ich schrieb mit Herzblut ...", "tokens": ["Ich", "schrieb", "mit", "Herz\u00b7blut", "..."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.11": {"text": "Homo sum.", "tokens": ["Ho\u00b7mo", "sum", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}