{"dta.poem.12581": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Zehenter Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "DeN Au\u00dfgang dieses Kriegs dem Mittel anzu-\nhangen/", "tokens": ["DeN", "Au\u00df\u00b7gang", "die\u00b7ses", "Kriegs", "dem", "Mit\u00b7tel", "an\u00b7zu", "han\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PDAT", "NN", "ART", "NN", "TRUNC", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "So sey nun wiederum (Hilff G\u00f6ttin) angefan-", "tokens": ["So", "sey", "nun", "wie\u00b7de\u00b7rum", "(", "Hilff", "G\u00f6t\u00b7tin", ")", "an\u00b7ge\u00b7fan"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "$(", "NE", "NE", "$(", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wier wollen aber nun des todten Weymars Heer/", "tokens": ["Wier", "wol\u00b7len", "a\u00b7ber", "nun", "des", "tod\u00b7ten", "Wey\u00b7mars", "Heer", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "ADV", "ART", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als ein verw\u00e4ystes Volck/ an ", "tokens": ["Als", "ein", "ver\u00b7w\u00e4ys\u00b7tes", "Volck", "/", "an"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["KOUS", "ART", "ADJA", "NN", "$(", "APPR"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "und Heldenhaffte Hand vertraut/ am Reyhne lassen/", "tokens": ["und", "Hel\u00b7den\u00b7haff\u00b7te", "Hand", "ver\u00b7traut", "/", "am", "Reyh\u00b7ne", "las\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "ADJD", "$(", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "und des Banniers Betrieb auf unsre Zunge fassen:", "tokens": ["und", "des", "Ban\u00b7niers", "Be\u00b7trieb", "auf", "uns\u00b7re", "Zun\u00b7ge", "fas\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "Er folgte seinem Feind/ und gab jhm solchen Hieb/", "tokens": ["Er", "folg\u00b7te", "sei\u00b7nem", "Feind", "/", "und", "gab", "jhm", "sol\u00b7chen", "Hieb", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$(", "KON", "VVFIN", "PPER", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Der jhn vom Balther Meer bi\u00df in Hochdeutschland trieb.", "tokens": ["Der", "jhn", "vom", "Bal\u00b7ther", "Meer", "bi\u00df", "in", "Hoch\u00b7deutschland", "trieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "ADJA", "NN", "APPR", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Und war vom R\u00fccken nichts als D\u00f6mitz zu bekriegen.", "tokens": ["Und", "war", "vom", "R\u00fc\u00b7cken", "nichts", "als", "D\u00f6\u00b7mitz", "zu", "be\u00b7krie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPRART", "NN", "PIS", "KOKOM", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Mit dem von L\u00fcneburg stundt\u2019 es auf dem Vergn\u00fcgen/", "tokens": ["Mit", "dem", "von", "L\u00fc\u00b7ne\u00b7burg", "stundt'", "es", "auf", "dem", "Ver\u00b7gn\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "NE", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "+--+--+-+--+-", "measure": "dactylic.di.plus"}, "line.11": {"text": "Weil er von beyden nichts als eitel Noht und Qual", "tokens": ["Weil", "er", "von", "bey\u00b7den", "nichts", "als", "ei\u00b7tel", "Noht", "und", "Qual"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "PIS", "KOKOM", "ADJD", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Verm\u00e4rckte/ sa\u00df er still und hielte sich ", "tokens": ["Ver\u00b7m\u00e4rck\u00b7te", "/", "sa\u00df", "er", "still", "und", "hiel\u00b7te", "sich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "VVFIN", "PPER", "ADJD", "KON", "VVFIN", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Wie auch der gantze Kr\u00e4y\u00df. ", "tokens": ["Wie", "auch", "der", "gant\u00b7ze", "Kr\u00e4y\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.14": {"text": "Da\u00df General Bannier sein Volck zur Weser brachte/", "tokens": ["Da\u00df", "Ge\u00b7ne\u00b7ral", "Ban\u00b7nier", "sein", "Volck", "zur", "We\u00b7ser", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "NE", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die Neuenburg gewann/ des Kings geschlagne Schaar", "tokens": ["Die", "Neu\u00b7en\u00b7burg", "ge\u00b7wann", "/", "des", "Kings", "ge\u00b7schlag\u00b7ne", "Schaar"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$(", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "An seine Seiten hieng/ Helm Wrangel durch Gefahr", "tokens": ["An", "sei\u00b7ne", "Sei\u00b7ten", "hieng", "/", "Helm", "Wran\u00b7gel", "durch", "Ge\u00b7fahr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "NE", "NE", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.18": {"text": "Aus Gardeleben nahm/ mit welchem er sein streiten", "tokens": ["Aus", "Gar\u00b7de\u00b7le\u00b7ben", "nahm", "/", "mit", "wel\u00b7chem", "er", "sein", "strei\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "$(", "APPR", "PRELS", "PPER", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Nicht wenig gl\u00fccklich sah/ wie folgen wird. ", "tokens": ["Nicht", "we\u00b7nig", "gl\u00fcck\u00b7lich", "sah", "/", "wie", "fol\u00b7gen", "wird", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ADJD", "VVFIN", "$(", "KOKOM", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "Gieng er mit aller Macht dem sch\u00f6nen Mei\u00dfen zu/", "tokens": ["Gieng", "er", "mit", "al\u00b7ler", "Macht", "dem", "sch\u00f6\u00b7nen", "Mei\u00b7\u00dfen", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PIAT", "NN", "ART", "ADJA", "NN", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "und nahm bey Kemnitz Sitz/ woselbst des Salis Schaaren/", "tokens": ["und", "nahm", "bey", "Kem\u00b7nitz", "Sitz", "/", "wo\u00b7selbst", "des", "Sa\u00b7lis", "Schaa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "NE", "$(", "PWAV", "ART", "NE", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Die von den K\u00e4ysrischen noch hinterlassen waren/", "tokens": ["Die", "von", "den", "K\u00e4y\u00b7sri\u00b7schen", "noch", "hin\u00b7ter\u00b7las\u00b7sen", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Durch Pfulens scharffen Sto\u00df und Wrangels strengen Hieb", "tokens": ["Durch", "Pfu\u00b7lens", "scharf\u00b7fen", "Sto\u00df", "und", "Wran\u00b7gels", "stren\u00b7gen", "Hieb"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "NN", "KON", "NE", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Also zersteubeten/ ", "tokens": ["Al\u00b7so", "zer\u00b7steu\u00b7be\u00b7ten", "/"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$("], "meter": "+--+--", "measure": "dactylic.di.plus"}, "line.25": {"text": "Er selbst mit Mandesloh und zehen hundert Knechten", "tokens": ["Er", "selbst", "mit", "Man\u00b7des\u00b7loh", "und", "ze\u00b7hen", "hun\u00b7dert", "Knech\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "APPR", "NN", "KON", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Geriethen in Verhafft/ durch jhr ungl\u00fccklich fechten.", "tokens": ["Ge\u00b7rie\u00b7then", "in", "Ver\u00b7hafft", "/", "durch", "jhr", "un\u00b7gl\u00fcck\u00b7lich", "fech\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$(", "APPR", "PPOSAT", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Hiedurch verst\u00e4rckte sich Bannier auf tausend Mann/", "tokens": ["Hie\u00b7durch", "ver\u00b7st\u00e4rck\u00b7te", "sich", "Ban\u00b7nier", "auf", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PRF", "NN", "APPR", "CARD", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.28": {"text": "Worauf er Zwickau/ Schnee- und Anneberg gewann.", "tokens": ["Wo\u00b7rauf", "er", "Zwi\u00b7ckau", "/", "Schnee", "und", "An\u00b7ne\u00b7berg", "ge\u00b7wann", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "NE", "$(", "TRUNC", "KON", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Er wolt\u2019 auch Freyberg an/ und brauchte gro\u00dfe St\u00e4rcke/", "tokens": ["Er", "wolt'", "auch", "Frey\u00b7berg", "an", "/", "und", "brauch\u00b7te", "gro\u00b7\u00dfe", "St\u00e4r\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "NN", "PTKVZ", "$(", "KON", "VVFIN", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Wurd\u2019 aber/ durch Entsatz des K\u00e4ysers/ solchem Wercke", "tokens": ["Wurd'", "a\u00b7ber", "/", "durch", "Ent\u00b7satz", "des", "K\u00e4y\u00b7sers", "/", "sol\u00b7chem", "Wer\u00b7cke"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "ADV", "$(", "APPR", "NN", "ART", "NN", "$(", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Mit Schaden abzustehn/ gedrungen. Sehet nach/", "tokens": ["Mit", "Scha\u00b7den", "ab\u00b7zu\u00b7stehn", "/", "ge\u00b7drun\u00b7gen", ".", "Se\u00b7het", "nach", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "VVIZU", "$(", "VVPP", "$.", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Was Art er diesen Trieb durch eine h\u00f6hre Sach", "tokens": ["Was", "Art", "er", "die\u00b7sen", "Trieb", "durch", "ei\u00b7ne", "h\u00f6h\u00b7re", "Sach"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "NN", "PPER", "PDAT", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Entnahmte. Bald hierauf verst\u00e4rckt\u2019 er sich mit diesen/", "tokens": ["Ent\u00b7nahm\u00b7te", ".", "Bald", "hier\u00b7auf", "ver\u00b7st\u00e4r\u00b7ckt'", "er", "sich", "mit", "die\u00b7sen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ADV", "PAV", "VVFIN", "PPER", "PRF", "APPR", "PDAT", "$("], "meter": "-+---+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.34": {"text": "Die/ durch den Torsten Sohn heldm\u00e4\u00dfig angewiesen/", "tokens": ["Die", "/", "durch", "den", "Tors\u00b7ten", "Sohn", "held\u00b7m\u00e4\u00b7\u00dfig", "an\u00b7ge\u00b7wie\u00b7sen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "APPR", "ART", "ADJA", "NN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Jm Lande Th\u00fcringen sich hatten eingesetzt/", "tokens": ["Jm", "Lan\u00b7de", "Th\u00fc\u00b7rin\u00b7gen", "sich", "hat\u00b7ten", "ein\u00b7ge\u00b7setzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "PRF", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Damit so sahe man die Scharten au\u00dfgewetzt/", "tokens": ["Da\u00b7mit", "so", "sa\u00b7he", "man", "die", "Schar\u00b7ten", "au\u00df\u00b7ge\u00b7wetzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "PIS", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "In dem er nnverhofft des ", "tokens": ["In", "dem", "er", "nn\u00b7ver\u00b7hofft", "des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "VVFIN", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.38": {"text": "Die Freyberg wie der frey zu machen kommen waren/", "tokens": ["Die", "Frey\u00b7berg", "wie", "der", "frey", "zu", "ma\u00b7chen", "kom\u00b7men", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOKOM", "ART", "ADJD", "PTKZU", "VVINF", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Bey Glauch und Hohenstein ", "tokens": ["Bey", "Glauch", "und", "Ho\u00b7hen\u00b7stein"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.40": {"text": "Da\u00df wenig \u00fcbrig blieb/ und also recht und fug", "tokens": ["Da\u00df", "we\u00b7nig", "\u00fcb\u00b7rig", "blieb", "/", "und", "al\u00b7so", "recht", "und", "fug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADJD", "VVFIN", "$(", "KON", "ADV", "ADJD", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Zu Freybergs Klage war. Es blieben alle St\u00fccke", "tokens": ["Zu", "Frey\u00b7bergs", "Kla\u00b7ge", "war", ".", "Es", "blie\u00b7ben", "al\u00b7le", "St\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "NN", "VAFIN", "$.", "PPER", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "und was darzu geh\u00f6rt/ den Schwedischen zum Gl\u00fccke.", "tokens": ["und", "was", "dar\u00b7zu", "ge\u00b7h\u00f6rt", "/", "den", "Schwe\u00b7di\u00b7schen", "zum", "Gl\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PAV", "VVFIN", "$(", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Graf Buchheim blieb verhafft/ Graf Broy nechst dabey/", "tokens": ["Graf", "Buch\u00b7heim", "blieb", "ver\u00b7hafft", "/", "Graf", "Broy", "nechst", "da\u00b7bey", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "VVPP", "$(", "NE", "NE", "VVFIN", "PAV", "$("], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}, "line.44": {"text": "Wie auch zwey tausend Knecht und alle Reuterey/", "tokens": ["Wie", "auch", "zwey", "tau\u00b7send", "Knecht", "und", "al\u00b7le", "Reu\u00b7te\u00b7rey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "CARD", "CARD", "NN", "KON", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Mit noch vier Obersten und ein und funfftzig Fahnen.", "tokens": ["Mit", "noch", "vier", "O\u00b7bers\u00b7ten", "und", "ein", "und", "funfft\u00b7zig", "Fah\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "KON", "ART", "KON", "CARD", "NN", "$."], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.46": {"text": "Seht diesen Sieg den Weg nach B\u00f6h\u00e4ims Grentzen bahnen/", "tokens": ["Seht", "die\u00b7sen", "Sieg", "den", "Weg", "nach", "B\u00f6\u00b7h\u00e4i\u00b7ms", "Grent\u00b7zen", "bah\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "ART", "NN", "APPR", "NE", "NE", "VVFIN", "$("], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.47": {"text": "Wohin sich ", "tokens": ["Wo\u00b7hin", "sich"], "token_info": ["word", "word"], "pos": ["PWAV", "PRF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.48": {"text": "So schlecht kam dieser Held von diesem Treffen ab.", "tokens": ["So", "schlecht", "kam", "die\u00b7ser", "Held", "von", "die\u00b7sem", "Tref\u00b7fen", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PDAT", "NN", "APPR", "PDAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Eh aber sich Bannier nach B\u00f6h\u00e4inis Grentzen f\u00fcgte/", "tokens": ["Eh", "a\u00b7ber", "sich", "Ban\u00b7nier", "nach", "B\u00f6\u00b7h\u00e4i\u00b7nis", "Grent\u00b7zen", "f\u00fcg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PRF", "NE", "APPR", "NE", "NN", "VVFIN", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.50": {"text": "Gieng er nach Pirna hin/ das er auch bald besiegte/", "tokens": ["Gieng", "er", "nach", "Pir\u00b7na", "hin", "/", "das", "er", "auch", "bald", "be\u00b7sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$(", "PRELS", "PPER", "ADV", "ADV", "VVFIN", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.51": {"text": "und alles nider hieb was in den Waffen war.", "tokens": ["und", "al\u00b7les", "ni\u00b7der", "hieb", "was", "in", "den", "Waf\u00b7fen", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PTKVZ", "VVFIN", "PIS", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.52": {"text": "Ich selber stundte da in eu\u00dferster Gefahr.", "tokens": ["Ich", "sel\u00b7ber", "stund\u00b7te", "da", "in", "eu\u00b7\u00dfers\u00b7ter", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.53": {"text": "Der Pirner ", "tokens": ["Der", "Pir\u00b7ner"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.54": {"text": "Solch Schrecken/ da\u00df sie sich nicht dorfften wieder setzen.", "tokens": ["Solch", "Schre\u00b7cken", "/", "da\u00df", "sie", "sich", "nicht", "dorff\u00b7ten", "wie\u00b7der", "set\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$(", "KOUS", "PPER", "PRF", "PTKNEG", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Und darum sahe man die Stolpe/ Hohenstein/", "tokens": ["Und", "da\u00b7rum", "sa\u00b7he", "man", "die", "Stol\u00b7pe", "/", "Ho\u00b7hen\u00b7stein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PIS", "ART", "NN", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "So Tetschen Bischoffswert und andre Schwedisch seyn.", "tokens": ["So", "Tet\u00b7schen", "Bi\u00b7schoffs\u00b7wert", "und", "and\u00b7re", "Schwe\u00b7disch", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NE", "KON", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Was Stadt von Hilff entbl\u00f6st sol also nicht erbl\u00f6den?", "tokens": ["Was", "Stadt", "von", "Hilff", "ent\u00b7bl\u00f6st", "sol", "al\u00b7so", "nicht", "er\u00b7bl\u00f6\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "APPR", "NN", "VVFIN", "VMFIN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Hier auf zertheilte sich die starcke Macht von Schweden/", "tokens": ["Hier", "auf", "zer\u00b7theil\u00b7te", "sich", "die", "star\u00b7cke", "Macht", "von", "Schwe\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "VVFIN", "PRF", "ART", "ADJA", "NN", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "und fiel ein gro\u00dfer Theil von neunmal tausend Mann/", "tokens": ["und", "fiel", "ein", "gro\u00b7\u00dfer", "Theil", "von", "neun\u00b7mal", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPR", "ADV", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Die Stalhansch f\u00fchrete/ das gute Schlesjen an/", "tokens": ["Die", "Stal\u00b7hansch", "f\u00fch\u00b7re\u00b7te", "/", "das", "gu\u00b7te", "Schles\u00b7jen", "an", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "Die S\u00e4chsische daselbst vom K\u00e4yser abzuhalten.", "tokens": ["Die", "S\u00e4ch\u00b7si\u00b7sche", "da\u00b7selbst", "vom", "K\u00e4y\u00b7ser", "ab\u00b7zu\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.62": {"text": "Und wuste dieser Held sein Ampt wol zu verwalten.", "tokens": ["Und", "wus\u00b7te", "die\u00b7ser", "Held", "sein", "Ampt", "wol", "zu", "ver\u00b7wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "NN", "PPOSAT", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "Der Rest mit dem Bannier gieng fort nach Leutmaritz/", "tokens": ["Der", "Rest", "mit", "dem", "Ban\u00b7nier", "gieng", "fort", "nach", "Leut\u00b7ma\u00b7ritz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVFIN", "PTKVZ", "APPR", "NE", "$("], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.64": {"text": "und nahm nicht lang hernach bey Brandei\u00df seinen Sitz.", "tokens": ["und", "nahm", "nicht", "lang", "her\u00b7nach", "bey", "Bran\u00b7dei\u00df", "sei\u00b7nen", "Sitz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADJD", "ADV", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Hierwider sahe man von K\u00e4yserlicher Seiten", "tokens": ["Hier\u00b7wi\u00b7der", "sa\u00b7he", "man", "von", "K\u00e4y\u00b7ser\u00b7li\u00b7cher", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "APPR", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.66": {"text": "Ein Heer/ ein m\u00e4chtig Heer/ von 60000 Leuthen", "tokens": ["Ein", "Heer", "/", "ein", "m\u00e4ch\u00b7tig", "Heer", "/", "von", "60000", "Leu\u00b7then"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "number", "word"], "pos": ["ART", "NN", "$(", "ART", "ADJD", "NN", "$(", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.67": {"text": "Versamlen/ dessen H\u00e4upt Ertz-Hertzog Leopold/", "tokens": ["Ver\u00b7sam\u00b7len", "/", "des\u00b7sen", "H\u00e4upt", "Ertz\u00b7Hert\u00b7zog", "Leo\u00b7pold", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PRELAT", "NN", "NE", "NE", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.68": {"text": "Des K\u00e4ysers Bruder/ war. Verm\u00e4rckt hierbey wie hold", "tokens": ["Des", "K\u00e4y\u00b7sers", "Bru\u00b7der", "/", "war", ".", "Ver\u00b7m\u00e4rckt", "hier\u00b7bey", "wie", "hold"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "$(", "VAFIN", "$.", "VVFIN", "ADV", "KOKOM", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "Das Gl\u00fcck den Schweden schien/ weil solche gro\u00dfe Scharen/", "tokens": ["Das", "Gl\u00fcck", "den", "Schwe\u00b7den", "schien", "/", "weil", "sol\u00b7che", "gro\u00b7\u00dfe", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NE", "VVFIN", "$(", "KOUS", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Noch eins so starck als sie/ das B\u00f6h\u00e4im zu bewahren/", "tokens": ["Noch", "eins", "so", "starck", "als", "sie", "/", "das", "B\u00f6\u00b7h\u00e4im", "zu", "be\u00b7wah\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "ADV", "ADJD", "KOKOM", "PPER", "$(", "ART", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "In Waffen muste seyn. Zw\u00f6lff tausend giengen ab/", "tokens": ["In", "Waf\u00b7fen", "mus\u00b7te", "seyn", ".", "Zw\u00f6lff", "tau\u00b7send", "gien\u00b7gen", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VMFIN", "VAINF", "$.", "NN", "CARD", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Zu welchen sich das Herr von der Cur-Sachsen gab/", "tokens": ["Zu", "wel\u00b7chen", "sich", "das", "Herr", "von", "der", "Cur\u00b7Sach\u00b7sen", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "PRF", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Des Stalhanschs seine Macht in Schlesien zu d\u00e4mpffen/", "tokens": ["Des", "Stal\u00b7hanschs", "sei\u00b7ne", "Macht", "in", "Schle\u00b7si\u00b7en", "zu", "d\u00e4mpf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.74": {"text": "Doch es gieng langsam zu/ denselben zu bek\u00e4mpffen.", "tokens": ["Doch", "es", "gieng", "lang\u00b7sam", "zu", "/", "den\u00b7sel\u00b7ben", "zu", "be\u00b7k\u00e4mpf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "PTKZU", "$(", "PDS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "Hergegen machte sich der Held Bannier vor Prag/", "tokens": ["Her\u00b7ge\u00b7gen", "mach\u00b7te", "sich", "der", "Held", "Ban\u00b7nier", "vor", "Prag", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "ART", "NN", "NE", "APPR", "NE", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.76": {"text": "und gab den K\u00e4ysrischen daselbsten solchen Schlag/", "tokens": ["und", "gab", "den", "K\u00e4y\u00b7sri\u00b7schen", "da\u00b7selbs\u00b7ten", "sol\u00b7chen", "Schlag", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Da\u00df ein paar tausend Mann im Felde ligen blieben/", "tokens": ["Da\u00df", "ein", "paar", "tau\u00b7send", "Mann", "im", "Fel\u00b7de", "li\u00b7gen", "blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "PIAT", "CARD", "NN", "APPRART", "NN", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.78": {"text": "und Hofkirch/ derer H\u00e4upt/ wiewol von vielen Hieben", "tokens": ["und", "Hof\u00b7kirch", "/", "de\u00b7rer", "H\u00e4upt", "/", "wie\u00b7wol", "von", "vie\u00b7len", "Hie\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "$(", "PDS", "NN", "$(", "KOUS", "APPR", "PIAT", "NN"], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.79": {"text": "Sehr wund und matt gemacht/ in Feindes H\u00e4nde fiel.", "tokens": ["Sehr", "wund", "und", "matt", "ge\u00b7macht", "/", "in", "Fein\u00b7des", "H\u00e4n\u00b7de", "fiel", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "VVPP", "$(", "APPR", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "Auf dieses h\u00f6rete das Prag der St\u00fccke Spiel/", "tokens": ["Auf", "die\u00b7ses", "h\u00f6\u00b7re\u00b7te", "das", "Prag", "der", "St\u00fc\u00b7cke", "Spiel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "ART", "NN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Voraus der Carels-Hof/ und war der Schweden sch\u00fcssen", "tokens": ["Vo\u00b7raus", "der", "Carels\u00b7Hof", "/", "und", "war", "der", "Schwe\u00b7den", "sch\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$(", "KON", "VAFIN", "ART", "NN", "VVINF"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.82": {"text": "So starck/ da\u00df sie ein Theil der Mauern niderrissen.", "tokens": ["So", "starck", "/", "da\u00df", "sie", "ein", "Theil", "der", "Mau\u00b7ern", "ni\u00b7der\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "In dem das gro\u00dfe Prag in gro\u00dfer Zagheit war", "tokens": ["In", "dem", "das", "gro\u00b7\u00dfe", "Prag", "in", "gro\u00b7\u00dfer", "Zag\u00b7heit", "war"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "und um Errettung schry/ kam Hatzfelds seine Schaar/", "tokens": ["und", "um", "Er\u00b7ret\u00b7tung", "schry", "/", "kam", "Hatz\u00b7felds", "sei\u00b7ne", "Schaar", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "NE", "$(", "VVFIN", "VAFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Denselben Hilff zu thun/ daher Bannier die Prager", "tokens": ["Den\u00b7sel\u00b7ben", "Hilff", "zu", "thun", "/", "da\u00b7her", "Ban\u00b7nier", "die", "Pra\u00b7ger"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "PTKZU", "VVINF", "$(", "PAV", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "Verlie\u00df/ und seinen Weg nach Brandei\u00df in sein Lager", "tokens": ["Ver\u00b7lie\u00df", "/", "und", "sei\u00b7nen", "Weg", "nach", "Bran\u00b7dei\u00df", "in", "sein", "La\u00b7ger"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "KON", "PPOSAT", "NN", "APPR", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.87": {"text": "Gantz ohne Schaden nahm. Worauf ein jeglich Theil/", "tokens": ["Gantz", "oh\u00b7ne", "Scha\u00b7den", "nahm", ".", "Wo\u00b7rauf", "ein", "jeg\u00b7lich", "Theil", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVFIN", "$.", "PAV", "ART", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "Das gantze K\u00f6nigreich/ zu einem schlechten Heyl/", "tokens": ["Das", "gant\u00b7ze", "K\u00f6\u00b7nig\u00b7reich", "/", "zu", "ei\u00b7nem", "schlech\u00b7ten", "Heyl", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Entz\u00e4umet \u00fcberlieff/ und must\u2019 aus allen Ecken", "tokens": ["Ent\u00b7z\u00e4u\u00b7met", "\u00fc\u00b7berl\u00b7ieff", "/", "und", "must'", "aus", "al\u00b7len", "E\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "$(", "KON", "VMFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Was dienlich war hervor/ dann man durch Brand und", "tokens": ["Was", "dien\u00b7lich", "war", "her\u00b7vor", "/", "dann", "man", "durch", "Brand", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADJD", "VAFIN", "PTKVZ", "$(", "ADV", "PIS", "APPR", "NN", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.91": {"text": "Schrecken", "tokens": ["Schre\u00b7cken"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.92": {"text": "Des blitzenden Gewehrs hierzu gezwungen war/", "tokens": ["Des", "blit\u00b7zen\u00b7den", "Ge\u00b7wehrs", "hier\u00b7zu", "ge\u00b7zwun\u00b7gen", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PAV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.93": {"text": "und dieses dauerte bey nah ein halbes Jahr.", "tokens": ["und", "die\u00b7ses", "dau\u00b7er\u00b7te", "bey", "nah", "ein", "hal\u00b7bes", "Jahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "APPR", "ADJD", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.94": {"text": "Hierzwischen kam es auch zu manchen scharmuzieren/", "tokens": ["Hier\u00b7zwi\u00b7schen", "kam", "es", "auch", "zu", "man\u00b7chen", "schar\u00b7mu\u00b7zie\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PTKA", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Wir wollen aber nur das gr\u00f6\u00dfeste ber\u00fchren.", "tokens": ["Wir", "wol\u00b7len", "a\u00b7ber", "nur", "das", "gr\u00f6\u00b7\u00dfes\u00b7te", "be\u00b7r\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "M\u00f6nchhausen/ ein Soldat von l\u00f6blichem Gericht\u2019/", "tokens": ["M\u00f6nch\u00b7hau\u00b7sen", "/", "ein", "Sol\u00b7dat", "von", "l\u00f6b\u00b7li\u00b7chem", "Ge\u00b7richt'", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ART", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "Erfuhr/ da\u00df sich ein Heer in K\u00e4yserlicher Pflicht", "tokens": ["Er\u00b7fuhr", "/", "da\u00df", "sich", "ein", "Heer", "in", "K\u00e4y\u00b7ser\u00b7li\u00b7cher", "Pflicht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "KOUS", "PRF", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Bey M\u00e4hren sehen lie\u00df. Kaum da\u00df es war erfahren", "tokens": ["Bey", "M\u00e4h\u00b7ren", "se\u00b7hen", "lie\u00df", ".", "Kaum", "da\u00df", "es", "war", "er\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVINF", "VVFIN", "$.", "ADV", "KOUS", "PPER", "VAFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.99": {"text": "Sah man denselbigen mit seinen Schweden Schaaren", "tokens": ["Sah", "man", "den\u00b7sel\u00b7bi\u00b7gen", "mit", "sei\u00b7nen", "Schwe\u00b7den", "Schaa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PDS", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Den Feinden auf dem Hal\u00df. Er that auch solchen Hieb/", "tokens": ["Den", "Fein\u00b7den", "auf", "dem", "Hal\u00df", ".", "Er", "that", "auch", "sol\u00b7chen", "Hieb", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$.", "PPER", "VVFIN", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Da\u00df fast ein tausend Mann theils auf der Wahlstatt blieb/", "tokens": ["Da\u00df", "fast", "ein", "tau\u00b7send", "Mann", "theils", "auf", "der", "Wahl\u00b7statt", "blieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "CARD", "NN", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.102": {"text": "Theils in Verhafftung kam. Der Rest zerstob durch fliehen.", "tokens": ["Theils", "in", "Ver\u00b7haff\u00b7tung", "kam", ".", "Der", "Rest", "zer\u00b7stob", "durch", "flie\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVFIN", "$.", "ART", "NN", "VVFIN", "APPR", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "Hief\u00fcr Vergelt zu thun/ war jeder vom bem\u00fchen/", "tokens": ["Hie\u00b7f\u00fcr", "Ver\u00b7gelt", "zu", "thun", "/", "war", "je\u00b7der", "vom", "be\u00b7m\u00fc\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "PTKZU", "VVINF", "$(", "VAFIN", "PIS", "APPRART", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Bevor der ", "tokens": ["Be\u00b7vor", "der"], "token_info": ["word", "word"], "pos": ["APPR", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.105": {"text": "und hatte seines Volcks ein f\u00fcnfmal tausend Mann.", "tokens": ["und", "hat\u00b7te", "sei\u00b7nes", "Volcks", "ein", "f\u00fcnf\u00b7mal", "tau\u00b7send", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "NN", "ART", "ADV", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Kaum da\u00df er aber hatt\u2019 einmal den Feind getroffen/", "tokens": ["Kaum", "da\u00df", "er", "a\u00b7ber", "hatt'", "ein\u00b7mal", "den", "Feind", "ge\u00b7trof\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADV", "VAFIN", "ADV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Kam sein zu muhtig Pferd mit jhm zu weit geloffen/", "tokens": ["Kam", "sein", "zu", "muh\u00b7tig", "Pferd", "mit", "jhm", "zu", "weit", "ge\u00b7lof\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "PTKA", "ADJD", "NN", "APPR", "PPER", "PTKA", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "und trug jhn in den Feind. Sein Heer sah seine Noht", "tokens": ["und", "trug", "jhn", "in", "den", "Feind", ".", "Sein", "Heer", "sah", "sei\u00b7ne", "Noht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$.", "PPOSAT", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "und wich/ er aber blieb von sieben Wunden todt.", "tokens": ["und", "wich", "/", "er", "a\u00b7ber", "blieb", "von", "sie\u00b7ben", "Wun\u00b7den", "todt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "PPER", "ADV", "VVFIN", "APPR", "CARD", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "Graf Hatzfeld aber sah sein Werck jhm b\u00e4sser gl\u00fccken/", "tokens": ["Graf", "Hatz\u00b7feld", "a\u00b7ber", "sah", "sein", "Werck", "jhm", "b\u00e4s\u00b7ser", "gl\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "VVFIN", "PPOSAT", "NN", "PPER", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Da er dreyhundert Mann bey der Colinschen Br\u00fccken", "tokens": ["Da", "er", "drey\u00b7hun\u00b7dert", "Mann", "bey", "der", "Co\u00b7lin\u00b7schen", "Br\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "CARD", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "--++-+---+-+-", "measure": "anapaest.init"}, "line.112": {"text": "Zerstreut\u2019 und niederhitb. Worauf er seinen Zug", "tokens": ["Zer\u00b7streut'", "und", "nie\u00b7der\u00b7hitb", ".", "Wo\u00b7rauf", "er", "sei\u00b7nen", "Zug"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "PTKVZ", "$.", "PAV", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "Zum sch\u00f6nen Mei\u00dfen nahm und die Gedancken trug/", "tokens": ["Zum", "sch\u00f6\u00b7nen", "Mei\u00b7\u00dfen", "nahm", "und", "die", "Ge\u00b7dan\u00b7cken", "trug", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "KON", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Nechst den Cur-S\u00e4chsischen das Chemnitz zu bekriegen/", "tokens": ["Nechst", "den", "Cur\u00b7S\u00e4ch\u00b7si\u00b7schen", "das", "Chem\u00b7nitz", "zu", "be\u00b7krie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.115": {"text": "und wo es gl\u00fccken wolt\u2019 auch Pirna zu besiegen.", "tokens": ["und", "wo", "es", "gl\u00fc\u00b7cken", "wolt'", "auch", "Pir\u00b7na", "zu", "be\u00b7sie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "VVINF", "VMFIN", "ADV", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Bannier verm\u00e4rckende was Hatzfelds Meynung war/", "tokens": ["Ban\u00b7nier", "ver\u00b7m\u00e4r\u00b7cken\u00b7de", "was", "Hatz\u00b7felds", "Mey\u00b7nung", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PWS", "VAFIN", "NN", "VAFIN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.117": {"text": "Brach unversehens auf und kam mit einer Schaar", "tokens": ["Brach", "un\u00b7ver\u00b7se\u00b7hens", "auf", "und", "kam", "mit", "ei\u00b7ner", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "PTKVZ", "KON", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "Von zw\u00f6lffmal taufend Mann den ", "tokens": ["Von", "zw\u00f6lff\u00b7mal", "tau\u00b7fend", "Mann", "den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "CARD", "NN", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.119": {"text": "Sie wichen/ und es kam nicht einmal zu dem streiten.", "tokens": ["Sie", "wi\u00b7chen", "/", "und", "es", "kam", "nicht", "ein\u00b7mal", "zu", "dem", "strei\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Damit kam Chemnitz frey/ und Pirna in den Brand/", "tokens": ["Da\u00b7mit", "kam", "Chem\u00b7nitz", "frey", "/", "und", "Pir\u00b7na", "in", "den", "Brand", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "ADJD", "$(", "KON", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Da\u00df weder Freind noch Feind hinf\u00fcro seinen Stand", "tokens": ["Da\u00df", "we\u00b7der", "Freind", "noch", "Feind", "hin\u00b7f\u00fc\u00b7ro", "sei\u00b7nen", "Stand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KON", "NN", "ADV", "NN", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Daselbsten halten mocht/ als es mit Gartz geschehen.", "tokens": ["Da\u00b7selbs\u00b7ten", "hal\u00b7ten", "mocht", "/", "als", "es", "mit", "Gartz", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "VMFIN", "$(", "KOUS", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.123": {"text": "Den Hatzfeld sahen wir hierauf nach Francken gehen/", "tokens": ["Den", "Hatz\u00b7feld", "sa\u00b7hen", "wir", "hier\u00b7auf", "nach", "Fran\u00b7cken", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PAV", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.124": {"text": "Dem tapfren K\u00f6nigsmarck ein Widerhalt zu seyn.", "tokens": ["Dem", "tapf\u00b7ren", "K\u00f6\u00b7nigs\u00b7marck", "ein", "Wi\u00b7der\u00b7halt", "zu", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "Bannier hergegen kam nochmals in B\u00f6h\u00e4im ein/", "tokens": ["Ban\u00b7nier", "her\u00b7ge\u00b7gen", "kam", "noch\u00b7mals", "in", "B\u00f6\u00b7h\u00e4im", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "ADV", "APPR", "NE", "ART", "$("], "meter": "+-+--+---+-+", "measure": "trochaic.penta.relaxed"}, "line.126": {"text": "und f\u00fcgte sich daselbst zu seinen andern Schaaren/", "tokens": ["und", "f\u00fcg\u00b7te", "sich", "da\u00b7selbst", "zu", "sei\u00b7nen", "an\u00b7dern", "Schaa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PAV", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "Die unter Torsten Sohn noch hinterst\u00e4llig waren/", "tokens": ["Die", "un\u00b7ter", "Tors\u00b7ten", "Sohn", "noch", "hin\u00b7ter\u00b7st\u00e4l\u00b7lig", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "NN", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "und war die gantze Macht von drey\u00dfig tausend Mann/", "tokens": ["und", "war", "die", "gant\u00b7ze", "Macht", "von", "drey\u00b7\u00dfig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJA", "NN", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Dann es kam kurtz zuvor viel Volck aus Schweden an.", "tokens": ["Dann", "es", "kam", "kurtz", "zu\u00b7vor", "viel", "Volck", "aus", "Schwe\u00b7den", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ADJD", "ADV", "PIAT", "NN", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Wan sah auch \u00fcber di\u00df auf mehr als 100 W\u00e4gen", "tokens": ["Wan", "sah", "auch", "\u00fc\u00b7ber", "di\u00df", "auf", "mehr", "als", "100", "W\u00e4\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "number", "word"], "pos": ["PWAV", "VVFIN", "ADV", "APPR", "PDS", "APPR", "PIAT", "KOKOM", "CARD", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.131": {"text": "Viel Mittel zu dem Krieg in Chemnitz fest gelegen/", "tokens": ["Viel", "Mit\u00b7tel", "zu", "dem", "Krieg", "in", "Chem\u00b7nitz", "fest", "ge\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "ART", "NN", "APPR", "NE", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.132": {"text": "Durch den Bannirschen Zug zum Lager eingebracht/", "tokens": ["Durch", "den", "Ban\u00b7nir\u00b7schen", "Zug", "zum", "La\u00b7ger", "ein\u00b7ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.133": {"text": "Und nochmals also fort mit einer gro\u00dfen Macht", "tokens": ["Und", "noch\u00b7mals", "al\u00b7so", "fort", "mit", "ei\u00b7ner", "gro\u00b7\u00dfen", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "PTKVZ", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.134": {"text": "Die gro\u00dfe Prager-Stadt beziehen und besch\u00fcssen.", "tokens": ["Die", "gro\u00b7\u00dfe", "Pra\u00b7ger\u00b7Stadt", "be\u00b7zie\u00b7hen", "und", "be\u00b7sch\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Und wolte man den Feind zur Schlacht gewillet wissen.", "tokens": ["Und", "wol\u00b7te", "man", "den", "Feind", "zur", "Schlacht", "ge\u00b7wil\u00b7let", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ART", "NN", "APPRART", "NN", "VVPP", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.136": {"text": "Er aber sorgete vor nichts als vor sein Prag/", "tokens": ["Er", "a\u00b7ber", "sor\u00b7ge\u00b7te", "vor", "nichts", "als", "vor", "sein", "Prag", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "APPR", "PIS", "KOKOM", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "Nicht r\u00e4chende was Schlang vor einen gro\u00dfen Schlag", "tokens": ["Nicht", "r\u00e4\u00b7chen\u00b7de", "was", "Schlang", "vor", "ei\u00b7nen", "gro\u00b7\u00dfen", "Schlag"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "VVFIN", "PIS", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.138": {"text": "Zwey Regimentern gab/ was gro\u00dfe Zahl von Pferden", "tokens": ["Zwey", "Re\u00b7gi\u00b7men\u00b7tern", "gab", "/", "was", "gro\u00b7\u00dfe", "Zahl", "von", "Pfer\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "$(", "PWS", "ADJA", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.139": {"text": "Bey Prag dem Torsten Sohn zur Beuthe musten werden/", "tokens": ["Bey", "Prag", "dem", "Tors\u00b7ten", "Sohn", "zur", "Beu\u00b7the", "mus\u00b7ten", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ART", "ADJA", "NN", "APPRART", "NN", "VMFIN", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "Was man den Wei\u00dfenberg/ der Ort vom ersten Sieg", "tokens": ["Was", "man", "den", "Wei\u00b7\u00dfen\u00b7berg", "/", "der", "Ort", "vom", "ers\u00b7ten", "Sieg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "PIS", "ART", "NN", "$(", "ART", "NN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.141": {"text": "In diesem Kriege/ nahm/ die Stern-Schantz \u00fcberstieg/", "tokens": ["In", "die\u00b7sem", "Krie\u00b7ge", "/", "nahm", "/", "die", "Stern\u00b7Schantz", "\u00fc\u00b7bers\u00b7tieg", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$(", "VVFIN", "$(", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "Das Schlo\u00df und auch die Stadt mit Kugeln lie\u00df erregen/", "tokens": ["Das", "Schlo\u00df", "und", "auch", "die", "Stadt", "mit", "Ku\u00b7geln", "lie\u00df", "er\u00b7re\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADV", "ART", "NN", "APPR", "NN", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Di\u00df alles wolte doch zu keiner Schlacht bewegen.", "tokens": ["Di\u00df", "al\u00b7les", "wol\u00b7te", "doch", "zu", "kei\u00b7ner", "Schlacht", "be\u00b7we\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VMFIN", "ADV", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "Hergegen war des Feinds sein donnderndes Betrieb", "tokens": ["Her\u00b7ge\u00b7gen", "war", "des", "Feinds", "sein", "donn\u00b7dern\u00b7des", "Be\u00b7trieb"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.145": {"text": "So gro\u00df/ da\u00df dem Bannier viel Volcks beliegen blieb.", "tokens": ["So", "gro\u00df", "/", "da\u00df", "dem", "Ban\u00b7nier", "viel", "Volcks", "be\u00b7lie\u00b7gen", "blieb", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "KOUS", "ART", "NN", "PIAT", "NN", "VVINF", "VVFIN", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.146": {"text": "Weil nichts zu richten war/ so gieng man von einander.", "tokens": ["Weil", "nichts", "zu", "rich\u00b7ten", "war", "/", "so", "gieng", "man", "von", "ein\u00b7an\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKZU", "VVINF", "VAFIN", "$(", "ADV", "VVFIN", "PIS", "APPR", "PRF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "Der Feind erhielt sein Prag/ der Schwedisch Alexander", "tokens": ["Der", "Feind", "er\u00b7hielt", "sein", "Prag", "/", "der", "Schwe\u00b7disch", "A\u00b7lex\u00b7an\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$(", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "Sein Ehr\u2019 und machte sich nach Brandei\u00df wieder hin.", "tokens": ["Sein", "Ehr'", "und", "mach\u00b7te", "sich", "nach", "Bran\u00b7dei\u00df", "wie\u00b7der", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "VVFIN", "PRF", "APPR", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.149": {"text": "Kein Ort war nun im Reich/ es war die Forcht darin.", "tokens": ["Kein", "Ort", "war", "nun", "im", "Reich", "/", "es", "war", "die", "Forcht", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADV", "APPRART", "NN", "$(", "PPER", "VAFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Er suchte Geld von Brix. Brix sagte: Brix gibt nix/", "tokens": ["Er", "such\u00b7te", "Geld", "von", "Brix", ".", "Brix", "sag\u00b7te", ":", "Brix", "gibt", "nix", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "NE", "$.", "NE", "VVFIN", "$.", "NE", "VVFIN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.151": {"text": "So werde/ sprach Bannier/ ein nix aus eurem Brix/", "tokens": ["So", "wer\u00b7de", "/", "sprach", "Ban\u00b7nier", "/", "ein", "nix", "aus", "eu\u00b7rem", "Brix", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$(", "VVFIN", "NE", "$(", "ART", "NE", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "und legete hierauf die gute Stadt zusammen/", "tokens": ["und", "le\u00b7ge\u00b7te", "hier\u00b7auf", "die", "gu\u00b7te", "Stadt", "zu\u00b7sam\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.153": {"text": "Die er mit Sturm gewann/ in Blut und Feuerflammen.", "tokens": ["Die", "er", "mit", "Sturm", "ge\u00b7wann", "/", "in", "Blut", "und", "Feu\u00b7er\u00b7flam\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVFIN", "$(", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Auf dieses wandt er sich mit Macht vor Rockezahn/", "tokens": ["Auf", "die\u00b7ses", "wandt", "er", "sich", "mit", "Macht", "vor", "Ro\u00b7cke\u00b7zahn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "PRF", "APPR", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Das was er forderte/ aus Schrecken hat gethan.", "tokens": ["Das", "was", "er", "for\u00b7der\u00b7te", "/", "aus", "Schre\u00b7cken", "hat", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PRELS", "PPER", "VVFIN", "$(", "APPR", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.156": {"text": "Und jetzund galt es Satz und was hierum gelegen/", "tokens": ["Und", "je\u00b7tzund", "galt", "es", "Satz", "und", "was", "hie\u00b7rum", "ge\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "NN", "KON", "PWS", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.157": {"text": "Was nicht gehorsam war sah Schwerdt und Fackeln regen/", "tokens": ["Was", "nicht", "ge\u00b7hor\u00b7sam", "war", "sah", "Schwerdt", "und", "Fa\u00b7ckeln", "re\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "ADJD", "VAFIN", "VVFIN", "NE", "KON", "NN", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "und sich darinnen seyn. Hieran war nicht genug", "tokens": ["und", "sich", "da\u00b7rin\u00b7nen", "seyn", ".", "Hie\u00b7ran", "war", "nicht", "ge\u00b7nug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PRF", "ADV", "VAINF", "$.", "PAV", "VAFIN", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "Da\u00df B\u00f6h\u00e4im leyden must\u2019/ er nahm auch einen Zug", "tokens": ["Da\u00df", "B\u00f6\u00b7h\u00e4im", "ley\u00b7den", "must'", "/", "er", "nahm", "auch", "ei\u00b7nen", "Zug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "VVINF", "VMFIN", "$(", "PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Bi\u00df in die Ober-Pfaltz/ in Oesterreich und M\u00e4hren.", "tokens": ["Bi\u00df", "in", "die", "O\u00b7ber\u00b7Pfaltz", "/", "in", "O\u00b7es\u00b7ter\u00b7reich", "und", "M\u00e4h\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$(", "APPR", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.161": {"text": "Es must\u2019 auch Torsten Son dem Stalhansch Hilff gew\u00e4hren/", "tokens": ["Es", "must'", "auch", "Tors\u00b7ten", "Son", "dem", "Stal\u00b7hansch", "Hilff", "ge\u00b7w\u00e4h\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "NN", "APPR", "ART", "NN", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Dem Bauzen ob zu seyn. War schon die Gegen-Macht", "tokens": ["Dem", "Bau\u00b7zen", "ob", "zu", "seyn", ".", "War", "schon", "die", "Ge\u00b7gen\u00b7Macht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "PTKZU", "VAINF", "$.", "VAFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "Sehr gro\u00df/ so war sie doch alhier wie nichts geacht.", "tokens": ["Sehr", "gro\u00df", "/", "so", "war", "sie", "doch", "al\u00b7hier", "wie", "nichts", "ge\u00b7acht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "ADV", "VAFIN", "PPER", "ADV", "ADV", "KOKOM", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "Und st\u00e4llte sich Bannier alhier mit seinen Kriegen", "tokens": ["Und", "st\u00e4ll\u00b7te", "sich", "Ban\u00b7nier", "al\u00b7hier", "mit", "sei\u00b7nen", "Krie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "NE", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.165": {"text": "Nicht anders als vielmehr dann B\u00f6h\u00e4im zu besiegen.", "tokens": ["Nicht", "an\u00b7ders", "als", "viel\u00b7mehr", "dann", "B\u00f6\u00b7h\u00e4im", "zu", "be\u00b7sie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "KOKOM", "ADV", "ADV", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "Doch wie der Hercules selbst zweyen was zu fchwach/", "tokens": ["Doch", "wie", "der", "Her\u00b7cu\u00b7les", "selbst", "zwe\u00b7yen", "was", "zu", "fchwach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "ADV", "VVFIN", "PIS", "APPR", "NE", "$("], "meter": "-+--+-+-++-+", "measure": "iambic.hexa.relaxed"}, "line.167": {"text": "Also gab auch Bannier/ zwar dreyen/ endlich nach.", "tokens": ["Al\u00b7so", "gab", "auch", "Ban\u00b7nier", "/", "zwar", "drey\u00b7en", "/", "end\u00b7lich", "nach", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NE", "$(", "ADV", "CARD", "$(", "ADV", "PTKVZ", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.168": {"text": "Was man erdencken kunt und m\u00f6glich war zu schaffen/", "tokens": ["Was", "man", "er\u00b7den\u00b7cken", "kunt", "und", "m\u00f6g\u00b7lich", "war", "zu", "schaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVINF", "PTKVZ", "KON", "ADJD", "VAFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.169": {"text": "Das kam auch in den Stand die Siegs-gewohnten Waffen", "tokens": ["Das", "kam", "auch", "in", "den", "Stand", "die", "Siegs\u00b7ge\u00b7wohn\u00b7ten", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Der Schwedischen Armee zu schmeltzen. Also gar/", "tokens": ["Der", "Schwe\u00b7di\u00b7schen", "Ar\u00b7mee", "zu", "schmelt\u00b7zen", ".", "Al\u00b7so", "gar", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$.", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Da\u00df fast der dritte Mann auf einen Schweden war.", "tokens": ["Da\u00df", "fast", "der", "drit\u00b7te", "Mann", "auf", "ei\u00b7nen", "Schwe\u00b7den", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "APPR", "ART", "NE", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Ich redete vorher von 60000. Seelen/", "tokens": ["Ich", "re\u00b7de\u00b7te", "vor\u00b7her", "von", "60000.", "See\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "ordinal", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.173": {"text": "Die waren nun bey ein. Zehn tausend mehr zu z\u00e4hlen/", "tokens": ["Die", "wa\u00b7ren", "nun", "bey", "ein", ".", "Zehn", "tau\u00b7send", "mehr", "zu", "z\u00e4h\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "APPR", "PTKVZ", "$.", "CARD", "CARD", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.174": {"text": "Lag keiner Warheit ob. Dargegen anzugehn/", "tokens": ["Lag", "kei\u00b7ner", "War\u00b7heit", "ob", ".", "Dar\u00b7ge\u00b7gen", "an\u00b7zu\u00b7gehn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "KOUS", "$.", "PAV", "VVIZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "War bey den Schweden nicht vor rathsam anzusehn.", "tokens": ["War", "bey", "den", "Schwe\u00b7den", "nicht", "vor", "rath\u00b7sam", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NE", "PTKNEG", "APPR", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.176": {"text": "Sie waren dort und da verlegt und abgegangen/", "tokens": ["Sie", "wa\u00b7ren", "dort", "und", "da", "ver\u00b7legt", "und", "ab\u00b7ge\u00b7gan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "KON", "ADV", "VVPP", "KON", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "Theils waren wund und schwach/ theils anderswo gefangen.", "tokens": ["Theils", "wa\u00b7ren", "wund", "und", "schwach", "/", "theils", "an\u00b7ders\u00b7wo", "ge\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "KON", "ADJD", "$(", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.178": {"text": "So gl\u00fccklich man auch kriegt/ so wil uns das nicht ein/", "tokens": ["So", "gl\u00fcck\u00b7lich", "man", "auch", "kriegt", "/", "so", "wil", "uns", "das", "nicht", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PIS", "ADV", "VVFIN", "$(", "ADV", "VMFIN", "PPER", "PDS", "PTKNEG", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Da\u00df dort und da nicht sol was Volck verlohren seyn.", "tokens": ["Da\u00df", "dort", "und", "da", "nicht", "sol", "was", "Volck", "ver\u00b7loh\u00b7ren", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "KON", "ADV", "PTKNEG", "VMFIN", "PIS", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "Wolan! Sie machten sich zu r\u00fccke nach der Elbe/", "tokens": ["Wo\u00b7lan", "!", "Sie", "mach\u00b7ten", "sich", "zu", "r\u00fc\u00b7cke", "nach", "der", "El\u00b7be", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VVFIN", "PRF", "PTKZU", "VVFIN", "APPR", "ART", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "und zielten Pommern nach/ wolwissend/ da\u00df dasselbe", "tokens": ["und", "ziel\u00b7ten", "Pom\u00b7mern", "nach", "/", "wol\u00b7wis\u00b7send", "/", "da\u00df", "das\u00b7sel\u00b7be"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ADJA", "NN", "APPR", "$(", "VVPP", "$(", "KOUS", "PDAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Die b\u00e4ste Zuflucht war/ daselbsten jhre Macht", "tokens": ["Die", "b\u00e4s\u00b7te", "Zu\u00b7flucht", "war", "/", "da\u00b7selbs\u00b7ten", "jhre", "Macht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "$(", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.183": {"text": "Zu st\u00e4rcken/ doch es war ein andrer Raht bedacht/", "tokens": ["Zu", "st\u00e4r\u00b7cken", "/", "doch", "es", "war", "ein", "an\u00b7drer", "Raht", "be\u00b7dacht", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Das au\u00dfgelegte Volck aus allen B\u00f6hmer-Pl\u00e4tzen", "tokens": ["Das", "au\u00df\u00b7ge\u00b7leg\u00b7te", "Volck", "aus", "al\u00b7len", "B\u00f6h\u00b7mer\u00b7Pl\u00e4t\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.185": {"text": "Zu samlen/ und damit auf Mei\u00dfen zu zu setzen/", "tokens": ["Zu", "sam\u00b7len", "/", "und", "da\u00b7mit", "auf", "Mei\u00b7\u00dfen", "zu", "zu", "set\u00b7zen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "PAV", "APPR", "NN", "PTKZU", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "Auf Vortheil und nicht leicht auf eine Schlacht zu gehn/", "tokens": ["Auf", "Vor\u00b7theil", "und", "nicht", "leicht", "auf", "ei\u00b7ne", "Schlacht", "zu", "gehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "PTKNEG", "ADJD", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "um Hilff von Fremder Macht sich eilends umzusehn.", "tokens": ["um", "Hilff", "von", "Frem\u00b7der", "Macht", "sich", "ei\u00b7lends", "um\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "NN", "PRF", "ADV", "VVIZU", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Das war der Raht und was aus Schweden war befohlen.", "tokens": ["Das", "war", "der", "Raht", "und", "was", "aus", "Schwe\u00b7den", "war", "be\u00b7foh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "KON", "PWS", "APPR", "NE", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "Wie aber dieser Zug das B\u00f6h\u00e4im in die Kolen", "tokens": ["Wie", "a\u00b7ber", "die\u00b7ser", "Zug", "das", "B\u00f6\u00b7h\u00e4im", "in", "die", "Ko\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "PDAT", "NN", "ART", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "und ander ", "tokens": ["und", "an\u00b7der"], "token_info": ["word", "word"], "pos": ["KON", "ADJD"], "meter": "-+-", "measure": "amphibrach.single"}, "line.191": {"text": "Zu schreiben/ und es giebt die K\u00fcrtze keinen Raum.", "tokens": ["Zu", "schrei\u00b7ben", "/", "und", "es", "giebt", "die", "K\u00fcrt\u00b7ze", "kei\u00b7nen", "Raum", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "PPER", "VVFIN", "ART", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Sonst hat man anderswo sehr viel hiervon geschrieben/", "tokens": ["Sonst", "hat", "man", "an\u00b7ders\u00b7wo", "sehr", "viel", "hier\u00b7von", "ge\u00b7schrie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "ADV", "ADV", "ADV", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "Da\u00df achtmal hundert Pl\u00e4tz in Feuer sind geblieben.", "tokens": ["Da\u00df", "acht\u00b7mal", "hun\u00b7dert", "Pl\u00e4tz", "in", "Feu\u00b7er", "sind", "ge\u00b7blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "CARD", "NN", "APPR", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Den Weg nach Mei\u00dfen zu und zwar nach Annenberck", "tokens": ["Den", "Weg", "nach", "Mei\u00b7\u00dfen", "zu", "und", "zwar", "nach", "An\u00b7nen\u00b7berck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "PTKVZ", "KON", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Zu bahnen/ war nun mehr des K\u00f6nigsmarcks sein Werck/", "tokens": ["Zu", "bah\u00b7nen", "/", "war", "nun", "mehr", "des", "K\u00f6\u00b7nigs\u00b7marcks", "sein", "Werck", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VAFIN", "ADV", "ADV", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "Auch gl\u00fccklich au\u00dfgericht/ dem alle Schweden Schaaren", "tokens": ["Auch", "gl\u00fcck\u00b7lich", "au\u00df\u00b7ge\u00b7richt", "/", "dem", "al\u00b7le", "Schwe\u00b7den", "Schaa\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "$(", "ART", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Aus B\u00f6h\u00e4imb allgemach zu folgen r\u00fcstig waren.", "tokens": ["Aus", "B\u00f6\u00b7h\u00e4i\u00b7mb", "all\u00b7ge\u00b7mach", "zu", "fol\u00b7gen", "r\u00fcs\u00b7tig", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.198": {"text": "Graf Hoditz aber must hierzwischen einen Schlag", "tokens": ["Graf", "Ho\u00b7ditz", "a\u00b7ber", "must", "hier\u00b7zwi\u00b7schen", "ei\u00b7nen", "Schlag"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "ADV", "VMFIN", "PAV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Erdulden/ und es war nunmehr der Feinde Sag\u2019", "tokens": ["E\u00b7rdul\u00b7den", "/", "und", "es", "war", "nun\u00b7mehr", "der", "Fein\u00b7de", "Sag'"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "KON", "PPER", "VAFIN", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Allein von einer Schlacht/ es schienen wahre Reden/", "tokens": ["Al\u00b7lein", "von", "ei\u00b7ner", "Schlacht", "/", "es", "schie\u00b7nen", "wah\u00b7re", "Re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$(", "PPER", "VVFIN", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.201": {"text": "und fiel des Vorzugs Volck dem Nachzug von den Schweden", "tokens": ["und", "fiel", "des", "Vor\u00b7zugs", "Volck", "dem", "Nach\u00b7zug", "von", "den", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "ART", "NN", "APPR", "ART", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.202": {"text": "Sehr grimmig auf den Hal\u00df. Schlang aber sah sich \u00fcm/", "tokens": ["Sehr", "grim\u00b7mig", "auf", "den", "Hal\u00df", ".", "Schlang", "a\u00b7ber", "sah", "sich", "\u00fcm", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "$.", "NN", "ADV", "VVFIN", "PRF", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "und schlug die K\u00e4ysrische mit solchem ", "tokens": ["und", "schlug", "die", "K\u00e4y\u00b7sri\u00b7sche", "mit", "sol\u00b7chem"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "PIAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.204": {"text": "Da\u00df tausend in Verhafft/ neun hundert todt verblieben.", "tokens": ["Da\u00df", "tau\u00b7send", "in", "Ver\u00b7hafft", "/", "neun", "hun\u00b7dert", "todt", "ver\u00b7blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "APPR", "NN", "$(", "CARD", "CARD", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "Wiewol nicht lang hierauf dergleichen Hertz-betr\u00fcben", "tokens": ["Wie\u00b7wol", "nicht", "lang", "hier\u00b7auf", "derg\u00b7lei\u00b7chen", "Hertz\u00b7be\u00b7tr\u00fc\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "ADJD", "PAV", "PIS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Den Schweden selber auch bey Plauen wiederf\u00fchr/", "tokens": ["Den", "Schwe\u00b7den", "sel\u00b7ber", "auch", "bey", "Plau\u00b7en", "wie\u00b7der\u00b7f\u00fchr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "und es gieng also zu/ da\u00df man des Schlages Spur", "tokens": ["und", "es", "gieng", "al\u00b7so", "zu", "/", "da\u00df", "man", "des", "Schla\u00b7ges", "Spur"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PTKZU", "$(", "KOUS", "PIS", "ART", "ADJA", "NN"], "meter": "+-+--+---+-+", "measure": "trochaic.penta.relaxed"}, "line.208": {"text": "Auf ein paar Meilen sah. Di\u00df ", "tokens": ["Auf", "ein", "paar", "Mei\u00b7len", "sah", ".", "Di\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "ART", "PIAT", "NN", "VVFIN", "$.", "PDS"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.209": {"text": "Erlitten/ mahnete nunmehr dahin zu schauen", "tokens": ["Er\u00b7lit\u00b7ten", "/", "mah\u00b7ne\u00b7te", "nun\u00b7mehr", "da\u00b7hin", "zu", "schau\u00b7en"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "VVFIN", "ADV", "PAV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "Wo Hilff und Rettung war. Man st\u00e4llete den Stab", "tokens": ["Wo", "Hilff", "und", "Ret\u00b7tung", "war", ".", "Man", "st\u00e4l\u00b7le\u00b7te", "den", "Stab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "KON", "NN", "VAFIN", "$.", "PIS", "VVFIN", "ART", "NN"], "meter": "-+-+-+---+-+", "measure": "unknown.measure.penta"}, "line.211": {"text": "um Jen\u2019 und Erfurt fest/ brach alle Br\u00fccken ab/", "tokens": ["um", "Jen'", "und", "Er\u00b7furt", "fest", "/", "brach", "al\u00b7le", "Br\u00fc\u00b7cken", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "PTKVZ", "$(", "VVFIN", "PIAT", "NN", "PTKVZ", "$("], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}, "line.212": {"text": "Schlug Lager/ schantzete/ bracht\u2019 alles was ern\u00e4hrte", "tokens": ["Schlug", "La\u00b7ger", "/", "schant\u00b7ze\u00b7te", "/", "bracht'", "al\u00b7les", "was", "er\u00b7n\u00e4hr\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "NN", "$(", "VVFIN", "$(", "VVFIN", "PIS", "PWS", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Ins Lager ein/ womit man alles Land verz\u00e4hrte/", "tokens": ["Ins", "La\u00b7ger", "ein", "/", "wo\u00b7mit", "man", "al\u00b7les", "Land", "ver\u00b7z\u00e4hr\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "$(", "PWAV", "PIS", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Dann es blieb nicht bey dem/ was schon gekommen war.", "tokens": ["Dann", "es", "blieb", "nicht", "bey", "dem", "/", "was", "schon", "ge\u00b7kom\u00b7men", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PTKNEG", "APPR", "ART", "$(", "PWS", "ADV", "VVPP", "VAFIN", "$."], "meter": "--+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.215": {"text": "Es gieng kein Tag vorbey/ da\u00df sich die Schweden-Schaar", "tokens": ["Es", "gieng", "kein", "Tag", "vor\u00b7bey", "/", "da\u00df", "sich", "die", "Schwe\u00b7den\u00b7Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PTKVZ", "$(", "KOUS", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Nicht mehr vermehrete. Dann was der Krohne Schwe-", "tokens": ["Nicht", "mehr", "ver\u00b7meh\u00b7re\u00b7te", ".", "Dann", "was", "der", "Kroh\u00b7ne", "Schwe"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADV", "VVFIN", "$.", "ADV", "PWS", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Zur Hilff verbunden war/ kam an. Fein kurtz zu reden/", "tokens": ["Zur", "Hilff", "ver\u00b7bun\u00b7den", "war", "/", "kam", "an", ".", "Fein", "kurtz", "zu", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "VAFIN", "$(", "VVFIN", "PTKVZ", "$.", "NN", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "Die Frantzen unter des von ", "tokens": ["Die", "Frant\u00b7zen", "un\u00b7ter", "des", "von"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "APPR"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.219": {"text": "Die L\u00fcneburgische mit Klitzing abgesand/", "tokens": ["Die", "L\u00fc\u00b7ne\u00b7bur\u00b7gi\u00b7sche", "mit", "Klit\u00b7zing", "ab\u00b7ge\u00b7sand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.220": {"text": "Die Cassel-Hessische mit jhrem H\u00e4upt Melander/", "tokens": ["Die", "Cas\u00b7sel\u00b7Hes\u00b7si\u00b7sche", "mit", "jhrem", "H\u00e4upt", "Me\u00b7lan\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "NE", "$("], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.221": {"text": "Die kamen allesamt sehr m\u00e4chtig bey einander/", "tokens": ["Die", "ka\u00b7men", "al\u00b7le\u00b7samt", "sehr", "m\u00e4ch\u00b7tig", "bey", "ein\u00b7an\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "ADJD", "APPR", "ADV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Den Schweden Hilff zu thun/ und sihe da zum Streit", "tokens": ["Den", "Schwe\u00b7den", "Hilff", "zu", "thun", "/", "und", "si\u00b7he", "da", "zum", "Streit"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$(", "KON", "VVFIN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "Ein sechszig tausend Mann in Th\u00fcringen bereit.", "tokens": ["Ein", "sechs\u00b7zig", "tau\u00b7send", "Mann", "in", "Th\u00fc\u00b7rin\u00b7gen", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "CARD", "NN", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.224": {"text": "Noch war des Feindes Zahl/ der nun in Salfelds Weyden", "tokens": ["Noch", "war", "des", "Fein\u00b7des", "Zahl", "/", "der", "nun", "in", "Sal\u00b7felds", "Wey\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "NN", "$(", "ART", "ADV", "APPR", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "Ein festes Lager schlug/ viel gr\u00f6\u00dfer. Denck was Leyden", "tokens": ["Ein", "fes\u00b7tes", "La\u00b7ger", "schlug", "/", "viel", "gr\u00f6\u00b7\u00dfer", ".", "Denck", "was", "Ley\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$(", "ADV", "ADJD", "$.", "NN", "PWS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "Dergleichen M\u00e4nge Volcks in einem Lande bring\u2019/", "tokens": ["Derg\u00b7lei\u00b7chen", "M\u00e4n\u00b7ge", "Volcks", "in", "ei\u00b7nem", "Lan\u00b7de", "bring'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "NN", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.227": {"text": "Ich bin/ dasselbige zu sagen/ zu gering.", "tokens": ["Ich", "bin", "/", "das\u00b7sel\u00b7bi\u00b7ge", "zu", "sa\u00b7gen", "/", "zu", "ge\u00b7ring", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$(", "PDS", "PTKZU", "VVINF", "$(", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.228": {"text": "Als beyde Theile sich genug vermehret hatten/", "tokens": ["Als", "bey\u00b7de", "Thei\u00b7le", "sich", "ge\u00b7nug", "ver\u00b7meh\u00b7ret", "hat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PRF", "ADV", "VVFIN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "Hergegen alle Noth/ sie s\u00e4mtlich abzumatten/", "tokens": ["Her\u00b7ge\u00b7gen", "al\u00b7le", "Noth", "/", "sie", "s\u00e4mt\u00b7lich", "ab\u00b7zu\u00b7mat\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "$(", "PPER", "ADJD", "VVIZU", "$("], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.230": {"text": "Sich unter sie begab/ bedachte sich Bannier", "tokens": ["Sich", "un\u00b7ter", "sie", "be\u00b7gab", "/", "be\u00b7dach\u00b7te", "sich", "Ban\u00b7nier"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PRF", "APPR", "PPER", "VVFIN", "$(", "ADJA", "PRF", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Zu einer rechten Schlacht/ mit neuem Ruhm von hier", "tokens": ["Zu", "ei\u00b7ner", "rech\u00b7ten", "Schlacht", "/", "mit", "neu\u00b7em", "Ruhm", "von", "hier"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "APPR", "ADJA", "NN", "APPR", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "Zu kommen. Doch es gieng gantz wieder sein begehren.", "tokens": ["Zu", "kom\u00b7men", ".", "Doch", "es", "gieng", "gantz", "wie\u00b7der", "sein", "be\u00b7geh\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "KON", "PPER", "VVFIN", "ADV", "ADV", "VAINF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Des gantzen Reiches Macht auf einmal zu gef\u00e4hren/", "tokens": ["Des", "gant\u00b7zen", "Rei\u00b7ches", "Macht", "auf", "ein\u00b7mal", "zu", "ge\u00b7f\u00e4h\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "APPR", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "Das war kein Raht von Heyl. Es war nicht aus der acht/", "tokens": ["Das", "war", "kein", "Raht", "von", "Heyl", ".", "Es", "war", "nicht", "aus", "der", "acht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PIAT", "NN", "APPR", "NN", "$.", "PPER", "VAFIN", "PTKNEG", "APPR", "ART", "CARD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Wie manches Gl\u00fccke sich in einer offnen Schlacht", "tokens": ["Wie", "man\u00b7ches", "Gl\u00fc\u00b7cke", "sich", "in", "ei\u00b7ner", "off\u00b7nen", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "VVFIN", "PRF", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "Den Schwedischen erwies/ drum war es nicht zu wagen/", "tokens": ["Den", "Schwe\u00b7di\u00b7schen", "er\u00b7wies", "/", "drum", "war", "es", "nicht", "zu", "wa\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "PAV", "VAFIN", "PPER", "PTKNEG", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.237": {"text": "und doch gerieth es noch zu einem solchem schlagen/ ", "tokens": ["und", "doch", "ge\u00b7rieth", "es", "noch", "zu", "ei\u00b7nem", "sol\u00b7chem", "schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "APPR", "ART", "PIAT", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "Da\u00df beyde Theile mehr dann dreymal tausend Mann", "tokens": ["Da\u00df", "bey\u00b7de", "Thei\u00b7le", "mehr", "dann", "drey\u00b7mal", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIAT", "NN", "ADV", "ADV", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "Verlohren/ und hiemit gab man das lagern an/", "tokens": ["Ver\u00b7loh\u00b7ren", "/", "und", "hie\u00b7mit", "gab", "man", "das", "la\u00b7gern", "an", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "PAV", "VVFIN", "PIS", "PDS", "VVFIN", "PTKVZ", "$("], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.240": {"text": "Sie musten beyde sich vor gro\u00dfer Noht erheben/", "tokens": ["Sie", "mus\u00b7ten", "bey\u00b7de", "sich", "vor", "gro\u00b7\u00dfer", "Noht", "er\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "PRF", "APPR", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Es hatten weder Vieh noch Menschen mehr zu leben.", "tokens": ["Es", "hat\u00b7ten", "we\u00b7der", "Vieh", "noch", "Men\u00b7schen", "mehr", "zu", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KON", "NN", "ADV", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "Bannier nahm seinen Weg nach Wildungen/ ein Ort", "tokens": ["Ban\u00b7nier", "nahm", "sei\u00b7nen", "Weg", "nach", "Wil\u00b7dun\u00b7gen", "/", "ein", "Ort"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["NE", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "$(", "ART", "NN"], "meter": "-+-+-+-++--+", "measure": "iambic.hexa.chol"}, "line.243": {"text": "Von herrlichem Getr\u00e4nck. Sein Feind hergegen fort", "tokens": ["Von", "herr\u00b7li\u00b7chem", "Ge\u00b7tr\u00e4nck", ".", "Sein", "Feind", "her\u00b7ge\u00b7gen", "fort"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$.", "PPOSAT", "NN", "ADV", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.244": {"text": "Nach Fritzlar hin/ woselbst sie beyde sonder Schlachten", "tokens": ["Nach", "Fritz\u00b7lar", "hin", "/", "wo\u00b7selbst", "sie", "bey\u00b7de", "son\u00b7der", "Schlach\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "PTKVZ", "$(", "PWAV", "PPER", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.245": {"text": "und m\u00e4rcklichen Betrieb/ die Sommer-Zeit verbrachten.", "tokens": ["und", "m\u00e4r\u00b7ck\u00b7li\u00b7chen", "Be\u00b7trieb", "/", "die", "Som\u00b7mer\u00b7Zeit", "ver\u00b7brach\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$(", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.246": {"text": "Wie es zum Herbste kam/ erschlug der Schweden-Feind", "tokens": ["Wie", "es", "zum", "Herbs\u00b7te", "kam", "/", "er\u00b7schlug", "der", "Schwe\u00b7den\u00b7Feind"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPRART", "NN", "VVFIN", "$(", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "Viel L\u00fcneburger Volck/ nunmehr der Schweden Freind/", "tokens": ["Viel", "L\u00fc\u00b7ne\u00b7bur\u00b7ger", "Volck", "/", "nun\u00b7mehr", "der", "Schwe\u00b7den", "Freind", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "NN", "$(", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "Nah an dem Weserstrom/ wie auch viel Weymar-Schaaren/", "tokens": ["Nah", "an", "dem", "We\u00b7ser\u00b7strom", "/", "wie", "auch", "viel", "Wey\u00b7ma\u00b7rSchaa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$(", "KOKOM", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Die jhren Feind bey Lohn zu suchen kommen waren/", "tokens": ["Die", "jhren", "Feind", "bey", "Lohn", "zu", "su\u00b7chen", "kom\u00b7men", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "VVINF", "VAFIN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.250": {"text": "Sie suchten aber so/ da\u00df ein drey hundert sich", "tokens": ["Sie", "such\u00b7ten", "a\u00b7ber", "so", "/", "da\u00df", "ein", "drey", "hun\u00b7dert", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$(", "KOUS", "ART", "CARD", "CARD", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.251": {"text": "Verlohren/ fast so viel verblieb auch in dem Stich/", "tokens": ["Ver\u00b7loh\u00b7ren", "/", "fast", "so", "viel", "ver\u00b7blieb", "auch", "in", "dem", "Stich", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADV", "ADV", "ADV", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Als Schlang vom Hatzfeld ward bi\u00df Hameln hin getrie-", "tokens": ["Als", "Schlang", "vom", "Hatz\u00b7feld", "ward", "bi\u00df", "Ha\u00b7meln", "hin", "ge\u00b7trie"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPRART", "NN", "VAFIN", "APPR", "NN", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "Hergegen war dem Ros\u2019 ein solcher Sieg geblieben/", "tokens": ["Her\u00b7ge\u00b7gen", "war", "dem", "Ros'", "ein", "sol\u00b7cher", "Sieg", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ART", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Da\u00df er den b\u00e4sten Kern des K\u00e4ysers Reuterey", "tokens": ["Da\u00df", "er", "den", "b\u00e4s\u00b7ten", "Kern", "des", "K\u00e4y\u00b7sers", "Reu\u00b7te\u00b7rey"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.255": {"text": "Bey Zigenhahn erschlug ", "tokens": ["Bey", "Zi\u00b7gen\u00b7hahn", "er\u00b7schlug"], "token_info": ["word", "word", "word"], "pos": ["APPR", "NN", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.256": {"text": "Als ihren General. Es war nun in den Tagen/", "tokens": ["Als", "ih\u00b7ren", "Ge\u00b7ne\u00b7ral", ".", "Es", "war", "nun", "in", "den", "Ta\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "$.", "PPER", "VAFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "Da man das Feld verl\u00e4sst/ und vor des Winters-Plagen", "tokens": ["Da", "man", "das", "Feld", "ver\u00b7l\u00e4sst", "/", "und", "vor", "des", "Win\u00b7ter\u00b7sPla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "NN", "VVFIN", "$(", "KON", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Sich nach den St\u00e4dten giebt/ daher auch jede Schaar", "tokens": ["Sich", "nach", "den", "St\u00e4d\u00b7ten", "giebt", "/", "da\u00b7her", "auch", "je\u00b7de", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "VVFIN", "$(", "PAV", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.259": {"text": "Jm Aufzug aus dem Feld in jhre St\u00e4dte war.", "tokens": ["Jm", "Auf\u00b7zug", "aus", "dem", "Feld", "in", "jhre", "St\u00e4d\u00b7te", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "APPR", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.260": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.261": {"text": "Der Wahl das Ertzstrifft C\u00f6lln/ Geleen des Reyhnes", "tokens": ["Der", "Wahl", "das", "Ertz\u00b7strifft", "C\u00f6lln", "/", "Ge\u00b7leen", "des", "Reyh\u00b7nes"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "NE", "$(", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.262": {"text": "Rancken/", "tokens": ["Ran\u00b7cken", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.263": {"text": "Hatzfeld das G\u00fclcher Land. Hergegen nahm Bannier", "tokens": ["Hatz\u00b7feld", "das", "G\u00fclc\u00b7her", "Land", ".", "Her\u00b7ge\u00b7gen", "nahm", "Ban\u00b7nier"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$.", "NN", "VVFIN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Das Quedlin-Magdeburg- und Halberst\u00e4ttsche f\u00fcr/", "tokens": ["Das", "Qued\u00b7lin\u00b7Mag\u00b7de\u00b7bur\u00b7g", "und", "Hal\u00b7ber\u00b7st\u00e4tt\u00b7sche", "f\u00fcr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "APPR", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.265": {"text": "Auch was zur ", "tokens": ["Auch", "was", "zur"], "token_info": ["word", "word", "word"], "pos": ["ADV", "PWS", "APPRART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.266": {"text": "In solchen F\u00e4llen wird kein Ort f\u00fcr frey gesch\u00e4tzet.", "tokens": ["In", "sol\u00b7chen", "F\u00e4l\u00b7len", "wird", "kein", "Ort", "f\u00fcr", "frey", "ge\u00b7sch\u00e4t\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "PIAT", "NN", "APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.267": {"text": "Das Weymar-Volck den Hartz/ als Stoll- und Kletten-", "tokens": ["Das", "Wey\u00b7ma\u00b7rVolck", "den", "Hartz", "/", "als", "Stoll", "und", "Klet\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "$(", "KOUS", "TRUNC", "KON", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.268": {"text": "Wie auch die Herrschafft Lor. Die Cassel-Hessen-St\u00e4rck", "tokens": ["Wie", "auch", "die", "Herr\u00b7schafft", "Lor", ".", "Die", "Cas\u00b7sel\u00b7Hes\u00b7sen\u00b7St\u00e4rck"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "NE", "$.", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.269": {"text": "Oost-Frie\u00dfland/ Schaumburg/ Marck und M\u00fcnster. Wolf-", "tokens": ["O\u00b7ost\u00b7Frie\u00df\u00b7land", "/", "Schaum\u00b7burg", "/", "Marck", "und", "M\u00fcns\u00b7ter", ".", "Wolf"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "$(", "NN", "KON", "NN", "$.", "TRUNC"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.270": {"text": "Wie auch gantz L\u00fcneburg gab fenem Volcke Mittel.", "tokens": ["Wie", "auch", "gantz", "L\u00fc\u00b7ne\u00b7burg", "gab", "fe\u00b7nem", "Vol\u00b7cke", "Mit\u00b7tel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "NE", "VVFIN", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Seht also theilten sich die gro\u00dfen Machten aus.", "tokens": ["Seht", "al\u00b7so", "theil\u00b7ten", "sich", "die", "gro\u00b7\u00dfen", "Mach\u00b7ten", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Drey Helden giengen auch um diese Zeit nach Hau\u00df.", "tokens": ["Drey", "Hel\u00b7den", "gien\u00b7gen", "auch", "um", "die\u00b7se", "Zeit", "nach", "Hau\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ADV", "APPR", "PDAT", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Als der von ", "tokens": ["Als", "der", "von"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "ART", "APPR"], "meter": "+-+", "measure": "trochaic.di"}, "line.274": {"text": "Dem der ", "tokens": ["Dem", "der"], "token_info": ["word", "word"], "pos": ["PDS", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.275": {"text": "Gefolget. Torsten Sohn/ der hochbegl\u00fcckte Mann/", "tokens": ["Ge\u00b7fol\u00b7get", ".", "Tors\u00b7ten", "Sohn", "/", "der", "hoch\u00b7be\u00b7gl\u00fcck\u00b7te", "Mann", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "NN", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "Der endlich nach Banniern so gro\u00dfes hat gethan.", "tokens": ["Der", "end\u00b7lich", "nach", "Ban\u00b7ni\u00b7ern", "so", "gro\u00b7\u00dfes", "hat", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NN", "ADV", "ADJA", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.277": {"text": "Melander/ welcher sich bey Hessen so gehalten/", "tokens": ["Me\u00b7lan\u00b7der", "/", "wel\u00b7cher", "sich", "bey", "Hes\u00b7sen", "so", "ge\u00b7hal\u00b7ten", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "PRELS", "PRF", "APPR", "NE", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.278": {"text": "Da\u00df dessen Landgraf jhn das gr\u00f6ste lie\u00df verwalten.", "tokens": ["Da\u00df", "des\u00b7sen", "Land\u00b7graf", "jhn", "das", "gr\u00f6s\u00b7te", "lie\u00df", "ver\u00b7wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "VVFIN", "PPER", "ART", "ADJA", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "Er/ wie auch Torsten Sohn/ gieng etwas zwistig ab/", "tokens": ["Er", "/", "wie", "auch", "Tors\u00b7ten", "Sohn", "/", "gieng", "et\u00b7was", "zwis\u00b7tig", "ab", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$(", "KOKOM", "ADV", "ADJA", "NN", "$(", "VVFIN", "ADV", "ADJD", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "und nahm der Eberstein hierauf Melanders Stab.", "tokens": ["und", "nahm", "der", "E\u00b7bers\u00b7tein", "hier\u00b7auf", "Me\u00b7lan\u00b7ders", "Stab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PAV", "NE", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Es war die Winter-Zeit noch nicht zur Helffte kommen/", "tokens": ["Es", "war", "die", "Win\u00b7ter\u00b7Zeit", "noch", "nicht", "zur", "Helff\u00b7te", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "PTKNEG", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Wurd unverhofft ein Zug von Schweden vorgenommen/", "tokens": ["Wurd", "un\u00b7ver\u00b7hofft", "ein", "Zug", "von", "Schwe\u00b7den", "vor\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "ART", "NN", "APPR", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "Der schnell und listig gieng. Es kam Cur-Sachsens Schaar", "tokens": ["Der", "schnell", "und", "lis\u00b7tig", "gieng", ".", "Es", "kam", "Cur\u00b7Sach\u00b7sens", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "KON", "ADJD", "VVFIN", "$.", "PPER", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Vor seinem Zwickau an/ und weil es Schwedisch war/", "tokens": ["Vor", "sei\u00b7nem", "Zwi\u00b7ckau", "an", "/", "und", "weil", "es", "Schwe\u00b7disch", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$(", "KON", "KOUS", "PPER", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "Gebrauchte sie Gewalt/ die Schweden rau\u00df zu bringen.", "tokens": ["Ge\u00b7brauch\u00b7te", "sie", "Ge\u00b7walt", "/", "die", "Schwe\u00b7den", "rau\u00df", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$(", "ART", "NE", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.286": {"text": "Da stellte sich Bannier/ sich nach der Stadt zu schwingen/", "tokens": ["Da", "stell\u00b7te", "sich", "Ban\u00b7nier", "/", "sich", "nach", "der", "Stadt", "zu", "schwin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "NE", "$(", "PRF", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.287": {"text": "um solcher Hilf zu thun/ und muste Pfuel voran.", "tokens": ["um", "sol\u00b7cher", "Hilf", "zu", "thun", "/", "und", "mus\u00b7te", "Pfu\u00b7el", "vo\u00b7ran", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PTKZU", "VVINF", "$(", "KON", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.288": {"text": "Doch es war nur darum/ jhm eine gute Bahn", "tokens": ["Doch", "es", "war", "nur", "da\u00b7rum", "/", "jhm", "ei\u00b7ne", "gu\u00b7te", "Bahn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "PAV", "$(", "PPER", "ART", "ADJA", "NN"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.289": {"text": "Nach einer b\u00e4ssern Stadt/ als Zwickau war/ zu machen/", "tokens": ["Nach", "ei\u00b7ner", "b\u00e4s\u00b7sern", "Stadt", "/", "als", "Zwi\u00b7ckau", "war", "/", "zu", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "KOUS", "NN", "VAFIN", "$(", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Jhr Weg/ den sie durch Schnee und Ei\u00df knickknasternd bra-", "tokens": ["Ihr", "Weg", "/", "den", "sie", "durch", "Schnee", "und", "Ei\u00df", "knick\u00b7knas\u00b7ternd", "bra"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$(", "ART", "PPER", "APPR", "NN", "KON", "NN", "VVPP", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "War recht nach Regenspurg/ ", "tokens": ["War", "recht", "nach", "Re\u00b7gen\u00b7spurg", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "NE", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.292": {"text": "Bey ein versamlet war/ von welchem schnellen Streich", "tokens": ["Bey", "ein", "ver\u00b7sam\u00b7let", "war", "/", "von", "wel\u00b7chem", "schnel\u00b7len", "Streich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "VVPP", "VAFIN", "$(", "APPR", "PWAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Ein jeder zaghafft schien. Es folgeten den Schweden", "tokens": ["Ein", "je\u00b7der", "zag\u00b7hafft", "schien", ".", "Es", "fol\u00b7ge\u00b7ten", "den", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Die Frantzen schleunig nach. Was gab es da zu reden?", "tokens": ["Die", "Frant\u00b7zen", "schleu\u00b7nig", "nach", ".", "Was", "gab", "es", "da", "zu", "re\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKVZ", "$.", "PWS", "VVFIN", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Da man vermeynete sie w\u00e4ren weit von hier/", "tokens": ["Da", "man", "ver\u00b7mey\u00b7ne\u00b7te", "sie", "w\u00e4\u00b7ren", "weit", "von", "hier", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "PPER", "VAFIN", "ADJD", "APPR", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "Sah sie das gantze Reich mit Hauffen vor der Th\u00fcr.", "tokens": ["Sah", "sie", "das", "gant\u00b7ze", "Reich", "mit", "Hauf\u00b7fen", "vor", "der", "Th\u00fcr."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.297": {"text": "Die Donau war beeist und frey zu \u00fcbergehen/", "tokens": ["Die", "Do\u00b7nau", "war", "be\u00b7eist", "und", "frey", "zu", "\u00fc\u00b7ber\u00b7ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "ADJD", "KON", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Es war kein Widerstand noch dort noch da zu sehen.", "tokens": ["Es", "war", "kein", "Wi\u00b7der\u00b7stand", "noch", "dort", "noch", "da", "zu", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "ADV", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.299": {"text": "Was in dem Wege war fiel durch das Schwerdt dahin.", "tokens": ["Was", "in", "dem", "We\u00b7ge", "war", "fiel", "durch", "das", "Schwerdt", "da\u00b7hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VAFIN", "VVFIN", "APPR", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Und nun sah man die Stadt mit Macht rund um beziehn", "tokens": ["Und", "nun", "sah", "man", "die", "Stadt", "mit", "Macht", "rund", "um", "be\u00b7ziehn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PIS", "ART", "NN", "APPR", "NN", "ADJD", "APPR", "VVINF"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.301": {"text": "Mit der Cartaunen-Knall und feuerigen Ballen/", "tokens": ["Mit", "der", "Car\u00b7tau\u00b7nen\u00b7Knall", "und", "feu\u00b7e\u00b7ri\u00b7gen", "Bal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.302": {"text": "Zu Jhrer Majest\u00e4t Entsetzung \u00fcberfallen.", "tokens": ["Zu", "Ih\u00b7rer", "Ma\u00b7jes\u00b7t\u00e4t", "Ent\u00b7set\u00b7zung", "\u00fc\u00b7berf\u00b7al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.303": {"text": "Wie aber dieser Zug sehr unvermuhtlich war/", "tokens": ["Wie", "a\u00b7ber", "die\u00b7ser", "Zug", "sehr", "un\u00b7ver\u00b7muht\u00b7lich", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PDAT", "NN", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Also gerieth er auch sehr schleunig in Gefahr.", "tokens": ["Al\u00b7so", "ge\u00b7rieth", "er", "auch", "sehr", "schleu\u00b7nig", "in", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.305": {"text": "Der K\u00e4yser lie\u00df sein Volck von allen Seiten kommen/", "tokens": ["Der", "K\u00e4y\u00b7ser", "lie\u00df", "sein", "Volck", "von", "al\u00b7len", "Sei\u00b7ten", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "APPR", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "Das auch sehr eilend hat den Weg nach jhm genommen/", "tokens": ["Das", "auch", "sehr", "ei\u00b7lend", "hat", "den", "Weg", "nach", "jhm", "ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "VVPP", "VAFIN", "ART", "NN", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Worauf Bannier bi\u00df Cam sich was zu r\u00fccke schwung/", "tokens": ["Wo\u00b7rauf", "Ban\u00b7nier", "bi\u00df", "Cam", "sich", "was", "zu", "r\u00fc\u00b7cke", "schwung", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "NN", "APPR", "NE", "PRF", "PIS", "PTKZU", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Weil eine gro\u00dfe Macht jhm auf den R\u00fccken drung.", "tokens": ["Weil", "ei\u00b7ne", "gro\u00b7\u00dfe", "Macht", "jhm", "auf", "den", "R\u00fc\u00b7cken", "drung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "PPER", "APPR", "ART", "NN", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Er satzte sich zu Cam/ daselbst der andern Hauffen/", "tokens": ["Er", "satz\u00b7te", "sich", "zu", "Cam", "/", "da\u00b7selbst", "der", "an\u00b7dern", "Hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NE", "$(", "PAV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.310": {"text": "Die in der Ober-Pfaltz weit waren au\u00dfgelauffen/", "tokens": ["Die", "in", "der", "O\u00b7ber\u00b7Pfaltz", "weit", "wa\u00b7ren", "au\u00df\u00b7ge\u00b7lauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.311": {"text": "Bevor des tapfren Schlangs mit viermal tausend Mann", "tokens": ["Be\u00b7vor", "des", "tapf\u00b7ren", "Schlangs", "mit", "vier\u00b7mal", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.312": {"text": "Zu warten. Aber seht/ da\u00df es nicht allzeit kan", "tokens": ["Zu", "war\u00b7ten", ".", "A\u00b7ber", "seht", "/", "da\u00df", "es", "nicht", "all\u00b7zeit", "kan"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$.", "KON", "VVFIN", "$(", "KOUS", "PPER", "PTKNEG", "ADV", "VMFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "Erfolgen wie man will! Der Pa\u00df wurd abgeschnitten/", "tokens": ["Er\u00b7fol\u00b7gen", "wie", "man", "will", "!", "Der", "Pa\u00df", "wurd", "ab\u00b7ge\u00b7schnit\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "PIS", "VMFIN", "$.", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.314": {"text": "und kam der schlaue Schlang zu Neuburg in die Mitten", "tokens": ["und", "kam", "der", "schlau\u00b7e", "Schlang", "zu", "Neu\u00b7burg", "in", "die", "Mit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPR", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Von seiner Wider-Part. Er kr\u00fcmmte sich/ er that", "tokens": ["Von", "sei\u00b7ner", "Wi\u00b7der\u00b7Part", ".", "Er", "kr\u00fcmm\u00b7te", "sich", "/", "er", "that"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "$.", "PPER", "VVFIN", "PRF", "$(", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.316": {"text": "Mit gro\u00dfer Gegenwehr/ so viel als solche Stadt", "tokens": ["Mit", "gro\u00b7\u00dfer", "Ge\u00b7gen\u00b7wehr", "/", "so", "viel", "als", "sol\u00b7che", "Stadt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$(", "ADV", "PIAT", "KOKOM", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.317": {"text": "Erley den kunt\u2019. Er schrieb um Hilff/ es war verlohren/", "tokens": ["Er\u00b7ley", "den", "kunt'", ".", "Er", "schrieb", "um", "Hilff", "/", "es", "war", "ver\u00b7loh\u00b7ren", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ART", "PTKVZ", "$.", "PPER", "VVFIN", "APPR", "NN", "$(", "PPER", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.318": {"text": "Es war die gantze Macht des Feindes vor den Thoren.", "tokens": ["Es", "war", "die", "gant\u00b7ze", "Macht", "des", "Fein\u00b7des", "vor", "den", "Tho\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "Er war zum dritten mal heraus/ sich durch die Macht", "tokens": ["Er", "war", "zum", "drit\u00b7ten", "mal", "he\u00b7raus", "/", "sich", "durch", "die", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPRART", "ADJA", "ADV", "PTKVZ", "$(", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.320": {"text": "Zu schlagen/ welches jhm nur Schaden hat gebracht.", "tokens": ["Zu", "schla\u00b7gen", "/", "wel\u00b7ches", "jhm", "nur", "Scha\u00b7den", "hat", "ge\u00b7bracht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPER", "ADV", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.321": {"text": "Es war auch endlich nichts vom Pulver mehr zugegen/", "tokens": ["Es", "war", "auch", "end\u00b7lich", "nichts", "vom", "Pul\u00b7ver", "mehr", "zu\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIS", "APPRART", "NN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.322": {"text": "und dennoch wehrt\u2019 er sich mit Steinen von den Wegen.", "tokens": ["und", "den\u00b7noch", "wehrt'", "er", "sich", "mit", "Stei\u00b7nen", "von", "den", "We\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "APPR", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.323": {"text": "Da alles was er that vor jhn vergebens war/", "tokens": ["Da", "al\u00b7les", "was", "er", "that", "vor", "jhn", "ver\u00b7ge\u00b7bens", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PWS", "PPER", "VVFIN", "APPR", "PPER", "ADV", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.324": {"text": "Ergab er thr\u00e4nend sich ", "tokens": ["Er\u00b7gab", "er", "thr\u00e4\u00b7nend", "sich"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD", "PRF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.325": {"text": "Von vierthalb tausend Mann/ auf Gnad und ", "tokens": ["Von", "vier\u00b7thalb", "tau\u00b7send", "Mann", "/", "auf", "Gnad", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "CARD", "CARD", "NN", "$(", "APPR", "NN", "KON"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.326": {"text": "Hierbey war Marggraf Carl vom Hause Durlach-Badeu", "tokens": ["Hier\u00b7bey", "war", "Marg\u00b7graf", "Carl", "vom", "Hau\u00b7se", "Dur\u00b7lach\u00b7Ba\u00b7deu"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "NE", "NE", "APPRART", "NN", "NE"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.327": {"text": "und andre gro\u00dfe mehr. Sih/ eine gute Macht", "tokens": ["und", "and\u00b7re", "gro\u00b7\u00dfe", "mehr", ".", "Sih", "/", "ei\u00b7ne", "gu\u00b7te", "Macht"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJA", "ADJA", "ADV", "$.", "NE", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-++--+-+", "measure": "iambic.hexa.relaxed"}, "line.328": {"text": "Von Feinden vor das Reich in Regenspurg gebracht.", "tokens": ["Von", "Fein\u00b7den", "vor", "das", "Reich", "in", "Re\u00b7gen\u00b7spurg", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.329": {"text": "Wie triumpfirend man sich hab hierob erzeiget/", "tokens": ["Wie", "tri\u00b7ump\u00b7fi\u00b7rend", "man", "sich", "hab", "hier\u00b7ob", "er\u00b7zei\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PIS", "PRF", "VAFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "Ist leicht erachtt/ daher mein Mund von solchem schweiget.", "tokens": ["Ist", "leicht", "e\u00b7rachtt", "/", "da\u00b7her", "mein", "Mund", "von", "sol\u00b7chem", "schwei\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$(", "PAV", "PPOSAT", "NN", "APPR", "PIAT", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Wie schmertzlich aber Jhn Banniern so gro\u00dfer Sto\u00df", "tokens": ["Wie", "schmertz\u00b7lich", "a\u00b7ber", "Jhn", "Ban\u00b7ni\u00b7ern", "so", "gro\u00b7\u00dfer", "Sto\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "KON", "PPER", "NN", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.332": {"text": "Getroffen/ zeigete hernach sich allzu gro\u00df.", "tokens": ["Ge\u00b7trof\u00b7fen", "/", "zei\u00b7ge\u00b7te", "her\u00b7nach", "sich", "all\u00b7zu", "gro\u00df", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "VVFIN", "ADV", "PRF", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.333": {"text": "Da\u00df er nicht so wie Schlang m\u00f6cht \u00fcbereilet werden/", "tokens": ["Da\u00df", "er", "nicht", "so", "wie", "Schlang", "m\u00f6cht", "\u00fc\u00b7be\u00b7rei\u00b7let", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "KOKOM", "NN", "VMFIN", "VVPP", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Lie\u00df er die Ober-Pfaltz und brauchte B\u00f6haims Erden", "tokens": ["Lie\u00df", "er", "die", "O\u00b7ber\u00b7Pfaltz", "und", "brauch\u00b7te", "B\u00f6\u00b7haims", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "KON", "VVFIN", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Zu seinem R\u00fccke-Zug nach Zwickau. Dem der Feind", "tokens": ["Zu", "sei\u00b7nem", "R\u00fc\u00b7cke\u00b7Zug", "nach", "Zwi\u00b7ckau", ".", "Dem", "der", "Feind"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "NE", "$.", "ART", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.336": {"text": "Allzeie im Nacken war/ dem er doch unvermeynt", "tokens": ["All\u00b7zei\u00b7e", "im", "Na\u00b7cken", "war", "/", "dem", "er", "doch", "un\u00b7ver\u00b7meynt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPRART", "NN", "VAFIN", "$(", "PRELS", "PPER", "ADV", "ADJD"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.337": {"text": "Entgieng. Sein Zug war schnell und dorffte keinen Pfer-", "tokens": ["Ent\u00b7gieng", ".", "Sein", "Zug", "war", "schnell", "und", "dorff\u00b7te", "kei\u00b7nen", "Pfer"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "PPOSAT", "NN", "VAFIN", "ADJD", "KON", "VMFIN", "PIAT", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.338": {"text": "Bey vierzehn Tagen lang die Last entnommen werden.", "tokens": ["Bey", "vier\u00b7zehn", "Ta\u00b7gen", "lang", "die", "Last", "ent\u00b7nom\u00b7men", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "ADJD", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.339": {"text": "Zu zwickau ruhet\u2019 er mit seiner gantzen Schaar/", "tokens": ["Zu", "zwi\u00b7ckau", "ru\u00b7het'", "er", "mit", "sei\u00b7ner", "gant\u00b7zen", "Schaar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.340": {"text": "Die neben jhm sehr m\u00fcd und abgemattet war.", "tokens": ["Die", "ne\u00b7ben", "jhm", "sehr", "m\u00fcd", "und", "ab\u00b7ge\u00b7mat\u00b7tet", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPER", "ADV", "ADJD", "KON", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.341": {"text": "Als bey den Weymarschen das ", "tokens": ["Als", "bey", "den", "Wey\u00b7mar\u00b7schen", "das"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "ART"], "meter": "-+-++-+", "measure": "unknown.measure.tetra"}, "line.342": {"text": "Ver\u00e4nderten sie stracks jhr wolgefa\u00dftes Wollen", "tokens": ["Ver\u00b7\u00e4n\u00b7der\u00b7ten", "sie", "stracks", "jhr", "wol\u00b7ge\u00b7fa\u00df\u00b7tes", "Wol\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.343": {"text": "Nach Beyern/ eileten auch bald den Schweden zu.", "tokens": ["Nach", "Be\u00b7yern", "/", "ei\u00b7le\u00b7ten", "auch", "bald", "den", "Schwe\u00b7den", "zu", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "VVFIN", "ADV", "ADV", "ART", "NE", "PTKVZ", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.344": {"text": "Sie hatten aber auch nechst jhnen wenig Ruh.", "tokens": ["Sie", "hat\u00b7ten", "a\u00b7ber", "auch", "nechst", "jh\u00b7nen", "we\u00b7nig", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.345": {"text": "Dieweil die K\u00e4ysrischen von dort und da ankamen/", "tokens": ["Die\u00b7weil", "die", "K\u00e4y\u00b7sri\u00b7schen", "von", "dort", "und", "da", "an\u00b7ka\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "ADV", "KON", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.346": {"text": "Jedoch nichts sonderlichs von Treffen unternahmen.", "tokens": ["Je\u00b7doch", "nichts", "son\u00b7der\u00b7lichs", "von", "Tref\u00b7fen", "un\u00b7ter\u00b7nah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "PIS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.347": {"text": "Der schnell und lange Zug hatt alle m\u00fcd gemacht/", "tokens": ["Der", "schnell", "und", "lan\u00b7ge", "Zug", "hatt", "al\u00b7le", "m\u00fcd", "ge\u00b7macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJA", "NN", "VAFIN", "PIS", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.348": {"text": "und darum scheuete sich jeglicher zur Schlacht.", "tokens": ["und", "da\u00b7rum", "scheu\u00b7e\u00b7te", "sich", "jeg\u00b7li\u00b7cher", "zur", "Schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PRF", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.349": {"text": "Es war auch dem Bannier solch Fieber angekommen/", "tokens": ["Es", "war", "auch", "dem", "Ban\u00b7nier", "solch", "Fie\u00b7ber", "an\u00b7ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.350": {"text": "Davon jhm alle Krafft wurd eilends weg genommen.", "tokens": ["Da\u00b7von", "jhm", "al\u00b7le", "Krafft", "wurd", "ei\u00b7lends", "weg", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "PIAT", "NN", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.351": {"text": "Um dieses gieng sein Heer mit jhm sehr schwach und matt", "tokens": ["Um", "die\u00b7ses", "gieng", "sein", "Heer", "mit", "jhm", "sehr", "schwach", "und", "matt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PDS", "VVFIN", "PPOSAT", "NN", "APPR", "PPER", "ADV", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.352": {"text": "Durchs Anhaltiner Land ins Stifft von Halber stadt/", "tokens": ["Durchs", "An\u00b7hal\u00b7ti\u00b7ner", "Land", "ins", "Stifft", "von", "Hal\u00b7ber", "stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPRART", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.353": {"text": "Woselben Jhn sein Geist nach einem tapfren Leben/", "tokens": ["Wo\u00b7sel\u00b7ben", "Jhn", "sein", "Geist", "nach", "ei\u00b7nem", "tapf\u00b7ren", "Le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.354": {"text": "Zu seines Heeres Leyd und weinen/ hat begeben.", "tokens": ["Zu", "sei\u00b7nes", "Hee\u00b7res", "Leyd", "und", "wei\u00b7nen", "/", "hat", "be\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "KON", "VVINF", "$(", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.355": {"text": "Er war ein Herr geschwind von Raht und starck von That.", "tokens": ["Er", "war", "ein", "Herr", "ge\u00b7schwind", "von", "Raht", "und", "starck", "von", "That", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "APPR", "NN", "KON", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.356": {"text": "H\u00f6r was man an sein Grab von jhm geschrieben hat:", "tokens": ["H\u00f6r", "was", "man", "an", "sein", "Grab", "von", "jhm", "ge\u00b7schrie\u00b7ben", "hat", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PWS", "PIS", "APPR", "PPOSAT", "NN", "APPR", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.357": {"text": "Enthebet euch der M\u00fch zu mahlen Herrn Bannieren/", "tokens": ["Ent\u00b7he\u00b7bet", "euch", "der", "M\u00fch", "zu", "mah\u00b7len", "Herrn", "Ban\u00b7nie\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKZU", "VVINF", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.358": {"text": "Er ist sein Mahler selbst/ er kan sich selber zieren", "tokens": ["Er", "ist", "sein", "Mah\u00b7ler", "selbst", "/", "er", "kan", "sich", "sel\u00b7ber", "zie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "ADV", "$(", "PPER", "VMFIN", "PRF", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.359": {"text": "Mit lebendiger Farb. Sein Pinsel ist sein Schwert/", "tokens": ["Mit", "le\u00b7ben\u00b7di\u00b7ger", "Farb", ".", "Sein", "Pin\u00b7sel", "ist", "sein", "Schwert", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.360": {"text": "Die Farb der Feinde Blut/ das Blat Europens Erd\u2019.", "tokens": ["Die", "Farb", "der", "Fein\u00b7de", "Blut", "/", "das", "Blat", "Eu\u00b7ro\u00b7pens", "Erd'", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "NN", "$(", "ART", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.361": {"text": "In dem sein Abscheid war in Schweden kuntbar worden/", "tokens": ["In", "dem", "sein", "Ab\u00b7scheid", "war", "in", "Schwe\u00b7den", "kunt\u00b7bar", "wor\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PPOSAT", "NN", "VAFIN", "APPR", "NE", "ADJD", "VAPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.362": {"text": "Sah dessen K\u00f6nigin nach jhren Helden-Orden/", "tokens": ["Sah", "des\u00b7sen", "K\u00f6\u00b7ni\u00b7gin", "nach", "jhren", "Hel\u00b7den\u00b7Or\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.363": {"text": "und w\u00e4hlte Torsten Sohn an des Verlebten Stat/", "tokens": ["und", "w\u00e4hl\u00b7te", "Tors\u00b7ten", "Sohn", "an", "des", "Ver\u00b7leb\u00b7ten", "Stat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.364": {"text": "Der/ solches anzugehn/ sein allerb\u00e4stes that.", "tokens": ["Der", "/", "sol\u00b7ches", "an\u00b7zu\u00b7gehn", "/", "sein", "al\u00b7ler\u00b7b\u00e4s\u00b7tes", "that", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PIS", "VVIZU", "$(", "PPOSAT", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.365": {"text": "In dem er aber sich zum Schiff- und Feld-Zug schickte/", "tokens": ["In", "dem", "er", "a\u00b7ber", "sich", "zum", "Schiff", "und", "Feld\u00b7Zug", "schick\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "PRF", "APPRART", "TRUNC", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.366": {"text": "Geschah es/ da\u00df der Feind nach Wolffenb\u00fcttel r\u00fcckte/", "tokens": ["Ge\u00b7schah", "es", "/", "da\u00df", "der", "Feind", "nach", "Wolf\u00b7fen\u00b7b\u00fct\u00b7tel", "r\u00fcck\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$(", "KOUS", "ART", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.367": {"text": "Die gro\u00dfe Vestung die/ vor jhrem rechten Herrn/", "tokens": ["Die", "gro\u00b7\u00dfe", "Ves\u00b7tung", "die", "/", "vor", "jhrem", "rech\u00b7ten", "Herrn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "$(", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.368": {"text": "Der sie sehr eng beschlo\u00df/ noch l\u00e4nger zu versperrn.", "tokens": ["Der", "sie", "sehr", "eng", "be\u00b7schlo\u00df", "/", "noch", "l\u00e4n\u00b7ger", "zu", "ver\u00b7sperrn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADJD", "VVFIN", "$(", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.369": {"text": "Und mocht' es m\u00f6glich seyn/ jhm einen Schlag zu geben.", "tokens": ["Und", "mocht'", "es", "m\u00f6g\u00b7lich", "seyn", "/", "jhm", "ei\u00b7nen", "Schlag", "zu", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "VAINF", "$(", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.370": {"text": "Auf dieses sahe man di\u00df alles sich erheben", "tokens": ["Auf", "die\u00b7ses", "sa\u00b7he", "man", "di\u00df", "al\u00b7les", "sich", "er\u00b7he\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "VVFIN", "PIS", "PDS", "PIS", "PRF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.371": {"text": "Was Schwed- und Hessisch war/ des L\u00fcneburgers Hand", "tokens": ["Was", "Schwe\u00b7d", "und", "Hes\u00b7sisch", "war", "/", "des", "L\u00fc\u00b7ne\u00b7bur\u00b7gers", "Hand"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "TRUNC", "KON", "NE", "VAFIN", "$(", "ART", "NN", "NN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.372": {"text": "Zu steiffen/ und hierdurch der Vestung nassen Stand", "tokens": ["Zu", "steif\u00b7fen", "/", "und", "hier\u00b7durch", "der", "Ves\u00b7tung", "nas\u00b7sen", "Stand"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$(", "KON", "PAV", "ART", "NN", "ADJA", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.373": {"text": "Noch mehr zu \u00e4ngstigen. Es kam hierob ", "tokens": ["Noch", "mehr", "zu", "\u00e4ngs\u00b7ti\u00b7gen", ".", "Es", "kam", "hier\u00b7ob"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "PTKZU", "VVINF", "$.", "PPER", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.374": {"text": "und lenckte sich der Sieg nochmahls zur Schwedschen Sei-", "tokens": ["und", "lenck\u00b7te", "sich", "der", "Sieg", "noch\u00b7mahls", "zur", "Schwed\u00b7schen", "Sei"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "ART", "NN", "ADV", "APPRART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.375": {"text": "So da\u00df zwey tansend Mann von K\u00e4ysrischer Parthey", "tokens": ["So", "da\u00df", "zwey", "tan\u00b7send", "Mann", "von", "K\u00e4y\u00b7sri\u00b7scher", "Par\u00b7they"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "CARD", "CARD", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.376": {"text": "Verstarben/ und so viel mit einer langen Rey", "tokens": ["Ver\u00b7star\u00b7ben", "/", "und", "so", "viel", "mit", "ei\u00b7ner", "lan\u00b7gen", "Rey"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "KON", "ADV", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.377": {"text": "Von Fahnen zum Triumff gefangen musten bleiben.", "tokens": ["Von", "Fah\u00b7nen", "zum", "Tri\u00b7umff", "ge\u00b7fan\u00b7gen", "mus\u00b7ten", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPRART", "NN", "VVINF", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.378": {"text": "Wem aber haben wir di\u00df Siegen zu zu schreiben?", "tokens": ["Wem", "a\u00b7ber", "ha\u00b7ben", "wir", "di\u00df", "Sie\u00b7gen", "zu", "zu", "schrei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VAFIN", "PPER", "PDS", "NN", "PTKZU", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.379": {"text": "Da waren Wrangel/ Pfuel und Wittenberg an statt", "tokens": ["Da", "wa\u00b7ren", "Wran\u00b7gel", "/", "Pfu\u00b7el", "und", "Wit\u00b7ten\u00b7berg", "an", "statt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "NE", "$(", "NN", "KON", "NN", "APPR", "NN"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.380": {"text": "Des tapferen Banniers/ da war Lars Kagg. Es that", "tokens": ["Des", "tap\u00b7fe\u00b7ren", "Ban\u00b7niers", "/", "da", "war", "Lars", "Kagg", ".", "Es", "that"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$(", "ADV", "VAFIN", "NE", "NE", "$.", "PPER", "VVFIN"], "meter": "-+--++-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.381": {"text": "Ein jeder was er solt. Es hat sich auch nechst diesen", "tokens": ["Ein", "je\u00b7der", "was", "er", "solt", ".", "Es", "hat", "sich", "auch", "nechst", "die\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "PWS", "PPER", "VMFIN", "$.", "PPER", "VAFIN", "PRF", "ADV", "ADV", "PDAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.382": {"text": "Der Graf von Eberstein und Klitzing so erwiesen/", "tokens": ["Der", "Graf", "von", "E\u00b7bers\u00b7tein", "und", "Klit\u00b7zing", "so", "er\u00b7wie\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.383": {"text": "Da\u00df sich das L\u00fcneburg und Hessen r\u00fchmen kunt\u2019/", "tokens": ["Da\u00df", "sich", "das", "L\u00fc\u00b7ne\u00b7burg", "und", "Hes\u00b7sen", "r\u00fch\u00b7men", "kunt'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "KON", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.384": {"text": "Auch war den Weymarschen hier Ehr und Sieg vergunt\u2019", "tokens": ["Auch", "war", "den", "Wey\u00b7mar\u00b7schen", "hier", "Ehr", "und", "Sieg", "vergunt'"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "NN", "KON", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.385": {"text": "Damit die K\u00e4ysrische den frisch-erlittnen Schaden", "tokens": ["Da\u00b7mit", "die", "K\u00e4y\u00b7sri\u00b7sche", "den", "frischer\u00b7litt\u00b7nen", "Scha\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+---+--+-", "measure": "iambic.tetra.relaxed"}, "line.386": {"text": "Verschmertzten/ wichen sie/ und nahmen Go\u00dflar/ Schladen/", "tokens": ["Ver\u00b7schmertz\u00b7ten", "/", "wi\u00b7chen", "sie", "/", "und", "nah\u00b7men", "Go\u00df\u00b7lar", "/", "Schla\u00b7den", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "PPER", "$(", "KON", "VVFIN", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.387": {"text": "Hornburg und Osterwiek zu jhrem Aufenthalt/", "tokens": ["Horn\u00b7burg", "und", "Os\u00b7ter\u00b7wiek", "zu", "jhrem", "Auf\u00b7ent\u00b7halt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.388": {"text": "und thaten jhrem Feind hiedurch sehr viel Gewalt", "tokens": ["und", "tha\u00b7ten", "jhrem", "Feind", "hie\u00b7durch", "sehr", "viel", "Ge\u00b7walt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "PAV", "ADV", "PIAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.389": {"text": "An seiner Futterung/ so/ da\u00df er Wolffenb\u00fcttel/", "tokens": ["An", "sei\u00b7ner", "Fut\u00b7te\u00b7rung", "/", "so", "/", "da\u00df", "er", "Wolf\u00b7fen\u00b7b\u00fct\u00b7tel", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$(", "ADV", "$(", "KOUS", "PPER", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.390": {"text": "Das schon im Letzten lag/ aus Noth vom Lebens-Mittel/", "tokens": ["Das", "schon", "im", "Letz\u00b7ten", "lag", "/", "aus", "Noth", "vom", "Le\u00b7bens\u00b7Mit\u00b7tel", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPRART", "NN", "VVFIN", "$(", "APPR", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.391": {"text": "Auch wol aus Zwist/ begab/ weil jeder diesen Ort", "tokens": ["Auch", "wol", "aus", "Zwist", "/", "be\u00b7gab", "/", "weil", "je\u00b7der", "die\u00b7sen", "Ort"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "NN", "$(", "VVFIN", "$(", "KOUS", "PIS", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.392": {"text": "Vor sich behalten wolt. Es zog ein jeder fort.", "tokens": ["Vor", "sich", "be\u00b7hal\u00b7ten", "wolt", ".", "Es", "zog", "ein", "je\u00b7der", "fort", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "VVINF", "VMFIN", "$.", "PPER", "VVFIN", "ART", "PIS", "PTKVZ", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.393": {"text": "Die Schweden satzten sich bey Giffhorn mit verlangen/", "tokens": ["Die", "Schwe\u00b7den", "satz\u00b7ten", "sich", "bey", "Giff\u00b7horn", "mit", "ver\u00b7lan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "PRF", "APPR", "NN", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.394": {"text": "Den tapfren Torsten Sohn/ jhr H\u00e4upt/ wol zu empfangen.", "tokens": ["Den", "tapf\u00b7ren", "Tors\u00b7ten", "Sohn", "/", "jhr", "H\u00e4upt", "/", "wol", "zu", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$(", "PPOSAT", "NN", "$(", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.395": {"text": "Die Weymar-Hessischen durchwanderten das Land", "tokens": ["Die", "Wey\u00b7ma\u00b7rHes\u00b7si\u00b7schen", "durch\u00b7wan\u00b7der\u00b7ten", "das", "Land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.396": {"text": "Von C\u00f6lln und brachten da Lamboyen zu dem Stand/", "tokens": ["Von", "C\u00f6lln", "und", "brach\u00b7ten", "da", "Lam\u00b7bo\u00b7yen", "zu", "dem", "Stand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "VVFIN", "ADV", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.397": {"text": "Jhm einen Schlag zu thun/ der also ", "tokens": ["Jhm", "ei\u00b7nen", "Schlag", "zu", "thun", "/", "der", "al\u00b7so"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "ART", "NN", "PTKZU", "VVINF", "$(", "ART", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.398": {"text": "Da\u00df man vier tausend Mann von jhm hat abgezwungen/", "tokens": ["Da\u00df", "man", "vier", "tau\u00b7send", "Mann", "von", "jhm", "hat", "ab\u00b7ge\u00b7zwun\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "CARD", "CARD", "NN", "APPR", "PPER", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.399": {"text": "Der siegenden Parthey in dem Verhafft zu stehn.", "tokens": ["Der", "sie\u00b7gen\u00b7den", "Par\u00b7they", "in", "dem", "Ver\u00b7hafft", "zu", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.400": {"text": "Es blieben auch hiermit von Fahnen zw\u00f6lffmal zehn/", "tokens": ["Es", "blie\u00b7ben", "auch", "hier\u00b7mit", "von", "Fah\u00b7nen", "zw\u00f6lff\u00b7mal", "zehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PAV", "APPR", "NN", "ADV", "CARD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.401": {"text": "und sechszehn noch darzu. Der Todten aber waren", "tokens": ["und", "sechs\u00b7zehn", "noch", "dar\u00b7zu", ".", "Der", "Tod\u00b7ten", "a\u00b7ber", "wa\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "CARD", "ADV", "PAV", "$.", "ART", "NN", "ADV", "VAFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.402": {"text": "Ein dreymal tausend Mann von den Lamboyschen Schaaren.", "tokens": ["Ein", "drey\u00b7mal", "tau\u00b7send", "Mann", "von", "den", "Lam\u00b7boy\u00b7schen", "Schaa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "CARD", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.403": {"text": "Auf dieses galt es Neu\u00df/ und nach dem Weynacht-Fest/", "tokens": ["Auf", "die\u00b7ses", "galt", "es", "Neu\u00df", "/", "und", "nach", "dem", "Wey\u00b7nacht\u00b7Fest", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "NN", "$(", "KON", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.404": {"text": "Zu einem nenen Jahr/ Laboyens gantzen Rest/", "tokens": ["Zu", "ei\u00b7nem", "ne\u00b7nen", "Jahr", "/", "La\u00b7bo\u00b7yens", "gant\u00b7zen", "Rest", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "NN", "ADJA", "NN", "$("], "meter": "-+---++--+-+", "measure": "iambic.penta.relaxed"}, "line.405": {"text": "So da\u00df nichts \u00fcbrig blieb. Di\u00df von den Weymar-Hessen.", "tokens": ["So", "da\u00df", "nichts", "\u00fcb\u00b7rig", "blieb", ".", "Di\u00df", "von", "den", "Wey\u00b7ma\u00b7rHes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PIS", "ADJD", "VVFIN", "$.", "PDS", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.406": {"text": "Die L\u00fcneburgische nicht g\u00e4ntzlich zu vergessen/", "tokens": ["Die", "L\u00fc\u00b7ne\u00b7bur\u00b7gi\u00b7sche", "nicht", "g\u00e4ntz\u00b7lich", "zu", "ver\u00b7ges\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKNEG", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.407": {"text": "Die braucheten jhr Land zu jhrem Aufenthalt/", "tokens": ["Die", "brau\u00b7che\u00b7ten", "jhr", "Land", "zu", "jhrem", "Auf\u00b7ent\u00b7halt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+---+--+-+", "measure": "iambic.tetra.relaxed"}, "line.408": {"text": "Und Wolffenb\u00fcttel kam in seines Herrn Gewalt/", "tokens": ["Und", "Wolf\u00b7fen\u00b7b\u00fct\u00b7tel", "kam", "in", "sei\u00b7nes", "Herrn", "Ge\u00b7walt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.409": {"text": "Das durch Vertrag geschah. Einbeck gieng an den K\u00e4yser/", "tokens": ["Das", "durch", "Ver\u00b7trag", "ge\u00b7schah", ".", "Ein\u00b7beck", "gieng", "an", "den", "K\u00e4y\u00b7ser", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "VVFIN", "$.", "NN", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.410": {"text": "und G\u00f6ttingen bekam dergleichen Wegeweiser/", "tokens": ["und", "G\u00f6t\u00b7tin\u00b7gen", "be\u00b7kam", "derg\u00b7lei\u00b7chen", "We\u00b7ge\u00b7wei\u00b7ser", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PIS", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.411": {"text": "Doch es blieb unverwandt/ gleich wie auch Hohentwil/", "tokens": ["Doch", "es", "blieb", "un\u00b7ver\u00b7wandt", "/", "gleich", "wie", "auch", "Ho\u00b7hent\u00b7wil", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "$(", "ADV", "KOKOM", "ADV", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.412": {"text": "Das Sparr so fest beschlo\u00df und durch der St\u00fccke Spiel", "tokens": ["Das", "Sparr", "so", "fest", "be\u00b7schlo\u00df", "und", "durch", "der", "St\u00fc\u00b7cke", "Spiel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "ADJD", "VVFIN", "KON", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.413": {"text": "Zur Demuht zwingen wolt\u2019. Hergegen aber kriegte", "tokens": ["Zur", "De\u00b7muht", "zwin\u00b7gen", "wolt'", ".", "Her\u00b7ge\u00b7gen", "a\u00b7ber", "krieg\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "VVINF", "VMFIN", "$.", "NN", "ADV", "VVFIN"], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}, "line.414": {"text": "Cur-Sachs sein Zwickau ein/ da\u00df er mit Macht besiegte/", "tokens": ["Cur\u00b7Sachs", "sein", "Zwi\u00b7ckau", "ein", "/", "da\u00df", "er", "mit", "Macht", "be\u00b7sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "ART", "$(", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.415": {"text": "Und G\u00f6rlitz folgte nach. Auch mich nimmt etwas ein/", "tokens": ["Und", "G\u00f6r\u00b7litz", "folg\u00b7te", "nach", ".", "Auch", "mich", "nimmt", "et\u00b7was", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PTKVZ", "$.", "ADV", "PPER", "VVFIN", "ADV", "ART", "$("], "meter": "-+-+--+-++-+", "measure": "iambic.hexa.relaxed"}, "line.416": {"text": "Da\u00df ich die m\u00fcde Hand mu\u00df lassen ruhig seyn.", "tokens": ["Da\u00df", "ich", "die", "m\u00fc\u00b7de", "Hand", "mu\u00df", "las\u00b7sen", "ru\u00b7hig", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VMFIN", "VVINF", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}