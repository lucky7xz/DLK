{"textgrid.poem.41454": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Der H\u00e4nfling des Papstes Johannes des dreiundzwanzigsten", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Zwei Dinge haben sich noch nie verbinden k\u00f6nnen:", "tokens": ["Zwei", "Din\u00b7ge", "ha\u00b7ben", "sich", "noch", "nie", "ver\u00b7bin\u00b7den", "k\u00f6n\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PRF", "ADV", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein Weib und recht verschwiegen sein.", "tokens": ["Ein", "Weib", "und", "recht", "ver\u00b7schwie\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJD", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Abt Grecourt sagt's. Ich mu\u00df ihn nennen,", "tokens": ["Abt", "Gre\u00b7court", "sagt'", "s.", "Ich", "mu\u00df", "ihn", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "VVIMP", "PPER", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Um mich Unschuldigen vom Argwohn zu befrein,", "tokens": ["Um", "mich", "Un\u00b7schul\u00b7di\u00b7gen", "vom", "Arg\u00b7wohn", "zu", "be\u00b7fr\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "NN", "APPRART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-++-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.5": {"text": "Als fiele mir dergleichen ein.", "tokens": ["Als", "fie\u00b7le", "mir", "derg\u00b7lei\u00b7chen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PPER", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ihm will ich stets den Ha\u00df verschwiegner Damen g\u00f6nnen.", "tokens": ["Ihm", "will", "ich", "stets", "den", "Ha\u00df", "ver\u00b7schwieg\u00b7ner", "Da\u00b7men", "g\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "ART", "NN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Zum sp\u00f6ttischen Beweis erz\u00e4hlt er ein Gedicht.", "tokens": ["Zum", "sp\u00f6t\u00b7ti\u00b7schen", "Be\u00b7weis", "er\u00b7z\u00e4hlt", "er", "ein", "Ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ihr Sch\u00f6nen, was erz\u00e4hlt man nicht?", "tokens": ["Ihr", "Sch\u00f6\u00b7nen", ",", "was", "er\u00b7z\u00e4hlt", "man", "nicht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der f\u00fcrchterliche Papst, der durch den Blitz des Bannes", "tokens": ["Der", "f\u00fcrch\u00b7ter\u00b7li\u00b7che", "Papst", ",", "der", "durch", "den", "Blitz", "des", "Ban\u00b7nes"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dem f\u00fcnften Ludewig, dem Bayern, widerstand,", "tokens": ["Dem", "f\u00fcnf\u00b7ten", "Lu\u00b7de\u00b7wig", ",", "dem", "Bay\u00b7ern", ",", "wi\u00b7der\u00b7stand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NE", "$,", "ART", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der dreiundzwanzigste Johannes", "tokens": ["Der", "drei\u00b7und\u00b7zwan\u00b7zigs\u00b7te", "Jo\u00b7han\u00b7nes"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "War, wie Franzosen sind, bei Nonnen recht galant:", "tokens": ["War", ",", "wie", "Fran\u00b7zo\u00b7sen", "sind", ",", "bei", "Non\u00b7nen", "recht", "ga\u00b7lant", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PWAV", "NN", "VAFIN", "$,", "APPR", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Galant; doch wie ein Papst, ohn' Abgang seiner W\u00fcrde.", "tokens": ["Ga\u00b7lant", ";", "doch", "wie", "ein", "Papst", ",", "ohn'", "Ab\u00b7gang", "sei\u00b7ner", "W\u00fcr\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ADV", "KOKOM", "ART", "NN", "$,", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Er sprach zu Frontevaux sehr oft den Schwestern zu,", "tokens": ["Er", "sprach", "zu", "Fron\u00b7te\u00b7vaux", "sehr", "oft", "den", "Schwes\u00b7tern", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "ADV", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Theils zur Erleichtrung seiner B\u00fcrde,", "tokens": ["Theils", "zur", "Er\u00b7leich\u00b7trung", "sei\u00b7ner", "B\u00fcr\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Theils zur Bef\u00f6rdrung ihrer Ruh'.", "tokens": ["Theils", "zur", "Be\u00b7f\u00f6r\u00b7drung", "ih\u00b7rer", "Ruh'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.9": {"text": "Dies Kloster war der Sitz geweihter Schw\u00e4tzerinnen.", "tokens": ["Dies", "Klos\u00b7ter", "war", "der", "Sitz", "ge\u00b7weih\u00b7ter", "Schw\u00e4t\u00b7ze\u00b7rin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Die suchten alles auszusinnen,", "tokens": ["Die", "such\u00b7ten", "al\u00b7les", "aus\u00b7zu\u00b7sin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Durch ihrer Zungen Fertigkeit", "tokens": ["Durch", "ih\u00b7rer", "Zun\u00b7gen", "Fer\u00b7tig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Den Schutz und die Gewogenheit", "tokens": ["Den", "Schutz", "und", "die", "Ge\u00b7wo\u00b7gen\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Des Oberhirten zu gewinnen;", "tokens": ["Des", "O\u00b7berh\u00b7ir\u00b7ten", "zu", "ge\u00b7win\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Und die Hochw\u00fcrdigen gewannen seine Huld.", "tokens": ["Und", "die", "Hoch\u00b7w\u00fcr\u00b7di\u00b7gen", "ge\u00b7wan\u00b7nen", "sei\u00b7ne", "Huld", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.15": {"text": "Sie war kaum reichlicher, noch sch\u00f6ner anzulegen.", "tokens": ["Sie", "war", "kaum", "reich\u00b7li\u00b7cher", ",", "noch", "sch\u00f6\u00b7ner", "an\u00b7zu\u00b7le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Was gab er ihnen nicht! Bald Abla\u00df, bald Indult,", "tokens": ["Was", "gab", "er", "ih\u00b7nen", "nicht", "!", "Bald", "Ab\u00b7la\u00df", ",", "bald", "In\u00b7dult", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PPER", "PTKNEG", "$.", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und bald, verschwendrisch, seinen Segen.", "tokens": ["Und", "bald", ",", "ver\u00b7schwend\u00b7risch", ",", "sei\u00b7nen", "Se\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADJD", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "War ihnen das genug? O nein.", "tokens": ["War", "ih\u00b7nen", "das", "ge\u00b7nug", "?", "O", "nein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PDS", "ADV", "$.", "NE", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Wann wei\u00df der Mensch vergn\u00fcgt zu sein?", "tokens": ["Wann", "wei\u00df", "der", "Mensch", "ver\u00b7gn\u00fcgt", "zu", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "VVPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Sie lie\u00dfen sich gar von dem Wahn beth\u00f6ren,", "tokens": ["Sie", "lie\u00b7\u00dfen", "sich", "gar", "von", "dem", "Wahn", "be\u00b7th\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Den M\u00e4nnern beichten, sei nicht recht,", "tokens": ["Den", "M\u00e4n\u00b7nern", "beich\u00b7ten", ",", "sei", "nicht", "recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und von dem weiblichen Geschlecht", "tokens": ["Und", "von", "dem", "weib\u00b7li\u00b7chen", "Ge\u00b7schlecht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sollt' eine stets der andern Beichte h\u00f6ren:", "tokens": ["Sollt'", "ei\u00b7ne", "stets", "der", "an\u00b7dern", "Beich\u00b7te", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADV", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Und dieses einzusehn, sei auch der P\u00e4pste Pflicht.", "tokens": ["Und", "die\u00b7ses", "ein\u00b7zu\u00b7sehn", ",", "sei", "auch", "der", "P\u00e4ps\u00b7te", "Pflicht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVINF", "$,", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Er k\u00f6mmt auch kaum ins Kloster wieder,", "tokens": ["Er", "k\u00f6mmt", "auch", "kaum", "ins", "Klos\u00b7ter", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So wirft vor ihm sich die Aebtissin nieder,", "tokens": ["So", "wirft", "vor", "ihm", "sich", "die", "A\u00b7eb\u00b7tis\u00b7sin", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "PRF", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "K\u00fc\u00dft z\u00e4rtlich seinen Fu\u00df, und spricht:", "tokens": ["K\u00fc\u00dft", "z\u00e4rt\u00b7lich", "sei\u00b7nen", "Fu\u00df", ",", "und", "spricht", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PPOSAT", "NN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "O heil'ger Vater, h\u00f6r' ein Flehen;", "tokens": ["O", "heil'\u00b7ger", "Va\u00b7ter", ",", "h\u00f6r'", "ein", "Fle\u00b7hen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "La\u00df bei dem Priester uns nicht mehr zur Beichte gehen!", "tokens": ["La\u00df", "bei", "dem", "Pries\u00b7ter", "uns", "nicht", "mehr", "zur", "Beich\u00b7te", "ge\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "APPR", "ART", "NN", "PPER", "PTKNEG", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wir alle sch\u00e4men uns, ihm alles zu gestehen.", "tokens": ["Wir", "al\u00b7le", "sch\u00e4\u00b7men", "uns", ",", "ihm", "al\u00b7les", "zu", "ge\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "VVFIN", "PPER", "$,", "PPER", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Im Wachen und im Schlaf gibt's manche Kleinigkeit,", "tokens": ["Im", "Wa\u00b7chen", "und", "im", "Schlaf", "gibt's", "man\u00b7che", "Klei\u00b7nig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "APPRART", "NN", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die, M\u00e4nnern zu vertraun, sich jede Nonne scheut.", "tokens": ["Die", ",", "M\u00e4n\u00b7nern", "zu", "ver\u00b7traun", ",", "sich", "je\u00b7de", "Non\u00b7ne", "scheut", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "NN", "PTKZU", "VVINF", "$,", "PRF", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "La\u00df k\u00fcnftig uns einander beichten.", "tokens": ["La\u00df", "k\u00fcnf\u00b7tig", "uns", "ein\u00b7an\u00b7der", "beich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJD", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Wir sind weit f\u00e4higer, die S\u00fcnden zu beleuchten.", "tokens": ["Wir", "sind", "weit", "f\u00e4\u00b7hi\u00b7ger", ",", "die", "S\u00fcn\u00b7den", "zu", "be\u00b7leuch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "ADJD", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Den Papst befremdet sehr der Bitte Dreistigkeit.", "tokens": ["Den", "Papst", "be\u00b7frem\u00b7det", "sehr", "der", "Bit\u00b7te", "Dreis\u00b7tig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie? sagt er: ihr wollt Beichte sitzen?", "tokens": ["Wie", "?", "sagt", "er", ":", "ihr", "wollt", "Beich\u00b7te", "sit\u00b7zen", "?"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "VVFIN", "PPER", "$.", "PPER", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr guten Kinderchen k\u00f6nnt sonst der Kirche n\u00fctzen.", "tokens": ["Ihr", "gu\u00b7ten", "Kin\u00b7der\u00b7chen", "k\u00f6nnt", "sonst", "der", "Kir\u00b7che", "n\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wi\u00dft: Dieses Sacrament erheischt Verschwiegenheit.", "tokens": ["Wi\u00dft", ":", "Die\u00b7ses", "Sa\u00b7cra\u00b7ment", "er\u00b7heischt", "Ver\u00b7schwie\u00b7gen\u00b7heit", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PDAT", "NN", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die ward euch nicht zu Theil. Ihr denkt sch\u00f6n und erhaben,", "tokens": ["Die", "ward", "euch", "nicht", "zu", "Theil", ".", "Ihr", "denkt", "sch\u00f6n", "und", "er\u00b7ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "PTKNEG", "APPR", "NN", "$.", "PPER", "VVFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und ihr, Geliebteste, besitzet viele Gaben:", "tokens": ["Und", "ihr", ",", "Ge\u00b7lieb\u00b7tes\u00b7te", ",", "be\u00b7sit\u00b7zet", "vie\u00b7le", "Ga\u00b7ben", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "NN", "$,", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch eine nicht, die Zuverl\u00e4ssigkeit.", "tokens": ["Doch", "ei\u00b7ne", "nicht", ",", "die", "Zu\u00b7ver\u00b7l\u00e4s\u00b7sig\u00b7keit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "PTKNEG", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Allein, ich nehm' es in Bedenken.", "tokens": ["Al\u00b7lein", ",", "ich", "nehm'", "es", "in", "Be\u00b7den\u00b7ken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Vielleicht wei\u00df Frontevaux sich kl\u00fcglich einzuschr\u00e4nken.", "tokens": ["Viel\u00b7leicht", "wei\u00df", "Fron\u00b7te\u00b7vaux", "sich", "kl\u00fcg\u00b7lich", "ein\u00b7zu\u00b7schr\u00e4n\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "PRF", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Ist die Aebtissin nicht verst\u00e4ndig wie ein Mann?", "tokens": ["Ist", "die", "A\u00b7eb\u00b7tis\u00b7sin", "nicht", "ver\u00b7st\u00e4n\u00b7dig", "wie", "ein", "Mann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "+-+--+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.11": {"text": "Zur Pr\u00fcfung will ich hier noch heut' ein K\u00e4stchen senden.", "tokens": ["Zur", "Pr\u00fc\u00b7fung", "will", "ich", "hier", "noch", "heut'", "ein", "K\u00e4st\u00b7chen", "sen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VMFIN", "PPER", "ADV", "ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Das \u00fcberliefre sich nur ihren keuschen H\u00e4nden!", "tokens": ["Das", "\u00fc\u00b7berl\u00b7ief\u00b7re", "sich", "nur", "ih\u00b7ren", "keu\u00b7schen", "H\u00e4n\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wenn sie, nichts ist so leicht, mir's wiedergeben kann;", "tokens": ["Wenn", "sie", ",", "nichts", "ist", "so", "leicht", ",", "mir's", "wie\u00b7der\u00b7ge\u00b7ben", "kann", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PIS", "VAFIN", "ADV", "ADJD", "$,", "NE", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Doch uner\u00f6ffnet, merkt dies an!", "tokens": ["Doch", "un\u00b7er\u00b7\u00f6ff\u00b7net", ",", "merkt", "dies", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "VVFIN", "PDS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "So bin ich ganz geneigt, euch alles zuzuwenden.", "tokens": ["So", "bin", "ich", "ganz", "ge\u00b7neigt", ",", "euch", "al\u00b7les", "zu\u00b7zu\u00b7wen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,", "PPER", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Das K\u00e4stchen k\u00f6mmt. Die Ankunft wird bekannt,", "tokens": ["Das", "K\u00e4st\u00b7chen", "k\u00f6mmt", ".", "Die", "An\u00b7kunft", "wird", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und jeder Nonne Blick und Hand", "tokens": ["Und", "je\u00b7der", "Non\u00b7ne", "Blick", "und", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Will, darf und mu\u00df es sehn, betasten und recht kennen.", "tokens": ["Will", ",", "darf", "und", "mu\u00df", "es", "sehn", ",", "be\u00b7tas\u00b7ten", "und", "recht", "ken\u00b7nen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "$,", "VMFIN", "KON", "VMFIN", "PPER", "VVINF", "$,", "VVFIN", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+++-", "measure": "unknown.measure.septa"}, "line.4": {"text": "Sie rei\u00dfen sich darum. Die Eifernden zu trennen,", "tokens": ["Sie", "rei\u00b7\u00dfen", "sich", "da\u00b7rum", ".", "Die", "Ei\u00b7fern\u00b7den", "zu", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PAV", "$.", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "K\u00f6mmt die Aebtissin, und die Nacht.", "tokens": ["K\u00f6mmt", "die", "A\u00b7eb\u00b7tis\u00b7sin", ",", "und", "die", "Nacht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "KON", "ART", "NN", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Das sch\u00f6ne K\u00e4stchen wird vorjetzt nicht aufgemacht.", "tokens": ["Das", "sch\u00f6\u00b7ne", "K\u00e4st\u00b7chen", "wird", "vor\u00b7jetzt", "nicht", "auf\u00b7ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Vorwitz qu\u00e4let oft mehr, als der Alp der Sorgen.", "tokens": ["Der", "Vor\u00b7witz", "qu\u00e4\u00b7let", "oft", "mehr", ",", "als", "der", "Alp", "der", "Sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "$,", "KOUS", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Nonnen flieht der Schlaf: auch die Aebtissin wacht,", "tokens": ["Die", "Non\u00b7nen", "flieht", "der", "Schlaf", ":", "auch", "die", "A\u00b7eb\u00b7tis\u00b7sin", "wacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$.", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--++--+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Voll reger Ungeduld, bis an den m\u00fcden Morgen.", "tokens": ["Voll", "re\u00b7ger", "Un\u00b7ge\u00b7duld", ",", "bis", "an", "den", "m\u00fc\u00b7den", "Mor\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$,", "KOUS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Die Messe geht nun an. Gebet, Gesang und Chor", "tokens": ["Die", "Mes\u00b7se", "geht", "nun", "an", ".", "Ge\u00b7bet", ",", "Ge\u00b7sang", "und", "Chor"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PTKVZ", "$.", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ger\u00e4th erb\u00e4rmlich schlecht; man zischelt sich ins Ohr,", "tokens": ["Ge\u00b7r\u00e4\u00b7th", "er\u00b7b\u00e4rm\u00b7lich", "schlecht", ";", "man", "zi\u00b7schelt", "sich", "ins", "Ohr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ADJD", "$.", "PIS", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Und singt nicht, sondern schwatzt, und fragt sich, und will wissen,", "tokens": ["Und", "singt", "nicht", ",", "son\u00b7dern", "schwatzt", ",", "und", "fragt", "sich", ",", "und", "will", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "KON", "VVFIN", "$,", "KON", "VVFIN", "PRF", "$,", "KON", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Warum sie nichts er\u00f6ffnen m\u00fcssen?", "tokens": ["Wa\u00b7rum", "sie", "nichts", "er\u00b7\u00f6ff\u00b7nen", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Die weibliche, verschleierte Clerisey", "tokens": ["Die", "weib\u00b7li\u00b7che", ",", "ver\u00b7schlei\u00b7er\u00b7te", "Cle\u00b7ri\u00b7sey"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "$,", "VVFIN", "NE"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "Versammlet sich noch vor der Mittagsstunde,", "tokens": ["Ver\u00b7samm\u00b7let", "sich", "noch", "vor", "der", "Mit\u00b7tags\u00b7stun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Und stimmet, als aus Einem Munde,", "tokens": ["Und", "stim\u00b7met", ",", "als", "aus", "Ei\u00b7nem", "Mun\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Gehorsamst der Aebtissin bei,", "tokens": ["Ge\u00b7hor\u00b7samst", "der", "A\u00b7eb\u00b7tis\u00b7sin", "bei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Da\u00df man, obgleich der Papst es nicht erlauben wolle,", "tokens": ["Da\u00df", "man", ",", "ob\u00b7gleich", "der", "Papst", "es", "nicht", "er\u00b7lau\u00b7ben", "wol\u00b7le", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "$,", "KOUS", "ART", "NN", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das K\u00e4stchen untersuchen solle.", "tokens": ["Das", "K\u00e4st\u00b7chen", "un\u00b7ter\u00b7su\u00b7chen", "sol\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Selbst unserm Arbrissel stand etwas Vorwitz frei.", "tokens": ["Selbst", "un\u00b7serm", "Ar\u00b7bris\u00b7sel", "stand", "et\u00b7was", "Vor\u00b7witz", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Es bleibt ja unter uns; wir alle k\u00f6nnen schweigen.", "tokens": ["Es", "bleibt", "ja", "un\u00b7ter", "uns", ";", "wir", "al\u00b7le", "k\u00f6n\u00b7nen", "schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPER", "$.", "PPER", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Das eben soll, uns selbst, jetzt die Er\u00f6ffnung zeigen.", "tokens": ["Das", "e\u00b7ben", "soll", ",", "uns", "selbst", ",", "jetzt", "die", "Er\u00b7\u00f6ff\u00b7nung", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VMFIN", "$,", "PPER", "ADV", "$,", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Auch kein Concilium err\u00e4th,", "tokens": ["Auch", "kein", "Con\u00b7ci\u00b7lium", "er\u00b7r\u00e4\u00b7th", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.24": {"text": "Da\u00df wir im mindsten nur am Deckelchen gedreht.", "tokens": ["Da\u00df", "wir", "im", "minds\u00b7ten", "nur", "am", "De\u00b7ckel\u00b7chen", "ge\u00b7dreht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADJA", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.25": {"text": "Doch damit lassen wir die Frau Aebtissin schalten.", "tokens": ["Doch", "da\u00b7mit", "las\u00b7sen", "wir", "die", "Frau", "A\u00b7eb\u00b7tis\u00b7sin", "schal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "Die nimmt den Deckel ab. Ein H\u00e4nfling fliegt heraus.", "tokens": ["Die", "nimmt", "den", "De\u00b7ckel", "ab", ".", "Ein", "H\u00e4nf\u00b7ling", "fliegt", "he\u00b7raus", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "PTKVZ", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Ein Wunderwerk hat ihn erhalten.", "tokens": ["Ein", "Wun\u00b7der\u00b7werk", "hat", "ihn", "er\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.28": {"text": "Er flattert, singt, entwischt, setzt sich aufs n\u00e4chste Haus.", "tokens": ["Er", "flat\u00b7tert", ",", "singt", ",", "ent\u00b7wischt", ",", "setzt", "sich", "aufs", "n\u00e4chs\u00b7te", "Haus", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "PRF", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Da mag f\u00fcr ihn der V\u00f6gel Schutzgeist walten.", "tokens": ["Da", "mag", "f\u00fcr", "ihn", "der", "V\u00f6\u00b7gel", "Schutz\u00b7geist", "wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "APPR", "PPER", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Man klopft gebietrisch an. Wer war's? ... Der Papst war da.", "tokens": ["Man", "klopft", "ge\u00b7bie\u00b7trisch", "an", ".", "Wer", "wa\u00b7r's", "?", "...", "Der", "Papst", "war", "da", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADJD", "PTKVZ", "$.", "PWS", "VAFIN", "$.", "$(", "ART", "NN", "VAFIN", "ADV", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Er kam. Sobald er nur den frommen Haufen sah,", "tokens": ["Er", "kam", ".", "So\u00b7bald", "er", "nur", "den", "from\u00b7men", "Hau\u00b7fen", "sah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wollt' er sein sch\u00f6nes K\u00e4stchen schauen;", "tokens": ["Wollt'", "er", "sein", "sch\u00f6\u00b7nes", "K\u00e4st\u00b7chen", "schau\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn, sprach er, es enth\u00e4lt, was ihr so sehr begehrt,", "tokens": ["Denn", ",", "sprach", "er", ",", "es", "ent\u00b7h\u00e4lt", ",", "was", "ihr", "so", "sehr", "be\u00b7gehrt", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "$,", "PWS", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die Bulle selbst, die euch den Beichtstuhl schon gew\u00e4hrt.", "tokens": ["Die", "Bul\u00b7le", "selbst", ",", "die", "euch", "den", "Beicht\u00b7stuhl", "schon", "ge\u00b7w\u00e4hrt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "PRELS", "PPER", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Allein! ... darf man auf Weiber bauen?", "tokens": ["Al\u00b7lein", "!", "...", "darf", "man", "auf", "Wei\u00b7ber", "bau\u00b7en", "?"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "$(", "VMFIN", "PIS", "APPR", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Ihr zaudert, wie mich d\u00e4ucht. Gebt her! ... Was seh' ich jetzt?", "tokens": ["Ihr", "zau\u00b7dert", ",", "wie", "mich", "d\u00e4ucht", ".", "Gebt", "her", "!", "...", "Was", "seh'", "ich", "jetzt", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$.", "VVIMP", "PTKVZ", "$.", "$(", "PWS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ist eure Bulle schon entflogen?", "tokens": ["Ist", "eu\u00b7re", "Bul\u00b7le", "schon", "ent\u00b7flo\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Das sch\u00f6nere Geschlecht ist sinnreich und verschmitzt,", "tokens": ["Das", "sch\u00f6\u00b7ne\u00b7re", "Ge\u00b7schlecht", "ist", "sinn\u00b7reich", "und", "ver\u00b7schmitzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Doch zum Geheimni\u00df nicht erzogen.", "tokens": ["Doch", "zum", "Ge\u00b7heim\u00b7ni\u00df", "nicht", "er\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Dem Priester nur geziemt, da\u00df er euch Beichte sitzt.", "tokens": ["Dem", "Pries\u00b7ter", "nur", "ge\u00b7ziemt", ",", "da\u00df", "er", "euch", "Beich\u00b7te", "sitzt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "$,", "KOUS", "PPER", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ein junges N\u00f6nnchen war dem alten Brauch gewogen,", "tokens": ["Ein", "jun\u00b7ges", "N\u00f6nn\u00b7chen", "war", "dem", "al\u00b7ten", "Brauch", "ge\u00b7wo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und sagt': Ich liebe nicht dergleichen Neuerung!", "tokens": ["Und", "sagt'", ":", "Ich", "lie\u00b7be", "nicht", "derg\u00b7lei\u00b7chen", "Neu\u00b7e\u00b7rung", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "PPER", "VVFIN", "PTKNEG", "PIS", "NN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.3": {"text": "Mein Beichtiger ist mir schon gut genung.", "tokens": ["Mein", "Beich\u00b7ti\u00b7ger", "ist", "mir", "schon", "gut", "ge\u00b7nung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "ADJD", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.8": {"line.1": {"text": "Zwei Dinge haben sich noch nie verbinden k\u00f6nnen:", "tokens": ["Zwei", "Din\u00b7ge", "ha\u00b7ben", "sich", "noch", "nie", "ver\u00b7bin\u00b7den", "k\u00f6n\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PRF", "ADV", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein Weib und recht verschwiegen sein.", "tokens": ["Ein", "Weib", "und", "recht", "ver\u00b7schwie\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJD", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Abt Grecourt sagt's. Ich mu\u00df ihn nennen,", "tokens": ["Abt", "Gre\u00b7court", "sagt'", "s.", "Ich", "mu\u00df", "ihn", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "VVIMP", "PPER", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Um mich Unschuldigen vom Argwohn zu befrein,", "tokens": ["Um", "mich", "Un\u00b7schul\u00b7di\u00b7gen", "vom", "Arg\u00b7wohn", "zu", "be\u00b7fr\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "NN", "APPRART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-++-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.5": {"text": "Als fiele mir dergleichen ein.", "tokens": ["Als", "fie\u00b7le", "mir", "derg\u00b7lei\u00b7chen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PPER", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ihm will ich stets den Ha\u00df verschwiegner Damen g\u00f6nnen.", "tokens": ["Ihm", "will", "ich", "stets", "den", "Ha\u00df", "ver\u00b7schwieg\u00b7ner", "Da\u00b7men", "g\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "ART", "NN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Zum sp\u00f6ttischen Beweis erz\u00e4hlt er ein Gedicht.", "tokens": ["Zum", "sp\u00f6t\u00b7ti\u00b7schen", "Be\u00b7weis", "er\u00b7z\u00e4hlt", "er", "ein", "Ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ihr Sch\u00f6nen, was erz\u00e4hlt man nicht?", "tokens": ["Ihr", "Sch\u00f6\u00b7nen", ",", "was", "er\u00b7z\u00e4hlt", "man", "nicht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Der f\u00fcrchterliche Papst, der durch den Blitz des Bannes", "tokens": ["Der", "f\u00fcrch\u00b7ter\u00b7li\u00b7che", "Papst", ",", "der", "durch", "den", "Blitz", "des", "Ban\u00b7nes"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dem f\u00fcnften Ludewig, dem Bayern, widerstand,", "tokens": ["Dem", "f\u00fcnf\u00b7ten", "Lu\u00b7de\u00b7wig", ",", "dem", "Bay\u00b7ern", ",", "wi\u00b7der\u00b7stand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NE", "$,", "ART", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der dreiundzwanzigste Johannes", "tokens": ["Der", "drei\u00b7und\u00b7zwan\u00b7zigs\u00b7te", "Jo\u00b7han\u00b7nes"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "War, wie Franzosen sind, bei Nonnen recht galant:", "tokens": ["War", ",", "wie", "Fran\u00b7zo\u00b7sen", "sind", ",", "bei", "Non\u00b7nen", "recht", "ga\u00b7lant", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PWAV", "NN", "VAFIN", "$,", "APPR", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Galant; doch wie ein Papst, ohn' Abgang seiner W\u00fcrde.", "tokens": ["Ga\u00b7lant", ";", "doch", "wie", "ein", "Papst", ",", "ohn'", "Ab\u00b7gang", "sei\u00b7ner", "W\u00fcr\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ADV", "KOKOM", "ART", "NN", "$,", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Er sprach zu Frontevaux sehr oft den Schwestern zu,", "tokens": ["Er", "sprach", "zu", "Fron\u00b7te\u00b7vaux", "sehr", "oft", "den", "Schwes\u00b7tern", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "ADV", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Theils zur Erleichtrung seiner B\u00fcrde,", "tokens": ["Theils", "zur", "Er\u00b7leich\u00b7trung", "sei\u00b7ner", "B\u00fcr\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Theils zur Bef\u00f6rdrung ihrer Ruh'.", "tokens": ["Theils", "zur", "Be\u00b7f\u00f6r\u00b7drung", "ih\u00b7rer", "Ruh'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.9": {"text": "Dies Kloster war der Sitz geweihter Schw\u00e4tzerinnen.", "tokens": ["Dies", "Klos\u00b7ter", "war", "der", "Sitz", "ge\u00b7weih\u00b7ter", "Schw\u00e4t\u00b7ze\u00b7rin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Die suchten alles auszusinnen,", "tokens": ["Die", "such\u00b7ten", "al\u00b7les", "aus\u00b7zu\u00b7sin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Durch ihrer Zungen Fertigkeit", "tokens": ["Durch", "ih\u00b7rer", "Zun\u00b7gen", "Fer\u00b7tig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Den Schutz und die Gewogenheit", "tokens": ["Den", "Schutz", "und", "die", "Ge\u00b7wo\u00b7gen\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Des Oberhirten zu gewinnen;", "tokens": ["Des", "O\u00b7berh\u00b7ir\u00b7ten", "zu", "ge\u00b7win\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Und die Hochw\u00fcrdigen gewannen seine Huld.", "tokens": ["Und", "die", "Hoch\u00b7w\u00fcr\u00b7di\u00b7gen", "ge\u00b7wan\u00b7nen", "sei\u00b7ne", "Huld", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.15": {"text": "Sie war kaum reichlicher, noch sch\u00f6ner anzulegen.", "tokens": ["Sie", "war", "kaum", "reich\u00b7li\u00b7cher", ",", "noch", "sch\u00f6\u00b7ner", "an\u00b7zu\u00b7le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Was gab er ihnen nicht! Bald Abla\u00df, bald Indult,", "tokens": ["Was", "gab", "er", "ih\u00b7nen", "nicht", "!", "Bald", "Ab\u00b7la\u00df", ",", "bald", "In\u00b7dult", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PPER", "PTKNEG", "$.", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und bald, verschwendrisch, seinen Segen.", "tokens": ["Und", "bald", ",", "ver\u00b7schwend\u00b7risch", ",", "sei\u00b7nen", "Se\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADJD", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "War ihnen das genug? O nein.", "tokens": ["War", "ih\u00b7nen", "das", "ge\u00b7nug", "?", "O", "nein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PDS", "ADV", "$.", "NE", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Wann wei\u00df der Mensch vergn\u00fcgt zu sein?", "tokens": ["Wann", "wei\u00df", "der", "Mensch", "ver\u00b7gn\u00fcgt", "zu", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "VVPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Sie lie\u00dfen sich gar von dem Wahn beth\u00f6ren,", "tokens": ["Sie", "lie\u00b7\u00dfen", "sich", "gar", "von", "dem", "Wahn", "be\u00b7th\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Den M\u00e4nnern beichten, sei nicht recht,", "tokens": ["Den", "M\u00e4n\u00b7nern", "beich\u00b7ten", ",", "sei", "nicht", "recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und von dem weiblichen Geschlecht", "tokens": ["Und", "von", "dem", "weib\u00b7li\u00b7chen", "Ge\u00b7schlecht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sollt' eine stets der andern Beichte h\u00f6ren:", "tokens": ["Sollt'", "ei\u00b7ne", "stets", "der", "an\u00b7dern", "Beich\u00b7te", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADV", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Und dieses einzusehn, sei auch der P\u00e4pste Pflicht.", "tokens": ["Und", "die\u00b7ses", "ein\u00b7zu\u00b7sehn", ",", "sei", "auch", "der", "P\u00e4ps\u00b7te", "Pflicht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVINF", "$,", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Er k\u00f6mmt auch kaum ins Kloster wieder,", "tokens": ["Er", "k\u00f6mmt", "auch", "kaum", "ins", "Klos\u00b7ter", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So wirft vor ihm sich die Aebtissin nieder,", "tokens": ["So", "wirft", "vor", "ihm", "sich", "die", "A\u00b7eb\u00b7tis\u00b7sin", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "PRF", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "K\u00fc\u00dft z\u00e4rtlich seinen Fu\u00df, und spricht:", "tokens": ["K\u00fc\u00dft", "z\u00e4rt\u00b7lich", "sei\u00b7nen", "Fu\u00df", ",", "und", "spricht", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PPOSAT", "NN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "O heil'ger Vater, h\u00f6r' ein Flehen;", "tokens": ["O", "heil'\u00b7ger", "Va\u00b7ter", ",", "h\u00f6r'", "ein", "Fle\u00b7hen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "La\u00df bei dem Priester uns nicht mehr zur Beichte gehen!", "tokens": ["La\u00df", "bei", "dem", "Pries\u00b7ter", "uns", "nicht", "mehr", "zur", "Beich\u00b7te", "ge\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "APPR", "ART", "NN", "PPER", "PTKNEG", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wir alle sch\u00e4men uns, ihm alles zu gestehen.", "tokens": ["Wir", "al\u00b7le", "sch\u00e4\u00b7men", "uns", ",", "ihm", "al\u00b7les", "zu", "ge\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "VVFIN", "PPER", "$,", "PPER", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Im Wachen und im Schlaf gibt's manche Kleinigkeit,", "tokens": ["Im", "Wa\u00b7chen", "und", "im", "Schlaf", "gibt's", "man\u00b7che", "Klei\u00b7nig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "APPRART", "NN", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die, M\u00e4nnern zu vertraun, sich jede Nonne scheut.", "tokens": ["Die", ",", "M\u00e4n\u00b7nern", "zu", "ver\u00b7traun", ",", "sich", "je\u00b7de", "Non\u00b7ne", "scheut", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "NN", "PTKZU", "VVINF", "$,", "PRF", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "La\u00df k\u00fcnftig uns einander beichten.", "tokens": ["La\u00df", "k\u00fcnf\u00b7tig", "uns", "ein\u00b7an\u00b7der", "beich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJD", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Wir sind weit f\u00e4higer, die S\u00fcnden zu beleuchten.", "tokens": ["Wir", "sind", "weit", "f\u00e4\u00b7hi\u00b7ger", ",", "die", "S\u00fcn\u00b7den", "zu", "be\u00b7leuch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "ADJD", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.11": {"line.1": {"text": "Den Papst befremdet sehr der Bitte Dreistigkeit.", "tokens": ["Den", "Papst", "be\u00b7frem\u00b7det", "sehr", "der", "Bit\u00b7te", "Dreis\u00b7tig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie? sagt er: ihr wollt Beichte sitzen?", "tokens": ["Wie", "?", "sagt", "er", ":", "ihr", "wollt", "Beich\u00b7te", "sit\u00b7zen", "?"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "VVFIN", "PPER", "$.", "PPER", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr guten Kinderchen k\u00f6nnt sonst der Kirche n\u00fctzen.", "tokens": ["Ihr", "gu\u00b7ten", "Kin\u00b7der\u00b7chen", "k\u00f6nnt", "sonst", "der", "Kir\u00b7che", "n\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wi\u00dft: Dieses Sacrament erheischt Verschwiegenheit.", "tokens": ["Wi\u00dft", ":", "Die\u00b7ses", "Sa\u00b7cra\u00b7ment", "er\u00b7heischt", "Ver\u00b7schwie\u00b7gen\u00b7heit", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PDAT", "NN", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die ward euch nicht zu Theil. Ihr denkt sch\u00f6n und erhaben,", "tokens": ["Die", "ward", "euch", "nicht", "zu", "Theil", ".", "Ihr", "denkt", "sch\u00f6n", "und", "er\u00b7ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "PTKNEG", "APPR", "NN", "$.", "PPER", "VVFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und ihr, Geliebteste, besitzet viele Gaben:", "tokens": ["Und", "ihr", ",", "Ge\u00b7lieb\u00b7tes\u00b7te", ",", "be\u00b7sit\u00b7zet", "vie\u00b7le", "Ga\u00b7ben", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "NN", "$,", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch eine nicht, die Zuverl\u00e4ssigkeit.", "tokens": ["Doch", "ei\u00b7ne", "nicht", ",", "die", "Zu\u00b7ver\u00b7l\u00e4s\u00b7sig\u00b7keit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "PTKNEG", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Allein, ich nehm' es in Bedenken.", "tokens": ["Al\u00b7lein", ",", "ich", "nehm'", "es", "in", "Be\u00b7den\u00b7ken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Vielleicht wei\u00df Frontevaux sich kl\u00fcglich einzuschr\u00e4nken.", "tokens": ["Viel\u00b7leicht", "wei\u00df", "Fron\u00b7te\u00b7vaux", "sich", "kl\u00fcg\u00b7lich", "ein\u00b7zu\u00b7schr\u00e4n\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "PRF", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Ist die Aebtissin nicht verst\u00e4ndig wie ein Mann?", "tokens": ["Ist", "die", "A\u00b7eb\u00b7tis\u00b7sin", "nicht", "ver\u00b7st\u00e4n\u00b7dig", "wie", "ein", "Mann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "+-+--+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.11": {"text": "Zur Pr\u00fcfung will ich hier noch heut' ein K\u00e4stchen senden.", "tokens": ["Zur", "Pr\u00fc\u00b7fung", "will", "ich", "hier", "noch", "heut'", "ein", "K\u00e4st\u00b7chen", "sen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VMFIN", "PPER", "ADV", "ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Das \u00fcberliefre sich nur ihren keuschen H\u00e4nden!", "tokens": ["Das", "\u00fc\u00b7berl\u00b7ief\u00b7re", "sich", "nur", "ih\u00b7ren", "keu\u00b7schen", "H\u00e4n\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wenn sie, nichts ist so leicht, mir's wiedergeben kann;", "tokens": ["Wenn", "sie", ",", "nichts", "ist", "so", "leicht", ",", "mir's", "wie\u00b7der\u00b7ge\u00b7ben", "kann", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PIS", "VAFIN", "ADV", "ADJD", "$,", "NE", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Doch uner\u00f6ffnet, merkt dies an!", "tokens": ["Doch", "un\u00b7er\u00b7\u00f6ff\u00b7net", ",", "merkt", "dies", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "VVFIN", "PDS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "So bin ich ganz geneigt, euch alles zuzuwenden.", "tokens": ["So", "bin", "ich", "ganz", "ge\u00b7neigt", ",", "euch", "al\u00b7les", "zu\u00b7zu\u00b7wen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,", "PPER", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Das K\u00e4stchen k\u00f6mmt. Die Ankunft wird bekannt,", "tokens": ["Das", "K\u00e4st\u00b7chen", "k\u00f6mmt", ".", "Die", "An\u00b7kunft", "wird", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und jeder Nonne Blick und Hand", "tokens": ["Und", "je\u00b7der", "Non\u00b7ne", "Blick", "und", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Will, darf und mu\u00df es sehn, betasten und recht kennen.", "tokens": ["Will", ",", "darf", "und", "mu\u00df", "es", "sehn", ",", "be\u00b7tas\u00b7ten", "und", "recht", "ken\u00b7nen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "$,", "VMFIN", "KON", "VMFIN", "PPER", "VVINF", "$,", "VVFIN", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+++-", "measure": "unknown.measure.septa"}, "line.4": {"text": "Sie rei\u00dfen sich darum. Die Eifernden zu trennen,", "tokens": ["Sie", "rei\u00b7\u00dfen", "sich", "da\u00b7rum", ".", "Die", "Ei\u00b7fern\u00b7den", "zu", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PAV", "$.", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "K\u00f6mmt die Aebtissin, und die Nacht.", "tokens": ["K\u00f6mmt", "die", "A\u00b7eb\u00b7tis\u00b7sin", ",", "und", "die", "Nacht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "KON", "ART", "NN", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Das sch\u00f6ne K\u00e4stchen wird vorjetzt nicht aufgemacht.", "tokens": ["Das", "sch\u00f6\u00b7ne", "K\u00e4st\u00b7chen", "wird", "vor\u00b7jetzt", "nicht", "auf\u00b7ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Vorwitz qu\u00e4let oft mehr, als der Alp der Sorgen.", "tokens": ["Der", "Vor\u00b7witz", "qu\u00e4\u00b7let", "oft", "mehr", ",", "als", "der", "Alp", "der", "Sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "$,", "KOUS", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Nonnen flieht der Schlaf: auch die Aebtissin wacht,", "tokens": ["Die", "Non\u00b7nen", "flieht", "der", "Schlaf", ":", "auch", "die", "A\u00b7eb\u00b7tis\u00b7sin", "wacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$.", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--++--+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Voll reger Ungeduld, bis an den m\u00fcden Morgen.", "tokens": ["Voll", "re\u00b7ger", "Un\u00b7ge\u00b7duld", ",", "bis", "an", "den", "m\u00fc\u00b7den", "Mor\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$,", "KOUS", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Die Messe geht nun an. Gebet, Gesang und Chor", "tokens": ["Die", "Mes\u00b7se", "geht", "nun", "an", ".", "Ge\u00b7bet", ",", "Ge\u00b7sang", "und", "Chor"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PTKVZ", "$.", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ger\u00e4th erb\u00e4rmlich schlecht; man zischelt sich ins Ohr,", "tokens": ["Ge\u00b7r\u00e4\u00b7th", "er\u00b7b\u00e4rm\u00b7lich", "schlecht", ";", "man", "zi\u00b7schelt", "sich", "ins", "Ohr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ADJD", "$.", "PIS", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Und singt nicht, sondern schwatzt, und fragt sich, und will wissen,", "tokens": ["Und", "singt", "nicht", ",", "son\u00b7dern", "schwatzt", ",", "und", "fragt", "sich", ",", "und", "will", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "KON", "VVFIN", "$,", "KON", "VVFIN", "PRF", "$,", "KON", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Warum sie nichts er\u00f6ffnen m\u00fcssen?", "tokens": ["Wa\u00b7rum", "sie", "nichts", "er\u00b7\u00f6ff\u00b7nen", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Die weibliche, verschleierte Clerisey", "tokens": ["Die", "weib\u00b7li\u00b7che", ",", "ver\u00b7schlei\u00b7er\u00b7te", "Cle\u00b7ri\u00b7sey"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "$,", "VVFIN", "NE"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "Versammlet sich noch vor der Mittagsstunde,", "tokens": ["Ver\u00b7samm\u00b7let", "sich", "noch", "vor", "der", "Mit\u00b7tags\u00b7stun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Und stimmet, als aus Einem Munde,", "tokens": ["Und", "stim\u00b7met", ",", "als", "aus", "Ei\u00b7nem", "Mun\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Gehorsamst der Aebtissin bei,", "tokens": ["Ge\u00b7hor\u00b7samst", "der", "A\u00b7eb\u00b7tis\u00b7sin", "bei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Da\u00df man, obgleich der Papst es nicht erlauben wolle,", "tokens": ["Da\u00df", "man", ",", "ob\u00b7gleich", "der", "Papst", "es", "nicht", "er\u00b7lau\u00b7ben", "wol\u00b7le", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "$,", "KOUS", "ART", "NN", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das K\u00e4stchen untersuchen solle.", "tokens": ["Das", "K\u00e4st\u00b7chen", "un\u00b7ter\u00b7su\u00b7chen", "sol\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Selbst unserm Arbrissel stand etwas Vorwitz frei.", "tokens": ["Selbst", "un\u00b7serm", "Ar\u00b7bris\u00b7sel", "stand", "et\u00b7was", "Vor\u00b7witz", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Es bleibt ja unter uns; wir alle k\u00f6nnen schweigen.", "tokens": ["Es", "bleibt", "ja", "un\u00b7ter", "uns", ";", "wir", "al\u00b7le", "k\u00f6n\u00b7nen", "schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPER", "$.", "PPER", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Das eben soll, uns selbst, jetzt die Er\u00f6ffnung zeigen.", "tokens": ["Das", "e\u00b7ben", "soll", ",", "uns", "selbst", ",", "jetzt", "die", "Er\u00b7\u00f6ff\u00b7nung", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VMFIN", "$,", "PPER", "ADV", "$,", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Auch kein Concilium err\u00e4th,", "tokens": ["Auch", "kein", "Con\u00b7ci\u00b7lium", "er\u00b7r\u00e4\u00b7th", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.24": {"text": "Da\u00df wir im mindsten nur am Deckelchen gedreht.", "tokens": ["Da\u00df", "wir", "im", "minds\u00b7ten", "nur", "am", "De\u00b7ckel\u00b7chen", "ge\u00b7dreht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADJA", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.25": {"text": "Doch damit lassen wir die Frau Aebtissin schalten.", "tokens": ["Doch", "da\u00b7mit", "las\u00b7sen", "wir", "die", "Frau", "A\u00b7eb\u00b7tis\u00b7sin", "schal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "Die nimmt den Deckel ab. Ein H\u00e4nfling fliegt heraus.", "tokens": ["Die", "nimmt", "den", "De\u00b7ckel", "ab", ".", "Ein", "H\u00e4nf\u00b7ling", "fliegt", "he\u00b7raus", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "PTKVZ", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Ein Wunderwerk hat ihn erhalten.", "tokens": ["Ein", "Wun\u00b7der\u00b7werk", "hat", "ihn", "er\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.28": {"text": "Er flattert, singt, entwischt, setzt sich aufs n\u00e4chste Haus.", "tokens": ["Er", "flat\u00b7tert", ",", "singt", ",", "ent\u00b7wischt", ",", "setzt", "sich", "aufs", "n\u00e4chs\u00b7te", "Haus", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "PRF", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Da mag f\u00fcr ihn der V\u00f6gel Schutzgeist walten.", "tokens": ["Da", "mag", "f\u00fcr", "ihn", "der", "V\u00f6\u00b7gel", "Schutz\u00b7geist", "wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "APPR", "PPER", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Man klopft gebietrisch an. Wer war's? ... Der Papst war da.", "tokens": ["Man", "klopft", "ge\u00b7bie\u00b7trisch", "an", ".", "Wer", "wa\u00b7r's", "?", "...", "Der", "Papst", "war", "da", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADJD", "PTKVZ", "$.", "PWS", "VAFIN", "$.", "$(", "ART", "NN", "VAFIN", "ADV", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Er kam. Sobald er nur den frommen Haufen sah,", "tokens": ["Er", "kam", ".", "So\u00b7bald", "er", "nur", "den", "from\u00b7men", "Hau\u00b7fen", "sah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wollt' er sein sch\u00f6nes K\u00e4stchen schauen;", "tokens": ["Wollt'", "er", "sein", "sch\u00f6\u00b7nes", "K\u00e4st\u00b7chen", "schau\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Denn, sprach er, es enth\u00e4lt, was ihr so sehr begehrt,", "tokens": ["Denn", ",", "sprach", "er", ",", "es", "ent\u00b7h\u00e4lt", ",", "was", "ihr", "so", "sehr", "be\u00b7gehrt", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "$,", "PWS", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die Bulle selbst, die euch den Beichtstuhl schon gew\u00e4hrt.", "tokens": ["Die", "Bul\u00b7le", "selbst", ",", "die", "euch", "den", "Beicht\u00b7stuhl", "schon", "ge\u00b7w\u00e4hrt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "PRELS", "PPER", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Allein! ... darf man auf Weiber bauen?", "tokens": ["Al\u00b7lein", "!", "...", "darf", "man", "auf", "Wei\u00b7ber", "bau\u00b7en", "?"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "$(", "VMFIN", "PIS", "APPR", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Ihr zaudert, wie mich d\u00e4ucht. Gebt her! ... Was seh' ich jetzt?", "tokens": ["Ihr", "zau\u00b7dert", ",", "wie", "mich", "d\u00e4ucht", ".", "Gebt", "her", "!", "...", "Was", "seh'", "ich", "jetzt", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$.", "VVIMP", "PTKVZ", "$.", "$(", "PWS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ist eure Bulle schon entflogen?", "tokens": ["Ist", "eu\u00b7re", "Bul\u00b7le", "schon", "ent\u00b7flo\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Das sch\u00f6nere Geschlecht ist sinnreich und verschmitzt,", "tokens": ["Das", "sch\u00f6\u00b7ne\u00b7re", "Ge\u00b7schlecht", "ist", "sinn\u00b7reich", "und", "ver\u00b7schmitzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Doch zum Geheimni\u00df nicht erzogen.", "tokens": ["Doch", "zum", "Ge\u00b7heim\u00b7ni\u00df", "nicht", "er\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Dem Priester nur geziemt, da\u00df er euch Beichte sitzt.", "tokens": ["Dem", "Pries\u00b7ter", "nur", "ge\u00b7ziemt", ",", "da\u00df", "er", "euch", "Beich\u00b7te", "sitzt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "$,", "KOUS", "PPER", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Ein junges N\u00f6nnchen war dem alten Brauch gewogen,", "tokens": ["Ein", "jun\u00b7ges", "N\u00f6nn\u00b7chen", "war", "dem", "al\u00b7ten", "Brauch", "ge\u00b7wo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und sagt': Ich liebe nicht dergleichen Neuerung!", "tokens": ["Und", "sagt'", ":", "Ich", "lie\u00b7be", "nicht", "derg\u00b7lei\u00b7chen", "Neu\u00b7e\u00b7rung", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "PPER", "VVFIN", "PTKNEG", "PIS", "NN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.3": {"text": "Mein Beichtiger ist mir schon gut genung.", "tokens": ["Mein", "Beich\u00b7ti\u00b7ger", "ist", "mir", "schon", "gut", "ge\u00b7nung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "ADJD", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}}}}