{"textgrid.poem.46993": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Auf die Schlacht von Leipzig", "genre": "verse", "period": "N.A.", "pub_year": 1827, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Kann denn kein Lied", "tokens": ["Kann", "denn", "kein", "Lied"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "PIAT", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Krachen mit Macht,", "tokens": ["Kra\u00b7chen", "mit", "Macht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "So laut, wie die Schlacht", "tokens": ["So", "laut", ",", "wie", "die", "Schlacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "$,", "PWAV", "ART", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Hat gekracht um Leipzigs Gebiet?", "tokens": ["Hat", "ge\u00b7kracht", "um", "Leip\u00b7zigs", "Ge\u00b7biet", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "APPR", "NN", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.2": {"line.1": {"text": "Drei Tag und drei Nacht,", "tokens": ["Drei", "Tag", "und", "drei", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Ohn' Unterla\u00df,", "tokens": ["Ohn'", "Un\u00b7ter\u00b7la\u00df", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und nicht zum Spa\u00df,", "tokens": ["Und", "nicht", "zum", "Spa\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPRART", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Hat die Schlacht gekracht.", "tokens": ["Hat", "die", "Schlacht", "ge\u00b7kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Drei Tag und drei Nacht", "tokens": ["Drei", "Tag", "und", "drei", "Nacht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "KON", "CARD", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Hat man gehalten Leipziger Messen,", "tokens": ["Hat", "man", "ge\u00b7hal\u00b7ten", "Leip\u00b7zi\u00b7ger", "Mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "ADJA", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Hat euch mit eiserner Elle gemessen,", "tokens": ["Hat", "euch", "mit", "ei\u00b7ser\u00b7ner", "El\u00b7le", "ge\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Rechnung mit euch ins Gleiche gebracht.", "tokens": ["Die", "Rech\u00b7nung", "mit", "euch", "ins", "Glei\u00b7che", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Drei Nacht und drei Tag,", "tokens": ["Drei", "Nacht", "und", "drei", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "W\u00e4hrte der Leipziger Lerchenfang;", "tokens": ["W\u00e4hr\u00b7te", "der", "Leip\u00b7zi\u00b7ger", "Ler\u00b7chen\u00b7fang", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Hundert fieng man auf einen Gang,", "tokens": ["Hun\u00b7dert", "fi\u00b7eng", "man", "auf", "ei\u00b7nen", "Gang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "+-----+-+", "measure": "dactylic.init"}, "line.4": {"text": "Tausend auf einen Schlag.", "tokens": ["Tau\u00b7send", "auf", "ei\u00b7nen", "Schlag", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "APPR", "ART", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.5": {"line.1": {"text": "Ei, es ist gut,", "tokens": ["Ei", ",", "es", "ist", "gut", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Da\u00df sich nicht k\u00f6nnen die Russen br\u00fcsten,", "tokens": ["Da\u00df", "sich", "nicht", "k\u00f6n\u00b7nen", "die", "Rus\u00b7sen", "br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PTKNEG", "VMFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da\u00df allein sie ihre W\u00fcsten", "tokens": ["Da\u00df", "al\u00b7lein", "sie", "ih\u00b7re", "W\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPER", "PPOSAT", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Tr\u00e4nken k\u00f6nnen mit Feindesblut.", "tokens": ["Tr\u00e4n\u00b7ken", "k\u00f6n\u00b7nen", "mit", "Fein\u00b7des\u00b7blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.6": {"line.1": {"text": "Nicht im kalten Ru\u00dfland allein,", "tokens": ["Nicht", "im", "kal\u00b7ten", "Ru\u00df\u00b7land", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPRART", "ADJA", "NN", "ADV", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Auch in Mei\u00dfen,", "tokens": ["Auch", "in", "Mei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Auch bei Leipzig an der Plei\u00dfen,", "tokens": ["Auch", "bei", "Leip\u00b7zig", "an", "der", "Plei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Kann der Franzose geschlagen sein.", "tokens": ["Kann", "der", "Fran\u00b7zo\u00b7se", "ge\u00b7schla\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.7": {"line.1": {"text": "Die seichte Plei\u00df' ist von Blut geschwollen,", "tokens": ["Die", "seich\u00b7te", "Plei\u00df'", "ist", "von", "Blut", "ge\u00b7schwol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Ebenen haben", "tokens": ["Die", "E\u00b7be\u00b7nen", "ha\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "VAFIN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "So viel zu begraben,", "tokens": ["So", "viel", "zu", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Da\u00df sie zu Bergen uns werden sollen.", "tokens": ["Da\u00df", "sie", "zu", "Ber\u00b7gen", "uns", "wer\u00b7den", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "PPER", "VAINF", "VMFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Wenn sie uns auch zu Bergen nicht werden,", "tokens": ["Wenn", "sie", "uns", "auch", "zu", "Ber\u00b7gen", "nicht", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "APPR", "NN", "PTKNEG", "VAINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wird der Ruhm", "tokens": ["Wird", "der", "Ruhm"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Zum Eigentum", "tokens": ["Zum", "Ei\u00b7gen\u00b7tum"], "token_info": ["word", "word"], "pos": ["APPRART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Auf ewig davon uns werden auf Erden.", "tokens": ["Auf", "e\u00b7wig", "da\u00b7von", "uns", "wer\u00b7den", "auf", "Er\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "PAV", "PPER", "VAFIN", "APPR", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Kann denn kein Lied", "tokens": ["Kann", "denn", "kein", "Lied"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "PIAT", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Krachen mit Macht,", "tokens": ["Kra\u00b7chen", "mit", "Macht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "So laut, wie die Schlacht", "tokens": ["So", "laut", ",", "wie", "die", "Schlacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "$,", "PWAV", "ART", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Hat gekracht um Leipzigs Gebiet?", "tokens": ["Hat", "ge\u00b7kracht", "um", "Leip\u00b7zigs", "Ge\u00b7biet", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "APPR", "NN", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.10": {"line.1": {"text": "Drei Tag und drei Nacht,", "tokens": ["Drei", "Tag", "und", "drei", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Ohn' Unterla\u00df,", "tokens": ["Ohn'", "Un\u00b7ter\u00b7la\u00df", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und nicht zum Spa\u00df,", "tokens": ["Und", "nicht", "zum", "Spa\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPRART", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Hat die Schlacht gekracht.", "tokens": ["Hat", "die", "Schlacht", "ge\u00b7kracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.11": {"line.1": {"text": "Drei Tag und drei Nacht", "tokens": ["Drei", "Tag", "und", "drei", "Nacht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "KON", "CARD", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Hat man gehalten Leipziger Messen,", "tokens": ["Hat", "man", "ge\u00b7hal\u00b7ten", "Leip\u00b7zi\u00b7ger", "Mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "ADJA", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Hat euch mit eiserner Elle gemessen,", "tokens": ["Hat", "euch", "mit", "ei\u00b7ser\u00b7ner", "El\u00b7le", "ge\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Rechnung mit euch ins Gleiche gebracht.", "tokens": ["Die", "Rech\u00b7nung", "mit", "euch", "ins", "Glei\u00b7che", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Drei Nacht und drei Tag,", "tokens": ["Drei", "Nacht", "und", "drei", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "W\u00e4hrte der Leipziger Lerchenfang;", "tokens": ["W\u00e4hr\u00b7te", "der", "Leip\u00b7zi\u00b7ger", "Ler\u00b7chen\u00b7fang", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Hundert fieng man auf einen Gang,", "tokens": ["Hun\u00b7dert", "fi\u00b7eng", "man", "auf", "ei\u00b7nen", "Gang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "+-----+-+", "measure": "dactylic.init"}, "line.4": {"text": "Tausend auf einen Schlag.", "tokens": ["Tau\u00b7send", "auf", "ei\u00b7nen", "Schlag", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "APPR", "ART", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.13": {"line.1": {"text": "Ei, es ist gut,", "tokens": ["Ei", ",", "es", "ist", "gut", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Da\u00df sich nicht k\u00f6nnen die Russen br\u00fcsten,", "tokens": ["Da\u00df", "sich", "nicht", "k\u00f6n\u00b7nen", "die", "Rus\u00b7sen", "br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PTKNEG", "VMFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da\u00df allein sie ihre W\u00fcsten", "tokens": ["Da\u00df", "al\u00b7lein", "sie", "ih\u00b7re", "W\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPER", "PPOSAT", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Tr\u00e4nken k\u00f6nnen mit Feindesblut.", "tokens": ["Tr\u00e4n\u00b7ken", "k\u00f6n\u00b7nen", "mit", "Fein\u00b7des\u00b7blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.14": {"line.1": {"text": "Nicht im kalten Ru\u00dfland allein,", "tokens": ["Nicht", "im", "kal\u00b7ten", "Ru\u00df\u00b7land", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPRART", "ADJA", "NN", "ADV", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Auch in Mei\u00dfen,", "tokens": ["Auch", "in", "Mei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Auch bei Leipzig an der Plei\u00dfen,", "tokens": ["Auch", "bei", "Leip\u00b7zig", "an", "der", "Plei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Kann der Franzose geschlagen sein.", "tokens": ["Kann", "der", "Fran\u00b7zo\u00b7se", "ge\u00b7schla\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.15": {"line.1": {"text": "Die seichte Plei\u00df' ist von Blut geschwollen,", "tokens": ["Die", "seich\u00b7te", "Plei\u00df'", "ist", "von", "Blut", "ge\u00b7schwol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Ebenen haben", "tokens": ["Die", "E\u00b7be\u00b7nen", "ha\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "VAFIN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "So viel zu begraben,", "tokens": ["So", "viel", "zu", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Da\u00df sie zu Bergen uns werden sollen.", "tokens": ["Da\u00df", "sie", "zu", "Ber\u00b7gen", "uns", "wer\u00b7den", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "PPER", "VAINF", "VMFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "Wenn sie uns auch zu Bergen nicht werden,", "tokens": ["Wenn", "sie", "uns", "auch", "zu", "Ber\u00b7gen", "nicht", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "APPR", "NN", "PTKNEG", "VAINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wird der Ruhm", "tokens": ["Wird", "der", "Ruhm"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Zum Eigentum", "tokens": ["Zum", "Ei\u00b7gen\u00b7tum"], "token_info": ["word", "word"], "pos": ["APPRART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Auf ewig davon uns werden auf Erden.", "tokens": ["Auf", "e\u00b7wig", "da\u00b7von", "uns", "wer\u00b7den", "auf", "Er\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "PAV", "PPER", "VAFIN", "APPR", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}}}}