{"dta.poem.4362": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Bew\u00e4hrtes Mittel f\u00fcr die Augen.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Wenn wir in einer sch\u00f6nen Landschaft, mit Anmuht", "tokens": ["Wenn", "wir", "in", "ei\u00b7ner", "sch\u00f6\u00b7nen", "Land\u00b7schaft", ",", "mit", "An\u00b7muht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ART", "ADJA", "NN", "$,", "APPR", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "rings umgeben, stehn,", "tokens": ["rings", "um\u00b7ge\u00b7ben", ",", "stehn", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Und, durch die Creatur ger\u00fchret, aufmerksamer, als sonst", "tokens": ["Und", ",", "durch", "die", "Crea\u00b7tur", "ge\u00b7r\u00fch\u00b7ret", ",", "auf\u00b7merk\u00b7sa\u00b7mer", ",", "als", "sonst"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "$,", "APPR", "ART", "NN", "VVPP", "$,", "ADJD", "$,", "KOUS", "ADV"], "meter": "-+--+-+------+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "geschehn,", "tokens": ["ge\u00b7schehn", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "Den Schmuck derselben zu betrachten und eigentlicher ein-", "tokens": ["Den", "Schmuck", "der\u00b7sel\u00b7ben", "zu", "be\u00b7trach\u00b7ten", "und", "ei\u00b7gent\u00b7li\u00b7cher", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PDAT", "PTKZU", "VVINF", "KON", "ADJA", "TRUNC"], "meter": "-+-+-+-+--++--+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "zusehn,", "tokens": ["zu\u00b7sehn", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "Noch einst vern\u00fcnft\u2019ge Triebe f\u00fchlen; so finden wir, da\u00df", "tokens": ["Noch", "einst", "ver\u00b7n\u00fcnft'\u00b7ge", "Trie\u00b7be", "f\u00fch\u00b7len", ";", "so", "fin\u00b7den", "wir", ",", "da\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ADV", "ADV", "ADJA", "NN", "VVINF", "$.", "ADV", "VVFIN", "PPER", "$,", "KOUS"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "unsre Augen", "tokens": ["uns\u00b7re", "Au\u00b7gen"], "token_info": ["word", "word"], "pos": ["PPOSAT", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "(durch die Gewohnheit fast verblendet, und gleichsam", "tokens": ["(", "durch", "die", "Ge\u00b7wohn\u00b7heit", "fast", "ver\u00b7blen\u00b7det", ",", "und", "gleich\u00b7sam"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["$(", "APPR", "ART", "NN", "ADV", "VVPP", "$,", "KON", "ADJD"], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.10": {"text": "ungeschickt gemacht)", "tokens": ["un\u00b7ge\u00b7schickt", "ge\u00b7macht", ")"], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.11": {"text": "Der Vorw\u00fcrf\u2019 Anzahl, Zierlichkeit, der Farben Harmonie", "tokens": ["Der", "Vor\u00b7w\u00fcr\u00b7f'", "An\u00b7zahl", ",", "Zier\u00b7lich\u00b7keit", ",", "der", "Far\u00b7ben", "Har\u00b7mo\u00b7nie"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "NN", "$,", "NN", "$,", "ART", "NN", "NN"], "meter": "-+--+-+-+-+-+-+", "measure": "iambic.septa.relaxed"}, "line.12": {"text": "und Pracht,", "tokens": ["und", "Pracht", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Indem sie sich zu sehr vertheilen, nicht ordentlich zu sehen", "tokens": ["In\u00b7dem", "sie", "sich", "zu", "sehr", "ver\u00b7thei\u00b7len", ",", "nicht", "or\u00b7dent\u00b7lich", "zu", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "APPR", "ADV", "VVINF", "$,", "PTKNEG", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+-+--++--+-", "measure": "iambic.septa.relaxed"}, "line.14": {"text": "taugen.", "tokens": ["tau\u00b7gen", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.15": {"text": "Es scheint, als ob sich die Gedanken, so wie der Augen", "tokens": ["Es", "scheint", ",", "als", "ob", "sich", "die", "Ge\u00b7dan\u00b7ken", ",", "so", "wie", "der", "Au\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "KOUS", "PRF", "ART", "NN", "$,", "ADV", "KOKOM", "ART", "NN"], "meter": "-+--+--+--+-+-", "measure": "amphibrach.tetra.plus"}, "line.16": {"text": "Strahl, zerstreuen,", "tokens": ["Strahl", ",", "zer\u00b7streu\u00b7en", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.17": {"text": "Und da\u00df die\u00df der betr\u00fcbte Grund, wodurch wir uns der", "tokens": ["Und", "da\u00df", "die\u00df", "der", "be\u00b7tr\u00fcb\u00b7te", "Grund", ",", "wo\u00b7durch", "wir", "uns", "der"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PDS", "ART", "ADJA", "NN", "$,", "PWAV", "PPER", "PRF", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Welt nicht freuen,", "tokens": ["Welt", "nicht", "freu\u00b7en", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.19": {"text": "Noch GOtt, in Seiner Creatur, mit mehrerm Eifer, ehren", "tokens": ["Noch", "Gott", ",", "in", "Sei\u00b7ner", "Crea\u00b7tur", ",", "mit", "meh\u00b7rerm", "Ei\u00b7fer", ",", "eh\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ADV", "NN", "$,", "APPR", "PPOSAT", "NN", "$,", "APPR", "PIAT", "NN", "$,", "VVINF"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "k\u00f6nnen.", "tokens": ["k\u00f6n\u00b7nen", "."], "token_info": ["word", "punct"], "pos": ["VMFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.21": {"text": "Wir lassen, mit dem hellen Licht, in unsre sehende Kry-", "tokens": ["Wir", "las\u00b7sen", ",", "mit", "dem", "hel\u00b7len", "Licht", ",", "in", "uns\u00b7re", "se\u00b7hen\u00b7de", "Kry"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "APPR", "ART", "ADJA", "NN", "$,", "APPR", "PPOSAT", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.22": {"text": "stallen", "tokens": ["stal\u00b7len"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+-", "measure": "trochaic.single"}, "line.23": {"text": "Zu viele Vorw\u00fcrf\u2019 auf einmahl, und zwar von allen", "tokens": ["Zu", "vie\u00b7le", "Vor\u00b7w\u00fcr\u00b7f'", "auf", "ein\u00b7mahl", ",", "und", "zwar", "von", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "APPR", "ADV", "$,", "KON", "ADV", "APPR", "PIAT"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.24": {"text": "Seiten, fallen.", "tokens": ["Sei\u00b7ten", ",", "fal\u00b7len", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.25": {"text": "Anstatt da\u00df unsere Vernunft, zu einer Einheit sie zu", "tokens": ["An\u00b7statt", "da\u00df", "un\u00b7se\u00b7re", "Ver\u00b7nunft", ",", "zu", "ei\u00b7ner", "Ein\u00b7heit", "sie", "zu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPOSAT", "NN", "$,", "APPR", "ART", "NN", "PPER", "APPR"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.26": {"text": "ziehn,", "tokens": ["ziehn", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+", "measure": "single.up"}, "line.27": {"text": "Sie nach einander zu betrachten, sie zu bewundern, sich", "tokens": ["Sie", "nach", "ein\u00b7an\u00b7der", "zu", "be\u00b7trach\u00b7ten", ",", "sie", "zu", "be\u00b7wun\u00b7dern", ",", "sich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PPER", "APPR", "PRF", "PTKZU", "VVINF", "$,", "PPER", "PTKZU", "VVINF", "$,", "PRF"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.28": {"text": "bem\u00fchn,", "tokens": ["be\u00b7m\u00fchn", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+", "measure": "iambic.single"}}, "stanza.2": {"line.1": {"text": "Und sich daran vergn\u00fcgen sollte; so springet, recht wie", "tokens": ["Und", "sich", "da\u00b7ran", "ver\u00b7gn\u00fc\u00b7gen", "soll\u00b7te", ";", "so", "sprin\u00b7get", ",", "recht", "wie"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KON", "PRF", "PAV", "VVINF", "VMFIN", "$.", "ADV", "VVFIN", "$,", "ADJD", "KOKOM"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Licht und Blick", "tokens": ["Licht", "und", "Blick"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Von allen pl\u00f6tzlich r\u00fcckwerts springet, auch ebenfalls der", "tokens": ["Von", "al\u00b7len", "pl\u00f6tz\u00b7lich", "r\u00fcck\u00b7werts", "sprin\u00b7get", ",", "auch", "e\u00b7ben\u00b7falls", "der"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJD", "ADV", "VVFIN", "$,", "ADV", "ADV", "ART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Geist zur\u00fcck,", "tokens": ["Geist", "zu\u00b7r\u00fcck", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Ohn\u2019 in der C\u00f6rper Schmuck und Ordnung, wie es doch", "tokens": ["Ohn'", "in", "der", "C\u00f6r\u00b7per", "Schmuck", "und", "Ord\u00b7nung", ",", "wie", "es", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "NN", "KON", "NN", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "n\u00f6htig, einzudringen,", "tokens": ["n\u00f6h\u00b7tig", ",", "ein\u00b7zu\u00b7drin\u00b7gen", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "VVIZU", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "Ohn\u2019 in uns Lust, Erkenntlichkeit und Dank aus uns", "tokens": ["Ohn'", "in", "uns", "Lust", ",", "Er\u00b7kennt\u00b7lich\u00b7keit", "und", "Dank", "aus", "uns"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPER", "NN", "$,", "NN", "KON", "NN", "APPR", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "herauszubringen.", "tokens": ["her\u00b7aus\u00b7zu\u00b7brin\u00b7gen", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Der Unlust und des Undanks Quell\u2019, den wahren Ungl\u00fccks-", "tokens": ["Der", "Un\u00b7lust", "und", "des", "Un\u00b7danks", "Quell'", ",", "den", "wah\u00b7ren", "Un\u00b7gl\u00fccks"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "KON", "ART", "NN", "NE", "$,", "ART", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Brunnen nun", "tokens": ["Brun\u00b7nen", "nun"], "token_info": ["word", "word"], "pos": ["NN", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Zu stopfen, und, nach Menschen-Art zu sehen, etwas doch", "tokens": ["Zu", "stop\u00b7fen", ",", "und", ",", "nach", "Men\u00b7schen\u00b7Art", "zu", "se\u00b7hen", ",", "et\u00b7was", "doch"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PTKZU", "VVINF", "$,", "KON", "$,", "APPR", "NN", "PTKZU", "VVINF", "$,", "ADV", "ADV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.12": {"text": "zu thun,", "tokens": ["zu", "thun", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Und uns zum Sehn geschickt zu machen; raht ich ein Mittel", "tokens": ["Und", "uns", "zum", "Sehn", "ge\u00b7schickt", "zu", "ma\u00b7chen", ";", "raht", "ich", "ein", "Mit\u00b7tel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PPER", "APPRART", "NN", "VVPP", "PTKZU", "VVINF", "$.", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.14": {"text": "anzuwenden,", "tokens": ["an\u00b7zu\u00b7wen\u00b7den", ","], "token_info": ["word", "punct"], "pos": ["VVIZU", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "Das, wie ich neulich auf dem Felde spatzieren ging, von unge-", "tokens": ["Das", ",", "wie", "ich", "neu\u00b7lich", "auf", "dem", "Fel\u00b7de", "spat\u00b7zie\u00b7ren", "ging", ",", "von", "un\u00b7ge"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PDS", "$,", "PWAV", "PPER", "ADV", "APPR", "ART", "NN", "VVINF", "VVFIN", "$,", "APPR", "TRUNC"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.16": {"text": "fehr,", "tokens": ["fehr", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "+", "measure": "single.up"}, "line.17": {"text": "Bey den Betrachtungen, mir beyfiel, und das, zu brauchen,", "tokens": ["Bey", "den", "Be\u00b7trach\u00b7tun\u00b7gen", ",", "mir", "bey\u00b7fi\u00b7el", ",", "und", "das", ",", "zu", "brau\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PPER", "PTKVZ", "$,", "KON", "PDS", "$,", "PTKZU", "VVINF", "$,"], "meter": "+--+-+-+-+-+-+-", "measure": "iambic.septa.invert"}, "line.18": {"text": "gar nicht schwehr.", "tokens": ["gar", "nicht", "schwehr", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ADJD", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.19": {"text": "Es hat ein jeder von uns allen die\u00df Mittel selber in den", "tokens": ["Es", "hat", "ein", "je\u00b7der", "von", "uns", "al\u00b7len", "die\u00df", "Mit\u00b7tel", "sel\u00b7ber", "in", "den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "PIS", "APPR", "PPER", "PIAT", "PDS", "NN", "ADV", "APPR", "ART"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.20": {"text": "H\u00e4nden.", "tokens": ["H\u00e4n\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.21": {"text": "Man darf, wofern man es gebraucht, insk\u00fcnftige nicht", "tokens": ["Man", "darf", ",", "wo\u00b7fern", "man", "es", "ge\u00b7braucht", ",", "ins\u00b7k\u00fcnf\u00b7ti\u00b7ge", "nicht"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PIS", "VMFIN", "$,", "KOUS", "PIS", "PPER", "VVPP", "$,", "ADV", "PTKNEG"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.22": {"text": "ferner klagen:", "tokens": ["fer\u00b7ner", "kla\u00b7gen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.23": {"text": "Ich wei\u00df nicht was ich sehen soll, das Feld ist gelb, die", "tokens": ["Ich", "wei\u00df", "nicht", "was", "ich", "se\u00b7hen", "soll", ",", "das", "Feld", "ist", "gelb", ",", "die"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "PWS", "PPER", "VVINF", "VMFIN", "$,", "ART", "NN", "VAFIN", "ADJD", "$,", "PRELS"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Luft ist blau,", "tokens": ["Luft", "ist", "blau", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.25": {"text": "Der Wald ist licht- und dunkel- gr\u00fcn, und die\u00df ist alles, was", "tokens": ["Der", "Wald", "ist", "licht", "und", "dun\u00b7kel", "gr\u00fcn", ",", "und", "die\u00df", "ist", "al\u00b7les", ",", "was"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VAFIN", "TRUNC", "KON", "TRUNC", "ADJD", "$,", "KON", "PDS", "VAFIN", "PIS", "$,", "PWS"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.26": {"text": "ich schau.", "tokens": ["ich", "schau", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "PTKVZ", "$."], "meter": "-+", "measure": "iambic.single"}, "line.27": {"text": "Jhr seyd, durch meinen schlechten Vorschlag, gewi\u00df geschick-", "tokens": ["Ihr", "seyd", ",", "durch", "mei\u00b7nen", "schlech\u00b7ten", "Vor\u00b7schlag", ",", "ge\u00b7wi\u00df", "ge\u00b7schick"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "APPR", "PPOSAT", "ADJA", "NN", "$,", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.28": {"text": "ter GOtt zu preisen.", "tokens": ["ter", "Gott", "zu", "prei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.29": {"text": "In einem flachen offnen Felde, in welchem ihr spatzieren", "tokens": ["In", "ei\u00b7nem", "fla\u00b7chen", "off\u00b7nen", "Fel\u00b7de", ",", "in", "wel\u00b7chem", "ihr", "spat\u00b7zie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$,", "APPR", "PRELS", "PPER", "VVINF"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.30": {"text": "geht,", "tokens": ["geht", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-", "measure": "single.down"}, "line.31": {"text": "Und, durch der Vorw\u00fcrf\u2019 Anzahl, nichts, als etwan Feld", "tokens": ["Und", ",", "durch", "der", "Vor\u00b7w\u00fcr\u00b7f'", "An\u00b7zahl", ",", "nichts", ",", "als", "et\u00b7wan", "Feld"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "APPR", "ART", "NN", "NN", "$,", "PIS", "$,", "KOUS", "ADV", "NN"], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.32": {"text": "und Himmel, seht,", "tokens": ["und", "Him\u00b7mel", ",", "seht", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Will ich euch, in verschiedner Sch\u00f6nheit, statt einer Land-", "tokens": ["Will", "ich", "euch", ",", "in", "ver\u00b7schied\u00b7ner", "Sch\u00f6n\u00b7heit", ",", "statt", "ei\u00b7ner", "Lan\u00b7d"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "PPER", "PPER", "$,", "APPR", "ADJA", "NN", "$,", "KOUI", "ART", "TRUNC"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "schaft, tausend weisen.", "tokens": ["schaft", ",", "tau\u00b7send", "wei\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Man darf nur blo\u00df von unsern H\u00e4nden die eine Hand", "tokens": ["Man", "darf", "nur", "blo\u00df", "von", "un\u00b7sern", "H\u00e4n\u00b7den", "die", "ei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "APPR", "PPOSAT", "NN", "ART", "ART", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "zusammenfalten,", "tokens": ["zu\u00b7sam\u00b7men\u00b7fal\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Und sie vors Auge, in der Form von einem Perspective,", "tokens": ["Und", "sie", "vors", "Au\u00b7ge", ",", "in", "der", "Form", "von", "ei\u00b7nem", "Per\u00b7spec\u00b7ti\u00b7ve", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "$,", "APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "halten;", "tokens": ["hal\u00b7ten", ";"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "So wird sich, durch die kleine Oeffnung, von den dadurch", "tokens": ["So", "wird", "sich", ",", "durch", "die", "klei\u00b7ne", "Oeff\u00b7nung", ",", "von", "den", "da\u00b7durch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PRF", "$,", "APPR", "ART", "ADJA", "NN", "$,", "APPR", "ART", "PAV"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.8": {"text": "gesehnen Sachen", "tokens": ["ge\u00b7seh\u00b7nen", "Sa\u00b7chen"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Ein Theil der allgemeinen Landschaft zu einer eignen", "tokens": ["Ein", "Theil", "der", "all\u00b7ge\u00b7mei\u00b7nen", "Land\u00b7schaft", "zu", "ei\u00b7ner", "eig\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "Landschaft machen,", "tokens": ["Land\u00b7schaft", "ma\u00b7chen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Von welcher, wenn man mahlen k\u00f6nnte, ein\u2019 eigne nette", "tokens": ["Von", "wel\u00b7cher", ",", "wenn", "man", "mah\u00b7len", "k\u00f6nn\u00b7te", ",", "ein'", "eig\u00b7ne", "net\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PRELS", "$,", "KOUS", "PIS", "VVINF", "VMFIN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Schilderey", "tokens": ["Schil\u00b7de\u00b7rey"], "token_info": ["word"], "pos": ["NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.13": {"text": "Zu zeichnen und zu mahlen w\u00e4re. Man darf sie nur ein", "tokens": ["Zu", "zeich\u00b7nen", "und", "zu", "mah\u00b7len", "w\u00e4\u00b7re", ".", "Man", "darf", "sie", "nur", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "VAFIN", "$.", "PIS", "VMFIN", "PPER", "ADV", "ART"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "wenig drehen;", "tokens": ["we\u00b7nig", "dre\u00b7hen", ";"], "token_info": ["word", "word", "punct"], "pos": ["PIS", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "So wird man alsbald eine neue, von ganz verschiedner", "tokens": ["So", "wird", "man", "als\u00b7bald", "ei\u00b7ne", "neu\u00b7e", ",", "von", "ganz", "ver\u00b7schied\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIS", "ADV", "ART", "ADJA", "$,", "APPR", "ADV", "ADJA"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "Sch\u00f6nheit, sehen.", "tokens": ["Sch\u00f6n\u00b7heit", ",", "se\u00b7hen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Was nun die Ursach\u2019, da\u00df die Sch\u00f6nheit f\u00fcr uns so sehr", "tokens": ["Was", "nun", "die", "Ur\u00b7sach'", ",", "da\u00df", "die", "Sch\u00f6n\u00b7heit", "f\u00fcr", "uns", "so", "sehr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ART", "NN", "$,", "KOUS", "ART", "NN", "APPR", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "vervielfacht sey,", "tokens": ["ver\u00b7viel\u00b7facht", "sey", ","], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "L\u00e4\u00dft sich ganz eigentlich erkl\u00e4ren: Zu viele Vorw\u00fcrf\u2019 in die", "tokens": ["L\u00e4\u00dft", "sich", "ganz", "ei\u00b7gent\u00b7lich", "er\u00b7kl\u00e4\u00b7ren", ":", "Zu", "vie\u00b7le", "Vor\u00b7w\u00fcr\u00b7f'", "in", "die"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "VVINF", "$.", "APPR", "PIAT", "NN", "APPR", "ART"], "meter": "-+-+---+--+-+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Augen,", "tokens": ["Au\u00b7gen", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Die wir, durch gar zu grosse Menge, nicht recht zu unter-", "tokens": ["Die", "wir", ",", "durch", "gar", "zu", "gros\u00b7se", "Men\u00b7ge", ",", "nicht", "recht", "zu", "un\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PPER", "$,", "APPR", "ADV", "APPR", "ADJA", "NN", "$,", "PTKNEG", "ADJD", "APPR", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "scheiden taugen,", "tokens": ["schei\u00b7den", "tau\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Sind abgehalten, und die Strahlen, die in die spiegelnde", "tokens": ["Sind", "ab\u00b7ge\u00b7hal\u00b7ten", ",", "und", "die", "Strah\u00b7len", ",", "die", "in", "die", "spie\u00b7geln\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "VVPP", "$,", "KON", "ART", "NN", "$,", "PRELS", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-+--+--", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Krystallen,", "tokens": ["Krys\u00b7tal\u00b7len", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "Mit den Figuren ihrer C\u00f6rper, an des Gesichtes Nerven", "tokens": ["Mit", "den", "Fi\u00b7gu\u00b7ren", "ih\u00b7rer", "C\u00f6r\u00b7per", ",", "an", "des", "Ge\u00b7sich\u00b7tes", "Ner\u00b7ven"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "PPOSAT", "NN", "$,", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--+-", "measure": "iambic.septa.relaxed"}, "line.10": {"text": "fallen,", "tokens": ["fal\u00b7len", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.11": {"text": "Sind nicht nur dadurch deutlicher, da\u00df unser Geist sie sch\u00e4r-", "tokens": ["Sind", "nicht", "nur", "da\u00b7durch", "deut\u00b7li\u00b7cher", ",", "da\u00df", "un\u00b7ser", "Geist", "sie", "sch\u00e4r"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PTKNEG", "ADV", "PAV", "ADJD", "$,", "KOUS", "PPOSAT", "NN", "PPER", "TRUNC"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.12": {"text": "fer merkt;", "tokens": ["fer", "merkt", ";"], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Der kleine, durch die hohle Hand, formierte kleine Schatte", "tokens": ["Der", "klei\u00b7ne", ",", "durch", "die", "hoh\u00b7le", "Hand", ",", "for\u00b7mier\u00b7te", "klei\u00b7ne", "Schat\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "$,", "APPR", "ART", "ADJA", "NN", "$,", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.14": {"text": "st\u00e4rkt,", "tokens": ["st\u00e4rkt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Durch sanfte Dunkelheit, das Auge, und folglich ist der", "tokens": ["Durch", "sanf\u00b7te", "Dun\u00b7kel\u00b7heit", ",", "das", "Au\u00b7ge", ",", "und", "folg\u00b7lich", "ist", "der"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$,", "ART", "NN", "$,", "KON", "ADV", "VAFIN", "ART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "Geist geschickt,", "tokens": ["Geist", "ge\u00b7schickt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Mit gr\u00f6\u00dfrer Achtsamkeit, auf Dinge, die einzeln, sch\u00e4rfer", "tokens": ["Mit", "gr\u00f6\u00df\u00b7rer", "Acht\u00b7sam\u00b7keit", ",", "auf", "Din\u00b7ge", ",", "die", "ein\u00b7zeln", ",", "sch\u00e4r\u00b7fer"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["APPR", "ADJA", "NN", "$,", "APPR", "NN", "$,", "PRELS", "ADJD", "$,", "ADJD"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "sich zu lenken,", "tokens": ["sich", "zu", "len\u00b7ken", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "PTKZU", "VVINF", "$,"], "meter": "--+-", "measure": "anapaest.init"}, "line.3": {"text": "Und die darinn vorhandne Sch\u00f6nheit, mit mehrerm Nach-", "tokens": ["Und", "die", "da\u00b7rinn", "vor\u00b7hand\u00b7ne", "Sch\u00f6n\u00b7heit", ",", "mit", "meh\u00b7rerm", "Nach"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ART", "PAV", "ADJA", "NN", "$,", "APPR", "PIAT", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "druck, zu bedenken.", "tokens": ["druck", ",", "zu", "be\u00b7den\u00b7ken", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PAV", "$,", "PTKZU", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Zumahlen es unwidersprechlich, und eine feste Wahrheit", "tokens": ["Zu\u00b7mah\u00b7len", "es", "un\u00b7wi\u00b7der\u00b7sprech\u00b7lich", ",", "und", "ei\u00b7ne", "fes\u00b7te", "Wahr\u00b7heit"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "$,", "KON", "ART", "ADJA", "NN"], "meter": "-+--+--+--+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.6": {"text": "bleibet", "tokens": ["blei\u00b7bet"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Das, was der Britten grosse Newton uns von dem Sinn", "tokens": ["Das", ",", "was", "der", "Brit\u00b7ten", "gros\u00b7se", "New\u00b7ton", "uns", "von", "dem", "Sinn"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "$,", "PRELS", "ART", "NN", "ADJA", "NN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.8": {"text": "der Augen schreibet,", "tokens": ["der", "Au\u00b7gen", "schrei\u00b7bet", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Sey vielen Menschen noch verborgen, so wie es vormahls", "tokens": ["Sey", "vie\u00b7len", "Men\u00b7schen", "noch", "ver\u00b7bor\u00b7gen", ",", "so", "wie", "es", "vor\u00b7mahls"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "ADV", "VVPP", "$,", "ADV", "KOKOM", "PPER", "ADV"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "auch gewesen:", "tokens": ["auch", "ge\u00b7we\u00b7sen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VAPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Es sey das Sehen eine Kunst, sowohl als Schreiben,", "tokens": ["Es", "sey", "das", "Se\u00b7hen", "ei\u00b7ne", "Kunst", ",", "so\u00b7wohl", "als", "Schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ART", "NN", "$,", "KON", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "oder Lesen,", "tokens": ["o\u00b7der", "Le\u00b7sen", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.13": {"text": "Wozu wir den Verstand sowohl, als wie zu allen andern", "tokens": ["Wo\u00b7zu", "wir", "den", "Ver\u00b7stand", "so\u00b7wohl", ",", "als", "wie", "zu", "al\u00b7len", "an\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ART", "NN", "KON", "$,", "KOUS", "KOKOM", "APPR", "PIAT", "ADJA"], "meter": "--+--+-+-+-+-+-", "measure": "anapaest.di.plus"}, "line.14": {"text": "Schl\u00fcssen,", "tokens": ["Schl\u00fcs\u00b7sen", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.15": {"text": "Ja \u00f6fters andre Sinnen mehr, um recht zu sehn, gebrau-", "tokens": ["Ja", "\u00f6f\u00b7ters", "and\u00b7re", "Sin\u00b7nen", "mehr", ",", "um", "recht", "zu", "sehn", ",", "ge\u00b7brau"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKANT", "ADV", "ADJA", "NN", "ADV", "$,", "KOUI", "ADJD", "PTKZU", "VVINF", "$,", "TRUNC"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.16": {"text": "chen m\u00fcssen.", "tokens": ["chen", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.6": {"line.1": {"text": "Ach, da\u00df wir uns denn dieses Mittels, um, wie die Crea-", "tokens": ["Ach", ",", "da\u00df", "wir", "uns", "denn", "die\u00b7ses", "Mit\u00b7tels", ",", "um", ",", "wie", "die", "Crea"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "$,", "KOUS", "PPER", "PRF", "ADV", "PDAT", "NN", "$,", "KOUI", "$,", "PWAV", "ART", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "tur so sch\u00f6n,", "tokens": ["tur", "so", "sch\u00f6n", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Zu GOttes Ruhm, und unsrer Lust, mit mehr Bedacht-", "tokens": ["Zu", "Got\u00b7tes", "Ruhm", ",", "und", "uns\u00b7rer", "Lust", ",", "mit", "mehr", "Be\u00b7dacht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "NN", "$,", "KON", "PPOSAT", "NN", "$,", "APPR", "PIAT", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "samkeit zu sehn,", "tokens": ["sam\u00b7keit", "zu", "sehn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Und ihren Schmuck zu unterscheiden, zuweilen doch gebrau-", "tokens": ["Und", "ih\u00b7ren", "Schmuck", "zu", "un\u00b7ter\u00b7schei\u00b7den", ",", "zu\u00b7wei\u00b7len", "doch", "ge\u00b7brau"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "PTKZU", "VVINF", "$,", "ADV", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "chen m\u00f6gten,", "tokens": ["chen", "m\u00f6g\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Damit wir \u00f6fters, wie bisher: ", "tokens": ["Da\u00b7mit", "wir", "\u00f6f\u00b7ters", ",", "wie", "bis\u00b7her", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "PWAV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Werke! d\u00e4chten!", "tokens": ["Wer\u00b7ke", "!", "d\u00e4ch\u00b7ten", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}}}}