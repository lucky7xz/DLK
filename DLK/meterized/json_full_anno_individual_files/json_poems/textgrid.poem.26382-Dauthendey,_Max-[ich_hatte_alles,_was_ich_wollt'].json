{"textgrid.poem.26382": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[ich hatte alles, was ich wollt']", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich hatte alles, was ich wollt',", "tokens": ["Ich", "hat\u00b7te", "al\u00b7les", ",", "was", "ich", "wollt'", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Weib und einen Haufen Gold,", "tokens": ["Ein", "Weib", "und", "ei\u00b7nen", "Hau\u00b7fen", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sprach: Mit dem Weibe ganz allein,", "tokens": ["Sprach", ":", "Mit", "dem", "Wei\u00b7be", "ganz", "al\u00b7lein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kann jeder Mann zufrieden sein.", "tokens": ["Kann", "je\u00b7der", "Mann", "zu\u00b7frie\u00b7den", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIAT", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Europa, dieser alte Fetzen,", "tokens": ["Eu\u00b7ro\u00b7pa", ",", "die\u00b7ser", "al\u00b7te", "Fet\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Weib, dacht' ich, kann ihn ersetzen.", "tokens": ["Mein", "Weib", ",", "dacht'", "ich", ",", "kann", "ihn", "er\u00b7set\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "PPER", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Ich will die Heimat nicht mehr sehn", "tokens": ["Ich", "will", "die", "Hei\u00b7mat", "nicht", "mehr", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und will zu Gegenf\u00fc\u00dflern gehn.", "tokens": ["Und", "will", "zu", "Ge\u00b7gen\u00b7f\u00fc\u00df\u00b7lern", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Niemand dir dort im Wege steht,", "tokens": ["Nie\u00b7mand", "dir", "dort", "im", "We\u00b7ge", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo die Uhr einen Tag vorgeht.", "tokens": ["Wo", "die", "Uhr", "ei\u00b7nen", "Tag", "vor\u00b7geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Empf\u00e4ngst du dort dein Morgenblatt,", "tokens": ["Emp\u00b7f\u00e4ngst", "du", "dort", "dein", "Mor\u00b7gen\u00b7blatt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu Haus man Abendzeitung hat.", "tokens": ["Zu", "Haus", "man", "A\u00b7bend\u00b7zei\u00b7tung", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PIS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Kein Gedank' kann dann bei ihr sein,", "tokens": ["Kein", "Ge\u00b7dank'", "kann", "dann", "bei", "ihr", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "ADV", "APPR", "PPOSAT", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stehst du dort auf, schl\u00e4ft sie grad' ein.", "tokens": ["Stehst", "du", "dort", "auf", ",", "schl\u00e4ft", "sie", "grad'", "ein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKVZ", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.8": {"line.1": {"text": "Also ich meine Heimat floh,", "tokens": ["Al\u00b7so", "ich", "mei\u00b7ne", "Hei\u00b7mat", "floh", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Verlegte mich nach Mexiko.", "tokens": ["Ver\u00b7leg\u00b7te", "mich", "nach", "Me\u00b7xi\u00b7ko", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Man ist ein gutes St\u00fcck dann fort,", "tokens": ["Man", "ist", "ein", "gu\u00b7tes", "St\u00fcck", "dann", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "ADJA", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn spanisch klingt dort jedes Wort.", "tokens": ["Denn", "spa\u00b7nisch", "klingt", "dort", "je\u00b7des", "Wort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Und da die alten Traditionen,", "tokens": ["Und", "da", "die", "al\u00b7ten", "Tra\u00b7di\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nur schwach im fernen Westen wohnen,", "tokens": ["Nur", "schwach", "im", "fer\u00b7nen", "Wes\u00b7ten", "woh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPRART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Nahm ich aus Europa das Best',", "tokens": ["Nahm", "ich", "aus", "Eu\u00b7ro\u00b7pa", "das", "Best'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was in der Eil' sich packen l\u00e4\u00dft.", "tokens": ["Was", "in", "der", "Eil'", "sich", "pa\u00b7cken", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "PRF", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Von Milos Venus lebensgro\u00df", "tokens": ["Von", "Mi\u00b7los", "Ve\u00b7nus", "le\u00b7bens\u00b7gro\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "NE", "NE"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Man mir f\u00fcr Geld den Gipsgu\u00df go\u00df,", "tokens": ["Man", "mir", "f\u00fcr", "Geld", "den", "Gips\u00b7gu\u00df", "go\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "APPR", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Tat sie in eine Riesenkist',", "tokens": ["Tat", "sie", "in", "ei\u00b7ne", "Rie\u00b7sen\u00b7kist'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Damit sie dr\u00fcben bei uns ist.", "tokens": ["Da\u00b7mit", "sie", "dr\u00fc\u00b7ben", "bei", "uns", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "In Bronzegu\u00df den Stier Apis,", "tokens": ["In", "Bron\u00b7ze\u00b7gu\u00df", "den", "Stier", "A\u00b7pis", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Sonnengott, den Osiris,", "tokens": ["Den", "Son\u00b7nen\u00b7gott", ",", "den", "O\u00b7si\u00b7ris", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Und nahm auch mit den Gott Buddha,", "tokens": ["Und", "nahm", "auch", "mit", "den", "Gott", "Budd\u00b7ha", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der sanft auf seinen Nabel sah,", "tokens": ["Der", "sanft", "auf", "sei\u00b7nen", "Na\u00b7bel", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Denn lebt man einsam gar so fern,", "tokens": ["Denn", "lebt", "man", "ein\u00b7sam", "gar", "so", "fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADJD", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "H\u00e4lt man doch noch auf G\u00f6tter gern.", "tokens": ["H\u00e4lt", "man", "doch", "noch", "auf", "G\u00f6t\u00b7ter", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "ADV", "APPR", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "So packte ich ins Schiff sie ein,", "tokens": ["So", "pack\u00b7te", "ich", "ins", "Schiff", "sie", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als sollt's die Arche Noah sein.", "tokens": ["Als", "sollt's", "die", "Ar\u00b7che", "Noah", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "NE", "VAINF", "$."], "meter": "-+-+---", "measure": "unknown.measure.di"}}, "stanza.18": {"line.1": {"text": "Ist dann das Schiff in Mexiko,", "tokens": ["Ist", "dann", "das", "Schiff", "in", "Me\u00b7xi\u00b7ko", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dacht' ich, liebt man sich g\u00f6ttlich wo.", "tokens": ["Dacht'", "ich", ",", "liebt", "man", "sich", "g\u00f6tt\u00b7lich", "wo", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "VVFIN", "PIS", "PRF", "ADJD", "PWAV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.19": {"line.1": {"text": "Gern schwitz' ich in der Tropenwelt,", "tokens": ["Gern", "schwitz'", "ich", "in", "der", "Tro\u00b7pen\u00b7welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn nur der Kitt der Herzen h\u00e4lt.", "tokens": ["Wenn", "nur", "der", "Kitt", "der", "Her\u00b7zen", "h\u00e4lt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Doch hatte ich es ganz vergessen:", "tokens": ["Doch", "hat\u00b7te", "ich", "es", "ganz", "ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fremd sind die Tropen zugemessen.", "tokens": ["Fremd", "sind", "die", "Tro\u00b7pen", "zu\u00b7ge\u00b7mes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.21": {"line.1": {"text": "Mexiko, die Indianerstadt,", "tokens": ["Me\u00b7xi\u00b7ko", ",", "die", "In\u00b7di\u00b7a\u00b7ner\u00b7stadt", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Dreihunderttausend Roth\u00e4ut' hat,", "tokens": ["Drei\u00b7hun\u00b7dert\u00b7tau\u00b7send", "Rot\u00b7h\u00e4ut'", "hat", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Die nur in wei\u00dfen Hemden stecken,", "tokens": ["Die", "nur", "in", "wei\u00b7\u00dfen", "Hem\u00b7den", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Regnet's, tragen sie rote Decken.", "tokens": ["Reg\u00b7net's", ",", "tra\u00b7gen", "sie", "ro\u00b7te", "De\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.23": {"line.1": {"text": "Wie ich kam, war just Totenfest,", "tokens": ["Wie", "ich", "kam", ",", "war", "just", "To\u00b7ten\u00b7fest", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "VAFIN", "ADV", "NN", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo man die Toten leben l\u00e4\u00dft,", "tokens": ["Wo", "man", "die", "To\u00b7ten", "le\u00b7ben", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Man trank statt Bier Milch von Kakteen,", "tokens": ["Man", "trank", "statt", "Bier", "Milch", "von", "Kak\u00b7te\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NN", "NN", "APPR", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich fand, man l\u00e4\u00dft sie besser stehen.", "tokens": ["Ich", "fand", ",", "man", "l\u00e4\u00dft", "sie", "bes\u00b7ser", "ste\u00b7hen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PIS", "VVFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Kakteen man wie K\u00fche molk,", "tokens": ["Kak\u00b7te\u00b7en", "man", "wie", "K\u00fc\u00b7he", "molk", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "KOKOM", "NN", "NE", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Denn seinen Rausch will jedes Volk.", "tokens": ["Denn", "sei\u00b7nen", "Rausch", "will", "je\u00b7des", "Volk", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Beim Marktplatz bei der Kathedral'", "tokens": ["Beim", "Markt\u00b7platz", "bei", "der", "Ka\u00b7the\u00b7dral'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tanzte man froh zu dem Cimbal,", "tokens": ["Tanz\u00b7te", "man", "froh", "zu", "dem", "Cim\u00b7bal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADJD", "APPR", "ART", "NE", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.27": {"line.1": {"text": "Aus Marzipan und Zuckerbrot", "tokens": ["Aus", "Mar\u00b7zi\u00b7pan", "und", "Zu\u00b7cker\u00b7brot"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gab's Totenk\u00f6pf' mit Augen rot,", "tokens": ["Gab's", "To\u00b7ten\u00b7k\u00f6pf'", "mit", "Au\u00b7gen", "rot", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Grabsteine, Sarg und Leichenwagen,", "tokens": ["Grab\u00b7stei\u00b7ne", ",", "Sarg", "und", "Lei\u00b7chen\u00b7wa\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Auch Schokolad' war s\u00fc\u00df dem Magen,", "tokens": ["Auch", "Scho\u00b7ko\u00b7lad'", "war", "s\u00fc\u00df", "dem", "Ma\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VAFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Der Tod schmeckte selbst als Skelett,", "tokens": ["Der", "Tod", "schmeck\u00b7te", "selbst", "als", "Ske\u00b7lett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "KOUS", "NN", "$,"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Aus Kuchen sogar macht er fett.", "tokens": ["Aus", "Ku\u00b7chen", "so\u00b7gar", "macht", "er", "fett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Bunt sa\u00df der Tod in hundert Buden,", "tokens": ["Bunt", "sa\u00df", "der", "Tod", "in", "hun\u00b7dert", "Bu\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die lebhaft zum Einkauf einluden,", "tokens": ["Die", "leb\u00b7haft", "zum", "Ein\u00b7kauf", "ein\u00b7lu\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPRART", "NN", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.31": {"line.1": {"text": "Ich brauchte nicht den Tod zu kaufen,", "tokens": ["Ich", "brauch\u00b7te", "nicht", "den", "Tod", "zu", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Heimweh lie\u00df mich kaum noch schnaufen.", "tokens": ["Denn", "Heim\u00b7weh", "lie\u00df", "mich", "kaum", "noch", "schnau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Fremd war der Gegenf\u00fc\u00dfler Welt,", "tokens": ["Fremd", "war", "der", "Ge\u00b7gen\u00b7f\u00fc\u00df\u00b7ler", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "F\u00fchlte mich st\u00fcndlich kopfgestellt.", "tokens": ["F\u00fchl\u00b7te", "mich", "st\u00fcnd\u00b7lich", "kopf\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.33": {"line.1": {"text": "Statt Spatzen, aufgereiht in Gassen,", "tokens": ["Statt", "Spat\u00b7zen", ",", "auf\u00b7ge\u00b7reiht", "in", "Gas\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Aasgeier auf den D\u00e4chern sa\u00dfen,", "tokens": ["Aas\u00b7gei\u00b7er", "auf", "den", "D\u00e4\u00b7chern", "sa\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Ihr Aug' stierte blutgierig still,", "tokens": ["Ihr", "Aug'", "stier\u00b7te", "blut\u00b7gie\u00b7rig", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Ob man sein Herz hinwerfen will,", "tokens": ["Ob", "man", "sein", "Herz", "hin\u00b7wer\u00b7fen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Rebellisch rauscht dann ihr Gefieder,", "tokens": ["Re\u00b7bel\u00b7lisch", "rauscht", "dann", "ihr", "Ge\u00b7fie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "St\u00fcrzen sie zu den Gossen nieder,", "tokens": ["St\u00fcr\u00b7zen", "sie", "zu", "den", "Gos\u00b7sen", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.36": {"line.1": {"text": "Rei\u00dfen sich wegen eines Bissens", "tokens": ["Rei\u00b7\u00dfen", "sich", "we\u00b7gen", "ei\u00b7nes", "Bis\u00b7sens"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schwarz wie die Geier des Gewissens.", "tokens": ["Schwarz", "wie", "die", "Gei\u00b7er", "des", "Ge\u00b7wis\u00b7sens", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.37": {"line.1": {"text": "Mein Herz schien mir dazu zu gut,", "tokens": ["Mein", "Herz", "schien", "mir", "da\u00b7zu", "zu", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "PAV", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn's auch was will, was man nicht tut.", "tokens": ["Wenn's", "auch", "was", "will", ",", "was", "man", "nicht", "tut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PWS", "VMFIN", "$,", "PRELS", "PIS", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Kaum tr\u00f6stlich wirkten Mi\u00dfgeburten,", "tokens": ["Kaum", "tr\u00f6st\u00b7lich", "wirk\u00b7ten", "Mi\u00df\u00b7ge\u00b7bur\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die liebevoll gepflegt hier wurden.", "tokens": ["Die", "lie\u00b7be\u00b7voll", "ge\u00b7pflegt", "hier", "wur\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVPP", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Menschen, die von Geburt nicht locken,", "tokens": ["Men\u00b7schen", ",", "die", "von", "Ge\u00b7burt", "nicht", "lo\u00b7cken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Taten an Stra\u00dfenecken hocken.", "tokens": ["Ta\u00b7ten", "an", "Stra\u00b7\u00dfen\u00b7ec\u00b7ken", "ho\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.40": {"line.1": {"text": "Halb Kalb, halb Hund manch einer war,", "tokens": ["Halb", "Kalb", ",", "halb", "Hund", "manch", "ei\u00b7ner", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ADJD", "NN", "PIAT", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein anderer zehnarmig gar.", "tokens": ["Ein", "an\u00b7de\u00b7rer", "zehn\u00b7ar\u00b7mig", "gar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJD", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Anbettelnd dich um dein Erbarmen,", "tokens": ["An\u00b7bet\u00b7telnd", "dich", "um", "dein", "Er\u00b7bar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Winkten sie dir gleich mit zehn Armen.", "tokens": ["Wink\u00b7ten", "sie", "dir", "gleich", "mit", "zehn", "Ar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "ADV", "APPR", "CARD", "NN", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.42": {"line.1": {"text": "Ich dacht', werd' ich nochmals geboren,", "tokens": ["Ich", "dacht'", ",", "werd'", "ich", "noch\u00b7mals", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Bring' ich gleich mit die Eselsohren.", "tokens": ["Bring'", "ich", "gleich", "mit", "die", "E\u00b7sel\u00b7soh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.43": {"line.1": {"text": "Warum hab' ich die Reis' gemacht,", "tokens": ["Wa\u00b7rum", "hab'", "ich", "die", "Reis'", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hier die G\u00f6tter hergebracht?", "tokens": ["Und", "hier", "die", "G\u00f6t\u00b7ter", "her\u00b7ge\u00b7bracht", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Ich ging noch zur Arena rot,", "tokens": ["Ich", "ging", "noch", "zur", "A\u00b7re\u00b7na", "rot", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Dort stach man festlich Stiere tot.", "tokens": ["Dort", "stach", "man", "fest\u00b7lich", "Stie\u00b7re", "tot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADJD", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Den sch\u00f6nen Stier, ich kann's nicht fassen,", "tokens": ["Den", "sch\u00f6\u00b7nen", "Stier", ",", "ich", "kann's", "nicht", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sollte man wirklich leben lassen.", "tokens": ["Soll\u00b7te", "man", "wirk\u00b7lich", "le\u00b7ben", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADJD", "VVINF", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.46": {"line.1": {"text": "Stolz auf vier Beinen angebracht,", "tokens": ["Stolz", "auf", "vier", "Bei\u00b7nen", "an\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "CARD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verk\u00f6rpert er die Mannespracht.", "tokens": ["Ver\u00b7k\u00f6r\u00b7pert", "er", "die", "Man\u00b7nes\u00b7pracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Nur weil das Rot ihn irritiert,", "tokens": ["Nur", "weil", "das", "Rot", "ihn", "ir\u00b7ri\u00b7tiert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wird er mit Kunst zu Tod verf\u00fchrt.", "tokens": ["Wird", "er", "mit", "Kunst", "zu", "Tod", "ver\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Auch ich kam nur nach Mexiko,", "tokens": ["Auch", "ich", "kam", "nur", "nach", "Me\u00b7xi\u00b7ko", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ADV", "APPR", "NE", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Weil ich vor etwas Rotem floh,", "tokens": ["Weil", "ich", "vor", "et\u00b7was", "Ro\u00b7tem", "floh", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Wie 's rote Tuch vor einem Stier,", "tokens": ["Wie", "'s", "ro\u00b7te", "Tuch", "vor", "ei\u00b7nem", "Stier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Hing stets mein Herz vorm Auge mir.", "tokens": ["Hing", "stets", "mein", "Herz", "vorm", "Au\u00b7ge", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "APPRART", "NN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Vor meinem Blut wollt' ich entfliehn,", "tokens": ["Vor", "mei\u00b7nem", "Blut", "wollt'", "ich", "ent\u00b7fliehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch tat mein Blut stets mit mir ziehn.", "tokens": ["Doch", "tat", "mein", "Blut", "stets", "mit", "mir", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Nachts war gar alle Ruhe hin,", "tokens": ["Nachts", "war", "gar", "al\u00b7le", "Ru\u00b7he", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "NN", "PTKVZ", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nachtigalln wie Trompeten schrien.", "tokens": ["Nach\u00b7ti\u00b7galln", "wie", "Trom\u00b7pe\u00b7ten", "schri\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "NN", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.52": {"line.1": {"text": "Die Nacht, die s\u00fc\u00df zum Liebeswerben,", "tokens": ["Die", "Nacht", ",", "die", "s\u00fc\u00df", "zum", "Lie\u00b7bes\u00b7wer\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Taten Kleinigkeiten verderben,", "tokens": ["Ta\u00b7ten", "Klei\u00b7nig\u00b7kei\u00b7ten", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.53": {"line.1": {"text": "Ich werde niemals sie vergessen,", "tokens": ["Ich", "wer\u00b7de", "nie\u00b7mals", "sie", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fast jede hat mich aufgefressen.", "tokens": ["Fast", "je\u00b7de", "hat", "mich", "auf\u00b7ge\u00b7fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Moskitos leben klein f\u00fcr sich,", "tokens": ["Mos\u00b7ki\u00b7tos", "le\u00b7ben", "klein", "f\u00fcr", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADJD", "APPR", "PRF", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Wie N\u00e4hmaschinen Stich bei Stich,", "tokens": ["Wie", "N\u00e4h\u00b7ma\u00b7schi\u00b7nen", "Stich", "bei", "Stich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Und liegst du unter dicken Netzen,", "tokens": ["Und", "liegst", "du", "un\u00b7ter", "di\u00b7cken", "Net\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie fressen dich auch dort in Fetzen,", "tokens": ["Sie", "fres\u00b7sen", "dich", "auch", "dort", "in", "Fet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Sie lieben mehr das fremde Blut,", "tokens": ["Sie", "lie\u00b7ben", "mehr", "das", "frem\u00b7de", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dazu ist der Fremde gut.", "tokens": ["Und", "da\u00b7zu", "ist", "der", "Frem\u00b7de", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Mu\u00dft n\u00e4chtlich blutig um dich schlagen,", "tokens": ["Mu\u00dft", "n\u00e4cht\u00b7lich", "blu\u00b7tig", "um", "dich", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADJD", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kennst bald nur Schlaf vom H\u00f6rensagen.", "tokens": ["Kennst", "bald", "nur", "Schlaf", "vom", "H\u00f6\u00b7ren\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Frau K\u00f6nigin ward ganz entstellt,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "ward", "ganz", "ent\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nat\u00fcrlich, da\u00df ein Weib das qu\u00e4lt.", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", ",", "da\u00df", "ein", "Weib", "das", "qu\u00e4lt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Sie sprach: \u00bbEs dauert nicht mehr lang,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Es", "dau\u00b7ert", "nicht", "mehr", "lang", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erkennst du mich nur noch am Gang.\u00ab", "tokens": ["Er\u00b7kennst", "du", "mich", "nur", "noch", "am", "Gang", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "ADV", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Morgens am Fenster wir auch fanden,", "tokens": ["Mor\u00b7gens", "am", "Fens\u00b7ter", "wir", "auch", "fan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PPER", "ADV", "VVFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Da\u00df drau\u00dfen fremde L\u00e4nder standen.", "tokens": ["Da\u00df", "drau\u00b7\u00dfen", "frem\u00b7de", "L\u00e4n\u00b7der", "stan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Statt fr\u00fch die Milchfrau klingeln tut,", "tokens": ["Statt", "fr\u00fch", "die", "Milch\u00b7frau", "klin\u00b7geln", "tut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nahn M\u00e4dchen mit Kaffee im Blut.", "tokens": ["Nahn", "M\u00e4d\u00b7chen", "mit", "Kaf\u00b7fee", "im", "Blut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.62": {"line.1": {"text": "Sie bieten schweigsam wie die Toten,", "tokens": ["Sie", "bie\u00b7ten", "schweig\u00b7sam", "wie", "die", "To\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Paprika scharf in roten Schoten.", "tokens": ["Pa\u00b7pri\u00b7ka", "scharf", "in", "ro\u00b7ten", "Scho\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Gef\u00e4rbte Rosen sie auch gaben,", "tokens": ["Ge\u00b7f\u00e4rb\u00b7te", "Ro\u00b7sen", "sie", "auch", "ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie angestrichne Waisenknaben.", "tokens": ["Wie", "an\u00b7ge\u00b7strich\u00b7ne", "Wai\u00b7sen\u00b7kna\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Raben, bemalt wie Hottentotten,", "tokens": ["Ra\u00b7ben", ",", "be\u00b7malt", "wie", "Hot\u00b7ten\u00b7tot\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "KOKOM", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Als Paradiesv\u00f6gel sie boten.", "tokens": ["Als", "Pa\u00b7ra\u00b7dies\u00b7v\u00f6\u00b7gel", "sie", "bo\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.65": {"line.1": {"text": "Im Hintergrunde standen Krater,", "tokens": ["Im", "Hin\u00b7ter\u00b7grun\u00b7de", "stan\u00b7den", "Kra\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Katzenbuckelnd wie falsche Kater,", "tokens": ["Kat\u00b7zen\u00b7bu\u00b7ckelnd", "wie", "fal\u00b7sche", "Ka\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.66": {"line.1": {"text": "Und Erdbeben trieb sich umher,", "tokens": ["Und", "Erd\u00b7be\u00b7ben", "trieb", "sich", "um\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PRF", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Es kollerte wie Bauchredner.", "tokens": ["Es", "kol\u00b7ler\u00b7te", "wie", "Bauch\u00b7red\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "NN", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}}, "stanza.67": {"line.1": {"text": "Sah H\u00e4ngelampen pendelnd schwanken,", "tokens": ["Sah", "H\u00e4n\u00b7ge\u00b7lam\u00b7pen", "pen\u00b7delnd", "schwan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hielt mich nur aufrecht in Gedanken.", "tokens": ["Hielt", "mich", "nur", "auf\u00b7recht", "in", "Ge\u00b7dan\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.68": {"line.1": {"text": "F\u00fchlte mich in dem Wiesenrain,", "tokens": ["F\u00fchl\u00b7te", "mich", "in", "dem", "Wie\u00b7sen\u00b7rain", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mit meiner Frau als Blattlaus klein,", "tokens": ["Mit", "mei\u00b7ner", "Frau", "als", "Blatt\u00b7laus", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KOUS", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "Tat jedes Graslager vermissen,", "tokens": ["Tat", "je\u00b7des", "Gras\u00b7la\u00b7ger", "ver\u00b7mis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn ringsum tat nur Kaktus schie\u00dfen.", "tokens": ["Denn", "ring\u00b7sum", "tat", "nur", "Kak\u00b7tus", "schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.70": {"line.1": {"text": "Die Welt schien mir verkauderwelscht", "tokens": ["Die", "Welt", "schien", "mir", "ver\u00b7kau\u00b7der\u00b7welscht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nur mein Heimweh unverf\u00e4lscht.", "tokens": ["Und", "nur", "mein", "Heim\u00b7weh", "un\u00b7ver\u00b7f\u00e4lscht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.71": {"line.1": {"text": "Frau K\u00f6nigin, wie immer mild,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", ",", "wie", "im\u00b7mer", "mild", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PWAV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Blieb mir im Schmerz Madonnenbild,", "tokens": ["Blieb", "mir", "im", "Schmerz", "Ma\u00b7don\u00b7nen\u00b7bild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Wenn neue Wunder uns geschahn,", "tokens": ["Wenn", "neu\u00b7e", "Wun\u00b7der", "uns", "ge\u00b7schahn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sah K\u00f6nigin mich fragend an.", "tokens": ["Sah", "K\u00f6\u00b7ni\u00b7gin", "mich", "fra\u00b7gend", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "Dann senkte sie die Augenlider", "tokens": ["Dann", "senk\u00b7te", "sie", "die", "Au\u00b7gen\u00b7li\u00b7der"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sah still in ihr Herze nieder.", "tokens": ["Und", "sah", "still", "in", "ihr", "Her\u00b7ze", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Das war der einzig glatte Fleck,", "tokens": ["Das", "war", "der", "ein\u00b7zig", "glat\u00b7te", "Fleck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hier war noch nicht die Ruhe weg.", "tokens": ["Hier", "war", "noch", "nicht", "die", "Ru\u00b7he", "weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PTKNEG", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Die Venus blieb im Lagerhaus,", "tokens": ["Die", "Ve\u00b7nus", "blieb", "im", "La\u00b7ger\u00b7haus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir packten sie schon gar nicht aus.", "tokens": ["Wir", "pack\u00b7ten", "sie", "schon", "gar", "nicht", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.76": {"line.1": {"text": "Ich sprach: \u00bbDies ist der erste Grund:", "tokens": ["Ich", "sprach", ":", "\u00bb", "Dies", "ist", "der", "ers\u00b7te", "Grund", ":"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PDS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nie ist ruhige Liebesstund',", "tokens": ["Nie", "ist", "ru\u00b7hi\u00b7ge", "Lie\u00b7besstund'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.77": {"line.1": {"text": "Und trotz der Hitz' hat kalt man da,", "tokens": ["Und", "trotz", "der", "Hitz'", "hat", "kalt", "man", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VAFIN", "ADJD", "PIS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es zieht mich heim nach Europa.", "tokens": ["Es", "zieht", "mich", "heim", "nach", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.78": {"line.1": {"text": "Auch sieh mal diese Palmen an,", "tokens": ["Auch", "sieh", "mal", "die\u00b7se", "Pal\u00b7men", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "PDAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Palm' mich nie verstehen kann,", "tokens": ["Die", "Palm'", "mich", "nie", "ver\u00b7ste\u00b7hen", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Ich tue alle sie verfluchen,", "tokens": ["Ich", "tue", "al\u00b7le", "sie", "ver\u00b7flu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PPER", "VVINF", "$,"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Sie sind durchaus nicht wie die Buchen,", "tokens": ["Sie", "sind", "durc\u00b7haus", "nicht", "wie", "die", "Bu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Und ich will nicht mein ganzes Leben,", "tokens": ["Und", "ich", "will", "nicht", "mein", "gan\u00b7zes", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PTKNEG", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hier diesen fremden Str\u00fcnken geben.", "tokens": ["Hier", "die\u00b7sen", "frem\u00b7den", "Str\u00fcn\u00b7ken", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "Es tut zu Mi\u00dfgeburten treiben,", "tokens": ["Es", "tut", "zu", "Mi\u00df\u00b7ge\u00b7bur\u00b7ten", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich will nicht eine Nacht mehr bleiben.\u00ab", "tokens": ["Ich", "will", "nicht", "ei\u00b7ne", "Nacht", "mehr", "blei\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ART", "NN", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.82": {"line.1": {"text": "K\u00f6nigin sprach: \u00bbWas gut ich seh',", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "sprach", ":", "\u00bb", "Was", "gut", "ich", "seh'", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "PWS", "ADJD", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gut riecht's nach Zucker und Kaffee,", "tokens": ["Gut", "riecht's", "nach", "Zu\u00b7cker", "und", "Kaf\u00b7fee", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.83": {"line.1": {"text": "Wir kaufen viele Pfunde ein,", "tokens": ["Wir", "kau\u00b7fen", "vie\u00b7le", "Pfun\u00b7de", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dann soll auch die Heimreis' sein.", "tokens": ["Und", "dann", "soll", "auch", "die", "Heim\u00b7reis'", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "ADV", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Die G\u00f6tter m\u00f6gen all hier bleiben,", "tokens": ["Die", "G\u00f6t\u00b7ter", "m\u00f6\u00b7gen", "all", "hier", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIAT", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie die Moskitos austreiben.", "tokens": ["Da\u00df", "sie", "die", "Mos\u00b7ki\u00b7tos", "aus\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.85": {"line.1": {"text": "Es ist ein Ach in jedem Wind,", "tokens": ["Es", "ist", "ein", "Ach", "in", "je\u00b7dem", "Wind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch ich die Heimreis' lohnend find'.\u00ab", "tokens": ["Auch", "ich", "die", "Heim\u00b7reis'", "loh\u00b7nend", "find'", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "VVPP", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "Ich kaufte klein ein Krokodil,", "tokens": ["Ich", "kauf\u00b7te", "klein", "ein", "Kro\u00b7ko\u00b7dil", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es weinte mir der Tr\u00e4nen viel,", "tokens": ["Es", "wein\u00b7te", "mir", "der", "Tr\u00e4\u00b7nen", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.87": {"line.1": {"text": "So da\u00df ich lachend davon kam,", "tokens": ["So", "da\u00df", "ich", "la\u00b7chend", "da\u00b7von", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADJD", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als ich vom Land schnell Abschied nahm.", "tokens": ["Als", "ich", "vom", "Land", "schnell", "Ab\u00b7schied", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.88": {"line.1": {"text": "Kaum traten wir auf hohe See,", "tokens": ["Kaum", "tra\u00b7ten", "wir", "auf", "ho\u00b7he", "See", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da rief das ganze Schiff: \u00bbJuchhe!\u00ab", "tokens": ["Da", "rief", "das", "gan\u00b7ze", "Schiff", ":", "\u00bb", "Juch\u00b7he", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$.", "$(", "ITJ", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.89": {"line.1": {"text": "Die Wellen rund wie Kugeln schossen,", "tokens": ["Die", "Wel\u00b7len", "rund", "wie", "Ku\u00b7geln", "schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und senkrecht auch wie lange Hosen,", "tokens": ["Und", "sen\u00b7krecht", "auch", "wie", "lan\u00b7ge", "Ho\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KOKOM", "ADV", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}}, "stanza.90": {"line.1": {"text": "Sturm kam da jeden Nachmittag,", "tokens": ["Sturm", "kam", "da", "je\u00b7den", "Nach\u00b7mit\u00b7tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Selig ich auf dem R\u00fccken lag,", "tokens": ["Se\u00b7lig", "ich", "auf", "dem", "R\u00fc\u00b7cken", "lag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.91": {"line.1": {"text": "Der Sturm kam wie Artillerie,", "tokens": ["Der", "Sturm", "kam", "wie", "Ar\u00b7til\u00b7le\u00b7rie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch st\u00fcndlich ich laut \u00bbVivat\u00ab schrie.", "tokens": ["Doch", "st\u00fcnd\u00b7lich", "ich", "laut", "\u00bb", "Vi\u00b7vat", "\u00ab", "schrie", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "ADJD", "PPER", "ADJD", "$(", "NN", "$(", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.92": {"line.1": {"text": "Das Schiff auf Hinterf\u00fc\u00dfen stand,", "tokens": ["Das", "Schiff", "auf", "Hin\u00b7ter\u00b7f\u00fc\u00b7\u00dfen", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ging fast vor Freud' aus Rand und Band.", "tokens": ["Ging", "fast", "vor", "Freud'", "aus", "Rand", "und", "Band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.93": {"line.1": {"text": "Mit Balken lag das Meer belegt,", "tokens": ["Mit", "Bal\u00b7ken", "lag", "das", "Meer", "be\u00b7legt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wracks kamen durch den Sturm gefegt,", "tokens": ["Wracks", "ka\u00b7men", "durch", "den", "Sturm", "ge\u00b7fegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.94": {"line.1": {"text": "Ich z\u00e4hl' es zu dem Wunderbaren,", "tokens": ["Ich", "z\u00e4hl'", "es", "zu", "dem", "Wun\u00b7der\u00b7ba\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df wir nicht auf dem Kopf gefahren.", "tokens": ["Da\u00df", "wir", "nicht", "auf", "dem", "Kopf", "ge\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.95": {"line.1": {"text": "Wei\u00df war im Schiff ein Marmorsaal,", "tokens": ["Wei\u00df", "war", "im", "Schiff", "ein", "Mar\u00b7mor\u00b7saal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6nigin lag dort w\u00e4schefahl,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "lag", "dort", "w\u00e4\u00b7sche\u00b7fahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.96": {"line.1": {"text": "Ihr sch\u00f6nes Haar h\u00fcllte ein Tuch,", "tokens": ["Ihr", "sch\u00f6\u00b7nes", "Haar", "h\u00fcll\u00b7te", "ein", "Tuch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Denn t\u00f6lpelhaft kam oft Besuch.", "tokens": ["Denn", "t\u00f6l\u00b7pel\u00b7haft", "kam", "oft", "Be\u00b7such", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.97": {"line.1": {"text": "Mit schweren Schritten wie ein Gong,", "tokens": ["Mit", "schwe\u00b7ren", "Schrit\u00b7ten", "wie", "ein", "Gong", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Warf sich das Meer in den Salon,", "tokens": ["Warf", "sich", "das", "Meer", "in", "den", "Sa\u00b7lon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.98": {"line.1": {"text": "Wusch scharf mit Salz das Haar uns aus,", "tokens": ["Wusch", "scharf", "mit", "Salz", "das", "Haar", "uns", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "NN", "ART", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich rief: \u00bbAll Gold geht uns heraus.", "tokens": ["Ich", "rief", ":", "\u00bb", "All", "Gold", "geht", "uns", "he\u00b7raus", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PIAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.99": {"line.1": {"text": "Heb' ich den Suppenteller auf,", "tokens": ["Heb'", "ich", "den", "Sup\u00b7pen\u00b7tel\u00b7ler", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Flie\u00dft die Supp' senkrecht mir hinauf,", "tokens": ["Flie\u00dft", "die", "Sup\u00b7p'", "sen\u00b7krecht", "mir", "hin\u00b7auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.100": {"line.1": {"text": "Es stirbt im Hirn jeder Begriff,", "tokens": ["Es", "stirbt", "im", "Hirn", "je\u00b7der", "Be\u00b7griff", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Ich seh' die ganze Zukunft schief.\u00ab", "tokens": ["Ich", "seh'", "die", "gan\u00b7ze", "Zu\u00b7kunft", "schief", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.101": {"line.1": {"text": "Man schrie sich aus, der Sturm war laut,", "tokens": ["Man", "schrie", "sich", "aus", ",", "der", "Sturm", "war", "laut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "PTKVZ", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schiffsg\u00f6tter hab' ich uns erbaut.", "tokens": ["Schiffs\u00b7g\u00f6t\u00b7ter", "hab'", "ich", "uns", "er\u00b7baut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.102": {"line.1": {"text": "H\u00f6rt' ich die Schiffskatze miauen,", "tokens": ["H\u00f6rt'", "ich", "die", "Schiffs\u00b7kat\u00b7ze", "mi\u00b7au\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "So wuchs in mir das Gottvertrauen,", "tokens": ["So", "wuchs", "in", "mir", "das", "Gott\u00b7ver\u00b7trau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.103": {"line.1": {"text": "Und morgens, wenn der Sturm noch schwach,", "tokens": ["Und", "mor\u00b7gens", ",", "wenn", "der", "Sturm", "noch", "schwach", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kr\u00e4hte ein K\u00fcchenhahn mich wach,", "tokens": ["Kr\u00e4h\u00b7te", "ein", "K\u00fc\u00b7chen\u00b7hahn", "mich", "wach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PPER", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.104": {"line.1": {"text": "Getr\u00f6stet hat mein Herz gelacht,", "tokens": ["Ge\u00b7tr\u00f6s\u00b7tet", "hat", "mein", "Herz", "ge\u00b7lacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat Mist und K\u00fche sich erdacht.", "tokens": ["Hat", "Mist", "und", "K\u00fc\u00b7he", "sich", "er\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.105": {"line.1": {"text": "Doch abends, war der Sturm zu viel,", "tokens": ["Doch", "a\u00b7bends", ",", "war", "der", "Sturm", "zu", "viel", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "VAFIN", "ART", "NN", "APPR", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Holt' ich das kleine Krokodil.", "tokens": ["Holt'", "ich", "das", "klei\u00b7ne", "Kro\u00b7ko\u00b7dil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.106": {"line.1": {"text": "Es fra\u00df nicht und hielt Winterschlaf;", "tokens": ["Es", "fra\u00df", "nicht", "und", "hielt", "Win\u00b7ter\u00b7schlaf", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "KON", "VVFIN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Streichelnd, wenn man den Nacken traf,", "tokens": ["Strei\u00b7chelnd", ",", "wenn", "man", "den", "Na\u00b7cken", "traf", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "KOUS", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.107": {"line.1": {"text": "Sah es aus seinem Traum heraus", "tokens": ["Sah", "es", "aus", "sei\u00b7nem", "Traum", "he\u00b7raus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und weinte sich statt meiner aus.", "tokens": ["Und", "wein\u00b7te", "sich", "statt", "mei\u00b7ner", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "PPOSAT", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.108": {"line.1": {"text": "Tat man zwischen zwei Welten schweben,", "tokens": ["Tat", "man", "zwi\u00b7schen", "zwei", "Wel\u00b7ten", "schwe\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "APPR", "CARD", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Ersehnt man endlich Festlandleben.", "tokens": ["Er\u00b7sehnt", "man", "end\u00b7lich", "Fest\u00b7land\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.109": {"line.1": {"text": "Beim ersten Leuchtturm von England,", "tokens": ["Beim", "ers\u00b7ten", "Leucht\u00b7turm", "von", "En\u00b7gland", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Frau K\u00f6nigin still auferstand,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "still", "auf\u00b7er\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.110": {"line.1": {"text": "Zur Nachtstund' brannten wir Raketen,", "tokens": ["Zur", "Nacht\u00b7stund'", "brann\u00b7ten", "wir", "Ra\u00b7ke\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df Lotsen uns bemerken t\u00e4ten,", "tokens": ["Da\u00df", "Lot\u00b7sen", "uns", "be\u00b7mer\u00b7ken", "t\u00e4\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.111": {"line.1": {"text": "Am Morgen war schon H\u00e2vre da,", "tokens": ["Am", "Mor\u00b7gen", "war", "schon", "H\u00e2vre", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ADV", "NE", "ADV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und hinter ihm ganz Europa.", "tokens": ["Und", "hin\u00b7ter", "ihm", "ganz", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "ADV", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.112": {"line.1": {"text": "Wenn man solch' Luftfahrt \u00fcberstand,", "tokens": ["Wenn", "man", "solch'", "Luft\u00b7fahrt", "\u00fc\u00b7bers\u00b7tand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann k\u00fc\u00dft man gern sein Heimatland.", "tokens": ["Dann", "k\u00fc\u00dft", "man", "gern", "sein", "Hei\u00b7mat\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.113": {"line.1": {"text": "Nachts voll Confetti flog Paris,", "tokens": ["Nachts", "voll", "Con\u00b7fet\u00b7ti", "flog", "Pa\u00b7ris", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NE", "VVFIN", "NE", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wo man den Karneval einblies,", "tokens": ["Wo", "man", "den", "Kar\u00b7ne\u00b7val", "ein\u00b7blies", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.114": {"line.1": {"text": "K\u00f6nigin sprach am Opernplatz:", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "sprach", "am", "O\u00b7pern\u00b7platz", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$."], "meter": "+--+-+--", "measure": "iambic.tri.invert"}, "line.2": {"text": "\u00bbhier ist wieder Europa, Schatz.", "tokens": ["\u00bb", "hier", "ist", "wie\u00b7der", "Eu\u00b7ro\u00b7pa", ",", "Schatz", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "ADV", "NE", "$,", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.115": {"line.1": {"text": "Moskitos konnt' ich nicht forthetzen,", "tokens": ["Mos\u00b7ki\u00b7tos", "konnt'", "ich", "nicht", "for\u00b7thet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und dir Europa nicht ersetzen.\u00ab", "tokens": ["Und", "dir", "Eu\u00b7ro\u00b7pa", "nicht", "er\u00b7set\u00b7zen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "NE", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.116": {"line.1": {"text": "Ich rief: \u00bbLa\u00df jeden Weltteil leben,", "tokens": ["Ich", "rief", ":", "\u00bb", "La\u00df", "je\u00b7den", "Welt\u00b7teil", "le\u00b7ben", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VVIMP", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir wollen tanzend weiterschweben.\u00ab", "tokens": ["Wir", "wol\u00b7len", "tan\u00b7zend", "wei\u00b7ter\u00b7schwe\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.117": {"line.1": {"text": "Ich hatte alles, was ich wollt',", "tokens": ["Ich", "hat\u00b7te", "al\u00b7les", ",", "was", "ich", "wollt'", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Weib und einen Haufen Gold,", "tokens": ["Ein", "Weib", "und", "ei\u00b7nen", "Hau\u00b7fen", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.118": {"line.1": {"text": "Sprach: Mit dem Weibe ganz allein,", "tokens": ["Sprach", ":", "Mit", "dem", "Wei\u00b7be", "ganz", "al\u00b7lein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "ART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kann jeder Mann zufrieden sein.", "tokens": ["Kann", "je\u00b7der", "Mann", "zu\u00b7frie\u00b7den", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIAT", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.119": {"line.1": {"text": "Europa, dieser alte Fetzen,", "tokens": ["Eu\u00b7ro\u00b7pa", ",", "die\u00b7ser", "al\u00b7te", "Fet\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Weib, dacht' ich, kann ihn ersetzen.", "tokens": ["Mein", "Weib", ",", "dacht'", "ich", ",", "kann", "ihn", "er\u00b7set\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "PPER", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.120": {"line.1": {"text": "Ich will die Heimat nicht mehr sehn", "tokens": ["Ich", "will", "die", "Hei\u00b7mat", "nicht", "mehr", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und will zu Gegenf\u00fc\u00dflern gehn.", "tokens": ["Und", "will", "zu", "Ge\u00b7gen\u00b7f\u00fc\u00df\u00b7lern", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.121": {"line.1": {"text": "Niemand dir dort im Wege steht,", "tokens": ["Nie\u00b7mand", "dir", "dort", "im", "We\u00b7ge", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo die Uhr einen Tag vorgeht.", "tokens": ["Wo", "die", "Uhr", "ei\u00b7nen", "Tag", "vor\u00b7geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.122": {"line.1": {"text": "Empf\u00e4ngst du dort dein Morgenblatt,", "tokens": ["Emp\u00b7f\u00e4ngst", "du", "dort", "dein", "Mor\u00b7gen\u00b7blatt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu Haus man Abendzeitung hat.", "tokens": ["Zu", "Haus", "man", "A\u00b7bend\u00b7zei\u00b7tung", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PIS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.123": {"line.1": {"text": "Kein Gedank' kann dann bei ihr sein,", "tokens": ["Kein", "Ge\u00b7dank'", "kann", "dann", "bei", "ihr", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "ADV", "APPR", "PPOSAT", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stehst du dort auf, schl\u00e4ft sie grad' ein.", "tokens": ["Stehst", "du", "dort", "auf", ",", "schl\u00e4ft", "sie", "grad'", "ein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKVZ", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.124": {"line.1": {"text": "Also ich meine Heimat floh,", "tokens": ["Al\u00b7so", "ich", "mei\u00b7ne", "Hei\u00b7mat", "floh", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Verlegte mich nach Mexiko.", "tokens": ["Ver\u00b7leg\u00b7te", "mich", "nach", "Me\u00b7xi\u00b7ko", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.125": {"line.1": {"text": "Man ist ein gutes St\u00fcck dann fort,", "tokens": ["Man", "ist", "ein", "gu\u00b7tes", "St\u00fcck", "dann", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "ADJA", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn spanisch klingt dort jedes Wort.", "tokens": ["Denn", "spa\u00b7nisch", "klingt", "dort", "je\u00b7des", "Wort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.126": {"line.1": {"text": "Und da die alten Traditionen,", "tokens": ["Und", "da", "die", "al\u00b7ten", "Tra\u00b7di\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nur schwach im fernen Westen wohnen,", "tokens": ["Nur", "schwach", "im", "fer\u00b7nen", "Wes\u00b7ten", "woh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPRART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.127": {"line.1": {"text": "Nahm ich aus Europa das Best',", "tokens": ["Nahm", "ich", "aus", "Eu\u00b7ro\u00b7pa", "das", "Best'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was in der Eil' sich packen l\u00e4\u00dft.", "tokens": ["Was", "in", "der", "Eil'", "sich", "pa\u00b7cken", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "PRF", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.128": {"line.1": {"text": "Von Milos Venus lebensgro\u00df", "tokens": ["Von", "Mi\u00b7los", "Ve\u00b7nus", "le\u00b7bens\u00b7gro\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "NE", "NE"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Man mir f\u00fcr Geld den Gipsgu\u00df go\u00df,", "tokens": ["Man", "mir", "f\u00fcr", "Geld", "den", "Gips\u00b7gu\u00df", "go\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "APPR", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.129": {"line.1": {"text": "Tat sie in eine Riesenkist',", "tokens": ["Tat", "sie", "in", "ei\u00b7ne", "Rie\u00b7sen\u00b7kist'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Damit sie dr\u00fcben bei uns ist.", "tokens": ["Da\u00b7mit", "sie", "dr\u00fc\u00b7ben", "bei", "uns", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.130": {"line.1": {"text": "In Bronzegu\u00df den Stier Apis,", "tokens": ["In", "Bron\u00b7ze\u00b7gu\u00df", "den", "Stier", "A\u00b7pis", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Sonnengott, den Osiris,", "tokens": ["Den", "Son\u00b7nen\u00b7gott", ",", "den", "O\u00b7si\u00b7ris", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.131": {"line.1": {"text": "Und nahm auch mit den Gott Buddha,", "tokens": ["Und", "nahm", "auch", "mit", "den", "Gott", "Budd\u00b7ha", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der sanft auf seinen Nabel sah,", "tokens": ["Der", "sanft", "auf", "sei\u00b7nen", "Na\u00b7bel", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.132": {"line.1": {"text": "Denn lebt man einsam gar so fern,", "tokens": ["Denn", "lebt", "man", "ein\u00b7sam", "gar", "so", "fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADJD", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "H\u00e4lt man doch noch auf G\u00f6tter gern.", "tokens": ["H\u00e4lt", "man", "doch", "noch", "auf", "G\u00f6t\u00b7ter", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "ADV", "APPR", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.133": {"line.1": {"text": "So packte ich ins Schiff sie ein,", "tokens": ["So", "pack\u00b7te", "ich", "ins", "Schiff", "sie", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als sollt's die Arche Noah sein.", "tokens": ["Als", "sollt's", "die", "Ar\u00b7che", "Noah", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "NE", "VAINF", "$."], "meter": "-+-+---", "measure": "unknown.measure.di"}}, "stanza.134": {"line.1": {"text": "Ist dann das Schiff in Mexiko,", "tokens": ["Ist", "dann", "das", "Schiff", "in", "Me\u00b7xi\u00b7ko", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dacht' ich, liebt man sich g\u00f6ttlich wo.", "tokens": ["Dacht'", "ich", ",", "liebt", "man", "sich", "g\u00f6tt\u00b7lich", "wo", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "VVFIN", "PIS", "PRF", "ADJD", "PWAV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.135": {"line.1": {"text": "Gern schwitz' ich in der Tropenwelt,", "tokens": ["Gern", "schwitz'", "ich", "in", "der", "Tro\u00b7pen\u00b7welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn nur der Kitt der Herzen h\u00e4lt.", "tokens": ["Wenn", "nur", "der", "Kitt", "der", "Her\u00b7zen", "h\u00e4lt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.136": {"line.1": {"text": "Doch hatte ich es ganz vergessen:", "tokens": ["Doch", "hat\u00b7te", "ich", "es", "ganz", "ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fremd sind die Tropen zugemessen.", "tokens": ["Fremd", "sind", "die", "Tro\u00b7pen", "zu\u00b7ge\u00b7mes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.137": {"line.1": {"text": "Mexiko, die Indianerstadt,", "tokens": ["Me\u00b7xi\u00b7ko", ",", "die", "In\u00b7di\u00b7a\u00b7ner\u00b7stadt", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Dreihunderttausend Roth\u00e4ut' hat,", "tokens": ["Drei\u00b7hun\u00b7dert\u00b7tau\u00b7send", "Rot\u00b7h\u00e4ut'", "hat", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.138": {"line.1": {"text": "Die nur in wei\u00dfen Hemden stecken,", "tokens": ["Die", "nur", "in", "wei\u00b7\u00dfen", "Hem\u00b7den", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Regnet's, tragen sie rote Decken.", "tokens": ["Reg\u00b7net's", ",", "tra\u00b7gen", "sie", "ro\u00b7te", "De\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.139": {"line.1": {"text": "Wie ich kam, war just Totenfest,", "tokens": ["Wie", "ich", "kam", ",", "war", "just", "To\u00b7ten\u00b7fest", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "VAFIN", "ADV", "NN", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo man die Toten leben l\u00e4\u00dft,", "tokens": ["Wo", "man", "die", "To\u00b7ten", "le\u00b7ben", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.140": {"line.1": {"text": "Man trank statt Bier Milch von Kakteen,", "tokens": ["Man", "trank", "statt", "Bier", "Milch", "von", "Kak\u00b7te\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NN", "NN", "APPR", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich fand, man l\u00e4\u00dft sie besser stehen.", "tokens": ["Ich", "fand", ",", "man", "l\u00e4\u00dft", "sie", "bes\u00b7ser", "ste\u00b7hen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PIS", "VVFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.141": {"line.1": {"text": "Kakteen man wie K\u00fche molk,", "tokens": ["Kak\u00b7te\u00b7en", "man", "wie", "K\u00fc\u00b7he", "molk", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "KOKOM", "NN", "NE", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Denn seinen Rausch will jedes Volk.", "tokens": ["Denn", "sei\u00b7nen", "Rausch", "will", "je\u00b7des", "Volk", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.142": {"line.1": {"text": "Beim Marktplatz bei der Kathedral'", "tokens": ["Beim", "Markt\u00b7platz", "bei", "der", "Ka\u00b7the\u00b7dral'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tanzte man froh zu dem Cimbal,", "tokens": ["Tanz\u00b7te", "man", "froh", "zu", "dem", "Cim\u00b7bal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADJD", "APPR", "ART", "NE", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.143": {"line.1": {"text": "Aus Marzipan und Zuckerbrot", "tokens": ["Aus", "Mar\u00b7zi\u00b7pan", "und", "Zu\u00b7cker\u00b7brot"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gab's Totenk\u00f6pf' mit Augen rot,", "tokens": ["Gab's", "To\u00b7ten\u00b7k\u00f6pf'", "mit", "Au\u00b7gen", "rot", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.144": {"line.1": {"text": "Grabsteine, Sarg und Leichenwagen,", "tokens": ["Grab\u00b7stei\u00b7ne", ",", "Sarg", "und", "Lei\u00b7chen\u00b7wa\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Auch Schokolad' war s\u00fc\u00df dem Magen,", "tokens": ["Auch", "Scho\u00b7ko\u00b7lad'", "war", "s\u00fc\u00df", "dem", "Ma\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VAFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.145": {"line.1": {"text": "Der Tod schmeckte selbst als Skelett,", "tokens": ["Der", "Tod", "schmeck\u00b7te", "selbst", "als", "Ske\u00b7lett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "KOUS", "NN", "$,"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Aus Kuchen sogar macht er fett.", "tokens": ["Aus", "Ku\u00b7chen", "so\u00b7gar", "macht", "er", "fett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.146": {"line.1": {"text": "Bunt sa\u00df der Tod in hundert Buden,", "tokens": ["Bunt", "sa\u00df", "der", "Tod", "in", "hun\u00b7dert", "Bu\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die lebhaft zum Einkauf einluden,", "tokens": ["Die", "leb\u00b7haft", "zum", "Ein\u00b7kauf", "ein\u00b7lu\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPRART", "NN", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.147": {"line.1": {"text": "Ich brauchte nicht den Tod zu kaufen,", "tokens": ["Ich", "brauch\u00b7te", "nicht", "den", "Tod", "zu", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Heimweh lie\u00df mich kaum noch schnaufen.", "tokens": ["Denn", "Heim\u00b7weh", "lie\u00df", "mich", "kaum", "noch", "schnau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.148": {"line.1": {"text": "Fremd war der Gegenf\u00fc\u00dfler Welt,", "tokens": ["Fremd", "war", "der", "Ge\u00b7gen\u00b7f\u00fc\u00df\u00b7ler", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "F\u00fchlte mich st\u00fcndlich kopfgestellt.", "tokens": ["F\u00fchl\u00b7te", "mich", "st\u00fcnd\u00b7lich", "kopf\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.149": {"line.1": {"text": "Statt Spatzen, aufgereiht in Gassen,", "tokens": ["Statt", "Spat\u00b7zen", ",", "auf\u00b7ge\u00b7reiht", "in", "Gas\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Aasgeier auf den D\u00e4chern sa\u00dfen,", "tokens": ["Aas\u00b7gei\u00b7er", "auf", "den", "D\u00e4\u00b7chern", "sa\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.150": {"line.1": {"text": "Ihr Aug' stierte blutgierig still,", "tokens": ["Ihr", "Aug'", "stier\u00b7te", "blut\u00b7gie\u00b7rig", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Ob man sein Herz hinwerfen will,", "tokens": ["Ob", "man", "sein", "Herz", "hin\u00b7wer\u00b7fen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.151": {"line.1": {"text": "Rebellisch rauscht dann ihr Gefieder,", "tokens": ["Re\u00b7bel\u00b7lisch", "rauscht", "dann", "ihr", "Ge\u00b7fie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "St\u00fcrzen sie zu den Gossen nieder,", "tokens": ["St\u00fcr\u00b7zen", "sie", "zu", "den", "Gos\u00b7sen", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.152": {"line.1": {"text": "Rei\u00dfen sich wegen eines Bissens", "tokens": ["Rei\u00b7\u00dfen", "sich", "we\u00b7gen", "ei\u00b7nes", "Bis\u00b7sens"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Schwarz wie die Geier des Gewissens.", "tokens": ["Schwarz", "wie", "die", "Gei\u00b7er", "des", "Ge\u00b7wis\u00b7sens", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.153": {"line.1": {"text": "Mein Herz schien mir dazu zu gut,", "tokens": ["Mein", "Herz", "schien", "mir", "da\u00b7zu", "zu", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "PAV", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn's auch was will, was man nicht tut.", "tokens": ["Wenn's", "auch", "was", "will", ",", "was", "man", "nicht", "tut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PWS", "VMFIN", "$,", "PRELS", "PIS", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.154": {"line.1": {"text": "Kaum tr\u00f6stlich wirkten Mi\u00dfgeburten,", "tokens": ["Kaum", "tr\u00f6st\u00b7lich", "wirk\u00b7ten", "Mi\u00df\u00b7ge\u00b7bur\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die liebevoll gepflegt hier wurden.", "tokens": ["Die", "lie\u00b7be\u00b7voll", "ge\u00b7pflegt", "hier", "wur\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVPP", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.155": {"line.1": {"text": "Menschen, die von Geburt nicht locken,", "tokens": ["Men\u00b7schen", ",", "die", "von", "Ge\u00b7burt", "nicht", "lo\u00b7cken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Taten an Stra\u00dfenecken hocken.", "tokens": ["Ta\u00b7ten", "an", "Stra\u00b7\u00dfen\u00b7ec\u00b7ken", "ho\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.156": {"line.1": {"text": "Halb Kalb, halb Hund manch einer war,", "tokens": ["Halb", "Kalb", ",", "halb", "Hund", "manch", "ei\u00b7ner", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ADJD", "NN", "PIAT", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein anderer zehnarmig gar.", "tokens": ["Ein", "an\u00b7de\u00b7rer", "zehn\u00b7ar\u00b7mig", "gar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJD", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.157": {"line.1": {"text": "Anbettelnd dich um dein Erbarmen,", "tokens": ["An\u00b7bet\u00b7telnd", "dich", "um", "dein", "Er\u00b7bar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Winkten sie dir gleich mit zehn Armen.", "tokens": ["Wink\u00b7ten", "sie", "dir", "gleich", "mit", "zehn", "Ar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "ADV", "APPR", "CARD", "NN", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.158": {"line.1": {"text": "Ich dacht', werd' ich nochmals geboren,", "tokens": ["Ich", "dacht'", ",", "werd'", "ich", "noch\u00b7mals", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Bring' ich gleich mit die Eselsohren.", "tokens": ["Bring'", "ich", "gleich", "mit", "die", "E\u00b7sel\u00b7soh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.159": {"line.1": {"text": "Warum hab' ich die Reis' gemacht,", "tokens": ["Wa\u00b7rum", "hab'", "ich", "die", "Reis'", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hier die G\u00f6tter hergebracht?", "tokens": ["Und", "hier", "die", "G\u00f6t\u00b7ter", "her\u00b7ge\u00b7bracht", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.160": {"line.1": {"text": "Ich ging noch zur Arena rot,", "tokens": ["Ich", "ging", "noch", "zur", "A\u00b7re\u00b7na", "rot", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Dort stach man festlich Stiere tot.", "tokens": ["Dort", "stach", "man", "fest\u00b7lich", "Stie\u00b7re", "tot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADJD", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.161": {"line.1": {"text": "Den sch\u00f6nen Stier, ich kann's nicht fassen,", "tokens": ["Den", "sch\u00f6\u00b7nen", "Stier", ",", "ich", "kann's", "nicht", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sollte man wirklich leben lassen.", "tokens": ["Soll\u00b7te", "man", "wirk\u00b7lich", "le\u00b7ben", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADJD", "VVINF", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.162": {"line.1": {"text": "Stolz auf vier Beinen angebracht,", "tokens": ["Stolz", "auf", "vier", "Bei\u00b7nen", "an\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "CARD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verk\u00f6rpert er die Mannespracht.", "tokens": ["Ver\u00b7k\u00f6r\u00b7pert", "er", "die", "Man\u00b7nes\u00b7pracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.163": {"line.1": {"text": "Nur weil das Rot ihn irritiert,", "tokens": ["Nur", "weil", "das", "Rot", "ihn", "ir\u00b7ri\u00b7tiert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wird er mit Kunst zu Tod verf\u00fchrt.", "tokens": ["Wird", "er", "mit", "Kunst", "zu", "Tod", "ver\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.164": {"line.1": {"text": "Auch ich kam nur nach Mexiko,", "tokens": ["Auch", "ich", "kam", "nur", "nach", "Me\u00b7xi\u00b7ko", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ADV", "APPR", "NE", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Weil ich vor etwas Rotem floh,", "tokens": ["Weil", "ich", "vor", "et\u00b7was", "Ro\u00b7tem", "floh", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.165": {"line.1": {"text": "Wie 's rote Tuch vor einem Stier,", "tokens": ["Wie", "'s", "ro\u00b7te", "Tuch", "vor", "ei\u00b7nem", "Stier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Hing stets mein Herz vorm Auge mir.", "tokens": ["Hing", "stets", "mein", "Herz", "vorm", "Au\u00b7ge", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "APPRART", "NN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.166": {"line.1": {"text": "Vor meinem Blut wollt' ich entfliehn,", "tokens": ["Vor", "mei\u00b7nem", "Blut", "wollt'", "ich", "ent\u00b7fliehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch tat mein Blut stets mit mir ziehn.", "tokens": ["Doch", "tat", "mein", "Blut", "stets", "mit", "mir", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.167": {"line.1": {"text": "Nachts war gar alle Ruhe hin,", "tokens": ["Nachts", "war", "gar", "al\u00b7le", "Ru\u00b7he", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "NN", "PTKVZ", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nachtigalln wie Trompeten schrien.", "tokens": ["Nach\u00b7ti\u00b7galln", "wie", "Trom\u00b7pe\u00b7ten", "schri\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "NN", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.168": {"line.1": {"text": "Die Nacht, die s\u00fc\u00df zum Liebeswerben,", "tokens": ["Die", "Nacht", ",", "die", "s\u00fc\u00df", "zum", "Lie\u00b7bes\u00b7wer\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Taten Kleinigkeiten verderben,", "tokens": ["Ta\u00b7ten", "Klei\u00b7nig\u00b7kei\u00b7ten", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.169": {"line.1": {"text": "Ich werde niemals sie vergessen,", "tokens": ["Ich", "wer\u00b7de", "nie\u00b7mals", "sie", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fast jede hat mich aufgefressen.", "tokens": ["Fast", "je\u00b7de", "hat", "mich", "auf\u00b7ge\u00b7fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.170": {"line.1": {"text": "Moskitos leben klein f\u00fcr sich,", "tokens": ["Mos\u00b7ki\u00b7tos", "le\u00b7ben", "klein", "f\u00fcr", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADJD", "APPR", "PRF", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Wie N\u00e4hmaschinen Stich bei Stich,", "tokens": ["Wie", "N\u00e4h\u00b7ma\u00b7schi\u00b7nen", "Stich", "bei", "Stich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.171": {"line.1": {"text": "Und liegst du unter dicken Netzen,", "tokens": ["Und", "liegst", "du", "un\u00b7ter", "di\u00b7cken", "Net\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie fressen dich auch dort in Fetzen,", "tokens": ["Sie", "fres\u00b7sen", "dich", "auch", "dort", "in", "Fet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.172": {"line.1": {"text": "Sie lieben mehr das fremde Blut,", "tokens": ["Sie", "lie\u00b7ben", "mehr", "das", "frem\u00b7de", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dazu ist der Fremde gut.", "tokens": ["Und", "da\u00b7zu", "ist", "der", "Frem\u00b7de", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.173": {"line.1": {"text": "Mu\u00dft n\u00e4chtlich blutig um dich schlagen,", "tokens": ["Mu\u00dft", "n\u00e4cht\u00b7lich", "blu\u00b7tig", "um", "dich", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADJD", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kennst bald nur Schlaf vom H\u00f6rensagen.", "tokens": ["Kennst", "bald", "nur", "Schlaf", "vom", "H\u00f6\u00b7ren\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.174": {"line.1": {"text": "Frau K\u00f6nigin ward ganz entstellt,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "ward", "ganz", "ent\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nat\u00fcrlich, da\u00df ein Weib das qu\u00e4lt.", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", ",", "da\u00df", "ein", "Weib", "das", "qu\u00e4lt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.175": {"line.1": {"text": "Sie sprach: \u00bbEs dauert nicht mehr lang,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Es", "dau\u00b7ert", "nicht", "mehr", "lang", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erkennst du mich nur noch am Gang.\u00ab", "tokens": ["Er\u00b7kennst", "du", "mich", "nur", "noch", "am", "Gang", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "ADV", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.176": {"line.1": {"text": "Morgens am Fenster wir auch fanden,", "tokens": ["Mor\u00b7gens", "am", "Fens\u00b7ter", "wir", "auch", "fan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PPER", "ADV", "VVFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Da\u00df drau\u00dfen fremde L\u00e4nder standen.", "tokens": ["Da\u00df", "drau\u00b7\u00dfen", "frem\u00b7de", "L\u00e4n\u00b7der", "stan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.177": {"line.1": {"text": "Statt fr\u00fch die Milchfrau klingeln tut,", "tokens": ["Statt", "fr\u00fch", "die", "Milch\u00b7frau", "klin\u00b7geln", "tut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nahn M\u00e4dchen mit Kaffee im Blut.", "tokens": ["Nahn", "M\u00e4d\u00b7chen", "mit", "Kaf\u00b7fee", "im", "Blut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "NN", "APPRART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.178": {"line.1": {"text": "Sie bieten schweigsam wie die Toten,", "tokens": ["Sie", "bie\u00b7ten", "schweig\u00b7sam", "wie", "die", "To\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Paprika scharf in roten Schoten.", "tokens": ["Pa\u00b7pri\u00b7ka", "scharf", "in", "ro\u00b7ten", "Scho\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.179": {"line.1": {"text": "Gef\u00e4rbte Rosen sie auch gaben,", "tokens": ["Ge\u00b7f\u00e4rb\u00b7te", "Ro\u00b7sen", "sie", "auch", "ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie angestrichne Waisenknaben.", "tokens": ["Wie", "an\u00b7ge\u00b7strich\u00b7ne", "Wai\u00b7sen\u00b7kna\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.180": {"line.1": {"text": "Raben, bemalt wie Hottentotten,", "tokens": ["Ra\u00b7ben", ",", "be\u00b7malt", "wie", "Hot\u00b7ten\u00b7tot\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "KOKOM", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Als Paradiesv\u00f6gel sie boten.", "tokens": ["Als", "Pa\u00b7ra\u00b7dies\u00b7v\u00f6\u00b7gel", "sie", "bo\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.181": {"line.1": {"text": "Im Hintergrunde standen Krater,", "tokens": ["Im", "Hin\u00b7ter\u00b7grun\u00b7de", "stan\u00b7den", "Kra\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Katzenbuckelnd wie falsche Kater,", "tokens": ["Kat\u00b7zen\u00b7bu\u00b7ckelnd", "wie", "fal\u00b7sche", "Ka\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.182": {"line.1": {"text": "Und Erdbeben trieb sich umher,", "tokens": ["Und", "Erd\u00b7be\u00b7ben", "trieb", "sich", "um\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PRF", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Es kollerte wie Bauchredner.", "tokens": ["Es", "kol\u00b7ler\u00b7te", "wie", "Bauch\u00b7red\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "NN", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}}, "stanza.183": {"line.1": {"text": "Sah H\u00e4ngelampen pendelnd schwanken,", "tokens": ["Sah", "H\u00e4n\u00b7ge\u00b7lam\u00b7pen", "pen\u00b7delnd", "schwan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hielt mich nur aufrecht in Gedanken.", "tokens": ["Hielt", "mich", "nur", "auf\u00b7recht", "in", "Ge\u00b7dan\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.184": {"line.1": {"text": "F\u00fchlte mich in dem Wiesenrain,", "tokens": ["F\u00fchl\u00b7te", "mich", "in", "dem", "Wie\u00b7sen\u00b7rain", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mit meiner Frau als Blattlaus klein,", "tokens": ["Mit", "mei\u00b7ner", "Frau", "als", "Blatt\u00b7laus", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KOUS", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.185": {"line.1": {"text": "Tat jedes Graslager vermissen,", "tokens": ["Tat", "je\u00b7des", "Gras\u00b7la\u00b7ger", "ver\u00b7mis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn ringsum tat nur Kaktus schie\u00dfen.", "tokens": ["Denn", "ring\u00b7sum", "tat", "nur", "Kak\u00b7tus", "schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.186": {"line.1": {"text": "Die Welt schien mir verkauderwelscht", "tokens": ["Die", "Welt", "schien", "mir", "ver\u00b7kau\u00b7der\u00b7welscht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nur mein Heimweh unverf\u00e4lscht.", "tokens": ["Und", "nur", "mein", "Heim\u00b7weh", "un\u00b7ver\u00b7f\u00e4lscht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.187": {"line.1": {"text": "Frau K\u00f6nigin, wie immer mild,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", ",", "wie", "im\u00b7mer", "mild", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PWAV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Blieb mir im Schmerz Madonnenbild,", "tokens": ["Blieb", "mir", "im", "Schmerz", "Ma\u00b7don\u00b7nen\u00b7bild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.188": {"line.1": {"text": "Wenn neue Wunder uns geschahn,", "tokens": ["Wenn", "neu\u00b7e", "Wun\u00b7der", "uns", "ge\u00b7schahn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sah K\u00f6nigin mich fragend an.", "tokens": ["Sah", "K\u00f6\u00b7ni\u00b7gin", "mich", "fra\u00b7gend", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.189": {"line.1": {"text": "Dann senkte sie die Augenlider", "tokens": ["Dann", "senk\u00b7te", "sie", "die", "Au\u00b7gen\u00b7li\u00b7der"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und sah still in ihr Herze nieder.", "tokens": ["Und", "sah", "still", "in", "ihr", "Her\u00b7ze", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.190": {"line.1": {"text": "Das war der einzig glatte Fleck,", "tokens": ["Das", "war", "der", "ein\u00b7zig", "glat\u00b7te", "Fleck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hier war noch nicht die Ruhe weg.", "tokens": ["Hier", "war", "noch", "nicht", "die", "Ru\u00b7he", "weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PTKNEG", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.191": {"line.1": {"text": "Die Venus blieb im Lagerhaus,", "tokens": ["Die", "Ve\u00b7nus", "blieb", "im", "La\u00b7ger\u00b7haus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir packten sie schon gar nicht aus.", "tokens": ["Wir", "pack\u00b7ten", "sie", "schon", "gar", "nicht", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.192": {"line.1": {"text": "Ich sprach: \u00bbDies ist der erste Grund:", "tokens": ["Ich", "sprach", ":", "\u00bb", "Dies", "ist", "der", "ers\u00b7te", "Grund", ":"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PDS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nie ist ruhige Liebesstund',", "tokens": ["Nie", "ist", "ru\u00b7hi\u00b7ge", "Lie\u00b7besstund'", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.193": {"line.1": {"text": "Und trotz der Hitz' hat kalt man da,", "tokens": ["Und", "trotz", "der", "Hitz'", "hat", "kalt", "man", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VAFIN", "ADJD", "PIS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es zieht mich heim nach Europa.", "tokens": ["Es", "zieht", "mich", "heim", "nach", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.194": {"line.1": {"text": "Auch sieh mal diese Palmen an,", "tokens": ["Auch", "sieh", "mal", "die\u00b7se", "Pal\u00b7men", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "PDAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Palm' mich nie verstehen kann,", "tokens": ["Die", "Palm'", "mich", "nie", "ver\u00b7ste\u00b7hen", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.195": {"line.1": {"text": "Ich tue alle sie verfluchen,", "tokens": ["Ich", "tue", "al\u00b7le", "sie", "ver\u00b7flu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PPER", "VVINF", "$,"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Sie sind durchaus nicht wie die Buchen,", "tokens": ["Sie", "sind", "durc\u00b7haus", "nicht", "wie", "die", "Bu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.196": {"line.1": {"text": "Und ich will nicht mein ganzes Leben,", "tokens": ["Und", "ich", "will", "nicht", "mein", "gan\u00b7zes", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PTKNEG", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hier diesen fremden Str\u00fcnken geben.", "tokens": ["Hier", "die\u00b7sen", "frem\u00b7den", "Str\u00fcn\u00b7ken", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.197": {"line.1": {"text": "Es tut zu Mi\u00dfgeburten treiben,", "tokens": ["Es", "tut", "zu", "Mi\u00df\u00b7ge\u00b7bur\u00b7ten", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich will nicht eine Nacht mehr bleiben.\u00ab", "tokens": ["Ich", "will", "nicht", "ei\u00b7ne", "Nacht", "mehr", "blei\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ART", "NN", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.198": {"line.1": {"text": "K\u00f6nigin sprach: \u00bbWas gut ich seh',", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "sprach", ":", "\u00bb", "Was", "gut", "ich", "seh'", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "PWS", "ADJD", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gut riecht's nach Zucker und Kaffee,", "tokens": ["Gut", "riecht's", "nach", "Zu\u00b7cker", "und", "Kaf\u00b7fee", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.199": {"line.1": {"text": "Wir kaufen viele Pfunde ein,", "tokens": ["Wir", "kau\u00b7fen", "vie\u00b7le", "Pfun\u00b7de", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dann soll auch die Heimreis' sein.", "tokens": ["Und", "dann", "soll", "auch", "die", "Heim\u00b7reis'", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "ADV", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.200": {"line.1": {"text": "Die G\u00f6tter m\u00f6gen all hier bleiben,", "tokens": ["Die", "G\u00f6t\u00b7ter", "m\u00f6\u00b7gen", "all", "hier", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIAT", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie die Moskitos austreiben.", "tokens": ["Da\u00df", "sie", "die", "Mos\u00b7ki\u00b7tos", "aus\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.201": {"line.1": {"text": "Es ist ein Ach in jedem Wind,", "tokens": ["Es", "ist", "ein", "Ach", "in", "je\u00b7dem", "Wind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch ich die Heimreis' lohnend find'.\u00ab", "tokens": ["Auch", "ich", "die", "Heim\u00b7reis'", "loh\u00b7nend", "find'", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "VVPP", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.202": {"line.1": {"text": "Ich kaufte klein ein Krokodil,", "tokens": ["Ich", "kauf\u00b7te", "klein", "ein", "Kro\u00b7ko\u00b7dil", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es weinte mir der Tr\u00e4nen viel,", "tokens": ["Es", "wein\u00b7te", "mir", "der", "Tr\u00e4\u00b7nen", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.203": {"line.1": {"text": "So da\u00df ich lachend davon kam,", "tokens": ["So", "da\u00df", "ich", "la\u00b7chend", "da\u00b7von", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADJD", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als ich vom Land schnell Abschied nahm.", "tokens": ["Als", "ich", "vom", "Land", "schnell", "Ab\u00b7schied", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.204": {"line.1": {"text": "Kaum traten wir auf hohe See,", "tokens": ["Kaum", "tra\u00b7ten", "wir", "auf", "ho\u00b7he", "See", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da rief das ganze Schiff: \u00bbJuchhe!\u00ab", "tokens": ["Da", "rief", "das", "gan\u00b7ze", "Schiff", ":", "\u00bb", "Juch\u00b7he", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$.", "$(", "ITJ", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.205": {"line.1": {"text": "Die Wellen rund wie Kugeln schossen,", "tokens": ["Die", "Wel\u00b7len", "rund", "wie", "Ku\u00b7geln", "schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und senkrecht auch wie lange Hosen,", "tokens": ["Und", "sen\u00b7krecht", "auch", "wie", "lan\u00b7ge", "Ho\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KOKOM", "ADV", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}}, "stanza.206": {"line.1": {"text": "Sturm kam da jeden Nachmittag,", "tokens": ["Sturm", "kam", "da", "je\u00b7den", "Nach\u00b7mit\u00b7tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Selig ich auf dem R\u00fccken lag,", "tokens": ["Se\u00b7lig", "ich", "auf", "dem", "R\u00fc\u00b7cken", "lag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.207": {"line.1": {"text": "Der Sturm kam wie Artillerie,", "tokens": ["Der", "Sturm", "kam", "wie", "Ar\u00b7til\u00b7le\u00b7rie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch st\u00fcndlich ich laut \u00bbVivat\u00ab schrie.", "tokens": ["Doch", "st\u00fcnd\u00b7lich", "ich", "laut", "\u00bb", "Vi\u00b7vat", "\u00ab", "schrie", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "ADJD", "PPER", "ADJD", "$(", "NN", "$(", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.208": {"line.1": {"text": "Das Schiff auf Hinterf\u00fc\u00dfen stand,", "tokens": ["Das", "Schiff", "auf", "Hin\u00b7ter\u00b7f\u00fc\u00b7\u00dfen", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ging fast vor Freud' aus Rand und Band.", "tokens": ["Ging", "fast", "vor", "Freud'", "aus", "Rand", "und", "Band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.209": {"line.1": {"text": "Mit Balken lag das Meer belegt,", "tokens": ["Mit", "Bal\u00b7ken", "lag", "das", "Meer", "be\u00b7legt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wracks kamen durch den Sturm gefegt,", "tokens": ["Wracks", "ka\u00b7men", "durch", "den", "Sturm", "ge\u00b7fegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.210": {"line.1": {"text": "Ich z\u00e4hl' es zu dem Wunderbaren,", "tokens": ["Ich", "z\u00e4hl'", "es", "zu", "dem", "Wun\u00b7der\u00b7ba\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df wir nicht auf dem Kopf gefahren.", "tokens": ["Da\u00df", "wir", "nicht", "auf", "dem", "Kopf", "ge\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.211": {"line.1": {"text": "Wei\u00df war im Schiff ein Marmorsaal,", "tokens": ["Wei\u00df", "war", "im", "Schiff", "ein", "Mar\u00b7mor\u00b7saal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6nigin lag dort w\u00e4schefahl,", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "lag", "dort", "w\u00e4\u00b7sche\u00b7fahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.212": {"line.1": {"text": "Ihr sch\u00f6nes Haar h\u00fcllte ein Tuch,", "tokens": ["Ihr", "sch\u00f6\u00b7nes", "Haar", "h\u00fcll\u00b7te", "ein", "Tuch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Denn t\u00f6lpelhaft kam oft Besuch.", "tokens": ["Denn", "t\u00f6l\u00b7pel\u00b7haft", "kam", "oft", "Be\u00b7such", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.213": {"line.1": {"text": "Mit schweren Schritten wie ein Gong,", "tokens": ["Mit", "schwe\u00b7ren", "Schrit\u00b7ten", "wie", "ein", "Gong", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Warf sich das Meer in den Salon,", "tokens": ["Warf", "sich", "das", "Meer", "in", "den", "Sa\u00b7lon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.214": {"line.1": {"text": "Wusch scharf mit Salz das Haar uns aus,", "tokens": ["Wusch", "scharf", "mit", "Salz", "das", "Haar", "uns", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "APPR", "NN", "ART", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich rief: \u00bbAll Gold geht uns heraus.", "tokens": ["Ich", "rief", ":", "\u00bb", "All", "Gold", "geht", "uns", "he\u00b7raus", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PIAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.215": {"line.1": {"text": "Heb' ich den Suppenteller auf,", "tokens": ["Heb'", "ich", "den", "Sup\u00b7pen\u00b7tel\u00b7ler", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Flie\u00dft die Supp' senkrecht mir hinauf,", "tokens": ["Flie\u00dft", "die", "Sup\u00b7p'", "sen\u00b7krecht", "mir", "hin\u00b7auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.216": {"line.1": {"text": "Es stirbt im Hirn jeder Begriff,", "tokens": ["Es", "stirbt", "im", "Hirn", "je\u00b7der", "Be\u00b7griff", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Ich seh' die ganze Zukunft schief.\u00ab", "tokens": ["Ich", "seh'", "die", "gan\u00b7ze", "Zu\u00b7kunft", "schief", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.217": {"line.1": {"text": "Man schrie sich aus, der Sturm war laut,", "tokens": ["Man", "schrie", "sich", "aus", ",", "der", "Sturm", "war", "laut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "PTKVZ", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schiffsg\u00f6tter hab' ich uns erbaut.", "tokens": ["Schiffs\u00b7g\u00f6t\u00b7ter", "hab'", "ich", "uns", "er\u00b7baut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.218": {"line.1": {"text": "H\u00f6rt' ich die Schiffskatze miauen,", "tokens": ["H\u00f6rt'", "ich", "die", "Schiffs\u00b7kat\u00b7ze", "mi\u00b7au\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "So wuchs in mir das Gottvertrauen,", "tokens": ["So", "wuchs", "in", "mir", "das", "Gott\u00b7ver\u00b7trau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.219": {"line.1": {"text": "Und morgens, wenn der Sturm noch schwach,", "tokens": ["Und", "mor\u00b7gens", ",", "wenn", "der", "Sturm", "noch", "schwach", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kr\u00e4hte ein K\u00fcchenhahn mich wach,", "tokens": ["Kr\u00e4h\u00b7te", "ein", "K\u00fc\u00b7chen\u00b7hahn", "mich", "wach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PPER", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.220": {"line.1": {"text": "Getr\u00f6stet hat mein Herz gelacht,", "tokens": ["Ge\u00b7tr\u00f6s\u00b7tet", "hat", "mein", "Herz", "ge\u00b7lacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat Mist und K\u00fche sich erdacht.", "tokens": ["Hat", "Mist", "und", "K\u00fc\u00b7he", "sich", "er\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.221": {"line.1": {"text": "Doch abends, war der Sturm zu viel,", "tokens": ["Doch", "a\u00b7bends", ",", "war", "der", "Sturm", "zu", "viel", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "VAFIN", "ART", "NN", "APPR", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Holt' ich das kleine Krokodil.", "tokens": ["Holt'", "ich", "das", "klei\u00b7ne", "Kro\u00b7ko\u00b7dil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.222": {"line.1": {"text": "Es fra\u00df nicht und hielt Winterschlaf;", "tokens": ["Es", "fra\u00df", "nicht", "und", "hielt", "Win\u00b7ter\u00b7schlaf", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "KON", "VVFIN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Streichelnd, wenn man den Nacken traf,", "tokens": ["Strei\u00b7chelnd", ",", "wenn", "man", "den", "Na\u00b7cken", "traf", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "KOUS", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.223": {"line.1": {"text": "Sah es aus seinem Traum heraus", "tokens": ["Sah", "es", "aus", "sei\u00b7nem", "Traum", "he\u00b7raus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und weinte sich statt meiner aus.", "tokens": ["Und", "wein\u00b7te", "sich", "statt", "mei\u00b7ner", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "PPOSAT", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.224": {"line.1": {"text": "Tat man zwischen zwei Welten schweben,", "tokens": ["Tat", "man", "zwi\u00b7schen", "zwei", "Wel\u00b7ten", "schwe\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "APPR", "CARD", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Ersehnt man endlich Festlandleben.", "tokens": ["Er\u00b7sehnt", "man", "end\u00b7lich", "Fest\u00b7land\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.225": {"line.1": {"text": "Beim ersten Leuchtturm von England,", "tokens": ["Beim", "ers\u00b7ten", "Leucht\u00b7turm", "von", "En\u00b7gland", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Frau K\u00f6nigin still auferstand,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "still", "auf\u00b7er\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.226": {"line.1": {"text": "Zur Nachtstund' brannten wir Raketen,", "tokens": ["Zur", "Nacht\u00b7stund'", "brann\u00b7ten", "wir", "Ra\u00b7ke\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df Lotsen uns bemerken t\u00e4ten,", "tokens": ["Da\u00df", "Lot\u00b7sen", "uns", "be\u00b7mer\u00b7ken", "t\u00e4\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.227": {"line.1": {"text": "Am Morgen war schon H\u00e2vre da,", "tokens": ["Am", "Mor\u00b7gen", "war", "schon", "H\u00e2vre", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ADV", "NE", "ADV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und hinter ihm ganz Europa.", "tokens": ["Und", "hin\u00b7ter", "ihm", "ganz", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "ADV", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.228": {"line.1": {"text": "Wenn man solch' Luftfahrt \u00fcberstand,", "tokens": ["Wenn", "man", "solch'", "Luft\u00b7fahrt", "\u00fc\u00b7bers\u00b7tand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann k\u00fc\u00dft man gern sein Heimatland.", "tokens": ["Dann", "k\u00fc\u00dft", "man", "gern", "sein", "Hei\u00b7mat\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.229": {"line.1": {"text": "Nachts voll Confetti flog Paris,", "tokens": ["Nachts", "voll", "Con\u00b7fet\u00b7ti", "flog", "Pa\u00b7ris", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NE", "VVFIN", "NE", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wo man den Karneval einblies,", "tokens": ["Wo", "man", "den", "Kar\u00b7ne\u00b7val", "ein\u00b7blies", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.230": {"line.1": {"text": "K\u00f6nigin sprach am Opernplatz:", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "sprach", "am", "O\u00b7pern\u00b7platz", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$."], "meter": "+--+-+--", "measure": "iambic.tri.invert"}, "line.2": {"text": "\u00bbhier ist wieder Europa, Schatz.", "tokens": ["\u00bb", "hier", "ist", "wie\u00b7der", "Eu\u00b7ro\u00b7pa", ",", "Schatz", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "ADV", "NE", "$,", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.231": {"line.1": {"text": "Moskitos konnt' ich nicht forthetzen,", "tokens": ["Mos\u00b7ki\u00b7tos", "konnt'", "ich", "nicht", "for\u00b7thet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und dir Europa nicht ersetzen.\u00ab", "tokens": ["Und", "dir", "Eu\u00b7ro\u00b7pa", "nicht", "er\u00b7set\u00b7zen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "NE", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.232": {"line.1": {"text": "Ich rief: \u00bbLa\u00df jeden Weltteil leben,", "tokens": ["Ich", "rief", ":", "\u00bb", "La\u00df", "je\u00b7den", "Welt\u00b7teil", "le\u00b7ben", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VVIMP", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir wollen tanzend weiterschweben.\u00ab", "tokens": ["Wir", "wol\u00b7len", "tan\u00b7zend", "wei\u00b7ter\u00b7schwe\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}