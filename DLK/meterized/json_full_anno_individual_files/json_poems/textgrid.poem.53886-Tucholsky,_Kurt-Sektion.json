{"textgrid.poem.53886": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Sektion", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Mediziner, namens L.,", "tokens": ["Ein", "Me\u00b7di\u00b7zi\u00b7ner", ",", "na\u00b7mens", "L.", ","], "token_info": ["word", "word", "punct", "word", "abbreviation", "punct"], "pos": ["ART", "NN", "$,", "ADV", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "zers\u00e4gte neulich scharf und schnell", "tokens": ["zer\u00b7s\u00e4g\u00b7te", "neu\u00b7lich", "scharf", "und", "schnell"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Iwan Kutisker.", "tokens": ["I\u00b7wan", "Ku\u00b7tis\u00b7ker", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Der lag da vor ihm h\u00fcllenbar,", "tokens": ["Der", "lag", "da", "vor", "ihm", "h\u00fcl\u00b7len\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "so wie er aus der Haft gekommen war \u2013", "tokens": ["so", "wie", "er", "aus", "der", "Haft", "ge\u00b7kom\u00b7men", "war", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PPER", "APPR", "ART", "NN", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "der tote Iwan Kutisker.", "tokens": ["der", "to\u00b7te", "I\u00b7wan", "Ku\u00b7tis\u00b7ker", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}}, "stanza.2": {"line.1": {"text": "Der Mediziner, namens L.,", "tokens": ["Der", "Me\u00b7di\u00b7zi\u00b7ner", ",", "na\u00b7mens", "L.", ","], "token_info": ["word", "word", "punct", "word", "abbreviation", "punct"], "pos": ["ART", "NN", "$,", "ADV", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "sprach also in des Bauches Fell", "tokens": ["sprach", "al\u00b7so", "in", "des", "Bau\u00b7ches", "Fell"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "des toten Iwan Kutisker:", "tokens": ["des", "to\u00b7ten", "I\u00b7wan", "Ku\u00b7tis\u00b7ker", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "\u00bbder Mann, der hier vor Ihnen liegt,", "tokens": ["\u00bb", "der", "Mann", ",", "der", "hier", "vor", "Ih\u00b7nen", "liegt", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "hat lange nicht genug gekriegt:", "tokens": ["hat", "lan\u00b7ge", "nicht", "ge\u00b7nug", "ge\u00b7kriegt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "er hie\u00df Kutisker, war ein Jude \u2013", "tokens": ["er", "hie\u00df", "Ku\u00b7tis\u00b7ker", ",", "war", "ein", "Ju\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "(sie sehen das schon an der Zude) \u2013", "tokens": ["(", "sie", "se\u00b7hen", "das", "schon", "an", "der", "Zu\u00b7de", ")", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "ADV", "APPR", "ART", "NN", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "er war ganz nikotinisiert", "tokens": ["er", "war", "ganz", "ni\u00b7ko\u00b7ti\u00b7ni\u00b7siert"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.9": {"text": "und syphilitisch infiziert \u2013", "tokens": ["und", "sy\u00b7phi\u00b7li\u00b7tisch", "in\u00b7fi\u00b7ziert", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "na ja, ein Jude!\u00ab", "tokens": ["na", "ja", ",", "ein", "Ju\u00b7de", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ITJ", "ADV", "$,", "ART", "NN", "$.", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.3": {"line.1": {"text": "Das Messer knirscht. Der Kantus stieg", "tokens": ["Das", "Mes\u00b7ser", "knirscht", ".", "Der", "Kan\u00b7tus", "stieg"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "voll \u00e4rztlichen Takts. Die Leiche schwieg.", "tokens": ["voll", "\u00e4rzt\u00b7li\u00b7chen", "Takts", ".", "Die", "Lei\u00b7che", "schwieg", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "La\u00dft uns die Z\u00e4hne zusammenbei\u00dfen:", "tokens": ["La\u00dft", "uns", "die", "Z\u00e4h\u00b7ne", "zu\u00b7sam\u00b7men\u00b7bei\u00b7\u00dfen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "es kann nicht jeder Lubarsch hei\u00dfen.", "tokens": ["es", "kann", "nicht", "je\u00b7der", "Lu\u00b7barsch", "hei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ein Mediziner, namens L.,", "tokens": ["Ein", "Me\u00b7di\u00b7zi\u00b7ner", ",", "na\u00b7mens", "L.", ","], "token_info": ["word", "word", "punct", "word", "abbreviation", "punct"], "pos": ["ART", "NN", "$,", "ADV", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "zers\u00e4gte neulich scharf und schnell", "tokens": ["zer\u00b7s\u00e4g\u00b7te", "neu\u00b7lich", "scharf", "und", "schnell"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Iwan Kutisker.", "tokens": ["I\u00b7wan", "Ku\u00b7tis\u00b7ker", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Der lag da vor ihm h\u00fcllenbar,", "tokens": ["Der", "lag", "da", "vor", "ihm", "h\u00fcl\u00b7len\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "so wie er aus der Haft gekommen war \u2013", "tokens": ["so", "wie", "er", "aus", "der", "Haft", "ge\u00b7kom\u00b7men", "war", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PPER", "APPR", "ART", "NN", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "der tote Iwan Kutisker.", "tokens": ["der", "to\u00b7te", "I\u00b7wan", "Ku\u00b7tis\u00b7ker", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}}, "stanza.5": {"line.1": {"text": "Der Mediziner, namens L.,", "tokens": ["Der", "Me\u00b7di\u00b7zi\u00b7ner", ",", "na\u00b7mens", "L.", ","], "token_info": ["word", "word", "punct", "word", "abbreviation", "punct"], "pos": ["ART", "NN", "$,", "ADV", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "sprach also in des Bauches Fell", "tokens": ["sprach", "al\u00b7so", "in", "des", "Bau\u00b7ches", "Fell"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "des toten Iwan Kutisker:", "tokens": ["des", "to\u00b7ten", "I\u00b7wan", "Ku\u00b7tis\u00b7ker", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "\u00bbder Mann, der hier vor Ihnen liegt,", "tokens": ["\u00bb", "der", "Mann", ",", "der", "hier", "vor", "Ih\u00b7nen", "liegt", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "hat lange nicht genug gekriegt:", "tokens": ["hat", "lan\u00b7ge", "nicht", "ge\u00b7nug", "ge\u00b7kriegt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "er hie\u00df Kutisker, war ein Jude \u2013", "tokens": ["er", "hie\u00df", "Ku\u00b7tis\u00b7ker", ",", "war", "ein", "Ju\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "(sie sehen das schon an der Zude) \u2013", "tokens": ["(", "sie", "se\u00b7hen", "das", "schon", "an", "der", "Zu\u00b7de", ")", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "ADV", "APPR", "ART", "NN", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "er war ganz nikotinisiert", "tokens": ["er", "war", "ganz", "ni\u00b7ko\u00b7ti\u00b7ni\u00b7siert"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.9": {"text": "und syphilitisch infiziert \u2013", "tokens": ["und", "sy\u00b7phi\u00b7li\u00b7tisch", "in\u00b7fi\u00b7ziert", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "na ja, ein Jude!\u00ab", "tokens": ["na", "ja", ",", "ein", "Ju\u00b7de", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ITJ", "ADV", "$,", "ART", "NN", "$.", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.6": {"line.1": {"text": "Das Messer knirscht. Der Kantus stieg", "tokens": ["Das", "Mes\u00b7ser", "knirscht", ".", "Der", "Kan\u00b7tus", "stieg"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "voll \u00e4rztlichen Takts. Die Leiche schwieg.", "tokens": ["voll", "\u00e4rzt\u00b7li\u00b7chen", "Takts", ".", "Die", "Lei\u00b7che", "schwieg", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "La\u00dft uns die Z\u00e4hne zusammenbei\u00dfen:", "tokens": ["La\u00dft", "uns", "die", "Z\u00e4h\u00b7ne", "zu\u00b7sam\u00b7men\u00b7bei\u00b7\u00dfen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "es kann nicht jeder Lubarsch hei\u00dfen.", "tokens": ["es", "kann", "nicht", "je\u00b7der", "Lu\u00b7barsch", "hei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}