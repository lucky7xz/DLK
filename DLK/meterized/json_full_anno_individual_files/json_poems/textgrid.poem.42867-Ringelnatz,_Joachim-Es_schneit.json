{"textgrid.poem.42867": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Es schneit", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es schneit dicke Flocken,", "tokens": ["Es", "schneit", "di\u00b7cke", "Flo\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJA", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Nicht warm, aber frisch gebacken.", "tokens": ["Nicht", "warm", ",", "a\u00b7ber", "frisch", "ge\u00b7ba\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$,", "ADV", "ADJD", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die setzen sich in meine Dichterlocken,", "tokens": ["Die", "set\u00b7zen", "sich", "in", "mei\u00b7ne", "Dich\u00b7ter\u00b7lo\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In meinen Schiebernacken,", "tokens": ["In", "mei\u00b7nen", "Schie\u00b7bern\u00b7a\u00b7cken", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Auf meine Smoking-Socken.", "tokens": ["Auf", "mei\u00b7ne", "Smo\u00b7king\u00b7So\u00b7cken", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Sie machen den Polizisten", "tokens": ["Sie", "ma\u00b7chen", "den", "Po\u00b7li\u00b7zis\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Gem\u00fctlich zum Weihnachtsmann.", "tokens": ["Ge\u00b7m\u00fct\u00b7lich", "zum", "Weih\u00b7nachts\u00b7mann", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da legen die Touristen", "tokens": ["Da", "le\u00b7gen", "die", "Tou\u00b7ris\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+--++-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihre Polarausr\u00fcstung an.", "tokens": ["Ih\u00b7re", "Po\u00b7lar\u00b7aus\u00b7r\u00fcs\u00b7tung", "an", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Wir wollen uns alle zusammentun,", "tokens": ["Wir", "wol\u00b7len", "uns", "al\u00b7le", "zu\u00b7sam\u00b7men\u00b7tun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Um den Beschlu\u00df zu fassen:", "tokens": ["Um", "den", "Be\u00b7schlu\u00df", "zu", "fas\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es d\u00fcrfen alle Sachsen von nun", "tokens": ["Es", "d\u00fcr\u00b7fen", "al\u00b7le", "Sach\u00b7sen", "von", "nun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "APPR", "ADV"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "An nicht mehr ihr Land verlassen.", "tokens": ["An", "nicht", "mehr", "ihr", "Land", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PTKNEG", "ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Sie querten mit wilder Behaglichkeit", "tokens": ["Sie", "quer\u00b7ten", "mit", "wil\u00b7der", "Be\u00b7hag\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Karlmayisch gedachte Fernen", "tokens": ["Karl\u00b7may\u00b7isch", "ge\u00b7dach\u00b7te", "Fer\u00b7nen"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und blieben Sachsen. Es wird f\u00fcr sie Zeit,", "tokens": ["Und", "blie\u00b7ben", "Sach\u00b7sen", ".", "Es", "wird", "f\u00fcr", "sie", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "$.", "PPER", "VAFIN", "APPR", "PPER", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sich selbst erst mal kennenzulernen.", "tokens": ["Sich", "selbst", "erst", "mal", "ken\u00b7nen\u00b7zu\u00b7ler\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "Es schneit.", "tokens": ["Es", "schneit", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.5": {"line.1": {"text": "Wenn hundert Leute sich einig sind,", "tokens": ["Wenn", "hun\u00b7dert", "Leu\u00b7te", "sich", "ei\u00b7nig", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "NN", "PRF", "ADJD", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dann f\u00fchlen sich die als Giganten", "tokens": ["Dann", "f\u00fch\u00b7len", "sich", "die", "als", "Gi\u00b7gan\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und schwafeln vor einem vern\u00fcnftigen Kind", "tokens": ["Und", "schwa\u00b7feln", "vor", "ei\u00b7nem", "ver\u00b7n\u00fcnf\u00b7ti\u00b7gen", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wie taube verwunschene Tanten.", "tokens": ["Wie", "tau\u00b7be", "ver\u00b7wun\u00b7sche\u00b7ne", "Tan\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.6": {"line.1": {"text": "Es schneit. Wie in unserer Kinderzeit.", "tokens": ["Es", "schneit", ".", "Wie", "in", "un\u00b7se\u00b7rer", "Kin\u00b7der\u00b7zeit", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Zum Wintersport eingeladen,", "tokens": ["Zum", "Win\u00b7ter\u00b7sport", "ein\u00b7ge\u00b7la\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Gehe ich schlafen. Es schneit. Es schneit.", "tokens": ["Ge\u00b7he", "ich", "schla\u00b7fen", ".", "Es", "schneit", ".", "Es", "schneit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVINF", "$.", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Es schneit f\u00fcr den Landmann Kuhfladen.", "tokens": ["Es", "schneit", "f\u00fcr", "den", "Land\u00b7mann", "Kuh\u00b7fla\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.7": {"line.1": {"text": "Es schneit f\u00fcr die Zukunft Stra\u00dfendreck.", "tokens": ["Es", "schneit", "f\u00fcr", "die", "Zu\u00b7kunft", "Stra\u00b7\u00dfen\u00b7dreck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Auf Gr\u00e4ber schneit's wei\u00dfe Rosen.", "tokens": ["Auf", "Gr\u00e4\u00b7ber", "schneit's", "wei\u00b7\u00dfe", "Ro\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch es schneit Erbsensuppe mit Speck", "tokens": ["Doch", "es", "schneit", "Erb\u00b7sen\u00b7sup\u00b7pe", "mit", "Speck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADJD", "NN", "APPR", "NN"], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "In die Taschen der Arbeitslosen.", "tokens": ["In", "die", "Ta\u00b7schen", "der", "Ar\u00b7beits\u00b7lo\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.8": {"line.1": {"text": "Es schneit dicke Flocken,", "tokens": ["Es", "schneit", "di\u00b7cke", "Flo\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJA", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Nicht warm, aber frisch gebacken.", "tokens": ["Nicht", "warm", ",", "a\u00b7ber", "frisch", "ge\u00b7ba\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$,", "ADV", "ADJD", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die setzen sich in meine Dichterlocken,", "tokens": ["Die", "set\u00b7zen", "sich", "in", "mei\u00b7ne", "Dich\u00b7ter\u00b7lo\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "In meinen Schiebernacken,", "tokens": ["In", "mei\u00b7nen", "Schie\u00b7bern\u00b7a\u00b7cken", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Auf meine Smoking-Socken.", "tokens": ["Auf", "mei\u00b7ne", "Smo\u00b7king\u00b7So\u00b7cken", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Sie machen den Polizisten", "tokens": ["Sie", "ma\u00b7chen", "den", "Po\u00b7li\u00b7zis\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Gem\u00fctlich zum Weihnachtsmann.", "tokens": ["Ge\u00b7m\u00fct\u00b7lich", "zum", "Weih\u00b7nachts\u00b7mann", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da legen die Touristen", "tokens": ["Da", "le\u00b7gen", "die", "Tou\u00b7ris\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+--++-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihre Polarausr\u00fcstung an.", "tokens": ["Ih\u00b7re", "Po\u00b7lar\u00b7aus\u00b7r\u00fcs\u00b7tung", "an", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Wir wollen uns alle zusammentun,", "tokens": ["Wir", "wol\u00b7len", "uns", "al\u00b7le", "zu\u00b7sam\u00b7men\u00b7tun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Um den Beschlu\u00df zu fassen:", "tokens": ["Um", "den", "Be\u00b7schlu\u00df", "zu", "fas\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es d\u00fcrfen alle Sachsen von nun", "tokens": ["Es", "d\u00fcr\u00b7fen", "al\u00b7le", "Sach\u00b7sen", "von", "nun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "APPR", "ADV"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "An nicht mehr ihr Land verlassen.", "tokens": ["An", "nicht", "mehr", "ihr", "Land", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PTKNEG", "ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Sie querten mit wilder Behaglichkeit", "tokens": ["Sie", "quer\u00b7ten", "mit", "wil\u00b7der", "Be\u00b7hag\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Karlmayisch gedachte Fernen", "tokens": ["Karl\u00b7may\u00b7isch", "ge\u00b7dach\u00b7te", "Fer\u00b7nen"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und blieben Sachsen. Es wird f\u00fcr sie Zeit,", "tokens": ["Und", "blie\u00b7ben", "Sach\u00b7sen", ".", "Es", "wird", "f\u00fcr", "sie", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "$.", "PPER", "VAFIN", "APPR", "PPER", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sich selbst erst mal kennenzulernen.", "tokens": ["Sich", "selbst", "erst", "mal", "ken\u00b7nen\u00b7zu\u00b7ler\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "Es schneit.", "tokens": ["Es", "schneit", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.12": {"line.1": {"text": "Wenn hundert Leute sich einig sind,", "tokens": ["Wenn", "hun\u00b7dert", "Leu\u00b7te", "sich", "ei\u00b7nig", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "NN", "PRF", "ADJD", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dann f\u00fchlen sich die als Giganten", "tokens": ["Dann", "f\u00fch\u00b7len", "sich", "die", "als", "Gi\u00b7gan\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und schwafeln vor einem vern\u00fcnftigen Kind", "tokens": ["Und", "schwa\u00b7feln", "vor", "ei\u00b7nem", "ver\u00b7n\u00fcnf\u00b7ti\u00b7gen", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wie taube verwunschene Tanten.", "tokens": ["Wie", "tau\u00b7be", "ver\u00b7wun\u00b7sche\u00b7ne", "Tan\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.13": {"line.1": {"text": "Es schneit. Wie in unserer Kinderzeit.", "tokens": ["Es", "schneit", ".", "Wie", "in", "un\u00b7se\u00b7rer", "Kin\u00b7der\u00b7zeit", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Zum Wintersport eingeladen,", "tokens": ["Zum", "Win\u00b7ter\u00b7sport", "ein\u00b7ge\u00b7la\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Gehe ich schlafen. Es schneit. Es schneit.", "tokens": ["Ge\u00b7he", "ich", "schla\u00b7fen", ".", "Es", "schneit", ".", "Es", "schneit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVINF", "$.", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Es schneit f\u00fcr den Landmann Kuhfladen.", "tokens": ["Es", "schneit", "f\u00fcr", "den", "Land\u00b7mann", "Kuh\u00b7fla\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.14": {"line.1": {"text": "Es schneit f\u00fcr die Zukunft Stra\u00dfendreck.", "tokens": ["Es", "schneit", "f\u00fcr", "die", "Zu\u00b7kunft", "Stra\u00b7\u00dfen\u00b7dreck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Auf Gr\u00e4ber schneit's wei\u00dfe Rosen.", "tokens": ["Auf", "Gr\u00e4\u00b7ber", "schneit's", "wei\u00b7\u00dfe", "Ro\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch es schneit Erbsensuppe mit Speck", "tokens": ["Doch", "es", "schneit", "Erb\u00b7sen\u00b7sup\u00b7pe", "mit", "Speck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADJD", "NN", "APPR", "NN"], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "In die Taschen der Arbeitslosen.", "tokens": ["In", "die", "Ta\u00b7schen", "der", "Ar\u00b7beits\u00b7lo\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}}}}