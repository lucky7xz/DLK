{"textgrid.poem.48366": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "K\u00f6nig Johann und der Bischof von Canterbury", "genre": "verse", "period": "N.A.", "pub_year": 1855, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Nun heb' einen lustigen Schwank ich an,", "tokens": ["Nun", "heb'", "ei\u00b7nen", "lus\u00b7ti\u00b7gen", "Schwank", "ich", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Ein M\u00e4rchen von unsrem K\u00f6nig Johann,", "tokens": ["Ein", "M\u00e4r\u00b7chen", "von", "uns\u00b7rem", "K\u00f6\u00b7nig", "Jo\u00b7hann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "NE", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mutwillig hat er im Lande regiert,", "tokens": ["Mut\u00b7wil\u00b7lig", "hat", "er", "im", "Lan\u00b7de", "re\u00b7giert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Ob's recht war, ob nicht \u2013 hat ihn wenig geschiert.", "tokens": ["Ob's", "recht", "war", ",", "ob", "nicht", "\u2013", "hat", "ihn", "we\u00b7nig", "ge\u00b7schiert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "VAFIN", "$,", "KOUS", "PTKNEG", "$(", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}}, "stanza.2": {"line.1": {"text": "Und erz\u00e4hlen auch will ich zur Stelle hie", "tokens": ["Und", "er\u00b7z\u00e4h\u00b7len", "auch", "will", "ich", "zur", "Stel\u00b7le", "hie"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "VMFIN", "PPER", "APPRART", "NN", "ADV"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Von dem hochweisen Bischof von Canterbury \u2013", "tokens": ["Von", "dem", "hoch\u00b7wei\u00b7sen", "Bi\u00b7schof", "von", "Can\u00b7ter\u00b7bu\u00b7ry", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "NE", "$("], "meter": "--++-+--+--+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Die K\u00fcche voll Wildpret, der Keller voll Wein", "tokens": ["Die", "K\u00fc\u00b7che", "voll", "Wild\u00b7pret", ",", "der", "Kel\u00b7ler", "voll", "Wein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "NN", "$,", "ART", "NN", "ADJD", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und Fr\u00fcchte von London, so mu\u00dft' es sein.", "tokens": ["Und", "Fr\u00fcch\u00b7te", "von", "Lon\u00b7don", ",", "so", "mu\u00dft'", "es", "sein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$,", "ADV", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Und hundert Diener tagein, tagaus,", "tokens": ["Und", "hun\u00b7dert", "Die\u00b7ner", "ta\u00b7gein", ",", "ta\u00b7gaus", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "CARD", "NN", "PTKVZ", "$,", "ADV", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die warteten seiner in Hof und Haus,", "tokens": ["Die", "war\u00b7te\u00b7ten", "sei\u00b7ner", "in", "Hof", "und", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Sie trugen Kleider von Sammet schwer", "tokens": ["Sie", "tru\u00b7gen", "Klei\u00b7der", "von", "Sam\u00b7met", "schwer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "APPR", "NE", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und goldene Ketten dar\u00fcber her.", "tokens": ["Und", "gol\u00b7de\u00b7ne", "Ket\u00b7ten", "da\u00b7r\u00fc\u00b7ber", "her", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "PAV", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.4": {"line.1": {"text": "Das h\u00f6rte der K\u00f6nig. \u00bbHe, Bischof, sprich,", "tokens": ["Das", "h\u00f6r\u00b7te", "der", "K\u00f6\u00b7nig", ".", "\u00bb", "He", ",", "Bi\u00b7schof", ",", "sprich", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$.", "$(", "ITJ", "$,", "NN", "$,", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Du h\u00e4ltst ja gl\u00e4nzender Haus als ich,", "tokens": ["Du", "h\u00e4ltst", "ja", "gl\u00e4n\u00b7zen\u00b7der", "Haus", "als", "ich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJA", "NN", "KOUS", "PPER", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ich wett', du betr\u00fcgst mich um Steuer und Zins", "tokens": ["Ich", "wett'", ",", "du", "be\u00b7tr\u00fcgst", "mich", "um", "Steu\u00b7er", "und", "Zins"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PRF", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und beraubst meinen Seckel seines Gewinns.\u00ab", "tokens": ["Und", "be\u00b7raubst", "mei\u00b7nen", "Se\u00b7ckel", "sei\u00b7nes", "Ge\u00b7winns", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "PPOSAT", "NN", "$.", "$("], "meter": "+--+-+-+--+", "measure": "iambic.penta.invert"}}, "stanza.5": {"line.1": {"text": "\u00bbherr\u00ab, seufzte der Bischof, \u00bbvor Gott ich bekenn',", "tokens": ["\u00bb", "herr", "\u00ab", ",", "seufz\u00b7te", "der", "Bi\u00b7schof", ",", "\u00bb", "vor", "Gott", "ich", "be\u00b7kenn'", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "NN", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "APPR", "NN", "PPER", "PTKVZ", "$,"], "meter": "+---+-++--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Ich hab' nur vertafelt, was mein ich nenn',", "tokens": ["Ich", "hab'", "nur", "ver\u00b7ta\u00b7felt", ",", "was", "mein", "ich", "nenn'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "PRELS", "PPOSAT", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und Ihr k\u00f6nnet und werdet mir kr\u00fcmmen kein Haar,", "tokens": ["Und", "Ihr", "k\u00f6n\u00b7net", "und", "wer\u00b7det", "mir", "kr\u00fcm\u00b7men", "kein", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "KON", "VAFIN", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Weil ich Wein getrunken, der meine war.\u00ab", "tokens": ["Weil", "ich", "Wein", "ge\u00b7trun\u00b7ken", ",", "der", "mei\u00b7ne", "war", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "NN", "VVPP", "$,", "PRELS", "PPOSAT", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbdoch, Bischof, doch, dein Verbrechen wiegt schwer,", "tokens": ["\u00bb", "doch", ",", "Bi\u00b7schof", ",", "doch", ",", "dein", "Ver\u00b7bre\u00b7chen", "wiegt", "schwer", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NN", "$,", "ADV", "$,", "PPOSAT", "NN", "VVFIN", "ADJD", "$,"], "meter": "+-+-+-+-++", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Du stirbst, es kann dich nichts retten mehr,", "tokens": ["Du", "stirbst", ",", "es", "kann", "dich", "nichts", "ret\u00b7ten", "mehr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "PRF", "PIS", "VVINF", "ADV", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es sei denn, du f\u00e4ndest die Antwort schnell", "tokens": ["Es", "sei", "denn", ",", "du", "f\u00e4n\u00b7dest", "die", "Ant\u00b7wort", "schnell"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "$,", "PPER", "VVFIN", "ART", "NN", "ADJD"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Auf drei winzige Fragen, die ich dir stell'.", "tokens": ["Auf", "drei", "win\u00b7zi\u00b7ge", "Fra\u00b7gen", ",", "die", "ich", "dir", "stell'", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "ADJA", "NN", "$,", "PRELS", "PPER", "PPER", "VVFIN", "$."], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.7": {"line.1": {"text": "Zum ersten: wenn ich auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ":", "wenn", "ich", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$.", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat halte mit meinen Grafen und Herrn,", "tokens": ["Rat", "hal\u00b7te", "mit", "mei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie viel ich dann wert bin, w\u00fc\u00dft' ich gern?", "tokens": ["Wie", "viel", "ich", "dann", "wert", "bin", ",", "w\u00fc\u00dft'", "ich", "gern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.8": {"line.1": {"text": "Und zum zweiten sollst du mir sagen dann,", "tokens": ["Und", "zum", "zwei\u00b7ten", "sollst", "du", "mir", "sa\u00b7gen", "dann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "PPER", "VVFIN", "ADV", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie rasch wohl die Welt ich umreiten kann?", "tokens": ["Wie", "rasch", "wohl", "die", "Welt", "ich", "um\u00b7rei\u00b7ten", "kann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ADV", "ART", "NN", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und zum dritten will ich wissen geschwind,", "tokens": ["Und", "zum", "drit\u00b7ten", "will", "ich", "wis\u00b7sen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "VVFIN", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was zur Stelle meine Gedanken sind?\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "mei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "\u00bbherr, Eure Fragen sind viel zu schwer,", "tokens": ["\u00bb", "herr", ",", "Eu\u00b7re", "Fra\u00b7gen", "sind", "viel", "zu", "schwer", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Da find' ich nicht L\u00f6sung flugs hinterher,", "tokens": ["Da", "find'", "ich", "nicht", "L\u00f6\u00b7sung", "flugs", "hin\u00b7ter\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "G\u00f6nnt mir drei Wochen vom heutigen Tag,", "tokens": ["G\u00f6nnt", "mir", "drei", "Wo\u00b7chen", "vom", "heu\u00b7ti\u00b7gen", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "CARD", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+---+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Da\u00df ich Frag' und Antwort ergr\u00fcnden mag.\u00ab", "tokens": ["Da\u00df", "ich", "Frag'", "und", "Ant\u00b7wort", "er\u00b7gr\u00fcn\u00b7den", "mag.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation", "punct"], "pos": ["KOUS", "PPER", "NN", "KON", "NN", "VVINF", "VMFIN", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.10": {"line.1": {"text": "\u00bbwohlan, es sei! doch nutze die Frist,", "tokens": ["\u00bb", "wo\u00b7hlan", ",", "es", "sei", "!", "doch", "nut\u00b7ze", "die", "Frist", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "PPER", "VAFIN", "$.", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "So lieb dir dein Land und dein Leben ist,", "tokens": ["So", "lieb", "dir", "dein", "Land", "und", "dein", "Le\u00b7ben", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Denn r\u00e4tst du falsch oder bist du nicht hier,", "tokens": ["Denn", "r\u00e4tst", "du", "falsch", "o\u00b7der", "bist", "du", "nicht", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "KON", "VAFIN", "PPER", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sind dein Land und dein Leben verfallen mir.\u00ab", "tokens": ["Sind", "dein", "Land", "und", "dein", "Le\u00b7ben", "ver\u00b7fal\u00b7len", "mir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "VVFIN", "PPER", "$.", "$("], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.11": {"line.1": {"text": "Der Bischof h\u00f6rt' es in tr\u00fcbem Sinn,", "tokens": ["Der", "Bi\u00b7schof", "h\u00f6rt'", "es", "in", "tr\u00fc\u00b7bem", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Gen Oxford und Cambridge ritt er hin,", "tokens": ["Gen", "Ox\u00b7ford", "und", "Cam\u00b7brid\u00b7ge", "ritt", "er", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "KON", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Da war kein Doktor, den er nicht frug,", "tokens": ["Da", "war", "kein", "Dok\u00b7tor", ",", "den", "er", "nicht", "frug", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "$,", "PRELS", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doch die Klugen waren nicht klug genug.", "tokens": ["Doch", "die", "Klu\u00b7gen", "wa\u00b7ren", "nicht", "klug", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PTKNEG", "ADJD", "ADV", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "So ritt er denn heimw\u00e4rts, das Kinn auf der Brust,", "tokens": ["So", "ritt", "er", "denn", "heim\u00b7w\u00e4rts", ",", "das", "Kinn", "auf", "der", "Brust", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Da kam sein Sch\u00e4fer des Weges just,", "tokens": ["Da", "kam", "sein", "Sch\u00e4\u00b7fer", "des", "We\u00b7ges", "just", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der rief ihm zu: \u00bbWillkommen zu Haus!", "tokens": ["Der", "rief", "ihm", "zu", ":", "\u00bb", "Will\u00b7kom\u00b7men", "zu", "Haus", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKVZ", "$.", "$(", "NN", "APPR", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was bringt Ihr? Wie sieht es in London aus?\u00ab", "tokens": ["Was", "bringt", "Ihr", "?", "Wie", "sieht", "es", "in", "Lon\u00b7don", "aus", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$.", "PWAV", "VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.13": {"line.1": {"text": "\u00bbschlecht\u00ab, seufzte der Bischof, \u00bbdrei Tage nach hier", "tokens": ["\u00bb", "schlecht", "\u00ab", ",", "seufz\u00b7te", "der", "Bi\u00b7schof", ",", "\u00bb", "drei", "Ta\u00b7ge", "nach", "hier"], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "CARD", "NN", "APPR", "ADV"], "meter": "-+--+-++--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "F\u00e4llt mein armer Kopf vor die F\u00fc\u00dfe mir,", "tokens": ["F\u00e4llt", "mein", "ar\u00b7mer", "Kopf", "vor", "die", "F\u00fc\u00b7\u00dfe", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "NN", "APPR", "ART", "NN", "PPER", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Es sei denn, da\u00df er auf Antwort verf\u00e4llt", "tokens": ["Es", "sei", "denn", ",", "da\u00df", "er", "auf", "Ant\u00b7wort", "ver\u00b7f\u00e4llt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auf drei Fragen, die mir der K\u00f6nig gestellt.", "tokens": ["Auf", "drei", "Fra\u00b7gen", ",", "die", "mir", "der", "K\u00f6\u00b7nig", "ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$,", "PRELS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "--+-++-+--+", "measure": "iambic.penta.chol"}}, "stanza.14": {"line.1": {"text": "Zum ersten, wenn er auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ",", "wenn", "er", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$,", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat h\u00e4lt mit seinen Grafen und Herrn,", "tokens": ["Rat", "h\u00e4lt", "mit", "sei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wieviel er dann wert ist, w\u00fc\u00dft' er gern.", "tokens": ["Wie\u00b7viel", "er", "dann", "wert", "ist", ",", "w\u00fc\u00dft'", "er", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.15": {"line.1": {"text": "Und zum zweiten soll ich ihm sagen dann,", "tokens": ["Und", "zum", "zwei\u00b7ten", "soll", "ich", "ihm", "sa\u00b7gen", "dann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "PPER", "VVFIN", "ADV", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie rasch er die Welt wohl umreiten kann;", "tokens": ["Wie", "rasch", "er", "die", "Welt", "wohl", "um\u00b7rei\u00b7ten", "kann", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "ART", "NN", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und zum dritten will er wissen geschwind,", "tokens": ["Und", "zum", "drit\u00b7ten", "will", "er", "wis\u00b7sen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "VVFIN", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was zur Stelle seine Gedanken sind.\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "sei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.16": {"line.1": {"text": "Da lachte der Sch\u00e4fer: \u00bbHerr, denket daran,", "tokens": ["Da", "lach\u00b7te", "der", "Sch\u00e4\u00b7fer", ":", "\u00bb", "Herr", ",", "den\u00b7ket", "da\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "NN", "$,", "VVFIN", "PAV", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da\u00df ein Narr einen Weisen lehren kann;", "tokens": ["Da\u00df", "ein", "Narr", "ei\u00b7nen", "Wei\u00b7sen", "leh\u00b7ren", "kann", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Gebt mir Euer Ro\u00df, Euren Stab, Euer Kleid,", "tokens": ["Gebt", "mir", "Eu\u00b7er", "Ro\u00df", ",", "Eu\u00b7ren", "Stab", ",", "Eu\u00b7er", "Kleid", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "--+-++-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Und ich fecht' Euch aus Euren ganzen Streit.", "tokens": ["Und", "ich", "fecht'", "Euch", "aus", "Eu\u00b7ren", "gan\u00b7zen", "Streit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Sorgt nicht; in Kentshire wei\u00df jedes Kind,", "tokens": ["Sorgt", "nicht", ";", "in", "Kents\u00b7hi\u00b7re", "wei\u00df", "je\u00b7des", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$.", "APPR", "NE", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+---+-+", "measure": "zehnsilber"}, "line.2": {"text": "Da\u00df wir zwei wie von einem Vater sind,", "tokens": ["Da\u00df", "wir", "zwei", "wie", "von", "ei\u00b7nem", "Va\u00b7ter", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "KOKOM", "APPR", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und trag' ich nur erst Euer pr\u00e4chtig Gewand,", "tokens": ["Und", "trag'", "ich", "nur", "erst", "Eu\u00b7er", "pr\u00e4ch\u00b7tig", "Ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "PPOSAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Unterscheidet uns keiner im ganzen Land.\u00ab", "tokens": ["Un\u00b7ter\u00b7schei\u00b7det", "uns", "kei\u00b7ner", "im", "gan\u00b7zen", "Land", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPRART", "ADJA", "NN", "$.", "$("], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "Da beschwor ihn der Bischof: \u00bbNimm Chorrock und Stab,", "tokens": ["Da", "be\u00b7schwor", "ihn", "der", "Bi\u00b7schof", ":", "\u00bb", "Nimm", "Chor\u00b7rock", "und", "Stab", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$.", "$(", "NE", "NE", "KON", "NN", "$,"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Nimm Diener und L\u00e4ufer, so viel ich hab',", "tokens": ["Nimm", "Die\u00b7ner", "und", "L\u00e4u\u00b7fer", ",", "so", "viel", "ich", "hab'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "$,", "ADV", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nimm Mitra, Kapuze, nimm was dir gef\u00e4llt,", "tokens": ["Nimm", "Mit\u00b7ra", ",", "Ka\u00b7pu\u00b7ze", ",", "nimm", "was", "dir", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$,", "VVIMP", "PWS", "PPER", "VVPP", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Nur l\u00f6se die Fragen, die er gestellt.\u00ab", "tokens": ["Nur", "l\u00f6\u00b7se", "die", "Fra\u00b7gen", ",", "die", "er", "ge\u00b7stellt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "PRELS", "PPER", "VVPP", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.19": {"line.1": {"text": "\u00bbwillkommen, Freund Bischof\u00ab, rief K\u00f6nig Johann,", "tokens": ["\u00bb", "will\u00b7kom\u00b7men", ",", "Freund", "Bi\u00b7schof", "\u00ab", ",", "rief", "K\u00f6\u00b7nig", "Jo\u00b7hann", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "NN", "NE", "$(", "$,", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "\u00bbdu h\u00e4ltst deine Zeit, das ist wohlgetan,", "tokens": ["\u00bb", "du", "h\u00e4ltst", "dei\u00b7ne", "Zeit", ",", "das", "ist", "wohl\u00b7ge\u00b7tan", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPOSAT", "NN", "$,", "PDS", "VAFIN", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und h\u00e4lt nur dein Witz auch so p\u00fcnktlich Stand,", "tokens": ["Und", "h\u00e4lt", "nur", "dein", "Witz", "auch", "so", "p\u00fcnkt\u00b7lich", "Stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPOSAT", "NN", "ADV", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Belehn' ich aufs neu dich mit Leuten und Land.", "tokens": ["Be\u00b7lehn'", "ich", "aufs", "neu", "dich", "mit", "Leu\u00b7ten", "und", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADJD", "PRF", "APPR", "NN", "KON", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.20": {"line.1": {"text": "Zum ersten: Wenn ich auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ":", "Wenn", "ich", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$.", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat halte mit meinen Grafen und Herrn,", "tokens": ["Rat", "hal\u00b7te", "mit", "mei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie viel ich dann wert bin, w\u00fc\u00dft' ich gern.\u00ab", "tokens": ["Wie", "viel", "ich", "dann", "wert", "bin", ",", "w\u00fc\u00dft'", "ich", "gern", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PIS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.21": {"line.1": {"text": "\u00bbunser Heiland wurde, so wahr ich getauft,", "tokens": ["\u00bb", "un\u00b7ser", "Hei\u00b7land", "wur\u00b7de", ",", "so", "wahr", "ich", "ge\u00b7tauft", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "VAFIN", "$,", "ADV", "ADJD", "PPER", "VVPP", "$,"], "meter": "+-+-+-++--+", "measure": "iambic.hexa.chol"}, "line.2": {"text": "Um drei\u00dfig Silberlinge verkauft,", "tokens": ["Um", "drei\u00b7\u00dfig", "Sil\u00b7ber\u00b7lin\u00b7ge", "ver\u00b7kauft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Drum neunundzwanzig sch\u00e4tz' ich Euch ein,", "tokens": ["Drum", "neun\u00b7und\u00b7zwan\u00b7zig", "sch\u00e4tz'", "ich", "Euch", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJD", "VVFIN", "PPER", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Um ", "tokens": ["Um"], "token_info": ["word"], "pos": ["KOUI"], "meter": "+", "measure": "single.up"}}, "stanza.22": {"line.1": {"text": "Da lachte der K\u00f6nig und schwur bei Sankt Velt:", "tokens": ["Da", "lach\u00b7te", "der", "K\u00f6\u00b7nig", "und", "schwur", "bei", "Sankt", "Velt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "APPR", "VVFIN", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "\u00bbich hab' nicht gedacht, da\u00df so wenig ich gelt'!", "tokens": ["\u00bb", "ich", "hab'", "nicht", "ge\u00b7dacht", ",", "da\u00df", "so", "we\u00b7nig", "ich", "gelt'", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKNEG", "VVPP", "$,", "KOUS", "ADV", "PIS", "PPER", "VVPP", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Nun aber zum zweiten sage mir an,", "tokens": ["Nun", "a\u00b7ber", "zum", "zwei\u00b7ten", "sa\u00b7ge", "mir", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "ADJA", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie rasch wohl die Welt ich umreiten kann?\u00ab", "tokens": ["Wie", "rasch", "wohl", "die", "Welt", "ich", "um\u00b7rei\u00b7ten", "kann", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "ADJD", "ADV", "ART", "NN", "PPER", "VVINF", "VMFIN", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.23": {"line.1": {"text": "\u00bbreit' aus mit der Sonn', immer neben ihr fort,", "tokens": ["\u00bb", "reit'", "aus", "mit", "der", "Sonn'", ",", "im\u00b7mer", "ne\u00b7ben", "ihr", "fort", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "APPR", "APPR", "ART", "NN", "$,", "ADV", "APPR", "PPER", "PTKVZ", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Bis du andren Tages am alten Ort,", "tokens": ["Bis", "du", "an\u00b7dren", "Ta\u00b7ges", "am", "al\u00b7ten", "Ort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJA", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So hast du die Reise in Tag und Nacht", "tokens": ["So", "hast", "du", "die", "Rei\u00b7se", "in", "Tag", "und", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Oder vierundzwanzig Stunden gemacht.\u00ab", "tokens": ["O\u00b7der", "vie\u00b7rund\u00b7zwan\u00b7zig", "Stun\u00b7den", "ge\u00b7macht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "CARD", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.24": {"line.1": {"text": "Da lachte der K\u00f6nig und schwur bei Sankt Veit:", "tokens": ["Da", "lach\u00b7te", "der", "K\u00f6\u00b7nig", "und", "schwur", "bei", "Sankt", "Veit", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "APPR", "VVFIN", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "\u00bbich hab' nicht gedacht, da\u00df so rasch ich reit'!", "tokens": ["\u00bb", "ich", "hab'", "nicht", "ge\u00b7dacht", ",", "da\u00df", "so", "rasch", "ich", "reit'", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKNEG", "VVPP", "$,", "KOUS", "ADV", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nun aber sollst du mir sagen geschwind,", "tokens": ["Nun", "a\u00b7ber", "sollst", "du", "mir", "sa\u00b7gen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "PPER", "VVFIN", "ADJD", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Was zur Stelle meine Gedanken sind.\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "mei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.25": {"line.1": {"text": "Da beugte der Sch\u00e4fer schnell sein Knie:", "tokens": ["Da", "beug\u00b7te", "der", "Sch\u00e4\u00b7fer", "schnell", "sein", "Knie", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbihr denkt, ich sei Bischof von Canterbury,", "tokens": ["\u00bb", "ihr", "denkt", ",", "ich", "sei", "Bi\u00b7schof", "von", "Can\u00b7ter\u00b7bu\u00b7ry", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "NN", "APPR", "NE", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Und bitt' um Gnade f\u00fcr ihn und f\u00fcr mich.\u00ab", "tokens": ["Und", "bitt'", "um", "Gna\u00b7de", "f\u00fcr", "ihn", "und", "f\u00fcr", "mich", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "APPR", "PPER", "KON", "APPR", "PPER", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.26": {"line.1": {"text": "Da schwur der K\u00f6nig und lachte hell:", "tokens": ["Da", "schwur", "der", "K\u00f6\u00b7nig", "und", "lach\u00b7te", "hell", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbdu sollst Bischof sein an seiner Stell'.\u00ab", "tokens": ["\u00bb", "du", "sollst", "Bi\u00b7schof", "sein", "an", "sei\u00b7ner", "Stell'", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VMFIN", "NN", "PPOSAT", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "-+----+-+", "measure": "dactylic.init"}, "line.3": {"text": "Der Sch\u00e4fer seufzte: \u00bb's geht halt nit mehr,", "tokens": ["Der", "Sch\u00e4\u00b7fer", "seufz\u00b7te", ":", "\u00bb", "'s", "geht", "halt", "nit", "mehr", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "PPER", "VVFIN", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wo n\u00e4hm' ich das Lesen und Schreiben her?\u00ab", "tokens": ["Wo", "n\u00e4hm'", "ich", "das", "Le\u00b7sen", "und", "Schrei\u00b7ben", "her", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ART", "NN", "KON", "NN", "PTKVZ", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.27": {"line.1": {"text": "\u00bbwohlan denn, so nimm zu Dank und Lohn", "tokens": ["\u00bb", "wo\u00b7hlan", "denn", ",", "so", "nimm", "zu", "Dank", "und", "Lohn"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "ADV", "$,", "ADV", "VVIMP", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Vier Nobel die Woche von mir, mein Sohn,", "tokens": ["Vier", "No\u00b7bel", "die", "Wo\u00b7che", "von", "mir", ",", "mein", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "ART", "NN", "APPR", "PPER", "$,", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und reitst du bei deinem Bischof heran,", "tokens": ["Und", "reitst", "du", "bei", "dei\u00b7nem", "Bi\u00b7schof", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "So bring ihm Verzeihung vom K\u00f6nig Johann.\u00ab", "tokens": ["So", "bring", "ihm", "Ver\u00b7zei\u00b7hung", "vom", "K\u00f6\u00b7nig", "Jo\u00b7hann", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPRART", "NN", "NE", "$.", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.28": {"line.1": {"text": "Nun heb' einen lustigen Schwank ich an,", "tokens": ["Nun", "heb'", "ei\u00b7nen", "lus\u00b7ti\u00b7gen", "Schwank", "ich", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Ein M\u00e4rchen von unsrem K\u00f6nig Johann,", "tokens": ["Ein", "M\u00e4r\u00b7chen", "von", "uns\u00b7rem", "K\u00f6\u00b7nig", "Jo\u00b7hann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "NE", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mutwillig hat er im Lande regiert,", "tokens": ["Mut\u00b7wil\u00b7lig", "hat", "er", "im", "Lan\u00b7de", "re\u00b7giert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Ob's recht war, ob nicht \u2013 hat ihn wenig geschiert.", "tokens": ["Ob's", "recht", "war", ",", "ob", "nicht", "\u2013", "hat", "ihn", "we\u00b7nig", "ge\u00b7schiert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "VAFIN", "$,", "KOUS", "PTKNEG", "$(", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}}, "stanza.29": {"line.1": {"text": "Und erz\u00e4hlen auch will ich zur Stelle hie", "tokens": ["Und", "er\u00b7z\u00e4h\u00b7len", "auch", "will", "ich", "zur", "Stel\u00b7le", "hie"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "VMFIN", "PPER", "APPRART", "NN", "ADV"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Von dem hochweisen Bischof von Canterbury \u2013", "tokens": ["Von", "dem", "hoch\u00b7wei\u00b7sen", "Bi\u00b7schof", "von", "Can\u00b7ter\u00b7bu\u00b7ry", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "NE", "$("], "meter": "--++-+--+--+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Die K\u00fcche voll Wildpret, der Keller voll Wein", "tokens": ["Die", "K\u00fc\u00b7che", "voll", "Wild\u00b7pret", ",", "der", "Kel\u00b7ler", "voll", "Wein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "NN", "$,", "ART", "NN", "ADJD", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und Fr\u00fcchte von London, so mu\u00dft' es sein.", "tokens": ["Und", "Fr\u00fcch\u00b7te", "von", "Lon\u00b7don", ",", "so", "mu\u00dft'", "es", "sein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$,", "ADV", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.30": {"line.1": {"text": "Und hundert Diener tagein, tagaus,", "tokens": ["Und", "hun\u00b7dert", "Die\u00b7ner", "ta\u00b7gein", ",", "ta\u00b7gaus", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "CARD", "NN", "PTKVZ", "$,", "ADV", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die warteten seiner in Hof und Haus,", "tokens": ["Die", "war\u00b7te\u00b7ten", "sei\u00b7ner", "in", "Hof", "und", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Sie trugen Kleider von Sammet schwer", "tokens": ["Sie", "tru\u00b7gen", "Klei\u00b7der", "von", "Sam\u00b7met", "schwer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "APPR", "NE", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und goldene Ketten dar\u00fcber her.", "tokens": ["Und", "gol\u00b7de\u00b7ne", "Ket\u00b7ten", "da\u00b7r\u00fc\u00b7ber", "her", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "PAV", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.31": {"line.1": {"text": "Das h\u00f6rte der K\u00f6nig. \u00bbHe, Bischof, sprich,", "tokens": ["Das", "h\u00f6r\u00b7te", "der", "K\u00f6\u00b7nig", ".", "\u00bb", "He", ",", "Bi\u00b7schof", ",", "sprich", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$.", "$(", "ITJ", "$,", "NN", "$,", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Du h\u00e4ltst ja gl\u00e4nzender Haus als ich,", "tokens": ["Du", "h\u00e4ltst", "ja", "gl\u00e4n\u00b7zen\u00b7der", "Haus", "als", "ich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJA", "NN", "KOUS", "PPER", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ich wett', du betr\u00fcgst mich um Steuer und Zins", "tokens": ["Ich", "wett'", ",", "du", "be\u00b7tr\u00fcgst", "mich", "um", "Steu\u00b7er", "und", "Zins"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PRF", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und beraubst meinen Seckel seines Gewinns.\u00ab", "tokens": ["Und", "be\u00b7raubst", "mei\u00b7nen", "Se\u00b7ckel", "sei\u00b7nes", "Ge\u00b7winns", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "PPOSAT", "NN", "$.", "$("], "meter": "+--+-+-+--+", "measure": "iambic.penta.invert"}}, "stanza.32": {"line.1": {"text": "\u00bbherr\u00ab, seufzte der Bischof, \u00bbvor Gott ich bekenn',", "tokens": ["\u00bb", "herr", "\u00ab", ",", "seufz\u00b7te", "der", "Bi\u00b7schof", ",", "\u00bb", "vor", "Gott", "ich", "be\u00b7kenn'", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "NN", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "APPR", "NN", "PPER", "PTKVZ", "$,"], "meter": "+---+-++--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Ich hab' nur vertafelt, was mein ich nenn',", "tokens": ["Ich", "hab'", "nur", "ver\u00b7ta\u00b7felt", ",", "was", "mein", "ich", "nenn'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "PRELS", "PPOSAT", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und Ihr k\u00f6nnet und werdet mir kr\u00fcmmen kein Haar,", "tokens": ["Und", "Ihr", "k\u00f6n\u00b7net", "und", "wer\u00b7det", "mir", "kr\u00fcm\u00b7men", "kein", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "KON", "VAFIN", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Weil ich Wein getrunken, der meine war.\u00ab", "tokens": ["Weil", "ich", "Wein", "ge\u00b7trun\u00b7ken", ",", "der", "mei\u00b7ne", "war", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "NN", "VVPP", "$,", "PRELS", "PPOSAT", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.33": {"line.1": {"text": "\u00bbdoch, Bischof, doch, dein Verbrechen wiegt schwer,", "tokens": ["\u00bb", "doch", ",", "Bi\u00b7schof", ",", "doch", ",", "dein", "Ver\u00b7bre\u00b7chen", "wiegt", "schwer", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NN", "$,", "ADV", "$,", "PPOSAT", "NN", "VVFIN", "ADJD", "$,"], "meter": "+-+-+-+-++", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Du stirbst, es kann dich nichts retten mehr,", "tokens": ["Du", "stirbst", ",", "es", "kann", "dich", "nichts", "ret\u00b7ten", "mehr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "PRF", "PIS", "VVINF", "ADV", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es sei denn, du f\u00e4ndest die Antwort schnell", "tokens": ["Es", "sei", "denn", ",", "du", "f\u00e4n\u00b7dest", "die", "Ant\u00b7wort", "schnell"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "$,", "PPER", "VVFIN", "ART", "NN", "ADJD"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Auf drei winzige Fragen, die ich dir stell'.", "tokens": ["Auf", "drei", "win\u00b7zi\u00b7ge", "Fra\u00b7gen", ",", "die", "ich", "dir", "stell'", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "ADJA", "NN", "$,", "PRELS", "PPER", "PPER", "VVFIN", "$."], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.34": {"line.1": {"text": "Zum ersten: wenn ich auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ":", "wenn", "ich", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$.", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat halte mit meinen Grafen und Herrn,", "tokens": ["Rat", "hal\u00b7te", "mit", "mei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie viel ich dann wert bin, w\u00fc\u00dft' ich gern?", "tokens": ["Wie", "viel", "ich", "dann", "wert", "bin", ",", "w\u00fc\u00dft'", "ich", "gern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.35": {"line.1": {"text": "Und zum zweiten sollst du mir sagen dann,", "tokens": ["Und", "zum", "zwei\u00b7ten", "sollst", "du", "mir", "sa\u00b7gen", "dann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "PPER", "VVFIN", "ADV", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie rasch wohl die Welt ich umreiten kann?", "tokens": ["Wie", "rasch", "wohl", "die", "Welt", "ich", "um\u00b7rei\u00b7ten", "kann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ADV", "ART", "NN", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und zum dritten will ich wissen geschwind,", "tokens": ["Und", "zum", "drit\u00b7ten", "will", "ich", "wis\u00b7sen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "VVFIN", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was zur Stelle meine Gedanken sind?\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "mei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.36": {"line.1": {"text": "\u00bbherr, Eure Fragen sind viel zu schwer,", "tokens": ["\u00bb", "herr", ",", "Eu\u00b7re", "Fra\u00b7gen", "sind", "viel", "zu", "schwer", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Da find' ich nicht L\u00f6sung flugs hinterher,", "tokens": ["Da", "find'", "ich", "nicht", "L\u00f6\u00b7sung", "flugs", "hin\u00b7ter\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "G\u00f6nnt mir drei Wochen vom heutigen Tag,", "tokens": ["G\u00f6nnt", "mir", "drei", "Wo\u00b7chen", "vom", "heu\u00b7ti\u00b7gen", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "CARD", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+---+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Da\u00df ich Frag' und Antwort ergr\u00fcnden mag.\u00ab", "tokens": ["Da\u00df", "ich", "Frag'", "und", "Ant\u00b7wort", "er\u00b7gr\u00fcn\u00b7den", "mag.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation", "punct"], "pos": ["KOUS", "PPER", "NN", "KON", "NN", "VVINF", "VMFIN", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.37": {"line.1": {"text": "\u00bbwohlan, es sei! doch nutze die Frist,", "tokens": ["\u00bb", "wo\u00b7hlan", ",", "es", "sei", "!", "doch", "nut\u00b7ze", "die", "Frist", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "PPER", "VAFIN", "$.", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "So lieb dir dein Land und dein Leben ist,", "tokens": ["So", "lieb", "dir", "dein", "Land", "und", "dein", "Le\u00b7ben", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Denn r\u00e4tst du falsch oder bist du nicht hier,", "tokens": ["Denn", "r\u00e4tst", "du", "falsch", "o\u00b7der", "bist", "du", "nicht", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "KON", "VAFIN", "PPER", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sind dein Land und dein Leben verfallen mir.\u00ab", "tokens": ["Sind", "dein", "Land", "und", "dein", "Le\u00b7ben", "ver\u00b7fal\u00b7len", "mir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "VVFIN", "PPER", "$.", "$("], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.38": {"line.1": {"text": "Der Bischof h\u00f6rt' es in tr\u00fcbem Sinn,", "tokens": ["Der", "Bi\u00b7schof", "h\u00f6rt'", "es", "in", "tr\u00fc\u00b7bem", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Gen Oxford und Cambridge ritt er hin,", "tokens": ["Gen", "Ox\u00b7ford", "und", "Cam\u00b7brid\u00b7ge", "ritt", "er", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "KON", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Da war kein Doktor, den er nicht frug,", "tokens": ["Da", "war", "kein", "Dok\u00b7tor", ",", "den", "er", "nicht", "frug", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "$,", "PRELS", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doch die Klugen waren nicht klug genug.", "tokens": ["Doch", "die", "Klu\u00b7gen", "wa\u00b7ren", "nicht", "klug", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PTKNEG", "ADJD", "ADV", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.39": {"line.1": {"text": "So ritt er denn heimw\u00e4rts, das Kinn auf der Brust,", "tokens": ["So", "ritt", "er", "denn", "heim\u00b7w\u00e4rts", ",", "das", "Kinn", "auf", "der", "Brust", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Da kam sein Sch\u00e4fer des Weges just,", "tokens": ["Da", "kam", "sein", "Sch\u00e4\u00b7fer", "des", "We\u00b7ges", "just", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der rief ihm zu: \u00bbWillkommen zu Haus!", "tokens": ["Der", "rief", "ihm", "zu", ":", "\u00bb", "Will\u00b7kom\u00b7men", "zu", "Haus", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKVZ", "$.", "$(", "NN", "APPR", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was bringt Ihr? Wie sieht es in London aus?\u00ab", "tokens": ["Was", "bringt", "Ihr", "?", "Wie", "sieht", "es", "in", "Lon\u00b7don", "aus", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$.", "PWAV", "VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.40": {"line.1": {"text": "\u00bbschlecht\u00ab, seufzte der Bischof, \u00bbdrei Tage nach hier", "tokens": ["\u00bb", "schlecht", "\u00ab", ",", "seufz\u00b7te", "der", "Bi\u00b7schof", ",", "\u00bb", "drei", "Ta\u00b7ge", "nach", "hier"], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "CARD", "NN", "APPR", "ADV"], "meter": "-+--+-++--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "F\u00e4llt mein armer Kopf vor die F\u00fc\u00dfe mir,", "tokens": ["F\u00e4llt", "mein", "ar\u00b7mer", "Kopf", "vor", "die", "F\u00fc\u00b7\u00dfe", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "NN", "APPR", "ART", "NN", "PPER", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Es sei denn, da\u00df er auf Antwort verf\u00e4llt", "tokens": ["Es", "sei", "denn", ",", "da\u00df", "er", "auf", "Ant\u00b7wort", "ver\u00b7f\u00e4llt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auf drei Fragen, die mir der K\u00f6nig gestellt.", "tokens": ["Auf", "drei", "Fra\u00b7gen", ",", "die", "mir", "der", "K\u00f6\u00b7nig", "ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$,", "PRELS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "--+-++-+--+", "measure": "iambic.penta.chol"}}, "stanza.41": {"line.1": {"text": "Zum ersten, wenn er auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ",", "wenn", "er", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$,", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat h\u00e4lt mit seinen Grafen und Herrn,", "tokens": ["Rat", "h\u00e4lt", "mit", "sei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wieviel er dann wert ist, w\u00fc\u00dft' er gern.", "tokens": ["Wie\u00b7viel", "er", "dann", "wert", "ist", ",", "w\u00fc\u00dft'", "er", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.42": {"line.1": {"text": "Und zum zweiten soll ich ihm sagen dann,", "tokens": ["Und", "zum", "zwei\u00b7ten", "soll", "ich", "ihm", "sa\u00b7gen", "dann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "PPER", "VVFIN", "ADV", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie rasch er die Welt wohl umreiten kann;", "tokens": ["Wie", "rasch", "er", "die", "Welt", "wohl", "um\u00b7rei\u00b7ten", "kann", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "ART", "NN", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und zum dritten will er wissen geschwind,", "tokens": ["Und", "zum", "drit\u00b7ten", "will", "er", "wis\u00b7sen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "VMFIN", "PPER", "VVFIN", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Was zur Stelle seine Gedanken sind.\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "sei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.43": {"line.1": {"text": "Da lachte der Sch\u00e4fer: \u00bbHerr, denket daran,", "tokens": ["Da", "lach\u00b7te", "der", "Sch\u00e4\u00b7fer", ":", "\u00bb", "Herr", ",", "den\u00b7ket", "da\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "NN", "$,", "VVFIN", "PAV", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da\u00df ein Narr einen Weisen lehren kann;", "tokens": ["Da\u00df", "ein", "Narr", "ei\u00b7nen", "Wei\u00b7sen", "leh\u00b7ren", "kann", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Gebt mir Euer Ro\u00df, Euren Stab, Euer Kleid,", "tokens": ["Gebt", "mir", "Eu\u00b7er", "Ro\u00df", ",", "Eu\u00b7ren", "Stab", ",", "Eu\u00b7er", "Kleid", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "--+-++-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Und ich fecht' Euch aus Euren ganzen Streit.", "tokens": ["Und", "ich", "fecht'", "Euch", "aus", "Eu\u00b7ren", "gan\u00b7zen", "Streit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.44": {"line.1": {"text": "Sorgt nicht; in Kentshire wei\u00df jedes Kind,", "tokens": ["Sorgt", "nicht", ";", "in", "Kents\u00b7hi\u00b7re", "wei\u00df", "je\u00b7des", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$.", "APPR", "NE", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+---+-+", "measure": "zehnsilber"}, "line.2": {"text": "Da\u00df wir zwei wie von einem Vater sind,", "tokens": ["Da\u00df", "wir", "zwei", "wie", "von", "ei\u00b7nem", "Va\u00b7ter", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "KOKOM", "APPR", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und trag' ich nur erst Euer pr\u00e4chtig Gewand,", "tokens": ["Und", "trag'", "ich", "nur", "erst", "Eu\u00b7er", "pr\u00e4ch\u00b7tig", "Ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "PPOSAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Unterscheidet uns keiner im ganzen Land.\u00ab", "tokens": ["Un\u00b7ter\u00b7schei\u00b7det", "uns", "kei\u00b7ner", "im", "gan\u00b7zen", "Land", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPRART", "ADJA", "NN", "$.", "$("], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.45": {"line.1": {"text": "Da beschwor ihn der Bischof: \u00bbNimm Chorrock und Stab,", "tokens": ["Da", "be\u00b7schwor", "ihn", "der", "Bi\u00b7schof", ":", "\u00bb", "Nimm", "Chor\u00b7rock", "und", "Stab", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$.", "$(", "NE", "NE", "KON", "NN", "$,"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Nimm Diener und L\u00e4ufer, so viel ich hab',", "tokens": ["Nimm", "Die\u00b7ner", "und", "L\u00e4u\u00b7fer", ",", "so", "viel", "ich", "hab'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "$,", "ADV", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nimm Mitra, Kapuze, nimm was dir gef\u00e4llt,", "tokens": ["Nimm", "Mit\u00b7ra", ",", "Ka\u00b7pu\u00b7ze", ",", "nimm", "was", "dir", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NN", "$,", "VVIMP", "PWS", "PPER", "VVPP", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Nur l\u00f6se die Fragen, die er gestellt.\u00ab", "tokens": ["Nur", "l\u00f6\u00b7se", "die", "Fra\u00b7gen", ",", "die", "er", "ge\u00b7stellt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "PRELS", "PPER", "VVPP", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.46": {"line.1": {"text": "\u00bbwillkommen, Freund Bischof\u00ab, rief K\u00f6nig Johann,", "tokens": ["\u00bb", "will\u00b7kom\u00b7men", ",", "Freund", "Bi\u00b7schof", "\u00ab", ",", "rief", "K\u00f6\u00b7nig", "Jo\u00b7hann", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "NN", "NE", "$(", "$,", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "\u00bbdu h\u00e4ltst deine Zeit, das ist wohlgetan,", "tokens": ["\u00bb", "du", "h\u00e4ltst", "dei\u00b7ne", "Zeit", ",", "das", "ist", "wohl\u00b7ge\u00b7tan", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPOSAT", "NN", "$,", "PDS", "VAFIN", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und h\u00e4lt nur dein Witz auch so p\u00fcnktlich Stand,", "tokens": ["Und", "h\u00e4lt", "nur", "dein", "Witz", "auch", "so", "p\u00fcnkt\u00b7lich", "Stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPOSAT", "NN", "ADV", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Belehn' ich aufs neu dich mit Leuten und Land.", "tokens": ["Be\u00b7lehn'", "ich", "aufs", "neu", "dich", "mit", "Leu\u00b7ten", "und", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADJD", "PRF", "APPR", "NN", "KON", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.47": {"line.1": {"text": "Zum ersten: Wenn ich auf Englands Thron,", "tokens": ["Zum", "ers\u00b7ten", ":", "Wenn", "ich", "auf", "En\u00b7glands", "Thron", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$.", "KOUS", "PPER", "APPR", "NE", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Zepter in H\u00e4nden, zu H\u00e4upten die Kron',", "tokens": ["Das", "Zep\u00b7ter", "in", "H\u00e4n\u00b7den", ",", "zu", "H\u00e4up\u00b7ten", "die", "Kron'", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "NN", "ART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Rat halte mit meinen Grafen und Herrn,", "tokens": ["Rat", "hal\u00b7te", "mit", "mei\u00b7nen", "Gra\u00b7fen", "und", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie viel ich dann wert bin, w\u00fc\u00dft' ich gern.\u00ab", "tokens": ["Wie", "viel", "ich", "dann", "wert", "bin", ",", "w\u00fc\u00dft'", "ich", "gern", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PIS", "PPER", "ADV", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.48": {"line.1": {"text": "\u00bbunser Heiland wurde, so wahr ich getauft,", "tokens": ["\u00bb", "un\u00b7ser", "Hei\u00b7land", "wur\u00b7de", ",", "so", "wahr", "ich", "ge\u00b7tauft", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "VAFIN", "$,", "ADV", "ADJD", "PPER", "VVPP", "$,"], "meter": "+-+-+-++--+", "measure": "iambic.hexa.chol"}, "line.2": {"text": "Um drei\u00dfig Silberlinge verkauft,", "tokens": ["Um", "drei\u00b7\u00dfig", "Sil\u00b7ber\u00b7lin\u00b7ge", "ver\u00b7kauft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Drum neunundzwanzig sch\u00e4tz' ich Euch ein,", "tokens": ["Drum", "neun\u00b7und\u00b7zwan\u00b7zig", "sch\u00e4tz'", "ich", "Euch", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJD", "VVFIN", "PPER", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Um ", "tokens": ["Um"], "token_info": ["word"], "pos": ["KOUI"], "meter": "+", "measure": "single.up"}}, "stanza.49": {"line.1": {"text": "Da lachte der K\u00f6nig und schwur bei Sankt Velt:", "tokens": ["Da", "lach\u00b7te", "der", "K\u00f6\u00b7nig", "und", "schwur", "bei", "Sankt", "Velt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "APPR", "VVFIN", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "\u00bbich hab' nicht gedacht, da\u00df so wenig ich gelt'!", "tokens": ["\u00bb", "ich", "hab'", "nicht", "ge\u00b7dacht", ",", "da\u00df", "so", "we\u00b7nig", "ich", "gelt'", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKNEG", "VVPP", "$,", "KOUS", "ADV", "PIS", "PPER", "VVPP", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Nun aber zum zweiten sage mir an,", "tokens": ["Nun", "a\u00b7ber", "zum", "zwei\u00b7ten", "sa\u00b7ge", "mir", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "ADJA", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie rasch wohl die Welt ich umreiten kann?\u00ab", "tokens": ["Wie", "rasch", "wohl", "die", "Welt", "ich", "um\u00b7rei\u00b7ten", "kann", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "ADJD", "ADV", "ART", "NN", "PPER", "VVINF", "VMFIN", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.50": {"line.1": {"text": "\u00bbreit' aus mit der Sonn', immer neben ihr fort,", "tokens": ["\u00bb", "reit'", "aus", "mit", "der", "Sonn'", ",", "im\u00b7mer", "ne\u00b7ben", "ihr", "fort", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "APPR", "APPR", "ART", "NN", "$,", "ADV", "APPR", "PPER", "PTKVZ", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Bis du andren Tages am alten Ort,", "tokens": ["Bis", "du", "an\u00b7dren", "Ta\u00b7ges", "am", "al\u00b7ten", "Ort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJA", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So hast du die Reise in Tag und Nacht", "tokens": ["So", "hast", "du", "die", "Rei\u00b7se", "in", "Tag", "und", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Oder vierundzwanzig Stunden gemacht.\u00ab", "tokens": ["O\u00b7der", "vie\u00b7rund\u00b7zwan\u00b7zig", "Stun\u00b7den", "ge\u00b7macht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "CARD", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.51": {"line.1": {"text": "Da lachte der K\u00f6nig und schwur bei Sankt Veit:", "tokens": ["Da", "lach\u00b7te", "der", "K\u00f6\u00b7nig", "und", "schwur", "bei", "Sankt", "Veit", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "APPR", "VVFIN", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "\u00bbich hab' nicht gedacht, da\u00df so rasch ich reit'!", "tokens": ["\u00bb", "ich", "hab'", "nicht", "ge\u00b7dacht", ",", "da\u00df", "so", "rasch", "ich", "reit'", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKNEG", "VVPP", "$,", "KOUS", "ADV", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nun aber sollst du mir sagen geschwind,", "tokens": ["Nun", "a\u00b7ber", "sollst", "du", "mir", "sa\u00b7gen", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "PPER", "VVFIN", "ADJD", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Was zur Stelle meine Gedanken sind.\u00ab", "tokens": ["Was", "zur", "Stel\u00b7le", "mei\u00b7ne", "Ge\u00b7dan\u00b7ken", "sind", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.52": {"line.1": {"text": "Da beugte der Sch\u00e4fer schnell sein Knie:", "tokens": ["Da", "beug\u00b7te", "der", "Sch\u00e4\u00b7fer", "schnell", "sein", "Knie", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbihr denkt, ich sei Bischof von Canterbury,", "tokens": ["\u00bb", "ihr", "denkt", ",", "ich", "sei", "Bi\u00b7schof", "von", "Can\u00b7ter\u00b7bu\u00b7ry", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "NN", "APPR", "NE", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Und bitt' um Gnade f\u00fcr ihn und f\u00fcr mich.\u00ab", "tokens": ["Und", "bitt'", "um", "Gna\u00b7de", "f\u00fcr", "ihn", "und", "f\u00fcr", "mich", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "APPR", "PPER", "KON", "APPR", "PPER", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.53": {"line.1": {"text": "Da schwur der K\u00f6nig und lachte hell:", "tokens": ["Da", "schwur", "der", "K\u00f6\u00b7nig", "und", "lach\u00b7te", "hell", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "VVFIN", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbdu sollst Bischof sein an seiner Stell'.\u00ab", "tokens": ["\u00bb", "du", "sollst", "Bi\u00b7schof", "sein", "an", "sei\u00b7ner", "Stell'", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VMFIN", "NN", "PPOSAT", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "-+----+-+", "measure": "dactylic.init"}, "line.3": {"text": "Der Sch\u00e4fer seufzte: \u00bb's geht halt nit mehr,", "tokens": ["Der", "Sch\u00e4\u00b7fer", "seufz\u00b7te", ":", "\u00bb", "'s", "geht", "halt", "nit", "mehr", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "PPER", "VVFIN", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wo n\u00e4hm' ich das Lesen und Schreiben her?\u00ab", "tokens": ["Wo", "n\u00e4hm'", "ich", "das", "Le\u00b7sen", "und", "Schrei\u00b7ben", "her", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ART", "NN", "KON", "NN", "PTKVZ", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.54": {"line.1": {"text": "\u00bbwohlan denn, so nimm zu Dank und Lohn", "tokens": ["\u00bb", "wo\u00b7hlan", "denn", ",", "so", "nimm", "zu", "Dank", "und", "Lohn"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "ADV", "$,", "ADV", "VVIMP", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Vier Nobel die Woche von mir, mein Sohn,", "tokens": ["Vier", "No\u00b7bel", "die", "Wo\u00b7che", "von", "mir", ",", "mein", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "ART", "NN", "APPR", "PPER", "$,", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und reitst du bei deinem Bischof heran,", "tokens": ["Und", "reitst", "du", "bei", "dei\u00b7nem", "Bi\u00b7schof", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "So bring ihm Verzeihung vom K\u00f6nig Johann.\u00ab", "tokens": ["So", "bring", "ihm", "Ver\u00b7zei\u00b7hung", "vom", "K\u00f6\u00b7nig", "Jo\u00b7hann", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPRART", "NN", "NE", "$.", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}}}}