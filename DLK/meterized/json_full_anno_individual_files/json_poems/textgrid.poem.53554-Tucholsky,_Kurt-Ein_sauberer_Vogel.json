{"textgrid.poem.53554": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Ein sauberer Vogel", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Fing ein Gericht ein V\u00f6gelein \u2013", "tokens": ["Fing", "ein", "Ge\u00b7richt", "ein", "V\u00f6\u00b7ge\u00b7lein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und steckt es in nen K\u00e4fig 'nein \u2013", "tokens": ["Und", "steckt", "es", "in", "nen", "K\u00e4\u00b7fig", "'n\u00b7ein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Man sagt, man wei\u00df es nicht genau \u2013", "tokens": ["Man", "sagt", ",", "man", "wei\u00df", "es", "nicht", "ge\u00b7nau", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PIS", "VVFIN", "PPER", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Er scho\u00df auf eine alte Frau \u2013", "tokens": ["Er", "scho\u00df", "auf", "ei\u00b7ne", "al\u00b7te", "Frau", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "So ein Kanal ist tief und na\u00df \u2013", "tokens": ["So", "ein", "Ka\u00b7nal", "ist", "tief", "und", "na\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Wer tut denn einem Leutnant was \u2013", "tokens": ["Wer", "tut", "denn", "ei\u00b7nem", "Leut\u00b7nant", "was", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ART", "NN", "PWS", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Da flog das V\u00f6glein aus dem Haus \u2013", "tokens": ["Da", "flog", "das", "V\u00f6\u00b7glein", "aus", "dem", "Haus", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und lacht die dummen Deutschen aus \u2013", "tokens": ["Und", "lacht", "die", "dum\u00b7men", "Deut\u00b7schen", "aus", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Fing ein Gericht ein V\u00f6gelein \u2013", "tokens": ["Fing", "ein", "Ge\u00b7richt", "ein", "V\u00f6\u00b7ge\u00b7lein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und steckt es in nen K\u00e4fig 'nein \u2013", "tokens": ["Und", "steckt", "es", "in", "nen", "K\u00e4\u00b7fig", "'n\u00b7ein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Man sagt, man wei\u00df es nicht genau \u2013", "tokens": ["Man", "sagt", ",", "man", "wei\u00df", "es", "nicht", "ge\u00b7nau", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PIS", "VVFIN", "PPER", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Er scho\u00df auf eine alte Frau \u2013", "tokens": ["Er", "scho\u00df", "auf", "ei\u00b7ne", "al\u00b7te", "Frau", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "So ein Kanal ist tief und na\u00df \u2013", "tokens": ["So", "ein", "Ka\u00b7nal", "ist", "tief", "und", "na\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Wer tut denn einem Leutnant was \u2013", "tokens": ["Wer", "tut", "denn", "ei\u00b7nem", "Leut\u00b7nant", "was", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ART", "NN", "PWS", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Da flog das V\u00f6glein aus dem Haus \u2013", "tokens": ["Da", "flog", "das", "V\u00f6\u00b7glein", "aus", "dem", "Haus", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hm, hm \u2013 So, so \u2013", "tokens": ["Hm", ",", "hm", "\u2013", "So", ",", "so", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "$,", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und lacht die dummen Deutschen aus \u2013", "tokens": ["Und", "lacht", "die", "dum\u00b7men", "Deut\u00b7schen", "aus", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hm, hm \u2013 So so!", "tokens": ["Hm", ",", "hm", "\u2013", "So", "so", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "$,", "NE", "$(", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}