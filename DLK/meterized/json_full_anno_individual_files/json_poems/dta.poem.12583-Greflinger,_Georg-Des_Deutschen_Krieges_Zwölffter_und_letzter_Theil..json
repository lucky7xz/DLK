{"dta.poem.12583": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Zw\u00f6lffter und letzter  \n Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Nach Linnert Torsten Sohn/ bey welchem nie kein", "tokens": ["Nach", "Lin\u00b7nert", "Tors\u00b7ten", "Sohn", "/", "bey", "wel\u00b7chem", "nie", "kein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "NE", "NN", "$(", "APPR", "PRELS", "ADV", "PIAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mangel", "tokens": ["Man\u00b7gel"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "An Sieg und Ehren war/ kam Carl Gustavus", "tokens": ["An", "Sieg", "und", "Eh\u00b7ren", "war", "/", "kam", "Carl", "Gus\u00b7ta\u00b7vus"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "VAFIN", "$(", "VVFIN", "NE", "NE"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Wrangel.", "tokens": ["Wran\u00b7gel", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Was sein verrichten war/ sol sonder falschen Schein/", "tokens": ["Was", "sein", "ver\u00b7rich\u00b7ten", "war", "/", "sol", "son\u00b7der", "fal\u00b7schen", "Schein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "ADJA", "VAFIN", "$(", "VMFIN", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "und sol also der Schlu\u00df des Kriegs berichtet seyn.", "tokens": ["und", "sol", "al\u00b7so", "der", "Schlu\u00df", "des", "Kriegs", "be\u00b7rich\u00b7tet", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "ART", "NN", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So bald wir seine Macht bey Gotha richtig sahen/", "tokens": ["So", "bald", "wir", "sei\u00b7ne", "Macht", "bey", "Go\u00b7tha", "rich\u00b7tig", "sa\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PPOSAT", "NN", "APPR", "NE", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Erhob er sich geschwind der Weser sich zu nahen.", "tokens": ["Er\u00b7hob", "er", "sich", "ge\u00b7schwind", "der", "We\u00b7ser", "sich", "zu", "na\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "ART", "NN", "PRF", "APPR", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Man gieng das Ei\u00dffeld durch/ auf Deng- und Heilgen Stadt/", "tokens": ["Man", "gieng", "das", "Ei\u00df\u00b7feld", "durch", "/", "auf", "Deng", "und", "Heil\u00b7gen", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "APPR", "$(", "APPR", "TRUNC", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Worauf das gantze Heer sehr dicht f\u00fcr H\u00f6xter trat/", "tokens": ["Wo\u00b7rauf", "das", "gant\u00b7ze", "Heer", "sehr", "dicht", "f\u00fcr", "H\u00f6x\u00b7ter", "trat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "ADV", "ADJD", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Das mit sechshundert Mann und sechsmal tausend Mal-", "tokens": ["Das", "mit", "sechs\u00b7hun\u00b7dert", "Mann", "und", "sechs\u00b7mal", "tau\u00b7send", "Ma\u00b7l"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "CARD", "NN", "KON", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.12": {"text": "Von Mehl versehen war/ das alles neuen Waltern", "tokens": ["Von", "Mehl", "ver\u00b7se\u00b7hen", "war", "/", "das", "al\u00b7les", "neu\u00b7en", "Wal\u00b7tern"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVPP", "VAFIN", "$(", "ART", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Durch Macht zu Handen fiel/ weil man die Stadt bekahm", "tokens": ["Durch", "Macht", "zu", "Han\u00b7den", "fiel", "/", "weil", "man", "die", "Stadt", "be\u00b7kahm"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "NN", "VVFIN", "$(", "KOUS", "PIS", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "und alles/ was sie hatt/ als ein besiegtes nahm.", "tokens": ["und", "al\u00b7les", "/", "was", "sie", "hatt", "/", "als", "ein", "be\u00b7sieg\u00b7tes", "nahm", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "$(", "PWS", "PPER", "VAFIN", "$(", "KOUS", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Das Volck wurd eingesteckt/ das Mehl hinweg geschaffet/", "tokens": ["Das", "Volck", "wurd", "ein\u00b7ge\u00b7steckt", "/", "das", "Mehl", "hin\u00b7weg", "ge\u00b7schaf\u00b7fet", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$(", "ART", "NN", "APZR", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Auch aller Vestung-Bau und Mauern so bestraffet/", "tokens": ["Auch", "al\u00b7ler", "Ves\u00b7tung\u00b7Bau", "und", "Mau\u00b7ern", "so", "be\u00b7straf\u00b7fet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "KON", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Da\u00df sie nun Hauffen sind. Was von der Schweden Macht", "tokens": ["Da\u00df", "sie", "nun", "Hauf\u00b7fen", "sind", ".", "Was", "von", "der", "Schwe\u00b7den", "Macht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "NN", "VAFIN", "$.", "PWS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.18": {"text": "Vor diesem Orte fiel/ wird gar gering geacht.", "tokens": ["Vor", "die\u00b7sem", "Or\u00b7te", "fiel", "/", "wird", "gar", "ge\u00b7ring", "ge\u00b7acht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "$(", "VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Verwundte waren wol/ bey welchen man Helm Wrangel", "tokens": ["Ver\u00b7wund\u00b7te", "wa\u00b7ren", "wol", "/", "bey", "wel\u00b7chen", "man", "Helm", "Wran\u00b7gel"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ADV", "$(", "APPR", "PWAT", "PIS", "NN", "NE"], "meter": "-+-+-+-+-+++-", "measure": "unknown.measure.septa"}, "line.20": {"text": "(der/ wie der Dehnen Fried in allem ohne Mangel", "tokens": ["(", "der", "/", "wie", "der", "Deh\u00b7nen", "Fried", "in", "al\u00b7lem", "oh\u00b7ne", "Man\u00b7gel"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ART", "$(", "KOKOM", "ART", "ADJA", "NN", "APPR", "PIS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.22": {"text": "und mit vertrauter Macht zu der vor H\u00f6chster gab)", "tokens": ["und", "mit", "ver\u00b7trau\u00b7ter", "Macht", "zu", "der", "vor", "H\u00f6chs\u00b7ter", "gab", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "APPR", "ART", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Nicht schlecht von Schaden sah. Nach allen diefen Dingen", "tokens": ["Nicht", "schlecht", "von", "Scha\u00b7den", "sah", ".", "Nach", "al\u00b7len", "die\u00b7fen", "Din\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADJD", "APPR", "NN", "VVFIN", "$.", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Vor H\u00f6chster/ musten auch Neuhau\u00df und Lipspring springen/", "tokens": ["Vor", "H\u00f6chs\u00b7ter", "/", "mus\u00b7ten", "auch", "Neu\u00b7hau\u00df", "und", "Lip\u00b7spring", "sprin\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "VMFIN", "ADV", "NN", "KON", "NN", "VVINF", "$("], "meter": "-+-+--++-+-+-", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "Die Duclas auf Genad und ", "tokens": ["Die", "Du\u00b7clas", "auf", "Ge\u00b7nad", "und"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "KON"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.26": {"text": "Worauf die gantze Macht vor Paderborn ankahm/", "tokens": ["Wo\u00b7rauf", "die", "gant\u00b7ze", "Macht", "vor", "Pa\u00b7der\u00b7born", "an\u00b7kahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Das gegen O\u00dfnabr\u00fcck und gegen M\u00fcnster liget.", "tokens": ["Das", "ge\u00b7gen", "O\u00df\u00b7na\u00b7br\u00fcck", "und", "ge\u00b7gen", "M\u00fcns\u00b7ter", "li\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "KON", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Es wurd auch bald darauf bekrieget und besieget.", "tokens": ["Es", "wurd", "auch", "bald", "da\u00b7rauf", "be\u00b7krie\u00b7get", "und", "be\u00b7sie\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PAV", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Wie hoch sah O\u00dfnabr\u00fcck/ wie hoch sah M\u00fcnster auff/", "tokens": ["Wie", "hoch", "sah", "O\u00df\u00b7na\u00b7br\u00fcck", "/", "wie", "hoch", "sah", "M\u00fcns\u00b7ter", "auff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "NN", "$(", "PWAV", "ADJD", "VVFIN", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Als dieser Schweden Mars in einem vollen Lauff", "tokens": ["Als", "die\u00b7ser", "Schwe\u00b7den", "Mars", "in", "ei\u00b7nem", "vol\u00b7len", "Lauff"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "An jhre Grentzen kam/ als wolt\u2019 er mit den Degen", "tokens": ["An", "jhre", "Grent\u00b7zen", "kam", "/", "als", "wolt'", "er", "mit", "den", "De\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "KOUS", "VMFIN", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.32": {"text": "Erwegen/ was man nicht mit Worten kunt\u2019 erwegen.", "tokens": ["Er\u00b7we\u00b7gen", "/", "was", "man", "nicht", "mit", "Wor\u00b7ten", "kunt'", "er\u00b7we\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWS", "PIS", "PTKNEG", "APPR", "NN", "PTKVZ", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Das war die Deutsche Ruh/ ob welcher man alhier", "tokens": ["Das", "war", "die", "Deut\u00b7sche", "Ruh", "/", "ob", "wel\u00b7cher", "man", "al\u00b7hier"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$(", "KOUS", "PRELS", "PIS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Ein langes spr\u00e4chen hatt\u2019/ und halff da nichts daf\u00fcr/", "tokens": ["Ein", "lan\u00b7ges", "spr\u00e4\u00b7chen", "hatt'", "/", "und", "halff", "da", "nichts", "da\u00b7f\u00fcr", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVINF", "VAFIN", "$(", "KON", "VVFIN", "ADV", "PIS", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Wie lang wie seufftzeten und um den Frieden baten.", "tokens": ["Wie", "lang", "wie", "seufft\u00b7ze\u00b7ten", "und", "um", "den", "Frie\u00b7den", "ba\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "KOKOM", "ADJA", "KON", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Nach langem wurd\u2019 uns doch (GOtt Lob und Danck) ge-", "tokens": ["Nach", "lan\u00b7gem", "wurd'", "uns", "doch", "(", "Gott", "Lob", "und", "Danck", ")", "ge"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "ADJA", "VAFIN", "PPER", "ADV", "$(", "NN", "NN", "KON", "NN", "$(", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.37": {"text": "Als man mit Paderborn in allem richtig war/", "tokens": ["Als", "man", "mit", "Pa\u00b7der\u00b7born", "in", "al\u00b7lem", "rich\u00b7tig", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "NN", "APPR", "PIS", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "Gieng man auf Bergen zu/ das solcher gro\u00dfen Schaar", "tokens": ["Gieng", "man", "auf", "Ber\u00b7gen", "zu", "/", "das", "sol\u00b7cher", "gro\u00b7\u00dfen", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "APPR", "NN", "PTKZU", "$(", "ART", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Nicht gro\u00df zuwider fiel. Es wurde so gesiebet/", "tokens": ["Nicht", "gro\u00df", "zu\u00b7wi\u00b7der", "fiel", ".", "Es", "wur\u00b7de", "so", "ge\u00b7sie\u00b7bet", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "ADJD", "VVFIN", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Da\u00df es noch heute da nicht viel er\u00fcbrigt giebet.", "tokens": ["Da\u00df", "es", "noch", "heu\u00b7te", "da", "nicht", "viel", "er\u00b7\u00fcb\u00b7rigt", "gie\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ADV", "PTKNEG", "ADV", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Di\u00df traff viel andre mehr. Hier gieng der K\u00f6nigsmarck", "tokens": ["Di\u00df", "traff", "viel", "and\u00b7re", "mehr", ".", "Hier", "gieng", "der", "K\u00f6\u00b7nigs\u00b7marck"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "PIS", "ADV", "$.", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "Mit vielen V\u00f6lckern ab/ und machte sich sehr starck", "tokens": ["Mit", "vie\u00b7len", "V\u00f6l\u00b7ckern", "ab", "/", "und", "mach\u00b7te", "sich", "sehr", "starck"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "PTKVZ", "$(", "KON", "VVFIN", "PRF", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Mit fechten vor die Fecht/ ein wolbefestes Wesen/", "tokens": ["Mit", "fech\u00b7ten", "vor", "die", "Fecht", "/", "ein", "wol\u00b7be\u00b7fes\u00b7tes", "We\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "APPR", "ART", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Wust\u2019 aber diesesmal den Knopf nicht auf zu l\u00f6sen/", "tokens": ["Wust'", "a\u00b7ber", "die\u00b7ses\u00b7mal", "den", "Knopf", "nicht", "auf", "zu", "l\u00f6\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "ART", "NN", "PTKNEG", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Die Zeit fiel viel zu kurtz. Er samlete sich auff", "tokens": ["Die", "Zeit", "fiel", "viel", "zu", "kurtz", ".", "Er", "sam\u00b7le\u00b7te", "sich", "auff"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PTKA", "ADJD", "$.", "PPER", "VVFIN", "PRF", "APPR"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.46": {"text": "und gieng f\u00fcr Lemgau hin/ gewanns/ und nahm den Lauff", "tokens": ["und", "gieng", "f\u00fcr", "Lem\u00b7gau", "hin", "/", "ge\u00b7wanns", "/", "und", "nahm", "den", "Lauff"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NE", "PTKVZ", "$(", "NE", "$(", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.47": {"text": "Hierauf zur Haupt-Armee/ die sich nach Wetzlar machte/", "tokens": ["Hier\u00b7auf", "zur", "Haup\u00b7tAr\u00b7mee", "/", "die", "sich", "nach", "Wetz\u00b7lar", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "APPRART", "NN", "$(", "PRELS", "PRF", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.48": {"text": "Woselbst des Cassels-H\u00e4upt (das ich wie Pallas achte)", "tokens": ["Wo\u00b7selbst", "des", "Cas\u00b7sels\u00b7H\u00e4upt", "(", "das", "ich", "wie", "Pal\u00b7las", "ach\u00b7te", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$(", "PRELS", "PPER", "KOKOM", "NE", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Durch jhren General/ den man den Geusen hie\u00df/", "tokens": ["Durch", "jhren", "Ge\u00b7ne\u00b7ral", "/", "den", "man", "den", "Geu\u00b7sen", "hie\u00df", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$(", "ART", "PIS", "ART", "NN", "VVFIN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.50": {"text": "Jhr Volck zu hauffen nahm/ und zu den Schweden stie\u00df.", "tokens": ["Ihr", "Volck", "zu", "hauf\u00b7fen", "nahm", "/", "und", "zu", "den", "Schwe\u00b7den", "stie\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "NN", "VVFIN", "$(", "KON", "APPR", "ART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "Was Cassel diesen that/ that Darmstadt bey dem K\u00e4yser/", "tokens": ["Was", "Cas\u00b7sel", "die\u00b7sen", "that", "/", "that", "Darm\u00b7stadt", "bey", "dem", "K\u00e4y\u00b7ser", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "PDS", "VVFIN", "$(", "VVFIN", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.52": {"text": "Weil sich zu dieser Zeit die beyden Hessen-H\u00e4user", "tokens": ["Weil", "sich", "zu", "die\u00b7ser", "Zeit", "die", "bey\u00b7den", "Hes\u00b7sen\u00b7H\u00e4u\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "APPR", "PDAT", "NN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.53": {"text": "Von wegen eines Erbs und andrer Dinge mehr", "tokens": ["Von", "we\u00b7gen", "ei\u00b7nes", "Erbs", "und", "an\u00b7drer", "Din\u00b7ge", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "NN", "KON", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Entzweyt/ da\u00df beyde Theil\u2019 ein ziemlich gro\u00dfes Heer/", "tokens": ["Ent\u00b7zweyt", "/", "da\u00df", "bey\u00b7de", "Theil'", "ein", "ziem\u00b7lich", "gro\u00b7\u00dfes", "Heer", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "KOUS", "PIAT", "NN", "ART", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Die Sachen abzuthun/ im freyen Felde hatten/", "tokens": ["Die", "Sa\u00b7chen", "ab\u00b7zu\u00b7thun", "/", "im", "frey\u00b7en", "Fel\u00b7de", "hat\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$(", "APPRART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "und meynte jeder Theil den andern abzumatten.", "tokens": ["und", "meyn\u00b7te", "je\u00b7der", "Theil", "den", "an\u00b7dern", "ab\u00b7zu\u00b7mat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "ART", "ADJA", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Es ist/ wie wol bewust/ im Cassel-Hessen-Land", "tokens": ["Es", "ist", "/", "wie", "wol", "be\u00b7wust", "/", "im", "Cas\u00b7sel\u00b7Hes\u00b7sen\u00b7Land"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "$(", "KOKOM", "ADV", "VVFIN", "$(", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Ein hoch-erhabnes Schlo\u00df Amoenenbug genannt/", "tokens": ["Ein", "hoch\u00b7er\u00b7hab\u00b7nes", "Schlo\u00df", "A\u00b7moe\u00b7nen\u00b7bug", "ge\u00b7nannt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "Hat von der Gegend-Lust den Nahmen \u00fcberkommen/", "tokens": ["Hat", "von", "der", "Ge\u00b7gen\u00b7dLust", "den", "Nah\u00b7men", "\u00fc\u00b7ber\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Di\u00df Schlo\u00df hatt\u2019 hier und da sich raubens angenommen.", "tokens": ["Di\u00df", "Schlo\u00df", "hatt'", "hier", "und", "da", "sich", "rau\u00b7bens", "an\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "KON", "KOUS", "PRF", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "Daher die Schweden-Macht samt Hessen sich erhob/", "tokens": ["Da\u00b7her", "die", "Schwe\u00b7den\u00b7Macht", "samt", "Hes\u00b7sen", "sich", "er\u00b7hob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "NE", "PRF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.62": {"text": "umringete den Ort und lag jhm gl\u00fccklich ob/", "tokens": ["um\u00b7rin\u00b7ge\u00b7te", "den", "Ort", "und", "lag", "jhm", "gl\u00fcck\u00b7lich", "ob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "KON", "VVFIN", "PPER", "ADJD", "KOUS", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "Er wurd\u2019 auch gantz geschleifft/ worauff sich beyde Theile", "tokens": ["Er", "wurd'", "auch", "gantz", "ge\u00b7schleifft", "/", "wo\u00b7rauff", "sich", "bey\u00b7de", "Thei\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "$(", "PWAV", "PRF", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.64": {"text": "Bey Kirchhayn (welche Stadt ein Viertel einer Meile", "tokens": ["Bey", "Kirch\u00b7hayn", "(", "wel\u00b7che", "Stadt", "ein", "Vier\u00b7tel", "ei\u00b7ner", "Mei\u00b7le"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$(", "PWAT", "NN", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Von diesem Schlosse lag) verschantzten/ da\u00df jhr Feind", "tokens": ["Von", "die\u00b7sem", "Schlos\u00b7se", "lag", ")", "ver\u00b7schantz\u00b7ten", "/", "da\u00df", "jhr", "Feind"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "$(", "VVFIN", "$(", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.66": {"text": "(von K\u00e4yser-Beyrischen- und Hessen-Volck vereint)", "tokens": ["(", "von", "K\u00e4y\u00b7ser\u00b7Bey\u00b7ri\u00b7schen", "und", "Hes\u00b7sen\u00b7Volck", "ver\u00b7eint", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "TRUNC", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.67": {"text": "Wie viel Versuchs er that/ sehr wenig kunte schaffen/", "tokens": ["Wie", "viel", "Ver\u00b7suchs", "er", "that", "/", "sehr", "we\u00b7nig", "kun\u00b7te", "schaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "PPER", "VVFIN", "$(", "ADV", "PIS", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.68": {"text": "Ertz-Hertzog Leopold versah des K\u00e4ysers Waffen/", "tokens": ["Ertz\u00b7Hert\u00b7zog", "Leo\u00b7pold", "ver\u00b7sah", "des", "K\u00e4y\u00b7sers", "Waf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "ART", "NN", "NN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.69": {"text": "Des Beyers Jan de Werth/ des Darmstadts/ Eberstein/", "tokens": ["Des", "Be\u00b7yers", "Jan", "de", "Werth", "/", "des", "Darm\u00b7stadts", "/", "E\u00b7bers\u00b7tein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "NE", "NE", "NE", "$(", "ART", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Die sonsten alle drey begl\u00fcckt gewesen seyn/", "tokens": ["Die", "sons\u00b7ten", "al\u00b7le", "drey", "be\u00b7gl\u00fcckt", "ge\u00b7we\u00b7sen", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "PIAT", "CARD", "VVPP", "VAPP", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "Hier aber kunten sie kein gro\u00dfes nicht verrichten/", "tokens": ["Hier", "a\u00b7ber", "kun\u00b7ten", "sie", "kein", "gro\u00b7\u00dfes", "nicht", "ver\u00b7rich\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "PIAT", "ADJA", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Weil sie auch \u00fcber das der Futterung vernichten/", "tokens": ["Weil", "sie", "auch", "\u00fc\u00b7ber", "das", "der", "Fut\u00b7te\u00b7rung", "ver\u00b7nich\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PRELS", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Auf andre Wege trieb/ das war auf Jlmstadt zu.", "tokens": ["Auf", "and\u00b7re", "We\u00b7ge", "trieb", "/", "das", "war", "auf", "Jlm\u00b7stadt", "zu", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$(", "PDS", "VAFIN", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.74": {"text": "Allhier begaben sie sich etwas in die Ruh/", "tokens": ["All\u00b7hier", "be\u00b7ga\u00b7ben", "sie", "sich", "et\u00b7was", "in", "die", "Ruh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "um jhrer Feinde thun in Augenschein zu nehmen.", "tokens": ["um", "jhrer", "Fein\u00b7de", "thun", "in", "Au\u00b7gen\u00b7schein", "zu", "neh\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.76": {"text": "Bald sah man alles Heer der Schweden sich bequemen", "tokens": ["Bald", "sah", "man", "al\u00b7les", "Heer", "der", "Schwe\u00b7den", "sich", "be\u00b7que\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PIAT", "NN", "ART", "NE", "PRF", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Gerad auf Giesen hin und Wetzlar zu zu gehn/", "tokens": ["Ge\u00b7rad", "auf", "Gie\u00b7sen", "hin", "und", "Wetz\u00b7lar", "zu", "zu", "gehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "PTKVZ", "KON", "NN", "PTKZU", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.78": {"text": "Bi\u00df auf der Frantzen Macht jhr\u2019 Ankunfft da zu stehn/", "tokens": ["Bi\u00df", "auf", "der", "Frant\u00b7zen", "Macht", "jhr'", "An\u00b7kunfft", "da", "zu", "stehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "NN", "PPOSAT", "NN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.79": {"text": "Die des Touraini Hand den Reyhn her\u00fcber f\u00fchrte.", "tokens": ["Die", "des", "Tou\u00b7rai\u00b7ni", "Hand", "den", "Reyhn", "her\u00b7\u00fc\u00b7ber", "f\u00fchr\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "NN", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "Das war die iene Macht/ die noch vom Bernhard r\u00fchrte/", "tokens": ["Das", "war", "die", "ie\u00b7ne", "Macht", "/", "die", "noch", "vom", "Bern\u00b7hard", "r\u00fchr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$(", "ART", "ADV", "APPRART", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Sonst Weymarisch benahmt. Sie kam/ und zimlich gro\u00df/", "tokens": ["Sonst", "Wey\u00b7ma\u00b7risch", "be\u00b7nahmt", ".", "Sie", "kam", "/", "und", "zim\u00b7lich", "gro\u00df", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "$.", "PPER", "VVFIN", "$(", "KON", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.82": {"text": "Man jauchtzt\u2019 und brennete die groben St\u00fccke lo\u00df/", "tokens": ["Man", "jauchtzt'", "und", "bren\u00b7ne\u00b7te", "die", "gro\u00b7ben", "St\u00fc\u00b7cke", "lo\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "und giengen beyde Theil anjetzt in einem Hauffen/", "tokens": ["und", "gien\u00b7gen", "bey\u00b7de", "Theil", "an\u00b7jetzt", "in", "ei\u00b7nem", "Hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "Als wolten sie nach H\u00f6chst/ und ferner das belauffen", "tokens": ["Als", "wol\u00b7ten", "sie", "nach", "H\u00f6chst", "/", "und", "fer\u00b7ner", "das", "be\u00b7lauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "VMFIN", "PPER", "APPR", "NE", "$(", "KON", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Des Meyn- und Reyhnes sehn/ das ober Mentz geschieht.", "tokens": ["Des", "Meyn", "und", "Reyh\u00b7nes", "sehn", "/", "das", "o\u00b7ber", "Mentz", "ge\u00b7schieht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "VVINF", "$(", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "Sie aber giengen hin/ wo man nach Beyern zieht/", "tokens": ["Sie", "a\u00b7ber", "gien\u00b7gen", "hin", "/", "wo", "man", "nach", "Be\u00b7yern", "zieht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PTKVZ", "$(", "PWAV", "PIS", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.87": {"text": "Als zwischen jhres Feinds und zwischen Franckfurts W\u00e4l-", "tokens": ["Als", "zwi\u00b7schen", "jhres", "Feinds", "und", "zwi\u00b7schen", "Fran\u00b7ck\u00b7furts", "W\u00e4l"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "PPOSAT", "NN", "KON", "APPR", "NE", "TRUNC"], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.88": {"text": "Worauf sich Steinheim must\u2019 in jhren Willen stellen.", "tokens": ["Wo\u00b7rauf", "sich", "Stein\u00b7heim", "must'", "in", "jhren", "Wil\u00b7len", "stel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PRF", "NE", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-++-+-+-+-", "measure": "unknown.measure.hexa"}, "line.89": {"text": "Wie auch Aschaffenburg/ de\u00dfgleichen Seelgenstadt.", "tokens": ["Wie", "auch", "A\u00b7schaf\u00b7fen\u00b7burg", "/", "de\u00df\u00b7glei\u00b7chen", "Seel\u00b7gen\u00b7stadt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADV", "NE", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Und weil Aschaffenburg ein h\u00f6ltzne Br\u00fccken hat/", "tokens": ["Und", "weil", "A\u00b7schaf\u00b7fen\u00b7burg", "ein", "h\u00f6ltz\u00b7ne", "Br\u00fc\u00b7cken", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NE", "ART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "Die \u00fcbern Mayn hingeht/ gieng aller Schwal der Schwe-", "tokens": ["Die", "\u00fc\u00b7bern", "Mayn", "hin\u00b7geht", "/", "gieng", "al\u00b7ler", "Schwal", "der", "Schwe"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$(", "VVFIN", "PIAT", "NN", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "und Frantzen \u00fcber hin. Ich mu\u00df von zwyen reden/", "tokens": ["und", "Frant\u00b7zen", "\u00fc\u00b7ber", "hin", ".", "Ich", "mu\u00df", "von", "zwyen", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "PTKVZ", "$.", "PPER", "VMFIN", "APPR", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.93": {"text": "Dann Cassel-Hessens Volck gieng wiederum nach Hau\u00df/", "tokens": ["Dann", "Cas\u00b7sel\u00b7Hes\u00b7sens", "Volck", "gieng", "wie\u00b7de\u00b7rum", "nach", "Hau\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NN", "VVFIN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.94": {"text": "und trieb des Feindes Volck aus seinen Pl\u00e4tzen aus.", "tokens": ["und", "trieb", "des", "Fein\u00b7des", "Volck", "aus", "sei\u00b7nen", "Pl\u00e4t\u00b7zen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Die andern fuhren fort und giengen weit in Francken/", "tokens": ["Die", "an\u00b7dern", "fuh\u00b7ren", "fort", "und", "gien\u00b7gen", "weit", "in", "Fran\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "PTKVZ", "KON", "VVFIN", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "Bezwungen (kurtz gedacht) fast \u00fcber die Gedancken", "tokens": ["Be\u00b7zwun\u00b7gen", "(", "kurtz", "ge\u00b7dacht", ")", "fast", "\u00fc\u00b7ber", "die", "Ge\u00b7dan\u00b7cken"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$(", "ADJD", "VVPP", "$(", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "Kling-Milt- und Freudenberg/ imgleichen Carolstadt/", "tokens": ["Kling\u00b7Mil\u00b7t", "und", "Freu\u00b7den\u00b7berg", "/", "im\u00b7glei\u00b7chen", "Ca\u00b7rol\u00b7stadt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["TRUNC", "KON", "NN", "$(", "ADJA", "NN", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.98": {"text": "und b\u00e4\u00dfre Pl\u00e4tze mehr/ die man am Necker hat.", "tokens": ["und", "b\u00e4\u00df\u00b7re", "Pl\u00e4t\u00b7ze", "mehr", "/", "die", "man", "am", "Ne\u00b7cker", "hat", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "$(", "PRELS", "PIS", "APPRART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.99": {"text": "Hieran war nicht genug/ sie satzten auch in Schwaben/", "tokens": ["Hie\u00b7ran", "war", "nicht", "ge\u00b7nug", "/", "sie", "satz\u00b7ten", "auch", "in", "Schwa\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PTKNEG", "ADV", "$(", "PPER", "VVFIN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "und lie\u00dfen jhren Feind gemachen Nachzug haben/", "tokens": ["und", "lie\u00b7\u00dfen", "jhren", "Feind", "ge\u00b7ma\u00b7chen", "Nach\u00b7zug", "ha\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ADJA", "NN", "VAFIN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.101": {"text": "Der sich auf Regenspurg und Ingolstadt begab/", "tokens": ["Der", "sich", "auf", "Re\u00b7gen\u00b7spurg", "und", "In\u00b7gol\u00b7stadt", "be\u00b7gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.102": {"text": "Des Darmstad-Hessens Volck gieng aber wieder ab.", "tokens": ["Des", "Darm\u00b7sta\u00b7dHes\u00b7sens", "Volck", "gieng", "a\u00b7ber", "wie\u00b7der", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "Eh sich die K\u00e4ysrischen auf Regenspurg verf\u00fcgten/", "tokens": ["Eh", "sich", "die", "K\u00e4y\u00b7sri\u00b7schen", "auf", "Re\u00b7gen\u00b7spurg", "ver\u00b7f\u00fcg\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.104": {"text": "Die St\u00e4dt\u2019 Aschaffenburg und Miltberg wieder kriegten/", "tokens": ["Die", "St\u00e4dt'", "A\u00b7schaf\u00b7fen\u00b7burg", "und", "Milt\u00b7berg", "wie\u00b7der", "krieg\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "KON", "NE", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "Gieng jhre Widerpart zur Stadt Heylbronn hinan/", "tokens": ["Gieng", "jhre", "Wi\u00b7der\u00b7part", "zur", "Stadt", "Heyl\u00b7bronn", "hi\u00b7nan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "APPRART", "NN", "NE", "PTKVZ", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.106": {"text": "Macht\u2019 Hall und N\u00f6rdlingen jhr schleunig unterthan.", "tokens": ["Macht'", "Hall", "und", "N\u00f6rd\u00b7lin\u00b7gen", "jhr", "schleu\u00b7nig", "un\u00b7ter\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.107": {"text": "Jmgleichen Schorendorff/ da\u00df die Frantzosen zwungen.", "tokens": ["Jm\u00b7glei\u00b7chen", "Scho\u00b7ren\u00b7dorff", "/", "da\u00df", "die", "Frant\u00b7zo\u00b7sen", "zwun\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "KOUS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "Wer hat in D\u00fcnckelspiel und Donawerth gedrungen?", "tokens": ["Wer", "hat", "in", "D\u00fcn\u00b7ckel\u00b7spiel", "und", "Do\u00b7na\u00b7werth", "ge\u00b7drun\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Der schnelle K\u00f6nigsmarck/ also in Wallerstein.", "tokens": ["Der", "schnel\u00b7le", "K\u00f6\u00b7nigs\u00b7marck", "/", "al\u00b7so", "in", "Wal\u00b7ler\u00b7stein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "Hierauf gieng aller Schwarm vor Stein und nahm es ein.", "tokens": ["Hier\u00b7auf", "gieng", "al\u00b7ler", "Schwarm", "vor", "Stein", "und", "nahm", "es", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIAT", "NN", "APPR", "NN", "KON", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Nach dem f\u00fcr Augspurg hin/ kunt\u2019 aber nichts erlangen/", "tokens": ["Nach", "dem", "f\u00fcr", "Augs\u00b7purg", "hin", "/", "kunt'", "a\u00b7ber", "nichts", "er\u00b7lan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "NE", "PTKVZ", "$(", "VMFIN", "ADV", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "Weil seine Widerpart auf jhn kam zugegangen/", "tokens": ["Weil", "sei\u00b7ne", "Wi\u00b7der\u00b7part", "auf", "jhn", "kam", "zu\u00b7ge\u00b7gan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "und das mit gro\u00dfer Macht/ dann er sechstausend Mann", "tokens": ["und", "das", "mit", "gro\u00b7\u00dfer", "Macht", "/", "dann", "er", "sech\u00b7stau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PDS", "APPR", "ADJA", "NN", "$(", "ADV", "PPER", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Aus B\u00f6h\u00e4imb/ Oesterreich und andern Orten an", "tokens": ["Aus", "B\u00f6\u00b7h\u00e4i\u00b7mb", "/", "O\u00b7es\u00b7ter\u00b7reich", "und", "an\u00b7dern", "Or\u00b7ten", "an"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "NE", "KON", "ADJA", "NN", "APPR"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.115": {"text": "Zu Helffern \u00fcberkam. Als Augspurg war entsetzet/", "tokens": ["Zu", "Helf\u00b7fern", "\u00fc\u00b7ber\u00b7kam", ".", "Als", "Augs\u00b7purg", "war", "ent\u00b7set\u00b7zet", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$.", "KOUS", "NE", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Wurd\u2019 alles Beyer-Land von allen so gesch\u00e4tzet/", "tokens": ["Wurd'", "al\u00b7les", "Beyer\u00b7Land", "von", "al\u00b7len", "so", "ge\u00b7sch\u00e4t\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "APPR", "PIS", "ADV", "VVPP", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.117": {"text": "Vorst\u00f6rt und umgekehrt/ da\u00df dessen H\u00e4upt um Ruh", "tokens": ["Vor\u00b7st\u00f6rt", "und", "um\u00b7ge\u00b7kehrt", "/", "da\u00df", "des\u00b7sen", "H\u00e4upt", "um", "Ruh"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ADJD", "$(", "KOUS", "PRELAT", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "und Waffen-Stillstand sprach. Es kam jhm auch ", "tokens": ["und", "Waf\u00b7fen\u00b7Still\u00b7stand", "sprach", ".", "Es", "kam", "jhm", "auch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.119": {"text": "Man denck\u2019 ein wenig nach/ vier Heer in einem Lande/", "tokens": ["Man", "denck", "ein", "we\u00b7nig", "nach", "/", "vier", "Heer", "in", "ei\u00b7nem", "Lan\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "PIS", "APPR", "$(", "CARD", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Zwey Freund-zwey Feindliche/ zu was f\u00fcr einem Stande", "tokens": ["Zwey", "Freun\u00b7dzwey", "Feind\u00b7li\u00b7che", "/", "zu", "was", "f\u00fcr", "ei\u00b7nem", "Stan\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "NN", "$(", "APPR", "PRELS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Solch Land gerathen kan. War einer schon gut Freind/", "tokens": ["Solch", "Land", "ge\u00b7ra\u00b7then", "kan", ".", "War", "ei\u00b7ner", "schon", "gut", "Freind", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVINF", "VMFIN", "$.", "VAFIN", "ART", "ADV", "ADJD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "So war er doch dabey des Landes-Baarschafft Feind.", "tokens": ["So", "war", "er", "doch", "da\u00b7bey", "des", "Lan\u00b7des\u00b7Baar\u00b7schafft", "Feind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PAV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.123": {"text": "Das Gut ist jedem gut/ es blieb fast nichts verschonet/", "tokens": ["Das", "Gut", "ist", "je\u00b7dem", "gut", "/", "es", "blieb", "fast", "nichts", "ver\u00b7scho\u00b7net", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "ADJD", "$(", "PPER", "VVFIN", "ADV", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.124": {"text": "Was in dem M\u00fcndelheim/ zu M\u00fcnch- und F\u00fc\u00dfen wohnet/", "tokens": ["Was", "in", "dem", "M\u00fcn\u00b7del\u00b7heim", "/", "zu", "M\u00fcn\u00b7ch", "und", "F\u00fc\u00b7\u00dfen", "woh\u00b7net", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "$(", "APPR", "TRUNC", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.125": {"text": "Zu Landsberg/ Ravenspurg und andern Orten mehr/", "tokens": ["Zu", "Lands\u00b7berg", "/", "Ra\u00b7ven\u00b7spurg", "und", "an\u00b7dern", "Or\u00b7ten", "mehr", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "NE", "KON", "ADJA", "NN", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.126": {"text": "Must\u2019 alles an das Joch/ dem Schwed- und Frantzen-Heer", "tokens": ["Must'", "al\u00b7les", "an", "das", "Joch", "/", "dem", "Schwe\u00b7d", "und", "Frant\u00b7zen\u00b7Heer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VMFIN", "PIS", "APPR", "ART", "NN", "$(", "ART", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.127": {"text": "Nach seiner Lust zu seyn. Was war es f\u00fcr ein hausen/", "tokens": ["Nach", "sei\u00b7ner", "Lust", "zu", "seyn", ".", "Was", "war", "es", "f\u00fcr", "ein", "hau\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VAINF", "$.", "PWS", "VAFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Als man am Boden-See die wolbeklippte Clausen", "tokens": ["Als", "man", "am", "Bo\u00b7den\u00b7See", "die", "wol\u00b7be\u00b7klipp\u00b7te", "Clau\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPRART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Mit samt Bregantz gewann? Was Sch\u00e4tze gab es da?", "tokens": ["Mit", "samt", "Breg\u00b7antz", "ge\u00b7wann", "?", "Was", "Sch\u00e4t\u00b7ze", "gab", "es", "da", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "NE", "VVFIN", "$.", "PWS", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Wo blieb die Gegen-Macht/ lie\u00df sie den Feind so nah", "tokens": ["Wo", "blieb", "die", "Ge\u00b7gen\u00b7Macht", "/", "lie\u00df", "sie", "den", "Feind", "so", "nah"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$(", "VVFIN", "PPER", "ART", "NN", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "An das Tyrolerland? Sie wolte viel dargegen/", "tokens": ["An", "das", "Ty\u00b7ro\u00b7ler\u00b7land", "?", "Sie", "wol\u00b7te", "viel", "dar\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "PPER", "VMFIN", "PIS", "PAV", "$("], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.132": {"text": "Kunt aber wenig thun/ wie dann der Schweden Degen", "tokens": ["Kunt", "a\u00b7ber", "we\u00b7nig", "thun", "/", "wie", "dann", "der", "Schwe\u00b7den", "De\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "PIS", "VVINF", "$(", "KOKOM", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "Auch dicht vor Lindau kam/ und es zur ", "tokens": ["Auch", "dicht", "vor", "Lin\u00b7dau", "kam", "/", "und", "es", "zur"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPR", "NE", "VVFIN", "$(", "KON", "PPER", "APPRART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.134": {"text": "Zu zwingen eufrig fiel/ lie\u00df aber endlich ab", "tokens": ["Zu", "zwin\u00b7gen", "euf\u00b7rig", "fiel", "/", "lie\u00df", "a\u00b7ber", "end\u00b7lich", "ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "ADJD", "VVFIN", "$(", "VVFIN", "ADV", "ADV", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Weil es zu m\u00e4chtig war. Nun wurde Fried in Beyern/", "tokens": ["Weil", "es", "zu", "m\u00e4ch\u00b7tig", "war", ".", "Nun", "wur\u00b7de", "Fried", "in", "Be\u00b7yern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKA", "ADJD", "VAFIN", "$.", "ADV", "VAFIN", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.136": {"text": "Weil man des Feindes Macht nicht anders kunte steuern/", "tokens": ["Weil", "man", "des", "Fein\u00b7des", "Macht", "nicht", "an\u00b7ders", "kun\u00b7te", "steu\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "NN", "PTKNEG", "ADV", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "Sprach man um Fried und Ruh. Man gieng es treulich", "tokens": ["Sprach", "man", "um", "Fried", "und", "Ruh", ".", "Man", "gieng", "es", "treu\u00b7lich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "PIS", "APPR", "NN", "KON", "NN", "$.", "PIS", "VVFIN", "PPER", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.138": {"text": "ein/", "tokens": ["ein", "/"], "token_info": ["word", "punct"], "pos": ["ART", "$("], "meter": "-", "measure": "single.down"}, "line.139": {"text": "Wie lang es dauerte sol bald berichtet seyn.", "tokens": ["Wie", "lang", "es", "dau\u00b7er\u00b7te", "sol", "bald", "be\u00b7rich\u00b7tet", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "VVFIN", "VMFIN", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "Als Wrangel f\u00fcr der Macht des Beyers sicher lebte/", "tokens": ["Als", "Wran\u00b7gel", "f\u00fcr", "der", "Macht", "des", "Be\u00b7yers", "si\u00b7cher", "leb\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "APPR", "ART", "NN", "ART", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.141": {"text": "und jhm des K\u00e4ysers Heer alleine wiederstrebte/", "tokens": ["und", "jhm", "des", "K\u00e4y\u00b7sers", "Heer", "al\u00b7lei\u00b7ne", "wie\u00b7der\u00b7streb\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "NN", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "Wiewol es in dem Feld\u2019 ein weniges betrieb/", "tokens": ["Wie\u00b7wol", "es", "in", "dem", "Feld'", "ein", "we\u00b7ni\u00b7ges", "be\u00b7trieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ART", "NN", "ART", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Gieng er von Beyern aus und f\u00fchrte seinen Hieb", "tokens": ["Gieng", "er", "von", "Be\u00b7yern", "aus", "und", "f\u00fchr\u00b7te", "sei\u00b7nen", "Hieb"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NN", "PTKVZ", "KON", "VVFIN", "PPOSAT", "NN"], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.144": {"text": "Auf das/ was K\u00e4ysrisch war/ vorau\u00df auf solche Pl\u00e4tze/", "tokens": ["Auf", "das", "/", "was", "K\u00e4y\u00b7srisch", "war", "/", "vor\u00b7au\u00df", "auf", "sol\u00b7che", "Pl\u00e4t\u00b7ze", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$(", "PWS", "NN", "VAFIN", "$(", "ADV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "Von welchen ich nur zween/ als die ber\u00fchmten setze.", "tokens": ["Von", "wel\u00b7chen", "ich", "nur", "zween", "/", "als", "die", "be\u00b7r\u00fchm\u00b7ten", "set\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "PPER", "ADV", "VVINF", "$(", "KOUS", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.146": {"text": "Der erst hieb Schweinfurt auf/ ist eine sch\u00f6ne Stadt/", "tokens": ["Der", "erst", "hieb", "Schwein\u00b7furt", "auf", "/", "ist", "ei\u00b7ne", "sch\u00f6\u00b7ne", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "NN", "APPR", "$(", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "Dergleichen man nicht viel im Lande Francken hat/", "tokens": ["Derg\u00b7lei\u00b7chen", "man", "nicht", "viel", "im", "Lan\u00b7de", "Fran\u00b7cken", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PIS", "PTKNEG", "ADV", "APPRART", "NN", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "In Francken/ da es ligt. Nach Schweinfurt galt es Eger/", "tokens": ["In", "Fran\u00b7cken", "/", "da", "es", "ligt", ".", "Nach", "Schwein\u00b7furt", "galt", "es", "E\u00b7ger", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KOUS", "PPER", "VVFIN", "$.", "APPR", "NE", "VVFIN", "PPER", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.149": {"text": "und halff da nichts daf\u00fcr/ wie starck des K\u00e4ysers L\u00e4ger/", "tokens": ["und", "halff", "da", "nichts", "da\u00b7f\u00fcr", "/", "wie", "starck", "des", "K\u00e4y\u00b7sers", "L\u00e4\u00b7ger", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIS", "PAV", "$(", "PWAV", "ADJD", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Das den Melander nun zum Feldherrn hatte/ kam/", "tokens": ["Das", "den", "Me\u00b7lan\u00b7der", "nun", "zum", "Feld\u00b7herrn", "hat\u00b7te", "/", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "ART", "NN", "ADV", "APPRART", "NN", "VAFIN", "$(", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.151": {"text": "Ja/ da\u00df der K\u00e4yser selbst die Stadt in Obacht nahm.", "tokens": ["Ja", "/", "da\u00df", "der", "K\u00e4y\u00b7ser", "selbst", "die", "Stadt", "in", "O\u00b7bacht", "nahm", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$(", "KOUS", "ART", "NN", "ADV", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.152": {"text": "Hier hatte Wrangel sich mit Wittenberg vermehret/", "tokens": ["Hier", "hat\u00b7te", "Wran\u00b7gel", "sich", "mit", "Wit\u00b7ten\u00b7berg", "ver\u00b7meh\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "PRF", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Der sich vor kurtzem hatt\u2019 in Schlesien gekehret/", "tokens": ["Der", "sich", "vor", "kurt\u00b7zem", "hatt'", "in", "Schle\u00b7si\u00b7en", "ge\u00b7keh\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "ADJA", "VAFIN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Dem Montecuculi und andren jhre Macht", "tokens": ["Dem", "Mon\u00b7te\u00b7cu\u00b7cu\u00b7li", "und", "an\u00b7dren", "jhre", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "PIS", "PPOSAT", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.155": {"text": "Zu stossen/ wurd\u2019 auch offt und wolbegl\u00fcckt verbracht.", "tokens": ["Zu", "stos\u00b7sen", "/", "wurd'", "auch", "offt", "und", "wol\u00b7be\u00b7gl\u00fcckt", "ver\u00b7bracht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VAFIN", "ADV", "ADV", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.156": {"text": "War Wrangel hier begl\u00fcckt/ Touraine dort imgleichen/", "tokens": ["War", "Wran\u00b7gel", "hier", "be\u00b7gl\u00fcckt", "/", "Tou\u00b7rai\u00b7ne", "dort", "im\u00b7glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "VVPP", "$(", "NE", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.157": {"text": "Dem alles Mentz und H\u00f6chst und Steinheim muste weiche\u0303/", "tokens": ["Dem", "al\u00b7les", "Mentz", "und", "H\u00f6chst", "und", "Stein\u00b7heim", "mus\u00b7te", "weich\u1ebd", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "KON", "NN", "KON", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "Ja alles Ertzstifft Meyntz. Er fiel auch Darmstadt an", "tokens": ["Ja", "al\u00b7les", "Ertz\u00b7stifft", "Meyntz", ".", "Er", "fiel", "auch", "Darm\u00b7stadt", "an"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "PIAT", "NN", "NE", "$.", "PPER", "VVFIN", "ADV", "NE", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "und that dem Cassel Hilff/ das nun den tapfren Mann", "tokens": ["und", "that", "dem", "Cas\u00b7sel", "Hilff", "/", "das", "nun", "den", "tapf\u00b7ren", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "$(", "PDS", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Mortaigne seiner Macht zum H\u00e4upt hatt\u2019 aufgesetzet/", "tokens": ["Mor\u00b7taig\u00b7ne", "sei\u00b7ner", "Macht", "zum", "H\u00e4upt", "hatt'", "auf\u00b7ge\u00b7set\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.161": {"text": "Er hatte sich nunmehr mit Schweden schon geletzet.", "tokens": ["Er", "hat\u00b7te", "sich", "nun\u00b7mehr", "mit", "Schwe\u00b7den", "schon", "ge\u00b7let\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "APPR", "NE", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "In dem di\u00df hier verlieff/ kam Post/ und gantz gewi\u00df/", "tokens": ["In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "/", "kam", "Post", "/", "und", "gantz", "ge\u00b7wi\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PDS", "ADV", "VVFIN", "$(", "VVFIN", "NN", "$(", "KON", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "Wie sich des Spannjers Macht nach Franckreich sehen lie\u00df.", "tokens": ["Wie", "sich", "des", "Spann\u00b7jers", "Macht", "nach", "Fran\u00b7ck\u00b7reich", "se\u00b7hen", "lie\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "NN", "APPR", "NE", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.164": {"text": "Daher Touraine sich nach Franckreich solt\u2019 erheben.", "tokens": ["Da\u00b7her", "Tou\u00b7rai\u00b7ne", "sich", "nach", "Fran\u00b7ck\u00b7reich", "solt'", "er\u00b7he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "NE", "PRF", "APPR", "NE", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.165": {"text": "Er thats/ man sah jhn auch sich \u00fcbern Reyhn begeben.", "tokens": ["Er", "thats", "/", "man", "sah", "jhn", "auch", "sich", "\u00fc\u00b7bern", "Reyhn", "be\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PIS", "VVFIN", "PPER", "ADV", "PRF", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "Als er ins Elsas kam und weiter fort gedacht\u2019/", "tokens": ["Als", "er", "ins", "El\u00b7sas", "kam", "und", "wei\u00b7ter", "fort", "ge\u00b7dacht'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NE", "VVFIN", "KON", "ADV", "PTKVZ", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "Entsponn sich unter jhm und seiner Deutschen Macht", "tokens": ["Ent\u00b7sponn", "sich", "un\u00b7ter", "jhm", "und", "sei\u00b7ner", "Deut\u00b7schen", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRF", "APPR", "PPER", "KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.168": {"text": "Ein Streit/ der m\u00e4chtig war. Er wolt in Franckreich ge-", "tokens": ["Ein", "Streit", "/", "der", "m\u00e4ch\u00b7tig", "war", ".", "Er", "wolt", "in", "Fran\u00b7ck\u00b7reich", "ge"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "ART", "ADJD", "VAFIN", "$.", "PPER", "VMFIN", "APPR", "NE", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.169": {"text": "Sie aber sagten: Nein/ wier dencken hier zu stehen", "tokens": ["Sie", "a\u00b7ber", "sag\u00b7ten", ":", "Nein", "/", "wier", "den\u00b7cken", "hier", "zu", "ste\u00b7hen"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "$.", "PTKANT", "$(", "ADV", "VVFIN", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Bi\u00df man uns richtig macht/ was man uns schuldig ist/", "tokens": ["Bi\u00df", "man", "uns", "rich\u00b7tig", "macht", "/", "was", "man", "uns", "schul\u00b7dig", "ist", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADJD", "VVFIN", "$(", "PWS", "PIS", "PPER", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Di\u00df machte beyde Theil\u2019 als offne Feind\u2019 entr\u00fcst.", "tokens": ["Di\u00df", "mach\u00b7te", "bey\u00b7de", "Theil'", "als", "off\u00b7ne", "Feind'", "ent\u00b7r\u00fcst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIAT", "NN", "KOUS", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Touraine nahm was blieb/ und gieng mit solchem Hauffen", "tokens": ["Tou\u00b7rai\u00b7ne", "nahm", "was", "blieb", "/", "und", "gieng", "mit", "sol\u00b7chem", "Hauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PIS", "VVFIN", "$(", "KON", "VVFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.173": {"text": "Nach Lothringen hinein/ wo zwischen das Verlauffen", "tokens": ["Nach", "Loth\u00b7rin\u00b7gen", "hin\u00b7ein", "/", "wo", "zwi\u00b7schen", "das", "Ver\u00b7lauf\u00b7fen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKVZ", "$(", "PWAV", "APPR", "ART", "NN"], "meter": "++-+-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.174": {"text": "Der hinterla\u00dfnen Macht jhm so zu Hertzen stieg/", "tokens": ["Der", "hin\u00b7ter\u00b7la\u00df\u00b7nen", "Macht", "jhm", "so", "zu", "Hert\u00b7zen", "stieg", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "Da\u00df er zu r\u00fccke gieng/ und einen neuen Krieg", "tokens": ["Da\u00df", "er", "zu", "r\u00fc\u00b7cke", "gieng", "/", "und", "ei\u00b7nen", "neu\u00b7en", "Krieg"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKZU", "VVFIN", "VVFIN", "$(", "KON", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.176": {"text": "Mit diesen V\u00f6lckern hielt. Er traff sie bey dem Meyhne/", "tokens": ["Mit", "die\u00b7sen", "V\u00f6l\u00b7ckern", "hielt", ".", "Er", "traff", "sie", "bey", "dem", "Meyh\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "Satzt an und schlug auf sie. Sie stellten jhre Beine", "tokens": ["Satzt", "an", "und", "schlug", "auf", "sie", ".", "Sie", "stell\u00b7ten", "jhre", "Bei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "PTKVZ", "KON", "VVFIN", "APPR", "PPER", "$.", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.178": {"text": "und gaben eben das/ was man auf sie betrieb/", "tokens": ["und", "ga\u00b7ben", "e\u00b7ben", "das", "/", "was", "man", "auf", "sie", "be\u00b7trieb", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "$(", "PWS", "PIS", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Da\u00df also beyderseits nicht wenig sitzen blieb.", "tokens": ["Da\u00df", "al\u00b7so", "bey\u00b7der\u00b7seits", "nicht", "we\u00b7nig", "sit\u00b7zen", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "PTKNEG", "ADV", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "Nach diesem setzten sie zwey tausend starck auf Pferden", "tokens": ["Nach", "die\u00b7sem", "setz\u00b7ten", "sie", "zwey", "tau\u00b7send", "starck", "auf", "Pfer\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "CARD", "CARD", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "Bey W\u00fcrtzburg durch den Meyhn/ und suchten nach der", "tokens": ["Bey", "W\u00fcrtz\u00b7burg", "durch", "den", "Meyhn", "/", "und", "such\u00b7ten", "nach", "der"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "$(", "KON", "VVFIN", "APPR", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.182": {"text": "Erden/", "tokens": ["Er\u00b7den", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.183": {"text": "Die K\u00f6nigsmarck betrat/ der jetzund seinen Krieg", "tokens": ["Die", "K\u00f6\u00b7nigs\u00b7marck", "be\u00b7trat", "/", "der", "je\u00b7tzund", "sei\u00b7nen", "Krieg"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$(", "ART", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Mit vielen St\u00e4dten hatt\u2019. Er hatte Krieg und Sieg.", "tokens": ["Mit", "vie\u00b7len", "St\u00e4d\u00b7ten", "hatt'", ".", "Er", "hat\u00b7te", "Krieg", "und", "Sieg", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "$.", "PPER", "VAFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.185": {"text": "Die Fechte wurde seyn/ die Fastenau imgleichen/", "tokens": ["Die", "Fech\u00b7te", "wur\u00b7de", "seyn", "/", "die", "Fas\u00b7te\u00b7nau", "im\u00b7glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VAINF", "$(", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "Es must\u2019 auch Weydenbr\u00fcck vor jhm die Segel streichen.", "tokens": ["Es", "must'", "auch", "Wey\u00b7den\u00b7br\u00fcck", "vor", "jhm", "die", "Se\u00b7gel", "strei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "NN", "APPR", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Jtzt aber hielt\u2019 er sich sehr nah um Paderborn.", "tokens": ["Jtzt", "a\u00b7ber", "hielt'", "er", "sich", "sehr", "nah", "um", "Pa\u00b7der\u00b7born", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "ADV", "ADJD", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Touraine aber nahm aus Rach Befehl und Zorn", "tokens": ["Tou\u00b7rai\u00b7ne", "a\u00b7ber", "nahm", "aus", "Rach", "Be\u00b7fehl", "und", "Zorn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "VVFIN", "APPR", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "Den Rosa/ welcher sie vor dieser Anffruhr f\u00fchrte/", "tokens": ["Den", "Ro\u00b7sa", "/", "wel\u00b7cher", "sie", "vor", "die\u00b7ser", "Anf\u00b7fruhr", "f\u00fchr\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PPER", "APPR", "PDAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "Gefangen/ meynende/ da\u00df es von jhme r\u00fchrte/", "tokens": ["Ge\u00b7fan\u00b7gen", "/", "meyn\u00b7en\u00b7de", "/", "da\u00df", "es", "von", "jh\u00b7me", "r\u00fchr\u00b7te", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "$(", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.191": {"text": "Da er doch ausser Schuld und/ sicher/ immerdar", "tokens": ["Da", "er", "doch", "aus\u00b7ser", "Schuld", "und", "/", "si\u00b7cher", "/", "im\u00b7mer\u00b7dar"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KOUS", "PPER", "ADV", "ADJA", "NN", "KON", "$(", "ADJD", "$(", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Ein treuer General bey dieser Krohne war.", "tokens": ["Ein", "treu\u00b7er", "Ge\u00b7ne\u00b7ral", "bey", "die\u00b7ser", "Kroh\u00b7ne", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PDAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "Es wurd\u2019 auch mit der Zeit von Franckreich selbst erkennet/", "tokens": ["Es", "wurd'", "auch", "mit", "der", "Zeit", "von", "Fran\u00b7ck\u00b7reich", "selbst", "er\u00b7ken\u00b7net", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "ART", "NN", "APPR", "NE", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.194": {"text": "So da\u00df man jhn nun neu derselben treuen nennet/", "tokens": ["So", "da\u00df", "man", "jhn", "nun", "neu", "der\u00b7sel\u00b7ben", "treu\u00b7en", "nen\u00b7net", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PIS", "PPER", "ADV", "ADJD", "PDAT", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Wie er dann dieser Zeit sie wiederum bedient/", "tokens": ["Wie", "er", "dann", "die\u00b7ser", "Zeit", "sie", "wie\u00b7de\u00b7rum", "be\u00b7dient", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PDAT", "NN", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "und in der alten Ehr\u2019 auch noch viel gr\u00f6\u00dfer gr\u00fcnt.", "tokens": ["und", "in", "der", "al\u00b7ten", "Ehr'", "auch", "noch", "viel", "gr\u00f6\u00b7\u00dfer", "gr\u00fcnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ADV", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Da\u00df wir nun wiederum auf diese V\u00f6lcker kommen/", "tokens": ["Da\u00df", "wir", "nun", "wie\u00b7de\u00b7rum", "auf", "die\u00b7se", "V\u00f6l\u00b7cker", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "APPR", "PDAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.198": {"text": "So hat sie K\u00f6nigsmarck gar gern an sich genommen/", "tokens": ["So", "hat", "sie", "K\u00f6\u00b7nigs\u00b7marck", "gar", "gern", "an", "sich", "ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "ADV", "ADV", "APPR", "PRF", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Weil er sie sonst zum Feind gewillt und fertig sah/", "tokens": ["Weil", "er", "sie", "sonst", "zum", "Feind", "ge\u00b7willt", "und", "fer\u00b7tig", "sah", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "APPRART", "NN", "VVPP", "KON", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Was sie begehreten/ das war jhm alles ja", "tokens": ["Was", "sie", "be\u00b7ge\u00b7hre\u00b7ten", "/", "das", "war", "jhm", "al\u00b7les", "ja"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "VVFIN", "$(", "PDS", "VAFIN", "PPER", "PIS", "ADV"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.201": {"text": "und schrifftlich auffgesetzt. Hierauff war er entschlossen/", "tokens": ["und", "schrifft\u00b7lich", "auff\u00b7ge\u00b7setzt", ".", "Hier\u00b7auff", "war", "er", "ent\u00b7schlos\u00b7sen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$.", "PAV", "VAFIN", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.202": {"text": "Mit dies- und seiner Macht zur Haupt-Armee zu sto\u00dfen/", "tokens": ["Mit", "dies", "und", "sei\u00b7ner", "Macht", "zur", "Haup\u00b7tAr\u00b7mee", "zu", "sto\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "TRUNC", "KON", "PPOSAT", "NN", "APPRART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Die nun umb Eger war. Di\u00df kunte doch nicht seyn/", "tokens": ["Die", "nun", "umb", "E\u00b7ger", "war", ".", "Di\u00df", "kun\u00b7te", "doch", "nicht", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NE", "VAFIN", "$.", "PDS", "VMFIN", "ADV", "PTKNEG", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "Dann General Lamboy fiel in Ost Frie\u00dfland ein/", "tokens": ["Dann", "Ge\u00b7ne\u00b7ral", "Lam\u00b7boy", "fiel", "in", "Ost", "Frie\u00df\u00b7land", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "NE", "VVFIN", "APPR", "NE", "NE", "ART", "$("], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.205": {"text": "Die Cassel-hessische darinnen auff zu reiben/", "tokens": ["Die", "Cas\u00b7sel\u00b7hes\u00b7si\u00b7sche", "da\u00b7rin\u00b7nen", "auff", "zu", "rei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Daher er/ diesen Feind in seinen Platz zu treiben/", "tokens": ["Da\u00b7her", "er", "/", "die\u00b7sen", "Feind", "in", "sei\u00b7nen", "Platz", "zu", "trei\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "$(", "PDAT", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Sich wieder r\u00fcckwerts gab/ jedoch must Hammerstein/", "tokens": ["Sich", "wie\u00b7der", "r\u00fcck\u00b7werts", "gab", "/", "je\u00b7doch", "must", "Ham\u00b7mer\u00b7stein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "VVFIN", "$(", "ADV", "VMFIN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "Sein General Major f\u00fcr jhn vor Eger seyn/", "tokens": ["Sein", "Ge\u00b7ne\u00b7ral", "Ma\u00b7jor", "f\u00fcr", "jhn", "vor", "E\u00b7ger", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "APPR", "PPER", "APPR", "NE", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Der auch zwey tausend starck sich schleunig hin verf\u00fcgte/", "tokens": ["Der", "auch", "zwey", "tau\u00b7send", "starck", "sich", "schleu\u00b7nig", "hin", "ver\u00b7f\u00fcg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "CARD", "CARD", "NN", "PRF", "ADJD", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "Weil man daselbst sehr scharff vons K\u00e4ysers Seiten krieg-", "tokens": ["Weil", "man", "da\u00b7selbst", "sehr", "scharff", "vons", "K\u00e4y\u00b7sers", "Sei\u00b7ten", "krieg"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "PAV", "ADV", "ADJD", "ADJA", "NN", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.211": {"text": "War hier der Hilff von thun. Was nun mit Hammerstein", "tokens": ["War", "hier", "der", "Hilff", "von", "thun", ".", "Was", "nun", "mit", "Ham\u00b7mer\u00b7stein"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "NN", "APPR", "VVINF", "$.", "PWS", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "Entgieng/ bracht Hessen bald mit andern wieder ein/", "tokens": ["Ent\u00b7gieng", "/", "bracht", "Hes\u00b7sen", "bald", "mit", "an\u00b7dern", "wie\u00b7der", "ein", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "VVFIN", "NE", "ADV", "APPR", "PIS", "ADV", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Lamboyens seine Macht genugsam zu bestreiten.", "tokens": ["Lam\u00b7bo\u00b7yens", "sei\u00b7ne", "Macht", "ge\u00b7nug\u00b7sam", "zu", "be\u00b7strei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Stund\u2019 also Rabenhaupt an des Versandten Seiten.", "tokens": ["Stund'", "al\u00b7so", "Ra\u00b7ben\u00b7haupt", "an", "des", "Ver\u00b7sand\u00b7ten", "Sei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "NN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "Ich sage Rabenhaupt/ ein General Major/", "tokens": ["Ich", "sa\u00b7ge", "Ra\u00b7ben\u00b7haupt", "/", "ein", "Ge\u00b7ne\u00b7ral", "Ma\u00b7jor", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$(", "ART", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "In der von Cassel Pflicht/ von welchem hiebevor", "tokens": ["In", "der", "von", "Cas\u00b7sel", "Pflicht", "/", "von", "wel\u00b7chem", "hie\u00b7be\u00b7vor"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NE", "NN", "$(", "APPR", "PWAT", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Noch wenig Meldung war. La\u00df uns zum Ende kommen.", "tokens": ["Noch", "we\u00b7nig", "Mel\u00b7dung", "war", ".", "La\u00df", "uns", "zum", "En\u00b7de", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VAFIN", "$.", "VVIMP", "PPER", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "Was that dann K\u00f6nigsmarck/ was hatt\u2019 er vorgenommen?", "tokens": ["Was", "that", "dann", "K\u00f6\u00b7nigs\u00b7marck", "/", "was", "hatt'", "er", "vor\u00b7ge\u00b7nom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "NN", "$(", "PWS", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.219": {"text": "Bestritt\u2019 er den Lamboy? Er bracht jhn auch so weit", "tokens": ["Be\u00b7stritt'", "er", "den", "Lam\u00b7boy", "?", "Er", "bracht", "jhn", "auch", "so", "weit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV", "ADV", "ADJD"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.220": {"text": "Da\u00df er nach Meppen wich/ und sich vor einem Streit", "tokens": ["Da\u00df", "er", "nach", "Mep\u00b7pen", "wich", "/", "und", "sich", "vor", "ei\u00b7nem", "Streit"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NE", "VVFIN", "$(", "KON", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.221": {"text": "Bey Rehnen in ein Werck von festen Schantzen setzte.", "tokens": ["Bey", "Reh\u00b7nen", "in", "ein", "Werck", "von", "fes\u00b7ten", "Schant\u00b7zen", "setz\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Da\u00df jhm der Weymar-Hauff viel Volck darnider metzte/", "tokens": ["Da\u00df", "jhm", "der", "Wey\u00b7ma\u00b7rHauff", "viel", "Volck", "dar\u00b7ni\u00b7der", "metz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PIAT", "NN", "PAV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "Wird nicht geleugnet seyn/ er brauchte keinen Glimpf/", "tokens": ["Wird", "nicht", "ge\u00b7leug\u00b7net", "seyn", "/", "er", "brauch\u00b7te", "kei\u00b7nen", "Glimpf", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "VVPP", "VAINF", "$(", "PPER", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.224": {"text": "Weil des Lamboy Heer demselben manchen Schimpf", "tokens": ["Weil", "des", "Lam\u00b7boy", "Heer", "dem\u00b7sel\u00b7ben", "man\u00b7chen", "Schimpf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "NN", "PDAT", "PIAT", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.225": {"text": "und Spott entbieten lie\u00df und gar Rebellisch nannte/", "tokens": ["und", "Spott", "ent\u00b7bie\u00b7ten", "lie\u00df", "und", "gar", "Re\u00b7bel\u00b7lisch", "nann\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "VVFIN", "KON", "ADV", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "Wof\u00fcr sein Hertz f\u00fcr Zorn als einem L\u00f6uen brandte.", "tokens": ["Wo\u00b7f\u00fcr", "sein", "Hertz", "f\u00fcr", "Zorn", "als", "ei\u00b7nem", "L\u00f6u\u00b7en", "brand\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "NN", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.227": {"text": "Nachdem Lamboyens Volck in solche Schw\u00e4che fiel/", "tokens": ["Nach\u00b7dem", "Lam\u00b7bo\u00b7yens", "Volck", "in", "sol\u00b7che", "Schw\u00e4\u00b7che", "fiel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NN", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.228": {"text": "Gieng aller Donner-St\u00fcck und der Mortirer Spiel", "tokens": ["Gieng", "al\u00b7ler", "Don\u00b7ner\u00b7St\u00fcck", "und", "der", "Mor\u00b7ti\u00b7rer", "Spiel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIAT", "NN", "KON", "ART", "NN", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.229": {"text": "Auff Rehnen/ welche Stadt Lamboyens Rest verpflegte/", "tokens": ["Auff", "Reh\u00b7nen", "/", "wel\u00b7che", "Stadt", "Lam\u00b7bo\u00b7yens", "Rest", "ver\u00b7pfleg\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "PWAT", "NN", "NE", "NN", "VVFIN", "$("], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}, "line.230": {"text": "Es w\u00e4hrte/ bi\u00df sie sich in Brand und Aschen legte.", "tokens": ["Es", "w\u00e4hr\u00b7te", "/", "bi\u00df", "sie", "sich", "in", "Brand", "und", "A\u00b7schen", "leg\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KOUS", "PPER", "PRF", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Nach Legung dieser Stadt und des Lamboy Macht/", "tokens": ["Nach", "Le\u00b7gung", "die\u00b7ser", "Stadt", "und", "des", "Lam\u00b7boy", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PDAT", "NN", "KON", "ART", "NN", "NN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.232": {"text": "Die/ vieler Meynung nach/ sehr weit hatt\u2019 au\u00dfgedacht/", "tokens": ["Die", "/", "vie\u00b7ler", "Mey\u00b7nung", "nach", "/", "sehr", "weit", "hatt'", "au\u00df\u00b7ge\u00b7dacht", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PIAT", "NN", "APPR", "$(", "ADV", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Beflammte K\u00f6nigsmarck sein auffgesetztes L\u00e4ger/", "tokens": ["Be\u00b7flamm\u00b7te", "K\u00f6\u00b7nigs\u00b7marck", "sein", "auff\u00b7ge\u00b7setz\u00b7tes", "L\u00e4\u00b7ger", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "und gieng mit aller Macht zur Haupt-Armee vor Eger.", "tokens": ["und", "gieng", "mit", "al\u00b7ler", "Macht", "zur", "Haup\u00b7tAr\u00b7mee", "vor", "E\u00b7ger", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "APPRART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Lamboy solt\u2019 es auch/ er wolt\u2019 es doch nicht thun.", "tokens": ["Lam\u00b7boy", "solt'", "es", "auch", "/", "er", "wolt'", "es", "doch", "nicht", "thun", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "ADV", "$(", "PPER", "VMFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.236": {"text": "Es kunt\u2019 auch fast nicht seyn/ weil jhm die Hessen nun/", "tokens": ["Es", "kunt'", "auch", "fast", "nicht", "seyn", "/", "weil", "jhm", "die", "Hes\u00b7sen", "nun", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "PTKNEG", "VAINF", "$(", "KOUS", "PPER", "ART", "NN", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.237": {"text": "Als zimlich siegende/ sehr viel zu schaffen machten/", "tokens": ["Als", "zim\u00b7lich", "sie\u00b7gen\u00b7de", "/", "sehr", "viel", "zu", "schaf\u00b7fen", "mach\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADJA", "$(", "ADV", "PIS", "PTKZU", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "und dort und da ein Ort in jhre H\u00e4nde brachten/", "tokens": ["und", "dort", "und", "da", "ein", "Ort", "in", "jhre", "H\u00e4n\u00b7de", "brach\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KON", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.239": {"text": "Di\u00df hielt jhn viel zu r\u00fcck. ", "tokens": ["Di\u00df", "hielt", "jhn", "viel", "zu", "r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PIS", "PTKZU", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.240": {"text": "Brach alle Waffen-Macht des Beyers wieder auff/", "tokens": ["Brach", "al\u00b7le", "Waf\u00b7fen\u00b7Macht", "des", "Be\u00b7yers", "wie\u00b7der", "auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ART", "NN", "ADV", "APPR", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.241": {"text": "Auffs neue wieder die von Schweden lo\u00df zu gehen/", "tokens": ["Auffs", "neu\u00b7e", "wie\u00b7der", "die", "von", "Schwe\u00b7den", "lo\u00df", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "ADV", "ART", "APPR", "NE", "PTKVZ", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "und also Ferdinands Partheyen bey zu stehen.", "tokens": ["und", "al\u00b7so", "Fer\u00b7di\u00b7nands", "Par\u00b7the\u00b7yen", "bey", "zu", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "NN", "APPR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Hilff GOtt wie sah man auf/ als man die Zeitung hatt\u2019/", "tokens": ["Hilff", "Gott", "wie", "sah", "man", "auf", "/", "als", "man", "die", "Zei\u00b7tung", "hatt'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KOKOM", "VVFIN", "PIS", "APPR", "$(", "KOUS", "PIS", "ART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.244": {"text": "Es st\u00fcndte seine Macht bereits vor einer Stadt/", "tokens": ["Es", "st\u00fcnd\u00b7te", "sei\u00b7ne", "Macht", "be\u00b7reits", "vor", "ei\u00b7ner", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.245": {"text": "Die Schwedisch Volck in hatt/ als Memmingen in Schwa-", "tokens": ["Die", "Schwe\u00b7disch", "Volck", "in", "hatt", "/", "als", "Mem\u00b7min\u00b7gen", "in", "Schwa"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "APPR", "VAFIN", "$(", "KOUS", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.246": {"text": "und wolte das heraus und sich darinnen haben.", "tokens": ["und", "wol\u00b7te", "das", "he\u00b7raus", "und", "sich", "da\u00b7rin\u00b7nen", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PDS", "PTKVZ", "KON", "PRF", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "Es lieff auch mit der Zeit nach jhrer Meynung aus/", "tokens": ["Es", "lieff", "auch", "mit", "der", "Zeit", "nach", "jhrer", "Mey\u00b7nung", "aus", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.248": {"text": "Jedoch sehr hoch erkaufft/ dann es war mancher Strau\u00df", "tokens": ["Je\u00b7doch", "sehr", "hoch", "er\u00b7kaufft", "/", "dann", "es", "war", "man\u00b7cher", "Strau\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "VVPP", "$(", "ADV", "PPER", "VAFIN", "PIAT", "NN"], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}, "line.249": {"text": "Eh es so weit gerieth. Was wurd hiemit verrichtet?", "tokens": ["Eh", "es", "so", "weit", "ge\u00b7rieth", ".", "Was", "wurd", "hie\u00b7mit", "ver\u00b7rich\u00b7tet", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$.", "PWS", "VAFIN", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Nicht mehr als Beyern selbst fast auf den Grund vernichtet/", "tokens": ["Nicht", "mehr", "als", "Be\u00b7yern", "selbst", "fast", "auf", "den", "Grund", "ver\u00b7nich\u00b7tet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "KOUS", "NN", "ADV", "ADV", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.251": {"text": "Wie balde folgen sol. Als Ferdinands Gewalt", "tokens": ["Wie", "bal\u00b7de", "fol\u00b7gen", "sol", ".", "Als", "Fer\u00b7di\u00b7nands", "Ge\u00b7walt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "ADV", "VVINF", "VMFIN", "$.", "KOUS", "NE", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Mit der von Beyern sich in voriger Gestalt", "tokens": ["Mit", "der", "von", "Be\u00b7yern", "sich", "in", "vo\u00b7ri\u00b7ger", "Ge\u00b7stalt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NN", "PRF", "APPR", "ADJA", "NN"], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.253": {"text": "und altem Glauben sah/ erhob sie jhre Fl\u00fcgel/", "tokens": ["und", "al\u00b7tem", "Glau\u00b7ben", "sah", "/", "er\u00b7hob", "sie", "jhre", "Fl\u00fc\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVFIN", "$(", "VVFIN", "PPER", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.254": {"text": "In Meynung jhren Feind/ den Schweden/ aus dem B\u00fcgel", "tokens": ["In", "Mey\u00b7nung", "jhren", "Feind", "/", "den", "Schwe\u00b7den", "/", "aus", "dem", "B\u00fc\u00b7gel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "PPOSAT", "NN", "$(", "ART", "NE", "$(", "APPR", "ART", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.255": {"text": "und Sattel au\u00df zu thun/ der nun mit gro\u00dfer Schaar", "tokens": ["und", "Sat\u00b7tel", "au\u00df", "zu", "thun", "/", "der", "nun", "mit", "gro\u00b7\u00dfer", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "PTKVZ", "PTKZU", "VVINF", "$(", "ART", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Von Eger fort nach Prag zu gehen r\u00fcstig war.", "tokens": ["Von", "E\u00b7ger", "fort", "nach", "Prag", "zu", "ge\u00b7hen", "r\u00fcs\u00b7tig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKVZ", "APPR", "NE", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "Es kam zu manchem Streit/ jedoch zu keinen Schlachten/", "tokens": ["Es", "kam", "zu", "man\u00b7chem", "Streit", "/", "je\u00b7doch", "zu", "kei\u00b7nen", "Schlach\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "$(", "ADV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Weil beyde Theile wol auf jhre Vortheil wachten.", "tokens": ["Weil", "bey\u00b7de", "Thei\u00b7le", "wol", "auf", "jhre", "Vor\u00b7theil", "wach\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.259": {"text": "Helm Wrangel hieb einmal so tieff ins K\u00e4ysers Heer/", "tokens": ["Helm", "Wran\u00b7gel", "hieb", "ein\u00b7mal", "so", "tieff", "ins", "K\u00e4y\u00b7sers", "Heer", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "ADV", "ADV", "ADJD", "APPRART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "Da\u00df er mit wenigen nicht wenig Beuth\u2019 und Ehr", "tokens": ["Da\u00df", "er", "mit", "we\u00b7ni\u00b7gen", "nicht", "we\u00b7nig", "Beuth'", "und", "Ehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "PTKNEG", "PIAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.261": {"text": "Erlangte. Ja man sprach: H\u00e4tt\u2019 er sich recht versehen", "tokens": ["Er\u00b7lang\u00b7te", ".", "Ja", "man", "sprach", ":", "H\u00e4tt'", "er", "sich", "recht", "ver\u00b7se\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$.", "PTKANT", "PIS", "VVFIN", "$.", "VVFIN", "PPER", "PRF", "ADJD", "VVINF"], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}, "line.262": {"text": "und Volck zur hand gehabt/ es w\u00e4re was geschehen/", "tokens": ["und", "Volck", "zur", "hand", "ge\u00b7habt", "/", "es", "w\u00e4\u00b7re", "was", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPRART", "NN", "VAPP", "$(", "PPER", "VAFIN", "PIS", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "Das kaum zu hoffen war. Nichts minders that de W\u00f6rt/", "tokens": ["Das", "kaum", "zu", "hof\u00b7fen", "war", ".", "Nichts", "min\u00b7ders", "that", "de", "W\u00f6rt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PTKZU", "VVINF", "VAFIN", "$.", "PIS", "ADV", "VVFIN", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Der solches Ding zu thun au\u00dfb\u00fcndig war gelehrt/", "tokens": ["Der", "sol\u00b7ches", "Ding", "zu", "thun", "au\u00df\u00b7b\u00fcn\u00b7dig", "war", "ge\u00b7lehrt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "PTKZU", "VVINF", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "In einer duncklen Nacht vor Eger an den Schweden.", "tokens": ["In", "ei\u00b7ner", "dunck\u00b7len", "Nacht", "vor", "E\u00b7ger", "an", "den", "Schwe\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "NE", "APPR", "ART", "NE", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.266": {"text": "Ich wei\u00df nicht welchem hier das Siegen nach zu reden.", "tokens": ["Ich", "wei\u00df", "nicht", "wel\u00b7chem", "hier", "das", "Sie\u00b7gen", "nach", "zu", "re\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PWAT", "ADV", "ART", "NN", "APPR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.267": {"text": "Es blieben beyderseits viel hohe tapfre Leuth/", "tokens": ["Es", "blie\u00b7ben", "bey\u00b7der\u00b7seits", "viel", "ho\u00b7he", "tapf\u00b7re", "Leuth", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Als Obristen und mehr/ und wurd\u2019 insonderheit", "tokens": ["Als", "O\u00b7bris\u00b7ten", "und", "mehr", "/", "und", "wurd'", "in\u00b7son\u00b7der\u00b7heit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "ADV", "$(", "KON", "VAFIN", "ADV"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.269": {"text": "Helm Wrangel hoch beklagt/ der nun auf langes Kriegen", "tokens": ["Helm", "Wran\u00b7gel", "hoch", "be\u00b7klagt", "/", "der", "nun", "auf", "lan\u00b7ges", "Krie\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "ADJD", "VVPP", "$(", "ART", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "und fieler Feinde Tod\u2019 erschossen must\u2019 erliegen/", "tokens": ["und", "fie\u00b7ler", "Fein\u00b7de", "Tod'", "er\u00b7schos\u00b7sen", "must'", "er\u00b7lie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "NN", "VVINF", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Gemeiner blieben mehr als zweymal tausend Mann/", "tokens": ["Ge\u00b7mei\u00b7ner", "blie\u00b7ben", "mehr", "als", "zwey\u00b7mal", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "KOKOM", "ADV", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Verstehe beyderseits vom Leben abgethan.", "tokens": ["Ver\u00b7ste\u00b7he", "bey\u00b7der\u00b7seits", "vom", "Le\u00b7ben", "ab\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Hierauf gieng alle Macht der Schweden theils zur Elbe/", "tokens": ["Hier\u00b7auf", "gieng", "al\u00b7le", "Macht", "der", "Schwe\u00b7den", "theils", "zur", "El\u00b7be", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIAT", "NN", "ART", "NE", "ADV", "APPRART", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "Theils zu der Weser hin/ zu ruhen/ weil dieselbe", "tokens": ["Theils", "zu", "der", "We\u00b7ser", "hin", "/", "zu", "ru\u00b7hen", "/", "weil", "die\u00b7sel\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["NN", "APPR", "ART", "NN", "PTKVZ", "$(", "PTKZU", "VVINF", "$(", "KOUS", "PDAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "Sehr Krafft- und Pferdlo\u00df war. Wo blieb die Gegen-", "tokens": ["Sehr", "Krafft", "und", "Pferd\u00b7lo\u00df", "war", ".", "Wo", "blieb", "die", "Ge\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "TRUNC", "KON", "NN", "VAFIN", "$.", "PWAV", "VVFIN", "ART", "TRUNC"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.276": {"text": "Die gieng auf Cassel zu/ und nahm es nicht in acht/", "tokens": ["Die", "gieng", "auf", "Cas\u00b7sel", "zu", "/", "und", "nahm", "es", "nicht", "in", "acht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "PTKZU", "$(", "KON", "VVFIN", "PPER", "PTKNEG", "APPR", "CARD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Da\u00df sich der Schweden-Hauff in Nieder-Sachsen st\u00e4rckte/", "tokens": ["Da\u00df", "sich", "der", "Schwe\u00b7den\u00b7Hauff", "in", "Nie\u00b7der\u00b7Sach\u00b7sen", "st\u00e4rck\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.278": {"text": "Wiewol man es hernach/ doch allzu spat/ verm\u00e4rckte.", "tokens": ["Wie\u00b7wol", "man", "es", "her\u00b7nach", "/", "doch", "all\u00b7zu", "spat", "/", "ver\u00b7m\u00e4rck\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "$(", "ADV", "PTKA", "ADJD", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "Was man in Hessen that/ war dieses fast allein/", "tokens": ["Was", "man", "in", "Hes\u00b7sen", "that", "/", "war", "die\u00b7ses", "fast", "al\u00b7lein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "NE", "VVFIN", "$(", "VAFIN", "PDAT", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "Da\u00df man das Land verdorb. Man nahm auch Marburg", "tokens": ["Da\u00df", "man", "das", "Land", "ver\u00b7dorb", ".", "Man", "nahm", "auch", "Mar\u00b7burg"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "NN", "VVFIN", "$.", "PIS", "VVFIN", "ADV", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.281": {"text": "ein/", "tokens": ["ein", "/"], "token_info": ["word", "punct"], "pos": ["ART", "$("], "meter": "-", "measure": "single.down"}, "line.282": {"text": "Doch nur die blo\u00dfe Stadt/ das Schlo\u00df scho\u00df hart darwider/", "tokens": ["Doch", "nur", "die", "blo\u00b7\u00dfe", "Stadt", "/", "das", "Schlo\u00df", "scho\u00df", "hart", "dar\u00b7wi\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$(", "ART", "NN", "VVFIN", "ADJD", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "und warff Melandern selbst in eine Schwachheit nieder.", "tokens": ["und", "warff", "Me\u00b7lan\u00b7dern", "selbst", "in", "ei\u00b7ne", "Schwach\u00b7heit", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NN", "ADV", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Damit so war der Streit mit Marburgs Schlo\u00df verbracht.", "tokens": ["Da\u00b7mit", "so", "war", "der", "Streit", "mit", "Mar\u00b7burgs", "Schlo\u00df", "ver\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VAFIN", "ART", "NN", "APPR", "NE", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "Und nun hielt Wrangels Hand bey des Touraine Macht", "tokens": ["Und", "nun", "hielt", "Wran\u00b7gels", "Hand", "bey", "des", "Tou\u00b7rai\u00b7ne", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "NE", "NN", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.286": {"text": "um einen Einbruch an/ Jtalien an dem Reyhne", "tokens": ["um", "ei\u00b7nen", "Ein\u00b7bruch", "an", "/", "Jta\u00b7li\u00b7en", "an", "dem", "Reyh\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "PTKVZ", "$(", "NE", "APPR", "ART", "NN"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.287": {"text": "Das ist die ", "tokens": ["Das", "ist", "die"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VAFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.288": {"text": "Die Deutschland haben mag/ zu st\u00fcrmen. Er griff an", "tokens": ["Die", "Deutschland", "ha\u00b7ben", "mag", "/", "zu", "st\u00fcr\u00b7men", ".", "Er", "griff", "an"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VAINF", "VMFIN", "$(", "PTKZU", "VVINF", "$.", "PPER", "VVFIN", "PTKVZ"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.289": {"text": "und gieng mit/ ungefehr/ sechs sieben tausend Mann", "tokens": ["und", "gieng", "mit", "/", "un\u00b7ge\u00b7fehr", "/", "sechs", "sie\u00b7ben", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "$(", "ADJD", "$(", "CARD", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Vereufert durch das Land/ fuhr \u00fcber Reyhn in Francken", "tokens": ["Ver\u00b7eu\u00b7fert", "durch", "das", "Land", "/", "fuhr", "\u00fc\u00b7ber", "Reyhn", "in", "Fran\u00b7cken"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVPP", "APPR", "ART", "NN", "$(", "VVFIN", "APPR", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "und ruhete bey Lor (ein Ort in lautern Rancken)", "tokens": ["und", "ru\u00b7he\u00b7te", "bey", "Lor", "(", "ein", "Ort", "in", "lau\u00b7tern", "Ran\u00b7cken", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "$(", "ART", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Bi\u00df sich die Wrangels Macht jhm n\u00e4her an die Hand", "tokens": ["Bi\u00df", "sich", "die", "Wran\u00b7gels", "Macht", "jhm", "n\u00e4\u00b7her", "an", "die", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRF", "ART", "NN", "NN", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Verf\u00fcgte/ welche sich bereits durchs Hessen Land", "tokens": ["Ver\u00b7f\u00fcg\u00b7te", "/", "wel\u00b7che", "sich", "be\u00b7reits", "durchs", "Hes\u00b7sen", "Land"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "PRELS", "PRF", "ADV", "APPRART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Mit Krafft und Eufer zog. Mit Krafft/ sie war mit Pfer-", "tokens": ["Mit", "Krafft", "und", "Eu\u00b7fer", "zog", ".", "Mit", "Krafft", "/", "sie", "war", "mit", "Pfer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NE", "VVFIN", "$.", "APPR", "NN", "$(", "PPER", "VAFIN", "APPR", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "und allem wol versehn/ mit eufrigen Geberden/", "tokens": ["und", "al\u00b7lem", "wol", "ver\u00b7sehn", "/", "mit", "euf\u00b7ri\u00b7gen", "Ge\u00b7ber\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADV", "VVINF", "$(", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "Weil Beyern anders that/ als es versprochen hatt\u2019/", "tokens": ["Weil", "Be\u00b7yern", "an\u00b7ders", "that", "/", "als", "es", "ver\u00b7spro\u00b7chen", "hatt'", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "VVFIN", "$(", "KOUS", "PPER", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "und welches Macht an jetzt um Schweinfurt/ eine Stadt", "tokens": ["und", "wel\u00b7ches", "Macht", "an", "jetzt", "um", "Schwein\u00b7furt", "/", "ei\u00b7ne", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "PWAT", "NN", "APPR", "ADV", "APPR", "NE", "$(", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Mit Schwedischen besetzt/ ein schrecklich Lager machte/", "tokens": ["Mit", "Schwe\u00b7di\u00b7schen", "be\u00b7setzt", "/", "ein", "schreck\u00b7lich", "La\u00b7ger", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$(", "ART", "ADJD", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.299": {"text": "Das gleichwol anders nichts als M\u00fch und Kosten brachte.", "tokens": ["Das", "gleich\u00b7wol", "an\u00b7ders", "nichts", "als", "M\u00fch", "und", "Kos\u00b7ten", "brach\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "PIS", "KOKOM", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Nach dem Melander fah/ was Wrangel willens war/", "tokens": ["Nach", "dem", "Me\u00b7lan\u00b7der", "fah", "/", "was", "Wran\u00b7gel", "wil\u00b7lens", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$(", "PWS", "NE", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Verst\u00e4rckt\u2019 er seine Macht mit der Cur-Beyern Schaar/", "tokens": ["Ver\u00b7st\u00e4r\u00b7ckt'", "er", "sei\u00b7ne", "Macht", "mit", "der", "Cur\u00b7Bey\u00b7ern", "Schaar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "APPR", "ART", "NN", "NN", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.302": {"text": "und gieng der Donau zu/ dieselbe zu bewachen.", "tokens": ["und", "gieng", "der", "Do\u00b7nau", "zu", "/", "die\u00b7sel\u00b7be", "zu", "be\u00b7wa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NE", "PTKZU", "$(", "PDAT", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.303": {"text": "Dann er/ als ein Soldat von Witz/ aus allen Sachen", "tokens": ["Dann", "er", "/", "als", "ein", "Sol\u00b7dat", "von", "Witz", "/", "aus", "al\u00b7len", "Sa\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "PPER", "$(", "KOUS", "ART", "NN", "APPR", "NN", "$(", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Wol m\u00e4rckte/ da\u00df sein Feind sein Aug auf Beyern wandt/", "tokens": ["Wol", "m\u00e4rck\u00b7te", "/", "da\u00df", "sein", "Feind", "sein", "Aug", "auf", "Be\u00b7yern", "wandt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "KOUS", "PPOSAT", "NN", "PPOSAT", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.305": {"text": "Als er vor diesem that. Jhm allen Widerstandt", "tokens": ["Als", "er", "vor", "die\u00b7sem", "that", ".", "Jhm", "al\u00b7len", "Wi\u00b7der\u00b7standt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PDAT", "VVFIN", "$.", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "Zu thun/ gieng er voran/ und eilte nach der Br\u00fccken/", "tokens": ["Zu", "thun", "/", "gieng", "er", "vo\u00b7ran", "/", "und", "eil\u00b7te", "nach", "der", "Br\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "PPER", "PTKVZ", "$(", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Die zwischen Regenspurg und Hof aus groben St\u00fccken", "tokens": ["Die", "zwi\u00b7schen", "Re\u00b7gen\u00b7spurg", "und", "Hof", "aus", "gro\u00b7ben", "St\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "KON", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Sehr fest zu schauen ist. Der Wrangel folgte nach/", "tokens": ["Sehr", "fest", "zu", "schau\u00b7en", "ist", ".", "Der", "Wran\u00b7gel", "folg\u00b7te", "nach", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVINF", "VAFIN", "$.", "ART", "NN", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Der K\u00f6nigsmarck mit jhm/ auch war Touraine wach/", "tokens": ["Der", "K\u00f6\u00b7nigs\u00b7marck", "mit", "jhm", "/", "auch", "war", "Tou\u00b7rai\u00b7ne", "wach", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "$(", "ADV", "VAFIN", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.310": {"text": "Und gieng also ein Heer von 30000, Seelen", "tokens": ["Und", "gieng", "al\u00b7so", "ein", "Heer", "von", "30000", ",", "See\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "number", "punct", "word"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "APPR", "CARD", "$,", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.311": {"text": "AnfAuf Beyern/ solches Land noch einmal was zu qu\u00e4len/", "tokens": ["Anf", "Auf", "Be\u00b7yern", "/", "sol\u00b7ches", "Land", "noch", "ein\u00b7mal", "was", "zu", "qu\u00e4\u00b7len", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$(", "PIAT", "NN", "ADV", "ADV", "PIS", "PTKZU", "VVINF", "$("], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.312": {"text": "und h\u00e4rter als zuvor. Eh man zur Donau kam", "tokens": ["und", "h\u00e4r\u00b7ter", "als", "zu\u00b7vor", ".", "Eh", "man", "zur", "Do\u00b7nau", "kam"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "KOKOM", "ADV", "$.", "KOUS", "PIS", "APPRART", "NE", "VVFIN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.313": {"text": "Lag Wintzheim in dem Weg/ das man bem\u00e4chtigt nahm/", "tokens": ["Lag", "Wintz\u00b7heim", "in", "dem", "Weg", "/", "das", "man", "be\u00b7m\u00e4ch\u00b7tigt", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "ART", "NN", "$(", "PRELS", "PIS", "VVFIN", "VVFIN", "$("], "meter": "-++--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.314": {"text": "und alles unterstie\u00df/ was selbiges besch\u00fcrmte.", "tokens": ["und", "al\u00b7les", "un\u00b7ter\u00b7stie\u00df", "/", "was", "sel\u00b7bi\u00b7ges", "be\u00b7sch\u00fcrm\u00b7te", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$(", "PWS", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Worauf die gantze Macht zugleich auf Beyern st\u00fcrmte/", "tokens": ["Wo\u00b7rauf", "die", "gant\u00b7ze", "Macht", "zu\u00b7gleich", "auf", "Be\u00b7yern", "st\u00fcrm\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.316": {"text": "Auch von den Feindlichen ein zwantzig hundert Mann", "tokens": ["Auch", "von", "den", "Feind\u00b7li\u00b7chen", "ein", "zwant\u00b7zig", "hun\u00b7dert", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "ART", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.317": {"text": "Bey Augspurg ", "tokens": ["Bey", "Augs\u00b7purg"], "token_info": ["word", "word"], "pos": ["APPR", "NE"], "meter": "-+-", "measure": "amphibrach.single"}, "line.318": {"text": "Hier blieb Melander selbst/ ein Mann der gl\u00fccklich siegte/", "tokens": ["Hier", "blieb", "Me\u00b7lan\u00b7der", "selbst", "/", "ein", "Mann", "der", "gl\u00fcck\u00b7lich", "sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "ADV", "$(", "ART", "NN", "ART", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "So lang er f\u00fcr das Land der Hessen-Pallas kriegte/", "tokens": ["So", "lang", "er", "f\u00fcr", "das", "Land", "der", "Hes\u00b7sen\u00b7Pal\u00b7las", "krieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "APPR", "ART", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.320": {"text": "So bald er aber sich ans K\u00e4ysers Seiten gab/", "tokens": ["So", "bald", "er", "a\u00b7ber", "sich", "ans", "K\u00e4y\u00b7sers", "Sei\u00b7ten", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADV", "PRF", "APPRART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.321": {"text": "Gab sich das falsche Gl\u00fcck von seiner Seiten ab/", "tokens": ["Gab", "sich", "das", "fal\u00b7sche", "Gl\u00fcck", "von", "sei\u00b7ner", "Sei\u00b7ten", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.322": {"text": "Wie andern mehr geschah. Auf solche Schlapp- und", "tokens": ["Wie", "an\u00b7dern", "mehr", "ge\u00b7schah", ".", "Auf", "sol\u00b7che", "Schlapp", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ADV", "VVFIN", "$.", "APPR", "PIAT", "TRUNC", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.323": {"text": "Schw\u00e4che", "tokens": ["Schw\u00e4\u00b7che"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.324": {"text": "Wich alles aus dem Feld durch Auspurg nach dem Leche/", "tokens": ["Wich", "al\u00b7les", "aus", "dem", "Feld", "durch", "Aus\u00b7purg", "nach", "dem", "Le\u00b7che", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PIS", "APPR", "ART", "NN", "APPR", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.325": {"text": "und wolten da bey Reyhn den Pa\u00df nach Beyern hin", "tokens": ["und", "wol\u00b7ten", "da", "bey", "Reyhn", "den", "Pa\u00df", "nach", "Be\u00b7yern", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "APPR", "NN", "ART", "NN", "APPR", "NN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.326": {"text": "Versperren. Aber ach! das Gl\u00fcck betrog den Sinn.", "tokens": ["Ver\u00b7sper\u00b7ren", ".", "A\u00b7ber", "ach", "!", "das", "Gl\u00fcck", "be\u00b7trog", "den", "Sinn", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "KON", "ADV", "$.", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.327": {"text": "Die Schweden drungen durch und fielen weit in Beyern/", "tokens": ["Die", "Schwe\u00b7den", "drun\u00b7gen", "durch", "und", "fie\u00b7len", "weit", "in", "Be\u00b7yern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "APPR", "KON", "VVFIN", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.328": {"text": "Es kunte sie noch Lech noch Jserstrohm besteuern/", "tokens": ["Es", "kun\u00b7te", "sie", "noch", "Lech", "noch", "Jser\u00b7strohm", "be\u00b7steu\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "NE", "ADV", "NN", "VVINF", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.329": {"text": "Noch das gedoppelt Heer. Wann GOtt ein Land bestrafft/", "tokens": ["Noch", "das", "ge\u00b7dop\u00b7pelt", "Heer", ".", "Wann", "Gott", "ein", "Land", "be\u00b7strafft", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NN", "$.", "PWAV", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "So schickt er einen Feind/ an dem man wenig schafft/", "tokens": ["So", "schickt", "er", "ei\u00b7nen", "Feind", "/", "an", "dem", "man", "we\u00b7nig", "schafft", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$(", "APPR", "PRELS", "PIS", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Dem alles gl\u00fccklich geht. Di\u00df war allhier zu sehen/", "tokens": ["Dem", "al\u00b7les", "gl\u00fcck\u00b7lich", "geht", ".", "Di\u00df", "war", "all\u00b7hier", "zu", "se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADJD", "VVFIN", "$.", "PDS", "VAFIN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.332": {"text": "Man wird es auch allhier nach langer Zeit gestehen/", "tokens": ["Man", "wird", "es", "auch", "all\u00b7hier", "nach", "lan\u00b7ger", "Zeit", "ge\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ADV", "ADV", "APPR", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.333": {"text": "Nach vielem machten sie sich an den strengen Inn/", "tokens": ["Nach", "vie\u00b7lem", "mach\u00b7ten", "sie", "sich", "an", "den", "stren\u00b7gen", "Inn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "PPER", "PRF", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Der voll von Klippen ist/ und wolten \u00fcber hinn/", "tokens": ["Der", "voll", "von", "Klip\u00b7pen", "ist", "/", "und", "wol\u00b7ten", "\u00fc\u00b7ber", "hinn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "NN", "VAFIN", "$(", "KON", "VMFIN", "APPR", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Ins L\u00e4ndlein ob der En\u00df und Oesterreich zu br\u00e4chen.", "tokens": ["Ins", "L\u00e4nd\u00b7lein", "ob", "der", "En\u00df", "und", "O\u00b7es\u00b7ter\u00b7reich", "zu", "br\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KOUS", "ART", "NN", "KON", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.336": {"text": "Es musten aber viel sich da zu todte zechen.", "tokens": ["Es", "mus\u00b7ten", "a\u00b7ber", "viel", "sich", "da", "zu", "tod\u00b7te", "ze\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIAT", "PRF", "ADV", "APPR", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "Der Gegenstadt war gro\u00df/ der Strohm zu schnell und", "tokens": ["Der", "Ge\u00b7gen\u00b7stadt", "war", "gro\u00df", "/", "der", "Strohm", "zu", "schnell", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$(", "ART", "NN", "PTKA", "ADJD", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.338": {"text": "streng/", "tokens": ["streng", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "-", "measure": "single.down"}, "line.339": {"text": "Die Futterung zu klein/ die Zeit zu schlecht und eng/", "tokens": ["Die", "Fut\u00b7te\u00b7rung", "zu", "klein", "/", "die", "Zeit", "zu", "schlecht", "und", "eng", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKA", "ADJD", "$(", "ART", "NN", "PTKA", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "Des K\u00e4ysers Hilff zu gro\u00df/ der nun viel tausend Krieger/", "tokens": ["Des", "K\u00e4y\u00b7sers", "Hilff", "zu", "gro\u00df", "/", "der", "nun", "viel", "tau\u00b7send", "Krie\u00b7ger", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKA", "ADJD", "$(", "ART", "ADV", "ADV", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.341": {"text": "Mit Piccolomini/ auf diesen steten Sieger", "tokens": ["Mit", "Pic\u00b7co\u00b7lo\u00b7mi\u00b7ni", "/", "auf", "die\u00b7sen", "ste\u00b7ten", "Sie\u00b7ger"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.342": {"text": "Zu gehen/ samlen lie\u00df, Di\u00df trieb jhn von dem Inn/", "tokens": ["Zu", "ge\u00b7hen", "/", "sam\u00b7len", "lie\u00df", ",", "Di\u00df", "trieb", "jhn", "von", "dem", "Inn", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVINF", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.343": {"text": "Der jhm viel hundert fra\u00df/ nach andern Pl\u00e4tzen hinn/", "tokens": ["Der", "jhm", "viel", "hun\u00b7dert", "fra\u00df", "/", "nach", "an\u00b7dern", "Pl\u00e4t\u00b7zen", "hinn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "CARD", "VVFIN", "$(", "APPR", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.344": {"text": "Die er im R\u00fccken hatt\u2019/ um sicherer zu leben/", "tokens": ["Die", "er", "im", "R\u00fc\u00b7cken", "hatt'", "/", "um", "si\u00b7che\u00b7rer", "zu", "le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "VAFIN", "$(", "KOUI", "PRF", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.345": {"text": "und auf des Feindes Thun ein scharffes Aug zu geben/", "tokens": ["und", "auf", "des", "Fein\u00b7des", "Thun", "ein", "scharf\u00b7fes", "Aug", "zu", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "NN", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.346": {"text": "Die Noth erheischte das/ es gieng auch scharff daher/", "tokens": ["Die", "Noth", "er\u00b7heischte", "das", "/", "es", "gieng", "auch", "scharff", "da\u00b7her", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "$(", "PPER", "VVFIN", "ADV", "VVFIN", "PAV", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.347": {"text": "und war jhm dieser Zeit das Siegen etwas schwer/", "tokens": ["und", "war", "jhm", "die\u00b7ser", "Zeit", "das", "Sie\u00b7gen", "et\u00b7was", "schwer", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PDAT", "NN", "ART", "NN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.348": {"text": "Wie man bey Dachau sah/ da er wol m\u00e4chtig stritte/", "tokens": ["Wie", "man", "bey", "Dac\u00b7hau", "sah", "/", "da", "er", "wol", "m\u00e4ch\u00b7tig", "strit\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NN", "VVFIN", "$(", "KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$("], "meter": "--+-+-+-++-+-", "measure": "anapaest.init"}, "line.349": {"text": "Jedoch dem M\u00e4chtigern ein ziemliches erlitte.", "tokens": ["Je\u00b7doch", "dem", "M\u00e4ch\u00b7ti\u00b7gern", "ein", "ziem\u00b7li\u00b7ches", "er\u00b7lit\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.350": {"text": "Hierauf erhob er sich der Donau wieder zu/", "tokens": ["Hier\u00b7auf", "er\u00b7hob", "er", "sich", "der", "Do\u00b7nau", "wie\u00b7der", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "ART", "NE", "ADV", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.351": {"text": "und gab sich in der Pfaltz mit seiner Macht zur Ruh/", "tokens": ["und", "gab", "sich", "in", "der", "Pfaltz", "mit", "sei\u00b7ner", "Macht", "zur", "Ruh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "ART", "NN", "APPR", "PPOSAT", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.352": {"text": "Weil er die s\u00fc\u00dfe Post vom lieben Frieden h\u00f6rte/", "tokens": ["Weil", "er", "die", "s\u00fc\u00b7\u00dfe", "Post", "vom", "lie\u00b7ben", "Frie\u00b7den", "h\u00f6r\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.353": {"text": "Die er nach Billigkeit mit guter Ruh verehrte.", "tokens": ["Die", "er", "nach", "Bil\u00b7lig\u00b7keit", "mit", "gu\u00b7ter", "Ruh", "ver\u00b7ehr\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.354": {"text": "Was er/ that auch sein Feind. In dem di\u00df hier verlieff/", "tokens": ["Was", "er", "/", "that", "auch", "sein", "Feind", ".", "In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "$(", "VVFIN", "ADV", "PPOSAT", "NN", "$.", "APPR", "ART", "PDS", "ADV", "VVFIN", "$("], "meter": "--+--+---+-+", "measure": "anapaest.di.plus"}, "line.355": {"text": "That General Lamboy bey Gesek\u2019 einen Griff", "tokens": ["That", "Ge\u00b7ne\u00b7ral", "Lam\u00b7boy", "bey", "Ge\u00b7sek'", "ei\u00b7nen", "Griff"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "NE", "APPR", "NN", "ART", "NN"], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}, "line.356": {"text": "Auf die Casselische/ und trieb sie solcher massen/", "tokens": ["Auf", "die", "Cas\u00b7se\u00b7li\u00b7sche", "/", "und", "trieb", "sie", "sol\u00b7cher", "mas\u00b7sen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$(", "KON", "VVFIN", "PPER", "PIAT", "NN", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.357": {"text": "Da\u00df fie in Geseke sich musten schr\u00e4ncken lassen.", "tokens": ["Da\u00df", "fie", "in", "Ge\u00b7se\u00b7ke", "sich", "mus\u00b7ten", "schr\u00e4n\u00b7cken", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "FM", "APPR", "NN", "PRF", "VMFIN", "VVINF", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.358": {"text": "Man bracht\u2019 Entsatz herbey/ Lamboy gieng frisch darauff", "tokens": ["Man", "bracht'", "Ent\u00b7satz", "her\u00b7bey", "/", "Lam\u00b7boy", "gieng", "frisch", "dar\u00b7auff"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "NN", "PTKVZ", "$(", "NE", "VVFIN", "ADJD", "PAV"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.359": {"text": "und schlug jhn/ fieng darzu desselben F\u00fchrer auf/", "tokens": ["und", "schlug", "jhn", "/", "fi\u00b7eng", "dar\u00b7zu", "des\u00b7sel\u00b7ben", "F\u00fch\u00b7rer", "auf", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$(", "VVFIN", "PAV", "PDAT", "NN", "APPR", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.360": {"text": "Das war der Landgraf Crnst. Nach allem/ als er meynte/", "tokens": ["Das", "war", "der", "Land\u00b7graf", "Crnst", ".", "Nach", "al\u00b7lem", "/", "als", "er", "meyn\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "NN", "$.", "APPR", "PIS", "$(", "KOUS", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.361": {"text": "Da\u00df jhm das liebe Gl\u00fcck nach seinem Willen scheinte/", "tokens": ["Da\u00df", "jhm", "das", "lie\u00b7be", "Gl\u00fcck", "nach", "sei\u00b7nem", "Wil\u00b7len", "schein\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.362": {"text": "Must\u2019 er von Geseke zu r\u00fcck und nach was Zeit", "tokens": ["Must'", "er", "von", "Ge\u00b7se\u00b7ke", "zu", "r\u00fcck", "und", "nach", "was", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "NN", "PTKZU", "PTKVZ", "KON", "APPR", "PRELS", "NN"], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.363": {"text": "Erleyden/ da\u00df der He\u00df in offenbahrem Streit/", "tokens": ["Er\u00b7ley\u00b7den", "/", "da\u00df", "der", "He\u00df", "in", "of\u00b7fen\u00b7bah\u00b7rem", "Streit", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KOUS", "ART", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.364": {"text": "Nicht weit von Grefenburg ", "tokens": ["Nicht", "weit", "von", "Gre\u00b7fen\u00b7burg"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKNEG", "ADJD", "APPR", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.365": {"text": "f\u00e4llte/", "tokens": ["f\u00e4ll\u00b7te", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.366": {"text": "Wie scharff sich feine Macht der andern widerst\u00e4llte.", "tokens": ["Wie", "scharff", "sich", "fei\u00b7ne", "Macht", "der", "an\u00b7dern", "wi\u00b7der\u00b7st\u00e4ll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PRF", "ADJA", "NN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.367": {"text": "Di\u00df alles ungeacht/ nahm er ein gr\u00f6\u00dfer Heer", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "/", "nahm", "er", "ein", "gr\u00f6\u00b7\u00dfer", "Heer"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "ADJD", "$(", "VVFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.368": {"text": "und gieng noch eines an/ weil aber jener sehr", "tokens": ["und", "gieng", "noch", "ei\u00b7nes", "an", "/", "weil", "a\u00b7ber", "je\u00b7ner", "sehr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "PIS", "PTKVZ", "$(", "KOUS", "ADV", "PDAT", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.369": {"text": "Verschw\u00e4cht war/ hielt\u2019 er sich/ bi\u00df man/ jhn zu verst\u00e4rcken/", "tokens": ["Ver\u00b7schw\u00e4cht", "war", "/", "hielt'", "er", "sich", "/", "bi\u00df", "man", "/", "jhn", "zu", "ver\u00b7st\u00e4r\u00b7cken", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$(", "VVFIN", "PPER", "PRF", "$(", "KOUS", "PIS", "$(", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.370": {"text": "Entsatz und Beystand that/ zu Neu\u00df in festen Wercken.", "tokens": ["Ent\u00b7satz", "und", "Beys\u00b7tand", "that", "/", "zu", "Neu\u00df", "in", "fes\u00b7ten", "Wer\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "$(", "APPR", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.371": {"text": "Die Hilff kam an/ Lamboy zog ab/ worauf die Schaar", "tokens": ["Die", "Hilff", "kam", "an", "/", "Lam\u00b7boy", "zog", "ab", "/", "wo\u00b7rauf", "die", "Schaar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$(", "NE", "VVFIN", "PTKVZ", "$(", "PWAV", "ART", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.372": {"text": "Der Hessen Paderborn/ das wieder K\u00e4ysrisch war/", "tokens": ["Der", "Hes\u00b7sen", "Pa\u00b7der\u00b7born", "/", "das", "wie\u00b7der", "K\u00e4y\u00b7srisch", "war", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$(", "ART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.373": {"text": "Berung. Lamboy kam an und trieb sie das sie wichen/", "tokens": ["Be\u00b7rung", ".", "Lam\u00b7boy", "kam", "an", "und", "trieb", "sie", "das", "sie", "wi\u00b7chen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NE", "VVFIN", "PTKVZ", "KON", "VVFIN", "PPER", "ART", "PPER", "VVFIN", "$("], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.374": {"text": "Damit so war auch hier die Streitigkeit verglichen/", "tokens": ["Da\u00b7mit", "so", "war", "auch", "hier", "die", "Strei\u00b7tig\u00b7keit", "ver\u00b7gli\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VAFIN", "ADV", "ADV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.375": {"text": "und blieben beyde Theil auf die gew\u00fcndschte M\u00e4hr", "tokens": ["und", "blie\u00b7ben", "bey\u00b7de", "Theil", "auf", "die", "ge\u00b7w\u00fcnd\u00b7schte", "M\u00e4hr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PIAT", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.376": {"text": "Vom Frieden in der Ruh. War also hin und her", "tokens": ["Vom", "Frie\u00b7den", "in", "der", "Ruh", ".", "War", "al\u00b7so", "hin", "und", "her"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$.", "VAFIN", "ADV", "PTKVZ", "KON", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.377": {"text": "Durch alles Deutschland Fried. Was noch von Waffen", "tokens": ["Durch", "al\u00b7les", "Deutschland", "Fried", ".", "Was", "noch", "von", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "NN", "$.", "PWS", "ADV", "APPR", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.378": {"text": "bebte/", "tokens": ["beb\u00b7te", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.379": {"text": "War B\u00f6h\u00e4im/ das numehr in h\u00f6chsten N\u00f6then lebte.", "tokens": ["War", "B\u00f6\u00b7h\u00e4im", "/", "das", "nu\u00b7mehr", "in", "h\u00f6chs\u00b7ten", "N\u00f6\u00b7then", "leb\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "$(", "PDS", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.380": {"text": "Dann als der Konigsmarck die zwantzig hundert Mann", "tokens": ["Dann", "als", "der", "Ko\u00b7nigs\u00b7marck", "die", "zwant\u00b7zig", "hun\u00b7dert", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "ART", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.381": {"text": "Bey Augfpurg durch das Schwerd von hier hatt\u2019 abgethan.", "tokens": ["Bey", "Aug\u00b7fpurg", "durch", "das", "Schwerd", "von", "hier", "hatt'", "ab\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "APPR", "ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.382": {"text": "Gieng er zur Ober-Pfaltz/ nahm und bew\u00e4llte Weyden/", "tokens": ["Gieng", "er", "zur", "O\u00b7ber\u00b7Pfaltz", "/", "nahm", "und", "be\u00b7w\u00e4ll\u00b7te", "Wey\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "$(", "VVFIN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.383": {"text": "Vor welchem Amberg sich ein gro\u00dfes muste leyden/", "tokens": ["Vor", "wel\u00b7chem", "Am\u00b7berg", "sich", "ein", "gro\u00b7\u00dfes", "mus\u00b7te", "ley\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "PRF", "ART", "ADJA", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.384": {"text": "Dann es Trutz-Amberg hie\u00df. Er gieng auch auf Waldeck/", "tokens": ["Dann", "es", "Trut\u00b7zAm\u00b7berg", "hie\u00df", ".", "Er", "gieng", "auch", "auf", "Wal\u00b7deck", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "APPR", "NE", "$("], "meter": "---+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.385": {"text": "Das annoch Jungfer war/ und keinen Mann so keck", "tokens": ["Das", "an\u00b7noch", "Jung\u00b7fer", "war", "/", "und", "kei\u00b7nen", "Mann", "so", "keck"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "ADV", "NE", "VAFIN", "$(", "KON", "PIAT", "NN", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.386": {"text": "F\u00fcr jhren Freyer hielt; Hier aber must\u2019 es weichen/", "tokens": ["F\u00fcr", "jhren", "Frey\u00b7er", "hielt", ";", "Hier", "a\u00b7ber", "must'", "es", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "ADV", "ADV", "VMFIN", "PPER", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.387": {"text": "und sich mit K\u00f6nigsmarck nach seiner Lust vergleichen/", "tokens": ["und", "sich", "mit", "K\u00f6\u00b7nigs\u00b7marck", "nach", "sei\u00b7ner", "Lust", "ver\u00b7glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.388": {"text": "Der Brautschatz war nicht schlecht. Hierauf erhob er sich", "tokens": ["Der", "Braut\u00b7schatz", "war", "nicht", "schlecht", ".", "Hier\u00b7auf", "er\u00b7hob", "er", "sich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "ADJD", "$.", "PAV", "VVFIN", "PPER", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.389": {"text": "Nach B\u00f6h\u00e4im/ und gewann ", "tokens": ["Nach", "B\u00f6\u00b7h\u00e4im", "/", "und", "ge\u00b7wann"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPR", "NE", "$(", "KON", "VVFIN"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.390": {"text": "Halb Prag/ und sonder Schlag. Gantz Prag schlieff ohne", "tokens": ["Halb", "Prag", "/", "und", "son\u00b7der", "Schlag", ".", "Gantz", "Prag", "schlieff", "oh\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "$(", "KON", "ADJA", "NN", "$.", "ADV", "NE", "VVFIN", "APPR"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.391": {"text": "Sorgen/", "tokens": ["Sor\u00b7gen", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.392": {"text": "War keines Feinds besorgt/ und eh der helle Morgen", "tokens": ["War", "kei\u00b7nes", "Feinds", "be\u00b7sorgt", "/", "und", "eh", "der", "hel\u00b7le", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "VVPP", "$(", "KON", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.393": {"text": "Erschien/ war alles Feind. Da waren Schlo\u00df/ Ratschin", "tokens": ["Er\u00b7schien", "/", "war", "al\u00b7les", "Feind", ".", "Da", "wa\u00b7ren", "Schlo\u00df", "/", "Rat\u00b7schin"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["NE", "$(", "VAFIN", "PIAT", "NN", "$.", "ADV", "VAFIN", "NN", "$(", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.394": {"text": "und kleine Seiten weg/ auch alles was darin", "tokens": ["und", "klei\u00b7ne", "Sei\u00b7ten", "weg", "/", "auch", "al\u00b7les", "was", "da\u00b7rin"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "PTKVZ", "$(", "ADV", "PIS", "PWS", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.395": {"text": "Gefangen und entbl\u00f6\u00dft/ die alte Stadt beschossen/", "tokens": ["Ge\u00b7fan\u00b7gen", "und", "ent\u00b7bl\u00f6\u00dft", "/", "die", "al\u00b7te", "Stadt", "be\u00b7schos\u00b7sen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVFIN", "$(", "ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.396": {"text": "und bald von Wittenbergs Armee rund um beschlosselt/", "tokens": ["und", "bald", "von", "Wit\u00b7ten\u00b7bergs", "Ar\u00b7mee", "rund", "um", "be\u00b7schlos\u00b7selt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NE", "NN", "ADJD", "APPR", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.397": {"text": "Da\u00df es sehr gro\u00dfe Noth in allen H\u00e4usern gab.", "tokens": ["Da\u00df", "es", "sehr", "gro\u00b7\u00dfe", "Noth", "in", "al\u00b7len", "H\u00e4u\u00b7sern", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJA", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.398": {"text": "So schleunig setzt das Gl\u00fcck von vielen offtmals ab-", "tokens": ["So", "schleu\u00b7nig", "setzt", "das", "Gl\u00fcck", "von", "vie\u00b7len", "offt\u00b7mals", "ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "APPR", "PIAT", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.399": {"text": "Man fleng ein zwantzig zehn gef\u00fcrst- und gro\u00dfe Leuthe/", "tokens": ["Man", "fleng", "ein", "zwant\u00b7zig", "zehn", "ge\u00b7f\u00fcr\u00b7st", "und", "gro\u00b7\u00dfe", "Leu\u00b7the", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "CARD", "CARD", "TRUNC", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.400": {"text": "und kriegte mehr von Gold und theurer Waar zur Beuthe", "tokens": ["und", "krieg\u00b7te", "mehr", "von", "Gold", "und", "theu\u00b7rer", "Waar", "zur", "Beu\u00b7the"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "APPR", "NN", "KON", "ADJD", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.401": {"text": "Als es zu glauben ist. Ein hundert funfftzig St\u00fcck/", "tokens": ["Als", "es", "zu", "glau\u00b7ben", "ist", ".", "Ein", "hun\u00b7dert", "funfft\u00b7zig", "St\u00fcck", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKZU", "VVINF", "VAFIN", "$.", "ART", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.402": {"text": "und was noch anders mehr. Di\u00df war ein solches Gl\u00fcck", "tokens": ["und", "was", "noch", "an\u00b7ders", "mehr", ".", "Di\u00df", "war", "ein", "sol\u00b7ches", "Gl\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "ADV", "ADV", "ADV", "$.", "PDS", "VAFIN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.403": {"text": "Als es die Schwedische kaum hofften oder hatten.", "tokens": ["Als", "es", "die", "Schwe\u00b7di\u00b7sche", "kaum", "hoff\u00b7ten", "o\u00b7der", "hat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVFIN", "KON", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.404": {"text": "Man fieng erschrecklich an die Alt-Stadt abzumatten/", "tokens": ["Man", "fi\u00b7eng", "er\u00b7schreck\u00b7lich", "an", "die", "Al\u00b7tStadt", "ab\u00b7zu\u00b7mat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADJD", "APPR", "ART", "NN", "VVIZU", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.405": {"text": "Scho\u00df Bleu und Feuer ein/ das Blut und Feuer bracht\u2019.", "tokens": ["Scho\u00df", "Bleu", "und", "Feu\u00b7er", "ein", "/", "das", "Blut", "und", "Feu\u00b7er", "bracht'", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "KON", "NN", "ART", "$(", "ART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.406": {"text": "Es fiel auch nun Tetschin in Wittenbergers Macht", "tokens": ["Es", "fiel", "auch", "nun", "Tet\u00b7schin", "in", "Wit\u00b7ten\u00b7ber\u00b7gers", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "NE", "APPR", "NE", "NN"], "meter": "-+--++-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.407": {"text": "und Tabor/ das sehr fest und nicht zu zwingen scheinet.", "tokens": ["und", "Ta\u00b7bor", "/", "das", "sehr", "fest", "und", "nicht", "zu", "zwin\u00b7gen", "schei\u00b7net", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$(", "PDS", "ADV", "ADJD", "KON", "PTKNEG", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.408": {"text": "Es wurd auch Buchheims Volck von dieser Macht um-", "tokens": ["Es", "wurd", "auch", "Buch\u00b7heims", "Volck", "von", "die\u00b7ser", "Macht", "um"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "NN", "NN", "APPR", "PDAT", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.409": {"text": "und er mit jhm besiegt. Es gieng hier anders nicht/", "tokens": ["und", "er", "mit", "jhm", "be\u00b7siegt", ".", "Es", "gieng", "hier", "an\u00b7ders", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPR", "PPER", "VVPP", "$.", "PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.410": {"text": "Als h\u00e4tte sich das Gl\u00fcck den Schwedischen verpflicht", "tokens": ["Als", "h\u00e4t\u00b7te", "sich", "das", "Gl\u00fcck", "den", "Schwe\u00b7di\u00b7schen", "ver\u00b7pflicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "VAFIN", "PRF", "ART", "NN", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.411": {"text": "In allem Treu zu seyn. Damit man nun den Sachen", "tokens": ["In", "al\u00b7lem", "Treu", "zu", "seyn", ".", "Da\u00b7mit", "man", "nun", "den", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "NN", "PTKZU", "VAINF", "$.", "KOUS", "PIS", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.412": {"text": "Vor Prag und anderswo ein Ende mochte machen/", "tokens": ["Vor", "Prag", "und", "an\u00b7ders\u00b7wo", "ein", "En\u00b7de", "moch\u00b7te", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "ADV", "ART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.413": {"text": "Kam Carl Gustavus selbst/ ein Pfaltzgraf bey dem Reyhn/", "tokens": ["Kam", "Carl", "Gus\u00b7ta\u00b7vus", "selbst", "/", "ein", "Pfaltz\u00b7graf", "bey", "dem", "Reyhn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "ADV", "$(", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.414": {"text": "Dem endlich noch das Reich solt\u2019 anvertrauet seyn/", "tokens": ["Dem", "end\u00b7lich", "noch", "das", "Reich", "solt'", "an\u00b7ver\u00b7trau\u00b7et", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "ART", "NN", "VMFIN", "VVPP", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.415": {"text": "und brachte neben sich den Edlen von der Linden/", "tokens": ["und", "brach\u00b7te", "ne\u00b7ben", "sich", "den", "Ed\u00b7len", "von", "der", "Lin\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PRF", "ART", "NN", "APPR", "ART", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.416": {"text": "Ein Geist/ der sich in Ruh und Streit beliebt l\u00e4\u00dft finden/", "tokens": ["Ein", "Geist", "/", "der", "sich", "in", "Ruh", "und", "Streit", "be\u00b7liebt", "l\u00e4\u00dft", "fin\u00b7den", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PRF", "APPR", "NN", "KON", "NN", "ADJD", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.417": {"text": "und eine gro\u00dfe Zahl von achtmal tausend Mann/", "tokens": ["und", "ei\u00b7ne", "gro\u00b7\u00dfe", "Zahl", "von", "acht\u00b7mal", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "APPR", "ADV", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.418": {"text": "Aus Schweden hergef\u00fchrt/ vor Prag/ zum st\u00fcrmen/ an.", "tokens": ["Aus", "Schwe\u00b7den", "her\u00b7ge\u00b7f\u00fchrt", "/", "vor", "Prag", "/", "zum", "st\u00fcr\u00b7men", "/", "an", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$(", "APPR", "NE", "$(", "APPRART", "VVINF", "$(", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.419": {"text": "Man fiel gewaltig an/ that sechszehntausend Sch\u00fcsse", "tokens": ["Man", "fiel", "ge\u00b7wal\u00b7tig", "an", "/", "that", "sechs\u00b7zehn\u00b7tau\u00b7send", "Sch\u00fcs\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADJD", "PTKVZ", "$(", "VVFIN", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.420": {"text": "Aus St\u00fccken auf die Stadt/ da\u00df mancher Bau zerrisse.", "tokens": ["Aus", "St\u00fc\u00b7cken", "auf", "die", "Stadt", "/", "da\u00df", "man\u00b7cher", "Bau", "zer\u00b7ris\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "$(", "KOUS", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.421": {"text": "Man st\u00fcrmte/ spr\u00e4ngt\u2019 und scho\u00df/ da\u00df es erschrecklich war.", "tokens": ["Man", "st\u00fcrm\u00b7te", "/", "spr\u00e4ngt'", "und", "scho\u00df", "/", "da\u00df", "es", "er\u00b7schreck\u00b7lich", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$(", "VVFIN", "KON", "VVFIN", "$(", "KOUS", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.422": {"text": "Di\u00df alles ungeacht that die beschr\u00e4nckte Schaar", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "that", "die", "be\u00b7schr\u00e4nck\u00b7te", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "ADJD", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.423": {"text": "In Prag kein anders nicht/ als starcke Gegenwehre/", "tokens": ["In", "Prag", "kein", "an\u00b7ders", "nicht", "/", "als", "star\u00b7cke", "Ge\u00b7gen\u00b7weh\u00b7re", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PIAT", "ADV", "PTKNEG", "$(", "KOUS", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.424": {"text": "Wobey die Musen-Schaar/ das ich jhr hier zur Ehre", "tokens": ["Wo\u00b7bey", "die", "Mu\u00b7sen\u00b7Schaar", "/", "das", "ich", "jhr", "hier", "zur", "Eh\u00b7re"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "$(", "PRELS", "PPER", "PPER", "ADV", "APPRART", "NN"], "meter": "-+-+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.425": {"text": "Gedenck\u2019/ ein gro\u00dfes that/ das K\u00e4yser Ferdinand/", "tokens": ["Ge\u00b7denck", "/", "ein", "gro\u00b7\u00dfes", "that", "/", "das", "K\u00e4y\u00b7ser", "Fer\u00b7di\u00b7nand", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ART", "ADJA", "VVFIN", "$(", "ART", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.426": {"text": "Als ein Gen\u00e4digster/ in Gnaden hat erkannt.", "tokens": ["Als", "ein", "Ge\u00b7n\u00e4\u00b7digs\u00b7ter", "/", "in", "Gna\u00b7den", "hat", "er\u00b7kannt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$(", "APPR", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.427": {"text": "Als es am \u00e4rgsten stundt/ erscholl die s\u00fc\u00dfe M\u00e4hre:", "tokens": ["Als", "es", "am", "\u00e4rgs\u00b7ten", "stundt", "/", "er\u00b7scholl", "die", "s\u00fc\u00b7\u00dfe", "M\u00e4h\u00b7re", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADJA", "VVFIN", "$(", "ADJD", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.428": {"text": "Wie da\u00df der liebe Fried\u2019 in Deutschland richtig w\u00e4re.", "tokens": ["Wie", "da\u00df", "der", "lie\u00b7be", "Fried'", "in", "Deutschland", "rich\u00b7tig", "w\u00e4\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOKOM", "KOUS", "ART", "ADJA", "NN", "APPR", "NE", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.429": {"text": "Was Art es Friede ", "tokens": ["Was", "Art", "es", "Frie\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "NN", "PPER", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.430": {"text": "Hier wird das gr\u00f6\u00dfeste zum k\u00fcrtzsten eingebracht.", "tokens": ["Hier", "wird", "das", "gr\u00f6\u00b7\u00dfes\u00b7te", "zum", "k\u00fcrtz\u00b7sten", "ein\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "APPRART", "ADJA", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.431": {"text": "Dem K\u00e4yser sahe man gantz B\u00f6h\u00e4im erblich werden/", "tokens": ["Dem", "K\u00e4y\u00b7ser", "sa\u00b7he", "man", "gantz", "B\u00f6\u00b7h\u00e4im", "er\u00b7blich", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "ADV", "NE", "ADJD", "VAINF", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.432": {"text": "Dem Schweden aber blieb Vor-Pommern/ Brehm- und", "tokens": ["Dem", "Schwe\u00b7den", "a\u00b7ber", "blieb", "Vor\u00b7Pom\u00b7mern", "/", "Brehm", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NE", "ADV", "VVFIN", "NN", "$(", "TRUNC", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.433": {"text": "Verden/", "tokens": ["Ver\u00b7den", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.434": {"text": "Wie auch die Wi\u00dfmar-Stadt/ und etwas mehr hierum.", "tokens": ["Wie", "auch", "die", "Wi\u00df\u00b7ma\u00b7rStadt", "/", "und", "et\u00b7was", "mehr", "hie\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "$(", "KON", "ADV", "ADV", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.435": {"text": "Das edel Elsas blieb des Frantzmanns Eigenthum.", "tokens": ["Das", "e\u00b7del", "El\u00b7sas", "blieb", "des", "Frantz\u00b7manns", "Ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NE", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.436": {"text": "Dem Beyer-F\u00fcrsten blieb die Ober-Pfaltz zur Beuthe.", "tokens": ["Dem", "Beyer\u00b7F\u00fcrs\u00b7ten", "blieb", "die", "O\u00b7ber\u00b7Pfaltz", "zur", "Beu\u00b7the", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.437": {"text": "Und also stillten sich des Deutschlands Kriegs-Leuthe.", "tokens": ["Und", "al\u00b7so", "still\u00b7ten", "sich", "des", "Deutschlands", "Kriegs\u00b7Leu\u00b7the", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.438": {"text": "Hierauf so wurd auch Prag befreyet und erfreut/", "tokens": ["Hier\u00b7auf", "so", "wurd", "auch", "Prag", "be\u00b7fre\u00b7yet", "und", "er\u00b7freut", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VAFIN", "ADV", "NE", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.439": {"text": "und lebten alle Heer in guter Einigkeit.", "tokens": ["und", "leb\u00b7ten", "al\u00b7le", "Heer", "in", "gu\u00b7ter", "Ei\u00b7nig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.440": {"text": "Ein Wunder! Wo der Krieg sich erstlich hat erreget/", "tokens": ["Ein", "Wun\u00b7der", "!", "Wo", "der", "Krieg", "sich", "erst\u00b7lich", "hat", "er\u00b7re\u00b7get", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PWAV", "ART", "NN", "PRF", "ADJD", "VAFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.441": {"text": "Da hat er/ drey\u00dfig Jahr hernach/ sich auch geleget.", "tokens": ["Da", "hat", "er", "/", "drey\u00b7\u00dfig", "Jahr", "her\u00b7nach", "/", "sich", "auch", "ge\u00b7le\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$(", "CARD", "NN", "ADV", "$(", "PRF", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.442": {"text": "Und was ein Pfaltzgraf hat erregt/ wie schon gedacht/", "tokens": ["Und", "was", "ein", "Pfaltz\u00b7graf", "hat", "er\u00b7regt", "/", "wie", "schon", "ge\u00b7dacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VAFIN", "VVPP", "$(", "KOKOM", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.443": {"text": "Das hat durch GOttes Hilff ein Pfaltzgraf still gemacht.", "tokens": ["Das", "hat", "durch", "Got\u00b7tes", "Hilff", "ein", "Pfaltz\u00b7graf", "still", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "NN", "NN", "ART", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.444": {"text": "Dan\u0303 Carl Gustavus sich von Prag nach N\u00fcrnberg machte/", "tokens": ["Da\u00f1", "Carl", "Gus\u00b7ta\u00b7vus", "sich", "von", "Prag", "nach", "N\u00fcrn\u00b7berg", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "PRF", "APPR", "NE", "APPR", "NE", "VVFIN", "$("], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.445": {"text": "und was noch streitig war/ daselbst zu rechte brachte/", "tokens": ["und", "was", "noch", "strei\u00b7tig", "war", "/", "da\u00b7selbst", "zu", "rech\u00b7te", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ADJD", "VAFIN", "$(", "PAV", "APPR", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.446": {"text": "Wiewol mit gro\u00dfer M\u00fch. Ein anderthalbes Jahr", "tokens": ["Wie\u00b7wol", "mit", "gro\u00b7\u00dfer", "M\u00fch", ".", "Ein", "an\u00b7der\u00b7thal\u00b7bes", "Jahr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "APPR", "ADJA", "NN", "$.", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.447": {"text": "Verlieff/ eh unsrer Ruh gemeiner Freud-Tag ", "tokens": ["Ver\u00b7lieff", "/", "eh", "uns\u00b7rer", "Ruh", "ge\u00b7mei\u00b7ner", "Freu\u00b7dTag"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "KOUS", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.448": {"text": "Damit war \u00fcberall durch Deutschland Fried und Freude.", "tokens": ["Da\u00b7mit", "war", "\u00fc\u00b7be\u00b7rall", "durch", "Deutschland", "Fried", "und", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ADV", "APPR", "NE", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.449": {"text": "Da\u00df derer keines sich von uns so balde scheide/", "tokens": ["Da\u00df", "de\u00b7rer", "kei\u00b7nes", "sich", "von", "uns", "so", "bal\u00b7de", "schei\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "PIS", "PRF", "APPR", "PPER", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.450": {"text": "W\u00fcndsch\u2019 ich und jederman/ der deutschen Nahmen hat.", "tokens": ["W\u00fcnd\u00b7sch'", "ich", "und", "je\u00b7der\u00b7man", "/", "der", "deut\u00b7schen", "Nah\u00b7men", "hat", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "PIS", "$(", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.451": {"text": "Erstick Achitophel mit deinem b\u00f6sen Raht.", "tokens": ["Er\u00b7stick", "A\u00b7chi\u00b7to\u00b7phel", "mit", "dei\u00b7nem", "b\u00f6\u00b7sen", "Raht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}