{"dta.poem.10039": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Bluhmen-Schrift.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1735", "urn": "urn:nbn:de:kobv:b4-20086-0", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Wie wir es wircklich hoch mit vielen K\u00fcnsten treiben;", "tokens": ["Wie", "wir", "es", "wir\u00b7ck\u00b7lich", "hoch", "mit", "vie\u00b7len", "K\u00fcns\u00b7ten", "trei\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "ADJD", "ADJD", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+----+-+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "So hat man eine Art, \u00fcm unbekannt zu schreiben,", "tokens": ["So", "hat", "man", "ei\u00b7ne", "Art", ",", "\u00fcm", "un\u00b7be\u00b7kannt", "zu", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "ART", "NN", "$,", "ADV", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Recht Kunst- und Sinn-reich ausgefunden.", "tokens": ["Recht", "Kunst", "und", "Sinn\u00b7reich", "aus\u00b7ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "TRUNC", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Man zeichnet Bluhmen recht nach Schilderer Manier,", "tokens": ["Man", "zeich\u00b7net", "Bluh\u00b7men", "recht", "nach", "Schil\u00b7de\u00b7rer", "Ma\u00b7nier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NN", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "An stat der Lettern, auf Papier,", "tokens": ["An", "stat", "der", "Let\u00b7tern", ",", "auf", "Pa\u00b7pier", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$,", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Als w\u00e4ren sie in einem Crantz gebunden.", "tokens": ["Als", "w\u00e4\u00b7ren", "sie", "in", "ei\u00b7nem", "Crantz", "ge\u00b7bun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wer nun den Schl\u00fcssel hat, kann alsobald ersehn,", "tokens": ["Wer", "nun", "den", "Schl\u00fcs\u00b7sel", "hat", ",", "kann", "al\u00b7so\u00b7bald", "er\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VAFIN", "$,", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nachdem sie bey einander stehn,", "tokens": ["Nach\u00b7dem", "sie", "bey", "ein\u00b7an\u00b7der", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Was sie f\u00fcr Worte deuten sollen:", "tokens": ["Was", "sie", "f\u00fcr", "Wor\u00b7te", "deu\u00b7ten", "sol\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Weil iede Bluhm\u2019 und iedes Blat", "tokens": ["Weil", "ie\u00b7de", "Bluhm'", "und", "ie\u00b7des", "Blat"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIAT", "NN", "KON", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Das Zeichen einer Letter hat,", "tokens": ["Das", "Zei\u00b7chen", "ei\u00b7ner", "Let\u00b7ter", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "So wie man sie bezeichnen wollen.", "tokens": ["So", "wie", "man", "sie", "be\u00b7zeich\u00b7nen", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PIS", "PPER", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Auf diese Weise kann man lesen,", "tokens": ["Auf", "die\u00b7se", "Wei\u00b7se", "kann", "man", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Was sonst unleserlich gewesen.", "tokens": ["Was", "sonst", "un\u00b7le\u00b7ser\u00b7lich", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Mich deucht, es sey im Buche dieser Welt,", "tokens": ["Mich", "deucht", ",", "es", "sey", "im", "Bu\u00b7che", "die\u00b7ser", "Welt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPRART", "NN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Bald hie, bald dort", "tokens": ["Bald", "hie", ",", "bald", "dort"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "$,", "ADV", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Dergleichen Schrift uns vorgestellt.", "tokens": ["Derg\u00b7lei\u00b7chen", "Schrift", "uns", "vor\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "An einem ieden Ort", "tokens": ["An", "ei\u00b7nem", "ie\u00b7den", "Ort"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Legt ein bebl\u00fchmtes Garten-Feld", "tokens": ["Legt", "ein", "be\u00b7bl\u00fchm\u00b7tes", "Gar\u00b7ten\u00b7Feld"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dergleichen Schrift uns vor die Augen.", "tokens": ["Derg\u00b7lei\u00b7chen", "Schrift", "uns", "vor", "die", "Au\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Ach m\u00f6gte man es doch recht zu entzieffern taugen!", "tokens": ["Ach", "m\u00f6g\u00b7te", "man", "es", "doch", "recht", "zu", "ent\u00b7zief\u00b7fern", "tau\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PIS", "PPER", "ADV", "ADJD", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Zuweilen kommt es mir", "tokens": ["Zu\u00b7wei\u00b7len", "kommt", "es", "mir"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "Nicht anders f\u00fcr,", "tokens": ["Nicht", "an\u00b7ders", "f\u00fcr", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Als w\u00e4r von mir der Schl\u00fcssel ausgefunden.", "tokens": ["Als", "w\u00e4r", "von", "mir", "der", "Schl\u00fcs\u00b7sel", "aus\u00b7ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "APPR", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Wenn ich von unterschiednen Nahmen", "tokens": ["Wenn", "ich", "von", "un\u00b7ter\u00b7schied\u00b7nen", "Nah\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Bluhmen, welche sie (wer wei\u00df, ob ungefehr)", "tokens": ["Der", "Bluh\u00b7men", ",", "wel\u00b7che", "sie", "(", "wer", "wei\u00df", ",", "ob", "un\u00b7ge\u00b7fehr", ")"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "$(", "PWS", "VVFIN", "$,", "KOUS", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Einft \u00fcberkamen,", "tokens": ["Einft", "\u00fc\u00b7ber\u00b7ka\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Die ersten Lettern nehm\u2019, und f\u00fcge;", "tokens": ["Die", "ers\u00b7ten", "Let\u00b7tern", "nehm'", ",", "und", "f\u00fc\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So scheint es, da\u00df darin was sonderliches liege,", "tokens": ["So", "scheint", "es", ",", "da\u00df", "da\u00b7rin", "was", "son\u00b7der\u00b7li\u00b7ches", "lie\u00b7ge", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PAV", "PWS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Woran ich wenigstens mich recht vergn\u00fcge:", "tokens": ["Wo\u00b7ran", "ich", "we\u00b7nigs\u00b7tens", "mich", "recht", "ver\u00b7gn\u00fc\u00b7ge", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Goldlacken hat ein G;", "tokens": ["Gold\u00b7la\u00b7cken", "hat", "ein", "G", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Der letztern setz ich zwey: so ist der Nahm zu lesen", "tokens": ["Der", "letz\u00b7tern", "setz", "ich", "zwey", ":", "so", "ist", "der", "Nahm", "zu", "le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "PPER", "CARD", "$.", "ADV", "VAFIN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Von dem allgegenw\u00e4rtgen Wesen.", "tokens": ["Von", "dem", "all\u00b7ge\u00b7gen\u00b7w\u00e4rt\u00b7gen", "We\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.5": {"text": "Mir drauf das W\u00f6rtchen ", "tokens": ["Mir", "drauf", "das", "W\u00f6rt\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "PAV", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Wenn ich ", "tokens": ["Wenn", "ich"], "token_info": ["word", "word"], "pos": ["KOUS", "PPER"], "meter": "++", "measure": "spondeus"}, "line.7": {"text": "So deucht mich, da\u00df ich voller Klarheit,", "tokens": ["So", "deucht", "mich", ",", "da\u00df", "ich", "vol\u00b7ler", "Klar\u00b7heit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Als eine unleugbare Wahrheit,", "tokens": ["Als", "ei\u00b7ne", "un\u00b7leug\u00b7ba\u00b7re", "Wahr\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die\u00df: ", "tokens": ["Die\u00df", ":"], "token_info": ["word", "punct"], "pos": ["PDS", "$."], "meter": "+", "measure": "single.up"}}, "stanza.6": {"line.1": {"text": "Formirete man sich dergleichen Zeichen mir,", "tokens": ["For\u00b7mi\u00b7re\u00b7te", "man", "sich", "derg\u00b7lei\u00b7chen", "Zei\u00b7chen", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PRF", "PIS", "NN", "PPER", "$,"], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Wie alle Lettern ja sonst nichts, als Zeichen, seyn;", "tokens": ["Wie", "al\u00b7le", "Let\u00b7tern", "ja", "sonst", "nichts", ",", "als", "Zei\u00b7chen", ",", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "ADV", "ADV", "PIS", "$,", "KOUS", "NN", "$,", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So w\u00fcrden wir vielleicht im Buche der Natur,", "tokens": ["So", "w\u00fcr\u00b7den", "wir", "viel\u00b7leicht", "im", "Bu\u00b7che", "der", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Von dem allgegenw\u00e4rtgen Wesen,", "tokens": ["Von", "dem", "all\u00b7ge\u00b7gen\u00b7w\u00e4rt\u00b7gen", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.5": {"text": "Und Seiner Allmacht Licht und Schein,", "tokens": ["Und", "Sei\u00b7ner", "All\u00b7macht", "Licht", "und", "Schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Gar bald viel Wunder lernen lesen.", "tokens": ["Gar", "bald", "viel", "Wun\u00b7der", "ler\u00b7nen", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PIAT", "NN", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}