{"textgrid.poem.46184": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie unverhinderlich ein jahr", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie unverhinderlich ein jahr", "tokens": ["Wie", "un\u00b7ver\u00b7hin\u00b7der\u00b7lich", "ein", "jahr"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schnell nach dem andern dahin fliehet!", "tokens": ["schnell", "nach", "dem", "an\u00b7dern", "da\u00b7hin", "flie\u00b7het", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "PAV", "VVFIN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "wie unempfindlich unser haar", "tokens": ["wie", "un\u00b7emp\u00b7find\u00b7lich", "un\u00b7ser", "haar"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich grau zu f\u00e4rben nicht verziehet!", "tokens": ["sich", "grau", "zu", "f\u00e4r\u00b7ben", "nicht", "ver\u00b7zie\u00b7het", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "PTKZU", "VVINF", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "umsunst die fromkeit selbs, die stirn", "tokens": ["um\u00b7sunst", "die", "from\u00b7keit", "selbs", ",", "die", "stirn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ART", "NN", "ADV", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "von r\u00fcnzlen und von sorg das hirn", "tokens": ["von", "r\u00fcnz\u00b7len", "und", "von", "sorg", "das", "hirn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "VVINF", "KON", "APPR", "NE", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "zu freien sich bem\u00fchet.", "tokens": ["zu", "frei\u00b7en", "sich", "be\u00b7m\u00fc\u00b7het", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Lauf alle tag der kirchen zu", "tokens": ["Lauf", "al\u00b7le", "tag", "der", "kir\u00b7chen", "zu"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und dien dem, der allein allm\u00e4chtig", "tokens": ["und", "di\u00b7en", "dem", ",", "der", "al\u00b7lein", "all\u00b7m\u00e4ch\u00b7tig"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "$,", "PRELS", "ADV", "ADJD"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "und ohn erquickung, nahrung, ruh", "tokens": ["und", "ohn", "er\u00b7quic\u00b7kung", ",", "nah\u00b7rung", ",", "ruh"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KON", "APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "erweis dich tag und nacht and\u00e4chtig", "tokens": ["er\u00b7weis", "dich", "tag", "und", "nacht", "an\u00b7d\u00e4ch\u00b7tig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "NN", "KON", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und christlich, so wird endlich doch", "tokens": ["und", "christ\u00b7lich", ",", "so", "wird", "end\u00b7lich", "doch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "ADV", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das unvermeidenliche joch", "tokens": ["das", "un\u00b7ver\u00b7mei\u00b7den\u00b7li\u00b7che", "joch"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "des tods auch durch dich pr\u00e4chtig.", "tokens": ["des", "tods", "auch", "durch", "dich", "pr\u00e4ch\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PPER", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Die s\u00fcnd, die alle menschen gleich", "tokens": ["Die", "s\u00fcnd", ",", "die", "al\u00b7le", "men\u00b7schen", "gleich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "$,", "PRELS", "PIAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "gemachet, machet sie fortgehen", "tokens": ["ge\u00b7ma\u00b7chet", ",", "ma\u00b7chet", "sie", "fort\u00b7ge\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVPP", "$,", "VVFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und lasset weder arm noch reich", "tokens": ["und", "las\u00b7set", "we\u00b7der", "arm", "noch", "reich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "KON", "ADJD", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich l\u00e4nger sprei\u00dfen noch still stehen.", "tokens": ["sich", "l\u00e4n\u00b7ger", "sprei\u00b7\u00dfen", "noch", "still", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "VVFIN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ein junker, herr, graf und monarch", "tokens": ["ein", "jun\u00b7ker", ",", "herr", ",", "graf", "und", "mon\u00b7arch"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "$,", "NN", "$,", "VVFIN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wird wie ein baur mit einem sarch", "tokens": ["wird", "wie", "ein", "baur", "mit", "ei\u00b7nem", "sarch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "KOKOM", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und einem grab versehen.", "tokens": ["und", "ei\u00b7nem", "grab", "ver\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Umsonst der forchtsam f\u00fcr ein weil", "tokens": ["Um\u00b7sonst", "der", "forcht\u00b7sam", "f\u00fcr", "ein", "weil"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJD", "APPR", "ART", "KOUS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem meer, dem krieg, der pest entfliehet,", "tokens": ["dem", "meer", ",", "dem", "krieg", ",", "der", "pest", "ent\u00b7flie\u00b7het", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "PRELS", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan ja der tod, der dan in eil,", "tokens": ["dan", "ja", "der", "tod", ",", "der", "dan", "in", "eil", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "dan langsam ist, nicht lang verziehet:", "tokens": ["dan", "lang\u00b7sam", "ist", ",", "nicht", "lang", "ver\u00b7zie\u00b7het", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "$,", "PTKNEG", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "gleich ist ihm der klein und der gro\u00df", "tokens": ["gleich", "ist", "ihm", "der", "klein", "und", "der", "gro\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "ADJD", "KON", "ART", "ADJD"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "und der gewafnet und der blo\u00df,", "tokens": ["und", "der", "ge\u00b7waf\u00b7net", "und", "der", "blo\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVPP", "KON", "ART", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "der welk und der noch bl\u00fchet.", "tokens": ["der", "welk", "und", "der", "noch", "bl\u00fc\u00b7het", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "ADV", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Umsunst sich setzet ungeduld,", "tokens": ["Um\u00b7sunst", "sich", "set\u00b7zet", "un\u00b7ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "forcht und geheul dem tod entgegen;", "tokens": ["forcht", "und", "ge\u00b7heul", "dem", "tod", "ent\u00b7ge\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "es mu\u00df ein jeder dise schuld", "tokens": ["es", "mu\u00df", "ein", "je\u00b7der", "di\u00b7se", "schuld"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "PIS", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "auf die bestimte zeit ablegen:", "tokens": ["auf", "die", "be\u00b7stim\u00b7te", "zeit", "ab\u00b7le\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "++-+--+--", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "nichts kan den tod, unser geschlecht,", "tokens": ["nichts", "kan", "den", "tod", ",", "un\u00b7ser", "ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "$,", "PPOSAT", "VVPP", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "von staub und aschen ein gem\u00e4cht,", "tokens": ["von", "staub", "und", "asc\u00b7hen", "ein", "ge\u00b7m\u00e4cht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "KON", "VVFIN", "ART", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "zu sparen je bewegen.", "tokens": ["zu", "spa\u00b7ren", "je", "be\u00b7we\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Da m\u00fcssen wir dan alles gut,", "tokens": ["Da", "m\u00fcs\u00b7sen", "wir", "dan", "al\u00b7les", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so wir begehret und erfassen,", "tokens": ["so", "wir", "be\u00b7ge\u00b7hret", "und", "er\u00b7fas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "KON", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "was uns mit hochmut und unmut", "tokens": ["was", "uns", "mit", "hoch\u00b7mut", "und", "un\u00b7mut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "NN", "KON", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "jemals zu lieben und zu hassen", "tokens": ["je\u00b7mals", "zu", "lie\u00b7ben", "und", "zu", "has\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "beliebet, mit dem lieben leib,", "tokens": ["be\u00b7lie\u00b7bet", ",", "mit", "dem", "lie\u00b7ben", "leib", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "haus, hof, spil, kurzweil, kinder, weib", "tokens": ["haus", ",", "hof", ",", "spil", ",", "kurz\u00b7weil", ",", "kin\u00b7der", ",", "weib"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "$,", "VVFIN", "$,", "VVFIN", "$,", "ADJD", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und freind dahinden lassen.", "tokens": ["und", "freind", "da\u00b7hin\u00b7den", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PAV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Hat einer (nichts mehr dan gestank)", "tokens": ["Hat", "ei\u00b7ner", "(", "nichts", "mehr", "dan", "ge\u00b7stank", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "$(", "PIS", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verlassend alles dan beschlossen,", "tokens": ["ver\u00b7las\u00b7send", "al\u00b7les", "dan", "be\u00b7schlos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "erfolget daf\u00fcr schlechter dank", "tokens": ["er\u00b7fol\u00b7get", "da\u00b7f\u00fcr", "schlech\u00b7ter", "dank"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PAV", "ADJD", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "vermischet mit spot, schmach und bossen:", "tokens": ["ver\u00b7mi\u00b7schet", "mit", "spot", ",", "schmach", "und", "bos\u00b7sen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wan er errungen vil alhie", "tokens": ["wan", "er", "er\u00b7run\u00b7gen", "vil", "al\u00b7hie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "VVFIN", "ADV", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "f\u00fcr andre leut mit sorg und m\u00fch", "tokens": ["f\u00fcr", "and\u00b7re", "leut", "mit", "sorg", "und", "m\u00fch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "VVFIN", "APPR", "NN", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und (narr) selbs nicht genossen.", "tokens": ["und", "(", "narr", ")", "selbs", "nicht", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$(", "ADJD", "$(", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Wie unverhinderlich ein jahr", "tokens": ["Wie", "un\u00b7ver\u00b7hin\u00b7der\u00b7lich", "ein", "jahr"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schnell nach dem andern dahin fliehet!", "tokens": ["schnell", "nach", "dem", "an\u00b7dern", "da\u00b7hin", "flie\u00b7het", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "PAV", "VVFIN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "wie unempfindlich unser haar", "tokens": ["wie", "un\u00b7emp\u00b7find\u00b7lich", "un\u00b7ser", "haar"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich grau zu f\u00e4rben nicht verziehet!", "tokens": ["sich", "grau", "zu", "f\u00e4r\u00b7ben", "nicht", "ver\u00b7zie\u00b7het", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "PTKZU", "VVINF", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "umsunst die fromkeit selbs, die stirn", "tokens": ["um\u00b7sunst", "die", "from\u00b7keit", "selbs", ",", "die", "stirn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ART", "NN", "ADV", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "von r\u00fcnzlen und von sorg das hirn", "tokens": ["von", "r\u00fcnz\u00b7len", "und", "von", "sorg", "das", "hirn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "VVINF", "KON", "APPR", "NE", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "zu freien sich bem\u00fchet.", "tokens": ["zu", "frei\u00b7en", "sich", "be\u00b7m\u00fc\u00b7het", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Lauf alle tag der kirchen zu", "tokens": ["Lauf", "al\u00b7le", "tag", "der", "kir\u00b7chen", "zu"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und dien dem, der allein allm\u00e4chtig", "tokens": ["und", "di\u00b7en", "dem", ",", "der", "al\u00b7lein", "all\u00b7m\u00e4ch\u00b7tig"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "$,", "PRELS", "ADV", "ADJD"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "und ohn erquickung, nahrung, ruh", "tokens": ["und", "ohn", "er\u00b7quic\u00b7kung", ",", "nah\u00b7rung", ",", "ruh"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KON", "APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "erweis dich tag und nacht and\u00e4chtig", "tokens": ["er\u00b7weis", "dich", "tag", "und", "nacht", "an\u00b7d\u00e4ch\u00b7tig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "NN", "KON", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und christlich, so wird endlich doch", "tokens": ["und", "christ\u00b7lich", ",", "so", "wird", "end\u00b7lich", "doch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "$,", "ADV", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das unvermeidenliche joch", "tokens": ["das", "un\u00b7ver\u00b7mei\u00b7den\u00b7li\u00b7che", "joch"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "des tods auch durch dich pr\u00e4chtig.", "tokens": ["des", "tods", "auch", "durch", "dich", "pr\u00e4ch\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PPER", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Die s\u00fcnd, die alle menschen gleich", "tokens": ["Die", "s\u00fcnd", ",", "die", "al\u00b7le", "men\u00b7schen", "gleich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "$,", "PRELS", "PIAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "gemachet, machet sie fortgehen", "tokens": ["ge\u00b7ma\u00b7chet", ",", "ma\u00b7chet", "sie", "fort\u00b7ge\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVPP", "$,", "VVFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und lasset weder arm noch reich", "tokens": ["und", "las\u00b7set", "we\u00b7der", "arm", "noch", "reich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "KON", "ADJD", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich l\u00e4nger sprei\u00dfen noch still stehen.", "tokens": ["sich", "l\u00e4n\u00b7ger", "sprei\u00b7\u00dfen", "noch", "still", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "VVFIN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ein junker, herr, graf und monarch", "tokens": ["ein", "jun\u00b7ker", ",", "herr", ",", "graf", "und", "mon\u00b7arch"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "$,", "NN", "$,", "VVFIN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wird wie ein baur mit einem sarch", "tokens": ["wird", "wie", "ein", "baur", "mit", "ei\u00b7nem", "sarch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "KOKOM", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und einem grab versehen.", "tokens": ["und", "ei\u00b7nem", "grab", "ver\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Umsonst der forchtsam f\u00fcr ein weil", "tokens": ["Um\u00b7sonst", "der", "forcht\u00b7sam", "f\u00fcr", "ein", "weil"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJD", "APPR", "ART", "KOUS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem meer, dem krieg, der pest entfliehet,", "tokens": ["dem", "meer", ",", "dem", "krieg", ",", "der", "pest", "ent\u00b7flie\u00b7het", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "PRELS", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan ja der tod, der dan in eil,", "tokens": ["dan", "ja", "der", "tod", ",", "der", "dan", "in", "eil", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "dan langsam ist, nicht lang verziehet:", "tokens": ["dan", "lang\u00b7sam", "ist", ",", "nicht", "lang", "ver\u00b7zie\u00b7het", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "$,", "PTKNEG", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "gleich ist ihm der klein und der gro\u00df", "tokens": ["gleich", "ist", "ihm", "der", "klein", "und", "der", "gro\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "ADJD", "KON", "ART", "ADJD"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "und der gewafnet und der blo\u00df,", "tokens": ["und", "der", "ge\u00b7waf\u00b7net", "und", "der", "blo\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVPP", "KON", "ART", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "der welk und der noch bl\u00fchet.", "tokens": ["der", "welk", "und", "der", "noch", "bl\u00fc\u00b7het", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "ADV", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Umsunst sich setzet ungeduld,", "tokens": ["Um\u00b7sunst", "sich", "set\u00b7zet", "un\u00b7ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "forcht und geheul dem tod entgegen;", "tokens": ["forcht", "und", "ge\u00b7heul", "dem", "tod", "ent\u00b7ge\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "es mu\u00df ein jeder dise schuld", "tokens": ["es", "mu\u00df", "ein", "je\u00b7der", "di\u00b7se", "schuld"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "PIS", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "auf die bestimte zeit ablegen:", "tokens": ["auf", "die", "be\u00b7stim\u00b7te", "zeit", "ab\u00b7le\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "++-+--+--", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "nichts kan den tod, unser geschlecht,", "tokens": ["nichts", "kan", "den", "tod", ",", "un\u00b7ser", "ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "$,", "PPOSAT", "VVPP", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "von staub und aschen ein gem\u00e4cht,", "tokens": ["von", "staub", "und", "asc\u00b7hen", "ein", "ge\u00b7m\u00e4cht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "KON", "VVFIN", "ART", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "zu sparen je bewegen.", "tokens": ["zu", "spa\u00b7ren", "je", "be\u00b7we\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Da m\u00fcssen wir dan alles gut,", "tokens": ["Da", "m\u00fcs\u00b7sen", "wir", "dan", "al\u00b7les", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so wir begehret und erfassen,", "tokens": ["so", "wir", "be\u00b7ge\u00b7hret", "und", "er\u00b7fas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "KON", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "was uns mit hochmut und unmut", "tokens": ["was", "uns", "mit", "hoch\u00b7mut", "und", "un\u00b7mut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "NN", "KON", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "jemals zu lieben und zu hassen", "tokens": ["je\u00b7mals", "zu", "lie\u00b7ben", "und", "zu", "has\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "beliebet, mit dem lieben leib,", "tokens": ["be\u00b7lie\u00b7bet", ",", "mit", "dem", "lie\u00b7ben", "leib", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "haus, hof, spil, kurzweil, kinder, weib", "tokens": ["haus", ",", "hof", ",", "spil", ",", "kurz\u00b7weil", ",", "kin\u00b7der", ",", "weib"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "$,", "VVFIN", "$,", "VVFIN", "$,", "ADJD", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und freind dahinden lassen.", "tokens": ["und", "freind", "da\u00b7hin\u00b7den", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PAV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Hat einer (nichts mehr dan gestank)", "tokens": ["Hat", "ei\u00b7ner", "(", "nichts", "mehr", "dan", "ge\u00b7stank", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "$(", "PIS", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verlassend alles dan beschlossen,", "tokens": ["ver\u00b7las\u00b7send", "al\u00b7les", "dan", "be\u00b7schlos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "erfolget daf\u00fcr schlechter dank", "tokens": ["er\u00b7fol\u00b7get", "da\u00b7f\u00fcr", "schlech\u00b7ter", "dank"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PAV", "ADJD", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "vermischet mit spot, schmach und bossen:", "tokens": ["ver\u00b7mi\u00b7schet", "mit", "spot", ",", "schmach", "und", "bos\u00b7sen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wan er errungen vil alhie", "tokens": ["wan", "er", "er\u00b7run\u00b7gen", "vil", "al\u00b7hie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "VVFIN", "ADV", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "f\u00fcr andre leut mit sorg und m\u00fch", "tokens": ["f\u00fcr", "and\u00b7re", "leut", "mit", "sorg", "und", "m\u00fch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "VVFIN", "APPR", "NN", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und (narr) selbs nicht genossen.", "tokens": ["und", "(", "narr", ")", "selbs", "nicht", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$(", "ADJD", "$(", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}