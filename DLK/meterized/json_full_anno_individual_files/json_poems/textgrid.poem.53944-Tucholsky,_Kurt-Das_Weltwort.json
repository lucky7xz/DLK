{"textgrid.poem.53944": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Das Weltwort", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es gibt in allen Sprachen ein Wort,", "tokens": ["Es", "gibt", "in", "al\u00b7len", "Spra\u00b7chen", "ein", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "das geht von Mund zu Munde;", "tokens": ["das", "geht", "von", "Mund", "zu", "Mun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "es pflanzt sich durch die Lande fort,", "tokens": ["es", "pflanzt", "sich", "durch", "die", "Lan\u00b7de", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und \u00fcberall machts die Runde.", "tokens": ["und", "\u00fc\u00b7be\u00b7rall", "machts", "die", "Run\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Es war einmal gewi\u00df kein Feingut,", "tokens": ["Es", "war", "ein\u00b7mal", "ge\u00b7wi\u00df", "kein", "Fein\u00b7gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "doch nach dem Kriege wurd es Allgemeingut.", "tokens": ["doch", "nach", "dem", "Krie\u00b7ge", "wurd", "es", "All\u00b7ge\u00b7mein\u00b7gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.7": {"text": "Weil ich ein feiner Knabe bin \u2013:", "tokens": ["Weil", "ich", "ein", "fei\u00b7ner", "Kna\u00b7be", "bin", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VAFIN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "wie sag ichs meiner Leserin,", "tokens": ["wie", "sag", "ichs", "mei\u00b7ner", "Le\u00b7se\u00b7rin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "so, da\u00df ich doch gesittet bleibe . . . ?", "tokens": ["so", ",", "da\u00df", "ich", "doch", "ge\u00b7sit\u00b7tet", "blei\u00b7be", ".", ".", ".", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "ADV", "VVPP", "VVFIN", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Vielleicht:", "tokens": ["Viel\u00b7leicht", ":"], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "-+", "measure": "iambic.single"}, "line.11": {"text": "Ja, Scheibe \u2013?", "tokens": ["Ja", ",", "Schei\u00b7be", "\u2013", "?"], "token_info": ["word", "punct", "word", "punct", "punct"], "pos": ["PTKANT", "$,", "NN", "$(", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.2": {"line.1": {"text": "Herr Sternheim ist so m\u00e4chtig eitel \u2013", "tokens": ["Herr", "Stern\u00b7heim", "ist", "so", "m\u00e4ch\u00b7tig", "ei\u00b7tel", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "er w\u00fcnscht sich Rosen auf den Schei \u2013", "tokens": ["er", "w\u00fcnscht", "sich", "Ro\u00b7sen", "auf", "den", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.3": {"line.1": {"text": "Willst du hier eine Ehe trennen,", "tokens": ["Willst", "du", "hier", "ei\u00b7ne", "E\u00b7he", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "so mu\u00dft du einen Grund benennen;", "tokens": ["so", "mu\u00dft", "du", "ei\u00b7nen", "Grund", "be\u00b7nen\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "drei M\u00e4nnchen in Talarverkleidung,", "tokens": ["drei", "M\u00e4nn\u00b7chen", "in", "Ta\u00b7lar\u00b7ver\u00b7klei\u00b7dung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "die w\u00fchlen im Morast der Schei \u2013", "tokens": ["die", "w\u00fch\u00b7len", "im", "Mo\u00b7rast", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "APPRART", "NN", "ART", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.4": {"line.1": {"text": "Da\u00df Deutschland milit\u00e4risch bleibe,", "tokens": ["Da\u00df", "Deutschland", "mi\u00b7li\u00b7t\u00e4\u00b7risch", "blei\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADJD", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "schie\u00dft jeder Stahlhelmfritze nach der Schei \u2013", "tokens": ["schie\u00dft", "je\u00b7der", "Stahl\u00b7helm\u00b7frit\u00b7ze", "nach", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "(schie\u00dfscheiben stehen aller Enden,", "tokens": ["(", "schie\u00df\u00b7schei\u00b7ben", "ste\u00b7hen", "al\u00b7ler", "En\u00b7den", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVINF", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dies Wort ist nur mit Vorsicht zu verwenden.)", "tokens": ["dies", "Wort", "ist", "nur", "mit", "Vor\u00b7sicht", "zu", "ver\u00b7wen\u00b7den", ".", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "APPR", "NN", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Auf da\u00df er seine Frau in Seide lege,", "tokens": ["Auf", "da\u00df", "er", "sei\u00b7ne", "Frau", "in", "Sei\u00b7de", "le\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PPER", "PPOSAT", "NN", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "kratzt mancher Arzt manchmal am Schei \u2013", "tokens": ["kratzt", "man\u00b7cher", "Arzt", "manch\u00b7mal", "am", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.6": {"line.1": {"text": "Das Kabinett? Mir scheint, als ob mir schiene:", "tokens": ["Das", "Ka\u00b7bi\u00b7nett", "?", "Mir", "scheint", ",", "als", "ob", "mir", "schie\u00b7ne", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PPER", "VVFIN", "$,", "KOKOM", "KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "sie machten Wahlen gegen die Marine,", "tokens": ["sie", "mach\u00b7ten", "Wah\u00b7len", "ge\u00b7gen", "die", "Ma\u00b7ri\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "dann fallen sie um und willigen f\u00fcr nen Kreuzer ein.", "tokens": ["dann", "fal\u00b7len", "sie", "um", "und", "wil\u00b7li\u00b7gen", "f\u00fcr", "nen", "Kreu\u00b7zer", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "KON", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Das ist des M\u00fcllers Lust. Wie oft tr\u00fcgt doch der Schei \u2013", "tokens": ["Das", "ist", "des", "M\u00fcl\u00b7lers", "Lust", ".", "Wie", "oft", "tr\u00fcgt", "doch", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "NN", "$.", "PWAV", "ADV", "VVFIN", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.7": {"line.1": {"text": "In allen Sprachen gibt es dies Wort,", "tokens": ["In", "al\u00b7len", "Spra\u00b7chen", "gibt", "es", "dies", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "PPER", "PDS", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "das geht von Mund zu Munde;", "tokens": ["das", "geht", "von", "Mund", "zu", "Mun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "es pflanzt sich durch alle L\u00e4nder fort", "tokens": ["es", "pflanzt", "sich", "durch", "al\u00b7le", "L\u00e4n\u00b7der", "fort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "PIAT", "NN", "PTKVZ"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "und \u00fcberall macht es die Runde.", "tokens": ["und", "\u00fc\u00b7be\u00b7rall", "macht", "es", "die", "Run\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.8": {"line.1": {"text": "Es pa\u00dft auf alles in der Welt . . .", "tokens": ["Es", "pa\u00dft", "auf", "al\u00b7les", "in", "der", "Welt", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "APPR", "ART", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "nur ein Ding gibts, das nicht darunter f\u00e4llt.", "tokens": ["nur", "ein", "Ding", "gibts", ",", "das", "nicht", "da\u00b7run\u00b7ter", "f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "$,", "PRELS", "PTKNEG", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dies Ding \u2013 ein jeder Kenner siehts \u2013", "tokens": ["Dies", "Ding", "\u2013", "ein", "je\u00b7der", "Ken\u00b7ner", "siehts", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "$(", "ART", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ist unsere deutsche Strafjustiz,", "tokens": ["ist", "un\u00b7se\u00b7re", "deut\u00b7sche", "Straf\u00b7jus\u00b7tiz", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Denn die \u2013 mit ihrem Riesenflei\u00dfe \u2013", "tokens": ["Denn", "die", "\u2013", "mit", "ih\u00b7rem", "Rie\u00b7sen\u00b7flei\u00b7\u00dfe", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "$(", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "die letzte Zeile fehlt.", "tokens": ["die", "letz\u00b7te", "Zei\u00b7le", "fehlt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Ich weisse, was ich weisse.", "tokens": ["Ich", "weis\u00b7se", ",", "was", "ich", "weis\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Es gibt in allen Sprachen ein Wort,", "tokens": ["Es", "gibt", "in", "al\u00b7len", "Spra\u00b7chen", "ein", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "das geht von Mund zu Munde;", "tokens": ["das", "geht", "von", "Mund", "zu", "Mun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "es pflanzt sich durch die Lande fort,", "tokens": ["es", "pflanzt", "sich", "durch", "die", "Lan\u00b7de", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und \u00fcberall machts die Runde.", "tokens": ["und", "\u00fc\u00b7be\u00b7rall", "machts", "die", "Run\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Es war einmal gewi\u00df kein Feingut,", "tokens": ["Es", "war", "ein\u00b7mal", "ge\u00b7wi\u00df", "kein", "Fein\u00b7gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "doch nach dem Kriege wurd es Allgemeingut.", "tokens": ["doch", "nach", "dem", "Krie\u00b7ge", "wurd", "es", "All\u00b7ge\u00b7mein\u00b7gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.7": {"text": "Weil ich ein feiner Knabe bin \u2013:", "tokens": ["Weil", "ich", "ein", "fei\u00b7ner", "Kna\u00b7be", "bin", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VAFIN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "wie sag ichs meiner Leserin,", "tokens": ["wie", "sag", "ichs", "mei\u00b7ner", "Le\u00b7se\u00b7rin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "so, da\u00df ich doch gesittet bleibe . . . ?", "tokens": ["so", ",", "da\u00df", "ich", "doch", "ge\u00b7sit\u00b7tet", "blei\u00b7be", ".", ".", ".", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "ADV", "VVPP", "VVFIN", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Vielleicht:", "tokens": ["Viel\u00b7leicht", ":"], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "-+", "measure": "iambic.single"}, "line.11": {"text": "Ja, Scheibe \u2013?", "tokens": ["Ja", ",", "Schei\u00b7be", "\u2013", "?"], "token_info": ["word", "punct", "word", "punct", "punct"], "pos": ["PTKANT", "$,", "NN", "$(", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.10": {"line.1": {"text": "Herr Sternheim ist so m\u00e4chtig eitel \u2013", "tokens": ["Herr", "Stern\u00b7heim", "ist", "so", "m\u00e4ch\u00b7tig", "ei\u00b7tel", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "er w\u00fcnscht sich Rosen auf den Schei \u2013", "tokens": ["er", "w\u00fcnscht", "sich", "Ro\u00b7sen", "auf", "den", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.11": {"line.1": {"text": "Willst du hier eine Ehe trennen,", "tokens": ["Willst", "du", "hier", "ei\u00b7ne", "E\u00b7he", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "so mu\u00dft du einen Grund benennen;", "tokens": ["so", "mu\u00dft", "du", "ei\u00b7nen", "Grund", "be\u00b7nen\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "drei M\u00e4nnchen in Talarverkleidung,", "tokens": ["drei", "M\u00e4nn\u00b7chen", "in", "Ta\u00b7lar\u00b7ver\u00b7klei\u00b7dung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "die w\u00fchlen im Morast der Schei \u2013", "tokens": ["die", "w\u00fch\u00b7len", "im", "Mo\u00b7rast", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "APPRART", "NN", "ART", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.12": {"line.1": {"text": "Da\u00df Deutschland milit\u00e4risch bleibe,", "tokens": ["Da\u00df", "Deutschland", "mi\u00b7li\u00b7t\u00e4\u00b7risch", "blei\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADJD", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "schie\u00dft jeder Stahlhelmfritze nach der Schei \u2013", "tokens": ["schie\u00dft", "je\u00b7der", "Stahl\u00b7helm\u00b7frit\u00b7ze", "nach", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "(schie\u00dfscheiben stehen aller Enden,", "tokens": ["(", "schie\u00df\u00b7schei\u00b7ben", "ste\u00b7hen", "al\u00b7ler", "En\u00b7den", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVINF", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dies Wort ist nur mit Vorsicht zu verwenden.)", "tokens": ["dies", "Wort", "ist", "nur", "mit", "Vor\u00b7sicht", "zu", "ver\u00b7wen\u00b7den", ".", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "APPR", "NN", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Auf da\u00df er seine Frau in Seide lege,", "tokens": ["Auf", "da\u00df", "er", "sei\u00b7ne", "Frau", "in", "Sei\u00b7de", "le\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PPER", "PPOSAT", "NN", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "kratzt mancher Arzt manchmal am Schei \u2013", "tokens": ["kratzt", "man\u00b7cher", "Arzt", "manch\u00b7mal", "am", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.14": {"line.1": {"text": "Das Kabinett? Mir scheint, als ob mir schiene:", "tokens": ["Das", "Ka\u00b7bi\u00b7nett", "?", "Mir", "scheint", ",", "als", "ob", "mir", "schie\u00b7ne", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PPER", "VVFIN", "$,", "KOKOM", "KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "sie machten Wahlen gegen die Marine,", "tokens": ["sie", "mach\u00b7ten", "Wah\u00b7len", "ge\u00b7gen", "die", "Ma\u00b7ri\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "dann fallen sie um und willigen f\u00fcr nen Kreuzer ein.", "tokens": ["dann", "fal\u00b7len", "sie", "um", "und", "wil\u00b7li\u00b7gen", "f\u00fcr", "nen", "Kreu\u00b7zer", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "KON", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Das ist des M\u00fcllers Lust. Wie oft tr\u00fcgt doch der Schei \u2013", "tokens": ["Das", "ist", "des", "M\u00fcl\u00b7lers", "Lust", ".", "Wie", "oft", "tr\u00fcgt", "doch", "der", "Schei", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "NN", "$.", "PWAV", "ADV", "VVFIN", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ja, Scheibe.", "tokens": ["Ja", ",", "Schei\u00b7be", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.15": {"line.1": {"text": "In allen Sprachen gibt es dies Wort,", "tokens": ["In", "al\u00b7len", "Spra\u00b7chen", "gibt", "es", "dies", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "PPER", "PDS", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "das geht von Mund zu Munde;", "tokens": ["das", "geht", "von", "Mund", "zu", "Mun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "es pflanzt sich durch alle L\u00e4nder fort", "tokens": ["es", "pflanzt", "sich", "durch", "al\u00b7le", "L\u00e4n\u00b7der", "fort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "PIAT", "NN", "PTKVZ"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "und \u00fcberall macht es die Runde.", "tokens": ["und", "\u00fc\u00b7be\u00b7rall", "macht", "es", "die", "Run\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.16": {"line.1": {"text": "Es pa\u00dft auf alles in der Welt . . .", "tokens": ["Es", "pa\u00dft", "auf", "al\u00b7les", "in", "der", "Welt", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "APPR", "ART", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "nur ein Ding gibts, das nicht darunter f\u00e4llt.", "tokens": ["nur", "ein", "Ding", "gibts", ",", "das", "nicht", "da\u00b7run\u00b7ter", "f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "$,", "PRELS", "PTKNEG", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dies Ding \u2013 ein jeder Kenner siehts \u2013", "tokens": ["Dies", "Ding", "\u2013", "ein", "je\u00b7der", "Ken\u00b7ner", "siehts", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "$(", "ART", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ist unsere deutsche Strafjustiz,", "tokens": ["ist", "un\u00b7se\u00b7re", "deut\u00b7sche", "Straf\u00b7jus\u00b7tiz", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Denn die \u2013 mit ihrem Riesenflei\u00dfe \u2013", "tokens": ["Denn", "die", "\u2013", "mit", "ih\u00b7rem", "Rie\u00b7sen\u00b7flei\u00b7\u00dfe", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "$(", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "die letzte Zeile fehlt.", "tokens": ["die", "letz\u00b7te", "Zei\u00b7le", "fehlt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Ich weisse, was ich weisse.", "tokens": ["Ich", "weis\u00b7se", ",", "was", "ich", "weis\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}