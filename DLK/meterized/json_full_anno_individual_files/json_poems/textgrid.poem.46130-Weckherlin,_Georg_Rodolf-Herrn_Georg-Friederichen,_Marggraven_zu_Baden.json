{"textgrid.poem.46130": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Herrn Georg-Friederichen, Marggraven zu Baden", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "O gro\u00dfer prinz, in dessen schutz", "tokens": ["O", "gro\u00b7\u00dfer", "prinz", ",", "in", "des\u00b7sen", "schutz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJA", "NN", "$,", "APPR", "PRELAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "sich alle tugenden ergeben,", "tokens": ["sich", "al\u00b7le", "tu\u00b7gen\u00b7den", "er\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df sie, der argen welt zu trutz,", "tokens": ["da\u00df", "sie", ",", "der", "ar\u00b7gen", "welt", "zu", "trutz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "auf erden m\u00f6gen sicher leben;", "tokens": ["auf", "er\u00b7den", "m\u00f6\u00b7gen", "si\u00b7cher", "le\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VMFIN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "prinz, meiner Musen zuversicht,", "tokens": ["prinz", ",", "mei\u00b7ner", "Mu\u00b7sen", "zu\u00b7ver\u00b7sicht", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "verschm\u00e4het dieses b\u00fcchlein nicht,", "tokens": ["ver\u00b7schm\u00e4\u00b7het", "die\u00b7ses", "b\u00fcch\u00b7lein", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "so billich euch, der Musen lehren", "tokens": ["so", "bil\u00b7lich", "euch", ",", "der", "Mu\u00b7sen", "leh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "PPER", "$,", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "gehorsamend, ich mu\u00df verehren.", "tokens": ["ge\u00b7hor\u00b7sa\u00b7mend", ",", "ich", "mu\u00df", "ver\u00b7eh\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Dan als ich neulich nachts allein", "tokens": ["Dan", "als", "ich", "neu\u00b7lich", "nachts", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "hin und her auf dem feld spazierte,", "tokens": ["hin", "und", "her", "auf", "dem", "feld", "spa\u00b7zier\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKVZ", "KON", "ADV", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "wa meine trit mit halbem schein", "tokens": ["wa", "mei\u00b7ne", "trit", "mit", "hal\u00b7bem", "schein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Diana selbs, unachtsam, f\u00fchrte,", "tokens": ["Di\u00b7a\u00b7na", "selbs", ",", "u\u00b7nacht\u00b7sam", ",", "f\u00fchr\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "ADV", "$,", "ADJD", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und, unw\u00fcrsch, bei mir selbs gedacht,", "tokens": ["und", ",", "un\u00b7w\u00fcrsch", ",", "bei", "mir", "selbs", "ge\u00b7dacht", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "$,", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wie \u00fcbel ich die zeit zubracht,", "tokens": ["wie", "\u00fc\u00b7bel", "ich", "die", "zeit", "zu\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "in welcher mit der Musen lehren", "tokens": ["in", "wel\u00b7cher", "mit", "der", "Mu\u00b7sen", "leh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "ich mich selbs thet umsunst beth\u00f6ren:", "tokens": ["ich", "mich", "selbs", "thet", "um\u00b7sunst", "be\u00b7th\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PRF", "ADV", "VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Da fand ich mich an einem ort", "tokens": ["Da", "fand", "ich", "mich", "an", "ei\u00b7nem", "ort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "von ihnen allen umgegeben,", "tokens": ["von", "ih\u00b7nen", "al\u00b7len", "um\u00b7ge\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und sie mit manchem s\u00fc\u00dfen wort", "tokens": ["und", "sie", "mit", "man\u00b7chem", "s\u00fc\u00b7\u00dfen", "wort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "gern meiner meinung widerstreben;", "tokens": ["gern", "mei\u00b7ner", "mei\u00b7nung", "wi\u00b7der\u00b7stre\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "insonderheit kam auch herf\u00fcr", "tokens": ["in\u00b7son\u00b7der\u00b7heit", "kam", "auch", "her\u00b7f\u00fcr"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ihr f\u00fchrer Ph\u00f6bus, welcher mir", "tokens": ["ihr", "f\u00fch\u00b7rer", "Ph\u00f6\u00b7bus", ",", "wel\u00b7cher", "mir"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "PRELS", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "gleich f\u00fcr sie all auf meine klagen", "tokens": ["gleich", "f\u00fcr", "sie", "all", "auf", "mei\u00b7ne", "kla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPER", "PIAT", "APPR", "PPOSAT", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "selbs ihr bedenken f\u00fcrgetragen.", "tokens": ["selbs", "ihr", "be\u00b7den\u00b7ken", "f\u00fcr\u00b7ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "\u00bbla\u00df, Filodor, sprach er, nu mehr", "tokens": ["\u00bb", "la\u00df", ",", "Fi\u00b7lo\u00b7dor", ",", "sprach", "er", ",", "nu", "mehr"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["$(", "VVIMP", "$,", "NE", "$,", "VVFIN", "PPER", "$,", "ADV", "ADV"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "forcht, sorgen und verdru\u00df hinfahren;", "tokens": ["forcht", ",", "sor\u00b7gen", "und", "ver\u00b7dru\u00df", "hin\u00b7fah\u00b7ren", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan ein f\u00fcrst, aller f\u00fcrsten ehr,", "tokens": ["dan", "ein", "f\u00fcrst", ",", "al\u00b7ler", "f\u00fcrs\u00b7ten", "ehr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADV", "$,", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "kan und will uns und dich bewahren,", "tokens": ["kan", "und", "will", "uns", "und", "dich", "be\u00b7wah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "KON", "VMFIN", "PPER", "KON", "PPER", "VVINF", "$,"], "meter": "-----+-+-", "measure": "unknown.measure.di"}, "line.5": {"text": "und ist sein nam (wie seine gunst", "tokens": ["und", "ist", "sein", "nam", "(", "wie", "sei\u00b7ne", "gunst"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPOSAT", "NN", "$(", "KOKOM", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ein schirm und ein pfand unsrer kunst)", "tokens": ["ein", "schirm", "und", "ein", "pfand", "uns\u00b7rer", "kunst", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Georg-Friderich Marggraf zu Baden,", "tokens": ["Ge\u00b7or\u00b7gFri\u00b7de\u00b7rich", "Marg\u00b7graf", "zu", "Ba\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "bei welchem wir und du in gnaden.", "tokens": ["bei", "wel\u00b7chem", "wir", "und", "du", "in", "gna\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "KON", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ihm opfer du auf dein gesang,", "tokens": ["Ihm", "op\u00b7fer", "du", "auf", "dein", "ge\u00b7sang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und scheu dich nicht f\u00fcr seinen wafen,", "tokens": ["und", "scheu", "dich", "nicht", "f\u00fcr", "sei\u00b7nen", "wa\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PPER", "PTKNEG", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ab welchen seinen feinden bang", "tokens": ["ab", "wel\u00b7chen", "sei\u00b7nen", "fein\u00b7den", "bang"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "weil sie der h\u00f6chst durch ihn will strafen:", "tokens": ["weil", "sie", "der", "h\u00f6chst", "durch", "ihn", "will", "stra\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADV", "APPR", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dan alle helden alle zeit", "tokens": ["dan", "al\u00b7le", "hel\u00b7den", "al\u00b7le", "zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PIAT", "NN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "die haben vor, in, nach dem streit", "tokens": ["die", "ha\u00b7ben", "vor", ",", "in", ",", "nach", "dem", "streit"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PTKVZ", "$,", "APPR", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "uns stets geliebet und geehret", "tokens": ["uns", "stets", "ge\u00b7lie\u00b7bet", "und", "ge\u00b7eh\u00b7ret"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVPP", "KON", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und ihrer gegenwart gewehret.", "tokens": ["und", "ih\u00b7rer", "ge\u00b7gen\u00b7wart", "ge\u00b7weh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Auch ist ihr lob noch billich kund,", "tokens": ["Auch", "ist", "ihr", "lob", "noch", "bil\u00b7lich", "kund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "kan ewiglich auch nicht vergehen,", "tokens": ["kan", "e\u00b7wig\u00b7lich", "auch", "nicht", "ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "als welches des poeten mund", "tokens": ["als", "wel\u00b7ches", "des", "po\u00b7e\u00b7ten", "mund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PRELS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der ganzen welt gab zu verstehen;", "tokens": ["der", "gan\u00b7zen", "welt", "gab", "zu", "ver\u00b7ste\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "darum gib ihm auch ehr und preis,", "tokens": ["da\u00b7rum", "gib", "ihm", "auch", "ehr", "und", "preis", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVIMP", "PPER", "ADV", "NN", "KON", "PTKVZ", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.6": {"text": "dan ja der menschen h\u00f6chster flei\u00df", "tokens": ["dan", "ja", "der", "men\u00b7schen", "h\u00f6chs\u00b7ter", "flei\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.7": {"text": "kan nichts den g\u00f6ttern mehr erweisen,", "tokens": ["kan", "nichts", "den", "g\u00f6t\u00b7tern", "mehr", "er\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "dan sie zu loben und zu preisen.", "tokens": ["dan", "sie", "zu", "lo\u00b7ben", "und", "zu", "prei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Als Jupiter durch seinen strahl", "tokens": ["Als", "Ju\u00b7pi\u00b7ter", "durch", "sei\u00b7nen", "strahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den risen ihren stolz verk\u00fcrzet", "tokens": ["den", "ri\u00b7sen", "ih\u00b7ren", "stolz", "ver\u00b7k\u00fcr\u00b7zet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "VVFIN", "PPOSAT", "ADJD", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und von des himmels hohem wahl", "tokens": ["und", "von", "des", "him\u00b7mels", "ho\u00b7hem", "wahl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sie \u00fcber und \u00fcber gest\u00fcrzet,", "tokens": ["sie", "\u00fc\u00b7ber", "und", "\u00fc\u00b7ber", "ge\u00b7st\u00fcr\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "KON", "APPR", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "begehrte er, sigreich, mehr nicht,", "tokens": ["be\u00b7gehr\u00b7te", "er", ",", "sig\u00b7reich", ",", "mehr", "nicht", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ADJD", "$,", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "dan da\u00df wir ihm laut ein gedicht", "tokens": ["dan", "da\u00df", "wir", "ihm", "laut", "ein", "ge\u00b7dicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "APPR", "ART", "NN"], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}, "line.7": {"text": "f\u00fcr seinen ohren wol erklangen", "tokens": ["f\u00fcr", "sei\u00b7nen", "oh\u00b7ren", "wol", "er\u00b7klan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und seinen sig und lob gleich sangen.", "tokens": ["und", "sei\u00b7nen", "sig", "und", "lob", "gleich", "san\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJD", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wolan dan, ewig weiser chor", "tokens": ["Wo\u00b7lan", "dan", ",", "e\u00b7wig", "wei\u00b7ser", "chor"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "la\u00dft uns auch disen f\u00fcrsten ehren,", "tokens": ["la\u00dft", "uns", "auch", "di\u00b7sen", "f\u00fcrs\u00b7ten", "eh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "PDS", "VVFIN", "VVINF", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "und du sing mit uns, Filodor,", "tokens": ["und", "du", "sing", "mit", "uns", ",", "Fi\u00b7lo\u00b7dor", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPER", "$,", "NE", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "als deinen mund wir werden lehren;", "tokens": ["als", "dei\u00b7nen", "mund", "wir", "wer\u00b7den", "leh\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PPER", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und weil er uns ja ein patron,", "tokens": ["und", "weil", "er", "uns", "ja", "ein", "pat\u00b7ron", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "so la\u00dft uns flechten eine kron,", "tokens": ["so", "la\u00dft", "uns", "flech\u00b7ten", "ei\u00b7ne", "kron", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "die du dem\u00fctig ihm solt bringen.\u00ab", "tokens": ["die", "du", "de\u00b7m\u00fc\u00b7tig", "ihm", "solt", "brin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPER", "ADJD", "PPER", "VMFIN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "fieng demnach also an zu singen.", "tokens": ["fi\u00b7eng", "dem\u00b7nach", "al\u00b7so", "an", "zu", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "ADV", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "O gro\u00dfer prinz, in dessen schutz", "tokens": ["O", "gro\u00b7\u00dfer", "prinz", ",", "in", "des\u00b7sen", "schutz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJA", "NN", "$,", "APPR", "PRELAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "sich alle tugenden ergeben,", "tokens": ["sich", "al\u00b7le", "tu\u00b7gen\u00b7den", "er\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df sie, der argen welt zu trutz,", "tokens": ["da\u00df", "sie", ",", "der", "ar\u00b7gen", "welt", "zu", "trutz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "auf erden m\u00f6gen sicher leben;", "tokens": ["auf", "er\u00b7den", "m\u00f6\u00b7gen", "si\u00b7cher", "le\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VMFIN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "prinz, meiner Musen zuversicht,", "tokens": ["prinz", ",", "mei\u00b7ner", "Mu\u00b7sen", "zu\u00b7ver\u00b7sicht", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "verschm\u00e4het dieses b\u00fcchlein nicht,", "tokens": ["ver\u00b7schm\u00e4\u00b7het", "die\u00b7ses", "b\u00fcch\u00b7lein", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "so billich euch, der Musen lehren", "tokens": ["so", "bil\u00b7lich", "euch", ",", "der", "Mu\u00b7sen", "leh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "PPER", "$,", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "gehorsamend, ich mu\u00df verehren.", "tokens": ["ge\u00b7hor\u00b7sa\u00b7mend", ",", "ich", "mu\u00df", "ver\u00b7eh\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Dan als ich neulich nachts allein", "tokens": ["Dan", "als", "ich", "neu\u00b7lich", "nachts", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "hin und her auf dem feld spazierte,", "tokens": ["hin", "und", "her", "auf", "dem", "feld", "spa\u00b7zier\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKVZ", "KON", "ADV", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "wa meine trit mit halbem schein", "tokens": ["wa", "mei\u00b7ne", "trit", "mit", "hal\u00b7bem", "schein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Diana selbs, unachtsam, f\u00fchrte,", "tokens": ["Di\u00b7a\u00b7na", "selbs", ",", "u\u00b7nacht\u00b7sam", ",", "f\u00fchr\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "ADV", "$,", "ADJD", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und, unw\u00fcrsch, bei mir selbs gedacht,", "tokens": ["und", ",", "un\u00b7w\u00fcrsch", ",", "bei", "mir", "selbs", "ge\u00b7dacht", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "$,", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wie \u00fcbel ich die zeit zubracht,", "tokens": ["wie", "\u00fc\u00b7bel", "ich", "die", "zeit", "zu\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "in welcher mit der Musen lehren", "tokens": ["in", "wel\u00b7cher", "mit", "der", "Mu\u00b7sen", "leh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "ich mich selbs thet umsunst beth\u00f6ren:", "tokens": ["ich", "mich", "selbs", "thet", "um\u00b7sunst", "be\u00b7th\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PRF", "ADV", "VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Da fand ich mich an einem ort", "tokens": ["Da", "fand", "ich", "mich", "an", "ei\u00b7nem", "ort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "von ihnen allen umgegeben,", "tokens": ["von", "ih\u00b7nen", "al\u00b7len", "um\u00b7ge\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und sie mit manchem s\u00fc\u00dfen wort", "tokens": ["und", "sie", "mit", "man\u00b7chem", "s\u00fc\u00b7\u00dfen", "wort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "gern meiner meinung widerstreben;", "tokens": ["gern", "mei\u00b7ner", "mei\u00b7nung", "wi\u00b7der\u00b7stre\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "insonderheit kam auch herf\u00fcr", "tokens": ["in\u00b7son\u00b7der\u00b7heit", "kam", "auch", "her\u00b7f\u00fcr"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ihr f\u00fchrer Ph\u00f6bus, welcher mir", "tokens": ["ihr", "f\u00fch\u00b7rer", "Ph\u00f6\u00b7bus", ",", "wel\u00b7cher", "mir"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "PRELS", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "gleich f\u00fcr sie all auf meine klagen", "tokens": ["gleich", "f\u00fcr", "sie", "all", "auf", "mei\u00b7ne", "kla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPER", "PIAT", "APPR", "PPOSAT", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "selbs ihr bedenken f\u00fcrgetragen.", "tokens": ["selbs", "ihr", "be\u00b7den\u00b7ken", "f\u00fcr\u00b7ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "\u00bbla\u00df, Filodor, sprach er, nu mehr", "tokens": ["\u00bb", "la\u00df", ",", "Fi\u00b7lo\u00b7dor", ",", "sprach", "er", ",", "nu", "mehr"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["$(", "VVIMP", "$,", "NE", "$,", "VVFIN", "PPER", "$,", "ADV", "ADV"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "forcht, sorgen und verdru\u00df hinfahren;", "tokens": ["forcht", ",", "sor\u00b7gen", "und", "ver\u00b7dru\u00df", "hin\u00b7fah\u00b7ren", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan ein f\u00fcrst, aller f\u00fcrsten ehr,", "tokens": ["dan", "ein", "f\u00fcrst", ",", "al\u00b7ler", "f\u00fcrs\u00b7ten", "ehr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADV", "$,", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "kan und will uns und dich bewahren,", "tokens": ["kan", "und", "will", "uns", "und", "dich", "be\u00b7wah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "KON", "VMFIN", "PPER", "KON", "PPER", "VVINF", "$,"], "meter": "-----+-+-", "measure": "unknown.measure.di"}, "line.5": {"text": "und ist sein nam (wie seine gunst", "tokens": ["und", "ist", "sein", "nam", "(", "wie", "sei\u00b7ne", "gunst"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPOSAT", "NN", "$(", "KOKOM", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ein schirm und ein pfand unsrer kunst)", "tokens": ["ein", "schirm", "und", "ein", "pfand", "uns\u00b7rer", "kunst", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Georg-Friderich Marggraf zu Baden,", "tokens": ["Ge\u00b7or\u00b7gFri\u00b7de\u00b7rich", "Marg\u00b7graf", "zu", "Ba\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "bei welchem wir und du in gnaden.", "tokens": ["bei", "wel\u00b7chem", "wir", "und", "du", "in", "gna\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "KON", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ihm opfer du auf dein gesang,", "tokens": ["Ihm", "op\u00b7fer", "du", "auf", "dein", "ge\u00b7sang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und scheu dich nicht f\u00fcr seinen wafen,", "tokens": ["und", "scheu", "dich", "nicht", "f\u00fcr", "sei\u00b7nen", "wa\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PPER", "PTKNEG", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ab welchen seinen feinden bang", "tokens": ["ab", "wel\u00b7chen", "sei\u00b7nen", "fein\u00b7den", "bang"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "weil sie der h\u00f6chst durch ihn will strafen:", "tokens": ["weil", "sie", "der", "h\u00f6chst", "durch", "ihn", "will", "stra\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADV", "APPR", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dan alle helden alle zeit", "tokens": ["dan", "al\u00b7le", "hel\u00b7den", "al\u00b7le", "zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PIAT", "NN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "die haben vor, in, nach dem streit", "tokens": ["die", "ha\u00b7ben", "vor", ",", "in", ",", "nach", "dem", "streit"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PTKVZ", "$,", "APPR", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "uns stets geliebet und geehret", "tokens": ["uns", "stets", "ge\u00b7lie\u00b7bet", "und", "ge\u00b7eh\u00b7ret"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVPP", "KON", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und ihrer gegenwart gewehret.", "tokens": ["und", "ih\u00b7rer", "ge\u00b7gen\u00b7wart", "ge\u00b7weh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Auch ist ihr lob noch billich kund,", "tokens": ["Auch", "ist", "ihr", "lob", "noch", "bil\u00b7lich", "kund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "kan ewiglich auch nicht vergehen,", "tokens": ["kan", "e\u00b7wig\u00b7lich", "auch", "nicht", "ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "als welches des poeten mund", "tokens": ["als", "wel\u00b7ches", "des", "po\u00b7e\u00b7ten", "mund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PRELS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der ganzen welt gab zu verstehen;", "tokens": ["der", "gan\u00b7zen", "welt", "gab", "zu", "ver\u00b7ste\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "darum gib ihm auch ehr und preis,", "tokens": ["da\u00b7rum", "gib", "ihm", "auch", "ehr", "und", "preis", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVIMP", "PPER", "ADV", "NN", "KON", "PTKVZ", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.6": {"text": "dan ja der menschen h\u00f6chster flei\u00df", "tokens": ["dan", "ja", "der", "men\u00b7schen", "h\u00f6chs\u00b7ter", "flei\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.7": {"text": "kan nichts den g\u00f6ttern mehr erweisen,", "tokens": ["kan", "nichts", "den", "g\u00f6t\u00b7tern", "mehr", "er\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "dan sie zu loben und zu preisen.", "tokens": ["dan", "sie", "zu", "lo\u00b7ben", "und", "zu", "prei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Als Jupiter durch seinen strahl", "tokens": ["Als", "Ju\u00b7pi\u00b7ter", "durch", "sei\u00b7nen", "strahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den risen ihren stolz verk\u00fcrzet", "tokens": ["den", "ri\u00b7sen", "ih\u00b7ren", "stolz", "ver\u00b7k\u00fcr\u00b7zet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "VVFIN", "PPOSAT", "ADJD", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und von des himmels hohem wahl", "tokens": ["und", "von", "des", "him\u00b7mels", "ho\u00b7hem", "wahl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sie \u00fcber und \u00fcber gest\u00fcrzet,", "tokens": ["sie", "\u00fc\u00b7ber", "und", "\u00fc\u00b7ber", "ge\u00b7st\u00fcr\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "KON", "APPR", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "begehrte er, sigreich, mehr nicht,", "tokens": ["be\u00b7gehr\u00b7te", "er", ",", "sig\u00b7reich", ",", "mehr", "nicht", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ADJD", "$,", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "dan da\u00df wir ihm laut ein gedicht", "tokens": ["dan", "da\u00df", "wir", "ihm", "laut", "ein", "ge\u00b7dicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "APPR", "ART", "NN"], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}, "line.7": {"text": "f\u00fcr seinen ohren wol erklangen", "tokens": ["f\u00fcr", "sei\u00b7nen", "oh\u00b7ren", "wol", "er\u00b7klan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und seinen sig und lob gleich sangen.", "tokens": ["und", "sei\u00b7nen", "sig", "und", "lob", "gleich", "san\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJD", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Wolan dan, ewig weiser chor", "tokens": ["Wo\u00b7lan", "dan", ",", "e\u00b7wig", "wei\u00b7ser", "chor"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "la\u00dft uns auch disen f\u00fcrsten ehren,", "tokens": ["la\u00dft", "uns", "auch", "di\u00b7sen", "f\u00fcrs\u00b7ten", "eh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "PDS", "VVFIN", "VVINF", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "und du sing mit uns, Filodor,", "tokens": ["und", "du", "sing", "mit", "uns", ",", "Fi\u00b7lo\u00b7dor", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPER", "$,", "NE", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "als deinen mund wir werden lehren;", "tokens": ["als", "dei\u00b7nen", "mund", "wir", "wer\u00b7den", "leh\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PPER", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und weil er uns ja ein patron,", "tokens": ["und", "weil", "er", "uns", "ja", "ein", "pat\u00b7ron", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "so la\u00dft uns flechten eine kron,", "tokens": ["so", "la\u00dft", "uns", "flech\u00b7ten", "ei\u00b7ne", "kron", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "die du dem\u00fctig ihm solt bringen.\u00ab", "tokens": ["die", "du", "de\u00b7m\u00fc\u00b7tig", "ihm", "solt", "brin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPER", "ADJD", "PPER", "VMFIN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "fieng demnach also an zu singen.", "tokens": ["fi\u00b7eng", "dem\u00b7nach", "al\u00b7so", "an", "zu", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "ADV", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}}}}