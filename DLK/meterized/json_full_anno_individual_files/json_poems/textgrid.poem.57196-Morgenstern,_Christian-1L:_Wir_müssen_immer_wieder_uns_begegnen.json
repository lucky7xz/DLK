{"textgrid.poem.57196": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wir m\u00fcssen immer wieder uns begegnen", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir m\u00fcssen immer wieder uns begegnen", "tokens": ["Wir", "m\u00fcs\u00b7sen", "im\u00b7mer", "wie\u00b7der", "uns", "be\u00b7geg\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "PPER", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und immer wieder durch einander leiden,", "tokens": ["und", "im\u00b7mer", "wie\u00b7der", "durch", "ein\u00b7an\u00b7der", "lei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "bis eines Tages wir das alles segnen.", "tokens": ["bis", "ei\u00b7nes", "Ta\u00b7ges", "wir", "das", "al\u00b7les", "seg\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PPER", "ART", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "An diesem Tage wird das Leiden weichen,", "tokens": ["An", "die\u00b7sem", "Ta\u00b7ge", "wird", "das", "Lei\u00b7den", "wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "das Leiden wenigstens, das Blindheit zeugte,", "tokens": ["das", "Lei\u00b7den", "we\u00b7nigs\u00b7tens", ",", "das", "Blind\u00b7heit", "zeug\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "das uns wie blinden Wald im Sturme beugte.", "tokens": ["das", "uns", "wie", "blin\u00b7den", "Wald", "im", "Stur\u00b7me", "beug\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "KOKOM", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Dann werden wir in neues Ziel und Leben", "tokens": ["Dann", "wer\u00b7den", "wir", "in", "neu\u00b7es", "Ziel", "und", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wie Fl\u00fcsse in ein Meer zusammenflie\u00dfen,", "tokens": ["wie", "Fl\u00fcs\u00b7se", "in", "ein", "Meer", "zu\u00b7sam\u00b7men\u00b7flie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und kein Getrenntsein wird uns mehr verdrie\u00dfen.", "tokens": ["und", "kein", "Ge\u00b7trenn\u00b7tsein", "wird", "uns", "mehr", "ver\u00b7drie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Dann endlich wird das \u00bb ... suchet nicht das Ihre\u00ab", "tokens": ["Dann", "end\u00b7lich", "wird", "das", "\u00bb", "...", "su\u00b7chet", "nicht", "das", "Ih\u00b7re", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ART", "$(", "$(", "VVFIN", "PTKNEG", "ART", "PPOSAT", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wahrheit geworden sein in unsern Seelen.", "tokens": ["Wahr\u00b7heit", "ge\u00b7wor\u00b7den", "sein", "in", "un\u00b7sern", "See\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAPP", "VAINF", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Und wie an Kraft wird's uns an Gl\u00fcck nicht fehlen.", "tokens": ["Und", "wie", "an", "Kraft", "wird's", "uns", "an", "Gl\u00fcck", "nicht", "feh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "APPR", "NN", "VAFIN", "PPER", "APPR", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Wir m\u00fcssen immer wieder uns begegnen", "tokens": ["Wir", "m\u00fcs\u00b7sen", "im\u00b7mer", "wie\u00b7der", "uns", "be\u00b7geg\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "PPER", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und immer wieder durch einander leiden,", "tokens": ["und", "im\u00b7mer", "wie\u00b7der", "durch", "ein\u00b7an\u00b7der", "lei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "bis eines Tages wir das alles segnen.", "tokens": ["bis", "ei\u00b7nes", "Ta\u00b7ges", "wir", "das", "al\u00b7les", "seg\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PPER", "ART", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "An diesem Tage wird das Leiden weichen,", "tokens": ["An", "die\u00b7sem", "Ta\u00b7ge", "wird", "das", "Lei\u00b7den", "wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "das Leiden wenigstens, das Blindheit zeugte,", "tokens": ["das", "Lei\u00b7den", "we\u00b7nigs\u00b7tens", ",", "das", "Blind\u00b7heit", "zeug\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "das uns wie blinden Wald im Sturme beugte.", "tokens": ["das", "uns", "wie", "blin\u00b7den", "Wald", "im", "Stur\u00b7me", "beug\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "KOKOM", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Dann werden wir in neues Ziel und Leben", "tokens": ["Dann", "wer\u00b7den", "wir", "in", "neu\u00b7es", "Ziel", "und", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wie Fl\u00fcsse in ein Meer zusammenflie\u00dfen,", "tokens": ["wie", "Fl\u00fcs\u00b7se", "in", "ein", "Meer", "zu\u00b7sam\u00b7men\u00b7flie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und kein Getrenntsein wird uns mehr verdrie\u00dfen.", "tokens": ["und", "kein", "Ge\u00b7trenn\u00b7tsein", "wird", "uns", "mehr", "ver\u00b7drie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Dann endlich wird das \u00bb ... suchet nicht das Ihre\u00ab", "tokens": ["Dann", "end\u00b7lich", "wird", "das", "\u00bb", "...", "su\u00b7chet", "nicht", "das", "Ih\u00b7re", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ART", "$(", "$(", "VVFIN", "PTKNEG", "ART", "PPOSAT", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wahrheit geworden sein in unsern Seelen.", "tokens": ["Wahr\u00b7heit", "ge\u00b7wor\u00b7den", "sein", "in", "un\u00b7sern", "See\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAPP", "VAINF", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Und wie an Kraft wird's uns an Gl\u00fcck nicht fehlen.", "tokens": ["Und", "wie", "an", "Kraft", "wird's", "uns", "an", "Gl\u00fcck", "nicht", "feh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "APPR", "NN", "VAFIN", "PPER", "APPR", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}