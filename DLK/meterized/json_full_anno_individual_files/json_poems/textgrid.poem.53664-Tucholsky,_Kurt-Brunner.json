{"textgrid.poem.53664": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Brunner", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Amor klopfte an ein Hosent\u00fcrchen,", "tokens": ["Ein", "A\u00b7mor", "klopf\u00b7te", "an", "ein", "Ho\u00b7sen\u00b7t\u00fcr\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "doch niemand rief; \u00bbHerein!\u00ab", "tokens": ["doch", "nie\u00b7mand", "rief", ";", "\u00bb", "Her\u00b7ein", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$.", "$(", "PTKVZ", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und nun entflattert leise das Fig\u00fcrchen", "tokens": ["Und", "nun", "ent\u00b7flat\u00b7tert", "lei\u00b7se", "das", "Fi\u00b7g\u00fcr\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und lie\u00df den Mann allein.", "tokens": ["und", "lie\u00df", "den", "Mann", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Der sieht die Nacktheit und geniert sich.", "tokens": ["Der", "sieht", "die", "Nackt\u00b7heit", "und", "ge\u00b7niert", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "KON", "VVFIN", "PRF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Er m\u00f6cht ja gern der Venus nahn,", "tokens": ["Er", "m\u00f6cht", "ja", "gern", "der", "Ve\u00b7nus", "nahn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "ART", "NN", "ADJA", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "doch was sich liebt, das konfisziert sich.", "tokens": ["doch", "was", "sich", "liebt", ",", "das", "kon\u00b7fis\u00b7ziert", "sich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PRF", "VVFIN", "$,", "PDS", "VMFIN", "PRF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Es liegt ein Brunner an der Lahn.", "tokens": ["Es", "liegt", "ein", "Brun\u00b7ner", "an", "der", "Lahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Als Kn\u00e4blein hat er schon \u2013 wie an mein Ohr kam \u2013", "tokens": ["Als", "Kn\u00e4\u00b7blein", "hat", "er", "schon", "\u2013", "wie", "an", "mein", "Ohr", "kam", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VAFIN", "PPER", "ADV", "$(", "KOKOM", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "in seinem Bibelband,", "tokens": ["in", "sei\u00b7nem", "Bi\u00b7bel\u00b7band", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "gez\u00e4hlt, wie oft da \u203azeugen\u2039 vorkam \u2013", "tokens": ["ge\u00b7z\u00e4hlt", ",", "wie", "oft", "da", "\u203a", "zeu\u00b7gen", "\u2039", "vor\u00b7kam", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "PWAV", "ADV", "ADV", "$(", "VVFIN", "$(", "VVFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und was er sonst noch fand.", "tokens": ["und", "was", "er", "sonst", "noch", "fand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die Bilder, die dem Reinen rein sind,", "tokens": ["Die", "Bil\u00b7der", ",", "die", "dem", "Rei\u00b7nen", "rein", "sind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "verwehrt er heut dem Untertan,", "tokens": ["ver\u00b7wehrt", "er", "heut", "dem", "Un\u00b7ter\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "stellt fest, wof\u00fcr wir noch zu klein sind \u2013", "tokens": ["stellt", "fest", ",", "wo\u00b7f\u00fcr", "wir", "noch", "zu", "klein", "sind", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "PWAV", "PPER", "ADV", "PTKA", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und denkt, ganz Deutschland liege an der Lahn.", "tokens": ["und", "denkt", ",", "ganz", "Deutschland", "lie\u00b7ge", "an", "der", "Lahn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ADV", "NE", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}}, "stanza.3": {"line.1": {"text": "Wer seine Nase nur in Schweinerein steckt,", "tokens": ["Wer", "sei\u00b7ne", "Na\u00b7se", "nur", "in", "Schwei\u00b7ne\u00b7rein", "steckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "verliert das Gleichgewicht:", "tokens": ["ver\u00b7liert", "das", "Gleich\u00b7ge\u00b7wicht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wenn auf den Bl\u00e4ttern auch die Frau das Bein streckt:", "tokens": ["Wenn", "auf", "den", "Bl\u00e4t\u00b7tern", "auch", "die", "Frau", "das", "Bein", "streckt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "uns st\u00f6rt das weiter nicht.", "tokens": ["uns", "st\u00f6rt", "das", "wei\u00b7ter", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADV", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Was schert uns frohe und gesunde Esser", "tokens": ["Was", "schert", "uns", "fro\u00b7he", "und", "ge\u00b7sun\u00b7de", "Es\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "ADJA", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "denn Seine Impotenz, der Herr Professor!", "tokens": ["denn", "Sei\u00b7ne", "Im\u00b7po\u00b7tenz", ",", "der", "Herr", "Pro\u00b7fes\u00b7sor", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+---", "measure": "unknown.measure.tetra"}, "line.7": {"text": "Da soll doch gleich . . . !", "tokens": ["Da", "soll", "doch", "gleich", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "VMFIN", "ADV", "ADV", "$.", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "Die Putten fliegen.", "tokens": ["Die", "Put\u00b7ten", "flie\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Hell seh ich Aphroditen liegen.", "tokens": ["Hell", "seh", "ich", "A\u00b7phro\u00b7di\u00b7ten", "lie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.10": {"text": "Sie l\u00e4chelt: \u00bbTiger, la\u00df ihn gehn!", "tokens": ["Sie", "l\u00e4\u00b7chelt", ":", "\u00bb", "Ti\u00b7ger", ",", "la\u00df", "ihn", "gehn", "!"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "$,", "VVIMP", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Er kennt mich nicht. Er hat mich nie gesehn!\u00ab", "tokens": ["Er", "kennt", "mich", "nicht", ".", "Er", "hat", "mich", "nie", "ge\u00b7sehn", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$.", "PPER", "VAFIN", "PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Ein Amor klopfte an ein Hosent\u00fcrchen,", "tokens": ["Ein", "A\u00b7mor", "klopf\u00b7te", "an", "ein", "Ho\u00b7sen\u00b7t\u00fcr\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "doch niemand rief; \u00bbHerein!\u00ab", "tokens": ["doch", "nie\u00b7mand", "rief", ";", "\u00bb", "Her\u00b7ein", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$.", "$(", "PTKVZ", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und nun entflattert leise das Fig\u00fcrchen", "tokens": ["Und", "nun", "ent\u00b7flat\u00b7tert", "lei\u00b7se", "das", "Fi\u00b7g\u00fcr\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und lie\u00df den Mann allein.", "tokens": ["und", "lie\u00df", "den", "Mann", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Der sieht die Nacktheit und geniert sich.", "tokens": ["Der", "sieht", "die", "Nackt\u00b7heit", "und", "ge\u00b7niert", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "KON", "VVFIN", "PRF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Er m\u00f6cht ja gern der Venus nahn,", "tokens": ["Er", "m\u00f6cht", "ja", "gern", "der", "Ve\u00b7nus", "nahn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "ART", "NN", "ADJA", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "doch was sich liebt, das konfisziert sich.", "tokens": ["doch", "was", "sich", "liebt", ",", "das", "kon\u00b7fis\u00b7ziert", "sich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PRF", "VVFIN", "$,", "PDS", "VMFIN", "PRF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Es liegt ein Brunner an der Lahn.", "tokens": ["Es", "liegt", "ein", "Brun\u00b7ner", "an", "der", "Lahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Als Kn\u00e4blein hat er schon \u2013 wie an mein Ohr kam \u2013", "tokens": ["Als", "Kn\u00e4\u00b7blein", "hat", "er", "schon", "\u2013", "wie", "an", "mein", "Ohr", "kam", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VAFIN", "PPER", "ADV", "$(", "KOKOM", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "in seinem Bibelband,", "tokens": ["in", "sei\u00b7nem", "Bi\u00b7bel\u00b7band", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "gez\u00e4hlt, wie oft da \u203azeugen\u2039 vorkam \u2013", "tokens": ["ge\u00b7z\u00e4hlt", ",", "wie", "oft", "da", "\u203a", "zeu\u00b7gen", "\u2039", "vor\u00b7kam", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "PWAV", "ADV", "ADV", "$(", "VVFIN", "$(", "VVFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und was er sonst noch fand.", "tokens": ["und", "was", "er", "sonst", "noch", "fand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die Bilder, die dem Reinen rein sind,", "tokens": ["Die", "Bil\u00b7der", ",", "die", "dem", "Rei\u00b7nen", "rein", "sind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "verwehrt er heut dem Untertan,", "tokens": ["ver\u00b7wehrt", "er", "heut", "dem", "Un\u00b7ter\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "stellt fest, wof\u00fcr wir noch zu klein sind \u2013", "tokens": ["stellt", "fest", ",", "wo\u00b7f\u00fcr", "wir", "noch", "zu", "klein", "sind", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "PWAV", "PPER", "ADV", "PTKA", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und denkt, ganz Deutschland liege an der Lahn.", "tokens": ["und", "denkt", ",", "ganz", "Deutschland", "lie\u00b7ge", "an", "der", "Lahn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ADV", "NE", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}}, "stanza.6": {"line.1": {"text": "Wer seine Nase nur in Schweinerein steckt,", "tokens": ["Wer", "sei\u00b7ne", "Na\u00b7se", "nur", "in", "Schwei\u00b7ne\u00b7rein", "steckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "verliert das Gleichgewicht:", "tokens": ["ver\u00b7liert", "das", "Gleich\u00b7ge\u00b7wicht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wenn auf den Bl\u00e4ttern auch die Frau das Bein streckt:", "tokens": ["Wenn", "auf", "den", "Bl\u00e4t\u00b7tern", "auch", "die", "Frau", "das", "Bein", "streckt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "uns st\u00f6rt das weiter nicht.", "tokens": ["uns", "st\u00f6rt", "das", "wei\u00b7ter", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADV", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Was schert uns frohe und gesunde Esser", "tokens": ["Was", "schert", "uns", "fro\u00b7he", "und", "ge\u00b7sun\u00b7de", "Es\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "ADJA", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "denn Seine Impotenz, der Herr Professor!", "tokens": ["denn", "Sei\u00b7ne", "Im\u00b7po\u00b7tenz", ",", "der", "Herr", "Pro\u00b7fes\u00b7sor", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+---", "measure": "unknown.measure.tetra"}, "line.7": {"text": "Da soll doch gleich . . . !", "tokens": ["Da", "soll", "doch", "gleich", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "VMFIN", "ADV", "ADV", "$.", "$.", "$.", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "Die Putten fliegen.", "tokens": ["Die", "Put\u00b7ten", "flie\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Hell seh ich Aphroditen liegen.", "tokens": ["Hell", "seh", "ich", "A\u00b7phro\u00b7di\u00b7ten", "lie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.10": {"text": "Sie l\u00e4chelt: \u00bbTiger, la\u00df ihn gehn!", "tokens": ["Sie", "l\u00e4\u00b7chelt", ":", "\u00bb", "Ti\u00b7ger", ",", "la\u00df", "ihn", "gehn", "!"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "$,", "VVIMP", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Er kennt mich nicht. Er hat mich nie gesehn!\u00ab", "tokens": ["Er", "kennt", "mich", "nicht", ".", "Er", "hat", "mich", "nie", "ge\u00b7sehn", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$.", "PPER", "VAFIN", "PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}