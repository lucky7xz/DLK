{"dta.poem.9267": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Das andere Gespr\u00e4ch.", "genre": "Lyrik; Drama; Prosa", "period": "N.A.", "pub_year": "1701", "urn": "urn:nbn:de:kobv:b4-25043-0", "language": ["de:0.99"], "booktitle": "Weise, Christian: \u00dcberfl\u00fc\u00dfige Gedancken Der gr\u00fcnenden jugend. Leipzig, 1701."}, "poem": {"stanza.1": {"line.1": {"text": "Die leber ist vom hecht und nicht von einem lamm", "tokens": ["Die", "le\u00b7ber", "ist", "vom", "hecht", "und", "nicht", "von", "ei\u00b7nem", "lamm"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "VAFIN", "APPRART", "NN", "KON", "PTKNEG", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ach h\u00e4tt ich armes kind doch einen br\u00e4utigam.", "tokens": ["Ach", "h\u00e4tt", "ich", "ar\u00b7mes", "kind", "doch", "ei\u00b7nen", "br\u00e4u\u00b7ti\u00b7gam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "VAFIN", "PPER", "ADJA", "NN", "ADV", "ART", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "2. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer fohren/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "foh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "PIS", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Dem k\u00fcssen bin ich feind/ doch hab ichs nicht verschwore\u0303.", "tokens": ["Dem", "k\u00fcs\u00b7sen", "bin", "ich", "feind", "/", "doch", "hab", "ichs", "nicht", "ver\u00b7schwor\u1ebd", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVINF", "VAFIN", "PPER", "NN", "$(", "ADV", "VAFIN", "PIS", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "3. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer endte/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "end\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Wenn ichs ja leiden sol/ so thu mirs ein studente.", "tokens": ["Wenn", "ichs", "ja", "lei\u00b7den", "sol", "/", "so", "thu", "mirs", "ein", "stu\u00b7den\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "VVINF", "VMFIN", "$(", "ADV", "VVFIN", "NE", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "4. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einem biber/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7nem", "bi\u00b7ber", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Ein wittwer w\u00e4r mir lieb/ ein junggeselle lieber.", "tokens": ["Ein", "witt\u00b7wer", "w\u00e4r", "mir", "lieb", "/", "ein", "jung\u00b7ge\u00b7sel\u00b7le", "lie\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VAFIN", "PPER", "ADJD", "$(", "ART", "ADJA", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "5. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer wachtel/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "wach\u00b7tel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Ich finde keinen trost an einer leeren schachtel.", "tokens": ["Ich", "fin\u00b7de", "kei\u00b7nen", "trost", "an", "ei\u00b7ner", "lee\u00b7ren", "schach\u00b7tel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "6. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einem b\u00e4ren/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7nem", "b\u00e4\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "ADJA", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Wir h\u00e4ttens alle gern und sollens nicht begehren.", "tokens": ["Wir", "h\u00e4t\u00b7tens", "al\u00b7le", "gern", "und", "sol\u00b7lens", "nicht", "be\u00b7geh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADV", "KON", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "7. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer grille/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "gril\u00b7le", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "ADJA", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Bey manche\u0303 schrey ich laut/ bey manche\u0303 schweig ich stille.", "tokens": ["Bey", "man\u00b7ch\u1ebd", "schrey", "ich", "laut", "/", "bey", "man\u00b7ch\u1ebd", "schweig", "ich", "stil\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "PPER", "ADJD", "$(", "APPR", "PIS", "VVFIN", "PPER", "VVFIN", "$."], "meter": "+-+--++--+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.8": {"line.1": {"text": "8. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einem raben/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7nem", "ra\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Wie k\u00f6stlich m\u00fcssens doch die lieben b\u00fcfgen haben.", "tokens": ["Wie", "k\u00f6st\u00b7lich", "m\u00fcs\u00b7sens", "doch", "die", "lie\u00b7ben", "b\u00fcf\u00b7gen", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VMFIN", "ADV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "9. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einem aal/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7nem", "aal", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Das ist die beste lust zwey liebgen auff einmahl.", "tokens": ["Das", "ist", "die", "bes\u00b7te", "lust", "zwey", "lieb\u00b7gen", "auff", "ein\u00b7mahl", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "CARD", "VVFIN", "APPR", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "10. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer sau/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "sau", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "ADJD", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Ach h\u00e4tt ich armer dieb bald eine jungefrau.", "tokens": ["Ach", "h\u00e4tt", "ich", "ar\u00b7mer", "dieb", "bald", "ei\u00b7ne", "jun\u00b7ge\u00b7frau", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "VAFIN", "PPER", "ADJA", "NN", "ADV", "ART", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "11. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einem schwein/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7nem", "schwein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "ADJD", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Die jungfern wollen nur fein hoch gebeten seyn.", "tokens": ["Die", "jung\u00b7fern", "wol\u00b7len", "nur", "fein", "hoch", "ge\u00b7be\u00b7ten", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VMFIN", "ADV", "ADJD", "ADJD", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "12. \u2012 \u2012 \u2012 \u2012 \u2012 und nicht von einer mau\u00df/", "tokens": ["\u2012", "\u2012", "\u2012", "\u2012", "\u2012", "und", "nicht", "von", "ei\u00b7ner", "mau\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "$(", "$(", "KON", "PTKNEG", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Die mich verachten will die lach ich wider aus.", "tokens": ["Die", "mich", "ver\u00b7ach\u00b7ten", "will", "die", "lach", "ich", "wi\u00b7der", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "VMFIN", "ART", "VVFIN", "PPER", "APPR", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}