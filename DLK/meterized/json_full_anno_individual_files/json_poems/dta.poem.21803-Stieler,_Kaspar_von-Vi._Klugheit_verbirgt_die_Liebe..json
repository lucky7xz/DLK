{"dta.poem.21803": {"metadata": {"author": {"name": "Stieler, Kaspar von", "birth": "N.A.", "death": "N.A."}, "title": "Vi.  \n  Klugheit verbirgt die Liebe.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1660", "urn": "urn:nbn:de:kobv:b4-20586-2", "language": ["de:0.99"], "booktitle": "Filidor der Dorfferer [i. e. Stieler, Kaspar von]: Die Geharnschte Venus. Hamburg, 1660."}, "poem": {"stanza.1": {"line.1": {"text": "Es ist genug der H\u00e4nde dr\u00fckken/", "tokens": ["Es", "ist", "ge\u00b7nug", "der", "H\u00e4n\u00b7de", "dr\u00fck\u00b7ken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der F\u00fcsse Tritt/ der Augen nikken/", "tokens": ["der", "F\u00fcs\u00b7se", "Tritt", "/", "der", "Au\u00b7gen", "nik\u00b7ken", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wenn/ B\u00fcschgen/ wir bey Leuten sind.", "tokens": ["wenn", "/", "B\u00fcschgen", "/", "wir", "bey", "Leu\u00b7ten", "sind", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "NN", "$(", "PPER", "APPR", "NN", "VAFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "H\u00f6r auff mit weitern Liebes Werken/", "tokens": ["H\u00f6r", "auff", "mit", "wei\u00b7tern", "Lie\u00b7bes", "Wer\u00b7ken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "APPR", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "man wil es fast zu scheinbar merken", "tokens": ["man", "wil", "es", "fast", "zu", "schein\u00b7bar", "mer\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PPER", "ADV", "PTKA", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "da\u00df wir uns lieben/ gutes Kind.", "tokens": ["da\u00df", "wir", "uns", "lie\u00b7ben", "/", "gu\u00b7tes", "Kind", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVINF", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sind wir denn in geheim bey sammen/", "tokens": ["Sind", "wir", "denn", "in", "ge\u00b7heim", "bey", "sam\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "ADJD", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "bin ich doch/ N\u00e4rrchen/ allzeit dein/", "tokens": ["bin", "ich", "doch", "/", "N\u00e4rr\u00b7chen", "/", "all\u00b7zeit", "dein", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "$(", "NN", "$(", "ADV", "PPOSAT", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn k\u00f6nnen wir uns satt zu k\u00fcssen/", "tokens": ["Denn", "k\u00f6n\u00b7nen", "wir", "uns", "satt", "zu", "k\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PRF", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und was wir ie zuweilen missen/", "tokens": ["und", "was", "wir", "ie", "zu\u00b7wei\u00b7len", "mis\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "mit Wucher wieder bringen ein.", "tokens": ["mit", "Wu\u00b7cher", "wie\u00b7der", "brin\u00b7gen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVINF", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Mein Buschgen/ kennstu nicht die Leute?", "tokens": ["Mein", "Buschgen", "/", "kenns\u00b7tu", "nicht", "die", "Leu\u00b7te", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "VVFIN", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "der dir ganz freundlich steht zur Seite/", "tokens": ["der", "dir", "ganz", "freund\u00b7lich", "steht", "zur", "Sei\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADJD", "VVFIN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "gibt achtung auff dich/ als ein Feind.", "tokens": ["gibt", "ach\u00b7tung", "auff", "dich", "/", "als", "ein", "Feind", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "PPER", "$(", "KOUS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die sich am n\u00e4chsten um dich stellen/", "tokens": ["Die", "sich", "am", "n\u00e4chs\u00b7ten", "um", "dich", "stel\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPRART", "ADJA", "APPR", "PRF", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "verr\u00e4ht- und m\u00f6rderlich gemeint.", "tokens": ["ver\u00b7r\u00e4ht", "und", "m\u00f6r\u00b7der\u00b7lich", "ge\u00b7meint", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Man kan sich nicht genugsam h\u00fcten/", "tokens": ["Man", "kan", "sich", "nicht", "ge\u00b7nug\u00b7sam", "h\u00fc\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PRF", "PTKNEG", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "auff Lieb\u2019 und Freundschafft f\u00fcr der T\u00fchr.", "tokens": ["auff", "Lieb'", "und", "Freund\u00b7schafft", "f\u00fcr", "der", "T\u00fchr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man mu\u00df iezt gar gelinde gehen/", "tokens": ["Man", "mu\u00df", "iezt", "gar", "ge\u00b7lin\u00b7de", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "ADV", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "es wei\u00df ein Luchs-aug\u2019 auch zu sehen", "tokens": ["es", "wei\u00df", "ein", "Luchs\u00b7aug'", "auch", "zu", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ich werd\u2019 es nicht f\u00fcr \u00fcbel deuten/", "tokens": ["Ich", "werd'", "es", "nicht", "f\u00fcr", "\u00fc\u00b7bel", "deu\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "APPR", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "ob du mich gleich ie f\u00fcr den Leuten", "tokens": ["ob", "du", "mich", "gleich", "ie", "f\u00fcr", "den", "Leu\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "verha\u00dft/ und heist mich von dir gehn.", "tokens": ["ver\u00b7ha\u00dft", "/", "und", "heist", "mich", "von", "dir", "gehn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$(", "KON", "VVFIN", "PRF", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Sp\u00f6ttchen kan ich leicht verschmerzen/", "tokens": ["Ein", "Sp\u00f6tt\u00b7chen", "kan", "ich", "leicht", "ver\u00b7schmer\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "l\u00e4stu mich nur in deinem Herzen", "tokens": ["l\u00e4s\u00b7tu", "mich", "nur", "in", "dei\u00b7nem", "Her\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "f\u00fcr deinen Freund und Schaz bestehn.", "tokens": ["f\u00fcr", "dei\u00b7nen", "Freund", "und", "Schaz", "be\u00b7stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Drum sey genug dir H\u00e4nde dr\u00fckken/", "tokens": ["Drum", "sey", "ge\u00b7nug", "dir", "H\u00e4n\u00b7de", "dr\u00fck\u00b7ken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ADV", "PPER", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der F\u00fcsse Tritt/ der Augen nikken/", "tokens": ["der", "F\u00fcs\u00b7se", "Tritt", "/", "der", "Au\u00b7gen", "nik\u00b7ken", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wenn/ B\u00fcschgen/ wir bey Leuten sind.", "tokens": ["wenn", "/", "B\u00fcschgen", "/", "wir", "bey", "Leu\u00b7ten", "sind", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "NN", "$(", "PPER", "APPR", "NN", "VAFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wer wei\u00df ob nicht au\u00df diesen Werken", "tokens": ["Wer", "wei\u00df", "ob", "nicht", "au\u00df", "die\u00b7sen", "Wer\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "KOUS", "PTKNEG", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "die schlauen Neider ab-was merken", "tokens": ["die", "schlau\u00b7en", "Nei\u00b7der", "ab\u00b7was", "mer\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NE", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "da\u00df wir uns lieben/ gutes Kind.", "tokens": ["da\u00df", "wir", "uns", "lie\u00b7ben", "/", "gu\u00b7tes", "Kind", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVINF", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}