{"dta.poem.2068": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Das Pantherthier.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1748", "urn": "urn:nbn:de:kobv:b4-200905198553", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Nunmehr k\u00f6mmt, in unsrer Ordnung, das ergrimmte", "tokens": ["Nun\u00b7mehr", "k\u00f6mmt", ",", "in", "uns\u00b7rer", "Ord\u00b7nung", ",", "das", "er\u00b7grimm\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "APPR", "PPOSAT", "NN", "$,", "ART", "ADJA"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Pantherthier,", "tokens": ["Pan\u00b7ther\u00b7thier", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Dem zu Ehren zu betrachten, welcher es gemacht, uns f\u00fcr,", "tokens": ["Dem", "zu", "Eh\u00b7ren", "zu", "be\u00b7trach\u00b7ten", ",", "wel\u00b7cher", "es", "ge\u00b7macht", ",", "uns", "f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "PTKZU", "VVINF", "$,", "PRELS", "PPER", "VVPP", "$,", "PPER", "APPR", "$,"], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.4": {"text": "Das in seiner Art nicht minder wohlgebildet ist und", "tokens": ["Das", "in", "sei\u00b7ner", "Art", "nicht", "min\u00b7der", "wohl\u00b7ge\u00b7bil\u00b7det", "ist", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "PPOSAT", "NN", "PTKNEG", "ADV", "ADJD", "VAFIN", "KON"], "meter": "+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.5": {"text": "sch\u00f6n,", "tokens": ["sch\u00f6n", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Da wir auf der ganzen Haut nichts als sch\u00f6ne schwarze", "tokens": ["Da", "wir", "auf", "der", "gan\u00b7zen", "Haut", "nichts", "als", "sch\u00f6\u00b7ne", "schwar\u00b7ze"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ART", "ADJA", "NN", "PIS", "KOKOM", "ADJA", "ADJA"], "meter": "--+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Flecken,", "tokens": ["Fle\u00b7cken", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.8": {"text": "Mit besonder scharfem Umstrich und sehr nett geformt,", "tokens": ["Mit", "be\u00b7son\u00b7der", "schar\u00b7fem", "Um\u00b7strich", "und", "sehr", "nett", "ge\u00b7formt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADJA", "NN", "KON", "ADV", "ADJD", "VVPP", "$,"], "meter": "+-+-+--+-+--+", "measure": "trochaic.hexa.relaxed"}, "line.9": {"text": "entdecken,", "tokens": ["ent\u00b7de\u00b7cken", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Die auf r\u00f6thlichgelbem Grunde in der sch\u00f6nsten Ordnung", "tokens": ["Die", "auf", "r\u00f6th\u00b7lich\u00b7gel\u00b7bem", "Grun\u00b7de", "in", "der", "sch\u00f6ns\u00b7ten", "Ord\u00b7nung"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-++-+-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.11": {"text": "stehn.", "tokens": ["stehn", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+", "measure": "single.up"}, "line.12": {"text": "Ob nun gleich sein Grimm, die St\u00e4rke, die Geschwindig-", "tokens": ["Ob", "nun", "gleich", "sein", "Grimm", ",", "die", "St\u00e4r\u00b7ke", ",", "die", "Ge\u00b7schwin\u00b7dig"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "PPOSAT", "NE", "$,", "ART", "NN", "$,", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.13": {"text": "keit, die Wut", "tokens": ["keit", ",", "die", "Wut"], "token_info": ["word", "punct", "word", "word"], "pos": ["ADV", "$,", "ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.14": {"text": "Oftermals den Menschen t\u00f6dtlich, und nicht selten Schaden", "tokens": ["Of\u00b7ter\u00b7mals", "den", "Men\u00b7schen", "t\u00f6dt\u00b7lich", ",", "und", "nicht", "sel\u00b7ten", "Scha\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "ADJD", "$,", "KON", "PTKNEG", "ADJA", "NN"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.15": {"text": "thut;", "tokens": ["thut", ";"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-", "measure": "single.down"}, "line.16": {"text": "Ist doch auch in diesem Thier\u2019, wie in allen andern Werken,", "tokens": ["Ist", "doch", "auch", "in", "die\u00b7sem", "Thier'", ",", "wie", "in", "al\u00b7len", "an\u00b7dern", "Wer\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "APPR", "PDAT", "NN", "$,", "PWAV", "APPR", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.17": {"text": "Eines m\u00e4cht\u2019gen Sch\u00f6pfers Ordnung bey dem Aufenthalt", "tokens": ["Ei\u00b7nes", "m\u00e4cht'\u00b7gen", "Sch\u00f6p\u00b7fers", "Ord\u00b7nung", "bey", "dem", "Auf\u00b7ent\u00b7halt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.18": {"text": "zu merken,", "tokens": ["zu", "mer\u00b7ken", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,"], "meter": "+--", "measure": "dactylic.init"}, "line.19": {"text": "In der ihnen angewies\u2019 nen Wohnung, da sie in den W\u00fcsten,", "tokens": ["In", "der", "ih\u00b7nen", "an\u00b7ge\u00b7wies'", "nen", "Woh\u00b7nung", ",", "da", "sie", "in", "den", "W\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PPER", "ADJD", "ADJA", "NN", "$,", "KOUS", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+-+-+-", "measure": "trochaic.octa.plus"}, "line.20": {"text": "Von der Menschheit abgesondert, und entfernet, einsam", "tokens": ["Von", "der", "Menschheit", "ab\u00b7ge\u00b7son\u00b7dert", ",", "und", "ent\u00b7fer\u00b7net", ",", "ein\u00b7sam"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["APPR", "ART", "NN", "VVPP", "$,", "KON", "VVFIN", "$,", "ADJD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "nisten,", "tokens": ["nis\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.22": {"text": "Und nur an sehr wenig Oertern. Jhrer bunten B\u00e4lge", "tokens": ["Und", "nur", "an", "sehr", "we\u00b7nig", "O\u00b7er\u00b7tern", ".", "Ih\u00b7rer", "bun\u00b7ten", "B\u00e4l\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADV", "PIAT", "NN", "$.", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.23": {"text": "Pracht", "tokens": ["Pracht"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.24": {"text": "Wird aus so entfernten L\u00e4ndern auch sogar zu uns ge-", "tokens": ["Wird", "aus", "so", "ent\u00b7fern\u00b7ten", "L\u00e4n\u00b7dern", "auch", "so\u00b7gar", "zu", "uns", "ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ADV", "ADJA", "NN", "ADV", "ADV", "APPR", "PPER", "TRUNC"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "bracht,", "tokens": ["bracht", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.26": {"text": "Und sehr gern von uns gen\u00fctzet: da\u00df wir also Vortheil", "tokens": ["Und", "sehr", "gern", "von", "uns", "ge\u00b7n\u00fct\u00b7zet", ":", "da\u00df", "wir", "al\u00b7so", "Vor\u00b7theil"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "APPR", "PPER", "VVPP", "$.", "KOUS", "PPER", "ADV", "NN"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.27": {"text": "sp\u00fcren,", "tokens": ["sp\u00fc\u00b7ren", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.28": {"text": "So im Handel als Gebrauch, und uns auch bereichert sehn", "tokens": ["So", "im", "Han\u00b7del", "als", "Ge\u00b7brauch", ",", "und", "uns", "auch", "be\u00b7rei\u00b7chert", "sehn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPRART", "NN", "KOUS", "NN", "$,", "KON", "PPER", "ADV", "VVPP", "VVINF"], "meter": "--+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.29": {"text": "Durch die\u00df sch\u00f6n\u2019 und wilde Thier, sonder in Gefahr", "tokens": ["Durch", "die\u00df", "sch\u00f6n'", "und", "wil\u00b7de", "Thier", ",", "son\u00b7der", "in", "Ge\u00b7fahr"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PDS", "VVFIN", "KON", "ADJA", "NN", "$,", "KON", "APPR", "NN"], "meter": "+-+-+-++-+-+", "measure": "unknown.measure.septa"}, "line.30": {"text": "zu stehn,", "tokens": ["zu", "stehn", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.31": {"text": "Leib und Leben zu verlieren.", "tokens": ["Leib", "und", "Le\u00b7ben", "zu", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}