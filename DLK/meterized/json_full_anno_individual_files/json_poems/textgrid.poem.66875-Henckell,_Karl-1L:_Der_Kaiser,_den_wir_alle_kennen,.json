{"textgrid.poem.66875": {"metadata": {"author": {"name": "Henckell, Karl", "birth": "N.A.", "death": "N.A."}, "title": "1L: Der Kaiser, den wir alle kennen,", "genre": "verse", "period": "N.A.", "pub_year": 1896, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Kaiser, den wir alle kennen,", "tokens": ["Der", "Kai\u00b7ser", ",", "den", "wir", "al\u00b7le", "ken\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sprach j\u00fcngst mit einem schlichten Mann.", "tokens": ["Sprach", "j\u00fcngst", "mit", "ei\u00b7nem", "schlich\u00b7ten", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich sah sie stehn im Sonnenbrennen,", "tokens": ["Ich", "sah", "sie", "stehn", "im", "Son\u00b7nen\u00b7bren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und ihre Worte h\u00f6rt' ich an.", "tokens": ["Und", "ih\u00b7re", "Wor\u00b7te", "h\u00f6rt'", "ich", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Man wird es ein Gesicht wohl nennen,", "tokens": ["Man", "wird", "es", "ein", "Ge\u00b7sicht", "wohl", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Weil man es sonst nicht glauben kann.", "tokens": ["Weil", "man", "es", "sonst", "nicht", "glau\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der Kaiser stieg vom stolzen Rosse", "tokens": ["Der", "Kai\u00b7ser", "stieg", "vom", "stol\u00b7zen", "Ros\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und trat zum Manne, der geb\u00fcckt", "tokens": ["Und", "trat", "zum", "Man\u00b7ne", ",", "der", "ge\u00b7b\u00fcckt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$,", "PRELS", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An einem Riesenschiffsgeschosse", "tokens": ["An", "ei\u00b7nem", "Rie\u00b7sen\u00b7schiffs\u00b7ge\u00b7schos\u00b7se"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die letzten Schrauben festgedr\u00fcckt.", "tokens": ["Die", "letz\u00b7ten", "Schrau\u00b7ben", "fest\u00b7ge\u00b7dr\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Grad in den Schlund dem Mordskolosse", "tokens": ["Grad", "in", "den", "Schlund", "dem", "Mords\u00b7ko\u00b7los\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN", "ART", "NN"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Sah Majest\u00e4t: \u00bbEs ist gegl\u00fcckt\u00ab,", "tokens": ["Sah", "Ma\u00b7jes\u00b7t\u00e4t", ":", "\u00bb", "Es", "ist", "ge\u00b7gl\u00fcckt", "\u00ab", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "NN", "$.", "$(", "PPER", "VAFIN", "VVPP", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Sprach er: \u00bbMein Segen solchem Werke,", "tokens": ["Sprach", "er", ":", "\u00bb", "Mein", "Se\u00b7gen", "sol\u00b7chem", "Wer\u00b7ke", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$.", "$(", "PPOSAT", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das du f\u00fcr Mich vollendet hast!", "tokens": ["Das", "du", "f\u00fcr", "Mich", "voll\u00b7en\u00b7det", "hast", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein kaiserliches Lob! Und merke", "tokens": ["Mein", "kai\u00b7ser\u00b7li\u00b7ches", "Lob", "!", "Und", "mer\u00b7ke"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$.", "KON", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wohl auf mein Wort, das trefflich pa\u00dft:", "tokens": ["Wohl", "auf", "mein", "Wort", ",", "das", "treff\u00b7lich", "pa\u00dft", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "$,", "PRELS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "\u203amit Gott und Krupp!\u2039 hei\u00dft unsre St\u00e4rke,", "tokens": ["\u203a", "mit", "Gott", "und", "Krupp", "!", "\u2039", "hei\u00dft", "uns\u00b7re", "St\u00e4r\u00b7ke", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "KON", "NN", "$.", "$(", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dies Wort sei drum ins Rohr gefa\u00dft!\u00ab", "tokens": ["Dies", "Wort", "sei", "drum", "ins", "Rohr", "ge\u00b7fa\u00dft", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "NN", "VAFIN", "PAV", "APPRART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Da hob der Mann sein Haupt zum Lichte", "tokens": ["Da", "hob", "der", "Mann", "sein", "Haupt", "zum", "Lich\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "PPOSAT", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wischt vom Angesicht den Schwei\u00df", "tokens": ["Und", "wischt", "vom", "An\u00b7ge\u00b7sicht", "den", "Schwei\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und schwieg in zagendem Verzichte.", "tokens": ["Und", "schwieg", "in", "za\u00b7gen\u00b7dem", "Ver\u00b7zich\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Pl\u00f6tzlich durchschie\u00dft's ihn kalt und hei\u00df,", "tokens": ["Pl\u00f6tz\u00b7lich", "durch\u00b7schie\u00dft's", "ihn", "kalt", "und", "hei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Doch milit\u00e4risch in der Richte", "tokens": ["Doch", "mi\u00b7li\u00b7t\u00e4\u00b7risch", "in", "der", "Rich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Steht gleich er stramm und spricht: \u00bbGott wei\u00df,", "tokens": ["Steht", "gleich", "er", "stramm", "und", "spricht", ":", "\u00bb", "Gott", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPER", "VVFIN", "KON", "VVFIN", "$.", "$(", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Was wahr ist. Majest\u00e4t verzeihen!", "tokens": ["Was", "wahr", "ist", ".", "Ma\u00b7jes\u00b7t\u00e4t", "ver\u00b7zei\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$.", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich schaffe f\u00fcr mein t\u00e4glich Brot.", "tokens": ["Ich", "schaf\u00b7fe", "f\u00fcr", "mein", "t\u00e4g\u00b7lich", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "ADJD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich w\u00fcnsche Deutschland gut Gedeihen", "tokens": ["Ich", "w\u00fcn\u00b7sche", "Deutschland", "gut", "Ge\u00b7dei\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "ADJD", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und starken Schutz f\u00fcr Krieg und Not.", "tokens": ["Und", "star\u00b7ken", "Schutz", "f\u00fcr", "Krieg", "und", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Doch ... darf der Mensch um Rache schreien,", "tokens": ["Doch", "...", "darf", "der", "Mensch", "um", "Ra\u00b7che", "schrei\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VMFIN", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wenn \u203aLiebe\u2039 lautet sein Gebot?", "tokens": ["Wenn", "\u203a", "Lie\u00b7be", "\u2039", "lau\u00b7tet", "sein", "Ge\u00b7bot", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "NN", "$(", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Pardon drum, Majest\u00e4t! Ich halte", "tokens": ["Par\u00b7don", "drum", ",", "Ma\u00b7jes\u00b7t\u00e4t", "!", "Ich", "hal\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["NN", "PAV", "$,", "NN", "$.", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fest an dem Neuen Testament.", "tokens": ["Fest", "an", "dem", "Neu\u00b7en", "Tes\u00b7ta\u00b7ment", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Gott schaut in jede Herzensfalte,", "tokens": ["Gott", "schaut", "in", "je\u00b7de", "Her\u00b7zens\u00b7fal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich will nicht, da\u00df er falsch mich nennt.", "tokens": ["Ich", "will", "nicht", ",", "da\u00df", "er", "falsch", "mich", "nennt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "$,", "KOUS", "PPER", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So wahr am Rohr nicht Ri\u00df noch Spalte,", "tokens": ["So", "wahr", "am", "Rohr", "nicht", "Ri\u00df", "noch", "Spal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPRART", "NN", "PTKNEG", "NN", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Kann Christ sein, wer \u2013 Pardon nicht kennt?\u00ab", "tokens": ["Kann", "Christ", "sein", ",", "wer", "\u2013", "Par\u00b7don", "nicht", "kennt", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "VAINF", "$,", "PWS", "$(", "NN", "PTKNEG", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Und schweigt. Das Sonnenlicht spielt heiter", "tokens": ["Und", "schweigt", ".", "Das", "Son\u00b7nen\u00b7licht", "spielt", "hei\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "$.", "ART", "NN", "VVFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Adlerhelm und Wappenschild.", "tokens": ["Auf", "Ad\u00b7ler\u00b7helm", "und", "Wap\u00b7pen\u00b7schild", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Kaiser gr\u00fc\u00dft den Werftarbeiter", "tokens": ["Der", "Kai\u00b7ser", "gr\u00fc\u00dft", "den", "Werf\u00b7tar\u00b7bei\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wortlos und reitet rasch, ein Bild", "tokens": ["Wort\u00b7los", "und", "rei\u00b7tet", "rasch", ",", "ein", "Bild"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "ADJD", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Blitzzuckender Tatbegierde, weiter", "tokens": ["Blitz\u00b7zu\u00b7cken\u00b7der", "Tat\u00b7be\u00b7gier\u00b7de", ",", "wei\u00b7ter"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADJA", "NN", "$,", "ADV"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Durchs glutenschwangere Gefild.", "tokens": ["Durchs", "glu\u00b7ten\u00b7schwan\u00b7ge\u00b7re", "Ge\u00b7fild", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Der Kaiser, den wir alle kennen,", "tokens": ["Der", "Kai\u00b7ser", ",", "den", "wir", "al\u00b7le", "ken\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sprach j\u00fcngst mit einem schlichten Mann.", "tokens": ["Sprach", "j\u00fcngst", "mit", "ei\u00b7nem", "schlich\u00b7ten", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich sah sie stehn im Sonnenbrennen,", "tokens": ["Ich", "sah", "sie", "stehn", "im", "Son\u00b7nen\u00b7bren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und ihre Worte h\u00f6rt' ich an.", "tokens": ["Und", "ih\u00b7re", "Wor\u00b7te", "h\u00f6rt'", "ich", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Man wird es ein Gesicht wohl nennen,", "tokens": ["Man", "wird", "es", "ein", "Ge\u00b7sicht", "wohl", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Weil man es sonst nicht glauben kann.", "tokens": ["Weil", "man", "es", "sonst", "nicht", "glau\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Der Kaiser stieg vom stolzen Rosse", "tokens": ["Der", "Kai\u00b7ser", "stieg", "vom", "stol\u00b7zen", "Ros\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und trat zum Manne, der geb\u00fcckt", "tokens": ["Und", "trat", "zum", "Man\u00b7ne", ",", "der", "ge\u00b7b\u00fcckt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$,", "PRELS", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An einem Riesenschiffsgeschosse", "tokens": ["An", "ei\u00b7nem", "Rie\u00b7sen\u00b7schiffs\u00b7ge\u00b7schos\u00b7se"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die letzten Schrauben festgedr\u00fcckt.", "tokens": ["Die", "letz\u00b7ten", "Schrau\u00b7ben", "fest\u00b7ge\u00b7dr\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Grad in den Schlund dem Mordskolosse", "tokens": ["Grad", "in", "den", "Schlund", "dem", "Mords\u00b7ko\u00b7los\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN", "ART", "NN"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Sah Majest\u00e4t: \u00bbEs ist gegl\u00fcckt\u00ab,", "tokens": ["Sah", "Ma\u00b7jes\u00b7t\u00e4t", ":", "\u00bb", "Es", "ist", "ge\u00b7gl\u00fcckt", "\u00ab", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "NN", "$.", "$(", "PPER", "VAFIN", "VVPP", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Sprach er: \u00bbMein Segen solchem Werke,", "tokens": ["Sprach", "er", ":", "\u00bb", "Mein", "Se\u00b7gen", "sol\u00b7chem", "Wer\u00b7ke", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$.", "$(", "PPOSAT", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das du f\u00fcr Mich vollendet hast!", "tokens": ["Das", "du", "f\u00fcr", "Mich", "voll\u00b7en\u00b7det", "hast", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mein kaiserliches Lob! Und merke", "tokens": ["Mein", "kai\u00b7ser\u00b7li\u00b7ches", "Lob", "!", "Und", "mer\u00b7ke"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$.", "KON", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wohl auf mein Wort, das trefflich pa\u00dft:", "tokens": ["Wohl", "auf", "mein", "Wort", ",", "das", "treff\u00b7lich", "pa\u00dft", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "$,", "PRELS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "\u203amit Gott und Krupp!\u2039 hei\u00dft unsre St\u00e4rke,", "tokens": ["\u203a", "mit", "Gott", "und", "Krupp", "!", "\u2039", "hei\u00dft", "uns\u00b7re", "St\u00e4r\u00b7ke", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "KON", "NN", "$.", "$(", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dies Wort sei drum ins Rohr gefa\u00dft!\u00ab", "tokens": ["Dies", "Wort", "sei", "drum", "ins", "Rohr", "ge\u00b7fa\u00dft", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "NN", "VAFIN", "PAV", "APPRART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Da hob der Mann sein Haupt zum Lichte", "tokens": ["Da", "hob", "der", "Mann", "sein", "Haupt", "zum", "Lich\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "PPOSAT", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wischt vom Angesicht den Schwei\u00df", "tokens": ["Und", "wischt", "vom", "An\u00b7ge\u00b7sicht", "den", "Schwei\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und schwieg in zagendem Verzichte.", "tokens": ["Und", "schwieg", "in", "za\u00b7gen\u00b7dem", "Ver\u00b7zich\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Pl\u00f6tzlich durchschie\u00dft's ihn kalt und hei\u00df,", "tokens": ["Pl\u00f6tz\u00b7lich", "durch\u00b7schie\u00dft's", "ihn", "kalt", "und", "hei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Doch milit\u00e4risch in der Richte", "tokens": ["Doch", "mi\u00b7li\u00b7t\u00e4\u00b7risch", "in", "der", "Rich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Steht gleich er stramm und spricht: \u00bbGott wei\u00df,", "tokens": ["Steht", "gleich", "er", "stramm", "und", "spricht", ":", "\u00bb", "Gott", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPER", "VVFIN", "KON", "VVFIN", "$.", "$(", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Was wahr ist. Majest\u00e4t verzeihen!", "tokens": ["Was", "wahr", "ist", ".", "Ma\u00b7jes\u00b7t\u00e4t", "ver\u00b7zei\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$.", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich schaffe f\u00fcr mein t\u00e4glich Brot.", "tokens": ["Ich", "schaf\u00b7fe", "f\u00fcr", "mein", "t\u00e4g\u00b7lich", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "ADJD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich w\u00fcnsche Deutschland gut Gedeihen", "tokens": ["Ich", "w\u00fcn\u00b7sche", "Deutschland", "gut", "Ge\u00b7dei\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "ADJD", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und starken Schutz f\u00fcr Krieg und Not.", "tokens": ["Und", "star\u00b7ken", "Schutz", "f\u00fcr", "Krieg", "und", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Doch ... darf der Mensch um Rache schreien,", "tokens": ["Doch", "...", "darf", "der", "Mensch", "um", "Ra\u00b7che", "schrei\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VMFIN", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wenn \u203aLiebe\u2039 lautet sein Gebot?", "tokens": ["Wenn", "\u203a", "Lie\u00b7be", "\u2039", "lau\u00b7tet", "sein", "Ge\u00b7bot", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "NN", "$(", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Pardon drum, Majest\u00e4t! Ich halte", "tokens": ["Par\u00b7don", "drum", ",", "Ma\u00b7jes\u00b7t\u00e4t", "!", "Ich", "hal\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["NN", "PAV", "$,", "NN", "$.", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fest an dem Neuen Testament.", "tokens": ["Fest", "an", "dem", "Neu\u00b7en", "Tes\u00b7ta\u00b7ment", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Gott schaut in jede Herzensfalte,", "tokens": ["Gott", "schaut", "in", "je\u00b7de", "Her\u00b7zens\u00b7fal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich will nicht, da\u00df er falsch mich nennt.", "tokens": ["Ich", "will", "nicht", ",", "da\u00df", "er", "falsch", "mich", "nennt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "$,", "KOUS", "PPER", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So wahr am Rohr nicht Ri\u00df noch Spalte,", "tokens": ["So", "wahr", "am", "Rohr", "nicht", "Ri\u00df", "noch", "Spal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPRART", "NN", "PTKNEG", "NN", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Kann Christ sein, wer \u2013 Pardon nicht kennt?\u00ab", "tokens": ["Kann", "Christ", "sein", ",", "wer", "\u2013", "Par\u00b7don", "nicht", "kennt", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "VAINF", "$,", "PWS", "$(", "NN", "PTKNEG", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Und schweigt. Das Sonnenlicht spielt heiter", "tokens": ["Und", "schweigt", ".", "Das", "Son\u00b7nen\u00b7licht", "spielt", "hei\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "$.", "ART", "NN", "VVFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Adlerhelm und Wappenschild.", "tokens": ["Auf", "Ad\u00b7ler\u00b7helm", "und", "Wap\u00b7pen\u00b7schild", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Kaiser gr\u00fc\u00dft den Werftarbeiter", "tokens": ["Der", "Kai\u00b7ser", "gr\u00fc\u00dft", "den", "Werf\u00b7tar\u00b7bei\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wortlos und reitet rasch, ein Bild", "tokens": ["Wort\u00b7los", "und", "rei\u00b7tet", "rasch", ",", "ein", "Bild"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "ADJD", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Blitzzuckender Tatbegierde, weiter", "tokens": ["Blitz\u00b7zu\u00b7cken\u00b7der", "Tat\u00b7be\u00b7gier\u00b7de", ",", "wei\u00b7ter"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADJA", "NN", "$,", "ADV"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Durchs glutenschwangere Gefild.", "tokens": ["Durchs", "glu\u00b7ten\u00b7schwan\u00b7ge\u00b7re", "Ge\u00b7fild", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}