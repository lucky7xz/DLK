{"textgrid.poem.53760": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Abend", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Jetzt ziehen zwanzig M\u00e4nner", "tokens": ["Jetzt", "zie\u00b7hen", "zwan\u00b7zig", "M\u00e4n\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "CARD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die Unterhosen aus.", "tokens": ["die", "Un\u00b7ter\u00b7ho\u00b7sen", "aus", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Gute Nacht, Marie \u2013 ein Kenner", "tokens": ["Gu\u00b7te", "Nacht", ",", "Ma\u00b7rie", "\u2013", "ein", "Ken\u00b7ner"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "NE", "$(", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "von Pechstein sitzt zu Haus", "tokens": ["von", "Pech\u00b7stein", "sitzt", "zu", "Haus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "APPR", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "und schreibt auf lange Bogen", "tokens": ["und", "schreibt", "auf", "lan\u00b7ge", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "von wegen: \u203asteht im Raum\u2039;", "tokens": ["von", "we\u00b7gen", ":", "\u203a", "steht", "im", "Raum", "\u2039", ";"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "APPR", "$.", "$(", "VVFIN", "APPRART", "NN", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "sein Bett wird frisch bezogen.", "tokens": ["sein", "Bett", "wird", "frisch", "be\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sie ruft \u2013 er h\u00f6rt es kaum.", "tokens": ["Sie", "ruft", "\u2013", "er", "h\u00f6rt", "es", "kaum", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PPER", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Verleger ruft: \u00bbIch fahre!\u00ab", "tokens": ["Ver\u00b7le\u00b7ger", "ruft", ":", "\u00bb", "Ich", "fah\u00b7re", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "PPER", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "und steigt ins Auto schlicht.", "tokens": ["und", "steigt", "ins", "Au\u00b7to", "schlicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bezahlte er Honorare,", "tokens": ["Be\u00b7zahl\u00b7te", "er", "Ho\u00b7no\u00b7ra\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "dann h\u00e4tte er das nicht.", "tokens": ["dann", "h\u00e4t\u00b7te", "er", "das", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PDS", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Jetzt sagt Charlotte grade:", "tokens": ["Jetzt", "sagt", "Char\u00b7lot\u00b7te", "gra\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbliebst du mich wegen so?\u00ab", "tokens": ["\u00bb", "liebst", "du", "mich", "we\u00b7gen", "so", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PPER", "PRF", "APPR", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er streichelt ihre Wade", "tokens": ["Er", "strei\u00b7chelt", "ih\u00b7re", "Wa\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "und klopft sie . . .", "tokens": ["und", "klopft", "sie", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "$.", "$.", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.5": {"line.1": {"text": "Zu Bette geht ein Dichter,", "tokens": ["Zu", "Bet\u00b7te", "geht", "ein", "Dich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die Nachttischt\u00fcr macht: schnapp.", "tokens": ["die", "Nacht\u00b7tischt\u00fcr", "macht", ":", "schnapp", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Sogar der deutsche Richter", "tokens": ["So\u00b7gar", "der", "deut\u00b7sche", "Rich\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "montiert die W\u00fcrde ab.", "tokens": ["mon\u00b7tiert", "die", "W\u00fcr\u00b7de", "ab", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Und morgen wieder:", "tokens": ["Und", "mor\u00b7gen", "wie\u00b7der", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Treten", "tokens": ["Tre\u00b7ten"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "von Armen und Verdrehten \u2013", "tokens": ["von", "Ar\u00b7men", "und", "Ver\u00b7dreh\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "lohnt sich das Ganze? Nein.", "tokens": ["lohnt", "sich", "das", "Gan\u00b7ze", "?", "Nein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$.", "PTKANT", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Lieber Gott, h\u00f6r du mein Beten:", "tokens": ["Lie\u00b7ber", "Gott", ",", "h\u00f6r", "du", "mein", "Be\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "La\u00df ewig Abend sein!", "tokens": ["La\u00df", "e\u00b7wig", "A\u00b7bend", "sein", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJD", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Jetzt ziehen zwanzig M\u00e4nner", "tokens": ["Jetzt", "zie\u00b7hen", "zwan\u00b7zig", "M\u00e4n\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "CARD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die Unterhosen aus.", "tokens": ["die", "Un\u00b7ter\u00b7ho\u00b7sen", "aus", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Gute Nacht, Marie \u2013 ein Kenner", "tokens": ["Gu\u00b7te", "Nacht", ",", "Ma\u00b7rie", "\u2013", "ein", "Ken\u00b7ner"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "NE", "$(", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "von Pechstein sitzt zu Haus", "tokens": ["von", "Pech\u00b7stein", "sitzt", "zu", "Haus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "APPR", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "und schreibt auf lange Bogen", "tokens": ["und", "schreibt", "auf", "lan\u00b7ge", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "von wegen: \u203asteht im Raum\u2039;", "tokens": ["von", "we\u00b7gen", ":", "\u203a", "steht", "im", "Raum", "\u2039", ";"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "APPR", "$.", "$(", "VVFIN", "APPRART", "NN", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "sein Bett wird frisch bezogen.", "tokens": ["sein", "Bett", "wird", "frisch", "be\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sie ruft \u2013 er h\u00f6rt es kaum.", "tokens": ["Sie", "ruft", "\u2013", "er", "h\u00f6rt", "es", "kaum", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PPER", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Verleger ruft: \u00bbIch fahre!\u00ab", "tokens": ["Ver\u00b7le\u00b7ger", "ruft", ":", "\u00bb", "Ich", "fah\u00b7re", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "$.", "$(", "PPER", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "und steigt ins Auto schlicht.", "tokens": ["und", "steigt", "ins", "Au\u00b7to", "schlicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bezahlte er Honorare,", "tokens": ["Be\u00b7zahl\u00b7te", "er", "Ho\u00b7no\u00b7ra\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "dann h\u00e4tte er das nicht.", "tokens": ["dann", "h\u00e4t\u00b7te", "er", "das", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PDS", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Jetzt sagt Charlotte grade:", "tokens": ["Jetzt", "sagt", "Char\u00b7lot\u00b7te", "gra\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbliebst du mich wegen so?\u00ab", "tokens": ["\u00bb", "liebst", "du", "mich", "we\u00b7gen", "so", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PPER", "PRF", "APPR", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er streichelt ihre Wade", "tokens": ["Er", "strei\u00b7chelt", "ih\u00b7re", "Wa\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "und klopft sie . . .", "tokens": ["und", "klopft", "sie", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "$.", "$.", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.11": {"line.1": {"text": "Zu Bette geht ein Dichter,", "tokens": ["Zu", "Bet\u00b7te", "geht", "ein", "Dich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die Nachttischt\u00fcr macht: schnapp.", "tokens": ["die", "Nacht\u00b7tischt\u00fcr", "macht", ":", "schnapp", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Sogar der deutsche Richter", "tokens": ["So\u00b7gar", "der", "deut\u00b7sche", "Rich\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "montiert die W\u00fcrde ab.", "tokens": ["mon\u00b7tiert", "die", "W\u00fcr\u00b7de", "ab", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Und morgen wieder:", "tokens": ["Und", "mor\u00b7gen", "wie\u00b7der", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Treten", "tokens": ["Tre\u00b7ten"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "von Armen und Verdrehten \u2013", "tokens": ["von", "Ar\u00b7men", "und", "Ver\u00b7dreh\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "lohnt sich das Ganze? Nein.", "tokens": ["lohnt", "sich", "das", "Gan\u00b7ze", "?", "Nein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$.", "PTKANT", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Lieber Gott, h\u00f6r du mein Beten:", "tokens": ["Lie\u00b7ber", "Gott", ",", "h\u00f6r", "du", "mein", "Be\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "La\u00df ewig Abend sein!", "tokens": ["La\u00df", "e\u00b7wig", "A\u00b7bend", "sein", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJD", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}