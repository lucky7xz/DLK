{"textgrid.poem.53507": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Professoren", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Er ging durch alte Winkelg\u00e4\u00dfchen,", "tokens": ["Er", "ging", "durch", "al\u00b7te", "Win\u00b7kel\u00b7g\u00e4\u00df\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "im schlappen Hut, in faltigem Rock.", "tokens": ["im", "schlap\u00b7pen", "Hut", ",", "in", "fal\u00b7ti\u00b7gem", "Rock", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Ein kleines B\u00e4uchlein wie ein F\u00e4\u00dfchen", "tokens": ["Ein", "klei\u00b7nes", "B\u00e4uch\u00b7lein", "wie", "ein", "F\u00e4\u00df\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": " . . . nicht jung mehr . . . graues Stirngelock . . .", "tokens": [".", ".", ".", "nicht", "jung", "mehr", ".", ".", ".", "grau\u00b7es", "Stirn\u00b7ge\u00b7lock", ".", ".", "."], "token_info": ["punct", "punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "PTKNEG", "ADJD", "ADV", "$.", "$.", "$.", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verga\u00df er auch sein Regendach,", "tokens": ["Ver\u00b7ga\u00df", "er", "auch", "sein", "Re\u00b7gen\u00b7dach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "man raunte: \u00bbDer versteht sein Fach!\u00ab", "tokens": ["man", "raun\u00b7te", ":", "\u00bb", "Der", "ver\u00b7steht", "sein", "Fach", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$.", "$(", "PDS", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ein stilles, manchmal tiefes Gew\u00e4sser:", "tokens": ["Ein", "stil\u00b7les", ",", "manch\u00b7mal", "tie\u00b7fes", "Ge\u00b7w\u00e4s\u00b7ser", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "der alte Professor.", "tokens": ["der", "al\u00b7te", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Und heut? Im lauten Weltgebrause", "tokens": ["Und", "heut", "?", "Im", "lau\u00b7ten", "Welt\u00b7ge\u00b7brau\u00b7se"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$.", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "bewegt sich der Privatdozent.", "tokens": ["be\u00b7wegt", "sich", "der", "Pri\u00b7vat\u00b7do\u00b7zent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er redet in und au\u00dferm Hause", "tokens": ["Er", "re\u00b7det", "in", "und", "au\u00b7\u00dferm", "Hau\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "KON", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "von Politik mit viel Talent.", "tokens": ["von", "Po\u00b7li\u00b7tik", "mit", "viel", "Ta\u00b7lent", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Beziehungen zur Industrie", "tokens": ["Be\u00b7zie\u00b7hun\u00b7gen", "zur", "In\u00b7dust\u00b7rie"], "token_info": ["word", "word", "word"], "pos": ["NN", "APPRART", "NN"], "meter": "--+--+--", "measure": "anapaest.di.plus"}, "line.6": {"text": "sind sehr beliebt, drum hat man sie.", "tokens": ["sind", "sehr", "be\u00b7liebt", ",", "drum", "hat", "man", "sie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "$,", "PAV", "VAFIN", "PIS", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wild fuchtelnd fordert den Krieg bis aufs Messer", "tokens": ["Wild", "fuch\u00b7telnd", "for\u00b7dert", "den", "Krieg", "bis", "aufs", "Mes\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVPP", "VVFIN", "ART", "NN", "APPR", "APPRART", "NN"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.8": {"text": "der neue Professor.", "tokens": ["der", "neu\u00b7e", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}}, "stanza.3": {"line.1": {"text": "Man sagt, weltfremd sei er gewesen.", "tokens": ["Man", "sagt", ",", "welt\u00b7fremd", "sei", "er", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PWAT", "VAFIN", "PPER", "VAPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wie sind sie heute so gewandt!", "tokens": ["Wie", "sind", "sie", "heu\u00b7te", "so", "ge\u00b7wandt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sagt: er konnte nichts als lesen.", "tokens": ["Man", "sagt", ":", "er", "konn\u00b7te", "nichts", "als", "le\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$.", "PPER", "VMFIN", "PIS", "KOKOM", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie w\u00e4scht sich heute Hand und Hand!", "tokens": ["Wie", "w\u00e4scht", "sich", "heu\u00b7te", "Hand", "und", "Hand", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PRF", "ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der lehrt nicht mehr. Der propagiert.", "tokens": ["Der", "lehrt", "nicht", "mehr", ".", "Der", "pro\u00b7pa\u00b7giert", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "ADV", "$.", "ART", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und wer erzieht den, der studiert?", "tokens": ["Und", "wer", "er\u00b7zieht", "den", ",", "der", "stu\u00b7diert", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ART", "$,", "PRELS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ich kann mir nicht helfen, er war doch viel besser:", "tokens": ["Ich", "kann", "mir", "nicht", "hel\u00b7fen", ",", "er", "war", "doch", "viel", "bes\u00b7ser", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,", "PPER", "VAFIN", "ADV", "ADV", "ADJD", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.8": {"text": "der alte, deutsche, zerstreute Professor.", "tokens": ["der", "al\u00b7te", ",", "deut\u00b7sche", ",", "zer\u00b7streu\u00b7te", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Er ging durch alte Winkelg\u00e4\u00dfchen,", "tokens": ["Er", "ging", "durch", "al\u00b7te", "Win\u00b7kel\u00b7g\u00e4\u00df\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "im schlappen Hut, in faltigem Rock.", "tokens": ["im", "schlap\u00b7pen", "Hut", ",", "in", "fal\u00b7ti\u00b7gem", "Rock", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Ein kleines B\u00e4uchlein wie ein F\u00e4\u00dfchen", "tokens": ["Ein", "klei\u00b7nes", "B\u00e4uch\u00b7lein", "wie", "ein", "F\u00e4\u00df\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": " . . . nicht jung mehr . . . graues Stirngelock . . .", "tokens": [".", ".", ".", "nicht", "jung", "mehr", ".", ".", ".", "grau\u00b7es", "Stirn\u00b7ge\u00b7lock", ".", ".", "."], "token_info": ["punct", "punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "PTKNEG", "ADJD", "ADV", "$.", "$.", "$.", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verga\u00df er auch sein Regendach,", "tokens": ["Ver\u00b7ga\u00df", "er", "auch", "sein", "Re\u00b7gen\u00b7dach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "man raunte: \u00bbDer versteht sein Fach!\u00ab", "tokens": ["man", "raun\u00b7te", ":", "\u00bb", "Der", "ver\u00b7steht", "sein", "Fach", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$.", "$(", "PDS", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ein stilles, manchmal tiefes Gew\u00e4sser:", "tokens": ["Ein", "stil\u00b7les", ",", "manch\u00b7mal", "tie\u00b7fes", "Ge\u00b7w\u00e4s\u00b7ser", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "der alte Professor.", "tokens": ["der", "al\u00b7te", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Und heut? Im lauten Weltgebrause", "tokens": ["Und", "heut", "?", "Im", "lau\u00b7ten", "Welt\u00b7ge\u00b7brau\u00b7se"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$.", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "bewegt sich der Privatdozent.", "tokens": ["be\u00b7wegt", "sich", "der", "Pri\u00b7vat\u00b7do\u00b7zent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er redet in und au\u00dferm Hause", "tokens": ["Er", "re\u00b7det", "in", "und", "au\u00b7\u00dferm", "Hau\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "KON", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "von Politik mit viel Talent.", "tokens": ["von", "Po\u00b7li\u00b7tik", "mit", "viel", "Ta\u00b7lent", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Beziehungen zur Industrie", "tokens": ["Be\u00b7zie\u00b7hun\u00b7gen", "zur", "In\u00b7dust\u00b7rie"], "token_info": ["word", "word", "word"], "pos": ["NN", "APPRART", "NN"], "meter": "--+--+--", "measure": "anapaest.di.plus"}, "line.6": {"text": "sind sehr beliebt, drum hat man sie.", "tokens": ["sind", "sehr", "be\u00b7liebt", ",", "drum", "hat", "man", "sie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "$,", "PAV", "VAFIN", "PIS", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wild fuchtelnd fordert den Krieg bis aufs Messer", "tokens": ["Wild", "fuch\u00b7telnd", "for\u00b7dert", "den", "Krieg", "bis", "aufs", "Mes\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVPP", "VVFIN", "ART", "NN", "APPR", "APPRART", "NN"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.8": {"text": "der neue Professor.", "tokens": ["der", "neu\u00b7e", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}}, "stanza.6": {"line.1": {"text": "Man sagt, weltfremd sei er gewesen.", "tokens": ["Man", "sagt", ",", "welt\u00b7fremd", "sei", "er", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PWAT", "VAFIN", "PPER", "VAPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wie sind sie heute so gewandt!", "tokens": ["Wie", "sind", "sie", "heu\u00b7te", "so", "ge\u00b7wandt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sagt: er konnte nichts als lesen.", "tokens": ["Man", "sagt", ":", "er", "konn\u00b7te", "nichts", "als", "le\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$.", "PPER", "VMFIN", "PIS", "KOKOM", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie w\u00e4scht sich heute Hand und Hand!", "tokens": ["Wie", "w\u00e4scht", "sich", "heu\u00b7te", "Hand", "und", "Hand", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PRF", "ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der lehrt nicht mehr. Der propagiert.", "tokens": ["Der", "lehrt", "nicht", "mehr", ".", "Der", "pro\u00b7pa\u00b7giert", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "ADV", "$.", "ART", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und wer erzieht den, der studiert?", "tokens": ["Und", "wer", "er\u00b7zieht", "den", ",", "der", "stu\u00b7diert", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ART", "$,", "PRELS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ich kann mir nicht helfen, er war doch viel besser:", "tokens": ["Ich", "kann", "mir", "nicht", "hel\u00b7fen", ",", "er", "war", "doch", "viel", "bes\u00b7ser", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,", "PPER", "VAFIN", "ADV", "ADV", "ADJD", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.8": {"text": "der alte, deutsche, zerstreute Professor.", "tokens": ["der", "al\u00b7te", ",", "deut\u00b7sche", ",", "zer\u00b7streu\u00b7te", "Pro\u00b7fes\u00b7sor", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}}}}