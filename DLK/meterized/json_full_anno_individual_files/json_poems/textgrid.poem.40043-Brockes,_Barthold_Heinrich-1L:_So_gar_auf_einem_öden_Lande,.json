{"textgrid.poem.40043": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: So gar auf einem \u00f6den Lande,", "genre": "verse", "period": "N.A.", "pub_year": 1713, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "So gar auf einem \u00f6den Lande,", "tokens": ["So", "gar", "auf", "ei\u00b7nem", "\u00f6\u00b7den", "Lan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo weder Baum, noch Strauch, noch Gras,", "tokens": ["Wo", "we\u00b7der", "Baum", ",", "noch", "Strauch", ",", "noch", "Gras", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "KON", "NE", "$,", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Selbst in dem unfruchtbaren Sande", "tokens": ["Selbst", "in", "dem", "un\u00b7frucht\u00b7ba\u00b7ren", "San\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Find't ein betrachtend Auge was,", "tokens": ["Find't", "ein", "be\u00b7trach\u00b7tend", "Au\u00b7ge", "was", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJD", "NN", "PWS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In diesem sch\u00f6nen Welt-Geb\u00e4ude,", "tokens": ["In", "die\u00b7sem", "sch\u00f6\u00b7nen", "Welt\u00b7Ge\u00b7b\u00e4u\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Zu GOTTES Ehr' und eigner Freude.", "tokens": ["Zu", "GoT\u00b7TES", "Ehr'", "und", "eig\u00b7ner", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Auf! lasset uns denn weiter gehn,", "tokens": ["Auf", "!", "las\u00b7set", "uns", "denn", "wei\u00b7ter", "gehn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$.", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und GOTT zum Ruhm was sehn, auch wenn wir nichts fast sehn!", "tokens": ["Und", "GoTT", "zum", "Ruhm", "was", "sehn", ",", "auch", "wenn", "wir", "nichts", "fast", "sehn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "APPRART", "NN", "PIS", "VVINF", "$,", "ADV", "KOUS", "PPER", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Sandes-K\u00f6rper selbst und Theilchen unsrer Erden,", "tokens": ["Die", "San\u00b7des\u00b7K\u00f6r\u00b7per", "selbst", "und", "Theil\u00b7chen", "uns\u00b7rer", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "KON", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Sind ebenfalls ja wircklich Creaturen,", "tokens": ["Sind", "e\u00b7ben\u00b7falls", "ja", "wir\u00b7ck\u00b7lich", "Crea\u00b7tu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Worin, wenn wir den Geist mit unserm Blick verbinden,", "tokens": ["Wo\u00b7rin", ",", "wenn", "wir", "den", "Geist", "mit", "un\u00b7serm", "Blick", "ver\u00b7bin\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "KOUS", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wir mancherley Vergn\u00fcgen finden,", "tokens": ["Wir", "man\u00b7cher\u00b7ley", "Ver\u00b7gn\u00fc\u00b7gen", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Da, wenn sonst nichts zu sehn, doch allerley Figuren", "tokens": ["Da", ",", "wenn", "sonst", "nichts", "zu", "sehn", ",", "doch", "al\u00b7ler\u00b7ley", "Fi\u00b7gu\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "ADV", "PIS", "PTKZU", "VVINF", "$,", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Von eingedruckten Spuren", "tokens": ["Von", "ein\u00b7ge\u00b7druck\u00b7ten", "Spu\u00b7ren"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Im d\u00fcrren Sande ja gefunden werden.", "tokens": ["Im", "d\u00fcr\u00b7ren", "San\u00b7de", "ja", "ge\u00b7fun\u00b7den", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "In kleinen Tiefen, kleinen H\u00f6h'n,", "tokens": ["In", "klei\u00b7nen", "Tie\u00b7fen", ",", "klei\u00b7nen", "H\u00f6h'n", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kann ein aufmercksam Hertz so Licht, als Schatten, seh'n.", "tokens": ["Kann", "ein", "auf\u00b7merck\u00b7sam", "Hertz", "so", "Licht", ",", "als", "Schat\u00b7ten", ",", "seh'", "n."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "abbreviation"], "pos": ["VMFIN", "ART", "ADJD", "NN", "ADV", "NN", "$,", "KOUS", "NN", "$,", "VVFIN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man trifft, wenn man so gar allein,", "tokens": ["Man", "trifft", ",", "wenn", "man", "so", "gar", "al\u00b7lein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "KOUS", "PIS", "ADV", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df weder Laub, noch Kraut, noch B\u00e4ume bey uns seyn,", "tokens": ["Da\u00df", "we\u00b7der", "Laub", ",", "noch", "Kraut", ",", "noch", "B\u00e4u\u00b7me", "bey", "uns", "seyn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KON", "NN", "$,", "ADV", "NN", "$,", "ADV", "NN", "APPR", "PPER", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dennoch Ver\u00e4nderung nicht ohn' Vergn\u00fcgen an,", "tokens": ["Den\u00b7noch", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", "nicht", "ohn'", "Ver\u00b7gn\u00fc\u00b7gen", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "PTKNEG", "APPR", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.6": {"text": "Wie jeder, der es recht betrachtet, finden kann.", "tokens": ["Wie", "je\u00b7der", ",", "der", "es", "recht", "be\u00b7trach\u00b7tet", ",", "fin\u00b7den", "kann", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PIS", "$,", "PRELS", "PPER", "ADJD", "VVPP", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Es kommet jedes Sand-Korn mir", "tokens": ["Es", "kom\u00b7met", "je\u00b7des", "San\u00b7dKorn", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als wie ein kleines Glied", "tokens": ["Als", "wie", "ein", "klei\u00b7nes", "Glied"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der allgemeinen Mutter f\u00fcr.", "tokens": ["Der", "all\u00b7ge\u00b7mei\u00b7nen", "Mut\u00b7ter", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von unsrer Welt ist es ein wircklich Theilchen mit.", "tokens": ["Von", "uns\u00b7rer", "Welt", "ist", "es", "ein", "wir\u00b7ck\u00b7lich", "Theil\u00b7chen", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "PPER", "ART", "ADJD", "NN", "PTKVZ", "$."], "meter": "-+-++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.5": {"text": "Die Kleinheit, Festigkeit, die Klarheit, Gl\u00e4tt' und R\u00fcnde,", "tokens": ["Die", "Klein\u00b7heit", ",", "Fes\u00b7tig\u00b7keit", ",", "die", "Klar\u00b7heit", ",", "Gl\u00e4tt'", "und", "R\u00fcn\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$,", "ART", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die ich in manchem Sand-Korn finde,", "tokens": ["Die", "ich", "in", "man\u00b7chem", "San\u00b7dKorn", "fin\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Wodurch sie sich nicht gantz verbinden k\u00f6nnen,", "tokens": ["Wo\u00b7durch", "sie", "sich", "nicht", "gantz", "ver\u00b7bin\u00b7den", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "PTKNEG", "ADV", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und eben dadurch allem Saft", "tokens": ["Und", "e\u00b7ben", "da\u00b7durch", "al\u00b7lem", "Saft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PAV", "PIS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Vom Regen oder Thau, zu der Gew\u00e4chse Kraft,", "tokens": ["Vom", "Re\u00b7gen", "o\u00b7der", "Thau", ",", "zu", "der", "Ge\u00b7w\u00e4ch\u00b7se", "Kraft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "$,", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Den Aufenthalt und Durchgang g\u00f6nnen,", "tokens": ["Den", "Auf\u00b7ent\u00b7halt", "und", "Durch\u00b7gang", "g\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Ist ja Bewunderns-werth. Noch mehr, da sie vereint,", "tokens": ["Ist", "ja", "Be\u00b7wun\u00b7derns\u00b7werth", ".", "Noch", "mehr", ",", "da", "sie", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "$.", "ADV", "ADV", "$,", "KOUS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und doch nicht gantz, (indem sie sonst versteint,)", "tokens": ["Und", "doch", "nicht", "gantz", ",", "(", "in\u00b7dem", "sie", "sonst", "ver\u00b7steint", ",", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADV", "$,", "$(", "KOUS", "PPER", "ADV", "VVFIN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "So k\u00f6nnen sie den Pflantzen n\u00fctzen,", "tokens": ["So", "k\u00f6n\u00b7nen", "sie", "den", "Pflant\u00b7zen", "n\u00fct\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Den Wurtzeln Raum, sich auszubreiten, geben,", "tokens": ["Den", "Wurt\u00b7zeln", "Raum", ",", "sich", "aus\u00b7zu\u00b7brei\u00b7ten", ",", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "PRF", "VVIZU", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Auch, wenn dieselbigen sich aufw\u00e4rts heben,", "tokens": ["Auch", ",", "wenn", "die\u00b7sel\u00b7bi\u00b7gen", "sich", "auf\u00b7w\u00e4rts", "he\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PDS", "PRF", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Dieselben so viel besser st\u00fctzen.", "tokens": ["Die\u00b7sel\u00b7ben", "so", "viel", "bes\u00b7ser", "st\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "ADV", "ADV", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Ich nahm hierauf ein H\u00e4uflein Sand,", "tokens": ["Ich", "nahm", "hier\u00b7auf", "ein", "H\u00e4uf\u00b7lein", "Sand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Betrachtet' es genau, und fand", "tokens": ["Be\u00b7trach\u00b7tet'", "es", "ge\u00b7nau", ",", "und", "fand"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD", "$,", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den Unterschied, da\u00df er nicht mancherley,", "tokens": ["Den", "Un\u00b7ter\u00b7schied", ",", "da\u00df", "er", "nicht", "man\u00b7cher\u00b7ley", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "PTKNEG", "PIS", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Nein, in der That unz\u00e4hlig sey.", "tokens": ["Nein", ",", "in", "der", "That", "un\u00b7z\u00e4h\u00b7lig", "sey", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich konnte tausend Form- und Ecken", "tokens": ["Ich", "konn\u00b7te", "tau\u00b7send", "For\u00b7m", "und", "E\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "CARD", "TRUNC", "KON", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Auch an dem kleinsten Sand entdecken.", "tokens": ["Auch", "an", "dem", "kleins\u00b7ten", "Sand", "ent\u00b7de\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Theils sind die K\u00f6rner lang, theils rund, theils gro\u00df, theils klein,", "tokens": ["Theils", "sind", "die", "K\u00f6r\u00b7ner", "lang", ",", "theils", "rund", ",", "theils", "gro\u00df", ",", "theils", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Theils schwartz, theils braun, theils gelb, theils grau,", "tokens": ["Theils", "schwartz", ",", "theils", "braun", ",", "theils", "gelb", ",", "theils", "grau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Theils r\u00f6thlich, wei\u00dflich theils, theils blau,", "tokens": ["Theils", "r\u00f6th\u00b7lich", ",", "wei\u00df\u00b7lich", "theils", ",", "theils", "blau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "ADJD", "ADV", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Es sind die meisten dicht und dunckel, viele helle,", "tokens": ["Es", "sind", "die", "meis\u00b7ten", "dicht", "und", "dun\u00b7ckel", ",", "vie\u00b7le", "hel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "VVFIN", "ADJD", "KON", "ADJD", "$,", "PIAT", "ADJA", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Durchsichtig, gl\u00e4ntzend, rein.", "tokens": ["Durch\u00b7sich\u00b7tig", ",", "gl\u00e4nt\u00b7zend", ",", "rein", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.12": {"text": "Ich wurd' auf mancher Stelle", "tokens": ["Ich", "wurd'", "auf", "man\u00b7cher", "Stel\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.13": {"text": "Verschiedener, die, wie Krystall so klar,", "tokens": ["Ver\u00b7schie\u00b7de\u00b7ner", ",", "die", ",", "wie", "Krys\u00b7tall", "so", "klar", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "$,", "PWAV", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}, "line.14": {"text": "Mit Lust und mit Verwunderung gewahr.", "tokens": ["Mit", "Lust", "und", "mit", "Ver\u00b7wun\u00b7de\u00b7rung", "ge\u00b7wahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Indem ich nun die Kleinheit \u00fcbersehe,", "tokens": ["In\u00b7dem", "ich", "nun", "die", "Klein\u00b7heit", "\u00fc\u00b7ber\u00b7se\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und alles dieses \u00fcberlege;", "tokens": ["Und", "al\u00b7les", "die\u00b7ses", "\u00fc\u00b7berl\u00b7e\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PDAT", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Erstaun' ich, wenn ich recht erwege,", "tokens": ["Er\u00b7staun'", "ich", ",", "wenn", "ich", "recht", "er\u00b7we\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df alle Gr\u00f6sse dieser Welt,", "tokens": ["Da\u00df", "al\u00b7le", "Gr\u00f6s\u00b7se", "die\u00b7ser", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ja selbst die Welt aus Kleinigkeiten nur,", "tokens": ["Ja", "selbst", "die", "Welt", "aus", "Klei\u00b7nig\u00b7kei\u00b7ten", "nur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "ART", "NN", "APPR", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Wie gro\u00df sie uns auch scheint und wircklich ist, bestehe.", "tokens": ["Wie", "gro\u00df", "sie", "uns", "auch", "scheint", "und", "wir\u00b7ck\u00b7lich", "ist", ",", "be\u00b7ste\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "PPER", "ADV", "VVFIN", "KON", "ADJD", "VAFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "Es fiel mir ferner bey,", "tokens": ["Es", "fiel", "mir", "fer\u00b7ner", "bey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Wie Kleinigkeiten fast in allen Sachen", "tokens": ["Wie", "Klei\u00b7nig\u00b7kei\u00b7ten", "fast", "in", "al\u00b7len", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Besondere Ver\u00e4nderungen machen.", "tokens": ["Be\u00b7son\u00b7de\u00b7re", "Ver\u00b7\u00e4n\u00b7de\u00b7run\u00b7gen", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Was ist die sch\u00f6ne Kunst der edlen Mahlerey,", "tokens": ["Was", "ist", "die", "sch\u00f6\u00b7ne", "Kunst", "der", "ed\u00b7len", "Mah\u00b7le\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Die guten Theils aus Farben nur bestehet,", "tokens": ["Die", "gu\u00b7ten", "Theils", "aus", "Far\u00b7ben", "nur", "be\u00b7ste\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Und diese wiederum aus Sand und Erden?", "tokens": ["Und", "die\u00b7se", "wie\u00b7de\u00b7rum", "aus", "Sand", "und", "Er\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Wodurch jedoch die sch\u00f6nsten Bilder werden.", "tokens": ["Wo\u00b7durch", "je\u00b7doch", "die", "sch\u00f6ns\u00b7ten", "Bil\u00b7der", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.14": {"text": "Denn das, was unser Aug' erfrischt", "tokens": ["Denn", "das", ",", "was", "un\u00b7ser", "Aug'", "er\u00b7frischt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PDS", "$,", "PRELS", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Auf solche wundersame Art,", "tokens": ["Auf", "sol\u00b7che", "wun\u00b7der\u00b7sa\u00b7me", "Art", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Ist blo\u00df ein wenig Sand mit Oel gemischt,", "tokens": ["Ist", "blo\u00df", "ein", "we\u00b7nig", "Sand", "mit", "O\u00b7el", "ge\u00b7mischt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "PIAT", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+----+", "measure": "unknown.measure.tetra"}, "line.17": {"text": "Ist so unglaublich d\u00fcnn und zart,", "tokens": ["Ist", "so", "un\u00b7glaub\u00b7lich", "d\u00fcnn", "und", "zart", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.18": {"text": "Da\u00df, wenn man es vom Tuche trennen wollte,", "tokens": ["Da\u00df", ",", "wenn", "man", "es", "vom", "Tu\u00b7che", "tren\u00b7nen", "woll\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "KOUS", "PIS", "PPER", "APPRART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "Man es f\u00fcr c\u00f6rperlich kaum halten sollte.", "tokens": ["Man", "es", "f\u00fcr", "c\u00f6r\u00b7per\u00b7lich", "kaum", "hal\u00b7ten", "soll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "APPR", "ADJD", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Noch mehr, wie wunderbar", "tokens": ["Noch", "mehr", ",", "wie", "wun\u00b7der\u00b7bar"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "$,", "PWAV", "ADJD"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Erhellt im Sande Gottes Macht,", "tokens": ["Er\u00b7hellt", "im", "San\u00b7de", "Got\u00b7tes", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der alles nicht allein aus Nichts hervor gebracht;", "tokens": ["Der", "al\u00b7les", "nicht", "al\u00b7lein", "aus", "Nichts", "her\u00b7vor", "ge\u00b7bracht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PTKNEG", "ADV", "APPR", "PIS", "PTKVZ", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der auch so gar", "tokens": ["Der", "auch", "so", "gar"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Durch solche Kleinigkeit das allergr\u00f6\u00dfte zwinget,", "tokens": ["Durch", "sol\u00b7che", "Klei\u00b7nig\u00b7keit", "das", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7te", "zwin\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "ADJA", "VVFIN", "$,"], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.6": {"text": "Indem Er, durch so kleinen Sand,", "tokens": ["In\u00b7dem", "Er", ",", "durch", "so", "klei\u00b7nen", "Sand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Die ungeheure Fluthen-Last", "tokens": ["Die", "un\u00b7ge\u00b7heu\u00b7re", "Flu\u00b7then\u00b7Last"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So wunderbarlich eingefasst,", "tokens": ["So", "wun\u00b7der\u00b7bar\u00b7lich", "ein\u00b7ge\u00b7fasst", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Da\u00df aller Wellen Wuth nicht durch ihn dringet.", "tokens": ["Da\u00df", "al\u00b7ler", "Wel\u00b7len", "Wuth", "nicht", "durch", "ihn", "drin\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "NN", "PTKNEG", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Hiemit stimmt alles \u00fcberein,", "tokens": ["Hie\u00b7mit", "stimmt", "al\u00b7les", "\u00fc\u00b7be\u00b7re\u00b7in", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PTKVZ", "$,"], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.11": {"text": "Da\u00df, wir f\u00fcr uns das allerkleinste gro\u00df,", "tokens": ["Da\u00df", ",", "wir", "f\u00fcr", "uns", "das", "al\u00b7ler\u00b7kleins\u00b7te", "gro\u00df", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PPER", "APPR", "PPER", "ART", "ADJA", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Also f\u00fcr GOTT das allergr\u00f6\u00dfte klein,", "tokens": ["Al\u00b7so", "f\u00fcr", "GoTT", "das", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7te", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "ART", "ADJA", "ADJD", "$,"], "meter": "---+-+-+-+", "measure": "zehnsilber"}, "line.13": {"text": "Daher denn David auch recht unvergleichlich schlo\u00df:", "tokens": ["Da\u00b7her", "denn", "Da\u00b7vid", "auch", "recht", "un\u00b7ver\u00b7gleich\u00b7lich", "schlo\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "KON", "NE", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wie das Z\u00fcnglein an der Wage, so ist, Herr, vor Dir die Welt;", "tokens": ["Wie", "das", "Z\u00fcn\u00b7glein", "an", "der", "Wa\u00b7ge", ",", "so", "ist", ",", "Herr", ",", "vor", "Dir", "die", "Welt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "ART", "NN", "$,", "ADV", "VAFIN", "$,", "NN", "$,", "APPR", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.2": {"text": "Wie der Tropfen aus dem Eimer, welcher auf die Erde f\u00e4llt.", "tokens": ["Wie", "der", "Trop\u00b7fen", "aus", "dem", "Ei\u00b7mer", ",", "wel\u00b7cher", "auf", "die", "Er\u00b7de", "f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "ART", "NN", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}}, "stanza.9": {"line.1": {"text": "So gar auf einem \u00f6den Lande,", "tokens": ["So", "gar", "auf", "ei\u00b7nem", "\u00f6\u00b7den", "Lan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo weder Baum, noch Strauch, noch Gras,", "tokens": ["Wo", "we\u00b7der", "Baum", ",", "noch", "Strauch", ",", "noch", "Gras", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "KON", "NE", "$,", "ADV", "NN", "$,", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Selbst in dem unfruchtbaren Sande", "tokens": ["Selbst", "in", "dem", "un\u00b7frucht\u00b7ba\u00b7ren", "San\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Find't ein betrachtend Auge was,", "tokens": ["Find't", "ein", "be\u00b7trach\u00b7tend", "Au\u00b7ge", "was", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJD", "NN", "PWS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In diesem sch\u00f6nen Welt-Geb\u00e4ude,", "tokens": ["In", "die\u00b7sem", "sch\u00f6\u00b7nen", "Welt\u00b7Ge\u00b7b\u00e4u\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Zu GOTTES Ehr' und eigner Freude.", "tokens": ["Zu", "GoT\u00b7TES", "Ehr'", "und", "eig\u00b7ner", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Auf! lasset uns denn weiter gehn,", "tokens": ["Auf", "!", "las\u00b7set", "uns", "denn", "wei\u00b7ter", "gehn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$.", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und GOTT zum Ruhm was sehn, auch wenn wir nichts fast sehn!", "tokens": ["Und", "GoTT", "zum", "Ruhm", "was", "sehn", ",", "auch", "wenn", "wir", "nichts", "fast", "sehn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "APPRART", "NN", "PIS", "VVINF", "$,", "ADV", "KOUS", "PPER", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Sandes-K\u00f6rper selbst und Theilchen unsrer Erden,", "tokens": ["Die", "San\u00b7des\u00b7K\u00f6r\u00b7per", "selbst", "und", "Theil\u00b7chen", "uns\u00b7rer", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "KON", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Sind ebenfalls ja wircklich Creaturen,", "tokens": ["Sind", "e\u00b7ben\u00b7falls", "ja", "wir\u00b7ck\u00b7lich", "Crea\u00b7tu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Worin, wenn wir den Geist mit unserm Blick verbinden,", "tokens": ["Wo\u00b7rin", ",", "wenn", "wir", "den", "Geist", "mit", "un\u00b7serm", "Blick", "ver\u00b7bin\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "KOUS", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wir mancherley Vergn\u00fcgen finden,", "tokens": ["Wir", "man\u00b7cher\u00b7ley", "Ver\u00b7gn\u00fc\u00b7gen", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Da, wenn sonst nichts zu sehn, doch allerley Figuren", "tokens": ["Da", ",", "wenn", "sonst", "nichts", "zu", "sehn", ",", "doch", "al\u00b7ler\u00b7ley", "Fi\u00b7gu\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "ADV", "PIS", "PTKZU", "VVINF", "$,", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Von eingedruckten Spuren", "tokens": ["Von", "ein\u00b7ge\u00b7druck\u00b7ten", "Spu\u00b7ren"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Im d\u00fcrren Sande ja gefunden werden.", "tokens": ["Im", "d\u00fcr\u00b7ren", "San\u00b7de", "ja", "ge\u00b7fun\u00b7den", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "In kleinen Tiefen, kleinen H\u00f6h'n,", "tokens": ["In", "klei\u00b7nen", "Tie\u00b7fen", ",", "klei\u00b7nen", "H\u00f6h'n", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kann ein aufmercksam Hertz so Licht, als Schatten, seh'n.", "tokens": ["Kann", "ein", "auf\u00b7merck\u00b7sam", "Hertz", "so", "Licht", ",", "als", "Schat\u00b7ten", ",", "seh'", "n."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "abbreviation"], "pos": ["VMFIN", "ART", "ADJD", "NN", "ADV", "NN", "$,", "KOUS", "NN", "$,", "VVFIN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man trifft, wenn man so gar allein,", "tokens": ["Man", "trifft", ",", "wenn", "man", "so", "gar", "al\u00b7lein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "KOUS", "PIS", "ADV", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df weder Laub, noch Kraut, noch B\u00e4ume bey uns seyn,", "tokens": ["Da\u00df", "we\u00b7der", "Laub", ",", "noch", "Kraut", ",", "noch", "B\u00e4u\u00b7me", "bey", "uns", "seyn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KON", "NN", "$,", "ADV", "NN", "$,", "ADV", "NN", "APPR", "PPER", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dennoch Ver\u00e4nderung nicht ohn' Vergn\u00fcgen an,", "tokens": ["Den\u00b7noch", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", "nicht", "ohn'", "Ver\u00b7gn\u00fc\u00b7gen", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "PTKNEG", "APPR", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.6": {"text": "Wie jeder, der es recht betrachtet, finden kann.", "tokens": ["Wie", "je\u00b7der", ",", "der", "es", "recht", "be\u00b7trach\u00b7tet", ",", "fin\u00b7den", "kann", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PIS", "$,", "PRELS", "PPER", "ADJD", "VVPP", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Es kommet jedes Sand-Korn mir", "tokens": ["Es", "kom\u00b7met", "je\u00b7des", "San\u00b7dKorn", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als wie ein kleines Glied", "tokens": ["Als", "wie", "ein", "klei\u00b7nes", "Glied"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der allgemeinen Mutter f\u00fcr.", "tokens": ["Der", "all\u00b7ge\u00b7mei\u00b7nen", "Mut\u00b7ter", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von unsrer Welt ist es ein wircklich Theilchen mit.", "tokens": ["Von", "uns\u00b7rer", "Welt", "ist", "es", "ein", "wir\u00b7ck\u00b7lich", "Theil\u00b7chen", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "PPER", "ART", "ADJD", "NN", "PTKVZ", "$."], "meter": "-+-++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.5": {"text": "Die Kleinheit, Festigkeit, die Klarheit, Gl\u00e4tt' und R\u00fcnde,", "tokens": ["Die", "Klein\u00b7heit", ",", "Fes\u00b7tig\u00b7keit", ",", "die", "Klar\u00b7heit", ",", "Gl\u00e4tt'", "und", "R\u00fcn\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$,", "ART", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die ich in manchem Sand-Korn finde,", "tokens": ["Die", "ich", "in", "man\u00b7chem", "San\u00b7dKorn", "fin\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Wodurch sie sich nicht gantz verbinden k\u00f6nnen,", "tokens": ["Wo\u00b7durch", "sie", "sich", "nicht", "gantz", "ver\u00b7bin\u00b7den", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "PTKNEG", "ADV", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und eben dadurch allem Saft", "tokens": ["Und", "e\u00b7ben", "da\u00b7durch", "al\u00b7lem", "Saft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PAV", "PIS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Vom Regen oder Thau, zu der Gew\u00e4chse Kraft,", "tokens": ["Vom", "Re\u00b7gen", "o\u00b7der", "Thau", ",", "zu", "der", "Ge\u00b7w\u00e4ch\u00b7se", "Kraft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "$,", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Den Aufenthalt und Durchgang g\u00f6nnen,", "tokens": ["Den", "Auf\u00b7ent\u00b7halt", "und", "Durch\u00b7gang", "g\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Ist ja Bewunderns-werth. Noch mehr, da sie vereint,", "tokens": ["Ist", "ja", "Be\u00b7wun\u00b7derns\u00b7werth", ".", "Noch", "mehr", ",", "da", "sie", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "$.", "ADV", "ADV", "$,", "KOUS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und doch nicht gantz, (indem sie sonst versteint,)", "tokens": ["Und", "doch", "nicht", "gantz", ",", "(", "in\u00b7dem", "sie", "sonst", "ver\u00b7steint", ",", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADV", "$,", "$(", "KOUS", "PPER", "ADV", "VVFIN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "So k\u00f6nnen sie den Pflantzen n\u00fctzen,", "tokens": ["So", "k\u00f6n\u00b7nen", "sie", "den", "Pflant\u00b7zen", "n\u00fct\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Den Wurtzeln Raum, sich auszubreiten, geben,", "tokens": ["Den", "Wurt\u00b7zeln", "Raum", ",", "sich", "aus\u00b7zu\u00b7brei\u00b7ten", ",", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "PRF", "VVIZU", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Auch, wenn dieselbigen sich aufw\u00e4rts heben,", "tokens": ["Auch", ",", "wenn", "die\u00b7sel\u00b7bi\u00b7gen", "sich", "auf\u00b7w\u00e4rts", "he\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PDS", "PRF", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Dieselben so viel besser st\u00fctzen.", "tokens": ["Die\u00b7sel\u00b7ben", "so", "viel", "bes\u00b7ser", "st\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "ADV", "ADV", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.13": {"line.1": {"text": "Ich nahm hierauf ein H\u00e4uflein Sand,", "tokens": ["Ich", "nahm", "hier\u00b7auf", "ein", "H\u00e4uf\u00b7lein", "Sand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Betrachtet' es genau, und fand", "tokens": ["Be\u00b7trach\u00b7tet'", "es", "ge\u00b7nau", ",", "und", "fand"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD", "$,", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den Unterschied, da\u00df er nicht mancherley,", "tokens": ["Den", "Un\u00b7ter\u00b7schied", ",", "da\u00df", "er", "nicht", "man\u00b7cher\u00b7ley", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "PTKNEG", "PIS", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Nein, in der That unz\u00e4hlig sey.", "tokens": ["Nein", ",", "in", "der", "That", "un\u00b7z\u00e4h\u00b7lig", "sey", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich konnte tausend Form- und Ecken", "tokens": ["Ich", "konn\u00b7te", "tau\u00b7send", "For\u00b7m", "und", "E\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "CARD", "TRUNC", "KON", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Auch an dem kleinsten Sand entdecken.", "tokens": ["Auch", "an", "dem", "kleins\u00b7ten", "Sand", "ent\u00b7de\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Theils sind die K\u00f6rner lang, theils rund, theils gro\u00df, theils klein,", "tokens": ["Theils", "sind", "die", "K\u00f6r\u00b7ner", "lang", ",", "theils", "rund", ",", "theils", "gro\u00df", ",", "theils", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Theils schwartz, theils braun, theils gelb, theils grau,", "tokens": ["Theils", "schwartz", ",", "theils", "braun", ",", "theils", "gelb", ",", "theils", "grau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Theils r\u00f6thlich, wei\u00dflich theils, theils blau,", "tokens": ["Theils", "r\u00f6th\u00b7lich", ",", "wei\u00df\u00b7lich", "theils", ",", "theils", "blau", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "ADJD", "ADV", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Es sind die meisten dicht und dunckel, viele helle,", "tokens": ["Es", "sind", "die", "meis\u00b7ten", "dicht", "und", "dun\u00b7ckel", ",", "vie\u00b7le", "hel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "VVFIN", "ADJD", "KON", "ADJD", "$,", "PIAT", "ADJA", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Durchsichtig, gl\u00e4ntzend, rein.", "tokens": ["Durch\u00b7sich\u00b7tig", ",", "gl\u00e4nt\u00b7zend", ",", "rein", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.12": {"text": "Ich wurd' auf mancher Stelle", "tokens": ["Ich", "wurd'", "auf", "man\u00b7cher", "Stel\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.13": {"text": "Verschiedener, die, wie Krystall so klar,", "tokens": ["Ver\u00b7schie\u00b7de\u00b7ner", ",", "die", ",", "wie", "Krys\u00b7tall", "so", "klar", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "$,", "PWAV", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}, "line.14": {"text": "Mit Lust und mit Verwunderung gewahr.", "tokens": ["Mit", "Lust", "und", "mit", "Ver\u00b7wun\u00b7de\u00b7rung", "ge\u00b7wahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Indem ich nun die Kleinheit \u00fcbersehe,", "tokens": ["In\u00b7dem", "ich", "nun", "die", "Klein\u00b7heit", "\u00fc\u00b7ber\u00b7se\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und alles dieses \u00fcberlege;", "tokens": ["Und", "al\u00b7les", "die\u00b7ses", "\u00fc\u00b7berl\u00b7e\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PDAT", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Erstaun' ich, wenn ich recht erwege,", "tokens": ["Er\u00b7staun'", "ich", ",", "wenn", "ich", "recht", "er\u00b7we\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df alle Gr\u00f6sse dieser Welt,", "tokens": ["Da\u00df", "al\u00b7le", "Gr\u00f6s\u00b7se", "die\u00b7ser", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ja selbst die Welt aus Kleinigkeiten nur,", "tokens": ["Ja", "selbst", "die", "Welt", "aus", "Klei\u00b7nig\u00b7kei\u00b7ten", "nur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "ART", "NN", "APPR", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Wie gro\u00df sie uns auch scheint und wircklich ist, bestehe.", "tokens": ["Wie", "gro\u00df", "sie", "uns", "auch", "scheint", "und", "wir\u00b7ck\u00b7lich", "ist", ",", "be\u00b7ste\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "PPER", "ADV", "VVFIN", "KON", "ADJD", "VAFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "Es fiel mir ferner bey,", "tokens": ["Es", "fiel", "mir", "fer\u00b7ner", "bey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Wie Kleinigkeiten fast in allen Sachen", "tokens": ["Wie", "Klei\u00b7nig\u00b7kei\u00b7ten", "fast", "in", "al\u00b7len", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Besondere Ver\u00e4nderungen machen.", "tokens": ["Be\u00b7son\u00b7de\u00b7re", "Ver\u00b7\u00e4n\u00b7de\u00b7run\u00b7gen", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Was ist die sch\u00f6ne Kunst der edlen Mahlerey,", "tokens": ["Was", "ist", "die", "sch\u00f6\u00b7ne", "Kunst", "der", "ed\u00b7len", "Mah\u00b7le\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Die guten Theils aus Farben nur bestehet,", "tokens": ["Die", "gu\u00b7ten", "Theils", "aus", "Far\u00b7ben", "nur", "be\u00b7ste\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Und diese wiederum aus Sand und Erden?", "tokens": ["Und", "die\u00b7se", "wie\u00b7de\u00b7rum", "aus", "Sand", "und", "Er\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Wodurch jedoch die sch\u00f6nsten Bilder werden.", "tokens": ["Wo\u00b7durch", "je\u00b7doch", "die", "sch\u00f6ns\u00b7ten", "Bil\u00b7der", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.14": {"text": "Denn das, was unser Aug' erfrischt", "tokens": ["Denn", "das", ",", "was", "un\u00b7ser", "Aug'", "er\u00b7frischt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PDS", "$,", "PRELS", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Auf solche wundersame Art,", "tokens": ["Auf", "sol\u00b7che", "wun\u00b7der\u00b7sa\u00b7me", "Art", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Ist blo\u00df ein wenig Sand mit Oel gemischt,", "tokens": ["Ist", "blo\u00df", "ein", "we\u00b7nig", "Sand", "mit", "O\u00b7el", "ge\u00b7mischt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "PIAT", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+----+", "measure": "unknown.measure.tetra"}, "line.17": {"text": "Ist so unglaublich d\u00fcnn und zart,", "tokens": ["Ist", "so", "un\u00b7glaub\u00b7lich", "d\u00fcnn", "und", "zart", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.18": {"text": "Da\u00df, wenn man es vom Tuche trennen wollte,", "tokens": ["Da\u00df", ",", "wenn", "man", "es", "vom", "Tu\u00b7che", "tren\u00b7nen", "woll\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "KOUS", "PIS", "PPER", "APPRART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "Man es f\u00fcr c\u00f6rperlich kaum halten sollte.", "tokens": ["Man", "es", "f\u00fcr", "c\u00f6r\u00b7per\u00b7lich", "kaum", "hal\u00b7ten", "soll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "APPR", "ADJD", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Noch mehr, wie wunderbar", "tokens": ["Noch", "mehr", ",", "wie", "wun\u00b7der\u00b7bar"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "$,", "PWAV", "ADJD"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Erhellt im Sande Gottes Macht,", "tokens": ["Er\u00b7hellt", "im", "San\u00b7de", "Got\u00b7tes", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der alles nicht allein aus Nichts hervor gebracht;", "tokens": ["Der", "al\u00b7les", "nicht", "al\u00b7lein", "aus", "Nichts", "her\u00b7vor", "ge\u00b7bracht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PTKNEG", "ADV", "APPR", "PIS", "PTKVZ", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der auch so gar", "tokens": ["Der", "auch", "so", "gar"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Durch solche Kleinigkeit das allergr\u00f6\u00dfte zwinget,", "tokens": ["Durch", "sol\u00b7che", "Klei\u00b7nig\u00b7keit", "das", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7te", "zwin\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "ADJA", "VVFIN", "$,"], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.6": {"text": "Indem Er, durch so kleinen Sand,", "tokens": ["In\u00b7dem", "Er", ",", "durch", "so", "klei\u00b7nen", "Sand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Die ungeheure Fluthen-Last", "tokens": ["Die", "un\u00b7ge\u00b7heu\u00b7re", "Flu\u00b7then\u00b7Last"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So wunderbarlich eingefasst,", "tokens": ["So", "wun\u00b7der\u00b7bar\u00b7lich", "ein\u00b7ge\u00b7fasst", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Da\u00df aller Wellen Wuth nicht durch ihn dringet.", "tokens": ["Da\u00df", "al\u00b7ler", "Wel\u00b7len", "Wuth", "nicht", "durch", "ihn", "drin\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "NN", "PTKNEG", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Hiemit stimmt alles \u00fcberein,", "tokens": ["Hie\u00b7mit", "stimmt", "al\u00b7les", "\u00fc\u00b7be\u00b7re\u00b7in", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PTKVZ", "$,"], "meter": "---+-+--+", "measure": "iambic.tri.chol"}, "line.11": {"text": "Da\u00df, wir f\u00fcr uns das allerkleinste gro\u00df,", "tokens": ["Da\u00df", ",", "wir", "f\u00fcr", "uns", "das", "al\u00b7ler\u00b7kleins\u00b7te", "gro\u00df", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PPER", "APPR", "PPER", "ART", "ADJA", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Also f\u00fcr GOTT das allergr\u00f6\u00dfte klein,", "tokens": ["Al\u00b7so", "f\u00fcr", "GoTT", "das", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7te", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "ART", "ADJA", "ADJD", "$,"], "meter": "---+-+-+-+", "measure": "zehnsilber"}, "line.13": {"text": "Daher denn David auch recht unvergleichlich schlo\u00df:", "tokens": ["Da\u00b7her", "denn", "Da\u00b7vid", "auch", "recht", "un\u00b7ver\u00b7gleich\u00b7lich", "schlo\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "KON", "NE", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Wie das Z\u00fcnglein an der Wage, so ist, Herr, vor Dir die Welt;", "tokens": ["Wie", "das", "Z\u00fcn\u00b7glein", "an", "der", "Wa\u00b7ge", ",", "so", "ist", ",", "Herr", ",", "vor", "Dir", "die", "Welt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "ART", "NN", "$,", "ADV", "VAFIN", "$,", "NN", "$,", "APPR", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.2": {"text": "Wie der Tropfen aus dem Eimer, welcher auf die Erde f\u00e4llt.", "tokens": ["Wie", "der", "Trop\u00b7fen", "aus", "dem", "Ei\u00b7mer", ",", "wel\u00b7cher", "auf", "die", "Er\u00b7de", "f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "ART", "NN", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}}}}}