{"textgrid.poem.57041": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Die Beh\u00f6rde", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Korf erh\u00e4lt vom Polizeib\u00fcro", "tokens": ["Korf", "er\u00b7h\u00e4lt", "vom", "Po\u00b7li\u00b7zei\u00b7b\u00fc\u00b7ro"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "ein geharnischt Formular,", "tokens": ["ein", "ge\u00b7har\u00b7nischt", "For\u00b7mu\u00b7lar", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wer er sei und wie und wo,", "tokens": ["wer", "er", "sei", "und", "wie", "und", "wo", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "KON", "PWAV", "KON", "PWAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "welchen Orts er bis anheute war,", "tokens": ["wel\u00b7chen", "Orts", "er", "bis", "an\u00b7heu\u00b7te", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "PPER", "ADV", "ADV", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "welchen Stands und \u00fcberhaupt,", "tokens": ["wel\u00b7chen", "Stands", "und", "\u00fc\u00b7ber\u00b7haupt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "KON", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wo geboren, Tag und Jahr.", "tokens": ["wo", "ge\u00b7bo\u00b7ren", ",", "Tag", "und", "Jahr", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Ob ihm \u00fcberhaupt erlaubt,", "tokens": ["Ob", "ihm", "\u00fc\u00b7ber\u00b7haupt", "er\u00b7laubt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "hier zu leben und zu welchem Zweck,", "tokens": ["hier", "zu", "le\u00b7ben", "und", "zu", "wel\u00b7chem", "Zweck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "KON", "APPR", "PWAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "wieviel Geld er hat und was er glaubt.", "tokens": ["wie\u00b7viel", "Geld", "er", "hat", "und", "was", "er", "glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPER", "VAFIN", "KON", "PWS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Umgekehrten Falls man ihn vom Fleck", "tokens": ["Um\u00b7ge\u00b7kehr\u00b7ten", "Falls", "man", "ihn", "vom", "Fleck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "KOUS", "PIS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "in Arrest verf\u00fchren w\u00fcrde, und", "tokens": ["in", "Ar\u00b7rest", "ver\u00b7f\u00fch\u00b7ren", "w\u00fcr\u00b7de", ",", "und"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "NN", "VVINF", "VAFIN", "$,", "KON"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "drunter steht: Borowsky, Heck.", "tokens": ["drun\u00b7ter", "steht", ":", "Bo\u00b7rows\u00b7ky", ",", "Heck", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PAV", "VVFIN", "$.", "NE", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Korf erwidert darauf kurz und rund:", "tokens": ["Korf", "er\u00b7wi\u00b7dert", "da\u00b7rauf", "kurz", "und", "rund", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PAV", "ADJD", "KON", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "\u00bbeiner hohen Direktion", "tokens": ["\u00bb", "ei\u00b7ner", "ho\u00b7hen", "Di\u00b7rek\u00b7ti\u00b7on"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "stellt sich, laut pers\u00f6nlichem Befund,", "tokens": ["stellt", "sich", ",", "laut", "per\u00b7s\u00f6n\u00b7li\u00b7chem", "Be\u00b7fund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "untig angefertigte Person", "tokens": ["un\u00b7tig", "an\u00b7ge\u00b7fer\u00b7tig\u00b7te", "Per\u00b7son"], "token_info": ["word", "word", "word"], "pos": ["ADJD", "ADJA", "NN"], "meter": "+-+-+-+++", "measure": "unknown.measure.hexa"}, "line.2": {"text": "als nichtexistent im Eigen-Sinn", "tokens": ["als", "nicht\u00b7e\u00b7xis\u00b7tent", "im", "Ei\u00b7gen\u00b7Sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "APPRART", "NN"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "b\u00fcrgerlicher Konvention", "tokens": ["b\u00fcr\u00b7ger\u00b7li\u00b7cher", "Kon\u00b7ven\u00b7ti\u00b7on"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.7": {"line.1": {"text": "vor und aus und zeichnet, wennschonhin", "tokens": ["vor", "und", "aus", "und", "zeich\u00b7net", ",", "wenn\u00b7schon\u00b7hin"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKVZ", "KON", "PTKVZ", "KON", "VVFIN", "$,", "PWAV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "mitbedauernd nebigen Betreff,", "tokens": ["mit\u00b7be\u00b7dau\u00b7ernd", "ne\u00b7bi\u00b7gen", "Be\u00b7treff", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "NN", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Staunend liest's der anbetroffne Chef.", "tokens": ["Stau\u00b7nend", "liest's", "der", "an\u00b7be\u00b7troff\u00b7ne", "Chef", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "Korf erh\u00e4lt vom Polizeib\u00fcro", "tokens": ["Korf", "er\u00b7h\u00e4lt", "vom", "Po\u00b7li\u00b7zei\u00b7b\u00fc\u00b7ro"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "ein geharnischt Formular,", "tokens": ["ein", "ge\u00b7har\u00b7nischt", "For\u00b7mu\u00b7lar", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wer er sei und wie und wo,", "tokens": ["wer", "er", "sei", "und", "wie", "und", "wo", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "KON", "PWAV", "KON", "PWAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "welchen Orts er bis anheute war,", "tokens": ["wel\u00b7chen", "Orts", "er", "bis", "an\u00b7heu\u00b7te", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "PPER", "ADV", "ADV", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "welchen Stands und \u00fcberhaupt,", "tokens": ["wel\u00b7chen", "Stands", "und", "\u00fc\u00b7ber\u00b7haupt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "KON", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wo geboren, Tag und Jahr.", "tokens": ["wo", "ge\u00b7bo\u00b7ren", ",", "Tag", "und", "Jahr", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Ob ihm \u00fcberhaupt erlaubt,", "tokens": ["Ob", "ihm", "\u00fc\u00b7ber\u00b7haupt", "er\u00b7laubt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "hier zu leben und zu welchem Zweck,", "tokens": ["hier", "zu", "le\u00b7ben", "und", "zu", "wel\u00b7chem", "Zweck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "KON", "APPR", "PWAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "wieviel Geld er hat und was er glaubt.", "tokens": ["wie\u00b7viel", "Geld", "er", "hat", "und", "was", "er", "glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPER", "VAFIN", "KON", "PWS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.12": {"line.1": {"text": "Umgekehrten Falls man ihn vom Fleck", "tokens": ["Um\u00b7ge\u00b7kehr\u00b7ten", "Falls", "man", "ihn", "vom", "Fleck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "KOUS", "PIS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "in Arrest verf\u00fchren w\u00fcrde, und", "tokens": ["in", "Ar\u00b7rest", "ver\u00b7f\u00fch\u00b7ren", "w\u00fcr\u00b7de", ",", "und"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "NN", "VVINF", "VAFIN", "$,", "KON"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "drunter steht: Borowsky, Heck.", "tokens": ["drun\u00b7ter", "steht", ":", "Bo\u00b7rows\u00b7ky", ",", "Heck", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PAV", "VVFIN", "$.", "NE", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Korf erwidert darauf kurz und rund:", "tokens": ["Korf", "er\u00b7wi\u00b7dert", "da\u00b7rauf", "kurz", "und", "rund", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PAV", "ADJD", "KON", "ADJD", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "\u00bbeiner hohen Direktion", "tokens": ["\u00bb", "ei\u00b7ner", "ho\u00b7hen", "Di\u00b7rek\u00b7ti\u00b7on"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "stellt sich, laut pers\u00f6nlichem Befund,", "tokens": ["stellt", "sich", ",", "laut", "per\u00b7s\u00f6n\u00b7li\u00b7chem", "Be\u00b7fund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.14": {"line.1": {"text": "untig angefertigte Person", "tokens": ["un\u00b7tig", "an\u00b7ge\u00b7fer\u00b7tig\u00b7te", "Per\u00b7son"], "token_info": ["word", "word", "word"], "pos": ["ADJD", "ADJA", "NN"], "meter": "+-+-+-+++", "measure": "unknown.measure.hexa"}, "line.2": {"text": "als nichtexistent im Eigen-Sinn", "tokens": ["als", "nicht\u00b7e\u00b7xis\u00b7tent", "im", "Ei\u00b7gen\u00b7Sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "APPRART", "NN"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "b\u00fcrgerlicher Konvention", "tokens": ["b\u00fcr\u00b7ger\u00b7li\u00b7cher", "Kon\u00b7ven\u00b7ti\u00b7on"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.15": {"line.1": {"text": "vor und aus und zeichnet, wennschonhin", "tokens": ["vor", "und", "aus", "und", "zeich\u00b7net", ",", "wenn\u00b7schon\u00b7hin"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKVZ", "KON", "PTKVZ", "KON", "VVFIN", "$,", "PWAV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "mitbedauernd nebigen Betreff,", "tokens": ["mit\u00b7be\u00b7dau\u00b7ernd", "ne\u00b7bi\u00b7gen", "Be\u00b7treff", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "NN", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "Staunend liest's der anbetroffne Chef.", "tokens": ["Stau\u00b7nend", "liest's", "der", "an\u00b7be\u00b7troff\u00b7ne", "Chef", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}}}}