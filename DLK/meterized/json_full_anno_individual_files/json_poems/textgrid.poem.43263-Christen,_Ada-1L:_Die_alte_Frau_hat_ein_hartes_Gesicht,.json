{"textgrid.poem.43263": {"metadata": {"author": {"name": "Christen, Ada", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die alte Frau hat ein hartes Gesicht,", "genre": "verse", "period": "N.A.", "pub_year": 1870, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die alte Frau hat ein hartes Gesicht,", "tokens": ["Die", "al\u00b7te", "Frau", "hat", "ein", "har\u00b7tes", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Doch kluge sanfte Augen,", "tokens": ["Doch", "klu\u00b7ge", "sanf\u00b7te", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die wenig mehr beim Pfenniglicht", "tokens": ["Die", "we\u00b7nig", "mehr", "beim", "Pfen\u00b7nig\u00b7licht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und nicht zum Weinen taugen.", "tokens": ["Und", "nicht", "zum", "Wei\u00b7nen", "tau\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Sie war ein Balg ... Als Findelkind", "tokens": ["Sie", "war", "ein", "Balg", "...", "Als", "Fin\u00b7del\u00b7kind"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "KOUS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verla\u00dfner als die Armen,", "tokens": ["Ver\u00b7la\u00df\u00b7ner", "als", "die", "Ar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bat weder Herren noch Gesind", "tokens": ["Bat", "we\u00b7der", "Her\u00b7ren", "noch", "Ge\u00b7sind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "NN", "ADV", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Um Futter und Erbarmen.", "tokens": ["Um", "Fut\u00b7ter", "und", "Er\u00b7bar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Sie griff fest zu und schaffte stramm", "tokens": ["Sie", "griff", "fest", "zu", "und", "schaff\u00b7te", "stramm"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "KON", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie ehrbar-ernste Leute,", "tokens": ["Wie", "ehr\u00b7ba\u00b7rerns\u00b7te", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df nie sie Unverdientes nahm", "tokens": ["Da\u00df", "nie", "sie", "Un\u00b7ver\u00b7dien\u00b7tes", "nahm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPER", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erfreut das Weib noch heute.", "tokens": ["Er\u00b7freut", "das", "Weib", "noch", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Sie zeigt auch jetzt mit Bauernstolz", "tokens": ["Sie", "zeigt", "auch", "jetzt", "mit", "Bau\u00b7ern\u00b7stolz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erdarbte Thalerscheine:", "tokens": ["Erd\u00b7arb\u00b7te", "Tha\u00b7ler\u00b7schei\u00b7ne", ":"], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "\u00bbdie sind mein ", "tokens": ["\u00bb", "die", "sind", "mein"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "PDS", "VAFIN", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Meine ", "tokens": ["Mei\u00b7ne"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.5": {"line.1": {"text": "Die sind mein ", "tokens": ["Die", "sind", "mein"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VAFIN", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Auf jedem steht geschrieben:", "tokens": ["Auf", "je\u00b7dem", "steht", "ge\u00b7schrie\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein Alter ohne Schand' und Noth ...", "tokens": ["Ein", "Al\u00b7ter", "oh\u00b7ne", "Schand'", "und", "Noth", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und was mir Gott schuldig geblieben.\u00ab", "tokens": ["Und", "was", "mir", "Gott", "schul\u00b7dig", "ge\u00b7blie\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWS", "PPER", "NN", "ADJD", "VVPP", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.6": {"line.1": {"text": "Die alte Frau hat ein hartes Gesicht,", "tokens": ["Die", "al\u00b7te", "Frau", "hat", "ein", "har\u00b7tes", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Doch kluge sanfte Augen,", "tokens": ["Doch", "klu\u00b7ge", "sanf\u00b7te", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die wenig mehr beim Pfenniglicht", "tokens": ["Die", "we\u00b7nig", "mehr", "beim", "Pfen\u00b7nig\u00b7licht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und nicht zum Weinen taugen.", "tokens": ["Und", "nicht", "zum", "Wei\u00b7nen", "tau\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Sie war ein Balg ... Als Findelkind", "tokens": ["Sie", "war", "ein", "Balg", "...", "Als", "Fin\u00b7del\u00b7kind"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "KOUS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verla\u00dfner als die Armen,", "tokens": ["Ver\u00b7la\u00df\u00b7ner", "als", "die", "Ar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bat weder Herren noch Gesind", "tokens": ["Bat", "we\u00b7der", "Her\u00b7ren", "noch", "Ge\u00b7sind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "NN", "ADV", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Um Futter und Erbarmen.", "tokens": ["Um", "Fut\u00b7ter", "und", "Er\u00b7bar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Sie griff fest zu und schaffte stramm", "tokens": ["Sie", "griff", "fest", "zu", "und", "schaff\u00b7te", "stramm"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "KON", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie ehrbar-ernste Leute,", "tokens": ["Wie", "ehr\u00b7ba\u00b7rerns\u00b7te", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df nie sie Unverdientes nahm", "tokens": ["Da\u00df", "nie", "sie", "Un\u00b7ver\u00b7dien\u00b7tes", "nahm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPER", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erfreut das Weib noch heute.", "tokens": ["Er\u00b7freut", "das", "Weib", "noch", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Sie zeigt auch jetzt mit Bauernstolz", "tokens": ["Sie", "zeigt", "auch", "jetzt", "mit", "Bau\u00b7ern\u00b7stolz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erdarbte Thalerscheine:", "tokens": ["Erd\u00b7arb\u00b7te", "Tha\u00b7ler\u00b7schei\u00b7ne", ":"], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "\u00bbdie sind mein ", "tokens": ["\u00bb", "die", "sind", "mein"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "PDS", "VAFIN", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Meine ", "tokens": ["Mei\u00b7ne"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.10": {"line.1": {"text": "Die sind mein ", "tokens": ["Die", "sind", "mein"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VAFIN", "PPOSAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Auf jedem steht geschrieben:", "tokens": ["Auf", "je\u00b7dem", "steht", "ge\u00b7schrie\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein Alter ohne Schand' und Noth ...", "tokens": ["Ein", "Al\u00b7ter", "oh\u00b7ne", "Schand'", "und", "Noth", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und was mir Gott schuldig geblieben.\u00ab", "tokens": ["Und", "was", "mir", "Gott", "schul\u00b7dig", "ge\u00b7blie\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWS", "PPER", "NN", "ADJD", "VVPP", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}}}}