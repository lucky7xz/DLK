{"textgrid.poem.40204": {"metadata": {"author": {"name": "Dehmel, Richard Fedor Leopold", "birth": "N.A.", "death": "N.A."}, "title": "Scheinkunst", "genre": "verse", "period": "N.A.", "pub_year": 1891, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Bild und Nichtbild: Bild der ", "tokens": ["Bild", "und", "Nicht\u00b7bild", ":", "Bild", "der"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "NN", "$.", "NN", "ART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Bild des ", "tokens": ["Bild", "des"], "token_info": ["word", "word"], "pos": ["NN", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "und die Wahrheit wird zur L\u00fcge,", "tokens": ["und", "die", "Wahr\u00b7heit", "wird", "zur", "L\u00fc\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "weil sie nicht von Herzen spricht!", "tokens": ["weil", "sie", "nicht", "von", "Her\u00b7zen", "spricht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Aengstlich haften bleibt sie Oben:", "tokens": ["A\u00b7engst\u00b7lich", "haf\u00b7ten", "bleibt", "sie", "O\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Punkt f\u00fcr Punkt und Strich um Strich,", "tokens": ["Punkt", "f\u00fcr", "Punkt", "und", "Strich", "um", "Strich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "APPR", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Alles einzeln ist zu loben,", "tokens": ["Al\u00b7les", "ein\u00b7zeln", "ist", "zu", "lo\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VAFIN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "aber nie vereint es sich!", "tokens": ["a\u00b7ber", "nie", "ver\u00b7eint", "es", "sich", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Und so h\u00f6re ich die kalten", "tokens": ["Und", "so", "h\u00f6\u00b7re", "ich", "die", "kal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "hohlen d\u00fcrren Worte nur,", "tokens": ["hoh\u00b7len", "d\u00fcr\u00b7ren", "Wor\u00b7te", "nur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "merke die Berechnung schalten:", "tokens": ["mer\u00b7ke", "die", "Be\u00b7rech\u00b7nung", "schal\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "wahren Wesens keine Spur!", "tokens": ["wah\u00b7ren", "We\u00b7sens", "kei\u00b7ne", "Spur", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Schulgerecht nach neuern Meistern", "tokens": ["Schul\u00b7ge\u00b7recht", "nach", "neu\u00b7ern", "Meis\u00b7tern"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wahrheit nach dem A B C \u2013", "tokens": ["Wahr\u00b7heit", "nach", "dem", "A", "B", "C", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "XY", "XY", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "ach, ihr thut mir leid und weh!", "tokens": ["ach", ",", "ihr", "thut", "mir", "leid", "und", "weh", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["XY", "$,", "PPER", "VVFIN", "PPER", "ADJD", "KON", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Bild und Nichtbild: Bild der ", "tokens": ["Bild", "und", "Nicht\u00b7bild", ":", "Bild", "der"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "NN", "$.", "NN", "ART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Bild des ", "tokens": ["Bild", "des"], "token_info": ["word", "word"], "pos": ["NN", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "und die Wahrheit wird zur L\u00fcge,", "tokens": ["und", "die", "Wahr\u00b7heit", "wird", "zur", "L\u00fc\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "weil sie nicht von Herzen spricht!", "tokens": ["weil", "sie", "nicht", "von", "Her\u00b7zen", "spricht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Aengstlich haften bleibt sie Oben:", "tokens": ["A\u00b7engst\u00b7lich", "haf\u00b7ten", "bleibt", "sie", "O\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Punkt f\u00fcr Punkt und Strich um Strich,", "tokens": ["Punkt", "f\u00fcr", "Punkt", "und", "Strich", "um", "Strich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "APPR", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Alles einzeln ist zu loben,", "tokens": ["Al\u00b7les", "ein\u00b7zeln", "ist", "zu", "lo\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VAFIN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "aber nie vereint es sich!", "tokens": ["a\u00b7ber", "nie", "ver\u00b7eint", "es", "sich", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Und so h\u00f6re ich die kalten", "tokens": ["Und", "so", "h\u00f6\u00b7re", "ich", "die", "kal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "hohlen d\u00fcrren Worte nur,", "tokens": ["hoh\u00b7len", "d\u00fcr\u00b7ren", "Wor\u00b7te", "nur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "merke die Berechnung schalten:", "tokens": ["mer\u00b7ke", "die", "Be\u00b7rech\u00b7nung", "schal\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "wahren Wesens keine Spur!", "tokens": ["wah\u00b7ren", "We\u00b7sens", "kei\u00b7ne", "Spur", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Schulgerecht nach neuern Meistern", "tokens": ["Schul\u00b7ge\u00b7recht", "nach", "neu\u00b7ern", "Meis\u00b7tern"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wahrheit nach dem A B C \u2013", "tokens": ["Wahr\u00b7heit", "nach", "dem", "A", "B", "C", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "XY", "XY", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "ach, ihr thut mir leid und weh!", "tokens": ["ach", ",", "ihr", "thut", "mir", "leid", "und", "weh", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["XY", "$,", "PPER", "VVFIN", "PPER", "ADJD", "KON", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}