{"textgrid.poem.60629": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Kater, Nagespeck genannt,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Kater, Nagespeck genannt,", "tokens": ["Ein", "Ka\u00b7ter", ",", "Na\u00b7ge\u00b7speck", "ge\u00b7nannt", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NE", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bereitete den Ratten solche Niederlagen,", "tokens": ["Be\u00b7rei\u00b7te\u00b7te", "den", "Rat\u00b7ten", "sol\u00b7che", "Nie\u00b7der\u00b7la\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df t\u00e4glich kleiner wurde ihr Bestand.", "tokens": ["Da\u00df", "t\u00e4g\u00b7lich", "klei\u00b7ner", "wur\u00b7de", "ihr", "Be\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Er hatte Hunderte schon in den Tod gesandt.", "tokens": ["Er", "hat\u00b7te", "Hun\u00b7der\u00b7te", "schon", "in", "den", "Tod", "ge\u00b7sandt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Von den Verbliebnen wagte keine ohne Zagen", "tokens": ["Von", "den", "Ver\u00b7blieb\u00b7nen", "wag\u00b7te", "kei\u00b7ne", "oh\u00b7ne", "Za\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "PIAT", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Heraus sich aus dem Loch,", "tokens": ["He\u00b7raus", "sich", "aus", "dem", "Loch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Und Schmalhans wurde Koch", "tokens": ["Und", "Schmal\u00b7hans", "wur\u00b7de", "Koch"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Bei dieser ungl\u00fcckseligen Rattenschar,", "tokens": ["Bei", "die\u00b7ser", "un\u00b7gl\u00fcck\u00b7se\u00b7li\u00b7gen", "Rat\u00b7ten\u00b7schar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Nach deren Meinung Nagespeck", "tokens": ["Nach", "de\u00b7ren", "Mei\u00b7nung", "Na\u00b7ge\u00b7speck"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELAT", "NN", "NE"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Kein Kater, nein, ein Teufel war!", "tokens": ["Kein", "Ka\u00b7ter", ",", "nein", ",", "ein", "Teu\u00b7fel", "war", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PTKANT", "$,", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Als eines Tags der H\u00f6llenschreck", "tokens": ["Als", "ei\u00b7nes", "Tags", "der", "H\u00f6l\u00b7len\u00b7schreck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Hoch auf die D\u00e4cher stieg als Freier", "tokens": ["Hoch", "auf", "die", "D\u00e4\u00b7cher", "stieg", "als", "Frei\u00b7er"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ART", "NN", "VVFIN", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Und weilte bei der Hochzeitsfeier,", "tokens": ["Und", "weil\u00b7te", "bei", "der", "Hoch\u00b7zeits\u00b7fei\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Hielt ein Kapitel ab der Ratten \u00dcberrest,", "tokens": ["Hielt", "ein", "Ka\u00b7pi\u00b7tel", "ab", "der", "Rat\u00b7ten", "\u00dc\u00b7ber\u00b7rest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Um zu beraten \u00fcber ihre schwere Not.", "tokens": ["Um", "zu", "be\u00b7ra\u00b7ten", "\u00fc\u00b7ber", "ih\u00b7re", "schwe\u00b7re", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Da meinte der Dekan, ein kluger Mann: \u00bbEs l\u00e4\u00dft", "tokens": ["Da", "mein\u00b7te", "der", "De\u00b7kan", ",", "ein", "klu\u00b7ger", "Mann", ":", "\u00bb", "Es", "l\u00e4\u00dft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "ART", "ADJA", "NN", "$.", "$(", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Sich wohl bek\u00e4mpfen, was uns droht.", "tokens": ["Sich", "wohl", "be\u00b7k\u00e4mp\u00b7fen", ",", "was", "uns", "droht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVINF", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Wir brauchen nur zu sorgen,", "tokens": ["Wir", "brau\u00b7chen", "nur", "zu", "sor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.19": {"text": "Da\u00df lieber heut als morgen", "tokens": ["Da\u00df", "lie\u00b7ber", "heut", "als", "mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "KOKOM", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.20": {"text": "Der Feind ein Gl\u00f6ckchen um den Hals erh\u00e4lt,", "tokens": ["Der", "Feind", "ein", "Gl\u00f6ck\u00b7chen", "um", "den", "Hals", "er\u00b7h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.21": {"text": "Das warnend schellt,", "tokens": ["Das", "war\u00b7nend", "schellt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVPP", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.22": {"text": "Sobald er naht:", "tokens": ["So\u00b7bald", "er", "naht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.23": {"text": "Wir fl\u00fcchten schnell \u2013 er kommt zu spat!\u00ab", "tokens": ["Wir", "fl\u00fcch\u00b7ten", "schnell", "\u2013", "er", "kommt", "zu", "spat", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$(", "PPER", "VVFIN", "PTKA", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.24": {"text": "Dies Mittel, sprach er, sei", "tokens": ["Dies", "Mit\u00b7tel", ",", "sprach", "er", ",", "sei"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PDS", "NN", "$,", "VVFIN", "PPER", "$,", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.25": {"text": "Das einz'ge, das er wisse.", "tokens": ["Das", "einz'\u00b7ge", ",", "das", "er", "wis\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.26": {"text": "Ein jeder stimmte bei,", "tokens": ["Ein", "je\u00b7der", "stimm\u00b7te", "bei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.27": {"text": "Da\u00df man das Gl\u00f6ckchen h\u00e4ngen m\u00fcsse.", "tokens": ["Da\u00df", "man", "das", "Gl\u00f6ck\u00b7chen", "h\u00e4n\u00b7gen", "m\u00fcs\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.28": {"text": "Doch wer? \u2013 Und wie? \u2013 Die armen Ratten sahn sich um.", "tokens": ["Doch", "wer", "?", "\u2013", "Und", "wie", "?", "\u2013", "Die", "ar\u00b7men", "Rat\u00b7ten", "sahn", "sich", "um", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "$.", "$(", "KON", "PWAV", "$.", "$(", "ART", "ADJA", "NN", "VVFIN", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Die eine meinte: \u00bbIch bin nicht so dumm.\u00ab", "tokens": ["Die", "ei\u00b7ne", "mein\u00b7te", ":", "\u00bb", "Ich", "bin", "nicht", "so", "dumm", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PTKNEG", "ADV", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Die andre sprach: \u00bbIch werd es nicht verstehn.\u00ab", "tokens": ["Die", "and\u00b7re", "sprach", ":", "\u00bb", "Ich", "werd", "es", "nicht", "ver\u00b7stehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PPER", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.31": {"text": "Kurzum, so trefflich auch der Rat,", "tokens": ["Kur\u00b7zum", ",", "so", "treff\u00b7lich", "auch", "der", "Rat", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ADV", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.32": {"text": "Man kam zu keiner Tat.", "tokens": ["Man", "kam", "zu", "kei\u00b7ner", "Tat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Ich habe viel Kapitel schon gesehn,", "tokens": ["Ich", "ha\u00b7be", "viel", "Ka\u00b7pi\u00b7tel", "schon", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die nichts als viele Worte hatten:", "tokens": ["Die", "nichts", "als", "vie\u00b7le", "Wor\u00b7te", "hat\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "KOKOM", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kapitel, nicht von Ratten,", "tokens": ["Ka\u00b7pi\u00b7tel", ",", "nicht", "von", "Rat\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nein, von der M\u00f6nche tapfrer Schar,", "tokens": ["Nein", ",", "von", "der", "M\u00f6n\u00b7che", "tapf\u00b7rer", "Schar", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Von Domherren sogar!", "tokens": ["Von", "Dom\u00b7her\u00b7ren", "so\u00b7gar", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Ein Kater, Nagespeck genannt,", "tokens": ["Ein", "Ka\u00b7ter", ",", "Na\u00b7ge\u00b7speck", "ge\u00b7nannt", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NE", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bereitete den Ratten solche Niederlagen,", "tokens": ["Be\u00b7rei\u00b7te\u00b7te", "den", "Rat\u00b7ten", "sol\u00b7che", "Nie\u00b7der\u00b7la\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df t\u00e4glich kleiner wurde ihr Bestand.", "tokens": ["Da\u00df", "t\u00e4g\u00b7lich", "klei\u00b7ner", "wur\u00b7de", "ihr", "Be\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Er hatte Hunderte schon in den Tod gesandt.", "tokens": ["Er", "hat\u00b7te", "Hun\u00b7der\u00b7te", "schon", "in", "den", "Tod", "ge\u00b7sandt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Von den Verbliebnen wagte keine ohne Zagen", "tokens": ["Von", "den", "Ver\u00b7blieb\u00b7nen", "wag\u00b7te", "kei\u00b7ne", "oh\u00b7ne", "Za\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "PIAT", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Heraus sich aus dem Loch,", "tokens": ["He\u00b7raus", "sich", "aus", "dem", "Loch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Und Schmalhans wurde Koch", "tokens": ["Und", "Schmal\u00b7hans", "wur\u00b7de", "Koch"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Bei dieser ungl\u00fcckseligen Rattenschar,", "tokens": ["Bei", "die\u00b7ser", "un\u00b7gl\u00fcck\u00b7se\u00b7li\u00b7gen", "Rat\u00b7ten\u00b7schar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Nach deren Meinung Nagespeck", "tokens": ["Nach", "de\u00b7ren", "Mei\u00b7nung", "Na\u00b7ge\u00b7speck"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELAT", "NN", "NE"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Kein Kater, nein, ein Teufel war!", "tokens": ["Kein", "Ka\u00b7ter", ",", "nein", ",", "ein", "Teu\u00b7fel", "war", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PTKANT", "$,", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Als eines Tags der H\u00f6llenschreck", "tokens": ["Als", "ei\u00b7nes", "Tags", "der", "H\u00f6l\u00b7len\u00b7schreck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Hoch auf die D\u00e4cher stieg als Freier", "tokens": ["Hoch", "auf", "die", "D\u00e4\u00b7cher", "stieg", "als", "Frei\u00b7er"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ART", "NN", "VVFIN", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Und weilte bei der Hochzeitsfeier,", "tokens": ["Und", "weil\u00b7te", "bei", "der", "Hoch\u00b7zeits\u00b7fei\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Hielt ein Kapitel ab der Ratten \u00dcberrest,", "tokens": ["Hielt", "ein", "Ka\u00b7pi\u00b7tel", "ab", "der", "Rat\u00b7ten", "\u00dc\u00b7ber\u00b7rest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Um zu beraten \u00fcber ihre schwere Not.", "tokens": ["Um", "zu", "be\u00b7ra\u00b7ten", "\u00fc\u00b7ber", "ih\u00b7re", "schwe\u00b7re", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Da meinte der Dekan, ein kluger Mann: \u00bbEs l\u00e4\u00dft", "tokens": ["Da", "mein\u00b7te", "der", "De\u00b7kan", ",", "ein", "klu\u00b7ger", "Mann", ":", "\u00bb", "Es", "l\u00e4\u00dft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,", "ART", "ADJA", "NN", "$.", "$(", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Sich wohl bek\u00e4mpfen, was uns droht.", "tokens": ["Sich", "wohl", "be\u00b7k\u00e4mp\u00b7fen", ",", "was", "uns", "droht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVINF", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Wir brauchen nur zu sorgen,", "tokens": ["Wir", "brau\u00b7chen", "nur", "zu", "sor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.19": {"text": "Da\u00df lieber heut als morgen", "tokens": ["Da\u00df", "lie\u00b7ber", "heut", "als", "mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "KOKOM", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.20": {"text": "Der Feind ein Gl\u00f6ckchen um den Hals erh\u00e4lt,", "tokens": ["Der", "Feind", "ein", "Gl\u00f6ck\u00b7chen", "um", "den", "Hals", "er\u00b7h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.21": {"text": "Das warnend schellt,", "tokens": ["Das", "war\u00b7nend", "schellt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVPP", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.22": {"text": "Sobald er naht:", "tokens": ["So\u00b7bald", "er", "naht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.23": {"text": "Wir fl\u00fcchten schnell \u2013 er kommt zu spat!\u00ab", "tokens": ["Wir", "fl\u00fcch\u00b7ten", "schnell", "\u2013", "er", "kommt", "zu", "spat", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$(", "PPER", "VVFIN", "PTKA", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.24": {"text": "Dies Mittel, sprach er, sei", "tokens": ["Dies", "Mit\u00b7tel", ",", "sprach", "er", ",", "sei"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PDS", "NN", "$,", "VVFIN", "PPER", "$,", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.25": {"text": "Das einz'ge, das er wisse.", "tokens": ["Das", "einz'\u00b7ge", ",", "das", "er", "wis\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.26": {"text": "Ein jeder stimmte bei,", "tokens": ["Ein", "je\u00b7der", "stimm\u00b7te", "bei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.27": {"text": "Da\u00df man das Gl\u00f6ckchen h\u00e4ngen m\u00fcsse.", "tokens": ["Da\u00df", "man", "das", "Gl\u00f6ck\u00b7chen", "h\u00e4n\u00b7gen", "m\u00fcs\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.28": {"text": "Doch wer? \u2013 Und wie? \u2013 Die armen Ratten sahn sich um.", "tokens": ["Doch", "wer", "?", "\u2013", "Und", "wie", "?", "\u2013", "Die", "ar\u00b7men", "Rat\u00b7ten", "sahn", "sich", "um", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "$.", "$(", "KON", "PWAV", "$.", "$(", "ART", "ADJA", "NN", "VVFIN", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Die eine meinte: \u00bbIch bin nicht so dumm.\u00ab", "tokens": ["Die", "ei\u00b7ne", "mein\u00b7te", ":", "\u00bb", "Ich", "bin", "nicht", "so", "dumm", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PTKNEG", "ADV", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Die andre sprach: \u00bbIch werd es nicht verstehn.\u00ab", "tokens": ["Die", "and\u00b7re", "sprach", ":", "\u00bb", "Ich", "werd", "es", "nicht", "ver\u00b7stehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PPER", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.31": {"text": "Kurzum, so trefflich auch der Rat,", "tokens": ["Kur\u00b7zum", ",", "so", "treff\u00b7lich", "auch", "der", "Rat", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ADV", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.32": {"text": "Man kam zu keiner Tat.", "tokens": ["Man", "kam", "zu", "kei\u00b7ner", "Tat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Ich habe viel Kapitel schon gesehn,", "tokens": ["Ich", "ha\u00b7be", "viel", "Ka\u00b7pi\u00b7tel", "schon", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die nichts als viele Worte hatten:", "tokens": ["Die", "nichts", "als", "vie\u00b7le", "Wor\u00b7te", "hat\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "KOKOM", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kapitel, nicht von Ratten,", "tokens": ["Ka\u00b7pi\u00b7tel", ",", "nicht", "von", "Rat\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nein, von der M\u00f6nche tapfrer Schar,", "tokens": ["Nein", ",", "von", "der", "M\u00f6n\u00b7che", "tapf\u00b7rer", "Schar", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Von Domherren sogar!", "tokens": ["Von", "Dom\u00b7her\u00b7ren", "so\u00b7gar", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}