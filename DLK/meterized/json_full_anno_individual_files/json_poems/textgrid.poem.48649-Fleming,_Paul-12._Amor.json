{"textgrid.poem.48649": {"metadata": {"author": {"name": "Fleming, Paul", "birth": "N.A.", "death": "N.A."}, "title": "12. Amor", "genre": "verse", "period": "N.A.", "pub_year": 1624, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Des kleinen Sch\u00fctzen hei\u00dfe Polzen,", "tokens": ["Des", "klei\u00b7nen", "Sch\u00fct\u00b7zen", "hei\u00b7\u00dfe", "Pol\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die stecken allzu tief in mir,", "tokens": ["die", "ste\u00b7cken", "all\u00b7zu", "tief", "in", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "PTKA", "ADJD", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "seither so ist mir f\u00fcr und f\u00fcr", "tokens": ["sei\u00b7ther", "so", "ist", "mir", "f\u00fcr", "und", "f\u00fcr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "APPR", "KON", "APPR"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "von ihnen Leib und Sin zerschmolzen.", "tokens": ["von", "ih\u00b7nen", "Leib", "und", "Sin", "zer\u00b7schmol\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Man hat mich oft bereden wollen.", "tokens": ["Man", "hat", "mich", "oft", "be\u00b7re\u00b7den", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die Liebe sei nichts als ein Wahn.", "tokens": ["die", "Lie\u00b7be", "sei", "nichts", "als", "ein", "Wahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "KOKOM", "ART", "NN", "$."], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Itzt wird mir an mir kund getan,", "tokens": ["Itzt", "wird", "mir", "an", "mir", "kund", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "PPER", "PTKVZ", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "was ich nicht h\u00e4tte gl\u00e4uben sollen.", "tokens": ["was", "ich", "nicht", "h\u00e4t\u00b7te", "gl\u00e4u\u00b7ben", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "VAFIN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ja, was noch mehr von diesem Knaben,", "tokens": ["Ja", ",", "was", "noch", "mehr", "von", "die\u00b7sem", "Kna\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "ADV", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "obschon der P\u00f6vel anders spricht:", "tokens": ["ob\u00b7schon", "der", "P\u00f6\u00b7vel", "an\u00b7ders", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "er traf und dennoch zielt' er nicht.", "tokens": ["er", "traf", "und", "den\u00b7noch", "zielt'", "er", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "ADV", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er mu\u00df ja ein Gesichte haben.", "tokens": ["Er", "mu\u00df", "ja", "ein", "Ge\u00b7sich\u00b7te", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "So kan ichs auch in mich nicht bringen,", "tokens": ["So", "kan", "ichs", "auch", "in", "mich", "nicht", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "APPR", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df er ein schwaches Kind soll sein.", "tokens": ["da\u00df", "er", "ein", "schwa\u00b7ches", "Kind", "soll", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich Armer bins nicht nur allein',", "tokens": ["Ich", "Ar\u00b7mer", "bins", "nicht", "nur", "al\u00b7lein'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "PTKNEG", "ADV", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "er kan die G\u00f6tter auch bezwingen.", "tokens": ["er", "kan", "die", "G\u00f6t\u00b7ter", "auch", "be\u00b7zwin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob er nicht mehr sei als ein Man!", "tokens": ["ob", "er", "nicht", "mehr", "sei", "als", "ein", "Man", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "VAFIN", "KOKOM", "ART", "PIS", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.5": {"line.1": {"text": "Ein Teil der spricht, er soll wol h\u00f6ren.", "tokens": ["Ein", "Teil", "der", "spricht", ",", "er", "soll", "wol", "h\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "VVFIN", "$,", "PPER", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "O, das ist wol ein gro\u00dfer Schnitt!", "tokens": ["O", ",", "das", "ist", "wol", "ein", "gro\u00b7\u00dfer", "Schnitt", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDS", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich ruf', ich seufz', ich fleh', ich bitt':", "tokens": ["Ich", "ruf'", ",", "ich", "seuf\u00b7z'", ",", "ich", "fleh'", ",", "ich", "bitt'", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "umsonst ists, da\u00df wir ihn so ehren.", "tokens": ["um\u00b7sonst", "ists", ",", "da\u00df", "wir", "ihn", "so", "eh\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$,", "KOUS", "PPER", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer spricht, da\u00df Amor h\u00f6ren kan,", "tokens": ["Wer", "spricht", ",", "da\u00df", "A\u00b7mor", "h\u00f6\u00b7ren", "kan", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "NE", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und gl\u00e4ubts, der sehe mich nur an!", "tokens": ["und", "gl\u00e4ubts", ",", "der", "se\u00b7he", "mich", "nur", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PRELS", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wie sch\u00e4ndlich hat auch der gelogen,", "tokens": ["Wie", "sch\u00e4nd\u00b7lich", "hat", "auch", "der", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ADV", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der michs beredt' und schwur darbei,", "tokens": ["der", "michs", "be\u00b7redt'", "und", "schwur", "dar\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "KON", "VVFIN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df Amor nichts als Freude sei!", "tokens": ["da\u00df", "A\u00b7mor", "nichts", "als", "Freu\u00b7de", "sei", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PIS", "KOKOM", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Itzt f\u00fchl' ichs, da\u00df ich bin betrogen.", "tokens": ["Itzt", "f\u00fchl'", "ichs", ",", "da\u00df", "ich", "bin", "be\u00b7tro\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "KOUS", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor nicht betr\u00fcben kan!", "tokens": ["ob", "A\u00b7mor", "nicht", "be\u00b7tr\u00fc\u00b7ben", "kan", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ein Ieder traue seinem Sinne,", "tokens": ["Ein", "Ie\u00b7der", "trau\u00b7e", "sei\u00b7nem", "Sin\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wer Amor sei und wie und was!", "tokens": ["wer", "A\u00b7mor", "sei", "und", "wie", "und", "was", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VAFIN", "KON", "PWAV", "KON", "PWS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sage di\u00df, man sage das:", "tokens": ["Man", "sa\u00b7ge", "di\u00df", ",", "man", "sa\u00b7ge", "das", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PDS", "$,", "PIS", "VVFIN", "PDS", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "ich bin es leider worden inne.", "tokens": ["ich", "bin", "es", "lei\u00b7der", "wor\u00b7den", "in\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VAPP", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Was Amor nicht kan oder kan,", "tokens": ["Was", "A\u00b7mor", "nicht", "kan", "o\u00b7der", "kan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "PTKNEG", "VMFIN", "KON", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das zeiget mein Exempel an.", "tokens": ["das", "zei\u00b7get", "mein", "Ex\u00b7em\u00b7pel", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.8": {"line.1": {"text": "Des kleinen Sch\u00fctzen hei\u00dfe Polzen,", "tokens": ["Des", "klei\u00b7nen", "Sch\u00fct\u00b7zen", "hei\u00b7\u00dfe", "Pol\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die stecken allzu tief in mir,", "tokens": ["die", "ste\u00b7cken", "all\u00b7zu", "tief", "in", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "PTKA", "ADJD", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "seither so ist mir f\u00fcr und f\u00fcr", "tokens": ["sei\u00b7ther", "so", "ist", "mir", "f\u00fcr", "und", "f\u00fcr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "APPR", "KON", "APPR"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "von ihnen Leib und Sin zerschmolzen.", "tokens": ["von", "ih\u00b7nen", "Leib", "und", "Sin", "zer\u00b7schmol\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Man hat mich oft bereden wollen.", "tokens": ["Man", "hat", "mich", "oft", "be\u00b7re\u00b7den", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die Liebe sei nichts als ein Wahn.", "tokens": ["die", "Lie\u00b7be", "sei", "nichts", "als", "ein", "Wahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "KOKOM", "ART", "NN", "$."], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Itzt wird mir an mir kund getan,", "tokens": ["Itzt", "wird", "mir", "an", "mir", "kund", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "PPER", "PTKVZ", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "was ich nicht h\u00e4tte gl\u00e4uben sollen.", "tokens": ["was", "ich", "nicht", "h\u00e4t\u00b7te", "gl\u00e4u\u00b7ben", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "VAFIN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Ja, was noch mehr von diesem Knaben,", "tokens": ["Ja", ",", "was", "noch", "mehr", "von", "die\u00b7sem", "Kna\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "ADV", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "obschon der P\u00f6vel anders spricht:", "tokens": ["ob\u00b7schon", "der", "P\u00f6\u00b7vel", "an\u00b7ders", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "er traf und dennoch zielt' er nicht.", "tokens": ["er", "traf", "und", "den\u00b7noch", "zielt'", "er", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "ADV", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er mu\u00df ja ein Gesichte haben.", "tokens": ["Er", "mu\u00df", "ja", "ein", "Ge\u00b7sich\u00b7te", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor sei ein blo\u00dfer Wahn!", "tokens": ["ob", "A\u00b7mor", "sei", "ein", "blo\u00b7\u00dfer", "Wahn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "So kan ichs auch in mich nicht bringen,", "tokens": ["So", "kan", "ichs", "auch", "in", "mich", "nicht", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "APPR", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df er ein schwaches Kind soll sein.", "tokens": ["da\u00df", "er", "ein", "schwa\u00b7ches", "Kind", "soll", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich Armer bins nicht nur allein',", "tokens": ["Ich", "Ar\u00b7mer", "bins", "nicht", "nur", "al\u00b7lein'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "PTKNEG", "ADV", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "er kan die G\u00f6tter auch bezwingen.", "tokens": ["er", "kan", "die", "G\u00f6t\u00b7ter", "auch", "be\u00b7zwin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob er nicht mehr sei als ein Man!", "tokens": ["ob", "er", "nicht", "mehr", "sei", "als", "ein", "Man", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "VAFIN", "KOKOM", "ART", "PIS", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.12": {"line.1": {"text": "Ein Teil der spricht, er soll wol h\u00f6ren.", "tokens": ["Ein", "Teil", "der", "spricht", ",", "er", "soll", "wol", "h\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "VVFIN", "$,", "PPER", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "O, das ist wol ein gro\u00dfer Schnitt!", "tokens": ["O", ",", "das", "ist", "wol", "ein", "gro\u00b7\u00dfer", "Schnitt", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PDS", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich ruf', ich seufz', ich fleh', ich bitt':", "tokens": ["Ich", "ruf'", ",", "ich", "seuf\u00b7z'", ",", "ich", "fleh'", ",", "ich", "bitt'", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "umsonst ists, da\u00df wir ihn so ehren.", "tokens": ["um\u00b7sonst", "ists", ",", "da\u00df", "wir", "ihn", "so", "eh\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$,", "KOUS", "PPER", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer spricht, da\u00df Amor h\u00f6ren kan,", "tokens": ["Wer", "spricht", ",", "da\u00df", "A\u00b7mor", "h\u00f6\u00b7ren", "kan", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "NE", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und gl\u00e4ubts, der sehe mich nur an!", "tokens": ["und", "gl\u00e4ubts", ",", "der", "se\u00b7he", "mich", "nur", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PRELS", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Wie sch\u00e4ndlich hat auch der gelogen,", "tokens": ["Wie", "sch\u00e4nd\u00b7lich", "hat", "auch", "der", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ADV", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der michs beredt' und schwur darbei,", "tokens": ["der", "michs", "be\u00b7redt'", "und", "schwur", "dar\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "KON", "VVFIN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df Amor nichts als Freude sei!", "tokens": ["da\u00df", "A\u00b7mor", "nichts", "als", "Freu\u00b7de", "sei", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PIS", "KOKOM", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Itzt f\u00fchl' ichs, da\u00df ich bin betrogen.", "tokens": ["Itzt", "f\u00fchl'", "ichs", ",", "da\u00df", "ich", "bin", "be\u00b7tro\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "KOUS", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer zweifelt, sehe mich nur an,", "tokens": ["Wer", "zwei\u00b7felt", ",", "se\u00b7he", "mich", "nur", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ob Amor nicht betr\u00fcben kan!", "tokens": ["ob", "A\u00b7mor", "nicht", "be\u00b7tr\u00fc\u00b7ben", "kan", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Ein Ieder traue seinem Sinne,", "tokens": ["Ein", "Ie\u00b7der", "trau\u00b7e", "sei\u00b7nem", "Sin\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wer Amor sei und wie und was!", "tokens": ["wer", "A\u00b7mor", "sei", "und", "wie", "und", "was", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VAFIN", "KON", "PWAV", "KON", "PWS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sage di\u00df, man sage das:", "tokens": ["Man", "sa\u00b7ge", "di\u00df", ",", "man", "sa\u00b7ge", "das", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PDS", "$,", "PIS", "VVFIN", "PDS", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "ich bin es leider worden inne.", "tokens": ["ich", "bin", "es", "lei\u00b7der", "wor\u00b7den", "in\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VAPP", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Was Amor nicht kan oder kan,", "tokens": ["Was", "A\u00b7mor", "nicht", "kan", "o\u00b7der", "kan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "PTKNEG", "VMFIN", "KON", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das zeiget mein Exempel an.", "tokens": ["das", "zei\u00b7get", "mein", "Ex\u00b7em\u00b7pel", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}}}}