{"textgrid.poem.57565": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Absagebrief", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Betrogen hast du mich, mein Schatz,", "tokens": ["Be\u00b7tro\u00b7gen", "hast", "du", "mich", ",", "mein", "Schatz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PRF", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlst dich hoch und stolz dabei,", "tokens": ["Und", "f\u00fchlst", "dich", "hoch", "und", "stolz", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df eines Dichters gro\u00dfes Herz", "tokens": ["Da\u00df", "ei\u00b7nes", "Dich\u00b7ters", "gro\u00b7\u00dfes", "Herz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um deinetwillen gebrochen sei.", "tokens": ["Um", "dei\u00b7net\u00b7wil\u00b7len", "ge\u00b7bro\u00b7chen", "sei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "So traurig ist es doch noch nicht,", "tokens": ["So", "trau\u00b7rig", "ist", "es", "doch", "noch", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Wunde heilt in kurzer Zeit", "tokens": ["Die", "Wun\u00b7de", "heilt", "in", "kur\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und das Gef\u00fchl, das mich durchtobt,", "tokens": ["Und", "das", "Ge\u00b7f\u00fchl", ",", "das", "mich", "durch\u00b7tobt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist nur verletzte Eitelkeit.", "tokens": ["Ist", "nur", "ver\u00b7letz\u00b7te", "Ei\u00b7tel\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Gem\u00fct und Seele \u2013 deine Brust", "tokens": ["Ge\u00b7m\u00fct", "und", "See\u00b7le", "\u2013", "dei\u00b7ne", "Brust"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "NN", "$(", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Besa\u00df davon nicht eine Spur:", "tokens": ["Be\u00b7sa\u00df", "da\u00b7von", "nicht", "ei\u00b7ne", "Spur", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Du hattest einen sch\u00f6nen Leib", "tokens": ["Du", "hat\u00b7test", "ei\u00b7nen", "sch\u00f6\u00b7nen", "Leib"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und warst mein Freudenm\u00e4dchen nur.", "tokens": ["Und", "warst", "mein", "Freu\u00b7den\u00b7m\u00e4d\u00b7chen", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Doch dankbar bin f\u00fcr alles ich,", "tokens": ["Doch", "dank\u00b7bar", "bin", "f\u00fcr", "al\u00b7les", "ich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "APPR", "PIS", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr jeden Ku\u00df und jeden Blick,", "tokens": ["F\u00fcr", "je\u00b7den", "Ku\u00df", "und", "je\u00b7den", "Blick", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An all die s\u00fc\u00dfen Stunden denk", "tokens": ["An", "all", "die", "s\u00fc\u00b7\u00dfen", "Stun\u00b7den", "denk"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich immer gerne noch zur\u00fcck.", "tokens": ["Ich", "im\u00b7mer", "ger\u00b7ne", "noch", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Adj\u00fcs! wir scheiden ohne Pein,", "tokens": ["Ad\u00b7j\u00fcs", "!", "wir", "schei\u00b7den", "oh\u00b7ne", "Pein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Antlitz bleich, kein Auge na\u00df \u2013", "tokens": ["Kein", "Ant\u00b7litz", "bleich", ",", "kein", "Au\u00b7ge", "na\u00df", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJD", "$,", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sch\u00f6n bist du, doch ich trinke nie", "tokens": ["Sch\u00f6n", "bist", "du", ",", "doch", "ich", "trin\u00b7ke", "nie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PPER", "$,", "KON", "PPER", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit andern aus ", "tokens": ["Mit", "an\u00b7dern", "aus"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PIS", "APPR"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Postscriptum: Dies noch w\u00fcnsch' ich dir:", "tokens": ["Post\u00b7scrip\u00b7tum", ":", "Dies", "noch", "w\u00fcn\u00b7sch'", "ich", "dir", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PDS", "ADV", "VVFIN", "PPER", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Da\u00df deine ", "tokens": ["Da\u00df", "dei\u00b7ne"], "token_info": ["word", "word"], "pos": ["KOUS", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Damit auch du erfahren m\u00f6gst,", "tokens": ["Da\u00b7mit", "auch", "du", "er\u00b7fah\u00b7ren", "m\u00f6gst", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Betrogen hast du mich, mein Schatz,", "tokens": ["Be\u00b7tro\u00b7gen", "hast", "du", "mich", ",", "mein", "Schatz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PRF", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlst dich hoch und stolz dabei,", "tokens": ["Und", "f\u00fchlst", "dich", "hoch", "und", "stolz", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "KON", "ADJD", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df eines Dichters gro\u00dfes Herz", "tokens": ["Da\u00df", "ei\u00b7nes", "Dich\u00b7ters", "gro\u00b7\u00dfes", "Herz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um deinetwillen gebrochen sei.", "tokens": ["Um", "dei\u00b7net\u00b7wil\u00b7len", "ge\u00b7bro\u00b7chen", "sei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "So traurig ist es doch noch nicht,", "tokens": ["So", "trau\u00b7rig", "ist", "es", "doch", "noch", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Wunde heilt in kurzer Zeit", "tokens": ["Die", "Wun\u00b7de", "heilt", "in", "kur\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und das Gef\u00fchl, das mich durchtobt,", "tokens": ["Und", "das", "Ge\u00b7f\u00fchl", ",", "das", "mich", "durch\u00b7tobt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist nur verletzte Eitelkeit.", "tokens": ["Ist", "nur", "ver\u00b7letz\u00b7te", "Ei\u00b7tel\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Gem\u00fct und Seele \u2013 deine Brust", "tokens": ["Ge\u00b7m\u00fct", "und", "See\u00b7le", "\u2013", "dei\u00b7ne", "Brust"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "NN", "$(", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Besa\u00df davon nicht eine Spur:", "tokens": ["Be\u00b7sa\u00df", "da\u00b7von", "nicht", "ei\u00b7ne", "Spur", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PAV", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Du hattest einen sch\u00f6nen Leib", "tokens": ["Du", "hat\u00b7test", "ei\u00b7nen", "sch\u00f6\u00b7nen", "Leib"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und warst mein Freudenm\u00e4dchen nur.", "tokens": ["Und", "warst", "mein", "Freu\u00b7den\u00b7m\u00e4d\u00b7chen", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Doch dankbar bin f\u00fcr alles ich,", "tokens": ["Doch", "dank\u00b7bar", "bin", "f\u00fcr", "al\u00b7les", "ich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "APPR", "PIS", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr jeden Ku\u00df und jeden Blick,", "tokens": ["F\u00fcr", "je\u00b7den", "Ku\u00df", "und", "je\u00b7den", "Blick", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An all die s\u00fc\u00dfen Stunden denk", "tokens": ["An", "all", "die", "s\u00fc\u00b7\u00dfen", "Stun\u00b7den", "denk"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich immer gerne noch zur\u00fcck.", "tokens": ["Ich", "im\u00b7mer", "ger\u00b7ne", "noch", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Adj\u00fcs! wir scheiden ohne Pein,", "tokens": ["Ad\u00b7j\u00fcs", "!", "wir", "schei\u00b7den", "oh\u00b7ne", "Pein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Antlitz bleich, kein Auge na\u00df \u2013", "tokens": ["Kein", "Ant\u00b7litz", "bleich", ",", "kein", "Au\u00b7ge", "na\u00df", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJD", "$,", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sch\u00f6n bist du, doch ich trinke nie", "tokens": ["Sch\u00f6n", "bist", "du", ",", "doch", "ich", "trin\u00b7ke", "nie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PPER", "$,", "KON", "PPER", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit andern aus ", "tokens": ["Mit", "an\u00b7dern", "aus"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PIS", "APPR"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Postscriptum: Dies noch w\u00fcnsch' ich dir:", "tokens": ["Post\u00b7scrip\u00b7tum", ":", "Dies", "noch", "w\u00fcn\u00b7sch'", "ich", "dir", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PDS", "ADV", "VVFIN", "PPER", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Da\u00df deine ", "tokens": ["Da\u00df", "dei\u00b7ne"], "token_info": ["word", "word"], "pos": ["KOUS", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Damit auch du erfahren m\u00f6gst,", "tokens": ["Da\u00b7mit", "auch", "du", "er\u00b7fah\u00b7ren", "m\u00f6gst", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}