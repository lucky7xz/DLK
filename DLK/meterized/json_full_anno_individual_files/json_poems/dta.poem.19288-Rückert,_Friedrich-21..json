{"dta.poem.19288": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "21.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1837", "urn": "urn:nbn:de:kobv:b4-200905195090", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Was sagt Bewu\u00dftseyn aus? es sagt Bewu\u00dft und Seyn;", "tokens": ["Was", "sagt", "Be\u00b7wu\u00df\u00b7tseyn", "aus", "?", "es", "sagt", "Be\u00b7wu\u00dft", "und", "Seyn", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "NN", "KON", "PPOSAT", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von Seyn und Wissen ist es also der Verein.", "tokens": ["Von", "Seyn", "und", "Wis\u00b7sen", "ist", "es", "al\u00b7so", "der", "Ver\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "KON", "NN", "VAFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Von beider welchem ward nun welches angenommen?", "tokens": ["Von", "bei\u00b7der", "wel\u00b7chem", "ward", "nun", "wel\u00b7ches", "an\u00b7ge\u00b7nom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "PWAT", "VAFIN", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist Wissen hin zum Seyn, zum Wissen Seyn gekommen?", "tokens": ["Ist", "Wis\u00b7sen", "hin", "zum", "Seyn", ",", "zum", "Wis\u00b7sen", "Seyn", "ge\u00b7kom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "APPRART", "NN", "$,", "APPRART", "NN", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Das Wissen steht zuerst, es steht das Seyn zuletzt,", "tokens": ["Das", "Wis\u00b7sen", "steht", "zu\u00b7erst", ",", "es", "steht", "das", "Seyn", "zu\u00b7letzt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "$,", "PPER", "VVFIN", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Wissen also ist dem Seyn vorausgesetzt.", "tokens": ["Das", "Wis\u00b7sen", "al\u00b7so", "ist", "dem", "Seyn", "vor\u00b7aus\u00b7ge\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Jawohl ist meinem Seyn vorausgesetzt ein Wissen,", "tokens": ["Ja\u00b7wohl", "ist", "mei\u00b7nem", "Seyn", "vor\u00b7aus\u00b7ge\u00b7setzt", "ein", "Wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein Wissen, welchem nie mein Seyn kann seyn entrissen.", "tokens": ["Ein", "Wis\u00b7sen", ",", "wel\u00b7chem", "nie", "mein", "Seyn", "kann", "seyn", "ent\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "PPOSAT", "NN", "VMFIN", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Ich bin von Gott gewu\u00dft, und bin dadurch allein;", "tokens": ["Ich", "bin", "von", "Gott", "ge\u00b7wu\u00dft", ",", "und", "bin", "da\u00b7durch", "al\u00b7lein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "VVPP", "$,", "KON", "VAFIN", "PAV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Selbstbewu\u00dftseyn ist, von Gott gewu\u00dft zu seyn.", "tokens": ["Mein", "Selbst\u00b7be\u00b7wu\u00df\u00b7tseyn", "ist", ",", "von", "Gott", "ge\u00b7wu\u00dft", "zu", "seyn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "$,", "APPR", "NN", "VVPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Ich war nicht mein bewu\u00dft, und war nicht dein bewu\u00dft,", "tokens": ["Ich", "war", "nicht", "mein", "be\u00b7wu\u00dft", ",", "und", "war", "nicht", "dein", "be\u00b7wu\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "PPOSAT", "ADJD", "$,", "KON", "VAFIN", "PTKNEG", "PPOSAT", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "O Gott, und war es doch, denn du warst mein bewu\u00dft.", "tokens": ["O", "Gott", ",", "und", "war", "es", "doch", ",", "denn", "du", "warst", "mein", "be\u00b7wu\u00dft", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "KON", "VAFIN", "PPER", "ADV", "$,", "KON", "PPER", "VAFIN", "PPOSAT", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Bewu\u00dftseyn aber wei\u00df nicht um sich selbst allein,", "tokens": ["Be\u00b7wu\u00df\u00b7tseyn", "a\u00b7ber", "wei\u00df", "nicht", "um", "sich", "selbst", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VVFIN", "PTKNEG", "APPR", "PRF", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es wei\u00df auch um die Welt, das wird es gleich entzwein.", "tokens": ["Es", "wei\u00df", "auch", "um", "die", "Welt", ",", "das", "wird", "es", "gleich", "ent\u00b7zwein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$,", "PDS", "VAFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Doch die Vers\u00f6hnung ist dem Streit schon eingewoben,", "tokens": ["Doch", "die", "Ver\u00b7s\u00f6h\u00b7nung", "ist", "dem", "Streit", "schon", "ein\u00b7ge\u00b7wo\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da ich die Welt und mich in Gott wei\u00df aufgehoben.", "tokens": ["Da", "ich", "die", "Welt", "und", "mich", "in", "Gott", "wei\u00df", "auf\u00b7ge\u00b7ho\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "KON", "PRF", "APPR", "NN", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Nicht aufgehoben, wie sich Ja und Nein aufhebt;", "tokens": ["Nicht", "auf\u00b7ge\u00b7ho\u00b7ben", ",", "wie", "sich", "Ja", "und", "Nein", "auf\u00b7hebt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$,", "PWAV", "PRF", "PTKANT", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Emporgehoben, wie zur Sonn' ein Adler schwebt.", "tokens": ["Em\u00b7por\u00b7ge\u00b7ho\u00b7ben", ",", "wie", "zur", "Sonn'", "ein", "Ad\u00b7ler", "schwebt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWAV", "APPRART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Im Gottbewu\u00dftseyn geht nicht mein Bewu\u00dftseyn aus;", "tokens": ["Im", "Gott\u00b7be\u00b7wu\u00df\u00b7tseyn", "geht", "nicht", "mein", "Be\u00b7wu\u00df\u00b7tseyn", "aus", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PTKNEG", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Eingeht es wie ein Kind in seines Vaters Haus.", "tokens": ["Ein\u00b7geht", "es", "wie", "ein", "Kind", "in", "sei\u00b7nes", "Va\u00b7ters", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "ART", "NN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}