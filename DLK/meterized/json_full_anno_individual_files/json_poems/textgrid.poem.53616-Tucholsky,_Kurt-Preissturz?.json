{"textgrid.poem.53616": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Preissturz?", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Amalie steht vorm Ehesegen.", "tokens": ["A\u00b7ma\u00b7lie", "steht", "vorm", "E\u00b7he\u00b7se\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er naht sich legitim und leis.", "tokens": ["Er", "naht", "sich", "le\u00b7gi\u00b7tim", "und", "leis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NE", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie kauft sich noch kein Bett. Von wegen:", "tokens": ["Sie", "kauft", "sich", "noch", "kein", "Bett", ".", "Von", "we\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "PIAT", "NN", "$.", "APPR", "APPR", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "es f\u00e4llt der Preis.", "tokens": ["es", "f\u00e4llt", "der", "Preis", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Sie wartet. Und die Tage eilen.", "tokens": ["Sie", "war\u00b7tet", ".", "Und", "die", "Ta\u00b7ge", "ei\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KON", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Preise falln zur Lenzsaison.", "tokens": ["Die", "Prei\u00b7se", "falln", "zur", "Lenz\u00b7sai\u00b7son", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sanft schl\u00e4ft ihr Br\u00e4utigam derweilen", "tokens": ["Sanft", "schl\u00e4ft", "ihr", "Br\u00e4u\u00b7ti\u00b7gam", "der\u00b7wei\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "NE", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "aufs Chaiselongue.", "tokens": ["aufs", "Chai\u00b7se\u00b7lon\u00b7gue", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.3": {"line.1": {"text": "Sie wartet. Und wir warten alle . . .", "tokens": ["Sie", "war\u00b7tet", ".", "Und", "wir", "war\u00b7ten", "al\u00b7le", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "KON", "PPER", "VVFIN", "PIS", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kommt uns der Preissturz blo\u00df so vor?", "tokens": ["Kommt", "uns", "der", "Preis\u00b7sturz", "blo\u00df", "so", "vor", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Dummheit in den Reichstagshallen", "tokens": ["Die", "Dumm\u00b7heit", "in", "den", "Reichs\u00b7tags\u00b7hal\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "steht nach wie vor.", "tokens": ["steht", "nach", "wie", "vor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "KOKOM", "APPR", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Kapp: dies Papier hat seine N\u00fccken.", "tokens": ["Kapp", ":", "dies", "Pa\u00b7pier", "hat", "sei\u00b7ne", "N\u00fc\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PDS", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Herr Ludendorff wird nicht notiert.", "tokens": ["Herr", "Lu\u00b7den\u00b7dorff", "wird", "nicht", "no\u00b7tiert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Gr\u00fcnhorn, wer in Achselst\u00fccken", "tokens": ["Ein", "Gr\u00fcn\u00b7horn", ",", "wer", "in", "Ach\u00b7sel\u00b7st\u00fc\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PWS", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "nicht spekuliert.", "tokens": ["nicht", "spe\u00b7ku\u00b7liert", "."], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Selbst Noske freut sich noch des Lebens,", "tokens": ["Selbst", "Nos\u00b7ke", "freut", "sich", "noch", "des", "Le\u00b7bens", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die St\u00fctze unsres Wallotbaus.", "tokens": ["die", "St\u00fct\u00b7ze", "uns\u00b7res", "Wal\u00b7lot\u00b7baus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir offerierten ihn vergebens", "tokens": ["Wir", "of\u00b7fe\u00b7rier\u00b7ten", "ihn", "ver\u00b7ge\u00b7bens"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "ab Haus.", "tokens": ["ab", "Haus", "."], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.6": {"line.1": {"text": "Wer will die Wilhelmstra\u00dfe deuten,", "tokens": ["Wer", "will", "die", "Wil\u00b7helms\u00b7tra\u00b7\u00dfe", "deu\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "f\u00fcr die man keinen Dollar bot?", "tokens": ["f\u00fcr", "die", "man", "kei\u00b7nen", "Dol\u00b7lar", "bot", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nachfrage ist nach t\u00fcchtgen Leuten . . .", "tokens": ["Nach\u00b7fra\u00b7ge", "ist", "nach", "t\u00fccht\u00b7gen", "Leu\u00b7ten", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "VAFIN", "APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.4": {"text": "kein Angebot.", "tokens": ["kein", "An\u00b7ge\u00b7bot", "."], "token_info": ["word", "word", "punct"], "pos": ["PIAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "Ein M\u00e4dchen nur \u2013 auf jenem Striche,", "tokens": ["Ein", "M\u00e4d\u00b7chen", "nur", "\u2013", "auf", "je\u00b7nem", "Stri\u00b7che", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$(", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wo man den Deutschen Mann sein l\u00e4\u00dft \u2013", "tokens": ["wo", "man", "den", "Deut\u00b7schen", "Mann", "sein", "l\u00e4\u00dft", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "ADJA", "NN", "VAINF", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "spricht froh: \u00bbIch kenn die B\u00f6rsenschliche.", "tokens": ["spricht", "froh", ":", "\u00bb", "Ich", "kenn", "die", "B\u00f6r\u00b7sen\u00b7schli\u00b7che", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "$.", "$(", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wenn der ganze Markt entwiche \u2013:", "tokens": ["Und", "wenn", "der", "gan\u00b7ze", "Markt", "ent\u00b7wi\u00b7che", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich bleibe fest \u2013!\u00ab", "tokens": ["Ich", "blei\u00b7be", "fest", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$(", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Amalie steht vorm Ehesegen.", "tokens": ["A\u00b7ma\u00b7lie", "steht", "vorm", "E\u00b7he\u00b7se\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er naht sich legitim und leis.", "tokens": ["Er", "naht", "sich", "le\u00b7gi\u00b7tim", "und", "leis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "NE", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie kauft sich noch kein Bett. Von wegen:", "tokens": ["Sie", "kauft", "sich", "noch", "kein", "Bett", ".", "Von", "we\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "PIAT", "NN", "$.", "APPR", "APPR", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "es f\u00e4llt der Preis.", "tokens": ["es", "f\u00e4llt", "der", "Preis", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Sie wartet. Und die Tage eilen.", "tokens": ["Sie", "war\u00b7tet", ".", "Und", "die", "Ta\u00b7ge", "ei\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KON", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Preise falln zur Lenzsaison.", "tokens": ["Die", "Prei\u00b7se", "falln", "zur", "Lenz\u00b7sai\u00b7son", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sanft schl\u00e4ft ihr Br\u00e4utigam derweilen", "tokens": ["Sanft", "schl\u00e4ft", "ihr", "Br\u00e4u\u00b7ti\u00b7gam", "der\u00b7wei\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "NE", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "aufs Chaiselongue.", "tokens": ["aufs", "Chai\u00b7se\u00b7lon\u00b7gue", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.10": {"line.1": {"text": "Sie wartet. Und wir warten alle . . .", "tokens": ["Sie", "war\u00b7tet", ".", "Und", "wir", "war\u00b7ten", "al\u00b7le", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "KON", "PPER", "VVFIN", "PIS", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kommt uns der Preissturz blo\u00df so vor?", "tokens": ["Kommt", "uns", "der", "Preis\u00b7sturz", "blo\u00df", "so", "vor", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Dummheit in den Reichstagshallen", "tokens": ["Die", "Dumm\u00b7heit", "in", "den", "Reichs\u00b7tags\u00b7hal\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "steht nach wie vor.", "tokens": ["steht", "nach", "wie", "vor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "KOKOM", "APPR", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Kapp: dies Papier hat seine N\u00fccken.", "tokens": ["Kapp", ":", "dies", "Pa\u00b7pier", "hat", "sei\u00b7ne", "N\u00fc\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PDS", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Herr Ludendorff wird nicht notiert.", "tokens": ["Herr", "Lu\u00b7den\u00b7dorff", "wird", "nicht", "no\u00b7tiert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Gr\u00fcnhorn, wer in Achselst\u00fccken", "tokens": ["Ein", "Gr\u00fcn\u00b7horn", ",", "wer", "in", "Ach\u00b7sel\u00b7st\u00fc\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PWS", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "nicht spekuliert.", "tokens": ["nicht", "spe\u00b7ku\u00b7liert", "."], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Selbst Noske freut sich noch des Lebens,", "tokens": ["Selbst", "Nos\u00b7ke", "freut", "sich", "noch", "des", "Le\u00b7bens", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die St\u00fctze unsres Wallotbaus.", "tokens": ["die", "St\u00fct\u00b7ze", "uns\u00b7res", "Wal\u00b7lot\u00b7baus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir offerierten ihn vergebens", "tokens": ["Wir", "of\u00b7fe\u00b7rier\u00b7ten", "ihn", "ver\u00b7ge\u00b7bens"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "ab Haus.", "tokens": ["ab", "Haus", "."], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.13": {"line.1": {"text": "Wer will die Wilhelmstra\u00dfe deuten,", "tokens": ["Wer", "will", "die", "Wil\u00b7helms\u00b7tra\u00b7\u00dfe", "deu\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "f\u00fcr die man keinen Dollar bot?", "tokens": ["f\u00fcr", "die", "man", "kei\u00b7nen", "Dol\u00b7lar", "bot", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nachfrage ist nach t\u00fcchtgen Leuten . . .", "tokens": ["Nach\u00b7fra\u00b7ge", "ist", "nach", "t\u00fccht\u00b7gen", "Leu\u00b7ten", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "VAFIN", "APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.4": {"text": "kein Angebot.", "tokens": ["kein", "An\u00b7ge\u00b7bot", "."], "token_info": ["word", "word", "punct"], "pos": ["PIAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.14": {"line.1": {"text": "Ein M\u00e4dchen nur \u2013 auf jenem Striche,", "tokens": ["Ein", "M\u00e4d\u00b7chen", "nur", "\u2013", "auf", "je\u00b7nem", "Stri\u00b7che", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$(", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wo man den Deutschen Mann sein l\u00e4\u00dft \u2013", "tokens": ["wo", "man", "den", "Deut\u00b7schen", "Mann", "sein", "l\u00e4\u00dft", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "ADJA", "NN", "VAINF", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "spricht froh: \u00bbIch kenn die B\u00f6rsenschliche.", "tokens": ["spricht", "froh", ":", "\u00bb", "Ich", "kenn", "die", "B\u00f6r\u00b7sen\u00b7schli\u00b7che", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "$.", "$(", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wenn der ganze Markt entwiche \u2013:", "tokens": ["Und", "wenn", "der", "gan\u00b7ze", "Markt", "ent\u00b7wi\u00b7che", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich bleibe fest \u2013!\u00ab", "tokens": ["Ich", "blei\u00b7be", "fest", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$(", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}}}}}