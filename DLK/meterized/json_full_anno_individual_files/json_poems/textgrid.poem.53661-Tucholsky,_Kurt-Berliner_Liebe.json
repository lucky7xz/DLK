{"textgrid.poem.53661": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Berliner Liebe", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Steht dir der Sinn nach Liebe in den Orten", "tokens": ["Steht", "dir", "der", "Sinn", "nach", "Lie\u00b7be", "in", "den", "Or\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Westend bis K\u00f6penick:", "tokens": ["Wes\u00b7tend", "bis", "K\u00f6\u00b7pe\u00b7nick", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "dann senk den Blick", "tokens": ["dann", "senk", "den", "Blick"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "und unterscheide im Objekte die drei Sorten:", "tokens": ["und", "un\u00b7ter\u00b7schei\u00b7de", "im", "Ob\u00b7jek\u00b7te", "die", "drei", "Sor\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Da gibt es Frauen mit den Scheitelhaaren,", "tokens": ["Da", "gibt", "es", "Frau\u00b7en", "mit", "den", "Schei\u00b7tel\u00b7haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "gepunztes Silber auf dem falschen Busen,", "tokens": ["ge\u00b7punz\u00b7tes", "Sil\u00b7ber", "auf", "dem", "fal\u00b7schen", "Bu\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "teils im Reformkleid, teils in Eigenblusen,", "tokens": ["teils", "im", "Re\u00b7form\u00b7kleid", ",", "teils", "in", "Ei\u00b7gen\u00b7blu\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "ADV", "APPR", "NN", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.8": {"text": "die einmal \u2013 ach, wie weit! \u2013 fast reinlich waren", "tokens": ["die", "ein\u00b7mal", "\u2013", "ach", ",", "wie", "weit", "!", "\u2013", "fast", "rein\u00b7lich", "wa\u00b7ren"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADV", "$(", "XY", "$,", "PWAV", "ADJD", "$.", "$(", "ADV", "ADJD", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "(jetzt dunkelwei\u00df).", "tokens": ["(", "jetzt", "dun\u00b7kel\u00b7wei\u00df", ")", "."], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADJD", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Bei Sturm und Regen", "tokens": ["Bei", "Sturm", "und", "Re\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.11": {"text": "gehn diese gern durch Wald und Flur allein,", "tokens": ["gehn", "die\u00b7se", "gern", "durch", "Wald", "und", "Flur", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "ADV", "APPR", "NN", "KON", "NN", "ADV", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.12": {"text": "das Lodenh\u00fctchen keck auf einem Ohre,", "tokens": ["das", "Lo\u00b7den\u00b7h\u00fct\u00b7chen", "keck", "auf", "ei\u00b7nem", "Oh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "und sprechen mit sich selbst und mit Tagore . . .", "tokens": ["und", "spre\u00b7chen", "mit", "sich", "selbst", "und", "mit", "Ta\u00b7go\u00b7re", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "PRF", "ADV", "KON", "APPR", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.15": {"text": "Sie sagen Feuilletons, eh man sie legt.", "tokens": ["Sie", "sa\u00b7gen", "Feu\u00b7il\u00b7le\u00b7tons", ",", "eh", "man", "sie", "legt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "$,", "KOUS", "PIS", "PPER", "VVFIN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Sie sind sehr edel.", "tokens": ["Sie", "sind", "sehr", "e\u00b7del", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.17": {"text": "Aber nicht gepflegt.", "tokens": ["A\u00b7ber", "nicht", "ge\u00b7pflegt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Da gibt es solche, unten rum aus Seide,", "tokens": ["Da", "gibt", "es", "sol\u00b7che", ",", "un\u00b7ten", "rum", "aus", "Sei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "$,", "ADV", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "im samtnen Mantel mit dem Waschb\u00e4rkragen \u2013", "tokens": ["im", "samt\u00b7nen", "Man\u00b7tel", "mit", "dem", "Waschb\u00e4r\u00b7kra\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "nach ihren Eltern mu\u00dft du sie nicht fragen.", "tokens": ["nach", "ih\u00b7ren", "El\u00b7tern", "mu\u00dft", "du", "sie", "nicht", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sie ist euch treu \u2013 und so liebt ihr drei beide.", "tokens": ["Sie", "ist", "euch", "treu", "\u2013", "und", "so", "liebt", "ihr", "drei", "bei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "$(", "KON", "ADV", "VVFIN", "PPER", "CARD", "PIS", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Gro\u00df ausgehn nennt der Fachmann dein Getue.", "tokens": ["Gro\u00df", "aus\u00b7gehn", "nennt", "der", "Fach\u00b7mann", "dein", "Ge\u00b7tue", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVIZU", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "F\u00fchr sie ins Kino, ins Theater ein!", "tokens": ["F\u00fchr", "sie", "ins", "Ki\u00b7no", ",", "ins", "The\u00b7a\u00b7ter", "ein", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "$,", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Sie tanzt den neusten Schritt, kennt alle Paare,", "tokens": ["Sie", "tanzt", "den", "neus\u00b7ten", "Schritt", ",", "kennt", "al\u00b7le", "Paa\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "hat jeden Monat frischgef\u00e4rbte Haare . . .", "tokens": ["hat", "je\u00b7den", "Mo\u00b7nat", "frischge\u00b7f\u00e4rb\u00b7te", "Haa\u00b7re", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VAFIN", "PIAT", "NN", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Sie spricht nicht viel.", "tokens": ["Sie", "spricht", "nicht", "viel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.11": {"text": "Doch was sie spricht, ist Kitt.", "tokens": ["Doch", "was", "sie", "spricht", ",", "ist", "Kitt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "Und sie nimmt alle s\u00fc\u00dfen Ecken mit.", "tokens": ["Und", "sie", "nimmt", "al\u00b7le", "s\u00fc\u00b7\u00dfen", "E\u00b7cken", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIAT", "ADJA", "NN", "PTKVZ", "$."], "meter": "-++--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Willst du die J\u00fcngerin Thaliens k\u00fcren?", "tokens": ["Willst", "du", "die", "J\u00fcn\u00b7ge\u00b7rin", "Tha\u00b7li\u00b7ens", "k\u00fc\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "NN", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Sie offenbart, wenn sie mit dir im Bund ist,", "tokens": ["Sie", "of\u00b7fen\u00b7bart", ",", "wenn", "sie", "mit", "dir", "im", "Bund", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "APPR", "PPER", "APPRART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "was ihr Direktor f\u00fcr ein Schweinehund ist:", "tokens": ["was", "ihr", "Di\u00b7rek\u00b7tor", "f\u00fcr", "ein", "Schwei\u00b7ne\u00b7hund", "ist", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "er wollt sie alle in Versuchung f\u00fchren \u2013", "tokens": ["er", "wollt", "sie", "al\u00b7le", "in", "Ver\u00b7su\u00b7chung", "f\u00fch\u00b7ren", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Das t\u00e4t sie nie. (Fast nie.)", "tokens": ["Das", "t\u00e4t", "sie", "nie", ".", "(", "Fast", "nie", ".", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "$.", "$(", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Es rinnt die Rede:", "tokens": ["Es", "rinnt", "die", "Re\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Von Proben, Premerieen, Klatscherein \u2013", "tokens": ["Von", "Pro\u00b7ben", ",", "Pre\u00b7me\u00b7ri\u00b7e\u00b7en", ",", "Klat\u00b7sche\u00b7rein", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "sie meistere Spiel und Sprache wie nur wenige,", "tokens": ["sie", "meis\u00b7te\u00b7re", "Spiel", "und", "Spra\u00b7che", "wie", "nur", "we\u00b7ni\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "NN", "KON", "NN", "KOKOM", "ADV", "PIS", "$,"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "sie spiele Olala und Iphigenie . . .", "tokens": ["sie", "spie\u00b7le", "O\u00b7la\u00b7la", "und", "Ip\u00b7hi\u00b7ge\u00b7nie", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "NE", "KON", "NE", "$.", "$.", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.11": {"text": "Beim Papa Rickelt! S\u00fc\u00df in allen Phasen:", "tokens": ["Beim", "Pa\u00b7pa", "Ri\u00b7ckelt", "!", "S\u00fc\u00df", "in", "al\u00b7len", "Pha\u00b7sen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$.", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Sie liebt.", "tokens": ["Sie", "liebt", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Und bringt dich zeitig untern Rasen.", "tokens": ["Und", "bringt", "dich", "zei\u00b7tig", "un\u00b7tern", "Ra\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "So geh, du Liebeswanderer, von Haus zu Haus.", "tokens": ["So", "geh", ",", "du", "Lie\u00b7bes\u00b7wan\u00b7de\u00b7rer", ",", "von", "Haus", "zu", "Haus", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PPER", "NN", "$,", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Berlin ist gro\u00df.", "tokens": ["Ber\u00b7lin", "ist", "gro\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Nun such dir eine aus!", "tokens": ["Nun", "such", "dir", "ei\u00b7ne", "aus", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Steht dir der Sinn nach Liebe in den Orten", "tokens": ["Steht", "dir", "der", "Sinn", "nach", "Lie\u00b7be", "in", "den", "Or\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Westend bis K\u00f6penick:", "tokens": ["Wes\u00b7tend", "bis", "K\u00f6\u00b7pe\u00b7nick", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "dann senk den Blick", "tokens": ["dann", "senk", "den", "Blick"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "und unterscheide im Objekte die drei Sorten:", "tokens": ["und", "un\u00b7ter\u00b7schei\u00b7de", "im", "Ob\u00b7jek\u00b7te", "die", "drei", "Sor\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ART", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Da gibt es Frauen mit den Scheitelhaaren,", "tokens": ["Da", "gibt", "es", "Frau\u00b7en", "mit", "den", "Schei\u00b7tel\u00b7haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "gepunztes Silber auf dem falschen Busen,", "tokens": ["ge\u00b7punz\u00b7tes", "Sil\u00b7ber", "auf", "dem", "fal\u00b7schen", "Bu\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "teils im Reformkleid, teils in Eigenblusen,", "tokens": ["teils", "im", "Re\u00b7form\u00b7kleid", ",", "teils", "in", "Ei\u00b7gen\u00b7blu\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "ADV", "APPR", "NN", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.8": {"text": "die einmal \u2013 ach, wie weit! \u2013 fast reinlich waren", "tokens": ["die", "ein\u00b7mal", "\u2013", "ach", ",", "wie", "weit", "!", "\u2013", "fast", "rein\u00b7lich", "wa\u00b7ren"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADV", "$(", "XY", "$,", "PWAV", "ADJD", "$.", "$(", "ADV", "ADJD", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "(jetzt dunkelwei\u00df).", "tokens": ["(", "jetzt", "dun\u00b7kel\u00b7wei\u00df", ")", "."], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADJD", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Bei Sturm und Regen", "tokens": ["Bei", "Sturm", "und", "Re\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.11": {"text": "gehn diese gern durch Wald und Flur allein,", "tokens": ["gehn", "die\u00b7se", "gern", "durch", "Wald", "und", "Flur", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "ADV", "APPR", "NN", "KON", "NN", "ADV", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.12": {"text": "das Lodenh\u00fctchen keck auf einem Ohre,", "tokens": ["das", "Lo\u00b7den\u00b7h\u00fct\u00b7chen", "keck", "auf", "ei\u00b7nem", "Oh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "und sprechen mit sich selbst und mit Tagore . . .", "tokens": ["und", "spre\u00b7chen", "mit", "sich", "selbst", "und", "mit", "Ta\u00b7go\u00b7re", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "PRF", "ADV", "KON", "APPR", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.15": {"text": "Sie sagen Feuilletons, eh man sie legt.", "tokens": ["Sie", "sa\u00b7gen", "Feu\u00b7il\u00b7le\u00b7tons", ",", "eh", "man", "sie", "legt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "$,", "KOUS", "PIS", "PPER", "VVFIN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Sie sind sehr edel.", "tokens": ["Sie", "sind", "sehr", "e\u00b7del", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.17": {"text": "Aber nicht gepflegt.", "tokens": ["A\u00b7ber", "nicht", "ge\u00b7pflegt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "Da gibt es solche, unten rum aus Seide,", "tokens": ["Da", "gibt", "es", "sol\u00b7che", ",", "un\u00b7ten", "rum", "aus", "Sei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "$,", "ADV", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "im samtnen Mantel mit dem Waschb\u00e4rkragen \u2013", "tokens": ["im", "samt\u00b7nen", "Man\u00b7tel", "mit", "dem", "Waschb\u00e4r\u00b7kra\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "nach ihren Eltern mu\u00dft du sie nicht fragen.", "tokens": ["nach", "ih\u00b7ren", "El\u00b7tern", "mu\u00dft", "du", "sie", "nicht", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "PPER", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sie ist euch treu \u2013 und so liebt ihr drei beide.", "tokens": ["Sie", "ist", "euch", "treu", "\u2013", "und", "so", "liebt", "ihr", "drei", "bei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "$(", "KON", "ADV", "VVFIN", "PPER", "CARD", "PIS", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Gro\u00df ausgehn nennt der Fachmann dein Getue.", "tokens": ["Gro\u00df", "aus\u00b7gehn", "nennt", "der", "Fach\u00b7mann", "dein", "Ge\u00b7tue", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVIZU", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "F\u00fchr sie ins Kino, ins Theater ein!", "tokens": ["F\u00fchr", "sie", "ins", "Ki\u00b7no", ",", "ins", "The\u00b7a\u00b7ter", "ein", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "$,", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Sie tanzt den neusten Schritt, kennt alle Paare,", "tokens": ["Sie", "tanzt", "den", "neus\u00b7ten", "Schritt", ",", "kennt", "al\u00b7le", "Paa\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "hat jeden Monat frischgef\u00e4rbte Haare . . .", "tokens": ["hat", "je\u00b7den", "Mo\u00b7nat", "frischge\u00b7f\u00e4rb\u00b7te", "Haa\u00b7re", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VAFIN", "PIAT", "NN", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Sie spricht nicht viel.", "tokens": ["Sie", "spricht", "nicht", "viel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.11": {"text": "Doch was sie spricht, ist Kitt.", "tokens": ["Doch", "was", "sie", "spricht", ",", "ist", "Kitt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "Und sie nimmt alle s\u00fc\u00dfen Ecken mit.", "tokens": ["Und", "sie", "nimmt", "al\u00b7le", "s\u00fc\u00b7\u00dfen", "E\u00b7cken", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIAT", "ADJA", "NN", "PTKVZ", "$."], "meter": "-++--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Willst du die J\u00fcngerin Thaliens k\u00fcren?", "tokens": ["Willst", "du", "die", "J\u00fcn\u00b7ge\u00b7rin", "Tha\u00b7li\u00b7ens", "k\u00fc\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "NN", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Sie offenbart, wenn sie mit dir im Bund ist,", "tokens": ["Sie", "of\u00b7fen\u00b7bart", ",", "wenn", "sie", "mit", "dir", "im", "Bund", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "APPR", "PPER", "APPRART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "was ihr Direktor f\u00fcr ein Schweinehund ist:", "tokens": ["was", "ihr", "Di\u00b7rek\u00b7tor", "f\u00fcr", "ein", "Schwei\u00b7ne\u00b7hund", "ist", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "er wollt sie alle in Versuchung f\u00fchren \u2013", "tokens": ["er", "wollt", "sie", "al\u00b7le", "in", "Ver\u00b7su\u00b7chung", "f\u00fch\u00b7ren", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Das t\u00e4t sie nie. (Fast nie.)", "tokens": ["Das", "t\u00e4t", "sie", "nie", ".", "(", "Fast", "nie", ".", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "$.", "$(", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Es rinnt die Rede:", "tokens": ["Es", "rinnt", "die", "Re\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Von Proben, Premerieen, Klatscherein \u2013", "tokens": ["Von", "Pro\u00b7ben", ",", "Pre\u00b7me\u00b7ri\u00b7e\u00b7en", ",", "Klat\u00b7sche\u00b7rein", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "sie meistere Spiel und Sprache wie nur wenige,", "tokens": ["sie", "meis\u00b7te\u00b7re", "Spiel", "und", "Spra\u00b7che", "wie", "nur", "we\u00b7ni\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "NN", "KON", "NN", "KOKOM", "ADV", "PIS", "$,"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "sie spiele Olala und Iphigenie . . .", "tokens": ["sie", "spie\u00b7le", "O\u00b7la\u00b7la", "und", "Ip\u00b7hi\u00b7ge\u00b7nie", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "NE", "KON", "NE", "$.", "$.", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "Soll die es sein \u2013?", "tokens": ["Soll", "die", "es", "sein", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ART", "PPER", "VAINF", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.11": {"text": "Beim Papa Rickelt! S\u00fc\u00df in allen Phasen:", "tokens": ["Beim", "Pa\u00b7pa", "Ri\u00b7ckelt", "!", "S\u00fc\u00df", "in", "al\u00b7len", "Pha\u00b7sen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$.", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Sie liebt.", "tokens": ["Sie", "liebt", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Und bringt dich zeitig untern Rasen.", "tokens": ["Und", "bringt", "dich", "zei\u00b7tig", "un\u00b7tern", "Ra\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "So geh, du Liebeswanderer, von Haus zu Haus.", "tokens": ["So", "geh", ",", "du", "Lie\u00b7bes\u00b7wan\u00b7de\u00b7rer", ",", "von", "Haus", "zu", "Haus", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PPER", "NN", "$,", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Berlin ist gro\u00df.", "tokens": ["Ber\u00b7lin", "ist", "gro\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Nun such dir eine aus!", "tokens": ["Nun", "such", "dir", "ei\u00b7ne", "aus", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}