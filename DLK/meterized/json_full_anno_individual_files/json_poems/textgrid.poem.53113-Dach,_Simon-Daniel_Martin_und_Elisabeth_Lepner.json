{"textgrid.poem.53113": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "Daniel Martin und Elisabeth Lepner", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der, an dem wir gantz verzaget,", "tokens": ["Der", ",", "an", "dem", "wir", "gantz", "ver\u00b7za\u00b7get", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "APPR", "PRELS", "PPER", "ADV", "VVFIN", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Hat es endlich noch gewaget,", "tokens": ["Hat", "es", "end\u00b7lich", "noch", "ge\u00b7wa\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd vngeschewet", "tokens": ["Vnd", "vn\u00b7ge\u00b7sche\u00b7wet"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Sich jetzt befreyet.", "tokens": ["Sich", "jetzt", "be\u00b7fre\u00b7yet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVFIN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.2": {"line.1": {"text": "Amor hat auff Ihn geschossen,", "tokens": ["A\u00b7mor", "hat", "auff", "Ihn", "ge\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "APPR", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bi\u00df es Ihn zuletzt verdrossen,", "tokens": ["Bi\u00df", "es", "Ihn", "zu\u00b7letzt", "ver\u00b7dros\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist mit dem Bogen", "tokens": ["Ist", "mit", "dem", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Kahl abgezogen.", "tokens": ["Kahl", "ab\u00b7ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Venus sprach: H\u00f6rt auff, jhr Kertzen,", "tokens": ["Ve\u00b7nus", "sprach", ":", "H\u00f6rt", "auff", ",", "jhr", "Kert\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "VVIMP", "PTKVZ", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Er empfindet keine Schmertzen,", "tokens": ["Er", "emp\u00b7fin\u00b7det", "kei\u00b7ne", "Schmert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist (wie ich meine)", "tokens": ["Ist", "(", "wie", "ich", "mei\u00b7ne", ")"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$(", "PWAV", "PPER", "PPOSAT", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Gleich Staal' vnd Steine.", "tokens": ["Gleich", "Staal'", "vnd", "Stei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Sagt, wer hat Ihn je gesehen", "tokens": ["Sagt", ",", "wer", "hat", "Ihn", "je", "ge\u00b7se\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PWS", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo vor einem M\u00e4gdchen flehen,", "tokens": ["Wo", "vor", "ei\u00b7nem", "M\u00e4gd\u00b7chen", "fle\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das Ihn zu lieben", "tokens": ["Das", "Ihn", "zu", "lie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "H\u00e4tt' angetrieben?", "tokens": ["H\u00e4tt'", "an\u00b7ge\u00b7trie\u00b7ben", "?"], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Tieger m\u00f6chte man noch z\u00e4hmen,", "tokens": ["Tie\u00b7ger", "m\u00f6ch\u00b7te", "man", "noch", "z\u00e4h\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Vnd den Grimm' auch B\u00e4hren nehmen,", "tokens": ["Vnd", "den", "Grimm'", "auch", "B\u00e4h\u00b7ren", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ja L\u00f6wen Rachen", "tokens": ["Ja", "L\u00f6\u00b7wen", "Ra\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["PTKANT", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sanfftm\u00fchtig machen.", "tokens": ["Sanfft\u00b7m\u00fch\u00b7tig", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Diesen \u00fcberreden wollen,", "tokens": ["Die\u00b7sen", "\u00fc\u00b7berr\u00b7e\u00b7den", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df er h\u00e4tte tantzen sollen,", "tokens": ["Da\u00df", "er", "h\u00e4t\u00b7te", "tant\u00b7zen", "sol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VAFIN", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wahr, was auff Erden", "tokens": ["Wahr", ",", "was", "auff", "Er\u00b7den"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "PRELS", "APPR", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Nicht kuntte werden.", "tokens": ["Nicht", "kunt\u00b7te", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "VAINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "Mag was s\u00fcssers auch entstehen,", "tokens": ["Mag", "was", "s\u00fcs\u00b7sers", "auch", "ent\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PWS", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als im Reyen fr\u00f6lich gehen,", "tokens": ["Als", "im", "Re\u00b7yen", "fr\u00f6\u00b7lich", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPRART", "NN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd eine f\u00fchren,", "tokens": ["Vnd", "ei\u00b7ne", "f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ART", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Die Ihn kan zieren?", "tokens": ["Die", "Ihn", "kan", "zie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Wenn ein Tantz, der nur aus Pohlen", "tokens": ["Wenn", "ein", "Tantz", ",", "der", "nur", "aus", "Poh\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommen ist, wird auff Violen", "tokens": ["Kom\u00b7men", "ist", ",", "wird", "auff", "Vi\u00b7o\u00b7len"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "VAFIN", "$,", "VAFIN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Recht wol gemachet,", "tokens": ["Recht", "wol", "ge\u00b7ma\u00b7chet", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "VVPP", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Da\u00df alles lachet?", "tokens": ["Da\u00df", "al\u00b7les", "la\u00b7chet", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Wenn man h\u00f6ret die Schalmeyen,", "tokens": ["Wenn", "man", "h\u00f6\u00b7ret", "die", "Schal\u00b7me\u00b7yen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Die man braucht im ersten Reyen,", "tokens": ["Die", "man", "braucht", "im", "ers\u00b7ten", "Re\u00b7yen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+---", "measure": "unknown.measure.tri"}, "line.3": {"text": "Bald auch die Fl\u00f6hten", "tokens": ["Bald", "auch", "die", "Fl\u00f6h\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sampt den Cornehten?", "tokens": ["Sampt", "den", "Cor\u00b7neh\u00b7ten", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.10": {"line.1": {"text": "Wenn der Stort nun prangt f\u00fcr allen,", "tokens": ["Wenn", "der", "Stort", "nun", "prangt", "f\u00fcr", "al\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVFIN", "APPR", "PIAT", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df die Hertzen m\u00fcssen wallen,", "tokens": ["Da\u00df", "die", "Hert\u00b7zen", "m\u00fcs\u00b7sen", "wal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd recht zu leben", "tokens": ["Vnd", "recht", "zu", "le\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Erst dan anheben?", "tokens": ["Erst", "dan", "an\u00b7he\u00b7ben", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Hie wird Anla\u00df her genommen,", "tokens": ["Hie", "wird", "An\u00b7la\u00df", "her", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "APZR", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "An das liebste Hertz zu kommen.", "tokens": ["An", "das", "liebs\u00b7te", "Hertz", "zu", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo wohnen Gr\u00fcsse?", "tokens": ["Wo", "woh\u00b7nen", "Gr\u00fcs\u00b7se", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Wo Schertz' vnd K\u00fcsse?", "tokens": ["Wo", "Schertz'", "vnd", "K\u00fcs\u00b7se", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Wo Gespr\u00e4che von der Liebe?", "tokens": ["Wo", "Ge\u00b7spr\u00e4\u00b7che", "von", "der", "Lie\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo das meiste, so ich \u00fcbe?", "tokens": ["Wo", "das", "meis\u00b7te", ",", "so", "ich", "\u00fc\u00b7be", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PDS", "VVFIN", "$,", "ADV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo H\u00e4ndedr\u00fccken?", "tokens": ["Wo", "H\u00e4n\u00b7de\u00b7dr\u00fc\u00b7cken", "?"], "token_info": ["word", "word", "punct"], "pos": ["PWAV", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Wo sich anblicken?", "tokens": ["Wo", "sich", "an\u00b7bli\u00b7cken", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.13": {"line.1": {"text": "Wo gelohst man W\u00fcrtz vnd Kr\u00e4ntze?", "tokens": ["Wo", "ge\u00b7lohst", "man", "W\u00fcrtz", "vnd", "Kr\u00e4nt\u00b7ze", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hie erst, da man heget T\u00e4ntze,", "tokens": ["Hie", "erst", ",", "da", "man", "he\u00b7get", "T\u00e4nt\u00b7ze", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PIS", "VVFIN", "NN", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Hie kehrt das Leiden", "tokens": ["Hie", "kehrt", "das", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sich gantz in Frewden.", "tokens": ["Sich", "gantz", "in", "Frew\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "APPR", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.14": {"line.1": {"text": "Alte, die von fern her sehen,", "tokens": ["Al\u00b7te", ",", "die", "von", "fern", "her", "se\u00b7hen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "ADJD", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "W\u00fcnschen: h\u00e4tten wir nur zehen", "tokens": ["W\u00fcn\u00b7schen", ":", "h\u00e4t\u00b7ten", "wir", "nur", "ze\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "VAFIN", "PPER", "ADV", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jahrchen zur\u00fccke,", "tokens": ["Jahr\u00b7chen", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wir argen Stricke!", "tokens": ["Wir", "ar\u00b7gen", "Stri\u00b7cke", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.15": {"line.1": {"text": "Dieser nur war nicht zu zwingen,", "tokens": ["Die\u00b7ser", "nur", "war", "nicht", "zu", "zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wahr an keinen Tantz zu bringen,", "tokens": ["Wahr", "an", "kei\u00b7nen", "Tantz", "zu", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nichts kuntt' Ihn fangen;", "tokens": ["Nichts", "kuntt'", "Ihn", "fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Nicht rohte Wangen,", "tokens": ["Nicht", "roh\u00b7te", "Wan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.16": {"line.1": {"text": "Nicht der weissen Stirnen Pflaster,", "tokens": ["Nicht", "der", "weis\u00b7sen", "Stir\u00b7nen", "Pflas\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ja nicht H\u00e4nd' aus Alabaster,", "tokens": ["Ja", "nicht", "H\u00e4nd'", "aus", "A\u00b7la\u00b7bas\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PTKNEG", "NN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nicht Gold der Hare,", "tokens": ["Nicht", "Gold", "der", "Ha\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "ART", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Nicht andre Wahre,", "tokens": ["Nicht", "and\u00b7re", "Wah\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.17": {"line.1": {"text": "Nicht der Zungen Milch vnd Reben,", "tokens": ["Nicht", "der", "Zun\u00b7gen", "Milch", "vnd", "Re\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nicht der Sitten Art vnd Leben,", "tokens": ["Nicht", "der", "Sit\u00b7ten", "Art", "vnd", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Noch, was f\u00fcr Sachen", "tokens": ["Noch", ",", "was", "f\u00fcr", "Sa\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "PRELS", "APPR", "NN"], "meter": "++-+-", "measure": "iambic.di"}, "line.4": {"text": "Verliebt sonst machen.", "tokens": ["Ver\u00b7liebt", "sonst", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.18": {"line.1": {"text": "Diesen soltt' ich mich bem\u00fchen", "tokens": ["Die\u00b7sen", "soltt'", "ich", "mich", "be\u00b7m\u00fc\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "PPER", "PRF", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Endlich an mein Joch zu ziehen,", "tokens": ["End\u00b7lich", "an", "mein", "Joch", "zu", "zie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "da\u00df Er auff Erden", "tokens": ["da\u00df", "Er", "auff", "Er\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Soltt' ehlich werden?", "tokens": ["Soltt'", "eh\u00b7lich", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "ADJD", "VAINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.19": {"line.1": {"text": "Nein, zerbrich, mein Kind, die Pfeile,", "tokens": ["Nein", ",", "zer\u00b7brich", ",", "mein", "Kind", ",", "die", "Pfei\u00b7le", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADJD", "$,", "PPOSAT", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mach dich auff die Flucht vnd eile,", "tokens": ["Mach", "dich", "auff", "die", "Flucht", "vnd", "ei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hat doch kein Possen", "tokens": ["Hat", "doch", "kein", "Pos\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Mich so verdrossen!", "tokens": ["Mich", "so", "ver\u00b7dros\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.20": {"line.1": {"text": "Die\u00df sprach aus ergrimmtem Hertzen", "tokens": ["Die\u00df", "sprach", "aus", "er\u00b7grimm\u00b7tem", "Hert\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Venus, vnd schwang jhre Kertzen,", "tokens": ["Ve\u00b7nus", ",", "vnd", "schwang", "jhre", "Kert\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KON", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df sie im schwingen", "tokens": ["Da\u00df", "sie", "im", "schwin\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPRART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Auch stracks au\u00dfgiengen.", "tokens": ["Auch", "stracks", "au\u00df\u00b7gi\u00b7en\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.21": {"line.1": {"text": "Amorn must' es auch verdriessen,", "tokens": ["A\u00b7morn", "must'", "es", "auch", "ver\u00b7dries\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Trat den K\u00f6cher-Zeug mit F\u00fcssen,", "tokens": ["Trat", "den", "K\u00f6\u00b7cher\u00b7Zeug", "mit", "F\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der muste brechen", "tokens": ["Der", "mus\u00b7te", "bre\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VMFIN", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Vnd so jhn r\u00e4chen.", "tokens": ["Vnd", "so", "jhn", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.22": {"line.1": {"text": "Amor, du ergrimmst vergebens,", "tokens": ["A\u00b7mor", ",", "du", "er\u00b7grimmst", "ver\u00b7ge\u00b7bens", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VVFIN", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Du auch, G\u00f6ttinn dieses Lebens,", "tokens": ["Du", "auch", ",", "G\u00f6t\u00b7tinn", "die\u00b7ses", "Le\u00b7bens", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "NN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Venus. Im Hertzen", "tokens": ["Ve\u00b7nus", ".", "Im", "Hert\u00b7zen"], "token_info": ["word", "punct", "word", "word"], "pos": ["NE", "$.", "APPRART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "F\u00fchlt Er schon Schmertzen.", "tokens": ["F\u00fchlt", "Er", "schon", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.23": {"line.1": {"text": "Seht, die Artigheit Elisen", "tokens": ["Seht", ",", "die", "Ar\u00b7tig\u00b7heit", "E\u00b7li\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVFIN", "$,", "ART", "NN", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat sich st\u00e4rcker noch erwiesen", "tokens": ["Hat", "sich", "st\u00e4r\u00b7cker", "noch", "er\u00b7wie\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PRF", "ADJD", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als ewre Waffen,", "tokens": ["Als", "ew\u00b7re", "Waf\u00b7fen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Die nichts hie schaffen.", "tokens": ["Die", "nichts", "hie", "schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.24": {"line.1": {"text": "Ihrer schwartzen Augen Sonnen", "tokens": ["Ih\u00b7rer", "schwart\u00b7zen", "Au\u00b7gen", "Son\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Haben Pfeil' vnd Brunst gewonnen,", "tokens": ["Ha\u00b7ben", "Pfeil'", "vnd", "Brunst", "ge\u00b7won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Brunst, die Ihn schl\u00e4get", "tokens": ["Brunst", ",", "die", "Ihn", "schl\u00e4\u00b7get"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "PRELS", "PPER", "VVFIN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Vnd nieder leget.", "tokens": ["Vnd", "nie\u00b7der", "le\u00b7get", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKVZ", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.25": {"line.1": {"text": "Br\u00e4utlein, du kanst triumphiren,", "tokens": ["Br\u00e4ut\u00b7lein", ",", "du", "kanst", "tri\u00b7um\u00b7phi\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dir mu\u00df aller Prei\u00df geb\u00fchren,", "tokens": ["Dir", "mu\u00df", "al\u00b7ler", "Prei\u00df", "ge\u00b7b\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der Venus Sohne", "tokens": ["Der", "Ve\u00b7nus", "Soh\u00b7ne"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Setz auff die Krohne!", "tokens": ["Setz", "auff", "die", "Kroh\u00b7ne", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.26": {"line.1": {"text": "Ihr, Herr Schwager, sucht zusammen,", "tokens": ["Ihr", ",", "Herr", "Schwa\u00b7ger", ",", "sucht", "zu\u00b7sam\u00b7men", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "$,", "NN", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Was Ihr jrgends wisst von Flammen,", "tokens": ["Was", "Ihr", "jr\u00b7gends", "wisst", "von", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihr habt zu sorgen", "tokens": ["Ihr", "habt", "zu", "sor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Heut oder morgen,", "tokens": ["Heut", "o\u00b7der", "mor\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.27": {"line.1": {"text": "Ob nicht etwa Venus Kertzen,", "tokens": ["Ob", "nicht", "et\u00b7wa", "Ve\u00b7nus", "Kert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKNEG", "ADV", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Welche sie verlescht f\u00fcr Schmertzen,", "tokens": ["Wel\u00b7che", "sie", "ver\u00b7lescht", "f\u00fcr", "Schmert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "(das wir nicht gl\u00e4uben)", "tokens": ["(", "das", "wir", "nicht", "gl\u00e4u\u00b7ben", ")"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PRELS", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Verloschen bleiben.", "tokens": ["Ver\u00b7lo\u00b7schen", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.28": {"line.1": {"text": "Der, an dem wir gantz verzaget,", "tokens": ["Der", ",", "an", "dem", "wir", "gantz", "ver\u00b7za\u00b7get", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "APPR", "PRELS", "PPER", "ADV", "VVFIN", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Hat es endlich noch gewaget,", "tokens": ["Hat", "es", "end\u00b7lich", "noch", "ge\u00b7wa\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd vngeschewet", "tokens": ["Vnd", "vn\u00b7ge\u00b7sche\u00b7wet"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Sich jetzt befreyet.", "tokens": ["Sich", "jetzt", "be\u00b7fre\u00b7yet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVFIN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.29": {"line.1": {"text": "Amor hat auff Ihn geschossen,", "tokens": ["A\u00b7mor", "hat", "auff", "Ihn", "ge\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "APPR", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bi\u00df es Ihn zuletzt verdrossen,", "tokens": ["Bi\u00df", "es", "Ihn", "zu\u00b7letzt", "ver\u00b7dros\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist mit dem Bogen", "tokens": ["Ist", "mit", "dem", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Kahl abgezogen.", "tokens": ["Kahl", "ab\u00b7ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.30": {"line.1": {"text": "Venus sprach: H\u00f6rt auff, jhr Kertzen,", "tokens": ["Ve\u00b7nus", "sprach", ":", "H\u00f6rt", "auff", ",", "jhr", "Kert\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$.", "VVIMP", "PTKVZ", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Er empfindet keine Schmertzen,", "tokens": ["Er", "emp\u00b7fin\u00b7det", "kei\u00b7ne", "Schmert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist (wie ich meine)", "tokens": ["Ist", "(", "wie", "ich", "mei\u00b7ne", ")"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$(", "PWAV", "PPER", "PPOSAT", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Gleich Staal' vnd Steine.", "tokens": ["Gleich", "Staal'", "vnd", "Stei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.31": {"line.1": {"text": "Sagt, wer hat Ihn je gesehen", "tokens": ["Sagt", ",", "wer", "hat", "Ihn", "je", "ge\u00b7se\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PWS", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo vor einem M\u00e4gdchen flehen,", "tokens": ["Wo", "vor", "ei\u00b7nem", "M\u00e4gd\u00b7chen", "fle\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das Ihn zu lieben", "tokens": ["Das", "Ihn", "zu", "lie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "H\u00e4tt' angetrieben?", "tokens": ["H\u00e4tt'", "an\u00b7ge\u00b7trie\u00b7ben", "?"], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.32": {"line.1": {"text": "Tieger m\u00f6chte man noch z\u00e4hmen,", "tokens": ["Tie\u00b7ger", "m\u00f6ch\u00b7te", "man", "noch", "z\u00e4h\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Vnd den Grimm' auch B\u00e4hren nehmen,", "tokens": ["Vnd", "den", "Grimm'", "auch", "B\u00e4h\u00b7ren", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ja L\u00f6wen Rachen", "tokens": ["Ja", "L\u00f6\u00b7wen", "Ra\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["PTKANT", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sanfftm\u00fchtig machen.", "tokens": ["Sanfft\u00b7m\u00fch\u00b7tig", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.33": {"line.1": {"text": "Diesen \u00fcberreden wollen,", "tokens": ["Die\u00b7sen", "\u00fc\u00b7berr\u00b7e\u00b7den", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df er h\u00e4tte tantzen sollen,", "tokens": ["Da\u00df", "er", "h\u00e4t\u00b7te", "tant\u00b7zen", "sol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VAFIN", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wahr, was auff Erden", "tokens": ["Wahr", ",", "was", "auff", "Er\u00b7den"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "PRELS", "APPR", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Nicht kuntte werden.", "tokens": ["Nicht", "kunt\u00b7te", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "VAINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.34": {"line.1": {"text": "Mag was s\u00fcssers auch entstehen,", "tokens": ["Mag", "was", "s\u00fcs\u00b7sers", "auch", "ent\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PWS", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als im Reyen fr\u00f6lich gehen,", "tokens": ["Als", "im", "Re\u00b7yen", "fr\u00f6\u00b7lich", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPRART", "NN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd eine f\u00fchren,", "tokens": ["Vnd", "ei\u00b7ne", "f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ART", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Die Ihn kan zieren?", "tokens": ["Die", "Ihn", "kan", "zie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.35": {"line.1": {"text": "Wenn ein Tantz, der nur aus Pohlen", "tokens": ["Wenn", "ein", "Tantz", ",", "der", "nur", "aus", "Poh\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommen ist, wird auff Violen", "tokens": ["Kom\u00b7men", "ist", ",", "wird", "auff", "Vi\u00b7o\u00b7len"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "VAFIN", "$,", "VAFIN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Recht wol gemachet,", "tokens": ["Recht", "wol", "ge\u00b7ma\u00b7chet", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "VVPP", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Da\u00df alles lachet?", "tokens": ["Da\u00df", "al\u00b7les", "la\u00b7chet", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.36": {"line.1": {"text": "Wenn man h\u00f6ret die Schalmeyen,", "tokens": ["Wenn", "man", "h\u00f6\u00b7ret", "die", "Schal\u00b7me\u00b7yen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Die man braucht im ersten Reyen,", "tokens": ["Die", "man", "braucht", "im", "ers\u00b7ten", "Re\u00b7yen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+---", "measure": "unknown.measure.tri"}, "line.3": {"text": "Bald auch die Fl\u00f6hten", "tokens": ["Bald", "auch", "die", "Fl\u00f6h\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sampt den Cornehten?", "tokens": ["Sampt", "den", "Cor\u00b7neh\u00b7ten", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.37": {"line.1": {"text": "Wenn der Stort nun prangt f\u00fcr allen,", "tokens": ["Wenn", "der", "Stort", "nun", "prangt", "f\u00fcr", "al\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVFIN", "APPR", "PIAT", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df die Hertzen m\u00fcssen wallen,", "tokens": ["Da\u00df", "die", "Hert\u00b7zen", "m\u00fcs\u00b7sen", "wal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd recht zu leben", "tokens": ["Vnd", "recht", "zu", "le\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Erst dan anheben?", "tokens": ["Erst", "dan", "an\u00b7he\u00b7ben", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.38": {"line.1": {"text": "Hie wird Anla\u00df her genommen,", "tokens": ["Hie", "wird", "An\u00b7la\u00df", "her", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "APZR", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "An das liebste Hertz zu kommen.", "tokens": ["An", "das", "liebs\u00b7te", "Hertz", "zu", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo wohnen Gr\u00fcsse?", "tokens": ["Wo", "woh\u00b7nen", "Gr\u00fcs\u00b7se", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Wo Schertz' vnd K\u00fcsse?", "tokens": ["Wo", "Schertz'", "vnd", "K\u00fcs\u00b7se", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.39": {"line.1": {"text": "Wo Gespr\u00e4che von der Liebe?", "tokens": ["Wo", "Ge\u00b7spr\u00e4\u00b7che", "von", "der", "Lie\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo das meiste, so ich \u00fcbe?", "tokens": ["Wo", "das", "meis\u00b7te", ",", "so", "ich", "\u00fc\u00b7be", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PDS", "VVFIN", "$,", "ADV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo H\u00e4ndedr\u00fccken?", "tokens": ["Wo", "H\u00e4n\u00b7de\u00b7dr\u00fc\u00b7cken", "?"], "token_info": ["word", "word", "punct"], "pos": ["PWAV", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Wo sich anblicken?", "tokens": ["Wo", "sich", "an\u00b7bli\u00b7cken", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.40": {"line.1": {"text": "Wo gelohst man W\u00fcrtz vnd Kr\u00e4ntze?", "tokens": ["Wo", "ge\u00b7lohst", "man", "W\u00fcrtz", "vnd", "Kr\u00e4nt\u00b7ze", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hie erst, da man heget T\u00e4ntze,", "tokens": ["Hie", "erst", ",", "da", "man", "he\u00b7get", "T\u00e4nt\u00b7ze", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PIS", "VVFIN", "NN", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Hie kehrt das Leiden", "tokens": ["Hie", "kehrt", "das", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Sich gantz in Frewden.", "tokens": ["Sich", "gantz", "in", "Frew\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "APPR", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.41": {"line.1": {"text": "Alte, die von fern her sehen,", "tokens": ["Al\u00b7te", ",", "die", "von", "fern", "her", "se\u00b7hen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "ADJD", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "W\u00fcnschen: h\u00e4tten wir nur zehen", "tokens": ["W\u00fcn\u00b7schen", ":", "h\u00e4t\u00b7ten", "wir", "nur", "ze\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "VAFIN", "PPER", "ADV", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jahrchen zur\u00fccke,", "tokens": ["Jahr\u00b7chen", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wir argen Stricke!", "tokens": ["Wir", "ar\u00b7gen", "Stri\u00b7cke", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.42": {"line.1": {"text": "Dieser nur war nicht zu zwingen,", "tokens": ["Die\u00b7ser", "nur", "war", "nicht", "zu", "zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wahr an keinen Tantz zu bringen,", "tokens": ["Wahr", "an", "kei\u00b7nen", "Tantz", "zu", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nichts kuntt' Ihn fangen;", "tokens": ["Nichts", "kuntt'", "Ihn", "fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Nicht rohte Wangen,", "tokens": ["Nicht", "roh\u00b7te", "Wan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.43": {"line.1": {"text": "Nicht der weissen Stirnen Pflaster,", "tokens": ["Nicht", "der", "weis\u00b7sen", "Stir\u00b7nen", "Pflas\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ja nicht H\u00e4nd' aus Alabaster,", "tokens": ["Ja", "nicht", "H\u00e4nd'", "aus", "A\u00b7la\u00b7bas\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PTKNEG", "NN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nicht Gold der Hare,", "tokens": ["Nicht", "Gold", "der", "Ha\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "ART", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Nicht andre Wahre,", "tokens": ["Nicht", "and\u00b7re", "Wah\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.44": {"line.1": {"text": "Nicht der Zungen Milch vnd Reben,", "tokens": ["Nicht", "der", "Zun\u00b7gen", "Milch", "vnd", "Re\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nicht der Sitten Art vnd Leben,", "tokens": ["Nicht", "der", "Sit\u00b7ten", "Art", "vnd", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Noch, was f\u00fcr Sachen", "tokens": ["Noch", ",", "was", "f\u00fcr", "Sa\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "PRELS", "APPR", "NN"], "meter": "++-+-", "measure": "iambic.di"}, "line.4": {"text": "Verliebt sonst machen.", "tokens": ["Ver\u00b7liebt", "sonst", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.45": {"line.1": {"text": "Diesen soltt' ich mich bem\u00fchen", "tokens": ["Die\u00b7sen", "soltt'", "ich", "mich", "be\u00b7m\u00fc\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "PPER", "PRF", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Endlich an mein Joch zu ziehen,", "tokens": ["End\u00b7lich", "an", "mein", "Joch", "zu", "zie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "da\u00df Er auff Erden", "tokens": ["da\u00df", "Er", "auff", "Er\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Soltt' ehlich werden?", "tokens": ["Soltt'", "eh\u00b7lich", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "ADJD", "VAINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.46": {"line.1": {"text": "Nein, zerbrich, mein Kind, die Pfeile,", "tokens": ["Nein", ",", "zer\u00b7brich", ",", "mein", "Kind", ",", "die", "Pfei\u00b7le", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADJD", "$,", "PPOSAT", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mach dich auff die Flucht vnd eile,", "tokens": ["Mach", "dich", "auff", "die", "Flucht", "vnd", "ei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hat doch kein Possen", "tokens": ["Hat", "doch", "kein", "Pos\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Mich so verdrossen!", "tokens": ["Mich", "so", "ver\u00b7dros\u00b7sen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.47": {"line.1": {"text": "Die\u00df sprach aus ergrimmtem Hertzen", "tokens": ["Die\u00df", "sprach", "aus", "er\u00b7grimm\u00b7tem", "Hert\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Venus, vnd schwang jhre Kertzen,", "tokens": ["Ve\u00b7nus", ",", "vnd", "schwang", "jhre", "Kert\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KON", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df sie im schwingen", "tokens": ["Da\u00df", "sie", "im", "schwin\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPRART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Auch stracks au\u00dfgiengen.", "tokens": ["Auch", "stracks", "au\u00df\u00b7gi\u00b7en\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.48": {"line.1": {"text": "Amorn must' es auch verdriessen,", "tokens": ["A\u00b7morn", "must'", "es", "auch", "ver\u00b7dries\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Trat den K\u00f6cher-Zeug mit F\u00fcssen,", "tokens": ["Trat", "den", "K\u00f6\u00b7cher\u00b7Zeug", "mit", "F\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der muste brechen", "tokens": ["Der", "mus\u00b7te", "bre\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VMFIN", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Vnd so jhn r\u00e4chen.", "tokens": ["Vnd", "so", "jhn", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.49": {"line.1": {"text": "Amor, du ergrimmst vergebens,", "tokens": ["A\u00b7mor", ",", "du", "er\u00b7grimmst", "ver\u00b7ge\u00b7bens", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VVFIN", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Du auch, G\u00f6ttinn dieses Lebens,", "tokens": ["Du", "auch", ",", "G\u00f6t\u00b7tinn", "die\u00b7ses", "Le\u00b7bens", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "NN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Venus. Im Hertzen", "tokens": ["Ve\u00b7nus", ".", "Im", "Hert\u00b7zen"], "token_info": ["word", "punct", "word", "word"], "pos": ["NE", "$.", "APPRART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "F\u00fchlt Er schon Schmertzen.", "tokens": ["F\u00fchlt", "Er", "schon", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.50": {"line.1": {"text": "Seht, die Artigheit Elisen", "tokens": ["Seht", ",", "die", "Ar\u00b7tig\u00b7heit", "E\u00b7li\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVFIN", "$,", "ART", "NN", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat sich st\u00e4rcker noch erwiesen", "tokens": ["Hat", "sich", "st\u00e4r\u00b7cker", "noch", "er\u00b7wie\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PRF", "ADJD", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als ewre Waffen,", "tokens": ["Als", "ew\u00b7re", "Waf\u00b7fen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Die nichts hie schaffen.", "tokens": ["Die", "nichts", "hie", "schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.51": {"line.1": {"text": "Ihrer schwartzen Augen Sonnen", "tokens": ["Ih\u00b7rer", "schwart\u00b7zen", "Au\u00b7gen", "Son\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Haben Pfeil' vnd Brunst gewonnen,", "tokens": ["Ha\u00b7ben", "Pfeil'", "vnd", "Brunst", "ge\u00b7won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Brunst, die Ihn schl\u00e4get", "tokens": ["Brunst", ",", "die", "Ihn", "schl\u00e4\u00b7get"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "PRELS", "PPER", "VVFIN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Vnd nieder leget.", "tokens": ["Vnd", "nie\u00b7der", "le\u00b7get", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PTKVZ", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.52": {"line.1": {"text": "Br\u00e4utlein, du kanst triumphiren,", "tokens": ["Br\u00e4ut\u00b7lein", ",", "du", "kanst", "tri\u00b7um\u00b7phi\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dir mu\u00df aller Prei\u00df geb\u00fchren,", "tokens": ["Dir", "mu\u00df", "al\u00b7ler", "Prei\u00df", "ge\u00b7b\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der Venus Sohne", "tokens": ["Der", "Ve\u00b7nus", "Soh\u00b7ne"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Setz auff die Krohne!", "tokens": ["Setz", "auff", "die", "Kroh\u00b7ne", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.53": {"line.1": {"text": "Ihr, Herr Schwager, sucht zusammen,", "tokens": ["Ihr", ",", "Herr", "Schwa\u00b7ger", ",", "sucht", "zu\u00b7sam\u00b7men", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "$,", "NN", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Was Ihr jrgends wisst von Flammen,", "tokens": ["Was", "Ihr", "jr\u00b7gends", "wisst", "von", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihr habt zu sorgen", "tokens": ["Ihr", "habt", "zu", "sor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Heut oder morgen,", "tokens": ["Heut", "o\u00b7der", "mor\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.54": {"line.1": {"text": "Ob nicht etwa Venus Kertzen,", "tokens": ["Ob", "nicht", "et\u00b7wa", "Ve\u00b7nus", "Kert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKNEG", "ADV", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Welche sie verlescht f\u00fcr Schmertzen,", "tokens": ["Wel\u00b7che", "sie", "ver\u00b7lescht", "f\u00fcr", "Schmert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "(das wir nicht gl\u00e4uben)", "tokens": ["(", "das", "wir", "nicht", "gl\u00e4u\u00b7ben", ")"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PRELS", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Verloschen bleiben.", "tokens": ["Ver\u00b7lo\u00b7schen", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}}}}