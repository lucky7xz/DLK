{"textgrid.poem.46166": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Herren Ernst graven von Mansfeld", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Als oft ich ihn nur sehen kan,", "tokens": ["Als", "oft", "ich", "ihn", "nur", "se\u00b7hen", "kan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so oft kan ich nicht einen man", "tokens": ["so", "oft", "kan", "ich", "nicht", "ei\u00b7nen", "man"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "PTKNEG", "ART", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "zu sehen gestehen:", "tokens": ["zu", "se\u00b7hen", "ge\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "weil under eines menschen schein", "tokens": ["weil", "un\u00b7der", "ei\u00b7nes", "men\u00b7schen", "schein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "mu\u00df ja ein got verborgen sein.", "tokens": ["mu\u00df", "ja", "ein", "got", "ver\u00b7bor\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.2": {"line.1": {"text": "Dan seines hohen geists verstand", "tokens": ["Dan", "sei\u00b7nes", "ho\u00b7hen", "geists", "ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seines sch\u00f6nen leibs wolstand", "tokens": ["und", "sei\u00b7nes", "sch\u00f6\u00b7nen", "leibs", "wol\u00b7stand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "vil ehren bewehren,", "tokens": ["vil", "eh\u00b7ren", "be\u00b7weh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "da\u00df ich wol einen zweifel trag,", "tokens": ["da\u00df", "ich", "wol", "ei\u00b7nen", "zwei\u00b7fel", "trag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "was es doch f\u00fcr ein got sein mag.", "tokens": ["was", "es", "doch", "f\u00fcr", "ein", "got", "sein", "mag."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["PWS", "PPER", "ADV", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Dan der ernst seines angesichts", "tokens": ["Dan", "der", "ernst", "sei\u00b7nes", "an\u00b7ge\u00b7sichts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seine tiefsin des gerichts", "tokens": ["und", "sei\u00b7ne", "tief\u00b7sin", "des", "ge\u00b7richts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "sein dunder mit wunder,", "tokens": ["sein", "dun\u00b7der", "mit", "wun\u00b7der", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "APPR", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "erschrecken den umkreis so sehr,", "tokens": ["er\u00b7schre\u00b7cken", "den", "um\u00b7kreis", "so", "sehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "als ob er Jupiter selbs w\u00e4r.", "tokens": ["als", "ob", "er", "Ju\u00b7pi\u00b7ter", "selbs", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOKOM", "KOUS", "PPER", "NN", "ADV", "VAFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.4": {"line.1": {"text": "Wan aber seine weise wort,", "tokens": ["Wan", "a\u00b7ber", "sei\u00b7ne", "wei\u00b7se", "wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der freindlichkeit und gnaden port,", "tokens": ["der", "freind\u00b7lich\u00b7keit", "und", "gna\u00b7den", "port", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die herzen entschmerzen,", "tokens": ["die", "her\u00b7zen", "ent\u00b7schmer\u00b7zen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so glaubet die bescheidenheit", "tokens": ["so", "glau\u00b7bet", "die", "be\u00b7schei\u00b7den\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "es sei got der wolredenheit.", "tokens": ["es", "sei", "got", "der", "wol\u00b7re\u00b7den\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "ART", "NN", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.5": {"line.1": {"text": "Wan aber sein gerechter grim", "tokens": ["Wan", "a\u00b7ber", "sein", "ge\u00b7rech\u00b7ter", "grim"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den feind mit gro\u00dfer ungest\u00fcm", "tokens": ["den", "feind", "mit", "gro\u00b7\u00dfer", "un\u00b7ge\u00b7st\u00fcm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "verst\u00f6ret verzehret,", "tokens": ["ver\u00b7st\u00f6\u00b7ret", "ver\u00b7zeh\u00b7ret", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "alsdan beweiset seine macht,", "tokens": ["als\u00b7dan", "be\u00b7wei\u00b7set", "sei\u00b7ne", "macht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "da\u00df er der got Mars in der schlacht.", "tokens": ["da\u00df", "er", "der", "got", "Mars", "in", "der", "schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NN", "APPR", "ART", "VVFIN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.6": {"line.1": {"text": "Doch alsbald seiner weisheit zier", "tokens": ["Doch", "als\u00b7bald", "sei\u00b7ner", "weis\u00b7heit", "zier"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mit seiner feder das papier", "tokens": ["mit", "sei\u00b7ner", "fe\u00b7der", "das", "pa\u00b7pier"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ber\u00fchret und zieret,", "tokens": ["be\u00b7r\u00fch\u00b7ret", "und", "zie\u00b7ret", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so lehret uns alsbald sein hirn,", "tokens": ["so", "leh\u00b7ret", "uns", "als\u00b7bald", "sein", "hirn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "da\u00df er des klaren tags gestirn.", "tokens": ["da\u00df", "er", "des", "kla\u00b7ren", "tags", "ge\u00b7stirn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Wan uns dan durch ihn auf einmal", "tokens": ["Wan", "uns", "dan", "durch", "ihn", "auf", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "APPR", "PPER", "APPR", "ADV"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "so viler wundern gro\u00dfe zahl", "tokens": ["so", "vi\u00b7ler", "wun\u00b7dern", "gro\u00b7\u00dfe", "zahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PIAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "begl\u00fccket erquicket,", "tokens": ["be\u00b7gl\u00fc\u00b7cket", "er\u00b7quic\u00b7ket", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so mu\u00df ihn nennen alle welt", "tokens": ["so", "mu\u00df", "ihn", "nen\u00b7nen", "al\u00b7le", "welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "der G\u00f6tter feld mehr, dan Mansfeld.", "tokens": ["der", "G\u00f6t\u00b7ter", "feld", "mehr", ",", "dan", "Mans\u00b7feld", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADV", "$,", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Als oft ich ihn nur sehen kan,", "tokens": ["Als", "oft", "ich", "ihn", "nur", "se\u00b7hen", "kan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "PPER", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so oft kan ich nicht einen man", "tokens": ["so", "oft", "kan", "ich", "nicht", "ei\u00b7nen", "man"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "PTKNEG", "ART", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "zu sehen gestehen:", "tokens": ["zu", "se\u00b7hen", "ge\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "weil under eines menschen schein", "tokens": ["weil", "un\u00b7der", "ei\u00b7nes", "men\u00b7schen", "schein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "mu\u00df ja ein got verborgen sein.", "tokens": ["mu\u00df", "ja", "ein", "got", "ver\u00b7bor\u00b7gen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.9": {"line.1": {"text": "Dan seines hohen geists verstand", "tokens": ["Dan", "sei\u00b7nes", "ho\u00b7hen", "geists", "ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seines sch\u00f6nen leibs wolstand", "tokens": ["und", "sei\u00b7nes", "sch\u00f6\u00b7nen", "leibs", "wol\u00b7stand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "vil ehren bewehren,", "tokens": ["vil", "eh\u00b7ren", "be\u00b7weh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "da\u00df ich wol einen zweifel trag,", "tokens": ["da\u00df", "ich", "wol", "ei\u00b7nen", "zwei\u00b7fel", "trag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "was es doch f\u00fcr ein got sein mag.", "tokens": ["was", "es", "doch", "f\u00fcr", "ein", "got", "sein", "mag."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["PWS", "PPER", "ADV", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Dan der ernst seines angesichts", "tokens": ["Dan", "der", "ernst", "sei\u00b7nes", "an\u00b7ge\u00b7sichts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seine tiefsin des gerichts", "tokens": ["und", "sei\u00b7ne", "tief\u00b7sin", "des", "ge\u00b7richts"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "sein dunder mit wunder,", "tokens": ["sein", "dun\u00b7der", "mit", "wun\u00b7der", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "APPR", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "erschrecken den umkreis so sehr,", "tokens": ["er\u00b7schre\u00b7cken", "den", "um\u00b7kreis", "so", "sehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "als ob er Jupiter selbs w\u00e4r.", "tokens": ["als", "ob", "er", "Ju\u00b7pi\u00b7ter", "selbs", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOKOM", "KOUS", "PPER", "NN", "ADV", "VAFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.11": {"line.1": {"text": "Wan aber seine weise wort,", "tokens": ["Wan", "a\u00b7ber", "sei\u00b7ne", "wei\u00b7se", "wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der freindlichkeit und gnaden port,", "tokens": ["der", "freind\u00b7lich\u00b7keit", "und", "gna\u00b7den", "port", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die herzen entschmerzen,", "tokens": ["die", "her\u00b7zen", "ent\u00b7schmer\u00b7zen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so glaubet die bescheidenheit", "tokens": ["so", "glau\u00b7bet", "die", "be\u00b7schei\u00b7den\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "es sei got der wolredenheit.", "tokens": ["es", "sei", "got", "der", "wol\u00b7re\u00b7den\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "ART", "NN", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.12": {"line.1": {"text": "Wan aber sein gerechter grim", "tokens": ["Wan", "a\u00b7ber", "sein", "ge\u00b7rech\u00b7ter", "grim"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den feind mit gro\u00dfer ungest\u00fcm", "tokens": ["den", "feind", "mit", "gro\u00b7\u00dfer", "un\u00b7ge\u00b7st\u00fcm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "verst\u00f6ret verzehret,", "tokens": ["ver\u00b7st\u00f6\u00b7ret", "ver\u00b7zeh\u00b7ret", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "alsdan beweiset seine macht,", "tokens": ["als\u00b7dan", "be\u00b7wei\u00b7set", "sei\u00b7ne", "macht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "da\u00df er der got Mars in der schlacht.", "tokens": ["da\u00df", "er", "der", "got", "Mars", "in", "der", "schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NN", "APPR", "ART", "VVFIN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.13": {"line.1": {"text": "Doch alsbald seiner weisheit zier", "tokens": ["Doch", "als\u00b7bald", "sei\u00b7ner", "weis\u00b7heit", "zier"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mit seiner feder das papier", "tokens": ["mit", "sei\u00b7ner", "fe\u00b7der", "das", "pa\u00b7pier"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ber\u00fchret und zieret,", "tokens": ["be\u00b7r\u00fch\u00b7ret", "und", "zie\u00b7ret", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so lehret uns alsbald sein hirn,", "tokens": ["so", "leh\u00b7ret", "uns", "als\u00b7bald", "sein", "hirn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "da\u00df er des klaren tags gestirn.", "tokens": ["da\u00df", "er", "des", "kla\u00b7ren", "tags", "ge\u00b7stirn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Wan uns dan durch ihn auf einmal", "tokens": ["Wan", "uns", "dan", "durch", "ihn", "auf", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "APPR", "PPER", "APPR", "ADV"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "so viler wundern gro\u00dfe zahl", "tokens": ["so", "vi\u00b7ler", "wun\u00b7dern", "gro\u00b7\u00dfe", "zahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PIAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "begl\u00fccket erquicket,", "tokens": ["be\u00b7gl\u00fc\u00b7cket", "er\u00b7quic\u00b7ket", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "so mu\u00df ihn nennen alle welt", "tokens": ["so", "mu\u00df", "ihn", "nen\u00b7nen", "al\u00b7le", "welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "der G\u00f6tter feld mehr, dan Mansfeld.", "tokens": ["der", "G\u00f6t\u00b7ter", "feld", "mehr", ",", "dan", "Mans\u00b7feld", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADV", "$,", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}