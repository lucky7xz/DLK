{"textgrid.poem.49642": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "1L: Mu\u00df ich als Deutscher Stellung fassen", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mu\u00df ich als Deutscher Stellung fassen", "tokens": ["Mu\u00df", "ich", "als", "Deut\u00b7scher", "Stel\u00b7lung", "fas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "KOUS", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu diesem Krieg? Er l\u00e4\u00dft mich k\u00fchl.", "tokens": ["Zu", "die\u00b7sem", "Krieg", "?", "Er", "l\u00e4\u00dft", "mich", "k\u00fchl", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich kann nicht lieben, kann nicht hassen,", "tokens": ["Ich", "kann", "nicht", "lie\u00b7ben", ",", "kann", "nicht", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es schweigt mein hohes Pflichtgef\u00fchl.", "tokens": ["Es", "schweigt", "mein", "ho\u00b7hes", "Pflicht\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "W\u00e4r' ich genauer unterrichtet,", "tokens": ["W\u00e4r'", "ich", "ge\u00b7nau\u00b7er", "un\u00b7ter\u00b7rich\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wohin man in Berlin sich neigt,", "tokens": ["Wo\u00b7hin", "man", "in", "Ber\u00b7lin", "sich", "neigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PRF", "VVFIN", "$,"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So w\u00e4re dieser Streit geschlichtet", "tokens": ["So", "w\u00e4\u00b7re", "die\u00b7ser", "Streit", "ge\u00b7schlich\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PDAT", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und mir der rechte Weg gezeigt.", "tokens": ["Und", "mir", "der", "rech\u00b7te", "Weg", "ge\u00b7zeigt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ich bin auch durchaus nicht imstande,", "tokens": ["Ich", "bin", "auch", "durc\u00b7haus", "nicht", "ims\u00b7tan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PTKNEG", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mir klar zu werden, wie es geht,", "tokens": ["Mir", "klar", "zu", "wer\u00b7den", ",", "wie", "es", "geht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "PTKZU", "VAINF", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und welchem p.p. Vaterlande", "tokens": ["Und", "wel\u00b7chem", "p.", "p.", "Va\u00b7ter\u00b7lan\u00b7de"], "token_info": ["word", "word", "abbreviation", "abbreviation", "word"], "pos": ["KON", "PWAT", "NN", "NE", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Der liebe Gott zur Seite steht.", "tokens": ["Der", "lie\u00b7be", "Gott", "zur", "Sei\u00b7te", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Er mu\u00df sich wohl sehr bald entschlie\u00dfen,", "tokens": ["Er", "mu\u00df", "sich", "wohl", "sehr", "bald", "ent\u00b7schlie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wen er zu Sieg und Ehre f\u00fchrt,", "tokens": ["Wen", "er", "zu", "Sieg", "und", "Eh\u00b7re", "f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wer in diesem Blutvergie\u00dfen", "tokens": ["Und", "wer", "in", "die\u00b7sem", "Blut\u00b7ver\u00b7gie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein hohes Walten dankbar sp\u00fcrt.", "tokens": ["Sein", "ho\u00b7hes", "Wal\u00b7ten", "dank\u00b7bar", "sp\u00fcrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "So seh' ich aus der Perspektive", "tokens": ["So", "seh'", "ich", "aus", "der", "Pers\u00b7pek\u00b7ti\u00b7ve"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Die Sache an mit Wissensdurst.", "tokens": ["Die", "Sa\u00b7che", "an", "mit", "Wis\u00b7sens\u00b7durst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr einen geht es sicher schiefe,", "tokens": ["F\u00fcr", "ei\u00b7nen", "geht", "es", "si\u00b7cher", "schie\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VVFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr wen, ist mir vors erste wurst.", "tokens": ["F\u00fcr", "wen", ",", "ist", "mir", "vors", "ers\u00b7te", "wurst", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWS", "$,", "VAFIN", "PPER", "APPRART", "ADJA", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Mu\u00df ich als Deutscher Stellung fassen", "tokens": ["Mu\u00df", "ich", "als", "Deut\u00b7scher", "Stel\u00b7lung", "fas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "KOUS", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu diesem Krieg? Er l\u00e4\u00dft mich k\u00fchl.", "tokens": ["Zu", "die\u00b7sem", "Krieg", "?", "Er", "l\u00e4\u00dft", "mich", "k\u00fchl", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich kann nicht lieben, kann nicht hassen,", "tokens": ["Ich", "kann", "nicht", "lie\u00b7ben", ",", "kann", "nicht", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es schweigt mein hohes Pflichtgef\u00fchl.", "tokens": ["Es", "schweigt", "mein", "ho\u00b7hes", "Pflicht\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "W\u00e4r' ich genauer unterrichtet,", "tokens": ["W\u00e4r'", "ich", "ge\u00b7nau\u00b7er", "un\u00b7ter\u00b7rich\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wohin man in Berlin sich neigt,", "tokens": ["Wo\u00b7hin", "man", "in", "Ber\u00b7lin", "sich", "neigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NE", "PRF", "VVFIN", "$,"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So w\u00e4re dieser Streit geschlichtet", "tokens": ["So", "w\u00e4\u00b7re", "die\u00b7ser", "Streit", "ge\u00b7schlich\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PDAT", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und mir der rechte Weg gezeigt.", "tokens": ["Und", "mir", "der", "rech\u00b7te", "Weg", "ge\u00b7zeigt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ich bin auch durchaus nicht imstande,", "tokens": ["Ich", "bin", "auch", "durc\u00b7haus", "nicht", "ims\u00b7tan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PTKNEG", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mir klar zu werden, wie es geht,", "tokens": ["Mir", "klar", "zu", "wer\u00b7den", ",", "wie", "es", "geht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "PTKZU", "VAINF", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und welchem p.p. Vaterlande", "tokens": ["Und", "wel\u00b7chem", "p.", "p.", "Va\u00b7ter\u00b7lan\u00b7de"], "token_info": ["word", "word", "abbreviation", "abbreviation", "word"], "pos": ["KON", "PWAT", "NN", "NE", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Der liebe Gott zur Seite steht.", "tokens": ["Der", "lie\u00b7be", "Gott", "zur", "Sei\u00b7te", "steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Er mu\u00df sich wohl sehr bald entschlie\u00dfen,", "tokens": ["Er", "mu\u00df", "sich", "wohl", "sehr", "bald", "ent\u00b7schlie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wen er zu Sieg und Ehre f\u00fchrt,", "tokens": ["Wen", "er", "zu", "Sieg", "und", "Eh\u00b7re", "f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wer in diesem Blutvergie\u00dfen", "tokens": ["Und", "wer", "in", "die\u00b7sem", "Blut\u00b7ver\u00b7gie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein hohes Walten dankbar sp\u00fcrt.", "tokens": ["Sein", "ho\u00b7hes", "Wal\u00b7ten", "dank\u00b7bar", "sp\u00fcrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "So seh' ich aus der Perspektive", "tokens": ["So", "seh'", "ich", "aus", "der", "Pers\u00b7pek\u00b7ti\u00b7ve"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Die Sache an mit Wissensdurst.", "tokens": ["Die", "Sa\u00b7che", "an", "mit", "Wis\u00b7sens\u00b7durst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr einen geht es sicher schiefe,", "tokens": ["F\u00fcr", "ei\u00b7nen", "geht", "es", "si\u00b7cher", "schie\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VVFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr wen, ist mir vors erste wurst.", "tokens": ["F\u00fcr", "wen", ",", "ist", "mir", "vors", "ers\u00b7te", "wurst", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWS", "$,", "VAFIN", "PPER", "APPRART", "ADJA", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}