{"textgrid.poem.54109": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Herz mit einem Sprung", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Im Gesicht und auch in Sachsen,", "tokens": ["Im", "Ge\u00b7sicht", "und", "auch", "in", "Sach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "ADV", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "wo die Meise piepst,", "tokens": ["wo", "die", "Mei\u00b7se", "piepst", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "la\u00df ich den Bart mir wachsen,", "tokens": ["la\u00df", "ich", "den", "Bart", "mir", "wach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "weil du mich nicht mehr liebst.", "tokens": ["weil", "du", "mich", "nicht", "mehr", "liebst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "weil du mich nicht mehr liebst.", "tokens": ["weil", "du", "mich", "nicht", "mehr", "liebst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Wir waren beide einsam;", "tokens": ["Wir", "wa\u00b7ren", "bei\u00b7de", "ein\u00b7sam", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "auch ich als Woll-Agent.", "tokens": ["auch", "ich", "als", "Woll\u00b7Agent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "KOUS", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Die Herzen waren gemeinsam,", "tokens": ["Die", "Her\u00b7zen", "wa\u00b7ren", "ge\u00b7mein\u00b7sam", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "die Kassen waren getrennt.", "tokens": ["die", "Kas\u00b7sen", "wa\u00b7ren", "ge\u00b7trennt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Da bin ich konsequent.", "tokens": ["Da", "bin", "ich", "kon\u00b7se\u00b7quent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Du sagst, du w\u00e4rst im Training", "tokens": ["Du", "sagst", ",", "du", "w\u00e4rst", "im", "Trai\u00b7ning"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "wohl f\u00fcr ein Fecht-Turnier.", "tokens": ["wohl", "f\u00fcr", "ein", "Fecht\u00b7Tur\u00b7nier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Du a\u00dfest gar nicht wening", "tokens": ["Du", "a\u00b7\u00dfest", "gar", "nicht", "we\u00b7ning"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "VVFIN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "und hattst nie Geld bei dir . . .", "tokens": ["und", "hattst", "nie", "Geld", "bei", "dir", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VAFIN", "ADV", "NN", "APPR", "PPER", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Man ist ja Kavalier.", "tokens": ["Man", "ist", "ja", "Ka\u00b7va\u00b7lier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Du a\u00dfest frisch und munter", "tokens": ["Du", "a\u00b7\u00dfest", "frisch", "und", "mun\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "nicht ohne jeden Charme", "tokens": ["nicht", "oh\u00b7ne", "je\u00b7den", "Char\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKNEG", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "die Karte rauf und runter,", "tokens": ["die", "Kar\u00b7te", "rauf", "und", "run\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "KON", "ADJD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "die K\u00fcche kalt und warm.", "tokens": ["die", "K\u00fc\u00b7che", "kalt", "und", "warm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "dem Kellner schmerzt der Arm.", "tokens": ["dem", "Kell\u00b7ner", "schmerzt", "der", "Arm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Ich fand das \u00fcbertrieben", "tokens": ["Ich", "fand", "das", "\u00fc\u00b7bert\u00b7rie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PDS", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "und sah dich zornig an.", "tokens": ["und", "sah", "dich", "zor\u00b7nig", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ein Mann will gratis lieben,", "tokens": ["Ein", "Mann", "will", "gra\u00b7tis", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "sonst ist er gar kein Mann!", "tokens": ["sonst", "ist", "er", "gar", "kein", "Mann", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Ich kann dich nicht vergessen.", "tokens": ["Ich", "kann", "dich", "nicht", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Noch heut k\u00f6nnt ich dich maln.", "tokens": ["Noch", "heut", "k\u00f6nnt", "ich", "dich", "maln", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Du hast zuviel gegessen . . .", "tokens": ["Du", "hast", "zu\u00b7viel", "ge\u00b7ges\u00b7sen", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVPP", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wer kann denn das bezahln!", "tokens": ["Wer", "kann", "denn", "das", "be\u00b7zahln", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "PDS", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Wer kann denn das bezahln!", "tokens": ["Wer", "kann", "denn", "das", "be\u00b7zahln", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "PDS", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Ums Kinn starrn mir die Stoppeln.", "tokens": ["Ums", "Kinn", "starrn", "mir", "die", "Stop\u00b7peln", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Mein Vollbart ist noch jung.", "tokens": ["Mein", "Voll\u00b7bart", "ist", "noch", "jung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So fahr ich nun nach Oppeln", "tokens": ["So", "fahr", "ich", "nun", "nach", "Op\u00b7peln"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "zu ner Versteigerung . . .", "tokens": ["zu", "ner", "Ver\u00b7stei\u00b7ge\u00b7rung", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "+--+--", "measure": "dactylic.di.plus"}, "line.5": {"text": "Doch mein Herz,", "tokens": ["Doch", "mein", "Herz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "doch mein Herz,", "tokens": ["doch", "mein", "Herz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "doch mein Herz", "tokens": ["doch", "mein", "Herz"], "token_info": ["word", "word", "word"], "pos": ["ADV", "PPOSAT", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "hat einen Sprung \u2013!", "tokens": ["hat", "ei\u00b7nen", "Sprung", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Im Gesicht und auch in Sachsen,", "tokens": ["Im", "Ge\u00b7sicht", "und", "auch", "in", "Sach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "ADV", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "wo die Meise piepst,", "tokens": ["wo", "die", "Mei\u00b7se", "piepst", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "la\u00df ich den Bart mir wachsen,", "tokens": ["la\u00df", "ich", "den", "Bart", "mir", "wach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "weil du mich nicht mehr liebst.", "tokens": ["weil", "du", "mich", "nicht", "mehr", "liebst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "weil du mich nicht mehr liebst.", "tokens": ["weil", "du", "mich", "nicht", "mehr", "liebst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Wir waren beide einsam;", "tokens": ["Wir", "wa\u00b7ren", "bei\u00b7de", "ein\u00b7sam", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "auch ich als Woll-Agent.", "tokens": ["auch", "ich", "als", "Woll\u00b7Agent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "KOUS", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Die Herzen waren gemeinsam,", "tokens": ["Die", "Her\u00b7zen", "wa\u00b7ren", "ge\u00b7mein\u00b7sam", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "die Kassen waren getrennt.", "tokens": ["die", "Kas\u00b7sen", "wa\u00b7ren", "ge\u00b7trennt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Da bin ich konsequent.", "tokens": ["Da", "bin", "ich", "kon\u00b7se\u00b7quent", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Du sagst, du w\u00e4rst im Training", "tokens": ["Du", "sagst", ",", "du", "w\u00e4rst", "im", "Trai\u00b7ning"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "wohl f\u00fcr ein Fecht-Turnier.", "tokens": ["wohl", "f\u00fcr", "ein", "Fecht\u00b7Tur\u00b7nier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Du a\u00dfest gar nicht wening", "tokens": ["Du", "a\u00b7\u00dfest", "gar", "nicht", "we\u00b7ning"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "VVFIN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "und hattst nie Geld bei dir . . .", "tokens": ["und", "hattst", "nie", "Geld", "bei", "dir", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VAFIN", "ADV", "NN", "APPR", "PPER", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Man ist ja Kavalier.", "tokens": ["Man", "ist", "ja", "Ka\u00b7va\u00b7lier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Du a\u00dfest frisch und munter", "tokens": ["Du", "a\u00b7\u00dfest", "frisch", "und", "mun\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "nicht ohne jeden Charme", "tokens": ["nicht", "oh\u00b7ne", "je\u00b7den", "Char\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKNEG", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "die Karte rauf und runter,", "tokens": ["die", "Kar\u00b7te", "rauf", "und", "run\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "KON", "ADJD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "die K\u00fcche kalt und warm.", "tokens": ["die", "K\u00fc\u00b7che", "kalt", "und", "warm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "dem Kellner schmerzt der Arm.", "tokens": ["dem", "Kell\u00b7ner", "schmerzt", "der", "Arm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Ich fand das \u00fcbertrieben", "tokens": ["Ich", "fand", "das", "\u00fc\u00b7bert\u00b7rie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PDS", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "und sah dich zornig an.", "tokens": ["und", "sah", "dich", "zor\u00b7nig", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ein Mann will gratis lieben,", "tokens": ["Ein", "Mann", "will", "gra\u00b7tis", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "sonst ist er gar kein Mann!", "tokens": ["sonst", "ist", "er", "gar", "kein", "Mann", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Ich kann dich nicht vergessen.", "tokens": ["Ich", "kann", "dich", "nicht", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Noch heut k\u00f6nnt ich dich maln.", "tokens": ["Noch", "heut", "k\u00f6nnt", "ich", "dich", "maln", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Du hast zuviel gegessen . . .", "tokens": ["Du", "hast", "zu\u00b7viel", "ge\u00b7ges\u00b7sen", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVPP", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wer kann denn das bezahln!", "tokens": ["Wer", "kann", "denn", "das", "be\u00b7zahln", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "PDS", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Susala und dusala \u2013", "tokens": ["Su\u00b7sa\u00b7la", "und", "du\u00b7sa\u00b7la", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "XY", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Wer kann denn das bezahln!", "tokens": ["Wer", "kann", "denn", "das", "be\u00b7zahln", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "PDS", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Ums Kinn starrn mir die Stoppeln.", "tokens": ["Ums", "Kinn", "starrn", "mir", "die", "Stop\u00b7peln", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Mein Vollbart ist noch jung.", "tokens": ["Mein", "Voll\u00b7bart", "ist", "noch", "jung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So fahr ich nun nach Oppeln", "tokens": ["So", "fahr", "ich", "nun", "nach", "Op\u00b7peln"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "zu ner Versteigerung . . .", "tokens": ["zu", "ner", "Ver\u00b7stei\u00b7ge\u00b7rung", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "+--+--", "measure": "dactylic.di.plus"}, "line.5": {"text": "Doch mein Herz,", "tokens": ["Doch", "mein", "Herz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "doch mein Herz,", "tokens": ["doch", "mein", "Herz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "doch mein Herz", "tokens": ["doch", "mein", "Herz"], "token_info": ["word", "word", "word"], "pos": ["ADV", "PPOSAT", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "hat einen Sprung \u2013!", "tokens": ["hat", "ei\u00b7nen", "Sprung", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}