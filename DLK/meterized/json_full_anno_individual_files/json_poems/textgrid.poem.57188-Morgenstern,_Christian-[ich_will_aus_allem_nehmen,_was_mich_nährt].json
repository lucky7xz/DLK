{"textgrid.poem.57188": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "[ich will aus allem nehmen, was mich n\u00e4hrt]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbich will aus allem nehmen, was mich n\u00e4hrt,", "tokens": ["\u00bb", "ich", "will", "aus", "al\u00b7lem", "neh\u00b7men", ",", "was", "mich", "n\u00e4hrt", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "APPR", "PIS", "VVINF", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "was \u00fcbereinstimmt mit mir l\u00e4ngst Vertrautem;", "tokens": ["was", "\u00fc\u00b7be\u00b7re\u00b7in\u00b7stimmt", "mit", "mir", "l\u00e4ngst", "Ver\u00b7trau\u00b7tem", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "PPER", "ADV", "NN", "$."], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "so wird mir manches stille Gl\u00fcck gew\u00e4hrt.", "tokens": ["so", "wird", "mir", "man\u00b7ches", "stil\u00b7le", "Gl\u00fcck", "ge\u00b7w\u00e4hrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "In Eurer Weisheit fand ich manch geheime", "tokens": ["In", "Eu\u00b7rer", "Weis\u00b7heit", "fand", "ich", "manch", "ge\u00b7hei\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "PIAT", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Best\u00e4tigung zu von mir selbst Geschautem", "tokens": ["Be\u00b7st\u00e4\u00b7ti\u00b7gung", "zu", "von", "mir", "selbst", "Ge\u00b7schau\u00b7tem"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "PTKZU", "APPR", "PPER", "ADV", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "und brachte sie zu meiner Art in Reime.", "tokens": ["und", "brach\u00b7te", "sie", "zu", "mei\u00b7ner", "Art", "in", "Rei\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Es gibt so vieles Sch\u00f6ne, Gute, Wahre;", "tokens": ["Es", "gibt", "so", "vie\u00b7les", "Sch\u00f6\u00b7ne", ",", "Gu\u00b7te", ",", "Wah\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "NN", "$,", "NN", "$,", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wie bin ich dankbar, da\u00df ich Mensch sein darf", "tokens": ["wie", "bin", "ich", "dank\u00b7bar", ",", "da\u00df", "ich", "Mensch", "sein", "darf"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "ADJD", "$,", "KOUS", "PPER", "NN", "VAINF", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und immer Neues solcher Art erfahre!\u00ab", "tokens": ["und", "im\u00b7mer", "Neu\u00b7es", "sol\u00b7cher", "Art", "er\u00b7fah\u00b7re", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "NN", "PIAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Erfahre denn noch dies dazu: entfernt", "tokens": ["Er\u00b7fah\u00b7re", "denn", "noch", "dies", "da\u00b7zu", ":", "ent\u00b7fernt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "ADV", "ADV", "PDS", "PTKVZ", "$.", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "bist du vom Ernst noch. Dein Gewissen warf", "tokens": ["bist", "du", "vom", "Ernst", "noch", ".", "Dein", "Ge\u00b7wis\u00b7sen", "warf"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPRART", "NN", "ADV", "$.", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "dir noch nicht vor, da\u00df Weisheit sich nur \u2013 lernt.", "tokens": ["dir", "noch", "nicht", "vor", ",", "da\u00df", "Weis\u00b7heit", "sich", "nur", "\u2013", "lernt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "ADV", "PTKNEG", "PTKVZ", "$,", "KOUS", "NN", "PRF", "ADV", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Mit solchem Blumenpfl\u00fccken, Kr\u00e4nzchenwinden \u2013", "tokens": ["Mit", "sol\u00b7chem", "Blu\u00b7men\u00b7pfl\u00fc\u00b7cken", ",", "Kr\u00e4nz\u00b7chen\u00b7win\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "was ist getan? sieh dir ins Angesicht", "tokens": ["was", "ist", "ge\u00b7tan", "?", "sieh", "dir", "ins", "An\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "VVPP", "$.", "VVIMP", "PPER", "APPRART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und pr\u00fcfe, ach, solch allzu lau Empfinden.", "tokens": ["und", "pr\u00fc\u00b7fe", ",", "ach", ",", "solch", "all\u00b7zu", "lau", "Emp\u00b7fin\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ITJ", "$,", "PIAT", "PTKA", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Du f\u00fchlst der Weisheit Weg noch nicht als \u2013 Pflicht.", "tokens": ["Du", "f\u00fchlst", "der", "Weis\u00b7heit", "Weg", "noch", "nicht", "als", "\u2013", "Pflicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "ADV", "PTKNEG", "KOUS", "$(", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und so: ob von Gl\u00fchw\u00fcrmchen oder Sternen", "tokens": ["Und", "so", ":", "ob", "von", "Gl\u00fch\u00b7w\u00fcrm\u00b7chen", "o\u00b7der", "Ster\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "$.", "KOUS", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "dir Licht zuflie\u00dft \u2013 dir ist's das gleiche Licht.", "tokens": ["dir", "Licht", "zu\u00b7flie\u00dft", "\u2013", "dir", "ist's", "das", "glei\u00b7che", "Licht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVFIN", "$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Dir sind die echten Tiefen, wahren Fernen", "tokens": ["Dir", "sind", "die", "ech\u00b7ten", "Tie\u00b7fen", ",", "wah\u00b7ren", "Fer\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "noch stumm; sie, deren Siegel einzig bricht:", "tokens": ["noch", "stumm", ";", "sie", ",", "de\u00b7ren", "Sie\u00b7gel", "ein\u00b7zig", "bricht", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "PPER", "$,", "PRELAT", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "ein tiefdem\u00fctig lebenlanges \u2013 Lernen.", "tokens": ["ein", "tief\u00b7de\u00b7m\u00fc\u00b7tig", "le\u00b7ben\u00b7lan\u00b7ges", "\u2013", "Ler\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "$(", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "\u00bbich will aus allem nehmen, was mich n\u00e4hrt,", "tokens": ["\u00bb", "ich", "will", "aus", "al\u00b7lem", "neh\u00b7men", ",", "was", "mich", "n\u00e4hrt", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "APPR", "PIS", "VVINF", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "was \u00fcbereinstimmt mit mir l\u00e4ngst Vertrautem;", "tokens": ["was", "\u00fc\u00b7be\u00b7re\u00b7in\u00b7stimmt", "mit", "mir", "l\u00e4ngst", "Ver\u00b7trau\u00b7tem", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "PPER", "ADV", "NN", "$."], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "so wird mir manches stille Gl\u00fcck gew\u00e4hrt.", "tokens": ["so", "wird", "mir", "man\u00b7ches", "stil\u00b7le", "Gl\u00fcck", "ge\u00b7w\u00e4hrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "In Eurer Weisheit fand ich manch geheime", "tokens": ["In", "Eu\u00b7rer", "Weis\u00b7heit", "fand", "ich", "manch", "ge\u00b7hei\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "PIAT", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Best\u00e4tigung zu von mir selbst Geschautem", "tokens": ["Be\u00b7st\u00e4\u00b7ti\u00b7gung", "zu", "von", "mir", "selbst", "Ge\u00b7schau\u00b7tem"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "PTKZU", "APPR", "PPER", "ADV", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "und brachte sie zu meiner Art in Reime.", "tokens": ["und", "brach\u00b7te", "sie", "zu", "mei\u00b7ner", "Art", "in", "Rei\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Es gibt so vieles Sch\u00f6ne, Gute, Wahre;", "tokens": ["Es", "gibt", "so", "vie\u00b7les", "Sch\u00f6\u00b7ne", ",", "Gu\u00b7te", ",", "Wah\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "NN", "$,", "NN", "$,", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wie bin ich dankbar, da\u00df ich Mensch sein darf", "tokens": ["wie", "bin", "ich", "dank\u00b7bar", ",", "da\u00df", "ich", "Mensch", "sein", "darf"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "ADJD", "$,", "KOUS", "PPER", "NN", "VAINF", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und immer Neues solcher Art erfahre!\u00ab", "tokens": ["und", "im\u00b7mer", "Neu\u00b7es", "sol\u00b7cher", "Art", "er\u00b7fah\u00b7re", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "NN", "PIAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Erfahre denn noch dies dazu: entfernt", "tokens": ["Er\u00b7fah\u00b7re", "denn", "noch", "dies", "da\u00b7zu", ":", "ent\u00b7fernt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "ADV", "ADV", "PDS", "PTKVZ", "$.", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "bist du vom Ernst noch. Dein Gewissen warf", "tokens": ["bist", "du", "vom", "Ernst", "noch", ".", "Dein", "Ge\u00b7wis\u00b7sen", "warf"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "APPRART", "NN", "ADV", "$.", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "dir noch nicht vor, da\u00df Weisheit sich nur \u2013 lernt.", "tokens": ["dir", "noch", "nicht", "vor", ",", "da\u00df", "Weis\u00b7heit", "sich", "nur", "\u2013", "lernt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "ADV", "PTKNEG", "PTKVZ", "$,", "KOUS", "NN", "PRF", "ADV", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Mit solchem Blumenpfl\u00fccken, Kr\u00e4nzchenwinden \u2013", "tokens": ["Mit", "sol\u00b7chem", "Blu\u00b7men\u00b7pfl\u00fc\u00b7cken", ",", "Kr\u00e4nz\u00b7chen\u00b7win\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "was ist getan? sieh dir ins Angesicht", "tokens": ["was", "ist", "ge\u00b7tan", "?", "sieh", "dir", "ins", "An\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "VVPP", "$.", "VVIMP", "PPER", "APPRART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und pr\u00fcfe, ach, solch allzu lau Empfinden.", "tokens": ["und", "pr\u00fc\u00b7fe", ",", "ach", ",", "solch", "all\u00b7zu", "lau", "Emp\u00b7fin\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ITJ", "$,", "PIAT", "PTKA", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Du f\u00fchlst der Weisheit Weg noch nicht als \u2013 Pflicht.", "tokens": ["Du", "f\u00fchlst", "der", "Weis\u00b7heit", "Weg", "noch", "nicht", "als", "\u2013", "Pflicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "ADV", "PTKNEG", "KOUS", "$(", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und so: ob von Gl\u00fchw\u00fcrmchen oder Sternen", "tokens": ["Und", "so", ":", "ob", "von", "Gl\u00fch\u00b7w\u00fcrm\u00b7chen", "o\u00b7der", "Ster\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "$.", "KOUS", "APPR", "NN", "KON", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "dir Licht zuflie\u00dft \u2013 dir ist's das gleiche Licht.", "tokens": ["dir", "Licht", "zu\u00b7flie\u00dft", "\u2013", "dir", "ist's", "das", "glei\u00b7che", "Licht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVFIN", "$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Dir sind die echten Tiefen, wahren Fernen", "tokens": ["Dir", "sind", "die", "ech\u00b7ten", "Tie\u00b7fen", ",", "wah\u00b7ren", "Fer\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "noch stumm; sie, deren Siegel einzig bricht:", "tokens": ["noch", "stumm", ";", "sie", ",", "de\u00b7ren", "Sie\u00b7gel", "ein\u00b7zig", "bricht", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "PPER", "$,", "PRELAT", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "ein tiefdem\u00fctig lebenlanges \u2013 Lernen.", "tokens": ["ein", "tief\u00b7de\u00b7m\u00fc\u00b7tig", "le\u00b7ben\u00b7lan\u00b7ges", "\u2013", "Ler\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "$(", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}