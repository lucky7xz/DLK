{"dta.poem.2154": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Versuch, die Anst\u00f6\u00dfigkeit vielerley  \n Religionen zu heben.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1748", "urn": "urn:nbn:de:kobv:b4-200905198553", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Ueberall ersichtliches, und dennoch verborgnes, We-", "tokens": ["Ue\u00b7be\u00b7rall", "er\u00b7sicht\u00b7li\u00b7ches", ",", "und", "den\u00b7noch", "ver\u00b7borg\u00b7nes", ",", "We"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ADV", "ADJA", "$,", "KON", "ADV", "ADJA", "$,", "TRUNC"], "meter": "+-+-+--+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.2": {"text": "sen,", "tokens": ["sen", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Wie verwirrt sich der Verstand, wann wir mit Erstau-", "tokens": ["Wie", "ver\u00b7wirrt", "sich", "der", "Ver\u00b7stand", ",", "wann", "wir", "mit", "Er\u00b7stau"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "PRF", "ART", "NN", "$,", "PWAV", "PPER", "APPR", "TRUNC"], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "nen lesen,", "tokens": ["nen", "le\u00b7sen", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Da\u00df so viele tausend Arten falscher G\u00f6tzendienst\u2019 auf Er-", "tokens": ["Da\u00df", "so", "vie\u00b7le", "tau\u00b7send", "Ar\u00b7ten", "fal\u00b7scher", "G\u00f6t\u00b7zen\u00b7dienst'", "auf", "Er"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PIAT", "CARD", "NN", "ADJA", "NN", "APPR", "TRUNC"], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.6": {"text": "den,", "tokens": ["den", ","], "token_info": ["word", "punct"], "pos": ["ART", "$,"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Jhres Unsinns unerachtet, doch von dir geduldet werden!", "tokens": ["Ih\u00b7res", "Un\u00b7sinns", "un\u00b7e\u00b7rach\u00b7tet", ",", "doch", "von", "dir", "ge\u00b7dul\u00b7det", "wer\u00b7den", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "$,", "ADV", "APPR", "PPER", "VVPP", "VAINF", "$."], "meter": "+-+-+-+-+-+-+-+-", "measure": "trochaic.octa.plus"}, "line.8": {"text": "Da es aber doch geschicht;", "tokens": ["Da", "es", "a\u00b7ber", "doch", "ge\u00b7schicht", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "Scheinet es der Menschen Pflicht,", "tokens": ["Schei\u00b7net", "es", "der", "Men\u00b7schen", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.10": {"text": "Die Vernunft zu Rath zu ziehen,", "tokens": ["Die", "Ver\u00b7nunft", "zu", "Rath", "zu", "zie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.11": {"text": "Und mit Ernst uns zu bem\u00fchen,", "tokens": ["Und", "mit", "Ernst", "uns", "zu", "be\u00b7m\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.12": {"text": "Hievon einen Grund zu finden, der uns zeig\u2019 und \u00fcber-", "tokens": ["Hie\u00b7von", "ei\u00b7nen", "Grund", "zu", "fin\u00b7den", ",", "der", "uns", "zeig'", "und", "\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "PTKZU", "VVINF", "$,", "PRELS", "PPER", "VVFIN", "KON", "TRUNC"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.13": {"text": "f\u00fchre:", "tokens": ["f\u00fch\u00b7re", ":"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.14": {"text": "Wie auch bey so vielen Secten doch die Gottheit nichts", "tokens": ["Wie", "auch", "bey", "so", "vie\u00b7len", "Sec\u00b7ten", "doch", "die", "Got\u00b7theit", "nichts"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "APPR", "ADV", "PIAT", "NN", "ADV", "ART", "NN", "PIS"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.15": {"text": "verliere.", "tokens": ["ver\u00b7lie\u00b7re", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.16": {"text": "Denn uns w\u00fcrdige Begriffe von der Vollenkommenheit", "tokens": ["Denn", "uns", "w\u00fcr\u00b7di\u00b7ge", "Be\u00b7grif\u00b7fe", "von", "der", "Vol\u00b7len\u00b7kom\u00b7men\u00b7heit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.17": {"text": "Gottes \u00fcberall zu machen, ist der Menschen Schuldig-", "tokens": ["Got\u00b7tes", "\u00fc\u00b7be\u00b7rall", "zu", "ma\u00b7chen", ",", "ist", "der", "Men\u00b7schen", "Schul\u00b7dig"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "ADV", "PTKZU", "VVINF", "$,", "VAFIN", "ART", "NN", "TRUNC"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.18": {"text": "keit.", "tokens": ["keit", "."], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "+", "measure": "single.up"}, "line.19": {"text": "Da\u00df ein falscher Gottesdienst, auch so gar Abg\u00f6tterey,", "tokens": ["Da\u00df", "ein", "fal\u00b7scher", "Got\u00b7tes\u00b7dienst", ",", "auch", "so", "gar", "Ab\u00b7g\u00f6t\u00b7te\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,", "ADV", "ADV", "ADV", "NN", "$,"], "meter": "--+-+-+---++-+", "measure": "anapaest.init"}, "line.20": {"text": "Blo\u00df nur ein\u2019 Unwissenheit- keine Bosheits\u00fcnde sey,", "tokens": ["Blo\u00df", "nur", "ein'", "Un\u00b7wis\u00b7sen\u00b7heit", "kei\u00b7ne", "Bos\u00b7hei\u00b7ts\u00fcn\u00b7de", "sey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "TRUNC", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.21": {"text": "Ist mit Recht wohl nicht zu leugnen. Wer der Gott-", "tokens": ["Ist", "mit", "Recht", "wohl", "nicht", "zu", "leug\u00b7nen", ".", "Wer", "der", "Got\u00b7t"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "APPR", "NN", "ADV", "PTKNEG", "PTKZU", "VVINF", "$.", "PWS", "ART", "TRUNC"], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}, "line.22": {"text": "heit Gr\u00f6\u00df' erkennet,", "tokens": ["heit", "Gr\u00f6\u00df'", "er\u00b7ken\u00b7net", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.23": {"text": "Wie sie uns, in ihren Werken, ein Erkenntni\u00df von sich", "tokens": ["Wie", "sie", "uns", ",", "in", "ih\u00b7ren", "Wer\u00b7ken", ",", "ein", "Er\u00b7kennt\u00b7ni\u00df", "von", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "PPER", "$,", "APPR", "PPOSAT", "NN", "$,", "ART", "NN", "APPR", "PRF"], "meter": "+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.24": {"text": "g\u00f6nnet;", "tokens": ["g\u00f6n\u00b7net", ";"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.25": {"text": "Da\u00df, wie diese fast unendlich, Gott noch weit unend-", "tokens": ["Da\u00df", ",", "wie", "die\u00b7se", "fast", "un\u00b7end\u00b7lich", ",", "Gott", "noch", "weit", "un\u00b7en\u00b7d"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "$,", "PWAV", "PDS", "ADV", "ADJD", "$,", "NN", "ADV", "ADJD", "TRUNC"], "meter": "--+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.26": {"text": "licher", "tokens": ["li\u00b7cher"], "token_info": ["word"], "pos": ["ADJA"], "meter": "+-", "measure": "trochaic.single"}, "line.27": {"text": "An Verstand, an Macht und Liebe, aller Sonn- und", "tokens": ["An", "Ver\u00b7stand", ",", "an", "Macht", "und", "Lie\u00b7be", ",", "al\u00b7ler", "Sonn", "und"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "APPR", "NN", "KON", "NN", "$,", "PIAT", "TRUNC", "KON"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.28": {"text": "Welten Herr,", "tokens": ["Wel\u00b7ten", "Herr", ","], "token_info": ["word", "word", "punct"], "pos": ["PWAT", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}}}}}