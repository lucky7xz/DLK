{"textgrid.poem.59991": {"metadata": {"author": {"name": "Herwegh, Georg", "birth": "N.A.", "death": "N.A."}, "title": "5.", "genre": "verse", "period": "N.A.", "pub_year": 1846, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das sind die K\u00e4mpfer f\u00fcr Recht und Licht,", "tokens": ["Das", "sind", "die", "K\u00e4mp\u00b7fer", "f\u00fcr", "Recht", "und", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die sich dir dringend empfehlen:", "tokens": ["Die", "sich", "dir", "drin\u00b7gend", "emp\u00b7feh\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "O deutsches Volk, vergi\u00df sie nicht", "tokens": ["O", "deut\u00b7sches", "Volk", ",", "ver\u00b7gi\u00df", "sie", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJA", "NN", "$,", "VVIMP", "PPER", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ins \u2013 Parlament zu w\u00e4hlen.", "tokens": ["Ins", "\u2013", "Par\u00b7la\u00b7ment", "zu", "w\u00e4h\u00b7len", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "$(", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Das sind die K\u00e4mpfer f\u00fcr Recht und Licht!", "tokens": ["Das", "sind", "die", "K\u00e4mp\u00b7fer", "f\u00fcr", "Recht", "und", "Licht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich seh manch lieben Bekannten,", "tokens": ["Ich", "seh", "manch", "lie\u00b7ben", "Be\u00b7kann\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich seh auch manches Schafsgesicht", "tokens": ["Ich", "seh", "auch", "man\u00b7ches", "Schafs\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und manchen Kom\u00f6dianten.", "tokens": ["Und", "man\u00b7chen", "Ko\u00b7m\u00f6\u00b7di\u00b7an\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Es ist der alte Mummenschanz,", "tokens": ["Es", "ist", "der", "al\u00b7te", "Mum\u00b7men\u00b7schanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von dem sie wieder tr\u00e4umen;", "tokens": ["Von", "dem", "sie", "wie\u00b7der", "tr\u00e4u\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Deutschland sucht wiederum beim Schwanz", "tokens": ["Deutschland", "sucht", "wie\u00b7de\u00b7rum", "beim", "Schwanz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Esel aufzuz\u00e4umen.", "tokens": ["Den", "E\u00b7sel", "auf\u00b7zu\u00b7z\u00e4u\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Deutschland l\u00e4\u00dft vor dem Tatenblitz", "tokens": ["Deutschland", "l\u00e4\u00dft", "vor", "dem", "Ta\u00b7ten\u00b7blitz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Den Donner der Rede rollen,", "tokens": ["Den", "Don\u00b7ner", "der", "Re\u00b7de", "rol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Mein Deutschland polstert den alten Sitz", "tokens": ["Mein", "Deutschland", "pols\u00b7tert", "den", "al\u00b7ten", "Sitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Mit neuen Protokollen.", "tokens": ["Mit", "neu\u00b7en", "Pro\u00b7to\u00b7kol\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Sagt an, wer mag den besten Kohl", "tokens": ["Sagt", "an", ",", "wer", "mag", "den", "bes\u00b7ten", "Kohl"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKVZ", "$,", "PWS", "VMFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im deutschen Lande bauen?", "tokens": ["Im", "deut\u00b7schen", "Lan\u00b7de", "bau\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wer ist der Cincinnatus wohl,", "tokens": ["Wer", "ist", "der", "Cin\u00b7cin\u00b7na\u00b7tus", "wohl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NE", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dem wir uns anvertrauen?", "tokens": ["Dem", "wir", "uns", "an\u00b7ver\u00b7trau\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Wir werden im Danaidenfa\u00df", "tokens": ["Wir", "wer\u00b7den", "im", "Da\u00b7na\u00b7i\u00b7den\u00b7fa\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPRART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aufs neue waschen den Zobel,", "tokens": ["Aufs", "neu\u00b7e", "wa\u00b7schen", "den", "Zo\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und werden machen den Pelz nicht na\u00df", "tokens": ["Und", "wer\u00b7den", "ma\u00b7chen", "den", "Pelz", "nicht", "na\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "VVINF", "ART", "NN", "PTKNEG", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und werden sein sehr nobel.", "tokens": ["Und", "wer\u00b7den", "sein", "sehr", "no\u00b7bel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Sehr nobel \u2013 es wird der gro\u00dfe Hinz,", "tokens": ["Sehr", "no\u00b7bel", "\u2013", "es", "wird", "der", "gro\u00b7\u00dfe", "Hinz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der gro\u00dfe Kunz ergie\u00dfen", "tokens": ["Der", "gro\u00b7\u00dfe", "Kunz", "er\u00b7gie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sein gro\u00dfes Herz \u2013 ein gro\u00dfer Prinz", "tokens": ["Sein", "gro\u00b7\u00dfes", "Herz", "\u2013", "ein", "gro\u00b7\u00dfer", "Prinz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird wohl auch einen erschie\u00dfen.", "tokens": ["Wird", "wohl", "auch", "ei\u00b7nen", "er\u00b7schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "ADJA", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Ich kenne das St\u00fcck, ich kenne den Saal \u2013", "tokens": ["Ich", "ken\u00b7ne", "das", "St\u00fcck", ",", "ich", "ken\u00b7ne", "den", "Saal", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ist schwarz-rot-golden behangen:", "tokens": ["Ist", "schwa\u00b7rz\u00b7rot\u00b7gol\u00b7den", "be\u00b7han\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Jakobus spielt zum zweitenmal", "tokens": ["Ja\u00b7ko\u00b7bus", "spielt", "zum", "zwei\u00b7ten\u00b7mal"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auf allgemeines Verlangen.", "tokens": ["Auf", "all\u00b7ge\u00b7mei\u00b7nes", "Ver\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "Das sind die K\u00e4mpfer f\u00fcr Recht und Licht,", "tokens": ["Das", "sind", "die", "K\u00e4mp\u00b7fer", "f\u00fcr", "Recht", "und", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die sich dir dringend empfehlen:", "tokens": ["Die", "sich", "dir", "drin\u00b7gend", "emp\u00b7feh\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "O deutsches Volk, vergi\u00df sie nicht", "tokens": ["O", "deut\u00b7sches", "Volk", ",", "ver\u00b7gi\u00df", "sie", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJA", "NN", "$,", "VVIMP", "PPER", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ins \u2013 Parlament zu w\u00e4hlen.", "tokens": ["Ins", "\u2013", "Par\u00b7la\u00b7ment", "zu", "w\u00e4h\u00b7len", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "$(", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Das sind die K\u00e4mpfer f\u00fcr Recht und Licht!", "tokens": ["Das", "sind", "die", "K\u00e4mp\u00b7fer", "f\u00fcr", "Recht", "und", "Licht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich seh manch lieben Bekannten,", "tokens": ["Ich", "seh", "manch", "lie\u00b7ben", "Be\u00b7kann\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich seh auch manches Schafsgesicht", "tokens": ["Ich", "seh", "auch", "man\u00b7ches", "Schafs\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und manchen Kom\u00f6dianten.", "tokens": ["Und", "man\u00b7chen", "Ko\u00b7m\u00f6\u00b7di\u00b7an\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.11": {"line.1": {"text": "Es ist der alte Mummenschanz,", "tokens": ["Es", "ist", "der", "al\u00b7te", "Mum\u00b7men\u00b7schanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von dem sie wieder tr\u00e4umen;", "tokens": ["Von", "dem", "sie", "wie\u00b7der", "tr\u00e4u\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Deutschland sucht wiederum beim Schwanz", "tokens": ["Deutschland", "sucht", "wie\u00b7de\u00b7rum", "beim", "Schwanz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Esel aufzuz\u00e4umen.", "tokens": ["Den", "E\u00b7sel", "auf\u00b7zu\u00b7z\u00e4u\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Deutschland l\u00e4\u00dft vor dem Tatenblitz", "tokens": ["Deutschland", "l\u00e4\u00dft", "vor", "dem", "Ta\u00b7ten\u00b7blitz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Den Donner der Rede rollen,", "tokens": ["Den", "Don\u00b7ner", "der", "Re\u00b7de", "rol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Mein Deutschland polstert den alten Sitz", "tokens": ["Mein", "Deutschland", "pols\u00b7tert", "den", "al\u00b7ten", "Sitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Mit neuen Protokollen.", "tokens": ["Mit", "neu\u00b7en", "Pro\u00b7to\u00b7kol\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Sagt an, wer mag den besten Kohl", "tokens": ["Sagt", "an", ",", "wer", "mag", "den", "bes\u00b7ten", "Kohl"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKVZ", "$,", "PWS", "VMFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im deutschen Lande bauen?", "tokens": ["Im", "deut\u00b7schen", "Lan\u00b7de", "bau\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wer ist der Cincinnatus wohl,", "tokens": ["Wer", "ist", "der", "Cin\u00b7cin\u00b7na\u00b7tus", "wohl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NE", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dem wir uns anvertrauen?", "tokens": ["Dem", "wir", "uns", "an\u00b7ver\u00b7trau\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Wir werden im Danaidenfa\u00df", "tokens": ["Wir", "wer\u00b7den", "im", "Da\u00b7na\u00b7i\u00b7den\u00b7fa\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPRART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aufs neue waschen den Zobel,", "tokens": ["Aufs", "neu\u00b7e", "wa\u00b7schen", "den", "Zo\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und werden machen den Pelz nicht na\u00df", "tokens": ["Und", "wer\u00b7den", "ma\u00b7chen", "den", "Pelz", "nicht", "na\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "VVINF", "ART", "NN", "PTKNEG", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und werden sein sehr nobel.", "tokens": ["Und", "wer\u00b7den", "sein", "sehr", "no\u00b7bel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Sehr nobel \u2013 es wird der gro\u00dfe Hinz,", "tokens": ["Sehr", "no\u00b7bel", "\u2013", "es", "wird", "der", "gro\u00b7\u00dfe", "Hinz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der gro\u00dfe Kunz ergie\u00dfen", "tokens": ["Der", "gro\u00b7\u00dfe", "Kunz", "er\u00b7gie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sein gro\u00dfes Herz \u2013 ein gro\u00dfer Prinz", "tokens": ["Sein", "gro\u00b7\u00dfes", "Herz", "\u2013", "ein", "gro\u00b7\u00dfer", "Prinz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird wohl auch einen erschie\u00dfen.", "tokens": ["Wird", "wohl", "auch", "ei\u00b7nen", "er\u00b7schie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "ADJA", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.16": {"line.1": {"text": "Ich kenne das St\u00fcck, ich kenne den Saal \u2013", "tokens": ["Ich", "ken\u00b7ne", "das", "St\u00fcck", ",", "ich", "ken\u00b7ne", "den", "Saal", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ist schwarz-rot-golden behangen:", "tokens": ["Ist", "schwa\u00b7rz\u00b7rot\u00b7gol\u00b7den", "be\u00b7han\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Jakobus spielt zum zweitenmal", "tokens": ["Ja\u00b7ko\u00b7bus", "spielt", "zum", "zwei\u00b7ten\u00b7mal"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auf allgemeines Verlangen.", "tokens": ["Auf", "all\u00b7ge\u00b7mei\u00b7nes", "Ver\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}