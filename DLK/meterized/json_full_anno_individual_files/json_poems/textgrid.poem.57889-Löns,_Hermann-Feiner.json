{"textgrid.poem.57889": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Feiner", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "So ist es doch geschehen,", "tokens": ["So", "ist", "es", "doch", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "So hat man es doch gemacht,", "tokens": ["So", "hat", "man", "es", "doch", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Heckengang, den hat man", "tokens": ["Den", "He\u00b7cken\u00b7gang", ",", "den", "hat", "man"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "ART", "VAFIN", "PIS"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Um seinen Namen gebracht.", "tokens": ["Um", "sei\u00b7nen", "Na\u00b7men", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.2": {"line.1": {"text": "Es liegen in den Museen", "tokens": ["Es", "lie\u00b7gen", "in", "den", "Mu\u00b7se\u00b7en"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Alte Sachen aller Art,", "tokens": ["Al\u00b7te", "Sa\u00b7chen", "al\u00b7ler", "Art", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die man mit vielen Kosten", "tokens": ["Die", "man", "mit", "vie\u00b7len", "Kos\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sammelt und aufbewahrt.", "tokens": ["Sam\u00b7melt", "und", "auf\u00b7be\u00b7wahrt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "VVPP", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.3": {"line.1": {"text": "Doch alte Stra\u00dfennamen", "tokens": ["Doch", "al\u00b7te", "Stra\u00b7\u00dfen\u00b7na\u00b7men"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Bewahren f\u00e4llt keinem ein,", "tokens": ["Be\u00b7wah\u00b7ren", "f\u00e4llt", "kei\u00b7nem", "ein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Heckengang klingt gew\u00f6hnlich,", "tokens": ["He\u00b7cken\u00b7gang", "klingt", "ge\u00b7w\u00f6hn\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADJD", "$,"], "meter": "+-++-+-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Arnswaldtstra\u00dfe klingt fein.", "tokens": ["Arns\u00b7waldt\u00b7stra\u00b7\u00dfe", "klingt", "fein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADJD", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Und wenn man schon einmal umtauft,", "tokens": ["Und", "wenn", "man", "schon", "ein\u00b7mal", "um\u00b7tauft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "So sei man konsequent,", "tokens": ["So", "sei", "man", "kon\u00b7se\u00b7quent", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "VMFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich wei\u00df nicht, warum man die Stra\u00dfe", "tokens": ["Ich", "wei\u00df", "nicht", ",", "wa\u00b7rum", "man", "die", "Stra\u00b7\u00dfe"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PWAV", "PIS", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Nicht Philisterstra\u00dfe nennt.", "tokens": ["Nicht", "Phi\u00b7lis\u00b7ter\u00b7stra\u00b7\u00dfe", "nennt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "So ist es doch geschehen,", "tokens": ["So", "ist", "es", "doch", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "So hat man es doch gemacht,", "tokens": ["So", "hat", "man", "es", "doch", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Heckengang, den hat man", "tokens": ["Den", "He\u00b7cken\u00b7gang", ",", "den", "hat", "man"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "ART", "VAFIN", "PIS"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Um seinen Namen gebracht.", "tokens": ["Um", "sei\u00b7nen", "Na\u00b7men", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.6": {"line.1": {"text": "Es liegen in den Museen", "tokens": ["Es", "lie\u00b7gen", "in", "den", "Mu\u00b7se\u00b7en"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Alte Sachen aller Art,", "tokens": ["Al\u00b7te", "Sa\u00b7chen", "al\u00b7ler", "Art", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die man mit vielen Kosten", "tokens": ["Die", "man", "mit", "vie\u00b7len", "Kos\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sammelt und aufbewahrt.", "tokens": ["Sam\u00b7melt", "und", "auf\u00b7be\u00b7wahrt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "VVPP", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.7": {"line.1": {"text": "Doch alte Stra\u00dfennamen", "tokens": ["Doch", "al\u00b7te", "Stra\u00b7\u00dfen\u00b7na\u00b7men"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Bewahren f\u00e4llt keinem ein,", "tokens": ["Be\u00b7wah\u00b7ren", "f\u00e4llt", "kei\u00b7nem", "ein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Heckengang klingt gew\u00f6hnlich,", "tokens": ["He\u00b7cken\u00b7gang", "klingt", "ge\u00b7w\u00f6hn\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADJD", "$,"], "meter": "+-++-+-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Arnswaldtstra\u00dfe klingt fein.", "tokens": ["Arns\u00b7waldt\u00b7stra\u00b7\u00dfe", "klingt", "fein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADJD", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.8": {"line.1": {"text": "Und wenn man schon einmal umtauft,", "tokens": ["Und", "wenn", "man", "schon", "ein\u00b7mal", "um\u00b7tauft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "So sei man konsequent,", "tokens": ["So", "sei", "man", "kon\u00b7se\u00b7quent", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "VMFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich wei\u00df nicht, warum man die Stra\u00dfe", "tokens": ["Ich", "wei\u00df", "nicht", ",", "wa\u00b7rum", "man", "die", "Stra\u00b7\u00dfe"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PWAV", "PIS", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Nicht Philisterstra\u00dfe nennt.", "tokens": ["Nicht", "Phi\u00b7lis\u00b7ter\u00b7stra\u00b7\u00dfe", "nennt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}