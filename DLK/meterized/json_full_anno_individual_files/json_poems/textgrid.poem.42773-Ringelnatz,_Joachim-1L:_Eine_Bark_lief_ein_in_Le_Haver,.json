{"textgrid.poem.42773": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Eine Bark lief ein in Le Haver,", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Eine Bark lief ein in Le Haver,", "tokens": ["Ei\u00b7ne", "Bark", "lief", "ein", "in", "Le", "Ha\u00b7ver", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "APPR", "NE", "NE", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Von Sidnee kommend, nachts elf Uhr drei.", "tokens": ["Von", "Sid\u00b7nee", "kom\u00b7mend", ",", "nachts", "elf", "Uhr", "drei", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$,", "APPR", "CARD", "NN", "CARD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Es roch nach Himbeeressig am Kai,", "tokens": ["Es", "roch", "nach", "Him\u00b7bee\u00b7res\u00b7sig", "am", "Kai", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und nach Hundekadaver.", "tokens": ["Und", "nach", "Hun\u00b7de\u00b7ka\u00b7da\u00b7ver", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Kuttel Daddeldu ging an Land.", "tokens": ["Kut\u00b7tel", "Dad\u00b7del\u00b7du", "ging", "an", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VVFIN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Die R\u00fc Albani war ihm bekannt.", "tokens": ["Die", "R\u00fc", "Al\u00b7ba\u00b7ni", "war", "ihm", "be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+---+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Er kannte nahezu alle Hafenpl\u00e4tze.", "tokens": ["Er", "kann\u00b7te", "na\u00b7he\u00b7zu", "al\u00b7le", "Ha\u00b7fen\u00b7pl\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Weil vor dem ersten Hause ein M\u00e4dchen stand,", "tokens": ["Weil", "vor", "dem", "ers\u00b7ten", "Hau\u00b7se", "ein", "M\u00e4d\u00b7chen", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Holte er sich im ersten Haus von dem M\u00e4dchen die Kr\u00e4tze.", "tokens": ["Hol\u00b7te", "er", "sich", "im", "ers\u00b7ten", "Haus", "von", "dem", "M\u00e4d\u00b7chen", "die", "Kr\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPRART", "ADJA", "NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+--+--+-", "measure": "hexameter"}}, "stanza.4": {"line.1": {"text": "Weil er das aber nat\u00fcrlich nicht gleich empfand,", "tokens": ["Weil", "er", "das", "a\u00b7ber", "na\u00b7t\u00fcr\u00b7lich", "nicht", "gleich", "emp\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "ADV", "ADV", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ging er weiter, \u2013 kreuzte topplastig auf wilder Fahrt.", "tokens": ["Ging", "er", "wei\u00b7ter", ",", "\u2013", "kreuz\u00b7te", "top\u00b7plas\u00b7tig", "auf", "wil\u00b7der", "Fahrt", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "$(", "VVFIN", "ADJD", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+--+--+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Achtzehn Monate Heuer hatte er sich zusammengespart.", "tokens": ["Acht\u00b7zehn", "Mo\u00b7na\u00b7te", "Heu\u00b7er", "hat\u00b7te", "er", "sich", "zu\u00b7sam\u00b7men\u00b7ge\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "NN", "VAFIN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}}, "stanza.5": {"line.1": {"text": "In Nr. 6 traktierte er Eiwie und K\u00e4tchen,", "tokens": ["In", "Nr.", "6", "trak\u00b7tier\u00b7te", "er", "Ei\u00b7wie", "und", "K\u00e4t\u00b7chen", ","], "token_info": ["word", "abbreviation", "number", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "CARD", "VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "In 8 besoff ihn ein neues straff lederbusiges Weib.", "tokens": ["In", "8", "be\u00b7soff", "ihn", "ein", "neu\u00b7es", "straff", "le\u00b7der\u00b7bu\u00b7si\u00b7ges", "Weib", "."], "token_info": ["word", "number", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "VVFIN", "PPER", "ART", "ADJA", "VVFIN", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "Nebenan bei Pierre sind allein sieben gediegene M\u00e4dchen,", "tokens": ["Ne\u00b7be\u00b7nan", "bei", "Pier\u00b7re", "sind", "al\u00b7lein", "sie\u00b7ben", "ge\u00b7die\u00b7ge\u00b7ne", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "VAFIN", "ADV", "CARD", "ADJA", "NN", "$,"], "meter": "+-+-+--+-+--+--+-", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": "Ohne die mit dem Celluloid-Unterleib.", "tokens": ["Oh\u00b7ne", "die", "mit", "dem", "Cel\u00b7lu\u00b7loi\u00b7dUn\u00b7ter\u00b7leib", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "ART", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Daddeldu, the old Seelerbeu Kuttel,", "tokens": ["Dad\u00b7del\u00b7du", ",", "the", "old", "See\u00b7ler\u00b7beu", "Kut\u00b7tel", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "VVFIN", "ADJD", "NN", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Verschenkte den Albatrosknochen,", "tokens": ["Ver\u00b7schenk\u00b7te", "den", "Al\u00b7bat\u00b7rosk\u00b7no\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das Haifischr\u00fcckgrat, die Schals,", "tokens": ["Das", "Hai\u00b7fisc\u00b7hr\u00fcck\u00b7grat", ",", "die", "Schals", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Den Elefanten und die Saragossabuttel.", "tokens": ["Den", "E\u00b7lef\u00b7an\u00b7ten", "und", "die", "Sa\u00b7ra\u00b7gos\u00b7sa\u00b7but\u00b7tel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das hatte er eigentlich alles der Mary versprochen,", "tokens": ["Das", "hat\u00b7te", "er", "ei\u00b7gent\u00b7lich", "al\u00b7les", "der", "Ma\u00b7ry", "ver\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "PIS", "ART", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.6": {"text": "Der anderen Mary; das war seine feste Braut.", "tokens": ["Der", "an\u00b7de\u00b7ren", "Ma\u00b7ry", ";", "das", "war", "sei\u00b7ne", "fes\u00b7te", "Braut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NE", "$.", "PDS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+---+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Daddeldu \u2013 Hallo! Daddeldu,", "tokens": ["Dad\u00b7del\u00b7du", "\u2013", "Hal\u00b7lo", "!", "Dad\u00b7del\u00b7du", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$.", "NE", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Daddeldu wurde fr\u00f6hlich und laut.", "tokens": ["Dad\u00b7del\u00b7du", "wur\u00b7de", "fr\u00f6h\u00b7lich", "und", "laut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Er wollte mit h\u00f6chster Verzerrung seines Gesichts", "tokens": ["Er", "woll\u00b7te", "mit", "h\u00f6chs\u00b7ter", "Ver\u00b7zer\u00b7rung", "sei\u00b7nes", "Ge\u00b7sichts"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+--+--+-+--+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Partu einen Niggersong singen", "tokens": ["Par\u00b7tu", "ei\u00b7nen", "Nig\u00b7ger\u00b7song", "sin\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "ART", "NN", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Und \u00bbBlu beus blu\u00ab.", "tokens": ["Und", "\u00bb", "Blu", "beus", "blu", "\u00ab", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$(", "FM.la", "FM.la", "FM.la", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Aber es entrang sich ihm nichts.", "tokens": ["A\u00b7ber", "es", "ent\u00b7rang", "sich", "ihm", "nichts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PRF", "PPER", "PIS", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "Daddeldu war nicht auf die Wache zu bringen.", "tokens": ["Dad\u00b7del\u00b7du", "war", "nicht", "auf", "die", "Wa\u00b7che", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PTKNEG", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Daddeldu Duddel Kuttelmuttel, Katteldu", "tokens": ["Dad\u00b7del\u00b7du", "Dud\u00b7del", "Kut\u00b7tel\u00b7mut\u00b7tel", ",", "Kat\u00b7tel\u00b7du"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "NE", "NN", "$,", "NE"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Erwachte erstaunt und singend morgens um vier", "tokens": ["Er\u00b7wach\u00b7te", "er\u00b7staunt", "und", "sin\u00b7gend", "mor\u00b7gens", "um", "vier"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "ADV", "APPR", "CARD"], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Zwischen Nasenbluten und Pomm de Schwall auf der Pier.", "tokens": ["Zwi\u00b7schen", "Na\u00b7sen\u00b7blu\u00b7ten", "und", "Pomm", "de", "Schwall", "auf", "der", "Pier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NE", "NE", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+-+--+", "measure": "trochaic.hexa.relaxed"}}, "stanza.9": {"line.1": {"text": "Daddeldu bedrohte zwecks Vorschu\u00df den Steuermann,", "tokens": ["Dad\u00b7del\u00b7du", "be\u00b7droh\u00b7te", "zwecks", "Vor\u00b7schu\u00df", "den", "Steu\u00b7er\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ADJA", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.2": {"text": "Schwitzte den Spiritus aus. Und wusch sich dann.", "tokens": ["Schwitz\u00b7te", "den", "Spi\u00b7ri\u00b7tus", "aus", ".", "Und", "wusch", "sich", "dann", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKVZ", "$.", "KON", "VVFIN", "PRF", "ADV", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}}, "stanza.10": {"line.1": {"text": "Daddeldu ging nachmittags wieder an Land,", "tokens": ["Dad\u00b7del\u00b7du", "ging", "nach\u00b7mit\u00b7tags", "wie\u00b7der", "an", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+--++-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Wo er ein Renntiergeweih, eine Schlangenhaut,", "tokens": ["Wo", "er", "ein", "Renn\u00b7tier\u00b7ge\u00b7weih", ",", "ei\u00b7ne", "Schlan\u00b7gen\u00b7haut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Zwei F\u00e4cherpalmen und Eskimoschuhe erstand.", "tokens": ["Zwei", "F\u00e4\u00b7cher\u00b7pal\u00b7men", "und", "Es\u00b7ki\u00b7mo\u00b7schu\u00b7he", "er\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "Das brachte er aus Australien seiner Braut.", "tokens": ["Das", "brach\u00b7te", "er", "aus", "Aust\u00b7ra\u00b7li\u00b7en", "sei\u00b7ner", "Braut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Eine Bark lief ein in Le Haver,", "tokens": ["Ei\u00b7ne", "Bark", "lief", "ein", "in", "Le", "Ha\u00b7ver", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "APPR", "NE", "NE", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Von Sidnee kommend, nachts elf Uhr drei.", "tokens": ["Von", "Sid\u00b7nee", "kom\u00b7mend", ",", "nachts", "elf", "Uhr", "drei", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$,", "APPR", "CARD", "NN", "CARD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Es roch nach Himbeeressig am Kai,", "tokens": ["Es", "roch", "nach", "Him\u00b7bee\u00b7res\u00b7sig", "am", "Kai", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und nach Hundekadaver.", "tokens": ["Und", "nach", "Hun\u00b7de\u00b7ka\u00b7da\u00b7ver", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Kuttel Daddeldu ging an Land.", "tokens": ["Kut\u00b7tel", "Dad\u00b7del\u00b7du", "ging", "an", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VVFIN", "APPR", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Die R\u00fc Albani war ihm bekannt.", "tokens": ["Die", "R\u00fc", "Al\u00b7ba\u00b7ni", "war", "ihm", "be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+---+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Er kannte nahezu alle Hafenpl\u00e4tze.", "tokens": ["Er", "kann\u00b7te", "na\u00b7he\u00b7zu", "al\u00b7le", "Ha\u00b7fen\u00b7pl\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.13": {"line.1": {"text": "Weil vor dem ersten Hause ein M\u00e4dchen stand,", "tokens": ["Weil", "vor", "dem", "ers\u00b7ten", "Hau\u00b7se", "ein", "M\u00e4d\u00b7chen", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Holte er sich im ersten Haus von dem M\u00e4dchen die Kr\u00e4tze.", "tokens": ["Hol\u00b7te", "er", "sich", "im", "ers\u00b7ten", "Haus", "von", "dem", "M\u00e4d\u00b7chen", "die", "Kr\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPRART", "ADJA", "NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+--+--+-", "measure": "hexameter"}}, "stanza.14": {"line.1": {"text": "Weil er das aber nat\u00fcrlich nicht gleich empfand,", "tokens": ["Weil", "er", "das", "a\u00b7ber", "na\u00b7t\u00fcr\u00b7lich", "nicht", "gleich", "emp\u00b7fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "ADV", "ADV", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ging er weiter, \u2013 kreuzte topplastig auf wilder Fahrt.", "tokens": ["Ging", "er", "wei\u00b7ter", ",", "\u2013", "kreuz\u00b7te", "top\u00b7plas\u00b7tig", "auf", "wil\u00b7der", "Fahrt", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "$(", "VVFIN", "ADJD", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+--+--+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Achtzehn Monate Heuer hatte er sich zusammengespart.", "tokens": ["Acht\u00b7zehn", "Mo\u00b7na\u00b7te", "Heu\u00b7er", "hat\u00b7te", "er", "sich", "zu\u00b7sam\u00b7men\u00b7ge\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "NN", "VAFIN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}}, "stanza.15": {"line.1": {"text": "In Nr. 6 traktierte er Eiwie und K\u00e4tchen,", "tokens": ["In", "Nr.", "6", "trak\u00b7tier\u00b7te", "er", "Ei\u00b7wie", "und", "K\u00e4t\u00b7chen", ","], "token_info": ["word", "abbreviation", "number", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "CARD", "VVFIN", "PPER", "NN", "KON", "NN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "In 8 besoff ihn ein neues straff lederbusiges Weib.", "tokens": ["In", "8", "be\u00b7soff", "ihn", "ein", "neu\u00b7es", "straff", "le\u00b7der\u00b7bu\u00b7si\u00b7ges", "Weib", "."], "token_info": ["word", "number", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "VVFIN", "PPER", "ART", "ADJA", "VVFIN", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "Nebenan bei Pierre sind allein sieben gediegene M\u00e4dchen,", "tokens": ["Ne\u00b7be\u00b7nan", "bei", "Pier\u00b7re", "sind", "al\u00b7lein", "sie\u00b7ben", "ge\u00b7die\u00b7ge\u00b7ne", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "VAFIN", "ADV", "CARD", "ADJA", "NN", "$,"], "meter": "+-+-+--+-+--+--+-", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": "Ohne die mit dem Celluloid-Unterleib.", "tokens": ["Oh\u00b7ne", "die", "mit", "dem", "Cel\u00b7lu\u00b7loi\u00b7dUn\u00b7ter\u00b7leib", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "ART", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.16": {"line.1": {"text": "Daddeldu, the old Seelerbeu Kuttel,", "tokens": ["Dad\u00b7del\u00b7du", ",", "the", "old", "See\u00b7ler\u00b7beu", "Kut\u00b7tel", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "VVFIN", "ADJD", "NN", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Verschenkte den Albatrosknochen,", "tokens": ["Ver\u00b7schenk\u00b7te", "den", "Al\u00b7bat\u00b7rosk\u00b7no\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das Haifischr\u00fcckgrat, die Schals,", "tokens": ["Das", "Hai\u00b7fisc\u00b7hr\u00fcck\u00b7grat", ",", "die", "Schals", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Den Elefanten und die Saragossabuttel.", "tokens": ["Den", "E\u00b7lef\u00b7an\u00b7ten", "und", "die", "Sa\u00b7ra\u00b7gos\u00b7sa\u00b7but\u00b7tel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das hatte er eigentlich alles der Mary versprochen,", "tokens": ["Das", "hat\u00b7te", "er", "ei\u00b7gent\u00b7lich", "al\u00b7les", "der", "Ma\u00b7ry", "ver\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "PIS", "ART", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.6": {"text": "Der anderen Mary; das war seine feste Braut.", "tokens": ["Der", "an\u00b7de\u00b7ren", "Ma\u00b7ry", ";", "das", "war", "sei\u00b7ne", "fes\u00b7te", "Braut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NE", "$.", "PDS", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+---+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.17": {"line.1": {"text": "Daddeldu \u2013 Hallo! Daddeldu,", "tokens": ["Dad\u00b7del\u00b7du", "\u2013", "Hal\u00b7lo", "!", "Dad\u00b7del\u00b7du", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$.", "NE", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Daddeldu wurde fr\u00f6hlich und laut.", "tokens": ["Dad\u00b7del\u00b7du", "wur\u00b7de", "fr\u00f6h\u00b7lich", "und", "laut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Er wollte mit h\u00f6chster Verzerrung seines Gesichts", "tokens": ["Er", "woll\u00b7te", "mit", "h\u00f6chs\u00b7ter", "Ver\u00b7zer\u00b7rung", "sei\u00b7nes", "Ge\u00b7sichts"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+--+--+-+--+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Partu einen Niggersong singen", "tokens": ["Par\u00b7tu", "ei\u00b7nen", "Nig\u00b7ger\u00b7song", "sin\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "ART", "NN", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Und \u00bbBlu beus blu\u00ab.", "tokens": ["Und", "\u00bb", "Blu", "beus", "blu", "\u00ab", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$(", "FM.la", "FM.la", "FM.la", "$(", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Aber es entrang sich ihm nichts.", "tokens": ["A\u00b7ber", "es", "ent\u00b7rang", "sich", "ihm", "nichts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PRF", "PPER", "PIS", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.18": {"line.1": {"text": "Daddeldu war nicht auf die Wache zu bringen.", "tokens": ["Dad\u00b7del\u00b7du", "war", "nicht", "auf", "die", "Wa\u00b7che", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PTKNEG", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Daddeldu Duddel Kuttelmuttel, Katteldu", "tokens": ["Dad\u00b7del\u00b7du", "Dud\u00b7del", "Kut\u00b7tel\u00b7mut\u00b7tel", ",", "Kat\u00b7tel\u00b7du"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "NE", "NN", "$,", "NE"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Erwachte erstaunt und singend morgens um vier", "tokens": ["Er\u00b7wach\u00b7te", "er\u00b7staunt", "und", "sin\u00b7gend", "mor\u00b7gens", "um", "vier"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "ADV", "APPR", "CARD"], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Zwischen Nasenbluten und Pomm de Schwall auf der Pier.", "tokens": ["Zwi\u00b7schen", "Na\u00b7sen\u00b7blu\u00b7ten", "und", "Pomm", "de", "Schwall", "auf", "der", "Pier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NE", "NE", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+-+--+", "measure": "trochaic.hexa.relaxed"}}, "stanza.19": {"line.1": {"text": "Daddeldu bedrohte zwecks Vorschu\u00df den Steuermann,", "tokens": ["Dad\u00b7del\u00b7du", "be\u00b7droh\u00b7te", "zwecks", "Vor\u00b7schu\u00df", "den", "Steu\u00b7er\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ADJA", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.2": {"text": "Schwitzte den Spiritus aus. Und wusch sich dann.", "tokens": ["Schwitz\u00b7te", "den", "Spi\u00b7ri\u00b7tus", "aus", ".", "Und", "wusch", "sich", "dann", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKVZ", "$.", "KON", "VVFIN", "PRF", "ADV", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}}, "stanza.20": {"line.1": {"text": "Daddeldu ging nachmittags wieder an Land,", "tokens": ["Dad\u00b7del\u00b7du", "ging", "nach\u00b7mit\u00b7tags", "wie\u00b7der", "an", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+--++-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Wo er ein Renntiergeweih, eine Schlangenhaut,", "tokens": ["Wo", "er", "ein", "Renn\u00b7tier\u00b7ge\u00b7weih", ",", "ei\u00b7ne", "Schlan\u00b7gen\u00b7haut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Zwei F\u00e4cherpalmen und Eskimoschuhe erstand.", "tokens": ["Zwei", "F\u00e4\u00b7cher\u00b7pal\u00b7men", "und", "Es\u00b7ki\u00b7mo\u00b7schu\u00b7he", "er\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "Das brachte er aus Australien seiner Braut.", "tokens": ["Das", "brach\u00b7te", "er", "aus", "Aust\u00b7ra\u00b7li\u00b7en", "sei\u00b7ner", "Braut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}