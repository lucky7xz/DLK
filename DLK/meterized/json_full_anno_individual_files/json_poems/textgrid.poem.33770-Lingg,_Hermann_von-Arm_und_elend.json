{"textgrid.poem.33770": {"metadata": {"author": {"name": "Lingg, Hermann von", "birth": "N.A.", "death": "N.A."}, "title": "Arm und elend", "genre": "verse", "period": "N.A.", "pub_year": 1862, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wen tr\u00e4gt man dort so blutig heraus?", "tokens": ["Wen", "tr\u00e4gt", "man", "dort", "so", "blu\u00b7tig", "he\u00b7raus", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PIS", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Er fiel vom Ger\u00fcst am Herrenhaus.", "tokens": ["Er", "fiel", "vom", "Ge\u00b7r\u00fcst", "am", "Her\u00b7ren\u00b7haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "APPRART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Man legt den Sterbenden auf sein Stroh:", "tokens": ["Man", "legt", "den", "Ster\u00b7ben\u00b7den", "auf", "sein", "Stroh", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbwo ist mein Weib, meine Kinder, wo?\u00ab", "tokens": ["\u00bb", "wo", "ist", "mein", "Weib", ",", "mei\u00b7ne", "Kin\u00b7der", ",", "wo", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PWAV", "$.", "$("], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.3": {"line.1": {"text": "Dein Weib ging heut ins Hospital.", "tokens": ["Dein", "Weib", "ging", "heut", "ins", "Hos\u00b7pi\u00b7tal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbwas sagt der Arzt?\u00ab Er sagt \u00bbletal\u00ab!", "tokens": ["\u00bb", "was", "sagt", "der", "Arzt", "?", "\u00ab", "Er", "sagt", "\u00bb", "le\u00b7tal", "\u00ab", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "ART", "NN", "$.", "$(", "PPER", "VVFIN", "$(", "ADV", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "\u00bbkommst du, meine Tochter, was sagst denn du?", "tokens": ["\u00bb", "kommst", "du", ",", "mei\u00b7ne", "Toch\u00b7ter", ",", "was", "sagst", "denn", "du", "?"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PPOSAT", "NN", "$,", "PWS", "VVFIN", "KON", "PPER", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Du hast zwei Kinder und nichts dazu.", "tokens": ["Du", "hast", "zwei", "Kin\u00b7der", "und", "nichts", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "CARD", "NN", "KON", "PIS", "PAV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Der Schurke, der dich hat verf\u00fchrt,", "tokens": ["Der", "Schur\u00b7ke", ",", "der", "dich", "hat", "ver\u00b7f\u00fchrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nach Algier ist er desertiert.\u00ab", "tokens": ["Nach", "Al\u00b7gier", "ist", "er", "de\u00b7ser\u00b7tiert", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NE", "VAFIN", "PPER", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Er legt sein Haupt an die feuchte Wand,", "tokens": ["Er", "legt", "sein", "Haupt", "an", "die", "feuch\u00b7te", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ein Hund leckt ihm die kalte Hand.", "tokens": ["Ein", "Hund", "leckt", "ihm", "die", "kal\u00b7te", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ein Priester kommt, er sieht: zu sp\u00e4t;", "tokens": ["Ein", "Pries\u00b7ter", "kommt", ",", "er", "sieht", ":", "zu", "sp\u00e4t", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PPER", "VVFIN", "$.", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wendet sich ab, und murrt und geht.", "tokens": ["Und", "wen\u00b7det", "sich", "ab", ",", "und", "murrt", "und", "geht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "$,", "KON", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Wen tr\u00e4gt man dort so blutig heraus?", "tokens": ["Wen", "tr\u00e4gt", "man", "dort", "so", "blu\u00b7tig", "he\u00b7raus", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PIS", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Er fiel vom Ger\u00fcst am Herrenhaus.", "tokens": ["Er", "fiel", "vom", "Ge\u00b7r\u00fcst", "am", "Her\u00b7ren\u00b7haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "APPRART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Man legt den Sterbenden auf sein Stroh:", "tokens": ["Man", "legt", "den", "Ster\u00b7ben\u00b7den", "auf", "sein", "Stroh", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbwo ist mein Weib, meine Kinder, wo?\u00ab", "tokens": ["\u00bb", "wo", "ist", "mein", "Weib", ",", "mei\u00b7ne", "Kin\u00b7der", ",", "wo", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PWAV", "$.", "$("], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.10": {"line.1": {"text": "Dein Weib ging heut ins Hospital.", "tokens": ["Dein", "Weib", "ging", "heut", "ins", "Hos\u00b7pi\u00b7tal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbwas sagt der Arzt?\u00ab Er sagt \u00bbletal\u00ab!", "tokens": ["\u00bb", "was", "sagt", "der", "Arzt", "?", "\u00ab", "Er", "sagt", "\u00bb", "le\u00b7tal", "\u00ab", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "ART", "NN", "$.", "$(", "PPER", "VVFIN", "$(", "ADV", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "\u00bbkommst du, meine Tochter, was sagst denn du?", "tokens": ["\u00bb", "kommst", "du", ",", "mei\u00b7ne", "Toch\u00b7ter", ",", "was", "sagst", "denn", "du", "?"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PPOSAT", "NN", "$,", "PWS", "VVFIN", "KON", "PPER", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Du hast zwei Kinder und nichts dazu.", "tokens": ["Du", "hast", "zwei", "Kin\u00b7der", "und", "nichts", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "CARD", "NN", "KON", "PIS", "PAV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Der Schurke, der dich hat verf\u00fchrt,", "tokens": ["Der", "Schur\u00b7ke", ",", "der", "dich", "hat", "ver\u00b7f\u00fchrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nach Algier ist er desertiert.\u00ab", "tokens": ["Nach", "Al\u00b7gier", "ist", "er", "de\u00b7ser\u00b7tiert", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NE", "VAFIN", "PPER", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Er legt sein Haupt an die feuchte Wand,", "tokens": ["Er", "legt", "sein", "Haupt", "an", "die", "feuch\u00b7te", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ein Hund leckt ihm die kalte Hand.", "tokens": ["Ein", "Hund", "leckt", "ihm", "die", "kal\u00b7te", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Ein Priester kommt, er sieht: zu sp\u00e4t;", "tokens": ["Ein", "Pries\u00b7ter", "kommt", ",", "er", "sieht", ":", "zu", "sp\u00e4t", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PPER", "VVFIN", "$.", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wendet sich ab, und murrt und geht.", "tokens": ["Und", "wen\u00b7det", "sich", "ab", ",", "und", "murrt", "und", "geht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "$,", "KON", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}}}}