{"textgrid.poem.41505": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Dauer der Scribenten", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein Cleon, Jahr' und Zeiten fliehen;", "tokens": ["Mein", "Cle\u00b7on", ",", "Jahr'", "und", "Zei\u00b7ten", "flie\u00b7hen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "$,", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie bald sind wir des Moders Raub!", "tokens": ["Wie", "bald", "sind", "wir", "des", "Mo\u00b7ders", "Raub", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie bald sind wir und alles Staub,", "tokens": ["Wie", "bald", "sind", "wir", "und", "al\u00b7les", "Staub", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was wir mit regem Kiel der Dunkelheit entziehen!", "tokens": ["Was", "wir", "mit", "re\u00b7gem", "Kiel", "der", "Dun\u00b7kel\u00b7heit", "ent\u00b7zie\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ADJA", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vergebens schreiben wir f\u00fcr Welt und Afterwelt,", "tokens": ["Ver\u00b7ge\u00b7bens", "schrei\u00b7ben", "wir", "f\u00fcr", "Welt", "und", "Af\u00b7ter\u00b7welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Vergebens werden wir, in B\u00e4nden, aufgestellt;", "tokens": ["Ver\u00b7ge\u00b7bens", "wer\u00b7den", "wir", ",", "in", "B\u00e4n\u00b7den", ",", "auf\u00b7ge\u00b7stellt", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$,", "APPR", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Motten zahlreich' Heer zernagt mit frechem Zahn", "tokens": ["Der", "Mot\u00b7ten", "zahl\u00b7reich'", "Heer", "zer\u00b7nagt", "mit", "fre\u00b7chem", "Zahn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Den bestverg\u00fcldten Schnitt, den sch\u00f6nsten Saffian.", "tokens": ["Den", "best\u00b7ver\u00b7g\u00fcld\u00b7ten", "Schnitt", ",", "den", "sch\u00f6ns\u00b7ten", "Saf\u00b7fi\u00b7an", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.2": {"line.1": {"text": "Ja, Cleon! n\u00e4hmen deine Schriften,", "tokens": ["Ja", ",", "Cle\u00b7on", "!", "n\u00e4h\u00b7men", "dei\u00b7ne", "Schrif\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "NE", "$.", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Um jede Messe zu erfreun,", "tokens": ["Um", "je\u00b7de", "Mes\u00b7se", "zu", "er\u00b7freun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch t\u00e4glich zwanzig Pressen ein,", "tokens": ["Auch", "t\u00e4g\u00b7lich", "zwan\u00b7zig", "Pres\u00b7sen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "CARD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie w\u00fcrden dir dennoch kein stetes Denkmal stiften.", "tokens": ["Sie", "w\u00fcr\u00b7den", "dir", "den\u00b7noch", "kein", "ste\u00b7tes", "Denk\u00b7mal", "stif\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dein st\u00e4rkster Foliant, der Fluch f\u00fcr den, der schreibt,", "tokens": ["Dein", "st\u00e4rks\u00b7ter", "Fo\u00b7li\u00b7ant", ",", "der", "Fluch", "f\u00fcr", "den", ",", "der", "schreibt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "ART", "NN", "APPR", "ART", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "War Lumpe, ward Papier, wird Kehrig, wird zerst\u00e4ubt.", "tokens": ["War", "Lum\u00b7pe", ",", "ward", "Pa\u00b7pier", ",", "wird", "Keh\u00b7rig", ",", "wird", "zer\u00b7st\u00e4ubt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "VAFIN", "NN", "$,", "VAFIN", "ADJD", "$,", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ja, der Vergessenheit und der Verwesung Reich", "tokens": ["Ja", ",", "der", "Ver\u00b7ges\u00b7sen\u00b7heit", "und", "der", "Ver\u00b7we\u00b7sung", "Reich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "ART", "NN", "KON", "ART", "NN", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.8": {"text": "Macht Carl dem Gro\u00dfen dich, wie seiner Sprachkunst, gleich.", "tokens": ["Macht", "Carl", "dem", "Gro\u00b7\u00dfen", "dich", ",", "wie", "sei\u00b7ner", "Sprach\u00b7kunst", ",", "gleich", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "ART", "NN", "PPER", "$,", "PWAV", "PPOSAT", "NN", "$,", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Kein Rang, kein Ruhm k\u00f6mmt uns zu statten,", "tokens": ["Kein", "Rang", ",", "kein", "Ruhm", "k\u00f6mmt", "uns", "zu", "stat\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "VVFIN", "PPER", "PTKZU", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tod sieht keinen Vorzug an,", "tokens": ["Der", "Tod", "sieht", "kei\u00b7nen", "Vor\u00b7zug", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und stellt den allergr\u00f6\u00dften Mann", "tokens": ["Und", "stellt", "den", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7ten", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum P\u00f6bel der gemeinen Schatten.", "tokens": ["Zum", "P\u00f6\u00b7bel", "der", "ge\u00b7mei\u00b7nen", "Schat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er f\u00e4llet ungescheut, der Eitelkeit zum Spott,", "tokens": ["Er", "f\u00e4l\u00b7let", "un\u00b7ge\u00b7scheut", ",", "der", "Ei\u00b7tel\u00b7keit", "zum", "Spott", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$,", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Den K\u00f6nig Galliens, wie den von Yvetot.", "tokens": ["Den", "K\u00f6\u00b7nig", "Gal\u00b7li\u00b7ens", ",", "wie", "den", "von", "Y\u00b7ve\u00b7tot", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$,", "PWAV", "ART", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch was sind K\u00f6nige? Selbst Helden vom Parna\u00df", "tokens": ["Doch", "was", "sind", "K\u00f6\u00b7ni\u00b7ge", "?", "Selbst", "Hel\u00b7den", "vom", "Par\u00b7na\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VAFIN", "NN", "$.", "ADV", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Sind ihm so f\u00fcrchterlich, als uns ein Hundibras.", "tokens": ["Sind", "ihm", "so", "f\u00fcrch\u00b7ter\u00b7lich", ",", "als", "uns", "ein", "Hun\u00b7di\u00b7bras", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "$,", "KOUS", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Verwahre deiner Weisheit Spuren,", "tokens": ["Ver\u00b7wah\u00b7re", "dei\u00b7ner", "Weis\u00b7heit", "Spu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Werk, das deinen Witz bew\u00e4hrt,", "tokens": ["Das", "Werk", ",", "das", "dei\u00b7nen", "Witz", "be\u00b7w\u00e4hrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit Buckeln, die kein Wurm verzehrt,", "tokens": ["Mit", "Bu\u00b7ckeln", ",", "die", "kein", "Wurm", "ver\u00b7zehrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit ewigem Metall in Spangen und Clausuren:", "tokens": ["Mit", "e\u00b7wi\u00b7gem", "Me\u00b7tall", "in", "Span\u00b7gen", "und", "Clau\u00b7su\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Auch dieses sch\u00fctzt dich nicht: vielleicht zerst\u00fcckt es doch", "tokens": ["Auch", "die\u00b7ses", "sch\u00fctzt", "dich", "nicht", ":", "viel\u00b7leicht", "zer\u00b7st\u00fcckt", "es", "doch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "PDS", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Schneider leichtes Volk, ein unbeles'ner Koch:", "tokens": ["Der", "Schnei\u00b7der", "leich\u00b7tes", "Volk", ",", "ein", "un\u00b7be\u00b7les'\u00b7ner", "Koch", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADJA", "NN", "$,", "ART", "ADJA", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und was entbl\u00e4ttern nicht der Haare Kr\u00e4uselei,", "tokens": ["Und", "was", "ent\u00b7bl\u00e4t\u00b7tern", "nicht", "der", "Haa\u00b7re", "Kr\u00e4u\u00b7se\u00b7lei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PTKNEG", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Tabak- und K\u00e4sekram, Confect und Specerei?", "tokens": ["Ta\u00b7bak", "und", "K\u00e4\u00b7se\u00b7kram", ",", "Con\u00b7fect", "und", "Spe\u00b7ce\u00b7rei", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "NE", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "So hat Eumolp dies Lied vollendet,", "tokens": ["So", "hat", "Eu\u00b7molp", "dies", "Lied", "voll\u00b7en\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "PDS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von schreiberischer Eitelkeit,", "tokens": ["Von", "schrei\u00b7be\u00b7ri\u00b7scher", "Ei\u00b7tel\u00b7keit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie er vermeinte, ganz befreit,", "tokens": ["Wie", "er", "ver\u00b7mein\u00b7te", ",", "ganz", "be\u00b7freit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und h\u00f6hnisch auf den Stolz, der Schriftverfasser blendet.", "tokens": ["Und", "h\u00f6h\u00b7nisch", "auf", "den", "Stolz", ",", "der", "Schrift\u00b7ver\u00b7fas\u00b7ser", "blen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "ART", "NN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch sein Verleger k\u00f6mmt, sein Tryphon, der ihn r\u00fchrt,", "tokens": ["Doch", "sein", "Ver\u00b7le\u00b7ger", "k\u00f6mmt", ",", "sein", "Try\u00b7phon", ",", "der", "ihn", "r\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$,", "PPOSAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ihm Lust und Feder sch\u00e4rft, ihn schmeichlerisch verf\u00fchrt.", "tokens": ["Ihm", "Lust", "und", "Fe\u00b7der", "sch\u00e4rft", ",", "ihn", "schmeich\u00b7le\u00b7risch", "ver\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "VVFIN", "$,", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Er wagt ein neues Werk, er gr\u00fcbelt Tag und Nacht,", "tokens": ["Er", "wagt", "ein", "neu\u00b7es", "Werk", ",", "er", "gr\u00fc\u00b7belt", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,", "PPER", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und schreibet um den Ruhm, den er zuvor belacht.", "tokens": ["Und", "schrei\u00b7bet", "um", "den", "Ruhm", ",", "den", "er", "zu\u00b7vor", "be\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Mein Cleon, Jahr' und Zeiten fliehen;", "tokens": ["Mein", "Cle\u00b7on", ",", "Jahr'", "und", "Zei\u00b7ten", "flie\u00b7hen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "$,", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie bald sind wir des Moders Raub!", "tokens": ["Wie", "bald", "sind", "wir", "des", "Mo\u00b7ders", "Raub", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie bald sind wir und alles Staub,", "tokens": ["Wie", "bald", "sind", "wir", "und", "al\u00b7les", "Staub", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was wir mit regem Kiel der Dunkelheit entziehen!", "tokens": ["Was", "wir", "mit", "re\u00b7gem", "Kiel", "der", "Dun\u00b7kel\u00b7heit", "ent\u00b7zie\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ADJA", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vergebens schreiben wir f\u00fcr Welt und Afterwelt,", "tokens": ["Ver\u00b7ge\u00b7bens", "schrei\u00b7ben", "wir", "f\u00fcr", "Welt", "und", "Af\u00b7ter\u00b7welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Vergebens werden wir, in B\u00e4nden, aufgestellt;", "tokens": ["Ver\u00b7ge\u00b7bens", "wer\u00b7den", "wir", ",", "in", "B\u00e4n\u00b7den", ",", "auf\u00b7ge\u00b7stellt", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$,", "APPR", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der Motten zahlreich' Heer zernagt mit frechem Zahn", "tokens": ["Der", "Mot\u00b7ten", "zahl\u00b7reich'", "Heer", "zer\u00b7nagt", "mit", "fre\u00b7chem", "Zahn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Den bestverg\u00fcldten Schnitt, den sch\u00f6nsten Saffian.", "tokens": ["Den", "best\u00b7ver\u00b7g\u00fcld\u00b7ten", "Schnitt", ",", "den", "sch\u00f6ns\u00b7ten", "Saf\u00b7fi\u00b7an", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.7": {"line.1": {"text": "Ja, Cleon! n\u00e4hmen deine Schriften,", "tokens": ["Ja", ",", "Cle\u00b7on", "!", "n\u00e4h\u00b7men", "dei\u00b7ne", "Schrif\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "NE", "$.", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Um jede Messe zu erfreun,", "tokens": ["Um", "je\u00b7de", "Mes\u00b7se", "zu", "er\u00b7freun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch t\u00e4glich zwanzig Pressen ein,", "tokens": ["Auch", "t\u00e4g\u00b7lich", "zwan\u00b7zig", "Pres\u00b7sen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "CARD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie w\u00fcrden dir dennoch kein stetes Denkmal stiften.", "tokens": ["Sie", "w\u00fcr\u00b7den", "dir", "den\u00b7noch", "kein", "ste\u00b7tes", "Denk\u00b7mal", "stif\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dein st\u00e4rkster Foliant, der Fluch f\u00fcr den, der schreibt,", "tokens": ["Dein", "st\u00e4rks\u00b7ter", "Fo\u00b7li\u00b7ant", ",", "der", "Fluch", "f\u00fcr", "den", ",", "der", "schreibt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "ART", "NN", "APPR", "ART", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "War Lumpe, ward Papier, wird Kehrig, wird zerst\u00e4ubt.", "tokens": ["War", "Lum\u00b7pe", ",", "ward", "Pa\u00b7pier", ",", "wird", "Keh\u00b7rig", ",", "wird", "zer\u00b7st\u00e4ubt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "VAFIN", "NN", "$,", "VAFIN", "ADJD", "$,", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ja, der Vergessenheit und der Verwesung Reich", "tokens": ["Ja", ",", "der", "Ver\u00b7ges\u00b7sen\u00b7heit", "und", "der", "Ver\u00b7we\u00b7sung", "Reich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "ART", "NN", "KON", "ART", "NN", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.8": {"text": "Macht Carl dem Gro\u00dfen dich, wie seiner Sprachkunst, gleich.", "tokens": ["Macht", "Carl", "dem", "Gro\u00b7\u00dfen", "dich", ",", "wie", "sei\u00b7ner", "Sprach\u00b7kunst", ",", "gleich", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "ART", "NN", "PPER", "$,", "PWAV", "PPOSAT", "NN", "$,", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Kein Rang, kein Ruhm k\u00f6mmt uns zu statten,", "tokens": ["Kein", "Rang", ",", "kein", "Ruhm", "k\u00f6mmt", "uns", "zu", "stat\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "VVFIN", "PPER", "PTKZU", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tod sieht keinen Vorzug an,", "tokens": ["Der", "Tod", "sieht", "kei\u00b7nen", "Vor\u00b7zug", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und stellt den allergr\u00f6\u00dften Mann", "tokens": ["Und", "stellt", "den", "al\u00b7ler\u00b7gr\u00f6\u00df\u00b7ten", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum P\u00f6bel der gemeinen Schatten.", "tokens": ["Zum", "P\u00f6\u00b7bel", "der", "ge\u00b7mei\u00b7nen", "Schat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er f\u00e4llet ungescheut, der Eitelkeit zum Spott,", "tokens": ["Er", "f\u00e4l\u00b7let", "un\u00b7ge\u00b7scheut", ",", "der", "Ei\u00b7tel\u00b7keit", "zum", "Spott", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$,", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Den K\u00f6nig Galliens, wie den von Yvetot.", "tokens": ["Den", "K\u00f6\u00b7nig", "Gal\u00b7li\u00b7ens", ",", "wie", "den", "von", "Y\u00b7ve\u00b7tot", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$,", "PWAV", "ART", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch was sind K\u00f6nige? Selbst Helden vom Parna\u00df", "tokens": ["Doch", "was", "sind", "K\u00f6\u00b7ni\u00b7ge", "?", "Selbst", "Hel\u00b7den", "vom", "Par\u00b7na\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VAFIN", "NN", "$.", "ADV", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Sind ihm so f\u00fcrchterlich, als uns ein Hundibras.", "tokens": ["Sind", "ihm", "so", "f\u00fcrch\u00b7ter\u00b7lich", ",", "als", "uns", "ein", "Hun\u00b7di\u00b7bras", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "$,", "KOUS", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Verwahre deiner Weisheit Spuren,", "tokens": ["Ver\u00b7wah\u00b7re", "dei\u00b7ner", "Weis\u00b7heit", "Spu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Werk, das deinen Witz bew\u00e4hrt,", "tokens": ["Das", "Werk", ",", "das", "dei\u00b7nen", "Witz", "be\u00b7w\u00e4hrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit Buckeln, die kein Wurm verzehrt,", "tokens": ["Mit", "Bu\u00b7ckeln", ",", "die", "kein", "Wurm", "ver\u00b7zehrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit ewigem Metall in Spangen und Clausuren:", "tokens": ["Mit", "e\u00b7wi\u00b7gem", "Me\u00b7tall", "in", "Span\u00b7gen", "und", "Clau\u00b7su\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Auch dieses sch\u00fctzt dich nicht: vielleicht zerst\u00fcckt es doch", "tokens": ["Auch", "die\u00b7ses", "sch\u00fctzt", "dich", "nicht", ":", "viel\u00b7leicht", "zer\u00b7st\u00fcckt", "es", "doch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "PDS", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Schneider leichtes Volk, ein unbeles'ner Koch:", "tokens": ["Der", "Schnei\u00b7der", "leich\u00b7tes", "Volk", ",", "ein", "un\u00b7be\u00b7les'\u00b7ner", "Koch", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADJA", "NN", "$,", "ART", "ADJA", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und was entbl\u00e4ttern nicht der Haare Kr\u00e4uselei,", "tokens": ["Und", "was", "ent\u00b7bl\u00e4t\u00b7tern", "nicht", "der", "Haa\u00b7re", "Kr\u00e4u\u00b7se\u00b7lei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PTKNEG", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Tabak- und K\u00e4sekram, Confect und Specerei?", "tokens": ["Ta\u00b7bak", "und", "K\u00e4\u00b7se\u00b7kram", ",", "Con\u00b7fect", "und", "Spe\u00b7ce\u00b7rei", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "NE", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "So hat Eumolp dies Lied vollendet,", "tokens": ["So", "hat", "Eu\u00b7molp", "dies", "Lied", "voll\u00b7en\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "PDS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von schreiberischer Eitelkeit,", "tokens": ["Von", "schrei\u00b7be\u00b7ri\u00b7scher", "Ei\u00b7tel\u00b7keit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie er vermeinte, ganz befreit,", "tokens": ["Wie", "er", "ver\u00b7mein\u00b7te", ",", "ganz", "be\u00b7freit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und h\u00f6hnisch auf den Stolz, der Schriftverfasser blendet.", "tokens": ["Und", "h\u00f6h\u00b7nisch", "auf", "den", "Stolz", ",", "der", "Schrift\u00b7ver\u00b7fas\u00b7ser", "blen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "ART", "NN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch sein Verleger k\u00f6mmt, sein Tryphon, der ihn r\u00fchrt,", "tokens": ["Doch", "sein", "Ver\u00b7le\u00b7ger", "k\u00f6mmt", ",", "sein", "Try\u00b7phon", ",", "der", "ihn", "r\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$,", "PPOSAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ihm Lust und Feder sch\u00e4rft, ihn schmeichlerisch verf\u00fchrt.", "tokens": ["Ihm", "Lust", "und", "Fe\u00b7der", "sch\u00e4rft", ",", "ihn", "schmeich\u00b7le\u00b7risch", "ver\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "VVFIN", "$,", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Er wagt ein neues Werk, er gr\u00fcbelt Tag und Nacht,", "tokens": ["Er", "wagt", "ein", "neu\u00b7es", "Werk", ",", "er", "gr\u00fc\u00b7belt", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,", "PPER", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und schreibet um den Ruhm, den er zuvor belacht.", "tokens": ["Und", "schrei\u00b7bet", "um", "den", "Ruhm", ",", "den", "er", "zu\u00b7vor", "be\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}