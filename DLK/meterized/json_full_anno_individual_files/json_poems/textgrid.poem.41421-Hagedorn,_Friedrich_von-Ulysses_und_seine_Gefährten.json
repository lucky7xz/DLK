{"textgrid.poem.41421": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Ulysses und seine Gef\u00e4hrten", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ulysses und der Rest der ihm getreuen Schaaren,", "tokens": ["U\u00b7lys\u00b7ses", "und", "der", "Rest", "der", "ihm", "ge\u00b7treu\u00b7en", "Schaa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ART", "NN", "ART", "PPER", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Die, vielen Helden gleich, nur selten gl\u00fccklich waren,", "tokens": ["Die", ",", "vie\u00b7len", "Hel\u00b7den", "gleich", ",", "nur", "sel\u00b7ten", "gl\u00fcck\u00b7lich", "wa\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PIAT", "NN", "ADV", "$,", "ADV", "ADJD", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Verlie\u00dfen kaum der L\u00e4strigoner Land,", "tokens": ["Ver\u00b7lie\u00b7\u00dfen", "kaum", "der", "L\u00e4st\u00b7ri\u00b7go\u00b7ner", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Als ihr Verh\u00e4ngni\u00df sie zu einer Insel f\u00fchrte,", "tokens": ["Als", "ihr", "Ver\u00b7h\u00e4ng\u00b7ni\u00df", "sie", "zu", "ei\u00b7ner", "In\u00b7sel", "f\u00fchr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wo Circe k\u00f6niglich regierte,", "tokens": ["Wo", "Cir\u00b7ce", "k\u00f6\u00b7nig\u00b7lich", "re\u00b7gier\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die mit Medeens Kunst Medeens Reiz verband.", "tokens": ["Die", "mit", "Me\u00b7de\u00b7ens", "Kunst", "Me\u00b7de\u00b7ens", "Reiz", "ver\u00b7band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "NE", "NE", "NE", "VVFIN", "$."], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Im Thal steht ihr Palast. Gekr\u00fcmmt zu ihren F\u00fc\u00dfen,", "tokens": ["Im", "Thal", "steht", "ihr", "Pa\u00b7last", ".", "Ge\u00b7kr\u00fcmmt", "zu", "ih\u00b7ren", "F\u00fc\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPOSAT", "NN", "$.", "VVPP", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "L\u00e4\u00dft sich ihr L\u00f6we dort von ihrem Arm umschlie\u00dfen.", "tokens": ["L\u00e4\u00dft", "sich", "ihr", "L\u00f6\u00b7we", "dort", "von", "ih\u00b7rem", "Arm", "um\u00b7schlie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PPOSAT", "NN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr Wolf verlernt die w\u00fcrgende Gewalt.", "tokens": ["Ihr", "Wolf", "ver\u00b7lernt", "die", "w\u00fcr\u00b7gen\u00b7de", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Vier T\u00f6chter der Natur, der W\u00e4lder und der Quellen,", "tokens": ["Vier", "T\u00f6ch\u00b7ter", "der", "Na\u00b7tur", ",", "der", "W\u00e4l\u00b7der", "und", "der", "Quel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ART", "NN", "$,", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und der ins Meer verstr\u00f6mten Wellen,", "tokens": ["Und", "der", "ins", "Meer", "ver\u00b7str\u00f6m\u00b7ten", "Wel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "APPRART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Bedienen Circen stets in jenem Aufenthalt.", "tokens": ["Be\u00b7die\u00b7nen", "Cir\u00b7cen", "stets", "in", "je\u00b7nem", "Auf\u00b7ent\u00b7halt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Der Nymphen G\u00f6ttin singt. Die frohen Haine hallen,", "tokens": ["Der", "Nym\u00b7phen", "G\u00f6t\u00b7tin", "singt", ".", "Die", "fro\u00b7hen", "Hai\u00b7ne", "hal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVFIN", "$.", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da Zephyrs Hauch und Scherz in ihren Haaren wallen,", "tokens": ["Da", "Ze\u00b7phyrs", "Hauch", "und", "Scherz", "in", "ih\u00b7ren", "Haa\u00b7ren", "wal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "NN", "KON", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die uns Homer, der Haare Kenner preist.", "tokens": ["Die", "uns", "Ho\u00b7mer", ",", "der", "Haa\u00b7re", "Ken\u00b7ner", "preist", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "NE", "$,", "ART", "NN", "NN", "VVFIN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Sie labt Ulyssens Volk: es zecht mit sicherm Muthe,", "tokens": ["Sie", "labt", "U\u00b7lys\u00b7sens", "Volk", ":", "es", "zecht", "mit", "si\u00b7cherm", "Mu\u00b7the", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "NN", "$.", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Bis pl\u00f6tzlich ihre Zauberruthe", "tokens": ["Bis", "pl\u00f6tz\u00b7lich", "ih\u00b7re", "Zau\u00b7ber\u00b7ru\u00b7the"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die\u00df Volk zu Thieren schl\u00e4gt, und ihre Kraft beweist.", "tokens": ["Die\u00df", "Volk", "zu", "Thie\u00b7ren", "schl\u00e4gt", ",", "und", "ih\u00b7re", "Kraft", "be\u00b7weist", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "APPR", "NN", "VVFIN", "$,", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Eurylochus entrinnt, und sagt, da\u00df diese Thoren", "tokens": ["Eu\u00b7ry\u00b7lo\u00b7chus", "ent\u00b7rinnt", ",", "und", "sagt", ",", "da\u00df", "die\u00b7se", "Tho\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "VVFIN", "$,", "KON", "VVFIN", "$,", "KOUS", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der S\u00e4ngerin gefolgt, und alle sich verloren.", "tokens": ["Der", "S\u00e4n\u00b7ge\u00b7rin", "ge\u00b7folgt", ",", "und", "al\u00b7le", "sich", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "KON", "PIS", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Uly\u00df macht sich, sie zu entdecken, auf.", "tokens": ["U\u00b7ly\u00df", "macht", "sich", ",", "sie", "zu", "ent\u00b7de\u00b7cken", ",", "auf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "VVFIN", "PRF", "$,", "PPER", "PTKZU", "VVINF", "$,", "PTKVZ", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Da soll ihm nun Mercur ein Kraut verehret haben;", "tokens": ["Da", "soll", "ihm", "nun", "Mer\u00b7cur", "ein", "Kraut", "ver\u00b7eh\u00b7ret", "ha\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "NE", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Jetzt aber schenkt er reichre Gaben;", "tokens": ["Jetzt", "a\u00b7ber", "schenkt", "er", "reich\u00b7re", "Ga\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der g\u00fcldne Wucher ist sein heut'ger Lebenslauf.", "tokens": ["Der", "g\u00fcld\u00b7ne", "Wu\u00b7cher", "ist", "sein", "heut'\u00b7ger", "Le\u00b7bens\u00b7lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Doch war es nicht die\u00df Kraut, das damals ihn besch\u00fctzte,", "tokens": ["Doch", "war", "es", "nicht", "die\u00df", "Kraut", ",", "das", "da\u00b7mals", "ihn", "be\u00b7sch\u00fctz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PTKNEG", "PDS", "NN", "$,", "PRELS", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Noch sein entbl\u00f6\u00dftes Schwert, womit er drohend blitzte,", "tokens": ["Noch", "sein", "ent\u00b7bl\u00f6\u00df\u00b7tes", "Schwert", ",", "wo\u00b7mit", "er", "dro\u00b7hend", "blitz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "$,", "PWAV", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als er nunmehr vor Circens Augen kam.", "tokens": ["Als", "er", "nun\u00b7mehr", "vor", "Cir\u00b7cens", "Au\u00b7gen", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Es war die M\u00e4nnlichkeit in seinen Heldenblicken,", "tokens": ["Es", "war", "die", "M\u00e4nn\u00b7lich\u00b7keit", "in", "sei\u00b7nen", "Hel\u00b7den\u00b7bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und ihre Sehnsucht, ihr Entz\u00fccken,", "tokens": ["Und", "ih\u00b7re", "Sehn\u00b7sucht", ",", "ihr", "Ent\u00b7z\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was ihr die Kraft und Lust, ihn zu verwandeln, nahm.", "tokens": ["Was", "ihr", "die", "Kraft", "und", "Lust", ",", "ihn", "zu", "ver\u00b7wan\u00b7deln", ",", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "KON", "NN", "$,", "PPER", "PTKZU", "VVINF", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Er sah, und konnte das nicht ohne Z\u00e4hren sehen,", "tokens": ["Er", "sah", ",", "und", "konn\u00b7te", "das", "nicht", "oh\u00b7ne", "Z\u00e4h\u00b7ren", "se\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VMFIN", "PDS", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er sah, die er gesucht, als Thiere, vor sich stehen,", "tokens": ["Er", "sah", ",", "die", "er", "ge\u00b7sucht", ",", "als", "Thie\u00b7re", ",", "vor", "sich", "ste\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "PPER", "VVPP", "$,", "KOUS", "NN", "$,", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Doch unerkannt bei ihrer Wiederkunft.", "tokens": ["Doch", "un\u00b7er\u00b7kannt", "bei", "ih\u00b7rer", "Wie\u00b7der\u00b7kunft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ach! ruft Ulysses aus, ach Circe! la\u00df dich r\u00fchren,", "tokens": ["Ach", "!", "ruft", "U\u00b7lys\u00b7ses", "aus", ",", "ach", "Cir\u00b7ce", "!", "la\u00df", "dich", "r\u00fch\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "NE", "PTKVZ", "$,", "XY", "NN", "$.", "VVIMP", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und gib, aus Mitleid, diesen Thieren", "tokens": ["Und", "gib", ",", "aus", "Mit\u00b7leid", ",", "die\u00b7sen", "Thie\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVIMP", "$,", "APPR", "NN", "$,", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die vorige Gestalt, die Sprache, die Vernunft.", "tokens": ["Die", "vo\u00b7ri\u00b7ge", "Ge\u00b7stalt", ",", "die", "Spra\u00b7che", ",", "die", "Ver\u00b7nunft", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "G\u00f6ttinnen d\u00fcrfen stets ihr ganzes Herz erkl\u00e4ren.", "tokens": ["G\u00f6t\u00b7tin\u00b7nen", "d\u00fcr\u00b7fen", "stets", "ihr", "gan\u00b7zes", "Herz", "er\u00b7kl\u00e4\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Aus Mitleid, sagt sie ihm, werd' ich dir nichts gew\u00e4hren;", "tokens": ["Aus", "Mit\u00b7leid", ",", "sagt", "sie", "ihm", ",", "werd'", "ich", "dir", "nichts", "ge\u00b7w\u00e4h\u00b7ren", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "VVFIN", "PPER", "PPER", "$,", "VAFIN", "PPER", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Aus Liebe nur geh' ich dein Bitten ein.", "tokens": ["Aus", "Lie\u00b7be", "nur", "geh'", "ich", "dein", "Bit\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Ich will es, da\u00df sie dir, als Menschen, folgen sollen:", "tokens": ["Ich", "will", "es", ",", "da\u00df", "sie", "dir", ",", "als", "Men\u00b7schen", ",", "fol\u00b7gen", "sol\u00b7len", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "$,", "KOUS", "PPER", "PPER", "$,", "KOUS", "NN", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch frage sie, ob sie auch wollen.", "tokens": ["Doch", "fra\u00b7ge", "sie", ",", "ob", "sie", "auch", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPER", "ADV", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dein L\u00f6we k\u00f6mmt hieher! la\u00df ihn den ersten sein.", "tokens": ["Dein", "L\u00f6\u00b7we", "k\u00f6mmt", "hie\u00b7her", "!", "la\u00df", "ihn", "den", "ers\u00b7ten", "sein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "VVFIN", "PAV", "$.", "VVIMP", "PPER", "ART", "ADJA", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Ulysses red't ihn an: Mein W\u00e4chter, mein Getreuer,", "tokens": ["U\u00b7lys\u00b7ses", "red't", "ihn", "an", ":", "Mein", "W\u00e4ch\u00b7ter", ",", "mein", "Ge\u00b7treu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PTKVZ", "$.", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Es endigt heute sich dein seltnes Abenteuer.", "tokens": ["Es", "en\u00b7digt", "heu\u00b7te", "sich", "dein", "selt\u00b7nes", "A\u00b7bent\u00b7eu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PRF", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So bald du willst, bist du ein Mensch, wie wir.", "tokens": ["So", "bald", "du", "willst", ",", "bist", "du", "ein", "Mensch", ",", "wie", "wir", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "VMFIN", "$,", "VAFIN", "PPER", "ART", "NN", "$,", "PWAV", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Der L\u00f6we, der sogleich aus wildem Eifer schnaubte,", "tokens": ["Der", "L\u00f6\u00b7we", ",", "der", "sog\u00b7leich", "aus", "wil\u00b7dem", "Ei\u00b7fer", "schnaub\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$,", "PRELS", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Spricht, da er noch zu br\u00fcllen glaubte:", "tokens": ["Spricht", ",", "da", "er", "noch", "zu", "br\u00fcl\u00b7len", "glaub\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "ADV", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "So th\u00f6richt bin ich nicht; die Menschheit g\u00f6nn' ich dir.", "tokens": ["So", "th\u00f6\u00b7richt", "bin", "ich", "nicht", ";", "die", "Menschheit", "g\u00f6nn'", "ich", "dir", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "PTKNEG", "$.", "ART", "NN", "VVFIN", "PPER", "PPER", "$."], "meter": "-+---+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Ich bleibe, was ich bin. Nur so erweck' ich Grauen,", "tokens": ["Ich", "blei\u00b7be", ",", "was", "ich", "bin", ".", "Nur", "so", "er\u00b7weck'", "ich", "Grau\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "VAFIN", "$.", "ADV", "ADV", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch meiner Z\u00e4hne Raub und durch den Sieg der Klauen.", "tokens": ["Durch", "mei\u00b7ner", "Z\u00e4h\u00b7ne", "Raub", "und", "durch", "den", "Sieg", "der", "Klau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "KON", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mir k\u00f6mmt kein Feind un\u00fcberwindlich nah'.", "tokens": ["Mir", "k\u00f6mmt", "kein", "Feind", "un\u00b7\u00fc\u00b7berw\u00b7ind\u00b7lich", "nah'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sonst war ich dein Soldat: ein Kriegsknecht gilt nur wenig.", "tokens": ["Sonst", "war", "ich", "dein", "Sol\u00b7dat", ":", "ein", "Kriegs\u00b7knecht", "gilt", "nur", "we\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "$.", "ART", "NN", "VVFIN", "ADV", "PIS", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "In jenem Walde bin ich K\u00f6nig:", "tokens": ["In", "je\u00b7nem", "Wal\u00b7de", "bin", "ich", "K\u00f6\u00b7nig", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den reizt kein B\u00fcrgerstand in deinem Ithaca.", "tokens": ["Den", "reizt", "kein", "B\u00fcr\u00b7ger\u00b7stand", "in", "dei\u00b7nem", "I\u00b7tha\u00b7ca", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIAT", "NN", "APPR", "PPOSAT", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Nun wird der B\u00e4r befragt: Willst du zum Menschen werden?", "tokens": ["Nun", "wird", "der", "B\u00e4r", "be\u00b7fragt", ":", "Willst", "du", "zum", "Men\u00b7schen", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$.", "VMFIN", "PPER", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du warst der sch\u00f6nste Kerl an Bildung und Geberden:", "tokens": ["Du", "warst", "der", "sch\u00f6ns\u00b7te", "Kerl", "an", "Bil\u00b7dung", "und", "Ge\u00b7ber\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nun sieht man fast nichts h\u00e4\u00dflicher, als dich.", "tokens": ["Nun", "sieht", "man", "fast", "nichts", "h\u00e4\u00df\u00b7li\u00b7cher", ",", "als", "dich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "PIS", "ADJA", "$,", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ich h\u00e4\u00dflich? brummt der B\u00e4r: Nein! sch\u00f6n, nach Art der B\u00e4ren.", "tokens": ["Ich", "h\u00e4\u00df\u00b7lich", "?", "brummt", "der", "B\u00e4r", ":", "Nein", "!", "sch\u00f6n", ",", "nach", "Art", "der", "B\u00e4\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "$.", "VVFIN", "ART", "NN", "$.", "PTKANT", "$.", "ADJD", "$,", "APPR", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das k\u00f6nnte dir mein Schatz erkl\u00e4ren:", "tokens": ["Das", "k\u00f6nn\u00b7te", "dir", "mein", "Schatz", "er\u00b7kl\u00e4\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die liebt den Honig selbst nicht halb so sehr, als mich.", "tokens": ["Die", "liebt", "den", "Ho\u00b7nig", "selbst", "nicht", "halb", "so", "sehr", ",", "als", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "ADV", "PTKNEG", "ADJD", "ADV", "ADV", "$,", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Woher bist du so klug? Was macht, da\u00df von Gestalten", "tokens": ["Wo\u00b7her", "bist", "du", "so", "klug", "?", "Was", "macht", ",", "da\u00df", "von", "Ge\u00b7stal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ADJD", "$.", "PWS", "VVFIN", "$,", "KOUS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dir jene widrig sind, und die dein Lob erhalten?", "tokens": ["Dir", "je\u00b7ne", "wid\u00b7rig", "sind", ",", "und", "die", "dein", "Lob", "er\u00b7hal\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PDS", "ADJD", "VAFIN", "$,", "KON", "ART", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur Vorurtheil, Gewohnheit, Eigensinn.", "tokens": ["Nur", "Vor\u00b7urt\u00b7heil", ",", "Ge\u00b7wohn\u00b7heit", ",", "Ei\u00b7gen\u00b7sinn", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Gefall' ich dir denn nicht, so meide die\u00df Gehege,", "tokens": ["Ge\u00b7fall'", "ich", "dir", "denn", "nicht", ",", "so", "mei\u00b7de", "die\u00df", "Ge\u00b7he\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "ADV", "PTKNEG", "$,", "ADV", "VVFIN", "PDS", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So packe dich aus meinem Wege.", "tokens": ["So", "pa\u00b7cke", "dich", "aus", "mei\u00b7nem", "We\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mit Lust geh' ich zu Holz, und bleibe, was ich bin.", "tokens": ["Mit", "Lust", "geh'", "ich", "zu", "Holz", ",", "und", "blei\u00b7be", ",", "was", "ich", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "APPR", "NN", "$,", "KON", "VVFIN", "$,", "PWS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Ulysses spricht zum Wolf: Wie viel ist dir entrissen!", "tokens": ["U\u00b7lys\u00b7ses", "spricht", "zum", "Wolf", ":", "Wie", "viel", "ist", "dir", "ent\u00b7ris\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "NE", "$.", "PWAV", "PIS", "VAFIN", "PPER", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Die Hoffnung und das Recht, die Sch\u00e4ferin zu k\u00fcssen,", "tokens": ["Die", "Hoff\u00b7nung", "und", "das", "Recht", ",", "die", "Sch\u00e4\u00b7fe\u00b7rin", "zu", "k\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die nun das Schaf, das du verschlingst, beweint.", "tokens": ["Die", "nun", "das", "Schaf", ",", "das", "du", "ver\u00b7schlingst", ",", "be\u00b7weint", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die Heerden fliehen dich; sonst sch\u00fctztest du die Heerden:", "tokens": ["Die", "Heer\u00b7den", "flie\u00b7hen", "dich", ";", "sonst", "sch\u00fctz\u00b7test", "du", "die", "Heer\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$.", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch, was du warst, das kannst du werden.", "tokens": ["Doch", ",", "was", "du", "warst", ",", "das", "kannst", "du", "wer\u00b7den", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWS", "PPER", "VAFIN", "$,", "PDS", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wohlan! Sei wiederum ein Mensch und Menschenfreund.", "tokens": ["Wo\u00b7hlan", "!", "Sei", "wie\u00b7de\u00b7rum", "ein", "Mensch", "und", "Men\u00b7schen\u00b7freund", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "VAFIN", "ADV", "ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Ihn h\u00f6rt der Wolf, und sagt: Wo gibt es Menschenfreunde?", "tokens": ["Ihn", "h\u00f6rt", "der", "Wolf", ",", "und", "sagt", ":", "Wo", "gibt", "es", "Men\u00b7schen\u00b7freun\u00b7de", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NE", "$,", "KON", "VVFIN", "$.", "PWAV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Menschen selber sind der Menschen \u00e4rgste Feinde,", "tokens": ["Die", "Men\u00b7schen", "sel\u00b7ber", "sind", "der", "Men\u00b7schen", "\u00e4rgs\u00b7te", "Fein\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und einer ist dem andern Wolf und B\u00e4r.", "tokens": ["Und", "ei\u00b7ner", "ist", "dem", "an\u00b7dern", "Wolf", "und", "B\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ART", "ADJA", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die Kunst, zu gleicher Zeit zu schmeicheln und zu hassen,", "tokens": ["Die", "Kunst", ",", "zu", "glei\u00b7cher", "Zeit", "zu", "schmei\u00b7cheln", "und", "zu", "has\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Will ich euch Menschen \u00fcberlassen:", "tokens": ["Will", "ich", "euch", "Men\u00b7schen", "\u00fc\u00b7ber\u00b7las\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Seit ich vom Hofe bin, f\u00e4llt mir die Falschheit schwer.", "tokens": ["Seit", "ich", "vom", "Ho\u00b7fe", "bin", ",", "f\u00e4llt", "mir", "die", "Falschheit", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VAFIN", "$,", "VVFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.14": {"line.1": {"text": "Das Schaf, das ich, aus Trieb und aus Beruf, gefressen,", "tokens": ["Das", "Schaf", ",", "das", "ich", ",", "aus", "Trieb", "und", "aus", "Be\u00b7ruf", ",", "ge\u00b7fres\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "$,", "APPR", "NN", "KON", "APPR", "NN", "$,", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das h\u00e4ttest du wol selbst, doch zierlicher, gegessen.", "tokens": ["Das", "h\u00e4t\u00b7test", "du", "wol", "selbst", ",", "doch", "zier\u00b7li\u00b7cher", ",", "ge\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "ADV", "$,", "ADV", "ADJA", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Herr, mein Geschmack ist hier dem deinen gleich.", "tokens": ["Herr", ",", "mein", "Ge\u00b7schmack", "ist", "hier", "dem", "dei\u00b7nen", "gleich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "ART", "PPOSAT", "ADV", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Soll ich, als Wolf, als Mensch, ja R\u00e4ubereien treiben,", "tokens": ["Soll", "ich", ",", "als", "Wolf", ",", "als", "Mensch", ",", "ja", "R\u00e4u\u00b7be\u00b7rei\u00b7en", "trei\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "KOUS", "NE", "$,", "KOUS", "NN", "$,", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So will ich stets ein Wolf verbleiben.", "tokens": ["So", "will", "ich", "stets", "ein", "Wolf", "ver\u00b7blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ART", "NE", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dann bin ich gl\u00fccklicher; die Reue trifft nur euch.", "tokens": ["Dann", "bin", "ich", "gl\u00fcck\u00b7li\u00b7cher", ";", "die", "Reu\u00b7e", "trifft", "nur", "euch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$.", "ART", "NN", "VVFIN", "ADV", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Laertens Sohn erforscht die \u00fcbrigen Gef\u00e4hrten,", "tokens": ["Laer\u00b7tens", "Sohn", "er\u00b7forscht", "die", "\u00fcb\u00b7ri\u00b7gen", "Ge\u00b7f\u00e4hr\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Und die erkl\u00e4ren sich, wie jene sich erkl\u00e4rten.", "tokens": ["Und", "die", "er\u00b7kl\u00e4\u00b7ren", "sich", ",", "wie", "je\u00b7ne", "sich", "er\u00b7kl\u00e4r\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVFIN", "PRF", "$,", "PWAV", "PDS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sie sind mit Lust den Thieren zugesellt.", "tokens": ["Sie", "sind", "mit", "Lust", "den", "Thie\u00b7ren", "zu\u00b7ge\u00b7sellt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Stand, Ruhm, Unsterblichkeit reizt sie zu keinem Neide.", "tokens": ["Stand", ",", "Ruhm", ",", "U\u00b7nsterb\u00b7lich\u00b7keit", "reizt", "sie", "zu", "kei\u00b7nem", "Nei\u00b7de", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "VVFIN", "PPER", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der freie Wald ist aller Freude.", "tokens": ["Der", "frei\u00b7e", "Wald", "ist", "al\u00b7ler", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht weiser ist der Mensch: er w\u00e4hlt, was ihm gef\u00e4llt.", "tokens": ["Nicht", "wei\u00b7ser", "ist", "der", "Mensch", ":", "er", "w\u00e4hlt", ",", "was", "ihm", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VAFIN", "ART", "NN", "$.", "PPER", "VVFIN", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Und was gef\u00e4llt uns denn? Kann Wahrheit uns vergn\u00fcgen?", "tokens": ["Und", "was", "ge\u00b7f\u00e4llt", "uns", "denn", "?", "Kann", "Wahr\u00b7heit", "uns", "ver\u00b7gn\u00fc\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "ADV", "$.", "VMFIN", "NN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "O nein! wir sind geneigt, uns selber zu betr\u00fcgen.", "tokens": ["O", "nein", "!", "wir", "sind", "ge\u00b7neigt", ",", "uns", "sel\u00b7ber", "zu", "be\u00b7tr\u00fc\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$.", "PPER", "VAFIN", "VVPP", "$,", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Empfindungen weicht unsrer Schl\u00fcsse Kraft.", "tokens": ["Emp\u00b7fin\u00b7dun\u00b7gen", "weicht", "uns\u00b7rer", "Schl\u00fcs\u00b7se", "Kraft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-++-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Vergn\u00fcget uns ein Recht, das Aller Wohlfahrt st\u00fctzet?", "tokens": ["Ver\u00b7gn\u00fc\u00b7get", "uns", "ein", "Recht", ",", "das", "Al\u00b7ler", "Wohl\u00b7fahrt", "st\u00fct\u00b7zet", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So lang es unsrer Absicht n\u00fctzet.", "tokens": ["So", "lang", "es", "uns\u00b7rer", "Ab\u00b7sicht", "n\u00fct\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was unser Thun bestimmt, ist Wahn und Leidenschaft.", "tokens": ["Was", "un\u00b7ser", "Thun", "be\u00b7stimmt", ",", "ist", "Wahn", "und", "Lei\u00b7den\u00b7schaft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVPP", "$,", "VAFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "Ulysses und der Rest der ihm getreuen Schaaren,", "tokens": ["U\u00b7lys\u00b7ses", "und", "der", "Rest", "der", "ihm", "ge\u00b7treu\u00b7en", "Schaa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ART", "NN", "ART", "PPER", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Die, vielen Helden gleich, nur selten gl\u00fccklich waren,", "tokens": ["Die", ",", "vie\u00b7len", "Hel\u00b7den", "gleich", ",", "nur", "sel\u00b7ten", "gl\u00fcck\u00b7lich", "wa\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PIAT", "NN", "ADV", "$,", "ADV", "ADJD", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Verlie\u00dfen kaum der L\u00e4strigoner Land,", "tokens": ["Ver\u00b7lie\u00b7\u00dfen", "kaum", "der", "L\u00e4st\u00b7ri\u00b7go\u00b7ner", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Als ihr Verh\u00e4ngni\u00df sie zu einer Insel f\u00fchrte,", "tokens": ["Als", "ihr", "Ver\u00b7h\u00e4ng\u00b7ni\u00df", "sie", "zu", "ei\u00b7ner", "In\u00b7sel", "f\u00fchr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wo Circe k\u00f6niglich regierte,", "tokens": ["Wo", "Cir\u00b7ce", "k\u00f6\u00b7nig\u00b7lich", "re\u00b7gier\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die mit Medeens Kunst Medeens Reiz verband.", "tokens": ["Die", "mit", "Me\u00b7de\u00b7ens", "Kunst", "Me\u00b7de\u00b7ens", "Reiz", "ver\u00b7band", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "NE", "NE", "NE", "VVFIN", "$."], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "Im Thal steht ihr Palast. Gekr\u00fcmmt zu ihren F\u00fc\u00dfen,", "tokens": ["Im", "Thal", "steht", "ihr", "Pa\u00b7last", ".", "Ge\u00b7kr\u00fcmmt", "zu", "ih\u00b7ren", "F\u00fc\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPOSAT", "NN", "$.", "VVPP", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "L\u00e4\u00dft sich ihr L\u00f6we dort von ihrem Arm umschlie\u00dfen.", "tokens": ["L\u00e4\u00dft", "sich", "ihr", "L\u00f6\u00b7we", "dort", "von", "ih\u00b7rem", "Arm", "um\u00b7schlie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PPOSAT", "NN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr Wolf verlernt die w\u00fcrgende Gewalt.", "tokens": ["Ihr", "Wolf", "ver\u00b7lernt", "die", "w\u00fcr\u00b7gen\u00b7de", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Vier T\u00f6chter der Natur, der W\u00e4lder und der Quellen,", "tokens": ["Vier", "T\u00f6ch\u00b7ter", "der", "Na\u00b7tur", ",", "der", "W\u00e4l\u00b7der", "und", "der", "Quel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ART", "NN", "$,", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und der ins Meer verstr\u00f6mten Wellen,", "tokens": ["Und", "der", "ins", "Meer", "ver\u00b7str\u00f6m\u00b7ten", "Wel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "APPRART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Bedienen Circen stets in jenem Aufenthalt.", "tokens": ["Be\u00b7die\u00b7nen", "Cir\u00b7cen", "stets", "in", "je\u00b7nem", "Auf\u00b7ent\u00b7halt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Der Nymphen G\u00f6ttin singt. Die frohen Haine hallen,", "tokens": ["Der", "Nym\u00b7phen", "G\u00f6t\u00b7tin", "singt", ".", "Die", "fro\u00b7hen", "Hai\u00b7ne", "hal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVFIN", "$.", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da Zephyrs Hauch und Scherz in ihren Haaren wallen,", "tokens": ["Da", "Ze\u00b7phyrs", "Hauch", "und", "Scherz", "in", "ih\u00b7ren", "Haa\u00b7ren", "wal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "NN", "KON", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die uns Homer, der Haare Kenner preist.", "tokens": ["Die", "uns", "Ho\u00b7mer", ",", "der", "Haa\u00b7re", "Ken\u00b7ner", "preist", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "NE", "$,", "ART", "NN", "NN", "VVFIN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Sie labt Ulyssens Volk: es zecht mit sicherm Muthe,", "tokens": ["Sie", "labt", "U\u00b7lys\u00b7sens", "Volk", ":", "es", "zecht", "mit", "si\u00b7cherm", "Mu\u00b7the", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "NN", "$.", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Bis pl\u00f6tzlich ihre Zauberruthe", "tokens": ["Bis", "pl\u00f6tz\u00b7lich", "ih\u00b7re", "Zau\u00b7ber\u00b7ru\u00b7the"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die\u00df Volk zu Thieren schl\u00e4gt, und ihre Kraft beweist.", "tokens": ["Die\u00df", "Volk", "zu", "Thie\u00b7ren", "schl\u00e4gt", ",", "und", "ih\u00b7re", "Kraft", "be\u00b7weist", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "APPR", "NN", "VVFIN", "$,", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Eurylochus entrinnt, und sagt, da\u00df diese Thoren", "tokens": ["Eu\u00b7ry\u00b7lo\u00b7chus", "ent\u00b7rinnt", ",", "und", "sagt", ",", "da\u00df", "die\u00b7se", "Tho\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "VVFIN", "$,", "KON", "VVFIN", "$,", "KOUS", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der S\u00e4ngerin gefolgt, und alle sich verloren.", "tokens": ["Der", "S\u00e4n\u00b7ge\u00b7rin", "ge\u00b7folgt", ",", "und", "al\u00b7le", "sich", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "KON", "PIS", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Uly\u00df macht sich, sie zu entdecken, auf.", "tokens": ["U\u00b7ly\u00df", "macht", "sich", ",", "sie", "zu", "ent\u00b7de\u00b7cken", ",", "auf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "VVFIN", "PRF", "$,", "PPER", "PTKZU", "VVINF", "$,", "PTKVZ", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Da soll ihm nun Mercur ein Kraut verehret haben;", "tokens": ["Da", "soll", "ihm", "nun", "Mer\u00b7cur", "ein", "Kraut", "ver\u00b7eh\u00b7ret", "ha\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "NE", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Jetzt aber schenkt er reichre Gaben;", "tokens": ["Jetzt", "a\u00b7ber", "schenkt", "er", "reich\u00b7re", "Ga\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der g\u00fcldne Wucher ist sein heut'ger Lebenslauf.", "tokens": ["Der", "g\u00fcld\u00b7ne", "Wu\u00b7cher", "ist", "sein", "heut'\u00b7ger", "Le\u00b7bens\u00b7lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Doch war es nicht die\u00df Kraut, das damals ihn besch\u00fctzte,", "tokens": ["Doch", "war", "es", "nicht", "die\u00df", "Kraut", ",", "das", "da\u00b7mals", "ihn", "be\u00b7sch\u00fctz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PTKNEG", "PDS", "NN", "$,", "PRELS", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Noch sein entbl\u00f6\u00dftes Schwert, womit er drohend blitzte,", "tokens": ["Noch", "sein", "ent\u00b7bl\u00f6\u00df\u00b7tes", "Schwert", ",", "wo\u00b7mit", "er", "dro\u00b7hend", "blitz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "$,", "PWAV", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als er nunmehr vor Circens Augen kam.", "tokens": ["Als", "er", "nun\u00b7mehr", "vor", "Cir\u00b7cens", "Au\u00b7gen", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Es war die M\u00e4nnlichkeit in seinen Heldenblicken,", "tokens": ["Es", "war", "die", "M\u00e4nn\u00b7lich\u00b7keit", "in", "sei\u00b7nen", "Hel\u00b7den\u00b7bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und ihre Sehnsucht, ihr Entz\u00fccken,", "tokens": ["Und", "ih\u00b7re", "Sehn\u00b7sucht", ",", "ihr", "Ent\u00b7z\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was ihr die Kraft und Lust, ihn zu verwandeln, nahm.", "tokens": ["Was", "ihr", "die", "Kraft", "und", "Lust", ",", "ihn", "zu", "ver\u00b7wan\u00b7deln", ",", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "KON", "NN", "$,", "PPER", "PTKZU", "VVINF", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Er sah, und konnte das nicht ohne Z\u00e4hren sehen,", "tokens": ["Er", "sah", ",", "und", "konn\u00b7te", "das", "nicht", "oh\u00b7ne", "Z\u00e4h\u00b7ren", "se\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VMFIN", "PDS", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er sah, die er gesucht, als Thiere, vor sich stehen,", "tokens": ["Er", "sah", ",", "die", "er", "ge\u00b7sucht", ",", "als", "Thie\u00b7re", ",", "vor", "sich", "ste\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "PPER", "VVPP", "$,", "KOUS", "NN", "$,", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Doch unerkannt bei ihrer Wiederkunft.", "tokens": ["Doch", "un\u00b7er\u00b7kannt", "bei", "ih\u00b7rer", "Wie\u00b7der\u00b7kunft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ach! ruft Ulysses aus, ach Circe! la\u00df dich r\u00fchren,", "tokens": ["Ach", "!", "ruft", "U\u00b7lys\u00b7ses", "aus", ",", "ach", "Cir\u00b7ce", "!", "la\u00df", "dich", "r\u00fch\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "NE", "PTKVZ", "$,", "XY", "NN", "$.", "VVIMP", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und gib, aus Mitleid, diesen Thieren", "tokens": ["Und", "gib", ",", "aus", "Mit\u00b7leid", ",", "die\u00b7sen", "Thie\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVIMP", "$,", "APPR", "NN", "$,", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die vorige Gestalt, die Sprache, die Vernunft.", "tokens": ["Die", "vo\u00b7ri\u00b7ge", "Ge\u00b7stalt", ",", "die", "Spra\u00b7che", ",", "die", "Ver\u00b7nunft", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.23": {"line.1": {"text": "G\u00f6ttinnen d\u00fcrfen stets ihr ganzes Herz erkl\u00e4ren.", "tokens": ["G\u00f6t\u00b7tin\u00b7nen", "d\u00fcr\u00b7fen", "stets", "ihr", "gan\u00b7zes", "Herz", "er\u00b7kl\u00e4\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Aus Mitleid, sagt sie ihm, werd' ich dir nichts gew\u00e4hren;", "tokens": ["Aus", "Mit\u00b7leid", ",", "sagt", "sie", "ihm", ",", "werd'", "ich", "dir", "nichts", "ge\u00b7w\u00e4h\u00b7ren", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "VVFIN", "PPER", "PPER", "$,", "VAFIN", "PPER", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Aus Liebe nur geh' ich dein Bitten ein.", "tokens": ["Aus", "Lie\u00b7be", "nur", "geh'", "ich", "dein", "Bit\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Ich will es, da\u00df sie dir, als Menschen, folgen sollen:", "tokens": ["Ich", "will", "es", ",", "da\u00df", "sie", "dir", ",", "als", "Men\u00b7schen", ",", "fol\u00b7gen", "sol\u00b7len", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "$,", "KOUS", "PPER", "PPER", "$,", "KOUS", "NN", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch frage sie, ob sie auch wollen.", "tokens": ["Doch", "fra\u00b7ge", "sie", ",", "ob", "sie", "auch", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPER", "ADV", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dein L\u00f6we k\u00f6mmt hieher! la\u00df ihn den ersten sein.", "tokens": ["Dein", "L\u00f6\u00b7we", "k\u00f6mmt", "hie\u00b7her", "!", "la\u00df", "ihn", "den", "ers\u00b7ten", "sein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "VVFIN", "PAV", "$.", "VVIMP", "PPER", "ART", "ADJA", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.24": {"line.1": {"text": "Ulysses red't ihn an: Mein W\u00e4chter, mein Getreuer,", "tokens": ["U\u00b7lys\u00b7ses", "red't", "ihn", "an", ":", "Mein", "W\u00e4ch\u00b7ter", ",", "mein", "Ge\u00b7treu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PTKVZ", "$.", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Es endigt heute sich dein seltnes Abenteuer.", "tokens": ["Es", "en\u00b7digt", "heu\u00b7te", "sich", "dein", "selt\u00b7nes", "A\u00b7bent\u00b7eu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PRF", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So bald du willst, bist du ein Mensch, wie wir.", "tokens": ["So", "bald", "du", "willst", ",", "bist", "du", "ein", "Mensch", ",", "wie", "wir", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "VMFIN", "$,", "VAFIN", "PPER", "ART", "NN", "$,", "PWAV", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Der L\u00f6we, der sogleich aus wildem Eifer schnaubte,", "tokens": ["Der", "L\u00f6\u00b7we", ",", "der", "sog\u00b7leich", "aus", "wil\u00b7dem", "Ei\u00b7fer", "schnaub\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$,", "PRELS", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Spricht, da er noch zu br\u00fcllen glaubte:", "tokens": ["Spricht", ",", "da", "er", "noch", "zu", "br\u00fcl\u00b7len", "glaub\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "ADV", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "So th\u00f6richt bin ich nicht; die Menschheit g\u00f6nn' ich dir.", "tokens": ["So", "th\u00f6\u00b7richt", "bin", "ich", "nicht", ";", "die", "Menschheit", "g\u00f6nn'", "ich", "dir", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "PTKNEG", "$.", "ART", "NN", "VVFIN", "PPER", "PPER", "$."], "meter": "-+---+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.25": {"line.1": {"text": "Ich bleibe, was ich bin. Nur so erweck' ich Grauen,", "tokens": ["Ich", "blei\u00b7be", ",", "was", "ich", "bin", ".", "Nur", "so", "er\u00b7weck'", "ich", "Grau\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "VAFIN", "$.", "ADV", "ADV", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch meiner Z\u00e4hne Raub und durch den Sieg der Klauen.", "tokens": ["Durch", "mei\u00b7ner", "Z\u00e4h\u00b7ne", "Raub", "und", "durch", "den", "Sieg", "der", "Klau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "KON", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mir k\u00f6mmt kein Feind un\u00fcberwindlich nah'.", "tokens": ["Mir", "k\u00f6mmt", "kein", "Feind", "un\u00b7\u00fc\u00b7berw\u00b7ind\u00b7lich", "nah'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sonst war ich dein Soldat: ein Kriegsknecht gilt nur wenig.", "tokens": ["Sonst", "war", "ich", "dein", "Sol\u00b7dat", ":", "ein", "Kriegs\u00b7knecht", "gilt", "nur", "we\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "$.", "ART", "NN", "VVFIN", "ADV", "PIS", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "In jenem Walde bin ich K\u00f6nig:", "tokens": ["In", "je\u00b7nem", "Wal\u00b7de", "bin", "ich", "K\u00f6\u00b7nig", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den reizt kein B\u00fcrgerstand in deinem Ithaca.", "tokens": ["Den", "reizt", "kein", "B\u00fcr\u00b7ger\u00b7stand", "in", "dei\u00b7nem", "I\u00b7tha\u00b7ca", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIAT", "NN", "APPR", "PPOSAT", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.26": {"line.1": {"text": "Nun wird der B\u00e4r befragt: Willst du zum Menschen werden?", "tokens": ["Nun", "wird", "der", "B\u00e4r", "be\u00b7fragt", ":", "Willst", "du", "zum", "Men\u00b7schen", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$.", "VMFIN", "PPER", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du warst der sch\u00f6nste Kerl an Bildung und Geberden:", "tokens": ["Du", "warst", "der", "sch\u00f6ns\u00b7te", "Kerl", "an", "Bil\u00b7dung", "und", "Ge\u00b7ber\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nun sieht man fast nichts h\u00e4\u00dflicher, als dich.", "tokens": ["Nun", "sieht", "man", "fast", "nichts", "h\u00e4\u00df\u00b7li\u00b7cher", ",", "als", "dich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "PIS", "ADJA", "$,", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ich h\u00e4\u00dflich? brummt der B\u00e4r: Nein! sch\u00f6n, nach Art der B\u00e4ren.", "tokens": ["Ich", "h\u00e4\u00df\u00b7lich", "?", "brummt", "der", "B\u00e4r", ":", "Nein", "!", "sch\u00f6n", ",", "nach", "Art", "der", "B\u00e4\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "$.", "VVFIN", "ART", "NN", "$.", "PTKANT", "$.", "ADJD", "$,", "APPR", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das k\u00f6nnte dir mein Schatz erkl\u00e4ren:", "tokens": ["Das", "k\u00f6nn\u00b7te", "dir", "mein", "Schatz", "er\u00b7kl\u00e4\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die liebt den Honig selbst nicht halb so sehr, als mich.", "tokens": ["Die", "liebt", "den", "Ho\u00b7nig", "selbst", "nicht", "halb", "so", "sehr", ",", "als", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "ADV", "PTKNEG", "ADJD", "ADV", "ADV", "$,", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.27": {"line.1": {"text": "Woher bist du so klug? Was macht, da\u00df von Gestalten", "tokens": ["Wo\u00b7her", "bist", "du", "so", "klug", "?", "Was", "macht", ",", "da\u00df", "von", "Ge\u00b7stal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ADJD", "$.", "PWS", "VVFIN", "$,", "KOUS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dir jene widrig sind, und die dein Lob erhalten?", "tokens": ["Dir", "je\u00b7ne", "wid\u00b7rig", "sind", ",", "und", "die", "dein", "Lob", "er\u00b7hal\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PDS", "ADJD", "VAFIN", "$,", "KON", "ART", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur Vorurtheil, Gewohnheit, Eigensinn.", "tokens": ["Nur", "Vor\u00b7urt\u00b7heil", ",", "Ge\u00b7wohn\u00b7heit", ",", "Ei\u00b7gen\u00b7sinn", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Gefall' ich dir denn nicht, so meide die\u00df Gehege,", "tokens": ["Ge\u00b7fall'", "ich", "dir", "denn", "nicht", ",", "so", "mei\u00b7de", "die\u00df", "Ge\u00b7he\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "ADV", "PTKNEG", "$,", "ADV", "VVFIN", "PDS", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So packe dich aus meinem Wege.", "tokens": ["So", "pa\u00b7cke", "dich", "aus", "mei\u00b7nem", "We\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mit Lust geh' ich zu Holz, und bleibe, was ich bin.", "tokens": ["Mit", "Lust", "geh'", "ich", "zu", "Holz", ",", "und", "blei\u00b7be", ",", "was", "ich", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "APPR", "NN", "$,", "KON", "VVFIN", "$,", "PWS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.28": {"line.1": {"text": "Ulysses spricht zum Wolf: Wie viel ist dir entrissen!", "tokens": ["U\u00b7lys\u00b7ses", "spricht", "zum", "Wolf", ":", "Wie", "viel", "ist", "dir", "ent\u00b7ris\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "NE", "$.", "PWAV", "PIS", "VAFIN", "PPER", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Die Hoffnung und das Recht, die Sch\u00e4ferin zu k\u00fcssen,", "tokens": ["Die", "Hoff\u00b7nung", "und", "das", "Recht", ",", "die", "Sch\u00e4\u00b7fe\u00b7rin", "zu", "k\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die nun das Schaf, das du verschlingst, beweint.", "tokens": ["Die", "nun", "das", "Schaf", ",", "das", "du", "ver\u00b7schlingst", ",", "be\u00b7weint", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die Heerden fliehen dich; sonst sch\u00fctztest du die Heerden:", "tokens": ["Die", "Heer\u00b7den", "flie\u00b7hen", "dich", ";", "sonst", "sch\u00fctz\u00b7test", "du", "die", "Heer\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$.", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch, was du warst, das kannst du werden.", "tokens": ["Doch", ",", "was", "du", "warst", ",", "das", "kannst", "du", "wer\u00b7den", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWS", "PPER", "VAFIN", "$,", "PDS", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wohlan! Sei wiederum ein Mensch und Menschenfreund.", "tokens": ["Wo\u00b7hlan", "!", "Sei", "wie\u00b7de\u00b7rum", "ein", "Mensch", "und", "Men\u00b7schen\u00b7freund", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "VAFIN", "ADV", "ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.29": {"line.1": {"text": "Ihn h\u00f6rt der Wolf, und sagt: Wo gibt es Menschenfreunde?", "tokens": ["Ihn", "h\u00f6rt", "der", "Wolf", ",", "und", "sagt", ":", "Wo", "gibt", "es", "Men\u00b7schen\u00b7freun\u00b7de", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NE", "$,", "KON", "VVFIN", "$.", "PWAV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Menschen selber sind der Menschen \u00e4rgste Feinde,", "tokens": ["Die", "Men\u00b7schen", "sel\u00b7ber", "sind", "der", "Men\u00b7schen", "\u00e4rgs\u00b7te", "Fein\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und einer ist dem andern Wolf und B\u00e4r.", "tokens": ["Und", "ei\u00b7ner", "ist", "dem", "an\u00b7dern", "Wolf", "und", "B\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ART", "ADJA", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die Kunst, zu gleicher Zeit zu schmeicheln und zu hassen,", "tokens": ["Die", "Kunst", ",", "zu", "glei\u00b7cher", "Zeit", "zu", "schmei\u00b7cheln", "und", "zu", "has\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Will ich euch Menschen \u00fcberlassen:", "tokens": ["Will", "ich", "euch", "Men\u00b7schen", "\u00fc\u00b7ber\u00b7las\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Seit ich vom Hofe bin, f\u00e4llt mir die Falschheit schwer.", "tokens": ["Seit", "ich", "vom", "Ho\u00b7fe", "bin", ",", "f\u00e4llt", "mir", "die", "Falschheit", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VAFIN", "$,", "VVFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.30": {"line.1": {"text": "Das Schaf, das ich, aus Trieb und aus Beruf, gefressen,", "tokens": ["Das", "Schaf", ",", "das", "ich", ",", "aus", "Trieb", "und", "aus", "Be\u00b7ruf", ",", "ge\u00b7fres\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "$,", "APPR", "NN", "KON", "APPR", "NN", "$,", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das h\u00e4ttest du wol selbst, doch zierlicher, gegessen.", "tokens": ["Das", "h\u00e4t\u00b7test", "du", "wol", "selbst", ",", "doch", "zier\u00b7li\u00b7cher", ",", "ge\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "ADV", "$,", "ADV", "ADJA", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Herr, mein Geschmack ist hier dem deinen gleich.", "tokens": ["Herr", ",", "mein", "Ge\u00b7schmack", "ist", "hier", "dem", "dei\u00b7nen", "gleich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "ART", "PPOSAT", "ADV", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Soll ich, als Wolf, als Mensch, ja R\u00e4ubereien treiben,", "tokens": ["Soll", "ich", ",", "als", "Wolf", ",", "als", "Mensch", ",", "ja", "R\u00e4u\u00b7be\u00b7rei\u00b7en", "trei\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "KOUS", "NE", "$,", "KOUS", "NN", "$,", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So will ich stets ein Wolf verbleiben.", "tokens": ["So", "will", "ich", "stets", "ein", "Wolf", "ver\u00b7blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ART", "NE", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dann bin ich gl\u00fccklicher; die Reue trifft nur euch.", "tokens": ["Dann", "bin", "ich", "gl\u00fcck\u00b7li\u00b7cher", ";", "die", "Reu\u00b7e", "trifft", "nur", "euch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$.", "ART", "NN", "VVFIN", "ADV", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.31": {"line.1": {"text": "Laertens Sohn erforscht die \u00fcbrigen Gef\u00e4hrten,", "tokens": ["Laer\u00b7tens", "Sohn", "er\u00b7forscht", "die", "\u00fcb\u00b7ri\u00b7gen", "Ge\u00b7f\u00e4hr\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Und die erkl\u00e4ren sich, wie jene sich erkl\u00e4rten.", "tokens": ["Und", "die", "er\u00b7kl\u00e4\u00b7ren", "sich", ",", "wie", "je\u00b7ne", "sich", "er\u00b7kl\u00e4r\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVFIN", "PRF", "$,", "PWAV", "PDS", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sie sind mit Lust den Thieren zugesellt.", "tokens": ["Sie", "sind", "mit", "Lust", "den", "Thie\u00b7ren", "zu\u00b7ge\u00b7sellt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Stand, Ruhm, Unsterblichkeit reizt sie zu keinem Neide.", "tokens": ["Stand", ",", "Ruhm", ",", "U\u00b7nsterb\u00b7lich\u00b7keit", "reizt", "sie", "zu", "kei\u00b7nem", "Nei\u00b7de", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "VVFIN", "PPER", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der freie Wald ist aller Freude.", "tokens": ["Der", "frei\u00b7e", "Wald", "ist", "al\u00b7ler", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht weiser ist der Mensch: er w\u00e4hlt, was ihm gef\u00e4llt.", "tokens": ["Nicht", "wei\u00b7ser", "ist", "der", "Mensch", ":", "er", "w\u00e4hlt", ",", "was", "ihm", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VAFIN", "ART", "NN", "$.", "PPER", "VVFIN", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.32": {"line.1": {"text": "Und was gef\u00e4llt uns denn? Kann Wahrheit uns vergn\u00fcgen?", "tokens": ["Und", "was", "ge\u00b7f\u00e4llt", "uns", "denn", "?", "Kann", "Wahr\u00b7heit", "uns", "ver\u00b7gn\u00fc\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "ADV", "$.", "VMFIN", "NN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "O nein! wir sind geneigt, uns selber zu betr\u00fcgen.", "tokens": ["O", "nein", "!", "wir", "sind", "ge\u00b7neigt", ",", "uns", "sel\u00b7ber", "zu", "be\u00b7tr\u00fc\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$.", "PPER", "VAFIN", "VVPP", "$,", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Empfindungen weicht unsrer Schl\u00fcsse Kraft.", "tokens": ["Emp\u00b7fin\u00b7dun\u00b7gen", "weicht", "uns\u00b7rer", "Schl\u00fcs\u00b7se", "Kraft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-++-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Vergn\u00fcget uns ein Recht, das Aller Wohlfahrt st\u00fctzet?", "tokens": ["Ver\u00b7gn\u00fc\u00b7get", "uns", "ein", "Recht", ",", "das", "Al\u00b7ler", "Wohl\u00b7fahrt", "st\u00fct\u00b7zet", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So lang es unsrer Absicht n\u00fctzet.", "tokens": ["So", "lang", "es", "uns\u00b7rer", "Ab\u00b7sicht", "n\u00fct\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was unser Thun bestimmt, ist Wahn und Leidenschaft.", "tokens": ["Was", "un\u00b7ser", "Thun", "be\u00b7stimmt", ",", "ist", "Wahn", "und", "Lei\u00b7den\u00b7schaft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVPP", "$,", "VAFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}