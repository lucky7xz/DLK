{"textgrid.poem.24679": {"metadata": {"author": {"name": "Hofmannsthal, Hugo von", "birth": "N.A.", "death": "N.A."}, "title": "Sonett der Welt", "genre": "verse", "period": "N.A.", "pub_year": 1891, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Unser Leiden, unsre Wonnen", "tokens": ["Un\u00b7ser", "Lei\u00b7den", ",", "uns\u00b7re", "Won\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Spiegelt uns die Allnatur,", "tokens": ["Spie\u00b7gelt", "uns", "die", "All\u00b7na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ewig gilt es unsrer Spur,", "tokens": ["E\u00b7wig", "gilt", "es", "uns\u00b7rer", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alles wird zum Gleichnisbronnen:", "tokens": ["Al\u00b7les", "wird", "zum", "Gleich\u00b7nis\u00b7bron\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Erstes Gr\u00fcn der frischen Flur,", "tokens": ["Ers\u00b7tes", "Gr\u00fcn", "der", "fri\u00b7schen", "Flur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mahnst an Neigung, zart begonnen,", "tokens": ["Mahnst", "an", "Nei\u00b7gung", ",", "zart", "be\u00b7gon\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hei\u00dfes Sengen reifer Sonnen,", "tokens": ["Hei\u00b7\u00dfes", "Sen\u00b7gen", "rei\u00b7fer", "Son\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bist der Liebe Abglanz nur!", "tokens": ["Bist", "der", "Lie\u00b7be", "Ab\u00b7glanz", "nur", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Schlingt sich um den Baum die Winde,", "tokens": ["Schlingt", "sich", "um", "den", "Baum", "die", "Win\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denken wir an uns aufs neue,", "tokens": ["Den\u00b7ken", "wir", "an", "uns", "aufs", "neu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PPER", "APPRART", "ADJA", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Sehnen uns nach einer Treue,", "tokens": ["Seh\u00b7nen", "uns", "nach", "ei\u00b7ner", "Treu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Die uns fest und z\u00e4rtlich binde ...", "tokens": ["Die", "uns", "fest", "und", "z\u00e4rt\u00b7lich", "bin\u00b7de", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKVZ", "KON", "ADJD", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und wir f\u00fchlen uns verwandt,", "tokens": ["Und", "wir", "f\u00fch\u00b7len", "uns", "ver\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie wir unser Bild erkannt.", "tokens": ["Wie", "wir", "un\u00b7ser", "Bild", "er\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Unser Leiden, unsre Wonnen", "tokens": ["Un\u00b7ser", "Lei\u00b7den", ",", "uns\u00b7re", "Won\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Spiegelt uns die Allnatur,", "tokens": ["Spie\u00b7gelt", "uns", "die", "All\u00b7na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ewig gilt es unsrer Spur,", "tokens": ["E\u00b7wig", "gilt", "es", "uns\u00b7rer", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alles wird zum Gleichnisbronnen:", "tokens": ["Al\u00b7les", "wird", "zum", "Gleich\u00b7nis\u00b7bron\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Erstes Gr\u00fcn der frischen Flur,", "tokens": ["Ers\u00b7tes", "Gr\u00fcn", "der", "fri\u00b7schen", "Flur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mahnst an Neigung, zart begonnen,", "tokens": ["Mahnst", "an", "Nei\u00b7gung", ",", "zart", "be\u00b7gon\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hei\u00dfes Sengen reifer Sonnen,", "tokens": ["Hei\u00b7\u00dfes", "Sen\u00b7gen", "rei\u00b7fer", "Son\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bist der Liebe Abglanz nur!", "tokens": ["Bist", "der", "Lie\u00b7be", "Ab\u00b7glanz", "nur", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Schlingt sich um den Baum die Winde,", "tokens": ["Schlingt", "sich", "um", "den", "Baum", "die", "Win\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denken wir an uns aufs neue,", "tokens": ["Den\u00b7ken", "wir", "an", "uns", "aufs", "neu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PPER", "APPRART", "ADJA", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Sehnen uns nach einer Treue,", "tokens": ["Seh\u00b7nen", "uns", "nach", "ei\u00b7ner", "Treu\u00b7e", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Die uns fest und z\u00e4rtlich binde ...", "tokens": ["Die", "uns", "fest", "und", "z\u00e4rt\u00b7lich", "bin\u00b7de", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKVZ", "KON", "ADJD", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und wir f\u00fchlen uns verwandt,", "tokens": ["Und", "wir", "f\u00fch\u00b7len", "uns", "ver\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie wir unser Bild erkannt.", "tokens": ["Wie", "wir", "un\u00b7ser", "Bild", "er\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}