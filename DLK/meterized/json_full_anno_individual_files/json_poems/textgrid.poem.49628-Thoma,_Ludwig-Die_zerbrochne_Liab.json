{"textgrid.poem.49628": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Die zerbrochne Liab", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sie war'n so guate Freund',", "tokens": ["Sie", "wa\u00b7r'n", "so", "gua\u00b7te", "Freund'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sie hamm si herzli g'liabt,", "tokens": ["Sie", "hamm", "si", "herz\u00b7li", "g'\u00b7li\u00b7abt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Koan oanzigs W\u00f6lkerl hat", "tokens": ["Koan", "o\u00b7an\u00b7zigs", "W\u00f6l\u00b7kerl", "hat"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den sch\u00f6nen Himmi tr\u00fcabt.", "tokens": ["Den", "sch\u00f6\u00b7nen", "Him\u00b7mi", "tr\u00fc\u00b7abt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "Sie hielten lange Zeit", "tokens": ["Sie", "hiel\u00b7ten", "lan\u00b7ge", "Zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wia Stahl und Eisen z'samm,", "tokens": ["Wia", "Stahl", "und", "Ei\u00b7sen", "z'\u00b7samm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Und was der oane will,", "tokens": ["Und", "was", "der", "o\u00b7a\u00b7ne", "will", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VMFIN", "$,"], "meter": "----+-+", "measure": "unknown.measure.di"}, "line.4": {"text": "D\u00f6s mua\u00df da ander hamm.", "tokens": ["D\u00f6s", "mu\u00b7a\u00df", "da", "an\u00b7der", "hamm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Und braucht da B\u00fclow was,", "tokens": ["Und", "braucht", "da", "B\u00fc\u00b7low", "was", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "NE", "PWS", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sprach er zum Pfaffen glei:", "tokens": ["Sprach", "er", "zum", "Pfaf\u00b7fen", "glei", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "ADV", "$."], "meter": "---+-+", "measure": "unknown.measure.di"}, "line.3": {"text": "\u00bbo Sie, mein liaba Freund,", "tokens": ["\u00bb", "o", "Sie", ",", "mein", "li\u00b7a\u00b7ba", "Freund", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "FM", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "I hab' koa Geld dabei.\u00ab", "tokens": ["I", "hab'", "koa", "Geld", "da\u00b7bei", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["XY", "VAFIN", "NE", "NN", "PAV", "$.", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.4": {"line.1": {"text": "Da Pfaff hat wieda g'sagt:", "tokens": ["Da", "Pfaff", "hat", "wie\u00b7da", "g'\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ADV", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "\u00bbich will f\u00fcr Sie bezahl'n,", "tokens": ["\u00bb", "ich", "will", "f\u00fcr", "Sie", "be\u00b7zahl'n", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Weil Sie mei Spezi san;", "tokens": ["Weil", "Sie", "mei", "Spe\u00b7zi", "san", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "FM", "FM", "FM", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Sie tean mir aa'r an G'fall'n.\u00ab", "tokens": ["Sie", "te\u00b7an", "mir", "aa'r", "an", "G'\u00b7fall'", "n.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.5": {"line.1": {"text": "So lebten sie dahin", "tokens": ["So", "leb\u00b7ten", "sie", "da\u00b7hin"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "In sch\u00f6nster Einigkeit,", "tokens": ["In", "sch\u00f6ns\u00b7ter", "Ei\u00b7nig\u00b7keit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "In ihrem Freundesbund,", "tokens": ["In", "ih\u00b7rem", "Freun\u00b7des\u00b7bund", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Da hat sich nie was g'feit.", "tokens": ["Da", "hat", "sich", "nie", "was", "g'\u00b7feit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "ADV", "PWS", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.6": {"line.1": {"text": "Auf oamal war es aus;", "tokens": ["Auf", "o\u00b7a\u00b7mal", "war", "es", "aus", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "VAFIN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So geht's auf dieser Welt.", "tokens": ["So", "geht's", "auf", "die\u00b7ser", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Auch diese Liab zerri\u00df,", "tokens": ["Auch", "die\u00b7se", "Li\u00b7ab", "zer\u00b7ri\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wia manche \u2013 z'weg'n an Geld.", "tokens": ["Wia", "man\u00b7che", "\u2013", "z'\u00b7weg'n", "an", "Geld", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "$(", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.7": {"line.1": {"text": "Zum B\u00fclow is a Freund", "tokens": ["Zum", "B\u00fc\u00b7low", "is", "a", "Freund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NE", "FM", "FM", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ganz hoamli zuawi g'roast,", "tokens": ["Ganz", "hoam\u00b7li", "zua\u00b7wi", "g'\u00b7roast", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "FM", "FM", "FM", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der wo ihm sehr gefallt,", "tokens": ["Der", "wo", "ihm", "sehr", "ge\u00b7fallt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Der wo sich Dernburg hoa\u00dft.", "tokens": ["Der", "wo", "sich", "Dern\u00b7burg", "hoa\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PWAV", "PRF", "NE", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.8": {"line.1": {"text": "Da sagt der Pfaff voll Zorn:", "tokens": ["Da", "sagt", "der", "Pfaff", "voll", "Zorn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NE", "ADJD", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbd\u00f6s war d\u00f6s letztemal,", "tokens": ["\u00bb", "d\u00f6s", "war", "d\u00f6s", "letz\u00b7te\u00b7mal", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "PDS", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wenn Sie an andern liab'n,", "tokens": ["Wenn", "Sie", "an", "an\u00b7dern", "li\u00b7a\u00b7b'n", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df i f\u00fcr Eahna zahl'.\u00ab", "tokens": ["Da\u00df", "i", "f\u00fcr", "E\u00b7ah\u00b7na", "zahl'", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NE", "APPR", "NE", "NE", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.9": {"line.1": {"text": "Er hat eahm nix mehr g'schenkt,", "tokens": ["Er", "hat", "e\u00b7ahm", "nix", "mehr", "g'\u00b7schenkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "FM", "FM", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er halt' sein Beutel zua!", "tokens": ["Er", "halt'", "sein", "Beu\u00b7tel", "zua", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Jetzt is die Freundschaft aus.", "tokens": ["Jetzt", "is", "die", "Freund\u00b7schaft", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "FM", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Herr, gib ihr d' ewig Ruah!", "tokens": ["Herr", ",", "gib", "ihr", "d'", "e\u00b7wig", "Ru\u00b7ah", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVIMP", "PPER", "ADV", "ADJD", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 huhu \u2013 huhu \u2013 hu!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "hu\u00b7hu", "\u2013", "hu\u00b7hu", "\u2013", "hu", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "XY", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Sie war'n so guate Freund',", "tokens": ["Sie", "wa\u00b7r'n", "so", "gua\u00b7te", "Freund'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sie hamm si herzli g'liabt,", "tokens": ["Sie", "hamm", "si", "herz\u00b7li", "g'\u00b7li\u00b7abt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Koan oanzigs W\u00f6lkerl hat", "tokens": ["Koan", "o\u00b7an\u00b7zigs", "W\u00f6l\u00b7kerl", "hat"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den sch\u00f6nen Himmi tr\u00fcabt.", "tokens": ["Den", "sch\u00f6\u00b7nen", "Him\u00b7mi", "tr\u00fc\u00b7abt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.11": {"line.1": {"text": "Sie hielten lange Zeit", "tokens": ["Sie", "hiel\u00b7ten", "lan\u00b7ge", "Zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wia Stahl und Eisen z'samm,", "tokens": ["Wia", "Stahl", "und", "Ei\u00b7sen", "z'\u00b7samm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Und was der oane will,", "tokens": ["Und", "was", "der", "o\u00b7a\u00b7ne", "will", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VMFIN", "$,"], "meter": "----+-+", "measure": "unknown.measure.di"}, "line.4": {"text": "D\u00f6s mua\u00df da ander hamm.", "tokens": ["D\u00f6s", "mu\u00b7a\u00df", "da", "an\u00b7der", "hamm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.12": {"line.1": {"text": "Und braucht da B\u00fclow was,", "tokens": ["Und", "braucht", "da", "B\u00fc\u00b7low", "was", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "NE", "PWS", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sprach er zum Pfaffen glei:", "tokens": ["Sprach", "er", "zum", "Pfaf\u00b7fen", "glei", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "ADV", "$."], "meter": "---+-+", "measure": "unknown.measure.di"}, "line.3": {"text": "\u00bbo Sie, mein liaba Freund,", "tokens": ["\u00bb", "o", "Sie", ",", "mein", "li\u00b7a\u00b7ba", "Freund", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "FM", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "I hab' koa Geld dabei.\u00ab", "tokens": ["I", "hab'", "koa", "Geld", "da\u00b7bei", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["XY", "VAFIN", "NE", "NN", "PAV", "$.", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.13": {"line.1": {"text": "Da Pfaff hat wieda g'sagt:", "tokens": ["Da", "Pfaff", "hat", "wie\u00b7da", "g'\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ADV", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "\u00bbich will f\u00fcr Sie bezahl'n,", "tokens": ["\u00bb", "ich", "will", "f\u00fcr", "Sie", "be\u00b7zahl'n", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Weil Sie mei Spezi san;", "tokens": ["Weil", "Sie", "mei", "Spe\u00b7zi", "san", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "FM", "FM", "FM", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Sie tean mir aa'r an G'fall'n.\u00ab", "tokens": ["Sie", "te\u00b7an", "mir", "aa'r", "an", "G'\u00b7fall'", "n.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.14": {"line.1": {"text": "So lebten sie dahin", "tokens": ["So", "leb\u00b7ten", "sie", "da\u00b7hin"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "In sch\u00f6nster Einigkeit,", "tokens": ["In", "sch\u00f6ns\u00b7ter", "Ei\u00b7nig\u00b7keit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "In ihrem Freundesbund,", "tokens": ["In", "ih\u00b7rem", "Freun\u00b7des\u00b7bund", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Da hat sich nie was g'feit.", "tokens": ["Da", "hat", "sich", "nie", "was", "g'\u00b7feit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "ADV", "PWS", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.15": {"line.1": {"text": "Auf oamal war es aus;", "tokens": ["Auf", "o\u00b7a\u00b7mal", "war", "es", "aus", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "VAFIN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So geht's auf dieser Welt.", "tokens": ["So", "geht's", "auf", "die\u00b7ser", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Auch diese Liab zerri\u00df,", "tokens": ["Auch", "die\u00b7se", "Li\u00b7ab", "zer\u00b7ri\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wia manche \u2013 z'weg'n an Geld.", "tokens": ["Wia", "man\u00b7che", "\u2013", "z'\u00b7weg'n", "an", "Geld", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "$(", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.16": {"line.1": {"text": "Zum B\u00fclow is a Freund", "tokens": ["Zum", "B\u00fc\u00b7low", "is", "a", "Freund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NE", "FM", "FM", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ganz hoamli zuawi g'roast,", "tokens": ["Ganz", "hoam\u00b7li", "zua\u00b7wi", "g'\u00b7roast", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "FM", "FM", "FM", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der wo ihm sehr gefallt,", "tokens": ["Der", "wo", "ihm", "sehr", "ge\u00b7fallt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Der wo sich Dernburg hoa\u00dft.", "tokens": ["Der", "wo", "sich", "Dern\u00b7burg", "hoa\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PWAV", "PRF", "NE", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.17": {"line.1": {"text": "Da sagt der Pfaff voll Zorn:", "tokens": ["Da", "sagt", "der", "Pfaff", "voll", "Zorn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NE", "ADJD", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbd\u00f6s war d\u00f6s letztemal,", "tokens": ["\u00bb", "d\u00f6s", "war", "d\u00f6s", "letz\u00b7te\u00b7mal", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "PDS", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wenn Sie an andern liab'n,", "tokens": ["Wenn", "Sie", "an", "an\u00b7dern", "li\u00b7a\u00b7b'n", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df i f\u00fcr Eahna zahl'.\u00ab", "tokens": ["Da\u00df", "i", "f\u00fcr", "E\u00b7ah\u00b7na", "zahl'", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NE", "APPR", "NE", "NE", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 hohu \u2013 hohu \u2013 ho!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "ho\u00b7hu", "\u2013", "ho\u00b7hu", "\u2013", "ho", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "ITJ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.18": {"line.1": {"text": "Er hat eahm nix mehr g'schenkt,", "tokens": ["Er", "hat", "e\u00b7ahm", "nix", "mehr", "g'\u00b7schenkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "FM", "FM", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er halt' sein Beutel zua!", "tokens": ["Er", "halt'", "sein", "Beu\u00b7tel", "zua", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Jetzt is die Freundschaft aus.", "tokens": ["Jetzt", "is", "die", "Freund\u00b7schaft", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "FM", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Herr, gib ihr d' ewig Ruah!", "tokens": ["Herr", ",", "gib", "ihr", "d'", "e\u00b7wig", "Ru\u00b7ah", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVIMP", "PPER", "ADV", "ADJD", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Huli\u00f6 \u2013 hu \u2013 huhu \u2013 huhu \u2013 hu!", "tokens": ["Hu\u00b7li\u00f6", "\u2013", "hu", "\u2013", "hu\u00b7hu", "\u2013", "hu\u00b7hu", "\u2013", "hu", "!"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "XY", "$(", "XY", "$(", "XY", "$(", "XY", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}}}}