{"textgrid.poem.49764": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "An die Kritiker", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Den Kritikern als Strafvollstreckern,", "tokens": ["Den", "Kri\u00b7ti\u00b7kern", "als", "Straf\u00b7voll\u00b7stre\u00b7ckern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als einer Art von Polizei,", "tokens": ["Als", "ei\u00b7ner", "Art", "von", "Po\u00b7li\u00b7zei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Stand, einen Dichter zu bekleckern", "tokens": ["Stand", ",", "ei\u00b7nen", "Dich\u00b7ter", "zu", "be\u00b7kle\u00b7ckern"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das sch\u00f6ne Vorrecht immer frei.", "tokens": ["Das", "sch\u00f6\u00b7ne", "Vor\u00b7recht", "im\u00b7mer", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Doch: animal \u2013 so hei\u00dft es \u2013 omne", "tokens": ["Doch", ":", "a\u00b7ni\u00b7mal", "\u2013", "so", "hei\u00dft", "es", "\u2013", "om\u00b7ne"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["KON", "$.", "ADV", "$(", "ADV", "VVFIN", "PPER", "$(", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist triste erst post coitum,", "tokens": ["Ist", "tris\u00b7te", "erst", "post", "coi\u00b7tum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Erst ", "tokens": ["Erst"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Belehrung f\u00fcr das Publikum.", "tokens": ["Be\u00b7leh\u00b7rung", "f\u00fcr", "das", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein Werk verrei\u00dfen, scheint verfr\u00fcht,", "tokens": ["Ein", "Werk", "ver\u00b7rei\u00b7\u00dfen", ",", "scheint", "ver\u00b7fr\u00fcht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es findet, was den Dichter kr\u00e4nkte,", "tokens": ["Es", "fin\u00b7det", ",", "was", "den", "Dich\u00b7ter", "kr\u00e4nk\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auch sp\u00e4ter, wer sich redlich m\u00fcht.", "tokens": ["Auch", "sp\u00e4\u00b7ter", ",", "wer", "sich", "red\u00b7lich", "m\u00fcht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PWS", "PRF", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Wie macht's der Spatz, wenn aus dem Pferde", "tokens": ["Wie", "macht's", "der", "Spatz", ",", "wenn", "aus", "dem", "Pfer\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Apfel rollt, den er gebraucht?", "tokens": ["Der", "Ap\u00b7fel", "rollt", ",", "den", "er", "ge\u00b7braucht", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PRELS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er l\u00e4\u00dft ihn k\u00fchlen auf der Erde,", "tokens": ["Er", "l\u00e4\u00dft", "ihn", "k\u00fch\u00b7len", "auf", "der", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zerhackt ihn nicht, solang' er raucht.", "tokens": ["Zer\u00b7hackt", "ihn", "nicht", ",", "so\u00b7lang'", "er", "raucht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$,", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "So seht auch ihr beim Lampenscheine", "tokens": ["So", "seht", "auch", "ihr", "beim", "Lam\u00b7pen\u00b7schei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "PPER", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Ruh', was Pegasus verlor,", "tokens": ["Mit", "Ruh'", ",", "was", "Pe\u00b7ga\u00b7sus", "ver\u00b7lor", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dann nehme jeder sich das seine", "tokens": ["Dann", "neh\u00b7me", "je\u00b7der", "sich", "das", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "ART", "PPOSAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Aus dessen \u00c4pfeln klug hervor.", "tokens": ["Aus", "des\u00b7sen", "\u00c4p\u00b7feln", "klug", "her\u00b7vor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Den Kritikern als Strafvollstreckern,", "tokens": ["Den", "Kri\u00b7ti\u00b7kern", "als", "Straf\u00b7voll\u00b7stre\u00b7ckern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als einer Art von Polizei,", "tokens": ["Als", "ei\u00b7ner", "Art", "von", "Po\u00b7li\u00b7zei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Stand, einen Dichter zu bekleckern", "tokens": ["Stand", ",", "ei\u00b7nen", "Dich\u00b7ter", "zu", "be\u00b7kle\u00b7ckern"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das sch\u00f6ne Vorrecht immer frei.", "tokens": ["Das", "sch\u00f6\u00b7ne", "Vor\u00b7recht", "im\u00b7mer", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Doch: animal \u2013 so hei\u00dft es \u2013 omne", "tokens": ["Doch", ":", "a\u00b7ni\u00b7mal", "\u2013", "so", "hei\u00dft", "es", "\u2013", "om\u00b7ne"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["KON", "$.", "ADV", "$(", "ADV", "VVFIN", "PPER", "$(", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist triste erst post coitum,", "tokens": ["Ist", "tris\u00b7te", "erst", "post", "coi\u00b7tum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Erst ", "tokens": ["Erst"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Belehrung f\u00fcr das Publikum.", "tokens": ["Be\u00b7leh\u00b7rung", "f\u00fcr", "das", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ein Werk verrei\u00dfen, scheint verfr\u00fcht,", "tokens": ["Ein", "Werk", "ver\u00b7rei\u00b7\u00dfen", ",", "scheint", "ver\u00b7fr\u00fcht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es findet, was den Dichter kr\u00e4nkte,", "tokens": ["Es", "fin\u00b7det", ",", "was", "den", "Dich\u00b7ter", "kr\u00e4nk\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auch sp\u00e4ter, wer sich redlich m\u00fcht.", "tokens": ["Auch", "sp\u00e4\u00b7ter", ",", "wer", "sich", "red\u00b7lich", "m\u00fcht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PWS", "PRF", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wie macht's der Spatz, wenn aus dem Pferde", "tokens": ["Wie", "macht's", "der", "Spatz", ",", "wenn", "aus", "dem", "Pfer\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Apfel rollt, den er gebraucht?", "tokens": ["Der", "Ap\u00b7fel", "rollt", ",", "den", "er", "ge\u00b7braucht", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PRELS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er l\u00e4\u00dft ihn k\u00fchlen auf der Erde,", "tokens": ["Er", "l\u00e4\u00dft", "ihn", "k\u00fch\u00b7len", "auf", "der", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zerhackt ihn nicht, solang' er raucht.", "tokens": ["Zer\u00b7hackt", "ihn", "nicht", ",", "so\u00b7lang'", "er", "raucht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$,", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "So seht auch ihr beim Lampenscheine", "tokens": ["So", "seht", "auch", "ihr", "beim", "Lam\u00b7pen\u00b7schei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "PPER", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Ruh', was Pegasus verlor,", "tokens": ["Mit", "Ruh'", ",", "was", "Pe\u00b7ga\u00b7sus", "ver\u00b7lor", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dann nehme jeder sich das seine", "tokens": ["Dann", "neh\u00b7me", "je\u00b7der", "sich", "das", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "ART", "PPOSAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Aus dessen \u00c4pfeln klug hervor.", "tokens": ["Aus", "des\u00b7sen", "\u00c4p\u00b7feln", "klug", "her\u00b7vor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}