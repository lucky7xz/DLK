{"dta.poem.2693": {"metadata": {"author": {"name": "M\u00fchlpfort, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Die Himmel-blaue Jris  \n  Bey Beerdigung Hn. W. G. hinterlassenen  \n T\u00f6chterleins J. R. den 18. Novembr. 1676.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1686", "urn": "urn:nbn:de:kobv:b4-20414-7", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Betr\u00fcbtste Frau/ wo ihren Thr\u00e4nen", "tokens": ["Be\u00b7tr\u00b7\u00fcbts\u00b7te", "Frau", "/", "wo", "ih\u00b7ren", "Thr\u00e4\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJA", "NN", "$(", "PWAV", "PPOSAT", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ein eintzig Anstand ist vergunt/", "tokens": ["Ein", "eint\u00b7zig", "An\u00b7stand", "ist", "ver\u00b7gunt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "VAFIN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und ihr mit Angst erf\u00fclltes Sehnen", "tokens": ["Und", "ihr", "mit", "Angst", "er\u00b7f\u00fcll\u00b7tes", "Seh\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "APPR", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Hertze nicht durchaus verwundt;", "tokens": ["Das", "Hert\u00b7ze", "nicht", "durc\u00b7haus", "ver\u00b7wundt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So blicke sie doch diese Zeilen", "tokens": ["So", "bli\u00b7cke", "sie", "doch", "die\u00b7se", "Zei\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Aus ihrem tieffsten Trauren an/", "tokens": ["Aus", "ih\u00b7rem", "tieffs\u00b7ten", "Trau\u00b7ren", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Sie sollen tr\u00f6sten/ wo nicht heilen/", "tokens": ["Sie", "sol\u00b7len", "tr\u00f6s\u00b7ten", "/", "wo", "nicht", "hei\u00b7len", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$(", "PWAV", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und selbst mit Leid seyn angethan.", "tokens": ["Und", "selbst", "mit", "Leid", "seyn", "an\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich geb es zu/ da\u00df ihrer Schmertzen", "tokens": ["Ich", "geb", "es", "zu", "/", "da\u00df", "ih\u00b7rer", "Schmert\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PTKZU", "$(", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht leicht was zuvergleichen ist.", "tokens": ["Nicht", "leicht", "was", "zu\u00b7ver\u00b7glei\u00b7chen", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "PIS", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Weil erst die Helffte von dem Hertzen", "tokens": ["Weil", "erst", "die", "Helff\u00b7te", "von", "dem", "Hert\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie ihren Ehschatz fr\u00fch vermist.", "tokens": ["Sie", "ih\u00b7ren", "Eh\u00b7schatz", "fr\u00fch", "ver\u00b7mist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und denn/ nach zwey verflo\u00dfnen Jahren/", "tokens": ["Und", "denn", "/", "nach", "zwey", "ver\u00b7flo\u00df\u00b7nen", "Jah\u00b7ren", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "APPR", "CARD", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Jhr eintzig Ebenbild und Pfand/", "tokens": ["Ihr", "eint\u00b7zig", "E\u00b7ben\u00b7bild", "und", "Pfand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Sieht gleichfals zu den Todten fahren", "tokens": ["Sieht", "gleich\u00b7fals", "zu", "den", "Tod\u00b7ten", "fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und eingescharret in den Sand.", "tokens": ["Und", "ein\u00b7ge\u00b7schar\u00b7ret", "in", "den", "Sand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Gedoppelt ist das Ungewitter/", "tokens": ["Ge\u00b7dop\u00b7pelt", "ist", "das", "Un\u00b7ge\u00b7wit\u00b7ter", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So \u00fcber ihrem Haupt erkracht.", "tokens": ["So", "\u00fc\u00b7ber", "ih\u00b7rem", "Haupt", "er\u00b7kracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dtr Creutz-Kelch nur zu voll und bitter/", "tokens": ["Dtr", "Creutz\u00b7Kelch", "nur", "zu", "voll", "und", "bit\u00b7ter", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "PTKA", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der ihr anjetzt wird beygebracht.", "tokens": ["Der", "ihr", "an\u00b7jetzt", "wird", "bey\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VAFIN", "VVPP", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Bricht doch die unerschrocknen Palmen", "tokens": ["Bricht", "doch", "die", "un\u00b7er\u00b7schrock\u00b7nen", "Pal\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Gantz \u00fcberh\u00e4uffte Last entzwey.", "tokens": ["Gantz", "\u00fc\u00b7berh\u00b7\u00e4uff\u00b7te", "Last", "ent\u00b7zwey", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und Ertz und Felsen mu\u00df zermalmen", "tokens": ["Und", "Ertz", "und", "Fel\u00b7sen", "mu\u00df", "zer\u00b7mal\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Der wiederholten St\u00f6sse Reih.", "tokens": ["Der", "wie\u00b7der\u00b7hol\u00b7ten", "St\u00f6s\u00b7se", "Reih", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Jedoch sie kan des Himmels-Schl\u00fcssen", "tokens": ["Je\u00b7doch", "sie", "kan", "des", "Him\u00b7mels\u00b7Sch\u00b7l\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VMFIN", "ART", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und Satzungen sich nicht entziehn.", "tokens": ["Und", "Sat\u00b7zun\u00b7gen", "sich", "nicht", "ent\u00b7ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Gott wil sie wol bewehret wissen/", "tokens": ["Gott", "wil", "sie", "wol", "be\u00b7weh\u00b7ret", "wis\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "ADV", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Jhr Glaubens-Gold sol also gl\u00fchn.", "tokens": ["Ihr", "Glau\u00b7bens\u00b7Gold", "sol", "al\u00b7so", "gl\u00fchn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Zudem sind ihr die Wechsel-G\u00e4nge", "tokens": ["Zu\u00b7dem", "sind", "ihr", "die", "Wech\u00b7sel\u00b7G\u00e4n\u00b7ge"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der Sterbligkeit gar wol bekand;", "tokens": ["Der", "Ster\u00b7blig\u00b7keit", "gar", "wol", "be\u00b7kand", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wie alles Fleisches G\u00fctt und Menge", "tokens": ["Wie", "al\u00b7les", "Flei\u00b7sches", "G\u00fctt", "und", "Men\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Sey unterthan des Todes Hand.", "tokens": ["Sey", "un\u00b7ter\u00b7than", "des", "To\u00b7des", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Jhr Kind trat in der W\u00e4ysen Orden/", "tokens": ["Ihr", "Kind", "trat", "in", "der", "W\u00e4y\u00b7sen", "Or\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Eh es das Licht der Welt erblickt.", "tokens": ["Eh", "es", "das", "Licht", "der", "Welt", "er\u00b7blickt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Ist wieder/ was es vor war/ worden", "tokens": ["Ist", "wie\u00b7der", "/", "was", "es", "vor", "war", "/", "wor\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["VAFIN", "ADV", "$(", "PWS", "PPER", "APPR", "VAFIN", "$(", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und zu der Mutter Scho\u00df geschickt.", "tokens": ["Und", "zu", "der", "Mut\u00b7ter", "Scho\u00df", "ge\u00b7schickt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der Mutter Leib/ die Scho\u00df der Erde/", "tokens": ["Der", "Mut\u00b7ter", "Leib", "/", "die", "Scho\u00df", "der", "Er\u00b7de", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Gab sie nechst her/ die nimmt sie an/", "tokens": ["Gab", "sie", "nechst", "her", "/", "die", "nimmt", "sie", "an", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKVZ", "$(", "ART", "VVFIN", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und lehret da\u00df dergleichen werde", "tokens": ["Und", "leh\u00b7ret", "da\u00df", "derg\u00b7lei\u00b7chen", "wer\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "KOUS", "PIS", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Mit jedem Sterblichen gethan.", "tokens": ["Mit", "je\u00b7dem", "Sterb\u00b7li\u00b7chen", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}