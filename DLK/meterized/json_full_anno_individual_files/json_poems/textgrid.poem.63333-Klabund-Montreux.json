{"textgrid.poem.63333": {"metadata": {"author": {"name": "Klabund", "birth": "N.A.", "death": "N.A."}, "title": "Montreux", "genre": "verse", "period": "N.A.", "pub_year": 1909, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hier sieht die Landschaft man nicht vor Hotels.", "tokens": ["Hier", "sieht", "die", "Land\u00b7schaft", "man", "nicht", "vor", "Ho\u00b7tels", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PIS", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Es riecht nach Beefsteak und nach faulen Eiern.", "tokens": ["Es", "riecht", "nach", "Beefs\u00b7teak", "und", "nach", "fau\u00b7len", "Ei\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "KON", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Schlo\u00df Chillon steht betr\u00fcbt auf einem Fels", "tokens": ["Schlo\u00df", "Chil\u00b7lon", "steht", "be\u00b7tr\u00fcbt", "auf", "ei\u00b7nem", "Fels"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und ist ber\u00fchmt durch Dichtungen von Byron.", "tokens": ["Und", "ist", "be\u00b7r\u00fchmt", "durch", "Dich\u00b7tun\u00b7gen", "von", "By\u00b7ron", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "APPR", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Der Tag beginnt mit einem fetten Lunch,", "tokens": ["Der", "Tag", "be\u00b7ginnt", "mit", "ei\u00b7nem", "fet\u00b7ten", "Lunch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dann schiebt zum Liegestuhl man sacht den vollen", "tokens": ["Dann", "schiebt", "zum", "Lie\u00b7ge\u00b7stuhl", "man", "sacht", "den", "vol\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "PIS", "VVFIN", "ART", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Geliebten Bauch. Und Wesen, die sich Mensch", "tokens": ["Ge\u00b7lieb\u00b7ten", "Bauch", ".", "Und", "We\u00b7sen", ",", "die", "sich", "Mensch"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "KON", "NN", "$,", "PRELS", "PRF", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "(mit Unrecht) nennen, h\u00fcgelabw\u00e4rts rollen.", "tokens": ["(", "mit", "Un\u00b7recht", ")", "nen\u00b7nen", ",", "h\u00fc\u00b7gel\u00b7ab\u00b7w\u00e4rts", "rol\u00b7len", "."], "token_info": ["punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "$(", "VVINF", "$,", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Wer unter hundert Franken Rente hat,", "tokens": ["Wer", "un\u00b7ter", "hun\u00b7dert", "Fran\u00b7ken", "Ren\u00b7te", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "CARD", "NN", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "(pro Tag), der ist ein w\u00fcster Proletarier.", "tokens": ["(", "pro", "Tag", ")", ",", "der", "ist", "ein", "w\u00fcs\u00b7ter", "Pro\u00b7le\u00b7ta\u00b7rier", "."], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "$(", "$,", "PRELS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Man fri\u00dft an Hummer sich und Kaviar satt,", "tokens": ["Man", "fri\u00dft", "an", "Hum\u00b7mer", "sich", "und", "Ka\u00b7vi\u00b7ar", "satt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NN", "PRF", "KON", "NN", "ADJD", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und ist kein Kassenha\u00df von Jud' und Arier.", "tokens": ["Und", "ist", "kein", "Kas\u00b7sen\u00b7ha\u00df", "von", "Jud'", "und", "A\u00b7rier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "APPR", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "In tausend Meter H\u00f6he erst ist Luft,", "tokens": ["In", "tau\u00b7send", "Me\u00b7ter", "H\u00f6\u00b7he", "erst", "ist", "Luft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "ADV", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dort findet man zwei \u00e4rmliche Narzissen.", "tokens": ["Dort", "fin\u00b7det", "man", "zwei", "\u00e4rm\u00b7li\u00b7che", "Nar\u00b7zis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "CARD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie wachsen einer Jungfrau aus der Gruft", "tokens": ["Sie", "wach\u00b7sen", "ei\u00b7ner", "Jung\u00b7frau", "aus", "der", "Gruft"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und sind versehentlich nicht ausgerissen.", "tokens": ["Und", "sind", "ver\u00b7se\u00b7hent\u00b7lich", "nicht", "aus\u00b7ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Hier sieht die Landschaft man nicht vor Hotels.", "tokens": ["Hier", "sieht", "die", "Land\u00b7schaft", "man", "nicht", "vor", "Ho\u00b7tels", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PIS", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Es riecht nach Beefsteak und nach faulen Eiern.", "tokens": ["Es", "riecht", "nach", "Beefs\u00b7teak", "und", "nach", "fau\u00b7len", "Ei\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "KON", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Schlo\u00df Chillon steht betr\u00fcbt auf einem Fels", "tokens": ["Schlo\u00df", "Chil\u00b7lon", "steht", "be\u00b7tr\u00fcbt", "auf", "ei\u00b7nem", "Fels"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und ist ber\u00fchmt durch Dichtungen von Byron.", "tokens": ["Und", "ist", "be\u00b7r\u00fchmt", "durch", "Dich\u00b7tun\u00b7gen", "von", "By\u00b7ron", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "APPR", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Der Tag beginnt mit einem fetten Lunch,", "tokens": ["Der", "Tag", "be\u00b7ginnt", "mit", "ei\u00b7nem", "fet\u00b7ten", "Lunch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dann schiebt zum Liegestuhl man sacht den vollen", "tokens": ["Dann", "schiebt", "zum", "Lie\u00b7ge\u00b7stuhl", "man", "sacht", "den", "vol\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "PIS", "VVFIN", "ART", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Geliebten Bauch. Und Wesen, die sich Mensch", "tokens": ["Ge\u00b7lieb\u00b7ten", "Bauch", ".", "Und", "We\u00b7sen", ",", "die", "sich", "Mensch"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "KON", "NN", "$,", "PRELS", "PRF", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "(mit Unrecht) nennen, h\u00fcgelabw\u00e4rts rollen.", "tokens": ["(", "mit", "Un\u00b7recht", ")", "nen\u00b7nen", ",", "h\u00fc\u00b7gel\u00b7ab\u00b7w\u00e4rts", "rol\u00b7len", "."], "token_info": ["punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "$(", "VVINF", "$,", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Wer unter hundert Franken Rente hat,", "tokens": ["Wer", "un\u00b7ter", "hun\u00b7dert", "Fran\u00b7ken", "Ren\u00b7te", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "CARD", "NN", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "(pro Tag), der ist ein w\u00fcster Proletarier.", "tokens": ["(", "pro", "Tag", ")", ",", "der", "ist", "ein", "w\u00fcs\u00b7ter", "Pro\u00b7le\u00b7ta\u00b7rier", "."], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "$(", "$,", "PRELS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Man fri\u00dft an Hummer sich und Kaviar satt,", "tokens": ["Man", "fri\u00dft", "an", "Hum\u00b7mer", "sich", "und", "Ka\u00b7vi\u00b7ar", "satt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NN", "PRF", "KON", "NN", "ADJD", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und ist kein Kassenha\u00df von Jud' und Arier.", "tokens": ["Und", "ist", "kein", "Kas\u00b7sen\u00b7ha\u00df", "von", "Jud'", "und", "A\u00b7rier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "APPR", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "In tausend Meter H\u00f6he erst ist Luft,", "tokens": ["In", "tau\u00b7send", "Me\u00b7ter", "H\u00f6\u00b7he", "erst", "ist", "Luft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "ADV", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dort findet man zwei \u00e4rmliche Narzissen.", "tokens": ["Dort", "fin\u00b7det", "man", "zwei", "\u00e4rm\u00b7li\u00b7che", "Nar\u00b7zis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "CARD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie wachsen einer Jungfrau aus der Gruft", "tokens": ["Sie", "wach\u00b7sen", "ei\u00b7ner", "Jung\u00b7frau", "aus", "der", "Gruft"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und sind versehentlich nicht ausgerissen.", "tokens": ["Und", "sind", "ver\u00b7se\u00b7hent\u00b7lich", "nicht", "aus\u00b7ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}