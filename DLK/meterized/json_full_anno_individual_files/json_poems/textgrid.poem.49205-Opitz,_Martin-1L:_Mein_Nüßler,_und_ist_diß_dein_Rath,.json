{"textgrid.poem.49205": {"metadata": {"author": {"name": "Opitz, Martin", "birth": "N.A.", "death": "N.A."}, "title": "1L: Mein N\u00fc\u00dfler, und ist di\u00df dein Rath,", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein N\u00fc\u00dfler, und ist di\u00df dein Rath,", "tokens": ["Mein", "N\u00fc\u00df\u00b7ler", ",", "und", "ist", "di\u00df", "dein", "Rath", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "KON", "VAFIN", "PDS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich sol die schn\u00f6de Wollust hassen", "tokens": ["Ich", "sol", "die", "schn\u00f6\u00b7de", "Wol\u00b7lust", "has\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und die, so mich beth\u00f6ret hat,", "tokens": ["Und", "die", ",", "so", "mich", "be\u00b7th\u00f6\u00b7ret", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "$,", "ADV", "PPER", "VVFIN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die sch\u00f6ne Flavia, verlassen?", "tokens": ["Die", "sch\u00f6\u00b7ne", "Fla\u00b7via", ",", "ver\u00b7las\u00b7sen", "?"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Sprich, sagst du, deine Musen an,", "tokens": ["Sprich", ",", "sagst", "du", ",", "dei\u00b7ne", "Mu\u00b7sen", "an", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "VVFIN", "PPER", "$,", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Setz' an die Feder, das zu schreiben,", "tokens": ["Setz'", "an", "die", "Fe\u00b7der", ",", "das", "zu", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$,", "PRELS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Durch welches dein Ger\u00fcchte kan", "tokens": ["Durch", "wel\u00b7ches", "dein", "Ge\u00b7r\u00fcch\u00b7te", "kan"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPOSAT", "NN", "VMFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Ewigkeit hernach bekleiben.", "tokens": ["In", "E\u00b7wig\u00b7keit", "her\u00b7nach", "be\u00b7klei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "La\u00df fahren, die zu wenig ist,", "tokens": ["La\u00df", "fah\u00b7ren", ",", "die", "zu", "we\u00b7nig", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "VVINF", "$,", "PRELS", "APPR", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie die viel gew\u00fcnschten Sachen,", "tokens": ["Da\u00df", "sie", "die", "viel", "ge\u00b7w\u00fcnschten", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die du zu tichten auserkiest,", "tokens": ["Die", "du", "zu", "tich\u00b7ten", "aus\u00b7er\u00b7kiest", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sol ganz und gar zu Wasser machen.", "tokens": ["Sol", "ganz", "und", "gar", "zu", "Was\u00b7ser", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "KON", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Denk' an den Ruhm, den du nunmehr", "tokens": ["Denk'", "an", "den", "Ruhm", ",", "den", "du", "nun\u00b7mehr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVIMP", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bei gro\u00dfen Leuten hast erworben;", "tokens": ["Bei", "gro\u00b7\u00dfen", "Leu\u00b7ten", "hast", "er\u00b7wor\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Seit da\u00df du liebst, ist schier die Ehr'", "tokens": ["Seit", "da\u00df", "du", "liebst", ",", "ist", "schier", "die", "Ehr'"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In ihrer ersten Bl\u00fct' erstorben.", "tokens": ["In", "ih\u00b7rer", "ers\u00b7ten", "Bl\u00fct'", "ers\u00b7tor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "War ist es, ich bin jetzund fast", "tokens": ["War", "ist", "es", ",", "ich", "bin", "je\u00b7tzund", "fast"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "VAFIN", "PPER", "$,", "PPER", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der B\u00fccher \u00e4rgster Todfeind worden,", "tokens": ["Der", "B\u00fc\u00b7cher", "\u00e4rgs\u00b7ter", "Tod\u00b7feind", "wor\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nun Venus, die gew\u00fcnschte Last,", "tokens": ["Nun", "Ve\u00b7nus", ",", "die", "ge\u00b7w\u00fcnschte", "Last", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Mich wieder hat in ihrem Orden.", "tokens": ["Mich", "wie\u00b7der", "hat", "in", "ih\u00b7rem", "Or\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Doch bin ich sehr in Zweifel noch,", "tokens": ["Doch", "bin", "ich", "sehr", "in", "Zwei\u00b7fel", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "APPR", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ob auch des blo\u00dfen Lobes wegen", "tokens": ["Ob", "auch", "des", "blo\u00b7\u00dfen", "Lo\u00b7bes", "we\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "APPR"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das Joch, das angenehme Joch,", "tokens": ["Das", "Joch", ",", "das", "an\u00b7ge\u00b7neh\u00b7me", "Joch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sei ganz und gar hinweg zu legen.", "tokens": ["Sei", "ganz", "und", "gar", "hin\u00b7weg", "zu", "le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "KON", "ADV", "APZR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Dieweil ich k\u00fcrzlich sol hernach", "tokens": ["Die\u00b7weil", "ich", "k\u00fcrz\u00b7lich", "sol", "her\u00b7nach"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "VMFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die lange Nacht vergraben liegen,", "tokens": ["Die", "lan\u00b7ge", "Nacht", "ver\u00b7gra\u00b7ben", "lie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was hilft michs, durch viel Ungemach", "tokens": ["Was", "hilft", "michs", ",", "durch", "viel", "Un\u00b7ge\u00b7mach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PIS", "$,", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und M\u00fch ein' Hand voll Ehre kriegen?", "tokens": ["Und", "M\u00fch", "ein'", "Hand", "voll", "Eh\u00b7re", "krie\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "NN", "ADJD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Kein Vers, wie k\u00fcnstlich er mag sein,", "tokens": ["Kein", "Vers", ",", "wie", "k\u00fcnst\u00b7lich", "er", "mag", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PWAV", "ADJD", "PPER", "VMFIN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der kan mir jetzund B\u00fcrge werden,", "tokens": ["Der", "kan", "mir", "je\u00b7tzund", "B\u00fcr\u00b7ge", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "NN", "VAINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man werde dieses mein Gebein", "tokens": ["Man", "wer\u00b7de", "die\u00b7ses", "mein", "Ge\u00b7bein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "PDAT", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bedecken mit fein leichter Erden.", "tokens": ["Be\u00b7de\u00b7cken", "mit", "fein", "leich\u00b7ter", "Er\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Doch wol, la\u00df meine Poesie", "tokens": ["Doch", "wol", ",", "la\u00df", "mei\u00b7ne", "Poe\u00b7sie"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "VVIMP", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und was ich sonsten m\u00f6chte schreiben,", "tokens": ["Und", "was", "ich", "sons\u00b7ten", "m\u00f6ch\u00b7te", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVINF", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als zu Ergetzung meiner M\u00fch,", "tokens": ["Als", "zu", "Er\u00b7get\u00b7zung", "mei\u00b7ner", "M\u00fch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Ein hundert Jahr' und lenger bleiben:", "tokens": ["Ein", "hun\u00b7dert", "Jahr'", "und", "len\u00b7ger", "blei\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Bin ich mehr als Anacreon,", "tokens": ["Bin", "ich", "mehr", "als", "A\u00b7na\u00b7cre\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "KOKOM", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als Stesichor, und Simonides,", "tokens": ["Als", "Ste\u00b7si\u00b7chor", ",", "und", "Si\u00b7mo\u00b7ni\u00b7des", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NN", "$,", "KON", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als Antimachus, und Bion,", "tokens": ["Als", "An\u00b7ti\u00b7ma\u00b7chus", ",", "und", "Bi\u00b7on", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NE", "$,", "KON", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Als Philet, oder Bacchylides?", "tokens": ["Als", "Phi\u00b7let", ",", "o\u00b7der", "Bac\u00b7chy\u00b7li\u00b7des", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NE", "$,", "KON", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Ist aber dir dann nicht bekant", "tokens": ["Ist", "a\u00b7ber", "dir", "dann", "nicht", "be\u00b7kant"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PPER", "ADV", "PTKNEG", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Griechen sch\u00f6ne Zier im Tichten?", "tokens": ["Der", "Grie\u00b7chen", "sch\u00f6\u00b7ne", "Zier", "im", "Tich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was sol nun diese meine Hand", "tokens": ["Was", "sol", "nun", "die\u00b7se", "mei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ADV", "PDAT", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In deutscher Sprache k\u00f6nnen richten?", "tokens": ["In", "deut\u00b7scher", "Spra\u00b7che", "k\u00f6n\u00b7nen", "rich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Nein, nein, ich lobe meinen Sinn", "tokens": ["Nein", ",", "nein", ",", "ich", "lo\u00b7be", "mei\u00b7nen", "Sinn"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PTKANT", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hoff', es sol mir auch gelingen,", "tokens": ["Und", "hoff'", ",", "es", "sol", "mir", "auch", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PPER", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df das, worauf ich kommen bin,", "tokens": ["Da\u00df", "das", ",", "wo\u00b7rauf", "ich", "kom\u00b7men", "bin", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "$,", "PWAV", "PPER", "VVINF", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Noch m\u00f6glich sei mir, zu vollbringen.", "tokens": ["Noch", "m\u00f6g\u00b7lich", "sei", "mir", ",", "zu", "voll\u00b7brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Das d\u00fcnkt mich gar viel besser sein,", "tokens": ["Das", "d\u00fcnkt", "mich", "gar", "viel", "bes\u00b7ser", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als derer Flei\u00df, die nichts erwerben", "tokens": ["Als", "de\u00b7rer", "Flei\u00df", ",", "die", "nichts", "er\u00b7wer\u00b7ben"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PDS", "NN", "$,", "PRELS", "PIS", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Durch ihre Reim' als leichten Schein,", "tokens": ["Durch", "ih\u00b7re", "Reim'", "als", "leich\u00b7ten", "Schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KOUS", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und doch f\u00fcr Hunger kaum nicht sterben.", "tokens": ["Und", "doch", "f\u00fcr", "Hun\u00b7ger", "kaum", "nicht", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Mein N\u00fc\u00dfler, und ist di\u00df dein Rath,", "tokens": ["Mein", "N\u00fc\u00df\u00b7ler", ",", "und", "ist", "di\u00df", "dein", "Rath", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "KON", "VAFIN", "PDS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich sol die schn\u00f6de Wollust hassen", "tokens": ["Ich", "sol", "die", "schn\u00f6\u00b7de", "Wol\u00b7lust", "has\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und die, so mich beth\u00f6ret hat,", "tokens": ["Und", "die", ",", "so", "mich", "be\u00b7th\u00f6\u00b7ret", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "$,", "ADV", "PPER", "VVFIN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die sch\u00f6ne Flavia, verlassen?", "tokens": ["Die", "sch\u00f6\u00b7ne", "Fla\u00b7via", ",", "ver\u00b7las\u00b7sen", "?"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Sprich, sagst du, deine Musen an,", "tokens": ["Sprich", ",", "sagst", "du", ",", "dei\u00b7ne", "Mu\u00b7sen", "an", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "VVFIN", "PPER", "$,", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Setz' an die Feder, das zu schreiben,", "tokens": ["Setz'", "an", "die", "Fe\u00b7der", ",", "das", "zu", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$,", "PRELS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Durch welches dein Ger\u00fcchte kan", "tokens": ["Durch", "wel\u00b7ches", "dein", "Ge\u00b7r\u00fcch\u00b7te", "kan"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPOSAT", "NN", "VMFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Ewigkeit hernach bekleiben.", "tokens": ["In", "E\u00b7wig\u00b7keit", "her\u00b7nach", "be\u00b7klei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "La\u00df fahren, die zu wenig ist,", "tokens": ["La\u00df", "fah\u00b7ren", ",", "die", "zu", "we\u00b7nig", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "VVINF", "$,", "PRELS", "APPR", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie die viel gew\u00fcnschten Sachen,", "tokens": ["Da\u00df", "sie", "die", "viel", "ge\u00b7w\u00fcnschten", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die du zu tichten auserkiest,", "tokens": ["Die", "du", "zu", "tich\u00b7ten", "aus\u00b7er\u00b7kiest", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sol ganz und gar zu Wasser machen.", "tokens": ["Sol", "ganz", "und", "gar", "zu", "Was\u00b7ser", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "KON", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Denk' an den Ruhm, den du nunmehr", "tokens": ["Denk'", "an", "den", "Ruhm", ",", "den", "du", "nun\u00b7mehr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVIMP", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bei gro\u00dfen Leuten hast erworben;", "tokens": ["Bei", "gro\u00b7\u00dfen", "Leu\u00b7ten", "hast", "er\u00b7wor\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Seit da\u00df du liebst, ist schier die Ehr'", "tokens": ["Seit", "da\u00df", "du", "liebst", ",", "ist", "schier", "die", "Ehr'"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In ihrer ersten Bl\u00fct' erstorben.", "tokens": ["In", "ih\u00b7rer", "ers\u00b7ten", "Bl\u00fct'", "ers\u00b7tor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "War ist es, ich bin jetzund fast", "tokens": ["War", "ist", "es", ",", "ich", "bin", "je\u00b7tzund", "fast"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "VAFIN", "PPER", "$,", "PPER", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der B\u00fccher \u00e4rgster Todfeind worden,", "tokens": ["Der", "B\u00fc\u00b7cher", "\u00e4rgs\u00b7ter", "Tod\u00b7feind", "wor\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nun Venus, die gew\u00fcnschte Last,", "tokens": ["Nun", "Ve\u00b7nus", ",", "die", "ge\u00b7w\u00fcnschte", "Last", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Mich wieder hat in ihrem Orden.", "tokens": ["Mich", "wie\u00b7der", "hat", "in", "ih\u00b7rem", "Or\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Doch bin ich sehr in Zweifel noch,", "tokens": ["Doch", "bin", "ich", "sehr", "in", "Zwei\u00b7fel", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "APPR", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ob auch des blo\u00dfen Lobes wegen", "tokens": ["Ob", "auch", "des", "blo\u00b7\u00dfen", "Lo\u00b7bes", "we\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "APPR"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das Joch, das angenehme Joch,", "tokens": ["Das", "Joch", ",", "das", "an\u00b7ge\u00b7neh\u00b7me", "Joch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sei ganz und gar hinweg zu legen.", "tokens": ["Sei", "ganz", "und", "gar", "hin\u00b7weg", "zu", "le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "KON", "ADV", "APZR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Dieweil ich k\u00fcrzlich sol hernach", "tokens": ["Die\u00b7weil", "ich", "k\u00fcrz\u00b7lich", "sol", "her\u00b7nach"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "VMFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die lange Nacht vergraben liegen,", "tokens": ["Die", "lan\u00b7ge", "Nacht", "ver\u00b7gra\u00b7ben", "lie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was hilft michs, durch viel Ungemach", "tokens": ["Was", "hilft", "michs", ",", "durch", "viel", "Un\u00b7ge\u00b7mach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PIS", "$,", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und M\u00fch ein' Hand voll Ehre kriegen?", "tokens": ["Und", "M\u00fch", "ein'", "Hand", "voll", "Eh\u00b7re", "krie\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "NN", "ADJD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Kein Vers, wie k\u00fcnstlich er mag sein,", "tokens": ["Kein", "Vers", ",", "wie", "k\u00fcnst\u00b7lich", "er", "mag", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PWAV", "ADJD", "PPER", "VMFIN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der kan mir jetzund B\u00fcrge werden,", "tokens": ["Der", "kan", "mir", "je\u00b7tzund", "B\u00fcr\u00b7ge", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "NN", "VAINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man werde dieses mein Gebein", "tokens": ["Man", "wer\u00b7de", "die\u00b7ses", "mein", "Ge\u00b7bein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "PDAT", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bedecken mit fein leichter Erden.", "tokens": ["Be\u00b7de\u00b7cken", "mit", "fein", "leich\u00b7ter", "Er\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Doch wol, la\u00df meine Poesie", "tokens": ["Doch", "wol", ",", "la\u00df", "mei\u00b7ne", "Poe\u00b7sie"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "VVIMP", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und was ich sonsten m\u00f6chte schreiben,", "tokens": ["Und", "was", "ich", "sons\u00b7ten", "m\u00f6ch\u00b7te", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVINF", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als zu Ergetzung meiner M\u00fch,", "tokens": ["Als", "zu", "Er\u00b7get\u00b7zung", "mei\u00b7ner", "M\u00fch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Ein hundert Jahr' und lenger bleiben:", "tokens": ["Ein", "hun\u00b7dert", "Jahr'", "und", "len\u00b7ger", "blei\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Bin ich mehr als Anacreon,", "tokens": ["Bin", "ich", "mehr", "als", "A\u00b7na\u00b7cre\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "KOKOM", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als Stesichor, und Simonides,", "tokens": ["Als", "Ste\u00b7si\u00b7chor", ",", "und", "Si\u00b7mo\u00b7ni\u00b7des", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NN", "$,", "KON", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als Antimachus, und Bion,", "tokens": ["Als", "An\u00b7ti\u00b7ma\u00b7chus", ",", "und", "Bi\u00b7on", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NE", "$,", "KON", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Als Philet, oder Bacchylides?", "tokens": ["Als", "Phi\u00b7let", ",", "o\u00b7der", "Bac\u00b7chy\u00b7li\u00b7des", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NE", "$,", "KON", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.24": {"line.1": {"text": "Ist aber dir dann nicht bekant", "tokens": ["Ist", "a\u00b7ber", "dir", "dann", "nicht", "be\u00b7kant"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PPER", "ADV", "PTKNEG", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Griechen sch\u00f6ne Zier im Tichten?", "tokens": ["Der", "Grie\u00b7chen", "sch\u00f6\u00b7ne", "Zier", "im", "Tich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was sol nun diese meine Hand", "tokens": ["Was", "sol", "nun", "die\u00b7se", "mei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ADV", "PDAT", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In deutscher Sprache k\u00f6nnen richten?", "tokens": ["In", "deut\u00b7scher", "Spra\u00b7che", "k\u00f6n\u00b7nen", "rich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Nein, nein, ich lobe meinen Sinn", "tokens": ["Nein", ",", "nein", ",", "ich", "lo\u00b7be", "mei\u00b7nen", "Sinn"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PTKANT", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hoff', es sol mir auch gelingen,", "tokens": ["Und", "hoff'", ",", "es", "sol", "mir", "auch", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PPER", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df das, worauf ich kommen bin,", "tokens": ["Da\u00df", "das", ",", "wo\u00b7rauf", "ich", "kom\u00b7men", "bin", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "$,", "PWAV", "PPER", "VVINF", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Noch m\u00f6glich sei mir, zu vollbringen.", "tokens": ["Noch", "m\u00f6g\u00b7lich", "sei", "mir", ",", "zu", "voll\u00b7brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Das d\u00fcnkt mich gar viel besser sein,", "tokens": ["Das", "d\u00fcnkt", "mich", "gar", "viel", "bes\u00b7ser", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als derer Flei\u00df, die nichts erwerben", "tokens": ["Als", "de\u00b7rer", "Flei\u00df", ",", "die", "nichts", "er\u00b7wer\u00b7ben"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PDS", "NN", "$,", "PRELS", "PIS", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Durch ihre Reim' als leichten Schein,", "tokens": ["Durch", "ih\u00b7re", "Reim'", "als", "leich\u00b7ten", "Schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KOUS", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und doch f\u00fcr Hunger kaum nicht sterben.", "tokens": ["Und", "doch", "f\u00fcr", "Hun\u00b7ger", "kaum", "nicht", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}