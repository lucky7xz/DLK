{"textgrid.poem.53609": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "1L: F\u00e4hrt in Berlin die Stra\u00dfenbahn,", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "F\u00e4hrt in Berlin die Stra\u00dfenbahn,", "tokens": ["F\u00e4hrt", "in", "Ber\u00b7lin", "die", "Stra\u00b7\u00dfen\u00b7bahn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "dann ist sie proppenvoll.", "tokens": ["dann", "ist", "sie", "prop\u00b7pen\u00b7voll", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was da mit dir, o Mensch, getan", "tokens": ["Was", "da", "mit", "dir", ",", "o", "Mensch", ",", "ge\u00b7tan"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PWS", "ADV", "APPR", "PPER", "$,", "FM", "NN", "$,", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wird, das ist einfach doll.", "tokens": ["wird", ",", "das", "ist", "ein\u00b7fach", "doll", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Man r\u00fcttelt dich,", "tokens": ["Man", "r\u00fct\u00b7telt", "dich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "man sch\u00fcttelt dich;", "tokens": ["man", "sch\u00fct\u00b7telt", "dich", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "man dr\u00e4ngt dich so", "tokens": ["man", "dr\u00e4ngt", "dich", "so"], "token_info": ["word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "und engt dich so \u2013", "tokens": ["und", "engt", "dich", "so", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "In einen Wagen gehen glatt", "tokens": ["In", "ei\u00b7nen", "Wa\u00b7gen", "ge\u00b7hen", "glatt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVINF", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "zweihundert Menschen rein . . .", "tokens": ["zwei\u00b7hun\u00b7dert", "Men\u00b7schen", "rein", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["CARD", "NN", "ADJD", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "New York sitzt an der Panke Strand.", "tokens": ["New", "Y\u00b7ork", "sitzt", "an", "der", "Pan\u00b7ke", "Strand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Es kauft uns arm der Gent.", "tokens": ["Es", "kauft", "uns", "arm", "der", "Gent", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er kriegt den ganzen Hektar Land", "tokens": ["Er", "kriegt", "den", "gan\u00b7zen", "Hek\u00b7tar", "Land"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "f\u00fcr sechseinhalben Cent.", "tokens": ["f\u00fcr", "sec\u00b7hsein\u00b7hal\u00b7ben", "Cent", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die B\u00f6rse winkt.", "tokens": ["Die", "B\u00f6r\u00b7se", "winkt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Valuta sinkt.", "tokens": ["Va\u00b7lu\u00b7ta", "sinkt", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "VVFIN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.7": {"text": "Wirf weg die Mark!", "tokens": ["Wirf", "weg", "die", "Mark", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "Was soll der Quark!", "tokens": ["Was", "soll", "der", "Quark", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.9": {"text": "Am Abend steht im Tageblatt:", "tokens": ["Am", "A\u00b7bend", "steht", "im", "Ta\u00b7ge\u00b7blatt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "\u00bbder Geldwert ist so klein . . . \u00ab", "tokens": ["\u00bb", "der", "Geld\u00b7wert", "ist", "so", "klein", ".", ".", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "ADJD", "$.", "$.", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Am Tische sitzt Herr Helfferich.", "tokens": ["Am", "Ti\u00b7sche", "sitzt", "Herr", "Helf\u00b7fe\u00b7rich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(abstehen seine Ohren.)", "tokens": ["(", "ab\u00b7ste\u00b7hen", "sei\u00b7ne", "Oh\u00b7ren", ".", ")"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Er schimpft und schilt gar f\u00fcrchterlich,", "tokens": ["Er", "schimpft", "und", "schilt", "gar", "f\u00fcrch\u00b7ter\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "weil wir den Krieg verloren.", "tokens": ["weil", "wir", "den", "Krieg", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Wie er enth\u00fcllt", "tokens": ["Wie", "er", "ent\u00b7h\u00fcllt"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PPER", "VVFIN"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "und schneidig br\u00fcllt!", "tokens": ["und", "schnei\u00b7dig", "br\u00fcllt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Pfui Politik", "tokens": ["Pfui", "Po\u00b7li\u00b7tik"], "token_info": ["word", "word"], "pos": ["NE", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "der Republik!", "tokens": ["der", "Re\u00b7pub\u00b7lik", "!"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "\u00bbdie neuen Herrn sind mau und matt!\u00ab", "tokens": ["\u00bb", "die", "neu\u00b7en", "Herrn", "sind", "mau", "und", "matt", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "So h\u00f6rst du laut ihn schrein . . .", "tokens": ["So", "h\u00f6rst", "du", "laut", "ihn", "schrein", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "November Achtzehn schlug die Uhr", "tokens": ["No\u00b7vem\u00b7ber", "Acht\u00b7zehn", "schlug", "die", "Uhr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zw\u00f6lfmal in deutschen Landen.", "tokens": ["zw\u00f6lf\u00b7mal", "in", "deut\u00b7schen", "Lan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJA", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Die Nationalen h\u00f6rtens nur,", "tokens": ["Die", "Na\u00b7ti\u00b7o\u00b7na\u00b7len", "h\u00f6r\u00b7tens", "nur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "als sie im Nu verschwanden.", "tokens": ["als", "sie", "im", "Nu", "ver\u00b7schwan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Nun sind sie ja", "tokens": ["Nun", "sind", "sie", "ja"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "all wieder da.", "tokens": ["all", "wie\u00b7der", "da", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Es kam so weit", "tokens": ["Es", "kam", "so", "weit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "im Lauf der Zeit.", "tokens": ["im", "Lauf", "der", "Zeit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "In jedem Dorf, in jeder Stadt,", "tokens": ["In", "je\u00b7dem", "Dorf", ",", "in", "je\u00b7der", "Stadt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "da p\u00f6beln sie allein . . .", "tokens": ["da", "p\u00f6\u00b7beln", "sie", "al\u00b7lein", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer sie fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "sie", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "F\u00e4hrt in Berlin die Stra\u00dfenbahn,", "tokens": ["F\u00e4hrt", "in", "Ber\u00b7lin", "die", "Stra\u00b7\u00dfen\u00b7bahn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "dann ist sie proppenvoll.", "tokens": ["dann", "ist", "sie", "prop\u00b7pen\u00b7voll", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was da mit dir, o Mensch, getan", "tokens": ["Was", "da", "mit", "dir", ",", "o", "Mensch", ",", "ge\u00b7tan"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PWS", "ADV", "APPR", "PPER", "$,", "FM", "NN", "$,", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wird, das ist einfach doll.", "tokens": ["wird", ",", "das", "ist", "ein\u00b7fach", "doll", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.5": {"text": "Man r\u00fcttelt dich,", "tokens": ["Man", "r\u00fct\u00b7telt", "dich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "man sch\u00fcttelt dich;", "tokens": ["man", "sch\u00fct\u00b7telt", "dich", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "man dr\u00e4ngt dich so", "tokens": ["man", "dr\u00e4ngt", "dich", "so"], "token_info": ["word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "und engt dich so \u2013", "tokens": ["und", "engt", "dich", "so", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "In einen Wagen gehen glatt", "tokens": ["In", "ei\u00b7nen", "Wa\u00b7gen", "ge\u00b7hen", "glatt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVINF", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "zweihundert Menschen rein . . .", "tokens": ["zwei\u00b7hun\u00b7dert", "Men\u00b7schen", "rein", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["CARD", "NN", "ADJD", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "New York sitzt an der Panke Strand.", "tokens": ["New", "Y\u00b7ork", "sitzt", "an", "der", "Pan\u00b7ke", "Strand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Es kauft uns arm der Gent.", "tokens": ["Es", "kauft", "uns", "arm", "der", "Gent", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er kriegt den ganzen Hektar Land", "tokens": ["Er", "kriegt", "den", "gan\u00b7zen", "Hek\u00b7tar", "Land"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "f\u00fcr sechseinhalben Cent.", "tokens": ["f\u00fcr", "sec\u00b7hsein\u00b7hal\u00b7ben", "Cent", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die B\u00f6rse winkt.", "tokens": ["Die", "B\u00f6r\u00b7se", "winkt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Valuta sinkt.", "tokens": ["Va\u00b7lu\u00b7ta", "sinkt", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "VVFIN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.7": {"text": "Wirf weg die Mark!", "tokens": ["Wirf", "weg", "die", "Mark", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "Was soll der Quark!", "tokens": ["Was", "soll", "der", "Quark", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.9": {"text": "Am Abend steht im Tageblatt:", "tokens": ["Am", "A\u00b7bend", "steht", "im", "Ta\u00b7ge\u00b7blatt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "\u00bbder Geldwert ist so klein . . . \u00ab", "tokens": ["\u00bb", "der", "Geld\u00b7wert", "ist", "so", "klein", ".", ".", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "ADJD", "$.", "$.", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Am Tische sitzt Herr Helfferich.", "tokens": ["Am", "Ti\u00b7sche", "sitzt", "Herr", "Helf\u00b7fe\u00b7rich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(abstehen seine Ohren.)", "tokens": ["(", "ab\u00b7ste\u00b7hen", "sei\u00b7ne", "Oh\u00b7ren", ".", ")"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Er schimpft und schilt gar f\u00fcrchterlich,", "tokens": ["Er", "schimpft", "und", "schilt", "gar", "f\u00fcrch\u00b7ter\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "weil wir den Krieg verloren.", "tokens": ["weil", "wir", "den", "Krieg", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Wie er enth\u00fcllt", "tokens": ["Wie", "er", "ent\u00b7h\u00fcllt"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PPER", "VVFIN"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "und schneidig br\u00fcllt!", "tokens": ["und", "schnei\u00b7dig", "br\u00fcllt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Pfui Politik", "tokens": ["Pfui", "Po\u00b7li\u00b7tik"], "token_info": ["word", "word"], "pos": ["NE", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "der Republik!", "tokens": ["der", "Re\u00b7pub\u00b7lik", "!"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "\u00bbdie neuen Herrn sind mau und matt!\u00ab", "tokens": ["\u00bb", "die", "neu\u00b7en", "Herrn", "sind", "mau", "und", "matt", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "So h\u00f6rst du laut ihn schrein . . .", "tokens": ["So", "h\u00f6rst", "du", "laut", "ihn", "schrein", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer ihn fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "ihn", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "November Achtzehn schlug die Uhr", "tokens": ["No\u00b7vem\u00b7ber", "Acht\u00b7zehn", "schlug", "die", "Uhr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zw\u00f6lfmal in deutschen Landen.", "tokens": ["zw\u00f6lf\u00b7mal", "in", "deut\u00b7schen", "Lan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJA", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Die Nationalen h\u00f6rtens nur,", "tokens": ["Die", "Na\u00b7ti\u00b7o\u00b7na\u00b7len", "h\u00f6r\u00b7tens", "nur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "als sie im Nu verschwanden.", "tokens": ["als", "sie", "im", "Nu", "ver\u00b7schwan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Nun sind sie ja", "tokens": ["Nun", "sind", "sie", "ja"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "all wieder da.", "tokens": ["all", "wie\u00b7der", "da", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Es kam so weit", "tokens": ["Es", "kam", "so", "weit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "im Lauf der Zeit.", "tokens": ["im", "Lauf", "der", "Zeit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.9": {"text": "In jedem Dorf, in jeder Stadt,", "tokens": ["In", "je\u00b7dem", "Dorf", ",", "in", "je\u00b7der", "Stadt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "da p\u00f6beln sie allein . . .", "tokens": ["da", "p\u00f6\u00b7beln", "sie", "al\u00b7lein", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$.", "$.", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Wer sie fr\u00fcher nicht gesehen hat,", "tokens": ["Wer", "sie", "fr\u00fc\u00b7her", "nicht", "ge\u00b7se\u00b7hen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.12": {"text": "der denkt, das mu\u00df so sein \u2013!", "tokens": ["der", "denkt", ",", "das", "mu\u00df", "so", "sein", "\u2013", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "VVFIN", "$,", "PDS", "VMFIN", "ADV", "VAINF", "$(", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}