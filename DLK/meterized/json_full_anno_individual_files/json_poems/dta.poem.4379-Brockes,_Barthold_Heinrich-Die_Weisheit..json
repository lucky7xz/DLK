{"dta.poem.4379": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Die Weisheit.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Oft hab ich bey mir \u00fcberleget, was eigentlich die Weis-", "tokens": ["Oft", "hab", "ich", "bey", "mir", "\u00fc\u00b7ber\u00b7le\u00b7get", ",", "was", "ei\u00b7gent\u00b7lich", "die", "Weis"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "PPER", "VVFIN", "$,", "PRELS", "ADV", "ART", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "heit sey.", "tokens": ["heit", "sey", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VAFIN", "$."], "meter": "--", "measure": "unknown.measure.zero"}, "line.3": {"text": "Nachdem man von derselben Wesen so vieles und so man-", "tokens": ["Nach\u00b7dem", "man", "von", "der\u00b7sel\u00b7ben", "We\u00b7sen", "so", "vie\u00b7les", "und", "so", "man"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "PDAT", "NN", "ADV", "PIS", "KON", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "cherley", "tokens": ["cher\u00b7ley"], "token_info": ["word"], "pos": ["NE"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "In mir nicht deutlichem Verstande gelehrt, geschrieben,", "tokens": ["In", "mir", "nicht", "deut\u00b7li\u00b7chem", "Ver\u00b7stan\u00b7de", "ge\u00b7lehrt", ",", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PPER", "PTKNEG", "ADJA", "NN", "VVPP", "$,", "VVPP", "$,"], "meter": "+--+-+-+--+-+-", "measure": "iambic.hexa.invert"}, "line.6": {"text": "vorgetragen;", "tokens": ["vor\u00b7ge\u00b7tra\u00b7gen", ";"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "So deucht mich, da\u00df man die\u00df von ihr mit allem Rechte", "tokens": ["So", "deucht", "mich", ",", "da\u00df", "man", "die\u00df", "von", "ihr", "mit", "al\u00b7lem", "Rech\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PIS", "PDS", "APPR", "PPER", "APPR", "PIS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "k\u00f6nne sagen:", "tokens": ["k\u00f6n\u00b7ne", "sa\u00b7gen", ":"], "token_info": ["word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "Sie sey ein Wollen und Verm\u00f6gen, so Thun als", "tokens": ["Sie", "sey", "ein", "Wol\u00b7len", "und", "Ver\u00b7m\u00f6\u00b7gen", ",", "so", "Thun", "als"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "KON", "NN", "$,", "ADV", "VVFIN", "KOKOM"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Denken nach den Pflichten", "tokens": ["Den\u00b7ken", "nach", "den", "Pflich\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.11": {"text": "Der g\u00f6ttlich- und nat\u00fcrlichen Gesetze kl\u00fcglich einzu-", "tokens": ["Der", "g\u00f6tt\u00b7lich", "und", "na\u00b7t\u00fcr\u00b7li\u00b7chen", "Ge\u00b7set\u00b7ze", "kl\u00fcg\u00b7lich", "ein\u00b7zu"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "TRUNC", "KON", "ADJA", "NN", "ADJD", "TRUNC"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.12": {"text": "richten.", "tokens": ["rich\u00b7ten", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.13": {"text": "Wann wir nun finden, da\u00df wir alle (so wie wir von uns", "tokens": ["Wann", "wir", "nun", "fin\u00b7den", ",", "da\u00df", "wir", "al\u00b7le", "(", "so", "wie", "wir", "von", "uns"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "VVINF", "$,", "KOUS", "PPER", "PIAT", "$(", "ADV", "KOKOM", "PPER", "APPR", "PPER"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "selbst nicht seyn)", "tokens": ["selbst", "nicht", "seyn", ")"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "VAINF", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.15": {"text": "Auch uns nicht selbst erhalten k\u00f6nnen;", "tokens": ["Auch", "uns", "nicht", "selbst", "er\u00b7hal\u00b7ten", "k\u00f6n\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PTKNEG", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Ja da\u00df, wenn wir uns recht betrachten, ein jeder ganz f\u00fcr", "tokens": ["Ja", "da\u00df", ",", "wenn", "wir", "uns", "recht", "be\u00b7trach\u00b7ten", ",", "ein", "je\u00b7der", "ganz", "f\u00fcr"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "KOUS", "$,", "KOUS", "PPER", "PRF", "ADJD", "VVINF", "$,", "ART", "PIAT", "ADV", "APPR"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.17": {"text": "sich allein,", "tokens": ["sich", "al\u00b7lein", ","], "token_info": ["word", "word", "punct"], "pos": ["PRF", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.18": {"text": "Von allen andern abgesondert, mit Recht ein\u2019 Insel sey zu", "tokens": ["Von", "al\u00b7len", "an\u00b7dern", "ab\u00b7ge\u00b7son\u00b7dert", ",", "mit", "Recht", "ein'", "In\u00b7sel", "sey", "zu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "PIS", "VVPP", "$,", "APPR", "NN", "ART", "NN", "VAFIN", "APPR"], "meter": "-+-+-+-+--+--+-+", "measure": "iambic.septa.relaxed"}, "line.19": {"text": "nennen,", "tokens": ["nen\u00b7nen", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "--", "measure": "unknown.measure.zero"}, "line.20": {"text": "Ja nicht einmahl, weil sonder Grund, den sonst ein\u2019 Insel", "tokens": ["Ja", "nicht", "ein\u00b7mahl", ",", "weil", "son\u00b7der", "Grund", ",", "den", "sonst", "ein'", "In\u00b7sel"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "PTKNEG", "ADV", "$,", "KOUS", "ADJA", "NN", "$,", "PRELS", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "hat, wir leben,", "tokens": ["hat", ",", "wir", "le\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PPER", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.22": {"text": "Und gar nicht an der Erde fest, nur Inseln sind, die gleich-", "tokens": ["Und", "gar", "nicht", "an", "der", "Er\u00b7de", "fest", ",", "nur", "In\u00b7seln", "sind", ",", "die", "gleich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ADV", "PTKNEG", "APPR", "ART", "NN", "PTKVZ", "$,", "ADV", "NN", "VAFIN", "$,", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.23": {"text": "sam schweben.", "tokens": ["sam", "schwe\u00b7ben", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.24": {"text": "Da GOtt hingegen allenthalben, und wir uns nicht zu", "tokens": ["Da", "Gott", "hin\u00b7ge\u00b7gen", "al\u00b7len\u00b7thal\u00b7ben", ",", "und", "wir", "uns", "nicht", "zu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "ADV", "ADV", "$,", "KON", "PPER", "PRF", "PTKNEG", "PTKZU"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.25": {"text": "helfen wissen,", "tokens": ["hel\u00b7fen", "wis\u00b7sen", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.26": {"text": "Vielmehr von Jhm unstreitig alles, so hier als dorten, haben", "tokens": ["Viel\u00b7mehr", "von", "Jhm", "un\u00b7strei\u00b7tig", "al\u00b7les", ",", "so", "hier", "als", "dor\u00b7ten", ",", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "APPR", "PPER", "ADJD", "PIS", "$,", "ADV", "ADV", "KOKOM", "ADV", "$,", "VAFIN"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.27": {"text": "m\u00fcssen,", "tokens": ["m\u00fcs\u00b7sen", ","], "token_info": ["word", "punct"], "pos": ["VMFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.28": {"text": "Sowohl im Leben, als im Tode; so bleibet es gewi\u00df dabey,", "tokens": ["So\u00b7wohl", "im", "Le\u00b7ben", ",", "als", "im", "To\u00b7de", ";", "so", "blei\u00b7bet", "es", "ge\u00b7wi\u00df", "da\u00b7bey", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "$,", "KOUS", "APPRART", "NN", "$.", "ADV", "VVFIN", "PPER", "ADV", "PAV", "$,"], "meter": "-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.29": {"text": "Da\u00df in der That die Furcht des HErrn der wahren", "tokens": ["Da\u00df", "in", "der", "That", "die", "Furcht", "des", "Herrn", "der", "wah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "ART", "NN", "ART", "NN", "ART", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.30": {"text": "Weisheit Anfang sey.", "tokens": ["Weis\u00b7heit", "An\u00b7fang", "sey", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}