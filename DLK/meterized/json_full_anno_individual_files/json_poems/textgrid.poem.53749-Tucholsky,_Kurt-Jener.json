{"textgrid.poem.53749": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Jener", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbwas haben Sie eigentlich gegen ihn?", "tokens": ["\u00bb", "was", "ha\u00b7ben", "Sie", "ei\u00b7gent\u00b7lich", "ge\u00b7gen", "ihn", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Er ist diskret und stets bescheiden.", "tokens": ["Er", "ist", "dis\u00b7kret", "und", "stets", "be\u00b7schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er hat doch alle Sympathien \u2013", "tokens": ["Er", "hat", "doch", "al\u00b7le", "Sym\u00b7pa\u00b7thi\u00b7en", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "was wolln Sie uns den Mann verleiden?\u00ab", "tokens": ["was", "wolln", "Sie", "uns", "den", "Mann", "ver\u00b7lei\u00b7den", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PRF", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ja, gegen Wilhelm ist er Gold.", "tokens": ["Ja", ",", "ge\u00b7gen", "Wil\u00b7helm", "ist", "er", "Gold", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "NE", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das will nun aber nichts besagen.", "tokens": ["Das", "will", "nun", "a\u00b7ber", "nichts", "be\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df jedermann ihm Achtung zollt,", "tokens": ["Da\u00df", "je\u00b7der\u00b7mann", "ihm", "Ach\u00b7tung", "zollt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "bedeutet: er ist leicht zu tragen.", "tokens": ["be\u00b7deu\u00b7tet", ":", "er", "ist", "leicht", "zu", "tra\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PPER", "VAFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und so bequem. Ist das ein Mann", "tokens": ["Und", "so", "be\u00b7quem", ".", "Ist", "das", "ein", "Mann"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJD", "$.", "VAFIN", "PDS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der Republik? Ein Mann der Massen,", "tokens": ["der", "Re\u00b7pub\u00b7lik", "?", "Ein", "Mann", "der", "Mas\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "daraus er stammt? Sehn Sie sichs an:", "tokens": ["da\u00b7raus", "er", "stammt", "?", "Sehn", "Sie", "sichs", "an", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "VVFIN", "$.", "VVFIN", "PPER", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er kann von aller Herkunft lassen.", "tokens": ["Er", "kann", "von", "al\u00b7ler", "Her\u00b7kunft", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich wei\u00df: man kann nicht immer so.", "tokens": ["Ich", "wei\u00df", ":", "man", "kann", "nicht", "im\u00b7mer", "so", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PIS", "VMFIN", "PTKNEG", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich wei\u00df: er soll repr\u00e4sentieren.", "tokens": ["Ich", "wei\u00df", ":", "er", "soll", "re\u00b7pr\u00e4\u00b7sen\u00b7tie\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich wei\u00df: abh\u00e4ngig vom B\u00fcro . . .", "tokens": ["Ich", "wei\u00df", ":", "ab\u00b7h\u00e4n\u00b7gig", "vom", "B\u00fc\u00b7ro", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "ADJD", "APPRART", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die Position . . . er mu\u00df paktieren . . .", "tokens": ["die", "Po\u00b7si\u00b7ti\u00b7on", ".", ".", ".", "er", "mu\u00df", "pak\u00b7tie\u00b7ren", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "$.", "$.", "$.", "PPER", "VMFIN", "VVINF", "$.", "$.", "$."], "meter": "--+-+-+-+-", "measure": "anapaest.init"}}, "stanza.5": {"line.1": {"text": "Der Arbeiter sah hoffnungsvoll", "tokens": ["Der", "Ar\u00b7bei\u00b7ter", "sah", "hoff\u00b7nungs\u00b7voll"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD"], "meter": "-++-++-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "auf seinen Mann. Dem wollt er dienen.", "tokens": ["auf", "sei\u00b7nen", "Mann", ".", "Dem", "wollt", "er", "die\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$.", "PDS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "In langen Jahren wuchs der Groll:", "tokens": ["In", "lan\u00b7gen", "Jah\u00b7ren", "wuchs", "der", "Groll", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbeiner von uns? Einer von ihnen!\u00ab", "tokens": ["\u00bb", "ei\u00b7ner", "von", "uns", "?", "Ei\u00b7ner", "von", "ih\u00b7nen", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "APPR", "PPER", "$.", "PIS", "APPR", "PPER", "$.", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Vergessen, was man lebenslang", "tokens": ["Ver\u00b7ges\u00b7sen", ",", "was", "man", "le\u00b7bens\u00b7lang"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVPP", "$,", "PRELS", "PIS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "f\u00fcr die Genossen sch\u00f6n gepredigt?", "tokens": ["f\u00fcr", "die", "Ge\u00b7nos\u00b7sen", "sch\u00f6n", "ge\u00b7pre\u00b7digt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Gang die Reichswehrfront entlang \u2013", "tokens": ["Ein", "Gang", "die", "Reichs\u00b7wehr\u00b7front", "ent\u00b7lang", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "APPO", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und Marx und Bebel sind erledigt.", "tokens": ["und", "Marx", "und", "Be\u00b7bel", "sind", "er\u00b7le\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "KON", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Sechs Jahr kein Wort, das uns bewegt.", "tokens": ["Sechs", "Jahr", "kein", "Wort", ",", "das", "uns", "be\u00b7wegt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "PIAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Wort f\u00fcr die in den Fabriken.", "tokens": ["Kein", "Wort", "f\u00fcr", "die", "in", "den", "Fab\u00b7ri\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "ART", "APPR", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Kein Wort, das unsre Zeit erregt \u2013", "tokens": ["Kein", "Wort", ",", "das", "uns\u00b7re", "Zeit", "er\u00b7regt", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "nur Gehrock, Messen und Musiken.", "tokens": ["nur", "Ge\u00b7hrock", ",", "Mes\u00b7sen", "und", "Mu\u00b7si\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ein wahres Herz verliert sich nie.", "tokens": ["Ein", "wah\u00b7res", "Herz", "ver\u00b7liert", "sich", "nie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PRF", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der ist den breiten Weg gegangen.", "tokens": ["Der", "ist", "den", "brei\u00b7ten", "Weg", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie die Partei. Er ist wie sie.", "tokens": ["Wie", "die", "Par\u00b7tei", ".", "Er", "ist", "wie", "sie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "$.", "PPER", "VAFIN", "KOKOM", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man darf wohl nicht zu viel verlangen.", "tokens": ["Man", "darf", "wohl", "nicht", "zu", "viel", "ver\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "PTKNEG", "PTKA", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "\u00bbwas haben Sie eigentlich gegen ihn?", "tokens": ["\u00bb", "was", "ha\u00b7ben", "Sie", "ei\u00b7gent\u00b7lich", "ge\u00b7gen", "ihn", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Er ist diskret und stets bescheiden.", "tokens": ["Er", "ist", "dis\u00b7kret", "und", "stets", "be\u00b7schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er hat doch alle Sympathien \u2013", "tokens": ["Er", "hat", "doch", "al\u00b7le", "Sym\u00b7pa\u00b7thi\u00b7en", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "was wolln Sie uns den Mann verleiden?\u00ab", "tokens": ["was", "wolln", "Sie", "uns", "den", "Mann", "ver\u00b7lei\u00b7den", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PRF", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Ja, gegen Wilhelm ist er Gold.", "tokens": ["Ja", ",", "ge\u00b7gen", "Wil\u00b7helm", "ist", "er", "Gold", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "APPR", "NE", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das will nun aber nichts besagen.", "tokens": ["Das", "will", "nun", "a\u00b7ber", "nichts", "be\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df jedermann ihm Achtung zollt,", "tokens": ["Da\u00df", "je\u00b7der\u00b7mann", "ihm", "Ach\u00b7tung", "zollt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "bedeutet: er ist leicht zu tragen.", "tokens": ["be\u00b7deu\u00b7tet", ":", "er", "ist", "leicht", "zu", "tra\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PPER", "VAFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Und so bequem. Ist das ein Mann", "tokens": ["Und", "so", "be\u00b7quem", ".", "Ist", "das", "ein", "Mann"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJD", "$.", "VAFIN", "PDS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der Republik? Ein Mann der Massen,", "tokens": ["der", "Re\u00b7pub\u00b7lik", "?", "Ein", "Mann", "der", "Mas\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "daraus er stammt? Sehn Sie sichs an:", "tokens": ["da\u00b7raus", "er", "stammt", "?", "Sehn", "Sie", "sichs", "an", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "VVFIN", "$.", "VVFIN", "PPER", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er kann von aller Herkunft lassen.", "tokens": ["Er", "kann", "von", "al\u00b7ler", "Her\u00b7kunft", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Ich wei\u00df: man kann nicht immer so.", "tokens": ["Ich", "wei\u00df", ":", "man", "kann", "nicht", "im\u00b7mer", "so", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PIS", "VMFIN", "PTKNEG", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich wei\u00df: er soll repr\u00e4sentieren.", "tokens": ["Ich", "wei\u00df", ":", "er", "soll", "re\u00b7pr\u00e4\u00b7sen\u00b7tie\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich wei\u00df: abh\u00e4ngig vom B\u00fcro . . .", "tokens": ["Ich", "wei\u00df", ":", "ab\u00b7h\u00e4n\u00b7gig", "vom", "B\u00fc\u00b7ro", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "ADJD", "APPRART", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die Position . . . er mu\u00df paktieren . . .", "tokens": ["die", "Po\u00b7si\u00b7ti\u00b7on", ".", ".", ".", "er", "mu\u00df", "pak\u00b7tie\u00b7ren", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "$.", "$.", "$.", "PPER", "VMFIN", "VVINF", "$.", "$.", "$."], "meter": "--+-+-+-+-", "measure": "anapaest.init"}}, "stanza.13": {"line.1": {"text": "Der Arbeiter sah hoffnungsvoll", "tokens": ["Der", "Ar\u00b7bei\u00b7ter", "sah", "hoff\u00b7nungs\u00b7voll"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD"], "meter": "-++-++-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "auf seinen Mann. Dem wollt er dienen.", "tokens": ["auf", "sei\u00b7nen", "Mann", ".", "Dem", "wollt", "er", "die\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$.", "PDS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "In langen Jahren wuchs der Groll:", "tokens": ["In", "lan\u00b7gen", "Jah\u00b7ren", "wuchs", "der", "Groll", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbeiner von uns? Einer von ihnen!\u00ab", "tokens": ["\u00bb", "ei\u00b7ner", "von", "uns", "?", "Ei\u00b7ner", "von", "ih\u00b7nen", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "APPR", "PPER", "$.", "PIS", "APPR", "PPER", "$.", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Vergessen, was man lebenslang", "tokens": ["Ver\u00b7ges\u00b7sen", ",", "was", "man", "le\u00b7bens\u00b7lang"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVPP", "$,", "PRELS", "PIS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "f\u00fcr die Genossen sch\u00f6n gepredigt?", "tokens": ["f\u00fcr", "die", "Ge\u00b7nos\u00b7sen", "sch\u00f6n", "ge\u00b7pre\u00b7digt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Gang die Reichswehrfront entlang \u2013", "tokens": ["Ein", "Gang", "die", "Reichs\u00b7wehr\u00b7front", "ent\u00b7lang", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "APPO", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und Marx und Bebel sind erledigt.", "tokens": ["und", "Marx", "und", "Be\u00b7bel", "sind", "er\u00b7le\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "KON", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Sechs Jahr kein Wort, das uns bewegt.", "tokens": ["Sechs", "Jahr", "kein", "Wort", ",", "das", "uns", "be\u00b7wegt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "PIAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Wort f\u00fcr die in den Fabriken.", "tokens": ["Kein", "Wort", "f\u00fcr", "die", "in", "den", "Fab\u00b7ri\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "ART", "APPR", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Kein Wort, das unsre Zeit erregt \u2013", "tokens": ["Kein", "Wort", ",", "das", "uns\u00b7re", "Zeit", "er\u00b7regt", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "nur Gehrock, Messen und Musiken.", "tokens": ["nur", "Ge\u00b7hrock", ",", "Mes\u00b7sen", "und", "Mu\u00b7si\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Ein wahres Herz verliert sich nie.", "tokens": ["Ein", "wah\u00b7res", "Herz", "ver\u00b7liert", "sich", "nie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PRF", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der ist den breiten Weg gegangen.", "tokens": ["Der", "ist", "den", "brei\u00b7ten", "Weg", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie die Partei. Er ist wie sie.", "tokens": ["Wie", "die", "Par\u00b7tei", ".", "Er", "ist", "wie", "sie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "$.", "PPER", "VAFIN", "KOKOM", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man darf wohl nicht zu viel verlangen.", "tokens": ["Man", "darf", "wohl", "nicht", "zu", "viel", "ver\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ADV", "PTKNEG", "PTKA", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}