{"dta.poem.12578": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Siebender Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Hastu Calliope die Hand bi\u00dfher gef\u00fchret/", "tokens": ["Has\u00b7tu", "Cal\u00b7li\u00b7o\u00b7pe", "die", "Hand", "bi\u00df\u00b7her", "ge\u00b7f\u00fch\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ART", "NN", "ADV", "VVPP", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Da\u00df ich den Deutschen Krieg bi\u00df diese Schlacht be-", "tokens": ["Da\u00df", "ich", "den", "Deut\u00b7schen", "Krieg", "bi\u00df", "die\u00b7se", "Schlacht", "be"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "PDAT", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und angezeichnet hab\u2019. Ey so steh ferner bey", "tokens": ["und", "an\u00b7ge\u00b7zeich\u00b7net", "hab'", ".", "Ey", "so", "steh", "fer\u00b7ner", "bey"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "VAFIN", "$.", "NN", "ADV", "VVFIN", "ADV", "APPR"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und sage was noch mehr hierauf erfolget sey:", "tokens": ["und", "sa\u00b7ge", "was", "noch", "mehr", "hier\u00b7auf", "er\u00b7fol\u00b7get", "sey", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PWS", "ADV", "ADV", "PAV", "VVFIN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gleich wie ein strenger Strohm/ wann er den Damm durch-", "tokens": ["Gleich", "wie", "ein", "stren\u00b7ger", "Strohm", "/", "wann", "er", "den", "Damm", "durch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "ADJA", "NN", "$(", "PWAV", "PPER", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "und keine Haltung hat/ mit sch\u00e4dlichem erg\u00fcssen", "tokens": ["und", "kei\u00b7ne", "Hal\u00b7tung", "hat", "/", "mit", "sch\u00e4d\u00b7li\u00b7chem", "er\u00b7g\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "VAFIN", "$(", "APPR", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Sich in die Weite macht/ Vieh/ Menschen/ Hau\u00df und Gut", "tokens": ["Sich", "in", "die", "Wei\u00b7te", "macht", "/", "Vieh", "/", "Men\u00b7schen", "/", "Hau\u00df", "und", "Gut"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "VVFIN", "$(", "NN", "$(", "NN", "$(", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "In das Verderben setzt/ und macht das Land zur Flut/", "tokens": ["In", "das", "Ver\u00b7der\u00b7ben", "setzt", "/", "und", "macht", "das", "Land", "zur", "Flut", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$(", "KON", "VVFIN", "ART", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Da\u00df man die Berge sucht/ sein Leben zu erretten.", "tokens": ["Da\u00df", "man", "die", "Ber\u00b7ge", "sucht", "/", "sein", "Le\u00b7ben", "zu", "er\u00b7ret\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "VVFIN", "$(", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Also that diese Schlacht bey vielen Volck und St\u00e4dten.", "tokens": ["Al\u00b7so", "that", "die\u00b7se", "Schlacht", "bey", "vie\u00b7len", "Volck", "und", "St\u00e4d\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PDAT", "NN", "APPR", "PIAT", "NN", "KON", "NN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.11": {"text": "Die siegende Parthey ergo\u00df sich wie ein Flu\u00df/", "tokens": ["Die", "sie\u00b7gen\u00b7de", "Par\u00b7they", "er\u00b7go\u00df", "sich", "wie", "ein", "Flu\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PRF", "KOKOM", "ART", "NN", "$("], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.12": {"text": "Der Widerhalt war weg und satzte seinen Fu\u00df", "tokens": ["Der", "Wi\u00b7der\u00b7halt", "war", "weg", "und", "satz\u00b7te", "sei\u00b7nen", "Fu\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADV", "KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Auf Bergen/ bi\u00df die Hilff von fernen L\u00e4ndern sch\u00fctzte.", "tokens": ["Auf", "Ber\u00b7gen", "/", "bi\u00df", "die", "Hilff", "von", "fer\u00b7nen", "L\u00e4n\u00b7dern", "sch\u00fctz\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Bannier/ der diese Zeit das B\u00f6h\u00e4im-Reich beblitzte/", "tokens": ["Ban\u00b7nier", "/", "der", "die\u00b7se", "Zeit", "das", "B\u00f6h\u00e4im\u00b7Reich", "be\u00b7blitz\u00b7te", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "ART", "PDAT", "NN", "ART", "NE", "VVFIN", "$("], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}, "line.15": {"text": "und guten Fortgang hatt\u2019 in Einahm mancher Stadt/", "tokens": ["und", "gu\u00b7ten", "Fort\u00b7gang", "hatt'", "in", "Ei\u00b7nahm", "man\u00b7cher", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "APPR", "ART", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Worzu das Sachsen-Heer nicht schlechten Vorschub that/", "tokens": ["Wor\u00b7zu", "das", "Sach\u00b7sen\u00b7Heer", "nicht", "schlech\u00b7ten", "Vor\u00b7schub", "that", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PTKNEG", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Must\u2019 auf die b\u00f6se Post der Niederlag in Schwaben/", "tokens": ["Must'", "auf", "die", "b\u00f6\u00b7se", "Post", "der", "Nie\u00b7der\u00b7lag", "in", "Schwa\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "ADJA", "NN", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Aus B\u00f6h\u00e4im uud geschwind/ um einen Schutz zu haben/", "tokens": ["Aus", "B\u00f6\u00b7h\u00e4im", "u\u00b7ud", "ge\u00b7schwind", "/", "um", "ei\u00b7nen", "Schutz", "zu", "ha\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "ADJD", "$(", "APPR", "ART", "NN", "PTKZU", "VAINF", "$("], "meter": "--+-+-+-+-+-+-", "measure": "anapaest.init"}, "line.19": {"text": "Nach Th\u00fcringen/ den Feind/ des Piccolomini", "tokens": ["Nach", "Th\u00fc\u00b7rin\u00b7gen", "/", "den", "Feind", "/", "des", "Pic\u00b7co\u00b7lo\u00b7mi\u00b7ni"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$(", "ART", "NN", "$(", "ART", "NN"], "meter": "++-+-+-+-+-+", "measure": "unknown.measure.septa"}, "line.20": {"text": "und Jsolani Heer/ das sonder gro\u00dfe M\u00fch", "tokens": ["und", "Jso\u00b7la\u00b7ni", "Heer", "/", "das", "son\u00b7der", "gro\u00b7\u00dfe", "M\u00fch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NE", "NN", "$(", "ART", "ADJA", "ADJA", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.21": {"text": "Daselbst sich eingesetzt und Hertzog Wilhelms Waffen", "tokens": ["Da\u00b7selbst", "sich", "ein\u00b7ge\u00b7setzt", "und", "Hert\u00b7zog", "Wil\u00b7helms", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PRF", "VVPP", "KON", "NE", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Starck angetastet hatt\u2019/ auf andren Weg zu schaffen.", "tokens": ["Starck", "an\u00b7ge\u00b7tas\u00b7tet", "hatt'", "/", "auf", "an\u00b7dren", "Weg", "zu", "schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "VAFIN", "$(", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Er kam/ und es gieng jhm der Abtrieb gl\u00fccklich an.", "tokens": ["Er", "kam", "/", "und", "es", "gieng", "jhm", "der", "Ab\u00b7tri\u00b7eb", "gl\u00fcck\u00b7lich", "an", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "PPER", "VVFIN", "PPER", "ART", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+--+--+--+-+", "measure": "amphibrach.tetra.plus"}, "line.24": {"text": "Hierauf verschicket\u2019 er ein etlich tausend Mann", "tokens": ["Hier\u00b7auf", "ver\u00b7schi\u00b7cket'", "er", "ein", "et\u00b7lich", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PPER", "ART", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Vor sich nach Hessen ab/ den Hatzfeld zu bestreiten/", "tokens": ["Vor", "sich", "nach", "Hes\u00b7sen", "ab", "/", "den", "Hatz\u00b7feld", "zu", "be\u00b7strei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "APPR", "NE", "PTKVZ", "$(", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Weil dessen siegend Heer auf die Casselsche Seiten", "tokens": ["Weil", "des\u00b7sen", "sie\u00b7gend", "Heer", "auf", "die", "Cas\u00b7sel\u00b7sche", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDS", "CARD", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "Jm strengen Anzug war. Es wurd\u2019 auch diese Flut", "tokens": ["Jm", "stren\u00b7gen", "An\u00b7zug", "war", ".", "Es", "wurd'", "auch", "die\u00b7se", "Flut"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VAFIN", "$.", "PPER", "VAFIN", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Gehemmet und bey Vach mit der Croaten Blut", "tokens": ["Ge\u00b7hem\u00b7met", "und", "bey", "Vach", "mit", "der", "Croa\u00b7ten", "Blut"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "KON", "APPR", "NE", "APPR", "ART", "ADJA", "NN"], "meter": "-+---+--+-+", "measure": "iambic.tetra.relaxed"}, "line.29": {"text": "Durch den von Hoditz starck gef\u00e4rbet. Di\u00df geschehen/", "tokens": ["Durch", "den", "von", "Ho\u00b7ditz", "starck", "ge\u00b7f\u00e4r\u00b7bet", ".", "Di\u00df", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "NE", "ADJD", "VVPP", "$.", "PDS", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Sah man das Gothen Heer von Gotha weiter gehen/", "tokens": ["Sah", "man", "das", "Go\u00b7then", "Heer", "von", "Go\u00b7tha", "wei\u00b7ter", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "APPR", "NE", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Das achtzehn tausend starck sich mit dem H\u00e4upt Bannier", "tokens": ["Das", "acht\u00b7zehn", "tau\u00b7send", "starck", "sich", "mit", "dem", "H\u00e4upt", "Ban\u00b7nier"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "CARD", "CARD", "NN", "PRF", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Bey Egeln niederlie\u00df. Der Frost war vor der Th\u00fcr/", "tokens": ["Bey", "E\u00b7geln", "nie\u00b7der\u00b7lie\u00df", ".", "Der", "Frost", "war", "vor", "der", "Th\u00fcr", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$.", "ART", "NN", "VAFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Daher er sich daselbst den Winter \u00fcber legte/", "tokens": ["Da\u00b7her", "er", "sich", "da\u00b7selbst", "den", "Win\u00b7ter", "\u00fc\u00b7ber", "leg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "PRF", "PAV", "ART", "NN", "APPR", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "und auf so manche M\u00fch in allem wol verpflegte.", "tokens": ["und", "auf", "so", "man\u00b7che", "M\u00fch", "in", "al\u00b7lem", "wol", "ver\u00b7pfleg\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "PIAT", "NN", "APPR", "PIS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Cur-Sachsens Volck kam nach und lie\u00df das B\u00f6mer-Land", "tokens": ["Cur\u00b7Sach\u00b7sens", "Volck", "kam", "nach", "und", "lie\u00df", "das", "B\u00f6\u00b7mer\u00b7Land"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "VVFIN", "APPR", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Damit den K\u00e4ysrischen/ die nun in gutem Stand", "tokens": ["Da\u00b7mit", "den", "K\u00e4y\u00b7sri\u00b7schen", "/", "die", "nun", "in", "gu\u00b7tem", "Stand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "$(", "PRELS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}, "line.37": {"text": "und doch nichts wenigers von Friedens Sinnen waren/", "tokens": ["und", "doch", "nichts", "we\u00b7ni\u00b7gers", "von", "Frie\u00b7dens", "Sin\u00b7nen", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIS", "PIS", "APPR", "NE", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "Kein ", "tokens": ["Kein"], "token_info": ["word"], "pos": ["PIAT"], "meter": "+", "measure": "single.up"}, "line.39": {"text": "Auf solches lo\u00df zu gehn. Wodurch der sch\u00f6ne Schein", "tokens": ["Auf", "sol\u00b7ches", "lo\u00df", "zu", "gehn", ".", "Wo\u00b7durch", "der", "sch\u00f6\u00b7ne", "Schein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "PTKZU", "VVINF", "$.", "PWAV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Der lang-gew\u00fcndschten Ruh benebelt w\u00fcrde seyn.", "tokens": ["Der", "lang\u00b7ge\u00b7w\u00fcndschten", "Ruh", "be\u00b7ne\u00b7belt", "w\u00fcr\u00b7de", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "VAFIN", "VAINF", "$."], "meter": "-+--+---+-+", "measure": "iambic.tetra.relaxed"}, "line.41": {"text": "Sie giengen/ und es kam hierauf ", "tokens": ["Sie", "gien\u00b7gen", "/", "und", "es", "kam", "hier\u00b7auf"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "KON", "PPER", "VVFIN", "PAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.42": {"text": "Der dann das gantze Reich von Schweden hat geschieden.", "tokens": ["Der", "dann", "das", "gant\u00b7ze", "Reich", "von", "Schwe\u00b7den", "hat", "ge\u00b7schie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "ADJA", "NN", "APPR", "NE", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Cur-Sachsen gieng jhn ein/ Cur-Brandenburg gieng nach/", "tokens": ["Cur\u00b7Sach\u00b7sen", "gieng", "jhn", "ein", "/", "Cur\u00b7Bran\u00b7den\u00b7burg", "gieng", "nach", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ART", "$(", "NE", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "und war dergleichen Thun ein\u2019 allgemeine Sach\u2019", "tokens": ["und", "war", "derg\u00b7lei\u00b7chen", "Thun", "ein'", "all\u00b7ge\u00b7mei\u00b7ne", "Sach'"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PIS", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Jm gantzen Deutschen Reich\u2019. Allein die Frantz- und", "tokens": ["Jm", "gant\u00b7zen", "Deut\u00b7schen", "Reich'", ".", "Al\u00b7lein", "die", "Frant\u00b7z", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "ADJA", "NN", "$.", "ADV", "ART", "TRUNC", "KON"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Schweden/", "tokens": ["Schwe\u00b7den", "/"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.47": {"text": "Nechst den Casselischen/ die hatten jhre Reden", "tokens": ["Nechst", "den", "Cas\u00b7se\u00b7li\u00b7schen", "/", "die", "hat\u00b7ten", "jhre", "Re\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "$(", "PDS", "VAFIN", "PPOSAT", "NN"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.48": {"text": "Dergleichen Friedens-Werck mit nichten einzugehn.", "tokens": ["Derg\u00b7lei\u00b7chen", "Frie\u00b7dens\u00b7\u00b7Werck", "mit", "nich\u00b7ten", "ein\u00b7zu\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "NN", "APPR", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Wieviel man sich auch hatt\u2019 um Mittel umgesehn/", "tokens": ["Wie\u00b7viel", "man", "sich", "auch", "hatt'", "um", "Mit\u00b7tel", "um\u00b7ge\u00b7sehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "ADV", "VAFIN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "Denselben gnug zu thun/ so hat doch nichts verschlagen.", "tokens": ["Den\u00b7sel\u00b7ben", "gnug", "zu", "thun", "/", "so", "hat", "doch", "nichts", "ver\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "ADV", "PTKZU", "VVINF", "$(", "ADV", "VAFIN", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "Damit so kriegten wir noch gr\u00f6\u00dfre landes-Plagen/", "tokens": ["Da\u00b7mit", "so", "krieg\u00b7ten", "wir", "noch", "gr\u00f6\u00df\u00b7re", "lan\u00b7des\u00b7Pla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.52": {"text": "und fast so lang als vor. Ja zweymal sieben Jahr", "tokens": ["und", "fast", "so", "lang", "als", "vor", ".", "Ja", "zwey\u00b7mal", "sie\u00b7ben", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "ADJD", "KOKOM", "PTKVZ", "$.", "PTKANT", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.53": {"text": "Entgiengen noch hierauf/ eh wieder Friede war.", "tokens": ["Ent\u00b7gien\u00b7gen", "noch", "hier\u00b7auf", "/", "eh", "wie\u00b7der", "Frie\u00b7de", "war", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PAV", "$(", "KOUS", "ADV", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Bannier lie\u00df diesen Ri\u00df sich noch so gro\u00df nicht schrecken.", "tokens": ["Ban\u00b7nier", "lie\u00df", "die\u00b7sen", "Ri\u00df", "sich", "noch", "so", "gro\u00df", "nicht", "schre\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PDAT", "NN", "PRF", "ADV", "ADV", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Er war auch nicht befelcht den Degen einzustecken/", "tokens": ["Er", "war", "auch", "nicht", "be\u00b7felcht", "den", "De\u00b7gen", "ein\u00b7zu\u00b7ste\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "VVFIN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Vielmehr den Gegentheil zu d\u00e4mpfen/ wie er kunt\u2019/", "tokens": ["Viel\u00b7mehr", "den", "Ge\u00b7gen\u00b7theil", "zu", "d\u00e4mp\u00b7fen", "/", "wie", "er", "kunt'", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PTKZU", "VVINF", "$(", "PWAV", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "und weil Cur-Sachsens Heer bey Leipzig fertig stundt\u2019/", "tokens": ["und", "weil", "Cur\u00b7Sach\u00b7sens", "Heer", "bey", "Leip\u00b7zig", "fer\u00b7tig", "stundt'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NE", "NN", "APPR", "NE", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "umb bey dem Prager-Schlu\u00df und Frieden steiff zu bleiben/", "tokens": ["umb", "bey", "dem", "Pra\u00b7ger\u00b7Schlu\u00df", "und", "Frie\u00b7den", "steiff", "zu", "blei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPR", "ART", "NN", "KON", "NN", "VVFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "und diese Nordsche Macht aus Mei\u00dfen weg zu treiben/", "tokens": ["und", "die\u00b7se", "Nord\u00b7sche", "Macht", "aus", "Mei\u00b7\u00dfen", "weg", "zu", "trei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "ADJA", "NN", "APPR", "NN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Verst\u00e4rckte sich Bannier in zwantzig tausend Mann/", "tokens": ["Ver\u00b7st\u00e4rck\u00b7te", "sich", "Ban\u00b7nier", "in", "zwant\u00b7zig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "NE", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "und sah also den Feind mit trotzgen Augen an/", "tokens": ["und", "sah", "al\u00b7so", "den", "Feind", "mit", "trotz\u00b7gen", "Au\u00b7gen", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.62": {"text": "Nicht achtend/ da\u00df er den in vielen schweren Z\u00fcgen/", "tokens": ["Nicht", "ach\u00b7tend", "/", "da\u00df", "er", "den", "in", "vie\u00b7len", "schwe\u00b7ren", "Z\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$(", "KOUS", "PPER", "ART", "APPR", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "unl\u00e4ngst mit jhm gethan/ hat gl\u00fccklich sehen siegen/", "tokens": ["un\u00b7l\u00e4ngst", "mit", "jhm", "ge\u00b7than", "/", "hat", "gl\u00fcck\u00b7lich", "se\u00b7hen", "sie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "VVPP", "$(", "VAFIN", "ADJD", "VVINF", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.64": {"text": "und da\u00df er durch ein Heer von K\u00e4yserlicher Macht/", "tokens": ["und", "da\u00df", "er", "durch", "ein", "Heer", "von", "K\u00e4y\u00b7ser\u00b7li\u00b7cher", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Durch ", "tokens": ["Durch"], "token_info": ["word"], "pos": ["APPR"], "meter": "+", "measure": "single.up"}, "line.66": {"text": "Gantz nichts erschrocken war. Die vormahls lieben Frein-", "tokens": ["Gantz", "nichts", "er\u00b7schro\u00b7cken", "war", ".", "Die", "vor\u00b7mahls", "lie\u00b7ben", "Frei\u00b7n"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "PIS", "VVPP", "VAFIN", "$.", "ART", "ADV", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.67": {"text": "Erhoben sich zum Streit und wurden gro\u00dfe Feinde.", "tokens": ["Er\u00b7ho\u00b7ben", "sich", "zum", "Streit", "und", "wur\u00b7den", "gro\u00b7\u00dfe", "Fein\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPRART", "NN", "KON", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.68": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.69": {"text": "Weil sie aus solchem Zwist sehr gro\u00dfen Vorthel zieht.", "tokens": ["Weil", "sie", "aus", "sol\u00b7chem", "Zwist", "sehr", "gro\u00b7\u00dfen", "Vor\u00b7thel", "zieht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Es lie\u00df sich anfangs schlecht mit dem Bannier ansehen/", "tokens": ["Es", "lie\u00df", "sich", "an\u00b7fangs", "schlecht", "mit", "dem", "Ban\u00b7nier", "an\u00b7se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.71": {"text": "So da\u00df er sehr gedr\u00e4ngt zu r\u00fccke muste gehen/", "tokens": ["So", "da\u00df", "er", "sehr", "ge\u00b7dr\u00e4ngt", "zu", "r\u00fc\u00b7cke", "mus\u00b7te", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADV", "VVPP", "PTKZU", "VVFIN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "und einer gr\u00f6\u00dfern Macht das Stifft von Halberstadt/", "tokens": ["und", "ei\u00b7ner", "gr\u00f6\u00b7\u00dfern", "Macht", "das", "Stifft", "von", "Hal\u00b7ber\u00b7stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "ART", "NN", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Auch das von Magdeburg verlassen/ wie er that.", "tokens": ["Auch", "das", "von", "Mag\u00b7de\u00b7burg", "ver\u00b7las\u00b7sen", "/", "wie", "er", "that", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "APPR", "NE", "VVINF", "$(", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.74": {"text": "Und weil auch \u00fcber di\u00df viel unter seinen Fahnen/", "tokens": ["Und", "weil", "auch", "\u00fc\u00b7ber", "di\u00df", "viel", "un\u00b7ter", "sei\u00b7nen", "Fah\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "APPR", "PDS", "ADV", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "So gro\u00df-als kleine Leuth\u2019/ auf ernstliches vermahnen", "tokens": ["So", "gro\u00df\u00b7als", "klei\u00b7ne", "Leuth'", "/", "auf", "ernst\u00b7li\u00b7ches", "ver\u00b7mah\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJA", "ADJA", "NN", "$(", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.76": {"text": "Des K\u00e4ysers/ jhren Fu\u00df zu wenden/ und der Macht", "tokens": ["Des", "K\u00e4y\u00b7sers", "/", "jhren", "Fu\u00df", "zu", "wen\u00b7den", "/", "und", "der", "Macht"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$(", "PPOSAT", "NN", "PTKZU", "VVINF", "$(", "KON", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.77": {"text": "Von Sachsen zu zugehn/ in m\u00e4rcklichem Verdacht/", "tokens": ["Von", "Sach\u00b7sen", "zu", "zu\u00b7gehn", "/", "in", "m\u00e4r\u00b7ck\u00b7li\u00b7chem", "Ver\u00b7dacht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKZU", "VVINF", "$(", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.78": {"text": "Auch viel der jenigen schon umgetreten waren/", "tokens": ["Auch", "viel", "der", "je\u00b7ni\u00b7gen", "schon", "um\u00b7ge\u00b7tre\u00b7ten", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "PIAT", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.79": {"text": "So hatt\u2019 er sich hierauf kein kleines zubefahren.", "tokens": ["So", "hatt'", "er", "sich", "hier\u00b7auf", "kein", "klei\u00b7nes", "zu\u00b7be\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "PAV", "PIAT", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "We\u00dfwegen er sich gantz nach Mecklenburg begab.", "tokens": ["We\u00df\u00b7we\u00b7gen", "er", "sich", "gantz", "nach", "Meck\u00b7len\u00b7burg", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "ADV", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Nun aber fiel das Gl\u00fcck vom Geaner wieder ab/", "tokens": ["Nun", "a\u00b7ber", "fiel", "das", "Gl\u00fcck", "vom", "Ge\u00b7a\u00b7ner", "wie\u00b7der", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "APPRART", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.82": {"text": "und folgte dem Bannier. Das Gl\u00fcck ist wanckelm\u00fctig/", "tokens": ["und", "folg\u00b7te", "dem", "Ban\u00b7nier", ".", "Das", "Gl\u00fcck", "ist", "wan\u00b7ckel\u00b7m\u00fc\u00b7tig", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "ART", "NN", "VAFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "Dem es anheunte zornt/ dem ist es morgen g\u00fctig/", "tokens": ["Dem", "es", "an\u00b7he\u00b7un\u00b7te", "zornt", "/", "dem", "ist", "es", "mor\u00b7gen", "g\u00fc\u00b7tig", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$(", "PDS", "VAFIN", "PPER", "ADV", "ADJD", "$("], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.84": {"text": "Es kennet keinen Stand/ es hat des Monden Art/", "tokens": ["Es", "ken\u00b7net", "kei\u00b7nen", "Stand", "/", "es", "hat", "des", "Mon\u00b7den", "Art", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$(", "PPER", "VAFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "und dauert nicht gar lang mit voller Gegenwart.", "tokens": ["und", "dau\u00b7ert", "nicht", "gar", "lang", "mit", "vol\u00b7ler", "Ge\u00b7gen\u00b7wart", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "ADJD", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "So schleunig sich Bannier zu r\u00fccke muste geben/", "tokens": ["So", "schleu\u00b7nig", "sich", "Ban\u00b7nier", "zu", "r\u00fc\u00b7cke", "mus\u00b7te", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PRF", "NN", "PTKZU", "VVFIN", "VMFIN", "VVINF", "$("], "meter": "-+-++--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.87": {"text": "So schleunig sah man jhn sich wiederum erheben", "tokens": ["So", "schleu\u00b7nig", "sah", "man", "jhn", "sich", "wie\u00b7de\u00b7rum", "er\u00b7he\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "PIS", "PPER", "PRF", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "und nach dem Feinde gehn. Hier war nicht lang gesucht/", "tokens": ["und", "nach", "dem", "Fein\u00b7de", "gehn", ".", "Hier", "war", "nicht", "lang", "ge\u00b7sucht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVINF", "$.", "ADV", "VAFIN", "PTKNEG", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Er traff und schlug den Dehn bey Altenburg zur Flucht/", "tokens": ["Er", "traff", "und", "schlug", "den", "Dehn", "bey", "Al\u00b7ten\u00b7burg", "zur", "Flucht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "ART", "NN", "APPR", "NE", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Wo er des Vizthums Fahn\u2019 und viel Gefangne kriegte/", "tokens": ["Wo", "er", "des", "Vizt\u00b7hums", "Fahn'", "und", "viel", "Ge\u00b7fang\u00b7ne", "krieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "NN", "KON", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "Auf welches Rittwein sich bald an den Baudis f\u00fcgte/", "tokens": ["Auf", "wel\u00b7ches", "Ritt\u00b7wein", "sich", "bald", "an", "den", "Bau\u00b7dis", "f\u00fcg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "PRF", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "An Baudis/ der nunmehr des Arnheims Platz vertrat/", "tokens": ["An", "Bau\u00b7dis", "/", "der", "nun\u00b7mehr", "des", "Arn\u00b7heims", "Platz", "ver\u00b7trat", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "ART", "ADV", "ART", "NE", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.93": {"text": "Und sieben tausend starck Gewalt an D\u00f6mitz 2. Novembr. that.", "tokens": ["Und", "sie\u00b7ben", "tau\u00b7send", "starck", "Ge\u00b7walt", "an", "D\u00f6\u00b7mitz", "2.", "No\u00b7vembr", ".", "that", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "ordinal", "word", "punct", "word", "punct"], "pos": ["KON", "CARD", "CARD", "NN", "NN", "APPR", "NE", "ADJA", "NN", "$.", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.94": {"text": "Wor\u00fcber es sehr bald zu starcken scharmutziren", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "es", "sehr", "bald", "zu", "star\u00b7cken", "schar\u00b7mut\u00b7zi\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "ADV", "PTKZU", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Gerathen ist/ und war der S\u00e4chsischen verlieren", "tokens": ["Ge\u00b7ra\u00b7then", "ist", "/", "und", "war", "der", "S\u00e4ch\u00b7si\u00b7schen", "ver\u00b7lie\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$(", "KON", "VAFIN", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "Sehr gro\u00df. Ein tausend Mann verschieden auf dem Plan/", "tokens": ["Sehr", "gro\u00df", ".", "Ein", "tau\u00b7send", "Mann", "ver\u00b7schie\u00b7den", "auf", "dem", "Plan", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "ART", "CARD", "NN", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "Ein tausend musten mit/ den Rest zerstreute man.", "tokens": ["Ein", "tau\u00b7send", "mus\u00b7ten", "mit", "/", "den", "Rest", "zer\u00b7streu\u00b7te", "man", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "VMFIN", "APPR", "$(", "ART", "NN", "VVFIN", "PIS", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Di\u00df senckete den Muth den laug-begl\u00fcckten Sachsen.", "tokens": ["Di\u00df", "sen\u00b7cke\u00b7te", "den", "Muth", "den", "laug\u00b7be\u00b7gl\u00fcck\u00b7ten", "Sach\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.99": {"text": "Hergegen sahe man der Schweden Hoffnung wachsen/", "tokens": ["Her\u00b7ge\u00b7gen", "sa\u00b7he", "man", "der", "Schwe\u00b7den", "Hoff\u00b7nung", "wach\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.100": {"text": "Noch mehr begl\u00fcckt zu seyn. Ein wenig Zeit hierauff", "tokens": ["Noch", "mehr", "be\u00b7gl\u00fcckt", "zu", "seyn", ".", "Ein", "we\u00b7nig", "Zeit", "hier\u00b7auff"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVPP", "PTKZU", "VAINF", "$.", "ART", "PIAT", "NN", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Wurd auch bey Goldenberg ein gro\u00dfer Sachsen-Hauff", "tokens": ["Wurd", "auch", "bey", "Gol\u00b7den\u00b7berg", "ein", "gro\u00b7\u00dfer", "Sach\u00b7sen\u00b7Hauff"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "NE", "ART", "ADJA", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.102": {"text": "Besprungen und zerstreut. Bey Kiritz kam dergleichen/", "tokens": ["Be\u00b7sprun\u00b7gen", "und", "zer\u00b7streut", ".", "Bey", "Ki\u00b7ritz", "kam", "derg\u00b7lei\u00b7chen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$.", "APPR", "NE", "VVFIN", "PIS", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "So/ da\u00df das gantze Heer nach Sandau muste weichen/", "tokens": ["So", "/", "da\u00df", "das", "gant\u00b7ze", "Heer", "nach", "San\u00b7dau", "mus\u00b7te", "wei\u00b7chen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KOUS", "ART", "ADJA", "NN", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Dem die Banniersche Macht sehr starck in R\u00fccken drung/", "tokens": ["Dem", "die", "Ban\u00b7nier\u00b7sche", "Macht", "sehr", "starck", "in", "R\u00fc\u00b7cken", "drung", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "ADJA", "NN", "ADV", "ADJD", "APPR", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "Den Dohm zu Havelberg in seine Macht bezwung/", "tokens": ["Den", "Dohm", "zu", "Ha\u00b7vel\u00b7berg", "in", "sei\u00b7ne", "Macht", "be\u00b7zwung", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "APPR", "PPOSAT", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Die Stadt mit Sturm bekam. Auf dieses galt es Wer-", "tokens": ["Die", "Stadt", "mit", "Sturm", "be\u00b7kam", ".", "Auf", "die\u00b7ses", "galt", "es", "Wer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$.", "APPR", "PDAT", "VVFIN", "PPER", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Das Torsten Sohn bezwung/ zu m\u00e4chtigem verderben", "tokens": ["Das", "Tors\u00b7ten", "Sohn", "be\u00b7zwung", "/", "zu", "m\u00e4ch\u00b7ti\u00b7gem", "ver\u00b7der\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NE", "$(", "APPR", "ADJA", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "Der S\u00e4chsischen Gewalt/ die endlich Marozin/", "tokens": ["Der", "S\u00e4ch\u00b7si\u00b7schen", "Ge\u00b7walt", "/", "die", "end\u00b7lich", "Ma\u00b7ro\u00b7zin", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "ART", "ADV", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Der bey den Schlesiern bi\u00dfher sehr gl\u00fcckhafft schien/", "tokens": ["Der", "bey", "den", "Schle\u00b7siern", "bi\u00df\u00b7her", "sehr", "gl\u00fcck\u00b7hafft", "schien", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADV", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.110": {"text": "Mit seiner Macht verst\u00e4rckt/ zu andrem Stande brachte/", "tokens": ["Mit", "sei\u00b7ner", "Macht", "ver\u00b7st\u00e4rckt", "/", "zu", "an\u00b7drem", "Stan\u00b7de", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVPP", "$(", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Worauf sie Havelberg vou neuem S\u00e4chsisch machte/", "tokens": ["Wo\u00b7rauf", "sie", "Ha\u00b7vel\u00b7berg", "vou", "neu\u00b7em", "S\u00e4ch\u00b7sisch", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "NE", "NE", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "und dann bey Verpelin das Winter-L\u00e4ger nahm/", "tokens": ["und", "dann", "bey", "Ver\u00b7pe\u00b7lin", "das", "Win\u00b7ter\u00b7L\u00e4\u00b7ger", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "ART", "NN", "VVFIN", "$("], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.113": {"text": "Wodurch der Gegentheil auch eins zu ruhen kam.", "tokens": ["Wo\u00b7durch", "der", "Ge\u00b7gen\u00b7theil", "auch", "eins", "zu", "ru\u00b7hen", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADV", "PIS", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Wir lassen sie alhier/ und gehen nach den Schwaben/", "tokens": ["Wir", "las\u00b7sen", "sie", "al\u00b7hier", "/", "und", "ge\u00b7hen", "nach", "den", "Schwa\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$(", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "Zu sehen/ wie sie sich nachlener Schlacht gehaben.", "tokens": ["Zu", "se\u00b7hen", "/", "wie", "sie", "sich", "nach\u00b7le\u00b7ner", "Schlacht", "ge\u00b7ha\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWAV", "PPER", "PRF", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Erbarmens-werter-Stadt! den man kaum sagen kan.", "tokens": ["Er\u00b7bar\u00b7mens\u00b7wer\u00b7ter\u00b7Stadt", "!", "den", "man", "kaum", "sa\u00b7gen", "kan", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "ART", "PIS", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.117": {"text": "So bald die gro\u00dfe Schlacht bey N\u00f6rdlingen gethan/", "tokens": ["So", "bald", "die", "gro\u00b7\u00dfe", "Schlacht", "bey", "N\u00f6rd\u00b7lin\u00b7gen", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.118": {"text": "Vorbey war/ muste sich die Stadt hierauf ergeben/", "tokens": ["Vor\u00b7bey", "war", "/", "mus\u00b7te", "sich", "die", "Stadt", "hier\u00b7auf", "er\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$(", "VMFIN", "PRF", "ART", "NN", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.119": {"text": "Weil da kein Mittel war noch mehr zu wiedersteben.", "tokens": ["Weil", "da", "kein", "Mit\u00b7tel", "war", "noch", "mehr", "zu", "wie\u00b7ders\u00b7te\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "NN", "VAFIN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Es war genug da\u00df sie f\u00fcnf gantzer Wochen stritt/", "tokens": ["Es", "war", "ge\u00b7nug", "da\u00df", "sie", "f\u00fcnf", "gant\u00b7zer", "Wo\u00b7chen", "stritt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "KOUS", "PPER", "CARD", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Viel gro\u00dfe St\u00fcrm ertrug/ drey tausend Sch\u00fc\u00df\u2019 erlitt\u2019/", "tokens": ["Viel", "gro\u00b7\u00dfe", "St\u00fcrm", "er\u00b7trug", "/", "drey", "tau\u00b7send", "Sch\u00fc\u00df'", "er\u00b7litt'", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VVFIN", "$(", "CARD", "CARD", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Auch \u00fcber tausend und f\u00fcnf hundert Feuer-Ballen/", "tokens": ["Auch", "\u00fc\u00b7ber", "tau\u00b7send", "und", "f\u00fcnf", "hun\u00b7dert", "Feu\u00b7er\u00b7Bal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "CARD", "KON", "CARD", "CARD", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.123": {"text": "Die aus des Feindes Heer sind in die Stadt gefallen/", "tokens": ["Die", "aus", "des", "Fein\u00b7des", "Heer", "sind", "in", "die", "Stadt", "ge\u00b7fal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "NN", "VAFIN", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.124": {"text": "In sich verschluckete. Hierauf kam man zu dier/", "tokens": ["In", "sich", "ver\u00b7schluc\u00b7ke\u00b7te", ".", "Hier\u00b7auf", "kam", "man", "zu", "dier", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "VVFIN", "$.", "PAV", "VVFIN", "PIS", "APPR", "PDAT", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "Du edles W\u00fcrtenberg/ das unsrer L\u00e4nder Zier", "tokens": ["Du", "ed\u00b7les", "W\u00fcr\u00b7ten\u00b7berg", "/", "das", "uns\u00b7rer", "L\u00e4n\u00b7der", "Zier"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.126": {"text": "und Paradei\u00df mag seyn. Wie man mit dir gehauset", "tokens": ["und", "Pa\u00b7ra\u00b7dei\u00df", "mag", "seyn", ".", "Wie", "man", "mit", "dir", "ge\u00b7hau\u00b7set"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VMFIN", "VAINF", "$.", "PWAV", "PIS", "APPR", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "und umgegangen hab/ ist also/ da\u00df uns grauset", "tokens": ["und", "um\u00b7ge\u00b7gan\u00b7gen", "hab", "/", "ist", "al\u00b7so", "/", "da\u00df", "uns", "grau\u00b7set"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVPP", "VAFIN", "$(", "VAFIN", "ADV", "$(", "KOUS", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Wann man davon vernimmt. Der F\u00fcrst verlie\u00df sein", "tokens": ["Wann", "man", "da\u00b7von", "ver\u00b7nimmt", ".", "Der", "F\u00fcrst", "ver\u00b7lie\u00df", "sein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "PAV", "VVFIN", "$.", "ART", "NN", "VVFIN", "PPOSAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.129": {"text": "land", "tokens": ["land"], "token_info": ["word"], "pos": ["NN"], "meter": "-", "measure": "single.down"}, "line.130": {"text": "Der Adel folgte nach. Was dieses f\u00fcr ein Stand", "tokens": ["Der", "A\u00b7del", "folg\u00b7te", "nach", ".", "Was", "die\u00b7ses", "f\u00fcr", "ein", "Stand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$.", "PWS", "PDAT", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "Bey einer Herde sey/ wann man die Hirten jaget/", "tokens": ["Bey", "ei\u00b7ner", "Her\u00b7de", "sey", "/", "wann", "man", "die", "Hir\u00b7ten", "ja\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "$(", "PWAV", "PIS", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.132": {"text": "Gedencket jeder leicht/ eh man jhm solches saget.", "tokens": ["Ge\u00b7den\u00b7cket", "je\u00b7der", "leicht", "/", "eh", "man", "jhm", "sol\u00b7ches", "sa\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADJD", "$(", "KOUS", "PIS", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "Was l\u00e4ngst den Necker kam/ das kam auch l\u00e4ngst den", "tokens": ["Was", "l\u00e4ngst", "den", "Ne\u00b7cker", "kam", "/", "das", "kam", "auch", "l\u00e4ngst", "den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ART", "NN", "VVFIN", "$(", "PDS", "VVFIN", "ADV", "ADV", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.134": {"text": "Meyn/", "tokens": ["Meyn", "/"], "token_info": ["word", "punct"], "pos": ["PPOSAT", "$("], "meter": "+", "measure": "single.up"}, "line.135": {"text": "und sah man diese land\u2019 in gro\u00dfem Jammer seyn.", "tokens": ["und", "sah", "man", "die\u00b7se", "land'", "in", "gro\u00b7\u00dfem", "Jam\u00b7mer", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PDAT", "NN", "APPR", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.136": {"text": "Di\u00df alles machete/ da\u00df Ferdinand/ der K\u00e4yser/", "tokens": ["Di\u00df", "al\u00b7les", "ma\u00b7che\u00b7te", "/", "da\u00df", "Fer\u00b7di\u00b7nand", "/", "der", "K\u00e4y\u00b7ser", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "$(", "KOUS", "NE", "$(", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "An das beblute Reich und alle F\u00fcrsten-H\u00e4user", "tokens": ["An", "das", "be\u00b7blu\u00b7te", "Reich", "und", "al\u00b7le", "F\u00fcrs\u00b7ten\u00b7H\u00e4u\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "KON", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.138": {"text": "Durch ", "tokens": ["Durch"], "token_info": ["word"], "pos": ["APPR"], "meter": "+", "measure": "single.up"}, "line.139": {"text": "Vetrachten/ welches sie dann wieder an das Liecht", "tokens": ["Vet\u00b7rach\u00b7ten", "/", "wel\u00b7ches", "sie", "dann", "wie\u00b7der", "an", "das", "Liecht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "PWS", "PPER", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "Des Friedens von dem Pful des Jammers w\u00fcrde bringen/", "tokens": ["Des", "Frie\u00b7dens", "von", "dem", "Pful", "des", "Jam\u00b7mers", "w\u00fcr\u00b7de", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "ART", "NN", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.141": {"text": "Worauf/ wie schon gesagt/ viel zu dem K\u00e4yser giengen.", "tokens": ["Wo\u00b7rauf", "/", "wie", "schon", "ge\u00b7sagt", "/", "viel", "zu", "dem", "K\u00e4y\u00b7ser", "gien\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$(", "KOKOM", "ADV", "VVPP", "$(", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "Was aber Schwedisch blieb/ dem dr\u00f6ute die Gewalt.", "tokens": ["Was", "a\u00b7ber", "Schwe\u00b7disch", "blieb", "/", "dem", "dr\u00f6u\u00b7te", "die", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJD", "VVFIN", "$(", "ART", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Da\u00df gleichwol selbige noch einen Widerhalt", "tokens": ["Da\u00df", "gleich\u00b7wol", "sel\u00b7bi\u00b7ge", "noch", "ei\u00b7nen", "Wi\u00b7der\u00b7halt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADJA", "ADV", "ART", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.144": {"text": "Bek\u00e4me/ h\u00e4uffte man die weit-zerstreute Scharen/", "tokens": ["Be\u00b7k\u00e4\u00b7me", "/", "h\u00e4uff\u00b7te", "man", "die", "weit\u00b7zer\u00b7streu\u00b7te", "Scha\u00b7ren", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "PIS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "und die bey andern noch in gutem Stande waren/", "tokens": ["und", "die", "bey", "an\u00b7dern", "noch", "in", "gu\u00b7tem", "Stan\u00b7de", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "APPR", "PIS", "ADV", "APPR", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.146": {"text": "Ein recht und v\u00f6llig Heer bey zwantzig tausend Mann", "tokens": ["Ein", "recht", "und", "v\u00f6l\u00b7lig", "Heer", "bey", "zwant\u00b7zig", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "KON", "ADJD", "NN", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "Zu machen. ", "tokens": ["Zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.148": {"text": "Auf welches man zu Worms versamlet war zu rathen/", "tokens": ["Auf", "wel\u00b7ches", "man", "zu", "Worms", "ver\u00b7sam\u00b7let", "war", "zu", "ra\u00b7then", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "APPR", "NN", "VVPP", "VAFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.149": {"text": "Wie man mit solcher Macht hinfort den gro\u00dfen Schaden", "tokens": ["Wie", "man", "mit", "sol\u00b7cher", "Macht", "hin\u00b7fort", "den", "gro\u00b7\u00dfen", "Scha\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "APPR", "PIAT", "NN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Ersetzen m\u00f6cht/ und war Herr Cantzler Oxenstern", "tokens": ["Er\u00b7set\u00b7zen", "m\u00f6cht", "/", "und", "war", "Herr", "Cantz\u00b7ler", "O\u00b7xens\u00b7tern"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "VMFIN", "$(", "KON", "VAFIN", "NN", "NE", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.151": {"text": "Das H\u00e4upt von diesem Raht. Es kam hierauf so fern/", "tokens": ["Das", "H\u00e4upt", "von", "die\u00b7sem", "Raht", ".", "Es", "kam", "hier\u00b7auf", "so", "fern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "$.", "PPER", "VVFIN", "PAV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "Da\u00df man die Schweden Macht dort in die lande-Sach-", "tokens": ["Da\u00df", "man", "die", "Schwe\u00b7den", "Macht", "dort", "in", "die", "lan\u00b7de\u00b7Sach"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "ADJA", "NN", "ADV", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Den K\u00e4ysrischen daselbst vor Einfall gnug gewachsen", "tokens": ["Den", "K\u00e4y\u00b7sri\u00b7schen", "da\u00b7selbst", "vor", "Ein\u00b7fall", "gnug", "ge\u00b7wach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PAV", "APPR", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Zu seyn/ die Weymar-Macht am Reyhn vertheilete/", "tokens": ["Zu", "seyn", "/", "die", "Wey\u00b7ma\u00b7rMacht", "am", "Reyhn", "ver\u00b7thei\u00b7le\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "$(", "ART", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Kein Palm steigt ohne last und drucken in die H\u00f6y.", "tokens": ["Kein", "Palm", "steigt", "oh\u00b7ne", "last", "und", "dru\u00b7cken", "in", "die", "H\u00f6y", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "APPR", "VVFIN", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.156": {"text": "Also ergieng es auch bey diesen zwo-Armeen/", "tokens": ["Al\u00b7so", "er\u00b7gieng", "es", "auch", "bey", "die\u00b7sen", "zwo\u00b7Ar\u00b7me\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "NN", "$("], "meter": "+--+-+-+--+-+", "measure": "iambic.hexa.invert"}, "line.157": {"text": "Sie musten manchen Schlag bald dort/ bald da au\u00dfstehen", "tokens": ["Sie", "mus\u00b7ten", "man\u00b7chen", "Schlag", "bald", "dort", "/", "bald", "da", "au\u00df\u00b7ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "ADV", "ADV", "$(", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "Eh jede wiederum/ wie vor/ den Glantz bekam.", "tokens": ["Eh", "je\u00b7de", "wie\u00b7de\u00b7rum", "/", "wie", "vor", "/", "den", "Glantz", "be\u00b7kam", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "ADV", "$(", "KOKOM", "APPR", "$(", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "Als K\u00e4yser Ferdinand von diesem Thun vernahm/", "tokens": ["Als", "K\u00e4y\u00b7ser", "Fer\u00b7di\u00b7nand", "von", "die\u00b7sem", "Thun", "ver\u00b7nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "APPR", "PDAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "und da\u00df es noch nicht gar nach Wundsche wolte kommen/", "tokens": ["und", "da\u00df", "es", "noch", "nicht", "gar", "nach", "Wund\u00b7sche", "wol\u00b7te", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "PTKNEG", "ADV", "APPR", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.161": {"text": "Ob schon der Sieg einmal vom Feinde war genommen/", "tokens": ["Ob", "schon", "der", "Sieg", "ein\u00b7mal", "vom", "Fein\u00b7de", "war", "ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Sah man auch seine Macht zertheilt. Die Helffte zog/", "tokens": ["Sah", "man", "auch", "sei\u00b7ne", "Macht", "zer\u00b7theilt", ".", "Die", "Helff\u00b7te", "zog", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PPOSAT", "NN", "VVPP", "$.", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "In zwantzig tausend starck/ nach B\u00f6h\u00e4imb und bewog", "tokens": ["In", "zwant\u00b7zig", "tau\u00b7send", "starck", "/", "nach", "B\u00f6\u00b7h\u00e4i\u00b7mb", "und", "be\u00b7wog"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "CARD", "NN", "$(", "APPR", "NE", "KON", "VVFIN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.164": {"text": "Cur-Sachsens Macht heraus. Die Helffte gleicher M\u00e4n-", "tokens": ["Cur\u00b7Sach\u00b7sens", "Macht", "he\u00b7raus", ".", "Die", "Helff\u00b7te", "glei\u00b7cher", "M\u00e4n"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NN", "PTKVZ", "$.", "ART", "NN", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.165": {"text": "Zog an dem krummen Meyn und Necker in die L\u00e4nge/", "tokens": ["Zog", "an", "dem", "krum\u00b7men", "Meyn", "und", "Ne\u00b7cker", "in", "die", "L\u00e4n\u00b7ge", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "ADJA", "NN", "KON", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "und hatte manchen Sieg ob mancher feinen Standt.", "tokens": ["und", "hat\u00b7te", "man\u00b7chen", "Sieg", "ob", "man\u00b7cher", "fei\u00b7nen", "Standt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "KOUS", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "Heylbronn gieng wea nach dem es wol aufochten hatt\u2019.", "tokens": ["Heyl\u00b7bronn", "gieng", "wea", "nach", "dem", "es", "wol", "au\u00b7foch\u00b7ten", "hatt'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "NE", "APPR", "PRELS", "PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.168": {"text": "Und nun gieng auch das Schlo\u00df von W\u00fcrtzburg aus den", "tokens": ["Und", "nun", "gieng", "auch", "das", "Schlo\u00df", "von", "W\u00fcrtz\u00b7burg", "aus", "den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ADV", "ART", "NN", "APPR", "NE", "APPR", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.169": {"text": "H\u00e4nden.", "tokens": ["H\u00e4n\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.170": {"text": "Es war bey dieser Zeit solch Ding nicht abzuwenden.", "tokens": ["Es", "war", "bey", "die\u00b7ser", "Zeit", "solch", "Ding", "nicht", "ab\u00b7zu\u00b7wen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PDAT", "NN", "PIAT", "NN", "PTKNEG", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Ich rede von dem Schlo\u00df/ die Stadt war schon besiegt/", "tokens": ["Ich", "re\u00b7de", "von", "dem", "Schlo\u00df", "/", "die", "Stadt", "war", "schon", "be\u00b7siegt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$(", "ART", "NN", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Hief\u00fcr hat Freytags H\u00e4upt das Schwerdt zur Straff ge-", "tokens": ["Hie\u00b7f\u00fcr", "hat", "Frey\u00b7tags", "H\u00e4upt", "das", "Schwerdt", "zur", "Straff", "ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "ADV", "NN", "ART", "NN", "APPRART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.173": {"text": "Weil er bey ", "tokens": ["Weil", "er", "bey"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "PPER", "APPR"], "meter": "+-+", "measure": "trochaic.di"}, "line.174": {"text": "Da\u00df es aus Mangel must\u2019 an seinen Bischoff gehen.", "tokens": ["Da\u00df", "es", "aus", "Man\u00b7gel", "must'", "an", "sei\u00b7nen", "Bi\u00b7schoff", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "Dem W\u00fcrtzburg folgete die Philippsburg am Reyhn/", "tokens": ["Dem", "W\u00fcrtz\u00b7burg", "fol\u00b7ge\u00b7te", "die", "Phi\u00b7lipps\u00b7burg", "am", "Reyhn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.176": {"text": "Ein fester Ort/ und nahm des Gallas V\u00f6lcker ein/", "tokens": ["Ein", "fes\u00b7ter", "Ort", "/", "und", "nahm", "des", "Gal\u00b7las", "V\u00f6l\u00b7cker", "ein", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "KON", "VVFIN", "ART", "NN", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "und Speyer eben so. H\u00f6chst that hierauf dergleichen.", "tokens": ["und", "Spey\u00b7er", "e\u00b7ben", "so", ".", "H\u00f6chst", "that", "hier\u00b7auf", "derg\u00b7lei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "ADV", "$.", "NN", "VVFIN", "PAV", "PIS", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.178": {"text": "Doch Bobenhausen wolt in keinem Wege weichen/", "tokens": ["Doch", "Bo\u00b7ben\u00b7hau\u00b7sen", "wolt", "in", "kei\u00b7nem", "We\u00b7ge", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VMFIN", "APPR", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Bi\u00df seine Krafft vergieng. Es war die Weymar-Macht", "tokens": ["Bi\u00df", "sei\u00b7ne", "Krafft", "ver\u00b7gieng", ".", "Es", "war", "die", "Wey\u00b7ma\u00b7rMacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "und vieler F\u00fcrsten Hilff noch nicht bey-ein gebracht/", "tokens": ["und", "vie\u00b7ler", "F\u00fcrs\u00b7ten", "Hilff", "noch", "nicht", "bey\u00b7ein", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "NN", "ADV", "PTKNEG", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "Di\u00df gab den Feinden Lufft. Meyntz wolte sich der Feinde", "tokens": ["Di\u00df", "gab", "den", "Fein\u00b7den", "Lufft", ".", "Meyntz", "wol\u00b7te", "sich", "der", "Fein\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "NN", "$.", "NE", "VMFIN", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Befreyen/ hielt hierum mit Man\u00dffeld seinem Freinde", "tokens": ["Be\u00b7fre\u00b7yen", "/", "hielt", "hie\u00b7rum", "mit", "Man\u00df\u00b7feld", "sei\u00b7nem", "Frein\u00b7de"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "VVFIN", "PAV", "APPR", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.183": {"text": "Durch Briefe heimlich Raht/ der bald verrathen war/", "tokens": ["Durch", "Brie\u00b7fe", "heim\u00b7lich", "Raht", "/", "der", "bald", "ver\u00b7ra\u00b7then", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "NN", "$(", "ART", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Di\u00df brachte viel ans Schwerdt und bracht der Stadt Ge-", "tokens": ["Di\u00df", "brach\u00b7te", "viel", "ans", "Schwerdt", "und", "bracht", "der", "Stadt", "Ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "APPRART", "NN", "KON", "VVFIN", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.185": {"text": "Und nun must Augspurg auch an Beyern sich ergeben.", "tokens": ["Und", "nun", "must", "Augs\u00b7purg", "auch", "an", "Be\u00b7yern", "sich", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "NE", "ADV", "APPR", "NN", "PRF", "VVPP", "$."], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.186": {"text": "Es hatte lange Zeit gek\u00e4mpfft/ und kaum das Leben/", "tokens": ["Es", "hat\u00b7te", "lan\u00b7ge", "Zeit", "ge\u00b7k\u00e4mpfft", "/", "und", "kaum", "das", "Le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJA", "NN", "VVPP", "$(", "KON", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Der Hunger war so gro\u00df/ da\u00df man die Todten fra\u00df/", "tokens": ["Der", "Hun\u00b7ger", "war", "so", "gro\u00df", "/", "da\u00df", "man", "die", "Tod\u00b7ten", "fra\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$(", "KOUS", "PIS", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Es war kein gifftig Ratz versichert/ wo er sa\u00df/", "tokens": ["Es", "war", "kein", "giff\u00b7tig", "Ratz", "ver\u00b7si\u00b7chert", "/", "wo", "er", "sa\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "ADJD", "NN", "VVPP", "$(", "PWAV", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "Er muste nach dem Mund und Magen der Soldaten/", "tokens": ["Er", "mus\u00b7te", "nach", "dem", "Mund", "und", "Ma\u00b7gen", "der", "Sol\u00b7da\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ART", "NN", "KON", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "O GOtt! in welche Noth kan eine Stadt gerathen/", "tokens": ["O", "Gott", "!", "in", "wel\u00b7che", "Noth", "kan", "ei\u00b7ne", "Stadt", "ge\u00b7ra\u00b7then", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$.", "APPR", "PWAT", "NN", "VMFIN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.191": {"text": "Was hastu Deutsches Rom im Frieden sch\u00f6ne lust?", "tokens": ["Was", "has\u00b7tu", "Deut\u00b7sches", "Rom", "im", "Frie\u00b7den", "sch\u00f6\u00b7ne", "lust", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJA", "NE", "APPRART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Was leckerbissen sind in dur sonst nicht bewust?", "tokens": ["Was", "le\u00b7cker\u00b7bis\u00b7sen", "sind", "in", "dur", "sonst", "nicht", "be\u00b7wust", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVINF", "VAFIN", "APPR", "ADV", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "Und nun must' all dem Volck solch Ungel\u00fcck erleben.", "tokens": ["Und", "nun", "must'", "all", "dem", "Volck", "solch", "Un\u00b7ge\u00b7l\u00fcck", "er\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PIAT", "ART", "NN", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Hierauf say man auch ", "tokens": ["Hier\u00b7auf", "say", "man", "auch"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PIS", "ADV"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.195": {"text": "und zwar durch einen Zwist. Die Norkan folgte nach/", "tokens": ["und", "zwar", "durch", "ei\u00b7nen", "Zwist", ".", "Die", "Nor\u00b7kan", "folg\u00b7te", "nach", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "$.", "ART", "NN", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "und alles was bi\u00dfher in Francken Schwedisch sprach/", "tokens": ["und", "al\u00b7les", "was", "bi\u00df\u00b7her", "in", "Fran\u00b7cken", "Schwe\u00b7disch", "sprach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PWS", "ADV", "APPR", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Auch in der Ober-Pfaltz. Was man mit langem Kriege", "tokens": ["Auch", "in", "der", "O\u00b7ber\u00b7Pfaltz", ".", "Was", "man", "mit", "lan\u00b7gem", "Krie\u00b7ge"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "PWS", "PIS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.198": {"text": "Erworben hatt\u2019/ entgieng hiemit nach einem Siege/", "tokens": ["Er\u00b7wor\u00b7ben", "hatt'", "/", "ent\u00b7gieng", "hie\u00b7mit", "nach", "ei\u00b7nem", "Sie\u00b7ge", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$(", "VVFIN", "PAV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Der mit dem K\u00e4yser war. Di\u00df war noch nicht genug/", "tokens": ["Der", "mit", "dem", "K\u00e4y\u00b7ser", "war", ".", "Di\u00df", "war", "noch", "nicht", "ge\u00b7nug", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "VAFIN", "$.", "PDS", "VAFIN", "ADV", "PTKNEG", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Was schon geschehen war/ man nahm auch einen Zug", "tokens": ["Was", "schon", "ge\u00b7sche\u00b7hen", "war", "/", "man", "nahm", "auch", "ei\u00b7nen", "Zug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "VVPP", "VAFIN", "$(", "PIS", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.201": {"text": "Nach Hessen/ wie zu vor/ hieselbst noch mehr zu kriegen/", "tokens": ["Nach", "Hes\u00b7sen", "/", "wie", "zu", "vor", "/", "hie\u00b7selbst", "noch", "mehr", "zu", "krie\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KOKOM", "APPR", "APPR", "$(", "ADV", "ADV", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.202": {"text": "Der Landgraf aber war bi\u00dfher von vielem siegen", "tokens": ["Der", "Land\u00b7graf", "a\u00b7ber", "war", "bi\u00df\u00b7her", "von", "vie\u00b7lem", "sie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VAFIN", "ADV", "APPR", "PIS", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "So starck/ da\u00df jhm der Feind sehr wenig abgewann.", "tokens": ["So", "starck", "/", "da\u00df", "jhm", "der", "Feind", "sehr", "we\u00b7nig", "ab\u00b7ge\u00b7wann", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "KOUS", "PPER", "ART", "NN", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "Man dr\u00f6ute Glut und Schwerdt/ man bot\u2019 jhm Frieden an/", "tokens": ["Man", "dr\u00f6u\u00b7te", "Glut", "und", "Schwerdt", "/", "man", "bot'", "jhm", "Frie\u00b7den", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NN", "KON", "NN", "$(", "PIS", "VVFIN", "PPER", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "Er aber wolte sich von Schweden niemals trennen", "tokens": ["Er", "a\u00b7ber", "wol\u00b7te", "sich", "von", "Schwe\u00b7den", "nie\u00b7mals", "tren\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VMFIN", "PRF", "APPR", "NE", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Bi\u00df er den Frieden w\u00fcrd\u2019 in b\u00e4\u00dfrem Stande kennen/", "tokens": ["Bi\u00df", "er", "den", "Frie\u00b7den", "w\u00fcrd'", "in", "b\u00e4\u00df\u00b7rem", "Stan\u00b7de", "ken\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VAFIN", "APPR", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Als er jhn jetzo sah. In dem er Antwort gab", "tokens": ["Als", "er", "jhn", "jet\u00b7zo", "sah", ".", "In", "dem", "er", "Ant\u00b7wort", "gab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVFIN", "$.", "APPR", "PRELS", "PPER", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "That doch der Feind so viel und nahm ihm Rhenen ab/", "tokens": ["That", "doch", "der", "Feind", "so", "viel", "und", "nahm", "ihm", "Rhe\u00b7nen", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "ADV", "ADV", "KON", "VVFIN", "PPER", "NE", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Jedoch durch eine list. Wir wollen nun durch Hessen", "tokens": ["Je\u00b7doch", "durch", "ei\u00b7ne", "list", ".", "Wir", "wol\u00b7len", "nun", "durch", "Hes\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "PPER", "VMFIN", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "In Sachsen/ und der Macht von Weymar was vergessen/", "tokens": ["In", "Sach\u00b7sen", "/", "und", "der", "Macht", "von", "Wey\u00b7mar", "was", "ver\u00b7ges\u00b7sen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KON", "ART", "NN", "APPR", "NE", "PWS", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.211": {"text": "Bi\u00df sie vollkommen wird/ und neuen L\u00e4rmen macht.", "tokens": ["Bi\u00df", "sie", "voll\u00b7kom\u00b7men", "wird", "/", "und", "neu\u00b7en", "L\u00e4r\u00b7men", "macht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$(", "KON", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "Es hatte der Bannier den Winter kaum verbracht", "tokens": ["Es", "hat\u00b7te", "der", "Ban\u00b7nier", "den", "Win\u00b7ter", "kaum", "ver\u00b7bracht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "ART", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Gieng er vor Barby hin/ best\u00fcrmt es solcher ma\u00dfen", "tokens": ["Gieng", "er", "vor", "Bar\u00b7by", "hin", "/", "be\u00b7st\u00fcrmt", "es", "sol\u00b7cher", "ma\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$(", "VVFIN", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Da\u00df alles Volck darin sein Leben muste la\u00dfen/", "tokens": ["Da\u00df", "al\u00b7les", "Volck", "da\u00b7rin", "sein", "Le\u00b7ben", "mus\u00b7te", "la\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PAV", "PPOSAT", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "Es war das jene Volck/ da\u00df sich von jhm begab.", "tokens": ["Es", "war", "das", "je\u00b7ne", "Volck", "/", "da\u00df", "sich", "von", "jhm", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "PDAT", "NN", "$(", "KOUS", "PRF", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Auf dieses lief sein Heer das halbe Mei\u00dfen ab/", "tokens": ["Auf", "die\u00b7ses", "lief", "sein", "Heer", "das", "hal\u00b7be", "Mei\u00b7\u00dfen", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPOSAT", "NN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Und that gleich wie ein L\u00f6u/ der seinem Band' entkommen/", "tokens": ["Und", "that", "gleich", "wie", "ein", "L\u00f6u", "/", "der", "sei\u00b7nem", "Band'", "ent\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KOKOM", "ART", "NN", "$(", "ART", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "Bald wurde beyder Theyi jhr Zug nach Hall genommen.", "tokens": ["Bald", "wur\u00b7de", "bey\u00b7der", "They\u00b7i", "jhr", "Zug", "nach", "Hall", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "NE", "PPOSAT", "NN", "APPR", "NE", "VVPP", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.219": {"text": "Doch/ wie Cur-Sachsens Heer von Hatzfelds seiner Macht/", "tokens": ["Doch", "/", "wie", "Cur\u00b7Sach\u00b7sens", "Heer", "von", "Hatz\u00b7felds", "sei\u00b7ner", "Macht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "KOKOM", "NE", "NN", "APPR", "NE", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.220": {"text": "Aus Hessen Francken und Westfalen her gebracht/", "tokens": ["Aus", "Hes\u00b7sen", "Fran\u00b7cken", "und", "West\u00b7fa\u00b7len", "her", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "KON", "NN", "APZR", "VVPP", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.221": {"text": "Seyr gro\u00df verst\u00e4rcket wurd\u2019/ erhoben sich die Schweden", "tokens": ["Seyr", "gro\u00df", "ver\u00b7st\u00e4r\u00b7cket", "wurd'", "/", "er\u00b7ho\u00b7ben", "sich", "die", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAIMP", "ADJD", "VVPP", "VAFIN", "$(", "VVFIN", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "und nahmen jhren Weg/ um sehr gewisse Reden/", "tokens": ["und", "nah\u00b7men", "jhren", "Weg", "/", "um", "sehr", "ge\u00b7wis\u00b7se", "Re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "$(", "KOUI", "ADV", "ADJA", "NN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.223": {"text": "Nach Sta\u00dffurt. Bald hierauf wurd auch Wettin besucht/", "tokens": ["Nach", "Sta\u00df\u00b7furt", ".", "Bald", "hier\u00b7auf", "wurd", "auch", "Wet\u00b7tin", "be\u00b7sucht", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$.", "ADV", "PAV", "VAFIN", "ADV", "NN", "VVPP", "$("], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.224": {"text": "Wo sie des Tauben Volck geschlagen und zur Flucht", "tokens": ["Wo", "sie", "des", "Tau\u00b7ben", "Volck", "ge\u00b7schla\u00b7gen", "und", "zur", "Flucht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ART", "NN", "NN", "VVPP", "KON", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "Getrieben/ auch sehr viel davon gefangen haben.", "tokens": ["Ge\u00b7trie\u00b7ben", "/", "auch", "sehr", "viel", "da\u00b7von", "ge\u00b7fan\u00b7gen", "ha\u00b7ben", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "ADV", "ADV", "ADV", "PAV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "Was jhnen bey lettin des Bandis V\u00f6lcker gaben/", "tokens": ["Was", "jh\u00b7nen", "bey", "let\u00b7tin", "des", "Ban\u00b7dis", "V\u00f6l\u00b7cker", "ga\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NE", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.227": {"text": "War bey Wettin bezahlt. Von gleicher Macht zu seyn/", "tokens": ["War", "bey", "Wet\u00b7tin", "be\u00b7zahlt", ".", "Von", "glei\u00b7cher", "Macht", "zu", "seyn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "VVPP", "$.", "APPR", "ADJA", "NN", "PTKZU", "VAINF", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.228": {"text": "Als die Cur-S\u00e4chsische/ kam Wrangel vor den Schein", "tokens": ["Als", "die", "Cur\u00b7S\u00e4ch\u00b7si\u00b7sche", "/", "kam", "Wran\u00b7gel", "vor", "den", "Schein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "$(", "VVFIN", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "und le\u00dfle neben jhm mit 20 tausend Seelen.", "tokens": ["und", "le\u00df\u00b7le", "ne\u00b7ben", "jhm", "mit", "20", "tau\u00b7send", "See\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "number", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPER", "APPR", "CARD", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.230": {"text": "Hierzwischen kunte man ein drey\u00dfig tausend z\u00e4hlen/", "tokens": ["Hier\u00b7zwi\u00b7schen", "kun\u00b7te", "man", "ein", "drey\u00b7\u00dfig", "tau\u00b7send", "z\u00e4h\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "ART", "CARD", "CARD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Die dich/ ", "tokens": ["Die", "dich", "/"], "token_info": ["word", "word", "punct"], "pos": ["ART", "PPER", "$("], "meter": "-+", "measure": "iambic.single"}, "line.232": {"text": "umbgaben/ da man doch in dir der ersten Qual", "tokens": ["umb\u00b7ga\u00b7ben", "/", "da", "man", "doch", "in", "dir", "der", "ers\u00b7ten", "Qual"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$(", "KOUS", "PIS", "ADV", "APPR", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Noch unvergessen war. H\u00f6rt was f\u00fcr Volck. Da waren", "tokens": ["Noch", "un\u00b7ver\u00b7ges\u00b7sen", "war", ".", "H\u00f6rt", "was", "f\u00fcr", "Volck", ".", "Da", "wa\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "$.", "VVFIN", "PIS", "APPR", "NN", "$.", "ADV", "VAFIN"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.234": {"text": "Die K\u00e4yser-S\u00e4chsischen und Brandenburgsche Scharen/", "tokens": ["Die", "K\u00e4y\u00b7ser\u00b7S\u00e4ch\u00b7si\u00b7schen", "und", "Bran\u00b7den\u00b7burg\u00b7sche", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Mit Weymar-Wilhelms Volck und l\u00fcneburger Macht", "tokens": ["Mit", "Wey\u00b7ma\u00b7rWil\u00b7helms", "Volck", "und", "l\u00fc\u00b7ne\u00b7bur\u00b7ger", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "Zusam in ein\u2019 Armee/ wie schon gedacht/ gebracht/", "tokens": ["Zu\u00b7sam", "in", "ein'", "Ar\u00b7mee", "/", "wie", "schon", "ge\u00b7dacht", "/", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$(", "KOKOM", "ADV", "VVPP", "$(", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.237": {"text": "Die alle schlo\u00dfen dich in jhre strenge Waffen/", "tokens": ["Die", "al\u00b7le", "schlo\u00b7\u00dfen", "dich", "in", "jhre", "stren\u00b7ge", "Waf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PRF", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.238": {"text": "Und mustestu dein Thor f\u00fcr jhnen offen schaffen.", "tokens": ["Und", "mus\u00b7te\u00b7stu", "dein", "Thor", "f\u00fcr", "jh\u00b7nen", "of\u00b7fen", "schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPOSAT", "NN", "APPR", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "Man schickte zwar Entsatz/ doch es kam keiner ein/", "tokens": ["Man", "schick\u00b7te", "zwar", "Ent\u00b7satz", "/", "doch", "es", "kam", "kei\u00b7ner", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "NN", "$(", "KON", "PPER", "VVFIN", "PIS", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "und must\u2019 ein gutes Theil hiervon des Todes seyn.", "tokens": ["und", "must'", "ein", "gu\u00b7tes", "Theil", "hier\u00b7von", "des", "To\u00b7des", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "ADJA", "NN", "PAV", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Rach dem der Schweden Feind das Magdaburg besiegte/", "tokens": ["Rach", "dem", "der", "Schwe\u00b7den", "Feind", "das", "Mag\u00b7da\u00b7burg", "be\u00b7sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "Geschachs/ da\u00df Le\u00dfle hier das L\u00fcneburg bekriegte", "tokens": ["Ge\u00b7schachs", "/", "da\u00df", "Le\u00df\u00b7le", "hier", "das", "L\u00fc\u00b7ne\u00b7burg", "be\u00b7krieg\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "KOUS", "NE", "ADV", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "und neben Win\u00dfheim nahm. Weil dieses F\u00fcrsten Heer", "tokens": ["und", "ne\u00b7ben", "Win\u00df\u00b7heim", "nahm", ".", "Weil", "die\u00b7ses", "F\u00fcrs\u00b7ten", "Heer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "VVFIN", "$.", "KOUS", "PDAT", "NN", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.244": {"text": "Den Schweden widrig war/ und nun/ wie vor/ nicht mehr", "tokens": ["Den", "Schwe\u00b7den", "wid\u00b7rig", "war", "/", "und", "nun", "/", "wie", "vor", "/", "nicht", "mehr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["ART", "NE", "ADJD", "VAFIN", "$(", "KON", "ADV", "$(", "KOKOM", "APPR", "$(", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.245": {"text": "Bey jhnen und bey dem von Hessen wolte stehen.", "tokens": ["Bey", "jh\u00b7nen", "und", "bey", "dem", "von", "Hes\u00b7sen", "wol\u00b7te", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "KON", "APPR", "ART", "APPR", "NE", "VMFIN", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.246": {"text": "Es war nun \u00fcberall/ wohin man mochte sehen/", "tokens": ["Es", "war", "nun", "\u00fc\u00b7be\u00b7rall", "/", "wo\u00b7hin", "man", "moch\u00b7te", "se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$(", "PWAV", "PIS", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "Bel\u00e4gern/ Einnahm/ Pest/ Raub/ Brand und Hungers", "tokens": ["Be\u00b7l\u00e4\u00b7gern", "/", "Ein\u00b7nahm", "/", "Pest", "/", "Raub", "/", "Brand", "und", "Hun\u00b7gers"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$(", "ART", "$(", "NE", "$(", "NN", "$(", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.248": {"text": "Noth/", "tokens": ["Noth", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+", "measure": "single.up"}, "line.249": {"text": "und blieb bald hier/ bald dort ein tapfer Tausend todt.", "tokens": ["und", "blieb", "bald", "hier", "/", "bald", "dort", "ein", "tap\u00b7fer", "Tau\u00b7send", "todt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "$(", "ADV", "ADV", "ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Bannier vermerckende/ da\u00df er mit schlechten Dingen", "tokens": ["Ban\u00b7nier", "ver\u00b7mer\u00b7cken\u00b7de", "/", "da\u00df", "er", "mit", "schlech\u00b7ten", "Din\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "$(", "KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.251": {"text": "Sich und sein Volck nicht wol zu rechte w\u00fcrde bringen/", "tokens": ["Sich", "und", "sein", "Volck", "nicht", "wol", "zu", "rech\u00b7te", "w\u00fcr\u00b7de", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "KON", "PPOSAT", "NN", "PTKNEG", "ADV", "APPR", "ADJA", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Bedachte sich zur Schlacht/ ermahnte seine Schar", "tokens": ["Be\u00b7dach\u00b7te", "sich", "zur", "Schlacht", "/", "er\u00b7mahn\u00b7te", "sei\u00b7ne", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PRF", "APPRART", "NN", "$(", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "Zum fechten/ und trieb weg/ was schlecht von Hertzen war.", "tokens": ["Zum", "fech\u00b7ten", "/", "und", "trieb", "weg", "/", "was", "schlecht", "von", "Hert\u00b7zen", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "VVINF", "$(", "KON", "VVFIN", "PTKVZ", "$(", "PWS", "VVFIN", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Es gieng nach seinem Sinn/ und musten ", "tokens": ["Es", "gieng", "nach", "sei\u00b7nem", "Sinn", "/", "und", "mus\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$(", "KON", "VMFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.255": {"text": "Ein viermal hundert Mann/ die auf dem Wege waren/", "tokens": ["Ein", "vier\u00b7mal", "hun\u00b7dert", "Mann", "/", "die", "auf", "dem", "We\u00b7ge", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "CARD", "NN", "$(", "ART", "APPR", "ART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Zur gro\u00dfen Macht zu gehn/ die ersten Todten seyn.", "tokens": ["Zur", "gro\u00b7\u00dfen", "Macht", "zu", "gehn", "/", "die", "ers\u00b7ten", "Tod\u00b7ten", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "PTKZU", "VVINF", "$(", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "Gleich wie der Donn der sich vom Anfang sacht und klein", "tokens": ["Gleich", "wie", "der", "Donn", "der", "sich", "vom", "An\u00b7fang", "sacht", "und", "klein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "NN", "ART", "PRF", "APPRART", "NN", "VVFIN", "KON", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Bezeiget/ endlich doch mit gro\u00dfem Knall und Krachen", "tokens": ["Be\u00b7zei\u00b7get", "/", "end\u00b7lich", "doch", "mit", "gro\u00b7\u00dfem", "Knall", "und", "Kra\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "ADV", "ADV", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.259": {"text": "Heraus bricht/ und die Welt vol Schrecken pflegt zu machen:", "tokens": ["He\u00b7raus", "bricht", "/", "und", "die", "Welt", "vol", "Schre\u00b7cken", "pflegt", "zu", "ma\u00b7chen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "KON", "ART", "NN", "ADJD", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "Also geschah auch hier. Es traffen beyde Theil/", "tokens": ["Al\u00b7so", "ge\u00b7schah", "auch", "hier", ".", "Es", "traf\u00b7fen", "bey\u00b7de", "Theil", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "$.", "PPER", "VVFIN", "PIAT", "NN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.261": {"text": "Ob schon von gro\u00dfer Macht/ je dach sehr lange weil", "tokens": ["Ob", "schon", "von", "gro\u00b7\u00dfer", "Macht", "/", "je", "dach", "sehr", "lan\u00b7ge", "weil"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "APPR", "ADJA", "NN", "$(", "ADV", "PAV", "ADV", "ADV", "KOUS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Nur schlecht scharm\u00fctzelnde zusammen/ bi\u00df die Sachsen", "tokens": ["Nur", "schlecht", "schar\u00b7m\u00fct\u00b7zeln\u00b7de", "zu\u00b7sam\u00b7men", "/", "bi\u00df", "die", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "PTKVZ", "$(", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "Mit samt den K\u00e4ysrischen den Schweden ba\u00df gewachsen", "tokens": ["Mit", "samt", "den", "K\u00e4y\u00b7sri\u00b7schen", "den", "Schwe\u00b7den", "ba\u00df", "ge\u00b7wach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "NN", "ART", "NE", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Zu seyn/ des Klizings Heer/ das man im nahen sah/", "tokens": ["Zu", "seyn", "/", "des", "Kli\u00b7zings", "Heer", "/", "das", "man", "im", "na\u00b7hen", "sah", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "$(", "ART", "NN", "NN", "$(", "PRELS", "PIS", "APPRART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "Von dar an sich zu ziehn/ verreisten. Sihe da", "tokens": ["Von", "dar", "an", "sich", "zu", "ziehn", "/", "ver\u00b7reis\u00b7ten", ".", "Si\u00b7he", "da"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["APPR", "PTKVZ", "APPR", "PRF", "PTKZU", "VVINF", "$(", "VVFIN", "$.", "VVIMP", "ADV"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.266": {"text": "Erhob sich der Bannier und gieng recht nach der Mitten/", "tokens": ["Er\u00b7hob", "sich", "der", "Ban\u00b7nier", "und", "gieng", "recht", "nach", "der", "Mit\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "KON", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.267": {"text": "Womit er jhnen Weg und Steg hatt\u2019 abgeschnitten/", "tokens": ["Wo\u00b7mit", "er", "jh\u00b7nen", "Weg", "und", "Steg", "hatt'", "ab\u00b7ge\u00b7schnit\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "NN", "KON", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Damit ja keiner nicht dem andern helffen kunt.", "tokens": ["Da\u00b7mit", "ja", "kei\u00b7ner", "nicht", "dem", "an\u00b7dern", "helf\u00b7fen", "kunt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "PIS", "PTKNEG", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.269": {"text": "Und als die gr\u00f6ste Macht nun bald bey Wittstock stundt'/", "tokens": ["Und", "als", "die", "gr\u00f6s\u00b7te", "Macht", "nun", "bald", "bey", "Witt\u00b7stock", "stundt'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "ADV", "ADV", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Ein Heer von mehr zu Fu\u00df\u2019 als sechzehn tausend Streitern/", "tokens": ["Ein", "Heer", "von", "mehr", "zu", "Fu\u00df'", "als", "sech\u00b7zehn", "tau\u00b7send", "Strei\u00b7tern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADV", "APPR", "NN", "KOKOM", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Mit einer sch\u00f6nen Schaar von vierzehn tausend Reitern/", "tokens": ["Mit", "ei\u00b7ner", "sch\u00f6\u00b7nen", "Schaar", "von", "vier\u00b7zehn", "tau\u00b7send", "Rei\u00b7tern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Vom gro\u00dfen Sachsen selbst/ wie auch vom ", "tokens": ["Vom", "gro\u00b7\u00dfen", "Sach\u00b7sen", "selbst", "/", "wie", "auch", "vom"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "ADV", "$(", "KOKOM", "ADV", "APPRART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.273": {"text": "und Hatzfeld angef\u00fchrt/ erhob er sich dahin", "tokens": ["und", "Hatz\u00b7feld", "an\u00b7ge\u00b7f\u00fchrt", "/", "er\u00b7hob", "er", "sich", "da\u00b7hin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVPP", "$(", "VVFIN", "PPER", "PRF", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "und gieng jhr unverhofft ", "tokens": ["und", "gieng", "jhr", "un\u00b7ver\u00b7hofft"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.275": {"text": "und zwantzig tausend Mann in ihren starcken R\u00fccken.", "tokens": ["und", "zwant\u00b7zig", "tau\u00b7send", "Mann", "in", "ih\u00b7ren", "star\u00b7cken", "R\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "CARD", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "That eine Schlacht auf sie/ und zwar mit solcher Macht/", "tokens": ["That", "ei\u00b7ne", "Schlacht", "auf", "sie", "/", "und", "zwar", "mit", "sol\u00b7cher", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "PPER", "$(", "KON", "ADV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Da\u00df er 6000. Mann und mehr hat umgebracht.", "tokens": ["Da\u00df", "er", "6000.", "Mann", "und", "mehr", "hat", "um\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "ordinal", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJA", "NN", "KON", "ADV", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.278": {"text": "Er kriegte f\u00fcnfftzig St\u00fcck und etlich tausend Wagen/", "tokens": ["Er", "krieg\u00b7te", "f\u00fcnf\u00b7ft\u00b7zig", "St\u00fcck", "und", "et\u00b7lich", "tau\u00b7send", "Wa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "CARD", "NN", "KON", "ADJD", "CARD", "NN", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.279": {"text": "Die gantze Tonnen Golds jhm haben eingetragen.", "tokens": ["Die", "gant\u00b7ze", "Ton\u00b7nen", "Golds", "jhm", "ha\u00b7ben", "ein\u00b7ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "Ein jeder seines Volcks bekam so viel davon", "tokens": ["Ein", "je\u00b7der", "sei\u00b7nes", "Volcks", "be\u00b7kam", "so", "viel", "da\u00b7von"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "PPOSAT", "NN", "VVFIN", "ADV", "ADV", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Da\u00df er sich freuen kunt. Es ist des schlagens Lohn", "tokens": ["Da\u00df", "er", "sich", "freu\u00b7en", "kunt", ".", "Es", "ist", "des", "schla\u00b7gens", "Lohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "PTKVZ", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Beuth oder Traurigkeit/ versteh in solchem Kriegen.", "tokens": ["Beuth", "o\u00b7der", "Trau\u00b7rig\u00b7keit", "/", "ver\u00b7steh", "in", "sol\u00b7chem", "Krie\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$(", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.283": {"text": "Noch waren \u00fcber di\u00df zur Beuth\u2019 auf dieses siegen", "tokens": ["Noch", "wa\u00b7ren", "\u00fc\u00b7ber", "di\u00df", "zur", "Beuth'", "auf", "die\u00b7ses", "sie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPR", "PDS", "APPRART", "NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Von Fahnen zw\u00f6lffmal zw\u00f6lff und noch vielmehr. Dabey", "tokens": ["Von", "Fah\u00b7nen", "zw\u00f6lff\u00b7mal", "zw\u00f6lff", "und", "noch", "viel\u00b7mehr", ".", "Da\u00b7bey"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "NN", "ADV", "CARD", "KON", "ADV", "ADV", "$.", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "F\u00fcnff Oberste/ und dann noch eine lange Rey", "tokens": ["F\u00fcnff", "O\u00b7bers\u00b7te", "/", "und", "dann", "noch", "ei\u00b7ne", "lan\u00b7ge", "Rey"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "$(", "KON", "ADV", "ADV", "ART", "ADJA", "NN"], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.286": {"text": "Von denen/ die dem Mars von Venus sind gegeben/", "tokens": ["Von", "de\u00b7nen", "/", "die", "dem", "Mars", "von", "Ve\u00b7nus", "sind", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "$(", "ART", "ART", "NN", "APPR", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.287": {"text": "Nach lang-gehabter Last auch einst in Lust zu leben/", "tokens": ["Nach", "lang\u00b7ge\u00b7hab\u00b7ter", "Last", "auch", "einst", "in", "Lust", "zu", "le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "ADV", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.288": {"text": "Di\u00df alles blieb im Stich. Hierauf so st\u00e4llte man", "tokens": ["Di\u00df", "al\u00b7les", "blieb", "im", "Stich", ".", "Hier\u00b7auf", "so", "st\u00e4ll\u00b7te", "man"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "VVFIN", "APPRART", "NN", "$.", "PAV", "ADV", "VVFIN", "PIS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.289": {"text": "Zu Wittstock vor den Sieg ein gro\u00dfes Danckfest an/", "tokens": ["Zu", "Witt\u00b7stock", "vor", "den", "Sieg", "ein", "gro\u00b7\u00dfes", "Dan\u00b7ck\u00b7fest", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.290": {"text": "und scho\u00df dreymal Triumpff aus des besiegten St\u00fccken/", "tokens": ["und", "scho\u00df", "drey\u00b7mal", "Tri\u00b7umpff", "aus", "des", "be\u00b7sieg\u00b7ten", "St\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Die man nicht lang hierauf nach Schweden sahe schicken", "tokens": ["Die", "man", "nicht", "lang", "hier\u00b7auf", "nach", "Schwe\u00b7den", "sa\u00b7he", "schi\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "PTKNEG", "ADJD", "PAV", "APPR", "NE", "VVFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Der Nordischen G\u00f6ttin-Christinen ins Gesicht", "tokens": ["Der", "Nor\u00b7di\u00b7schen", "G\u00f6t\u00b7tin\u00b7Chris\u00b7ti\u00b7nen", "ins", "Ge\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.293": {"text": "Zu st\u00e4llen/ was jhr Heer nunmehr hatt\u2019 au\u00dfgericht/", "tokens": ["Zu", "st\u00e4l\u00b7len", "/", "was", "jhr", "Heer", "nun\u00b7mehr", "hatt'", "au\u00df\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPOSAT", "NN", "ADV", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Jedoch durch GOttes Hand. Es ist vorwahr zu sagen:", "tokens": ["Je\u00b7doch", "durch", "Got\u00b7tes", "Hand", ".", "Es", "ist", "vor\u00b7wahr", "zu", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "NN", "$.", "PPER", "VAFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Da\u00df sich der Gegner hab im schlagen so getragen", "tokens": ["Da\u00df", "sich", "der", "Geg\u00b7ner", "hab", "im", "schla\u00b7gen", "so", "ge\u00b7tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "ART", "NN", "VAFIN", "APPRART", "ADJA", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "Als Helden zugeh\u00f6rt. Es lag sein Volck also/", "tokens": ["Als", "Hel\u00b7den", "zu\u00b7ge\u00b7h\u00f6rt", ".", "Es", "lag", "sein", "Volck", "al\u00b7so", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVPP", "$.", "PPER", "VVFIN", "PPOSAT", "NN", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "(das keine Flucht bezeigt) als abgemeyhtes Stroh.", "tokens": ["(", "das", "kei\u00b7ne", "Flucht", "be\u00b7zeigt", ")", "als", "ab\u00b7ge\u00b7meyh\u00b7tes", "Stroh", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ART", "PIAT", "NN", "VVPP", "$(", "KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Wann aber sich das Gl\u00fcck bey einem widrig st\u00e4llet/", "tokens": ["Wann", "a\u00b7ber", "sich", "das", "Gl\u00fcck", "bey", "ei\u00b7nem", "wid\u00b7rig", "st\u00e4l\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PRF", "ART", "NN", "APPR", "ART", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.299": {"text": "So hilfft es nicht/ wie sehr da\u00df man sich rund umw\u00e4llet", "tokens": ["So", "hilfft", "es", "nicht", "/", "wie", "sehr", "da\u00df", "man", "sich", "rund", "um\u00b7w\u00e4l\u00b7let"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "$(", "KOKOM", "ADV", "KOUS", "PIS", "PRF", "ADJD", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "und voll von Wei\u00dfheit weis. Durch diese gro\u00dfe Schlacht", "tokens": ["und", "voll", "von", "Wei\u00df\u00b7heit", "weis", ".", "Durch", "die\u00b7se", "gro\u00b7\u00dfe", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "NN", "PTKVZ", "$.", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Hat sich Bannier noch mehr als je bef\u00f6rcht gemacht/", "tokens": ["Hat", "sich", "Ban\u00b7nier", "noch", "mehr", "als", "je", "be\u00b7f\u00f6rcht", "ge\u00b7macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "NE", "ADV", "PIAT", "KOKOM", "ADV", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.302": {"text": "und seiner Krohnen Ehr\u2019/ jhm selbst zum Ruhm/ vermehret.", "tokens": ["und", "sei\u00b7ner", "Kroh\u00b7nen", "Ehr'", "/", "jhm", "selbst", "zum", "Ruhm", "/", "ver\u00b7meh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "$(", "PPER", "ADV", "APPRART", "NN", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.303": {"text": "Da kurtz vor dieser Schlacht kein anders war geh\u00f6ret/", "tokens": ["Da", "kurtz", "vor", "die\u00b7ser", "Schlacht", "kein", "an\u00b7ders", "war", "ge\u00b7h\u00f6\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "PDAT", "NN", "PIAT", "ADV", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Als da\u00df die Schweden sich bi\u00df nach der Balther-See", "tokens": ["Als", "da\u00df", "die", "Schwe\u00b7den", "sich", "bi\u00df", "nach", "der", "Balt\u00b7her\u00b7See"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "ART", "NE", "PRF", "ADV", "APPR", "ART", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.305": {"text": "und so anheim gewandt/ da kamen sie zur H\u00f6h", "tokens": ["und", "so", "an\u00b7heim", "ge\u00b7wandt", "/", "da", "ka\u00b7men", "sie", "zur", "H\u00f6h"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "VVPP", "$(", "ADV", "VVFIN", "PPER", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "und triumfireten durch jhrer Feinde lande.", "tokens": ["und", "tri\u00b7um\u00b7fi\u00b7re\u00b7ten", "durch", "jhrer", "Fein\u00b7de", "lan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.307": {"text": "So bald bringt eine Schlacht ein Werck zu andrem Stande.", "tokens": ["So", "bald", "bringt", "ei\u00b7ne", "Schlacht", "ein", "Werck", "zu", "an\u00b7drem", "Stan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Sie ruhen nach der Schlacht ein wenig aus/ und ich", "tokens": ["Sie", "ru\u00b7hen", "nach", "der", "Schlacht", "ein", "we\u00b7nig", "aus", "/", "und", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ART", "PIS", "PTKVZ", "$(", "KON", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Nehm auch auf diese Schrifft ein wenig Ruh vor mich.", "tokens": ["Nehm", "auch", "auf", "die\u00b7se", "Schrifft", "ein", "we\u00b7nig", "Ruh", "vor", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PDAT", "NN", "ART", "PIAT", "NN", "APPR", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}