{"textgrid.poem.44380": {"metadata": {"author": {"name": "Grillparzer, Franz", "birth": "N.A.", "death": "N.A."}, "title": "Der feindliche Bruder, Seitenst\u00fcck zu Schlegels feindlichen Br\u00fcdern in der Aglaja f\u00fcr 1821", "genre": "verse", "period": "N.A.", "pub_year": 1820, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dem Schludrian und Schlendrian,", "tokens": ["Dem", "Schlu\u00b7dri\u00b7an", "und", "Schlen\u00b7dri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Samt ihrem Vater lobesan,", "tokens": ["Samt", "ih\u00b7rem", "Va\u00b7ter", "lo\u00b7be\u00b7san", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist noch ein Bruder: Schundrian,", "tokens": ["Ist", "noch", "ein", "Bru\u00b7der", ":", "Schund\u00b7ri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$.", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sonst auch genannt: der Wetterhahn.", "tokens": ["Sonst", "auch", "ge\u00b7nannt", ":", "der", "Wet\u00b7ter\u00b7hahn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Und wie der eine ", "tokens": ["Und", "wie", "der", "ei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Der andre ", "tokens": ["Der", "and\u00b7re"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "So geht der dritte ", "tokens": ["So", "geht", "der", "drit\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Das hei\u00dft, sowie dem Herren deucht,", "tokens": ["Das", "hei\u00dft", ",", "so\u00b7wie", "dem", "Her\u00b7ren", "deucht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie aus ihrem Wege weicht,", "tokens": ["Da\u00df", "sie", "aus", "ih\u00b7rem", "We\u00b7ge", "weicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So springt er vor, gewandt und leicht,", "tokens": ["So", "springt", "er", "vor", ",", "ge\u00b7wandt", "und", "leicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "VVPP", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und hat das Ziel vor ihr erreicht.", "tokens": ["Und", "hat", "das", "Ziel", "vor", "ihr", "er\u00b7reicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und, richtend sich den Hahnenkamm,", "tokens": ["Und", ",", "rich\u00b7tend", "sich", "den", "Hah\u00b7nen\u00b7kamm", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ruft er: Seid mir gegr\u00fc\u00dft, Madam!", "tokens": ["Ruft", "er", ":", "Seid", "mir", "ge\u00b7gr\u00fc\u00dft", ",", "Ma\u00b7dam", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "$.", "VAIMP", "PPER", "VVPP", "$,", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Schon l\u00e4ngst vor euch hierher ich kam,", "tokens": ["Schon", "l\u00e4ngst", "vor", "euch", "hier\u00b7her", "ich", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "PPER", "PAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wohl dem, der mich zum F\u00fchrer nahm!", "tokens": ["Wohl", "dem", ",", "der", "mich", "zum", "F\u00fch\u00b7rer", "nahm", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "$,", "PRELS", "PRF", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Und was nun in der Zeit rumort,", "tokens": ["Und", "was", "nun", "in", "der", "Zeit", "ru\u00b7mort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Daf\u00fcr hat er sogleich ein Wort:", "tokens": ["Da\u00b7f\u00fcr", "hat", "er", "sog\u00b7leich", "ein", "Wort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Romantisch, absolut, naiv,", "tokens": ["Ro\u00b7man\u00b7tisch", ",", "ab\u00b7so\u00b7lut", ",", "naiv", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Antik, lebendig-positiv,", "tokens": ["An\u00b7tik", ",", "le\u00b7ben\u00b7dig\u00b7po\u00b7si\u00b7tiv", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Was l\u00e4ngst schon da war still und tief,", "tokens": ["Was", "l\u00e4ngst", "schon", "da", "war", "still", "und", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "ADV", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hei\u00dft sein, weil ers beim Namen rief.", "tokens": ["Hei\u00dft", "sein", ",", "weil", "ers", "beim", "Na\u00b7men", "rief", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "VAINF", "$,", "KOUS", "PIS", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und so von Wort zu Wort herum", "tokens": ["Und", "so", "von", "Wort", "zu", "Wort", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "NN", "APPR", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geht er mit seinem S\u00e4kulum,", "tokens": ["Geht", "er", "mit", "sei\u00b7nem", "S\u00e4\u00b7ku\u00b7lum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist griechisch, indisch, kreuz und krumm,", "tokens": ["Ist", "grie\u00b7chisch", ",", "in\u00b7disch", ",", "kreuz", "und", "krumm", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "ADJD", "$,", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dann wieder spanisch, Hand kehr um.", "tokens": ["Dann", "wie\u00b7der", "spa\u00b7nisch", ",", "Hand", "kehr", "um", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$,", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "W\u00e4lzt sich wohl auch im Kote gern,", "tokens": ["W\u00e4lzt", "sich", "wohl", "auch", "im", "Ko\u00b7te", "gern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Tr\u00e4gt mal der Sund die Schand-Luzern,", "tokens": ["Tr\u00e4gt", "mal", "der", "Sund", "die", "Schan\u00b7dLu\u00b7zern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "H\u00e4lt Skrupel sich und Zweifel fern,", "tokens": ["H\u00e4lt", "Skru\u00b7pel", "sich", "und", "Zwei\u00b7fel", "fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PRF", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Findt im Genu\u00df des Lebens Kern.", "tokens": ["Findt", "im", "Ge\u00b7nu\u00df", "des", "Le\u00b7bens", "Kern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "NN", "ART", "NN", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.8": {"line.1": {"text": "Doch alles das nicht so gemein,", "tokens": ["Doch", "al\u00b7les", "das", "nicht", "so", "ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PDS", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Erst idealisiert ers fein", "tokens": ["Erst", "i\u00b7dea\u00b7li\u00b7siert", "ers", "fein"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und gibt die Quintessenz allein,", "tokens": ["Und", "gibt", "die", "Quin\u00b7tes\u00b7senz", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das Sublimat, den Lesern ein.", "tokens": ["Das", "Sub\u00b7li\u00b7mat", ",", "den", "Le\u00b7sern", "ein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Ist nun die ganze Welt verpufft", "tokens": ["Ist", "nun", "die", "gan\u00b7ze", "Welt", "ver\u00b7pufft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und leer und hohl, wie leere Luft,", "tokens": ["Und", "leer", "und", "hohl", ",", "wie", "lee\u00b7re", "Luft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "KON", "ADJD", "$,", "PWAV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "H\u00e4lt auch das Letzte l\u00e4nger nicht,", "tokens": ["H\u00e4lt", "auch", "das", "Letz\u00b7te", "l\u00e4n\u00b7ger", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "ADJD", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zerbr\u00f6ckelt, wo mans fa\u00dft und bricht,", "tokens": ["Zer\u00b7br\u00f6\u00b7ckelt", ",", "wo", "mans", "fa\u00dft", "und", "bricht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PWAV", "PIS", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dann mi\u00dftraut er dem eignen Licht,", "tokens": ["Dann", "mi\u00df\u00b7traut", "er", "dem", "eig\u00b7nen", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "Wie alte Hur zum Betstuhl kriecht.", "tokens": ["Wie", "al\u00b7te", "Hur", "zum", "Bet\u00b7stuhl", "kriecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Ihm, der nur Ideales trug,", "tokens": ["Ihm", ",", "der", "nur", "I\u00b7dea\u00b7les", "trug", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "ADV", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist nun nichts ", "tokens": ["Ist", "nun", "nichts"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "ADV", "PIS"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Und: \u00bbReligion und Porterkrug\u00ab", "tokens": ["Und", ":", "\u00bb", "Re\u00b7li\u00b7gi\u00b7on", "und", "Por\u00b7ter\u00b7krug", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$.", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ist von nun an sein Weidmannsspruch.", "tokens": ["Ist", "von", "nun", "an", "sein", "Weid\u00b7manns\u00b7spruch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "So duckt und b\u00fc\u00dft der Jammermann", "tokens": ["So", "duckt", "und", "b\u00fc\u00dft", "der", "Jam\u00b7mer\u00b7mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und feindet jeden andern an,", "tokens": ["Und", "fein\u00b7det", "je\u00b7den", "an\u00b7dern", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der, so wie er, nicht ", "tokens": ["Der", ",", "so", "wie", "er", ",", "nicht"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ART", "$,", "ADV", "KOKOM", "PPER", "$,", "PTKNEG"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Weil er nicht das, was er, ", "tokens": ["Weil", "er", "nicht", "das", ",", "was", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "PDS", "$,", "PWS", "PPER", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.12": {"line.1": {"text": "Verl\u00e4stert alles rings herum,", "tokens": ["Ver\u00b7l\u00e4s\u00b7tert", "al\u00b7les", "rings", "he\u00b7rum", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schreit \u00fcber H\u00f6ll und Heidentum,", "tokens": ["Schreit", "\u00fc\u00b7ber", "H\u00f6ll", "und", "Hei\u00b7den\u00b7tum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und m\u00f6chte Kraft und Licht verschw\u00e4rzen,", "tokens": ["Und", "m\u00f6ch\u00b7te", "Kraft", "und", "Licht", "ver\u00b7schw\u00e4r\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil sie erl\u00f6scht in seinem Herzen.", "tokens": ["Weil", "sie", "er\u00b7l\u00f6scht", "in", "sei\u00b7nem", "Her\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Das ist die M\u00e4r vom Schundrian,", "tokens": ["Das", "ist", "die", "M\u00e4r", "vom", "Schund\u00b7ri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPRART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem dritten Bruder lobesan", "tokens": ["Dem", "drit\u00b7ten", "Bru\u00b7der", "lo\u00b7be\u00b7san"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Des Schlendrian und Schludrian,", "tokens": ["Des", "Schlen\u00b7dri\u00b7an", "und", "Schlu\u00b7dri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Gemein genannt: der Wetterhahn.", "tokens": ["Ge\u00b7mein", "ge\u00b7nannt", ":", "der", "Wet\u00b7ter\u00b7hahn", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Dem Schludrian und Schlendrian,", "tokens": ["Dem", "Schlu\u00b7dri\u00b7an", "und", "Schlen\u00b7dri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Samt ihrem Vater lobesan,", "tokens": ["Samt", "ih\u00b7rem", "Va\u00b7ter", "lo\u00b7be\u00b7san", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist noch ein Bruder: Schundrian,", "tokens": ["Ist", "noch", "ein", "Bru\u00b7der", ":", "Schund\u00b7ri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$.", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sonst auch genannt: der Wetterhahn.", "tokens": ["Sonst", "auch", "ge\u00b7nannt", ":", "der", "Wet\u00b7ter\u00b7hahn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Und wie der eine ", "tokens": ["Und", "wie", "der", "ei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Der andre ", "tokens": ["Der", "and\u00b7re"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "So geht der dritte ", "tokens": ["So", "geht", "der", "drit\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.16": {"line.1": {"text": "Das hei\u00dft, sowie dem Herren deucht,", "tokens": ["Das", "hei\u00dft", ",", "so\u00b7wie", "dem", "Her\u00b7ren", "deucht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie aus ihrem Wege weicht,", "tokens": ["Da\u00df", "sie", "aus", "ih\u00b7rem", "We\u00b7ge", "weicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So springt er vor, gewandt und leicht,", "tokens": ["So", "springt", "er", "vor", ",", "ge\u00b7wandt", "und", "leicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "VVPP", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und hat das Ziel vor ihr erreicht.", "tokens": ["Und", "hat", "das", "Ziel", "vor", "ihr", "er\u00b7reicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Und, richtend sich den Hahnenkamm,", "tokens": ["Und", ",", "rich\u00b7tend", "sich", "den", "Hah\u00b7nen\u00b7kamm", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ruft er: Seid mir gegr\u00fc\u00dft, Madam!", "tokens": ["Ruft", "er", ":", "Seid", "mir", "ge\u00b7gr\u00fc\u00dft", ",", "Ma\u00b7dam", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "$.", "VAIMP", "PPER", "VVPP", "$,", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Schon l\u00e4ngst vor euch hierher ich kam,", "tokens": ["Schon", "l\u00e4ngst", "vor", "euch", "hier\u00b7her", "ich", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "PPER", "PAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wohl dem, der mich zum F\u00fchrer nahm!", "tokens": ["Wohl", "dem", ",", "der", "mich", "zum", "F\u00fch\u00b7rer", "nahm", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "$,", "PRELS", "PRF", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Und was nun in der Zeit rumort,", "tokens": ["Und", "was", "nun", "in", "der", "Zeit", "ru\u00b7mort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Daf\u00fcr hat er sogleich ein Wort:", "tokens": ["Da\u00b7f\u00fcr", "hat", "er", "sog\u00b7leich", "ein", "Wort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Romantisch, absolut, naiv,", "tokens": ["Ro\u00b7man\u00b7tisch", ",", "ab\u00b7so\u00b7lut", ",", "naiv", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Antik, lebendig-positiv,", "tokens": ["An\u00b7tik", ",", "le\u00b7ben\u00b7dig\u00b7po\u00b7si\u00b7tiv", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Was l\u00e4ngst schon da war still und tief,", "tokens": ["Was", "l\u00e4ngst", "schon", "da", "war", "still", "und", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "ADV", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hei\u00dft sein, weil ers beim Namen rief.", "tokens": ["Hei\u00dft", "sein", ",", "weil", "ers", "beim", "Na\u00b7men", "rief", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "VAINF", "$,", "KOUS", "PIS", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Und so von Wort zu Wort herum", "tokens": ["Und", "so", "von", "Wort", "zu", "Wort", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "NN", "APPR", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geht er mit seinem S\u00e4kulum,", "tokens": ["Geht", "er", "mit", "sei\u00b7nem", "S\u00e4\u00b7ku\u00b7lum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist griechisch, indisch, kreuz und krumm,", "tokens": ["Ist", "grie\u00b7chisch", ",", "in\u00b7disch", ",", "kreuz", "und", "krumm", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "ADJD", "$,", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dann wieder spanisch, Hand kehr um.", "tokens": ["Dann", "wie\u00b7der", "spa\u00b7nisch", ",", "Hand", "kehr", "um", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$,", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "W\u00e4lzt sich wohl auch im Kote gern,", "tokens": ["W\u00e4lzt", "sich", "wohl", "auch", "im", "Ko\u00b7te", "gern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Tr\u00e4gt mal der Sund die Schand-Luzern,", "tokens": ["Tr\u00e4gt", "mal", "der", "Sund", "die", "Schan\u00b7dLu\u00b7zern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "H\u00e4lt Skrupel sich und Zweifel fern,", "tokens": ["H\u00e4lt", "Skru\u00b7pel", "sich", "und", "Zwei\u00b7fel", "fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PRF", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Findt im Genu\u00df des Lebens Kern.", "tokens": ["Findt", "im", "Ge\u00b7nu\u00df", "des", "Le\u00b7bens", "Kern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "NN", "ART", "NN", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.21": {"line.1": {"text": "Doch alles das nicht so gemein,", "tokens": ["Doch", "al\u00b7les", "das", "nicht", "so", "ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PDS", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Erst idealisiert ers fein", "tokens": ["Erst", "i\u00b7dea\u00b7li\u00b7siert", "ers", "fein"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und gibt die Quintessenz allein,", "tokens": ["Und", "gibt", "die", "Quin\u00b7tes\u00b7senz", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das Sublimat, den Lesern ein.", "tokens": ["Das", "Sub\u00b7li\u00b7mat", ",", "den", "Le\u00b7sern", "ein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Ist nun die ganze Welt verpufft", "tokens": ["Ist", "nun", "die", "gan\u00b7ze", "Welt", "ver\u00b7pufft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und leer und hohl, wie leere Luft,", "tokens": ["Und", "leer", "und", "hohl", ",", "wie", "lee\u00b7re", "Luft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "KON", "ADJD", "$,", "PWAV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "H\u00e4lt auch das Letzte l\u00e4nger nicht,", "tokens": ["H\u00e4lt", "auch", "das", "Letz\u00b7te", "l\u00e4n\u00b7ger", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "ADJD", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zerbr\u00f6ckelt, wo mans fa\u00dft und bricht,", "tokens": ["Zer\u00b7br\u00f6\u00b7ckelt", ",", "wo", "mans", "fa\u00dft", "und", "bricht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PWAV", "PIS", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dann mi\u00dftraut er dem eignen Licht,", "tokens": ["Dann", "mi\u00df\u00b7traut", "er", "dem", "eig\u00b7nen", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "Wie alte Hur zum Betstuhl kriecht.", "tokens": ["Wie", "al\u00b7te", "Hur", "zum", "Bet\u00b7stuhl", "kriecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Ihm, der nur Ideales trug,", "tokens": ["Ihm", ",", "der", "nur", "I\u00b7dea\u00b7les", "trug", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "ADV", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist nun nichts ", "tokens": ["Ist", "nun", "nichts"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "ADV", "PIS"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Und: \u00bbReligion und Porterkrug\u00ab", "tokens": ["Und", ":", "\u00bb", "Re\u00b7li\u00b7gi\u00b7on", "und", "Por\u00b7ter\u00b7krug", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$.", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ist von nun an sein Weidmannsspruch.", "tokens": ["Ist", "von", "nun", "an", "sein", "Weid\u00b7manns\u00b7spruch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "So duckt und b\u00fc\u00dft der Jammermann", "tokens": ["So", "duckt", "und", "b\u00fc\u00dft", "der", "Jam\u00b7mer\u00b7mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und feindet jeden andern an,", "tokens": ["Und", "fein\u00b7det", "je\u00b7den", "an\u00b7dern", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der, so wie er, nicht ", "tokens": ["Der", ",", "so", "wie", "er", ",", "nicht"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ART", "$,", "ADV", "KOKOM", "PPER", "$,", "PTKNEG"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Weil er nicht das, was er, ", "tokens": ["Weil", "er", "nicht", "das", ",", "was", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "PDS", "$,", "PWS", "PPER", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.25": {"line.1": {"text": "Verl\u00e4stert alles rings herum,", "tokens": ["Ver\u00b7l\u00e4s\u00b7tert", "al\u00b7les", "rings", "he\u00b7rum", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schreit \u00fcber H\u00f6ll und Heidentum,", "tokens": ["Schreit", "\u00fc\u00b7ber", "H\u00f6ll", "und", "Hei\u00b7den\u00b7tum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und m\u00f6chte Kraft und Licht verschw\u00e4rzen,", "tokens": ["Und", "m\u00f6ch\u00b7te", "Kraft", "und", "Licht", "ver\u00b7schw\u00e4r\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil sie erl\u00f6scht in seinem Herzen.", "tokens": ["Weil", "sie", "er\u00b7l\u00f6scht", "in", "sei\u00b7nem", "Her\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Das ist die M\u00e4r vom Schundrian,", "tokens": ["Das", "ist", "die", "M\u00e4r", "vom", "Schund\u00b7ri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "APPRART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem dritten Bruder lobesan", "tokens": ["Dem", "drit\u00b7ten", "Bru\u00b7der", "lo\u00b7be\u00b7san"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Des Schlendrian und Schludrian,", "tokens": ["Des", "Schlen\u00b7dri\u00b7an", "und", "Schlu\u00b7dri\u00b7an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Gemein genannt: der Wetterhahn.", "tokens": ["Ge\u00b7mein", "ge\u00b7nannt", ":", "der", "Wet\u00b7ter\u00b7hahn", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}