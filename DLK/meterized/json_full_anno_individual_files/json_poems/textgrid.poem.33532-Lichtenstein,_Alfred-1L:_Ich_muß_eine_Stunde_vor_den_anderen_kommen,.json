{"textgrid.poem.33532": {"metadata": {"author": {"name": "Lichtenstein, Alfred", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ich mu\u00df eine Stunde vor den anderen kommen,", "genre": "verse", "period": "N.A.", "pub_year": 1902, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich mu\u00df eine Stunde vor den anderen kommen,", "tokens": ["Ich", "mu\u00df", "ei\u00b7ne", "Stun\u00b7de", "vor", "den", "an\u00b7de\u00b7ren", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "APPR", "ART", "ADJA", "VVINF", "$,"], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Weil ich schlecht geschossen habe.", "tokens": ["Weil", "ich", "schlecht", "ge\u00b7schos\u00b7sen", "ha\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich werde wohl nicht bef\u00f6rdert werden.", "tokens": ["Ich", "wer\u00b7de", "wohl", "nicht", "be\u00b7f\u00f6r\u00b7dert", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "VVPP", "VAINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und nachexerzieren mu\u00df ich zur Strafe,", "tokens": ["Und", "na\u00b7ch\u00b7ex\u00b7er\u00b7zie\u00b7ren", "mu\u00df", "ich", "zur", "Stra\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}, "line.5": {"text": "Weil ich, w\u00e4hrend die anderen vorschriftsm\u00e4\u00dfig", "tokens": ["Weil", "ich", ",", "w\u00e4h\u00b7rend", "die", "an\u00b7de\u00b7ren", "vor\u00b7schrifts\u00b7m\u00e4\u00b7\u00dfig"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "KOUS", "ART", "ADJA", "ADJD"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Starr auf die M\u00fctze der Vorderen blickten,", "tokens": ["Starr", "auf", "die", "M\u00fct\u00b7ze", "der", "Vor\u00b7de\u00b7ren", "blick\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Als wir vor der roten Sonne", "tokens": ["Als", "wir", "vor", "der", "ro\u00b7ten", "Son\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "\u00dcber die leuchtenden Felder marschierten,", "tokens": ["\u00dc\u00b7ber", "die", "leuch\u00b7ten\u00b7den", "Fel\u00b7der", "mar\u00b7schier\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.9": {"text": "Vorsichtig zu dem kleinen Flieger schielte,", "tokens": ["Vor\u00b7sich\u00b7tig", "zu", "dem", "klei\u00b7nen", "Flie\u00b7ger", "schiel\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Der \u00fcber mir in dem gro\u00dfen, gl\u00fchenden", "tokens": ["Der", "\u00fc\u00b7ber", "mir", "in", "dem", "gro\u00b7\u00dfen", ",", "gl\u00fc\u00b7hen\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "APPR", "PPER", "APPR", "ART", "ADJA", "$,", "ADJA"], "meter": "-+-+--+-+--", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Abendhimmel wie eine Biene summte.", "tokens": ["A\u00b7bend\u00b7him\u00b7mel", "wie", "ei\u00b7ne", "Bie\u00b7ne", "summ\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Ich mu\u00df eine Stunde vor den anderen kommen,", "tokens": ["Ich", "mu\u00df", "ei\u00b7ne", "Stun\u00b7de", "vor", "den", "an\u00b7de\u00b7ren", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "APPR", "ART", "ADJA", "VVINF", "$,"], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Weil ich schlecht geschossen habe.", "tokens": ["Weil", "ich", "schlecht", "ge\u00b7schos\u00b7sen", "ha\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich werde wohl nicht bef\u00f6rdert werden.", "tokens": ["Ich", "wer\u00b7de", "wohl", "nicht", "be\u00b7f\u00f6r\u00b7dert", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "VVPP", "VAINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und nachexerzieren mu\u00df ich zur Strafe,", "tokens": ["Und", "na\u00b7ch\u00b7ex\u00b7er\u00b7zie\u00b7ren", "mu\u00df", "ich", "zur", "Stra\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}, "line.5": {"text": "Weil ich, w\u00e4hrend die anderen vorschriftsm\u00e4\u00dfig", "tokens": ["Weil", "ich", ",", "w\u00e4h\u00b7rend", "die", "an\u00b7de\u00b7ren", "vor\u00b7schrifts\u00b7m\u00e4\u00b7\u00dfig"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "KOUS", "ART", "ADJA", "ADJD"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Starr auf die M\u00fctze der Vorderen blickten,", "tokens": ["Starr", "auf", "die", "M\u00fct\u00b7ze", "der", "Vor\u00b7de\u00b7ren", "blick\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Als wir vor der roten Sonne", "tokens": ["Als", "wir", "vor", "der", "ro\u00b7ten", "Son\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "\u00dcber die leuchtenden Felder marschierten,", "tokens": ["\u00dc\u00b7ber", "die", "leuch\u00b7ten\u00b7den", "Fel\u00b7der", "mar\u00b7schier\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.9": {"text": "Vorsichtig zu dem kleinen Flieger schielte,", "tokens": ["Vor\u00b7sich\u00b7tig", "zu", "dem", "klei\u00b7nen", "Flie\u00b7ger", "schiel\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Der \u00fcber mir in dem gro\u00dfen, gl\u00fchenden", "tokens": ["Der", "\u00fc\u00b7ber", "mir", "in", "dem", "gro\u00b7\u00dfen", ",", "gl\u00fc\u00b7hen\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "APPR", "PPER", "APPR", "ART", "ADJA", "$,", "ADJA"], "meter": "-+-+--+-+--", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Abendhimmel wie eine Biene summte.", "tokens": ["A\u00b7bend\u00b7him\u00b7mel", "wie", "ei\u00b7ne", "Bie\u00b7ne", "summ\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}}}}