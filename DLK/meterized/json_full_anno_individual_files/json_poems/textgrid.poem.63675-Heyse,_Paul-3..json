{"textgrid.poem.63675": {"metadata": {"author": {"name": "Heyse, Paul", "birth": "N.A.", "death": "N.A."}, "title": "3.", "genre": "verse", "period": "N.A.", "pub_year": 1872, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie lang schon haben, was wir selbst geschaffen,", "tokens": ["Wie", "lang", "schon", "ha\u00b7ben", ",", "was", "wir", "selbst", "ge\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ADV", "VAFIN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zu w\u00fcrd'gen wir nur zaghaft uns getraut", "tokens": ["Zu", "w\u00fcrd'\u00b7gen", "wir", "nur", "zag\u00b7haft", "uns", "ge\u00b7traut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "VVFIN", "PPER", "ADV", "ADJD", "PPER", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und fuhren gern in jede fremde Haut,", "tokens": ["Und", "fuh\u00b7ren", "gern", "in", "je\u00b7de", "frem\u00b7de", "Haut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Statt zu gerechtem Stolz uns aufzuraffen!", "tokens": ["Statt", "zu", "ge\u00b7rech\u00b7tem", "Stolz", "uns", "auf\u00b7zu\u00b7raf\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "PPER", "VVINF", "$."], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}}, "stanza.2": {"line.1": {"text": "Erst spreizten wir uns als Franzosenaffen,", "tokens": ["Erst", "spreiz\u00b7ten", "wir", "uns", "als", "Fran\u00b7zo\u00b7sen\u00b7af\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dann haben wir nach Norden ausgeschaut,", "tokens": ["Dann", "ha\u00b7ben", "wir", "nach", "Nor\u00b7den", "aus\u00b7ge\u00b7schaut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "An mystischen Gespenstern uns erbaut,", "tokens": ["An", "mys\u00b7ti\u00b7schen", "Ge\u00b7spens\u00b7tern", "uns", "er\u00b7baut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Um ins Perverse jetzt uns zu vergaffen.", "tokens": ["Um", "ins", "Per\u00b7ver\u00b7se", "jetzt", "uns", "zu", "ver\u00b7gaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPRART", "NN", "ADV", "PPER", "PTKZU", "VVINF", "$."], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Noch steckt uns der Bescheidenheit Gebresten", "tokens": ["Noch", "steckt", "uns", "der", "Be\u00b7schei\u00b7den\u00b7heit", "Ge\u00b7bres\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zu tief im Blut. Doch mehren sich die Zeichen,", "tokens": ["Zu", "tief", "im", "Blut", ".", "Doch", "meh\u00b7ren", "sich", "die", "Zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "APPRART", "NN", "$.", "KON", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df nun zu ehren kommen unsre Besten.", "tokens": ["Da\u00df", "nun", "zu", "eh\u00b7ren", "kom\u00b7men", "uns\u00b7re", "Bes\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PTKZU", "VVINF", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Wem haben wir im Tragischen zu weichen?", "tokens": ["Wem", "ha\u00b7ben", "wir", "im", "Tra\u00b7gi\u00b7schen", "zu", "wei\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo sind, die unter all den fremden G\u00e4sten", "tokens": ["Wo", "sind", ",", "die", "un\u00b7ter", "all", "den", "frem\u00b7den", "G\u00e4s\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "$,", "PRELS", "APPR", "PIAT", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "An unsre Kleist, Grillparzer, Hebbel reichen?", "tokens": ["An", "uns\u00b7re", "Kleist", ",", "Grill\u00b7par\u00b7zer", ",", "Heb\u00b7bel", "rei\u00b7chen", "?"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "NN", "$,", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Wie lang schon haben, was wir selbst geschaffen,", "tokens": ["Wie", "lang", "schon", "ha\u00b7ben", ",", "was", "wir", "selbst", "ge\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ADV", "VAFIN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zu w\u00fcrd'gen wir nur zaghaft uns getraut", "tokens": ["Zu", "w\u00fcrd'\u00b7gen", "wir", "nur", "zag\u00b7haft", "uns", "ge\u00b7traut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "VVFIN", "PPER", "ADV", "ADJD", "PPER", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und fuhren gern in jede fremde Haut,", "tokens": ["Und", "fuh\u00b7ren", "gern", "in", "je\u00b7de", "frem\u00b7de", "Haut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Statt zu gerechtem Stolz uns aufzuraffen!", "tokens": ["Statt", "zu", "ge\u00b7rech\u00b7tem", "Stolz", "uns", "auf\u00b7zu\u00b7raf\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "PPER", "VVINF", "$."], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}}, "stanza.6": {"line.1": {"text": "Erst spreizten wir uns als Franzosenaffen,", "tokens": ["Erst", "spreiz\u00b7ten", "wir", "uns", "als", "Fran\u00b7zo\u00b7sen\u00b7af\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dann haben wir nach Norden ausgeschaut,", "tokens": ["Dann", "ha\u00b7ben", "wir", "nach", "Nor\u00b7den", "aus\u00b7ge\u00b7schaut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "An mystischen Gespenstern uns erbaut,", "tokens": ["An", "mys\u00b7ti\u00b7schen", "Ge\u00b7spens\u00b7tern", "uns", "er\u00b7baut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Um ins Perverse jetzt uns zu vergaffen.", "tokens": ["Um", "ins", "Per\u00b7ver\u00b7se", "jetzt", "uns", "zu", "ver\u00b7gaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPRART", "NN", "ADV", "PPER", "PTKZU", "VVINF", "$."], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Noch steckt uns der Bescheidenheit Gebresten", "tokens": ["Noch", "steckt", "uns", "der", "Be\u00b7schei\u00b7den\u00b7heit", "Ge\u00b7bres\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zu tief im Blut. Doch mehren sich die Zeichen,", "tokens": ["Zu", "tief", "im", "Blut", ".", "Doch", "meh\u00b7ren", "sich", "die", "Zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "APPRART", "NN", "$.", "KON", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df nun zu ehren kommen unsre Besten.", "tokens": ["Da\u00df", "nun", "zu", "eh\u00b7ren", "kom\u00b7men", "uns\u00b7re", "Bes\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PTKZU", "VVINF", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Wem haben wir im Tragischen zu weichen?", "tokens": ["Wem", "ha\u00b7ben", "wir", "im", "Tra\u00b7gi\u00b7schen", "zu", "wei\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo sind, die unter all den fremden G\u00e4sten", "tokens": ["Wo", "sind", ",", "die", "un\u00b7ter", "all", "den", "frem\u00b7den", "G\u00e4s\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "$,", "PRELS", "APPR", "PIAT", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "An unsre Kleist, Grillparzer, Hebbel reichen?", "tokens": ["An", "uns\u00b7re", "Kleist", ",", "Grill\u00b7par\u00b7zer", ",", "Heb\u00b7bel", "rei\u00b7chen", "?"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "NN", "$,", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}