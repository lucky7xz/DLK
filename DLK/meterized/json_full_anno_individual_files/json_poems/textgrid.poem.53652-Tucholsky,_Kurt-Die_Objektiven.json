{"textgrid.poem.53652": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Die Objektiven", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn so f\u00fcnf M\u00e4nner Sonnabend abends", "tokens": ["Wenn", "so", "f\u00fcnf", "M\u00e4n\u00b7ner", "Sonn\u00b7a\u00b7bend", "a\u00b7bends"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "CARD", "NN", "NN", "ADV"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "um einen runden Tisch herum", "tokens": ["um", "ei\u00b7nen", "run\u00b7den", "Tisch", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "den Bierskat kloppen \u2013 und sie habens", "tokens": ["den", "Bier\u00b7skat", "klop\u00b7pen", "\u2013", "und", "sie", "ha\u00b7bens"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVINF", "$(", "KON", "PPER", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "auch niemals \u00fcber (wegen dumm) \u2013;", "tokens": ["auch", "nie\u00b7mals", "\u00fc\u00b7ber", "(", "we\u00b7gen", "dumm", ")", "\u2013", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "ADV", "APPR", "$(", "APPR", "ADJD", "$(", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "wenn sie von Politike brummeln,", "tokens": ["wenn", "sie", "von", "Po\u00b7li\u00b7ti\u00b7ke", "brum\u00b7meln", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "im Maule die Zigarrenstummel,", "tokens": ["im", "Mau\u00b7le", "die", "Zi\u00b7gar\u00b7ren\u00b7stum\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "von Hindenburg und Ludendorffen,", "tokens": ["von", "Hin\u00b7den\u00b7burg", "und", "Lu\u00b7den\u00b7dorf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und wie wir fast Paris geworfen,", "tokens": ["und", "wie", "wir", "fast", "Pa\u00b7ris", "ge\u00b7wor\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "vom Pr\u00e4sidenten in der Stadt,", "tokens": ["vom", "Pr\u00e4\u00b7si\u00b7den\u00b7ten", "in", "der", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "der keinen Bohrt wie Wilhelm hat;", "tokens": ["der", "kei\u00b7nen", "Bohrt", "wie", "Wil\u00b7helm", "hat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "KOKOM", "NE", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "wenn sie so zwischen As und Tr\u00fcmpfen", "tokens": ["wenn", "sie", "so", "zwi\u00b7schen", "As", "und", "Tr\u00fcmp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "\u2013 (\u00bbWer jibt?\u00ab) \u2013 auf Kommunisten schimpfen \u2013", "tokens": ["\u2013", "(", "\u00bb", "Wer", "jibt", "?", "\u00ab", ")", "\u2013", "auf", "Kom\u00b7mu\u00b7nis\u00b7ten", "schimp\u00b7fen", "\u2013"], "token_info": ["punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "PWS", "VVFIN", "$.", "$(", "$(", "$(", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "die Welt ist eine Kinderfibel,", "tokens": ["die", "Welt", "ist", "ei\u00b7ne", "Kin\u00b7der\u00b7fi\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "das Morgenblatt ist ihre Bibel \u2013;", "tokens": ["das", "Mor\u00b7gen\u00b7blatt", "ist", "ih\u00b7re", "Bi\u00b7bel", "\u2013", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "wenn Lehmann ansagt: \u00bbKnautschke hats!\u00ab \u2013:", "tokens": ["wenn", "Leh\u00b7mann", "an\u00b7sagt", ":", "\u00bb", "Knautschke", "hats", "!", "\u00ab", "\u2013", ":"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KOUS", "NE", "VVFIN", "$.", "$(", "NN", "VAFIN", "$.", "$(", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.16": {"text": "Das nennt man einen Stammtischschwatz.", "tokens": ["Das", "nennt", "man", "ei\u00b7nen", "Stamm\u00b7tischschwatz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Doch wenn f\u00fcnf M\u00e4nner in Talaren", "tokens": ["Doch", "wenn", "f\u00fcnf", "M\u00e4n\u00b7ner", "in", "Ta\u00b7la\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "CARD", "NN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "an einem langen Sitzungstisch", "tokens": ["an", "ei\u00b7nem", "lan\u00b7gen", "Sit\u00b7zungs\u00b7tisch"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "ein Menschenkind da im Verfahren", "tokens": ["ein", "Men\u00b7schen\u00b7kind", "da", "im", "Ver\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "verknacken frisch und k\u00fcnstlerisch \u2013", "tokens": ["ver\u00b7kna\u00b7cken", "frisch", "und", "k\u00fcnst\u00b7le\u00b7risch", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "was sie als Studios schon forcierten,", "tokens": ["was", "sie", "als", "Stu\u00b7di\u00b7os", "schon", "for\u00b7cier\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KOUS", "NN", "ADV", "VVFIN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.6": {"text": "gef\u00fchrt von ihren Erstchargierten;", "tokens": ["ge\u00b7f\u00fchrt", "von", "ih\u00b7ren", "Er\u00b7stchar\u00b7gier\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "was sie als Referendare lernten:", "tokens": ["was", "sie", "als", "Re\u00b7fe\u00b7ren\u00b7da\u00b7re", "lern\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KOUS", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.8": {"text": "den Glauben an die reich Besternten;", "tokens": ["den", "Glau\u00b7ben", "an", "die", "reich", "Bes\u00b7tern\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "was der Assessor, scheu geduckt,", "tokens": ["was", "der", "As\u00b7ses\u00b7sor", ",", "scheu", "ge\u00b7duckt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.10": {"text": "dem Staatsanwalte abgeguckt \u2013", "tokens": ["dem", "Staats\u00b7an\u00b7wal\u00b7te", "ab\u00b7ge\u00b7guckt", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "wenn diese f\u00fcnf bei den Prozessen", "tokens": ["wenn", "die\u00b7se", "f\u00fcnf", "bei", "den", "Pro\u00b7zes\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "CARD", "APPR", "ART", "NN"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.12": {"text": "ihr ganzes Leben glatt vergessen,", "tokens": ["ihr", "gan\u00b7zes", "Le\u00b7ben", "glatt", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "weil Orgeschmann und Sozialist", "tokens": ["weil", "Or\u00b7ge\u00b7schmann", "und", "So\u00b7zi\u00b7a\u00b7list"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.14": {"text": "f\u00fcr das Gericht dasselbe ist!", "tokens": ["f\u00fcr", "das", "Ge\u00b7richt", "das\u00b7sel\u00b7be", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PDAT", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "wenn da was f\u00e4llt wie'n Donnerkeil \u2013:", "tokens": ["wenn", "da", "was", "f\u00e4llt", "wie'n", "Don\u00b7ner\u00b7keil", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "PWS", "VVFIN", "ADJA", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Das nennt man ein Gerichtsurteil.", "tokens": ["Das", "nennt", "man", "ein", "Ge\u00b7richts\u00b7ur\u00b7teil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Justitia! Ich wein bitterlich:", "tokens": ["Jus\u00b7ti\u00b7tia", "!", "Ich", "wein", "bit\u00b7ter\u00b7lich", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Du gehst auf einen langen \u2013\u2013\u2013\u2013\u2013\u2013\u2013", "tokens": ["Du", "gehst", "auf", "ei\u00b7nen", "lan\u00b7gen", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "$(", "$(", "$(", "$(", "$(", "$(", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Wenn so f\u00fcnf M\u00e4nner Sonnabend abends", "tokens": ["Wenn", "so", "f\u00fcnf", "M\u00e4n\u00b7ner", "Sonn\u00b7a\u00b7bend", "a\u00b7bends"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "CARD", "NN", "NN", "ADV"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "um einen runden Tisch herum", "tokens": ["um", "ei\u00b7nen", "run\u00b7den", "Tisch", "he\u00b7rum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "den Bierskat kloppen \u2013 und sie habens", "tokens": ["den", "Bier\u00b7skat", "klop\u00b7pen", "\u2013", "und", "sie", "ha\u00b7bens"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVINF", "$(", "KON", "PPER", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "auch niemals \u00fcber (wegen dumm) \u2013;", "tokens": ["auch", "nie\u00b7mals", "\u00fc\u00b7ber", "(", "we\u00b7gen", "dumm", ")", "\u2013", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "ADV", "APPR", "$(", "APPR", "ADJD", "$(", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "wenn sie von Politike brummeln,", "tokens": ["wenn", "sie", "von", "Po\u00b7li\u00b7ti\u00b7ke", "brum\u00b7meln", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "im Maule die Zigarrenstummel,", "tokens": ["im", "Mau\u00b7le", "die", "Zi\u00b7gar\u00b7ren\u00b7stum\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "von Hindenburg und Ludendorffen,", "tokens": ["von", "Hin\u00b7den\u00b7burg", "und", "Lu\u00b7den\u00b7dorf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "und wie wir fast Paris geworfen,", "tokens": ["und", "wie", "wir", "fast", "Pa\u00b7ris", "ge\u00b7wor\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "vom Pr\u00e4sidenten in der Stadt,", "tokens": ["vom", "Pr\u00e4\u00b7si\u00b7den\u00b7ten", "in", "der", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "der keinen Bohrt wie Wilhelm hat;", "tokens": ["der", "kei\u00b7nen", "Bohrt", "wie", "Wil\u00b7helm", "hat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "KOKOM", "NE", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "wenn sie so zwischen As und Tr\u00fcmpfen", "tokens": ["wenn", "sie", "so", "zwi\u00b7schen", "As", "und", "Tr\u00fcmp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "\u2013 (\u00bbWer jibt?\u00ab) \u2013 auf Kommunisten schimpfen \u2013", "tokens": ["\u2013", "(", "\u00bb", "Wer", "jibt", "?", "\u00ab", ")", "\u2013", "auf", "Kom\u00b7mu\u00b7nis\u00b7ten", "schimp\u00b7fen", "\u2013"], "token_info": ["punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "$(", "$(", "PWS", "VVFIN", "$.", "$(", "$(", "$(", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "die Welt ist eine Kinderfibel,", "tokens": ["die", "Welt", "ist", "ei\u00b7ne", "Kin\u00b7der\u00b7fi\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "das Morgenblatt ist ihre Bibel \u2013;", "tokens": ["das", "Mor\u00b7gen\u00b7blatt", "ist", "ih\u00b7re", "Bi\u00b7bel", "\u2013", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "wenn Lehmann ansagt: \u00bbKnautschke hats!\u00ab \u2013:", "tokens": ["wenn", "Leh\u00b7mann", "an\u00b7sagt", ":", "\u00bb", "Knautschke", "hats", "!", "\u00ab", "\u2013", ":"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KOUS", "NE", "VVFIN", "$.", "$(", "NN", "VAFIN", "$.", "$(", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.16": {"text": "Das nennt man einen Stammtischschwatz.", "tokens": ["Das", "nennt", "man", "ei\u00b7nen", "Stamm\u00b7tischschwatz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Doch wenn f\u00fcnf M\u00e4nner in Talaren", "tokens": ["Doch", "wenn", "f\u00fcnf", "M\u00e4n\u00b7ner", "in", "Ta\u00b7la\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "CARD", "NN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "an einem langen Sitzungstisch", "tokens": ["an", "ei\u00b7nem", "lan\u00b7gen", "Sit\u00b7zungs\u00b7tisch"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "ein Menschenkind da im Verfahren", "tokens": ["ein", "Men\u00b7schen\u00b7kind", "da", "im", "Ver\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "verknacken frisch und k\u00fcnstlerisch \u2013", "tokens": ["ver\u00b7kna\u00b7cken", "frisch", "und", "k\u00fcnst\u00b7le\u00b7risch", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "was sie als Studios schon forcierten,", "tokens": ["was", "sie", "als", "Stu\u00b7di\u00b7os", "schon", "for\u00b7cier\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KOUS", "NN", "ADV", "VVFIN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.6": {"text": "gef\u00fchrt von ihren Erstchargierten;", "tokens": ["ge\u00b7f\u00fchrt", "von", "ih\u00b7ren", "Er\u00b7stchar\u00b7gier\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "was sie als Referendare lernten:", "tokens": ["was", "sie", "als", "Re\u00b7fe\u00b7ren\u00b7da\u00b7re", "lern\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KOUS", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.8": {"text": "den Glauben an die reich Besternten;", "tokens": ["den", "Glau\u00b7ben", "an", "die", "reich", "Bes\u00b7tern\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJD", "NN", "$."], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "was der Assessor, scheu geduckt,", "tokens": ["was", "der", "As\u00b7ses\u00b7sor", ",", "scheu", "ge\u00b7duckt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.10": {"text": "dem Staatsanwalte abgeguckt \u2013", "tokens": ["dem", "Staats\u00b7an\u00b7wal\u00b7te", "ab\u00b7ge\u00b7guckt", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "wenn diese f\u00fcnf bei den Prozessen", "tokens": ["wenn", "die\u00b7se", "f\u00fcnf", "bei", "den", "Pro\u00b7zes\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "CARD", "APPR", "ART", "NN"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.12": {"text": "ihr ganzes Leben glatt vergessen,", "tokens": ["ihr", "gan\u00b7zes", "Le\u00b7ben", "glatt", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "weil Orgeschmann und Sozialist", "tokens": ["weil", "Or\u00b7ge\u00b7schmann", "und", "So\u00b7zi\u00b7a\u00b7list"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.14": {"text": "f\u00fcr das Gericht dasselbe ist!", "tokens": ["f\u00fcr", "das", "Ge\u00b7richt", "das\u00b7sel\u00b7be", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PDAT", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "wenn da was f\u00e4llt wie'n Donnerkeil \u2013:", "tokens": ["wenn", "da", "was", "f\u00e4llt", "wie'n", "Don\u00b7ner\u00b7keil", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "PWS", "VVFIN", "ADJA", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Das nennt man ein Gerichtsurteil.", "tokens": ["Das", "nennt", "man", "ein", "Ge\u00b7richts\u00b7ur\u00b7teil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Justitia! Ich wein bitterlich:", "tokens": ["Jus\u00b7ti\u00b7tia", "!", "Ich", "wein", "bit\u00b7ter\u00b7lich", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Du gehst auf einen langen \u2013\u2013\u2013\u2013\u2013\u2013\u2013", "tokens": ["Du", "gehst", "auf", "ei\u00b7nen", "lan\u00b7gen", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "$(", "$(", "$(", "$(", "$(", "$(", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}