{"dta.poem.10114": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Einige aus dem Englischen genommene  \n Gedancken.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1735", "urn": "urn:nbn:de:kobv:b4-20086-0", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Der erste Prediger, der zu des Sch\u00f6pfers Ehren,", "tokens": ["Der", "ers\u00b7te", "Pre\u00b7di\u00b7ger", ",", "der", "zu", "des", "Sch\u00f6p\u00b7fers", "Eh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und zur Verherrlichung von Seiner Majest\u00e4t,", "tokens": ["Und", "zur", "Ver\u00b7herr\u00b7li\u00b7chung", "von", "Sei\u00b7ner", "Ma\u00b7jes\u00b7t\u00e4t", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die alles alles \u00fcbergeht,", "tokens": ["Die", "al\u00b7les", "al\u00b7les", "\u00fc\u00b7ber\u00b7geht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sich l\u00e4sset h\u00f6ren,", "tokens": ["Sich", "l\u00e4s\u00b7set", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "VVFIN", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Ist das erhabne Firmament:", "tokens": ["Ist", "das", "er\u00b7hab\u00b7ne", "Fir\u00b7ma\u00b7ment", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wo, neben Sonn und Mond, ein ungezehltes Heer", "tokens": ["Wo", ",", "ne\u00b7ben", "Sonn", "und", "Mond", ",", "ein", "un\u00b7ge\u00b7zehl\u00b7tes", "Heer"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "$,", "APPR", "NN", "KON", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Von Sternen funckelt, blitzt und brennt.", "tokens": ["Von", "Ster\u00b7nen", "fun\u00b7ckelt", ",", "blitzt", "und", "brennt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Es kann die\u00df grosse Buch allein", "tokens": ["Es", "kann", "die\u00df", "gros\u00b7se", "Buch", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PDS", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Lettern, so von Licht geschrieben,", "tokens": ["Mit", "Let\u00b7tern", ",", "so", "von", "Licht", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Menschen, wenn sie GOTT nicht ehren, f\u00fcrchten, lieben,", "tokens": ["Die", "Men\u00b7schen", ",", "wenn", "sie", "GoTT", "nicht", "eh\u00b7ren", ",", "f\u00fcrch\u00b7ten", ",", "lie\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "NE", "PTKNEG", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Zu \u00fcberf\u00fchren gnugsam seyn,", "tokens": ["Zu", "\u00fc\u00b7berf\u00b7\u00fch\u00b7ren", "gnug\u00b7sam", "seyn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie sehr sie sich vergehn.", "tokens": ["Wie", "sehr", "sie", "sich", "ver\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Doch ist des Sch\u00f6pfers Weisheits-Licht", "tokens": ["Doch", "ist", "des", "Sch\u00f6p\u00b7fers", "Weis\u00b7heits\u00b7Licht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht minder in den kleinsten Wercken,", "tokens": ["Nicht", "min\u00b7der", "in", "den", "kleins\u00b7ten", "Wer\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die durch Sein grosses Wort entstehn,", "tokens": ["Die", "durch", "Sein", "gros\u00b7ses", "Wort", "ent\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPOSAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit tieffer Ehr-Furcht anzusehn,", "tokens": ["Mit", "tief\u00b7fer", "Ehr\u00b7Furcht", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und mit Erstauen zu bemercken.", "tokens": ["Und", "mit", "Er\u00b7stau\u00b7en", "zu", "be\u00b7mer\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "In diesen hat Er gleichsam sich,", "tokens": ["In", "die\u00b7sen", "hat", "Er", "gleich\u00b7sam", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VAFIN", "PPER", "ADJD", "PRF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wenn man so sagen darf, zu uns herab gelassen,", "tokens": ["Wenn", "man", "so", "sa\u00b7gen", "darf", ",", "zu", "uns", "her\u00b7ab", "ge\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "VVINF", "VMFIN", "$,", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und scheinet es, ob lad\u2019 Er eigentlich,", "tokens": ["Und", "schei\u00b7net", "es", ",", "ob", "lad'", "Er", "ei\u00b7gent\u00b7lich", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "NE", "PPER", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Jhn n\u00e4her anzusehn, und von Jhm was zu fassen,", "tokens": ["Jhn", "n\u00e4\u00b7her", "an\u00b7zu\u00b7sehn", ",", "und", "von", "Jhm", "was", "zu", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVIZU", "$,", "KON", "APPR", "PPER", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Uns selbst in ihnen ein.", "tokens": ["Uns", "selbst", "in", "ih\u00b7nen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Die Pflantzen k\u00f6nnen uns hievon belehren.", "tokens": ["Die", "Pflant\u00b7zen", "k\u00f6n\u00b7nen", "uns", "hie\u00b7von", "be\u00b7leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und unverwerffliche gewisse Zeugen seyn.", "tokens": ["Und", "un\u00b7ver\u00b7werf\u00b7fli\u00b7che", "ge\u00b7wis\u00b7se", "Zeu\u00b7gen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "VAINF", "$."], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Auch die ver\u00e4chtlichste von ihnen machet sich", "tokens": ["Auch", "die", "ver\u00b7\u00e4cht\u00b7lichs\u00b7te", "von", "ih\u00b7nen", "ma\u00b7chet", "sich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "APPR", "PPER", "VVFIN", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der allerweisesten und gr\u00f6ssten Geister,", "tokens": ["Der", "al\u00b7ler\u00b7wei\u00b7ses\u00b7ten", "und", "gr\u00f6ss\u00b7ten", "Geis\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Durch ihre k\u00fcnstliche Zusammenf\u00fcgung, Meister:", "tokens": ["Durch", "ih\u00b7re", "k\u00fcnst\u00b7li\u00b7che", "Zu\u00b7sam\u00b7men\u00b7f\u00fc\u00b7gung", ",", "Meis\u00b7ter", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ob sie von ihnen gleich nur das, was c\u00f6rperlich,", "tokens": ["Ob", "sie", "von", "ih\u00b7nen", "gleich", "nur", "das", ",", "was", "c\u00f6r\u00b7per\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPER", "ADV", "ADV", "PDS", "$,", "PRELS", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und was das gr\u00f6bste nur, zu sehen taugen.", "tokens": ["Und", "was", "das", "gr\u00f6bs\u00b7te", "nur", ",", "zu", "se\u00b7hen", "tau\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "ADJA", "ADV", "$,", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Da ja dasjenige, durch welches sie sich nehren,", "tokens": ["Da", "ja", "das\u00b7je\u00b7ni\u00b7ge", ",", "durch", "wel\u00b7ches", "sie", "sich", "neh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PDS", "$,", "APPR", "PRELS", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wodurch sie leben, sich vermehren,", "tokens": ["Wo\u00b7durch", "sie", "le\u00b7ben", ",", "sich", "ver\u00b7meh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVINF", "$,", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Sowol den geistigen als c\u00f6rperlichen Augen", "tokens": ["So\u00b7wol", "den", "geis\u00b7ti\u00b7gen", "als", "c\u00f6r\u00b7per\u00b7li\u00b7chen", "Au\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.11": {"text": "Unsichtbar, unbekannt. Kein Blat ist \u00fcbergangen;", "tokens": ["Un\u00b7sicht\u00b7bar", ",", "un\u00b7be\u00b7kannt", ".", "Kein", "Blat", "ist", "\u00fc\u00b7ber\u00b7gan\u00b7gen", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$.", "PIAT", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ein iegliches hat Ordnung, Symmetrie,", "tokens": ["Ein", "ieg\u00b7li\u00b7ches", "hat", "Ord\u00b7nung", ",", "Sym\u00b7me\u00b7trie", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "In einem reichen Ma\u00df empfangen.", "tokens": ["In", "ei\u00b7nem", "rei\u00b7chen", "Ma\u00df", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Es ist erstaunens wehrt, auf welche Weise sie", "tokens": ["Es", "ist", "er\u00b7stau\u00b7nens", "wehrt", ",", "auf", "wel\u00b7che", "Wei\u00b7se", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN", "$,", "APPR", "PWAT", "NN", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Sich von einander unterscheiden,", "tokens": ["Sich", "von", "ein\u00b7an\u00b7der", "un\u00b7ter\u00b7schei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "An Balsam, an Figur,", "tokens": ["An", "Bal\u00b7sam", ",", "an", "Fi\u00b7gur", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "APPR", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.17": {"text": "An Kr\u00e4fften, an Natur,", "tokens": ["An", "Kr\u00e4ff\u00b7ten", ",", "an", "Na\u00b7tur", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "APPR", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.18": {"text": "An Farben und an Schmuck, in welchen sie sich kleiden.", "tokens": ["An", "Far\u00b7ben", "und", "an", "Schmuck", ",", "in", "wel\u00b7chen", "sie", "sich", "klei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "$,", "APPR", "PWAT", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Was haben wir von dem, was in dem Samen stecket,", "tokens": ["Was", "ha\u00b7ben", "wir", "von", "dem", ",", "was", "in", "dem", "Sa\u00b7men", "ste\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "ART", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch Microscopia nicht allererst entdecket!", "tokens": ["Durch", "Mi\u00b7cro\u00b7sco\u00b7pia", "nicht", "al\u00b7le\u00b7rerst", "ent\u00b7de\u00b7cket", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Allein, was hat nicht GOTT f\u00fcr mannichfache Krafft,", "tokens": ["Al\u00b7lein", ",", "was", "hat", "nicht", "GoTT", "f\u00fcr", "man\u00b7nich\u00b7fa\u00b7che", "Krafft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWS", "VAFIN", "PTKNEG", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "F\u00fcr Wirckungen und Eigenschaft,", "tokens": ["F\u00fcr", "Wir\u00b7ckun\u00b7gen", "und", "Ei\u00b7gen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.5": {"text": "Blo\u00df durch ein Wort in sie gesencket!", "tokens": ["Blo\u00df", "durch", "ein", "Wort", "in", "sie", "ge\u00b7sen\u00b7cket", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wodurch er gleichsam denn das Pflantzen-Reich", "tokens": ["Wo\u00b7durch", "er", "gleich\u00b7sam", "denn", "das", "Pflant\u00b7zen\u00b7Reich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADJD", "KON", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Mit einer Art Unsterblichkeit beschencket.", "tokens": ["Mit", "ei\u00b7ner", "Art", "U\u00b7nsterb\u00b7lich\u00b7keit", "be\u00b7schen\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Ist etwas, wof\u00fcr GOtt mehr Ehr und Danck geb\u00fchret,", "tokens": ["Ist", "et\u00b7was", ",", "wo\u00b7f\u00fcr", "Gott", "mehr", "Ehr", "und", "Danck", "ge\u00b7b\u00fch\u00b7ret", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "$,", "PWAV", "NN", "PIAT", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und welches auch zugleich", "tokens": ["Und", "wel\u00b7ches", "auch", "zu\u00b7gleich"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWS", "ADV", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bewunderns-w\u00fcrdiger, als wie der Farben Schein,", "tokens": ["Be\u00b7wun\u00b7derns\u00b7w\u00fcr\u00b7di\u00b7ger", ",", "als", "wie", "der", "Far\u00b7ben", "Schein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUS", "KOKOM", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der allen Pflantzen allgemein,", "tokens": ["Der", "al\u00b7len", "Pflant\u00b7zen", "all\u00b7ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und welcher Feld-und W\u00e4lder zieret?", "tokens": ["Und", "wel\u00b7cher", "Feld\u00b7\u00b7und", "W\u00e4l\u00b7der", "zie\u00b7ret", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAT", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "H\u00e4tt er auf dieser Welt", "tokens": ["H\u00e4tt", "er", "auf", "die\u00b7ser", "Welt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "PDAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Wald, H\u00fcgel, Th\u00e4ler, Berg\u2019 und Feld", "tokens": ["Wald", ",", "H\u00fc\u00b7gel", ",", "Th\u00e4\u00b7ler", ",", "Ber\u00b7g'", "und", "Feld"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Wei\u00df oder roth gef\u00e4rbt; wer h\u00e4tt\u2019 ihr brennen", "tokens": ["Wei\u00df", "o\u00b7der", "roth", "ge\u00b7f\u00e4rbt", ";", "wer", "h\u00e4tt'", "ihr", "bren\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "KON", "ADJD", "VVPP", "$.", "PWS", "VAFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Und ihrer Farben H\u00e4rt\u2019 ertragen k\u00f6nnen?", "tokens": ["Und", "ih\u00b7rer", "Far\u00b7ben", "H\u00e4rt'", "er\u00b7tra\u00b7gen", "k\u00f6n\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "H\u00e4tt\u2019 er dieselben nun mit einer Dunckelheit", "tokens": ["H\u00e4tt'", "er", "die\u00b7sel\u00b7ben", "nun", "mit", "ei\u00b7ner", "Dun\u00b7ckel\u00b7heit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PDAT", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Von Farben angethan; wer k\u00f6nnt\u2019 Ergetzlichkeit", "tokens": ["Von", "Far\u00b7ben", "an\u00b7ge\u00b7than", ";", "wer", "k\u00f6nnt'", "Er\u00b7getz\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "VVPP", "$.", "PWS", "VMFIN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "In solchem traurigen und w\u00fcsten Anblick haben?", "tokens": ["In", "sol\u00b7chem", "trau\u00b7ri\u00b7gen", "und", "w\u00fcs\u00b7ten", "An\u00b7blick", "ha\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "KON", "VVFIN", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wer k\u00f6nnte sich an solcher Schw\u00e4rtze laben?", "tokens": ["Wer", "k\u00f6nn\u00b7te", "sich", "an", "sol\u00b7cher", "Schw\u00e4rt\u00b7ze", "la\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PRF", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Ein lieblich holdes Gr\u00fcn hat zwischen beiden", "tokens": ["Ein", "lieb\u00b7lich", "hol\u00b7des", "Gr\u00fcn", "hat", "zwi\u00b7schen", "bei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "ADJA", "NN", "VAFIN", "APPR", "PIAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mit Recht den Mittel-Nang.", "tokens": ["Mit", "Recht", "den", "Mit\u00b7tel\u00b7\u00b7N\u00b7ang", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Und mit der Augen Bau und zarten Sehnen", "tokens": ["Und", "mit", "der", "Au\u00b7gen", "Bau", "und", "zar\u00b7ten", "Seh\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Solch einen richtigen Zusammenhang,", "tokens": ["Solch", "ei\u00b7nen", "rich\u00b7ti\u00b7gen", "Zu\u00b7sam\u00b7men\u00b7hang", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Da\u00df es, an stat sie auszudehnen,", "tokens": ["Da\u00df", "es", ",", "an", "stat", "sie", "aus\u00b7zu\u00b7deh\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "VVFIN", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Sie nachl\u00e4sst, st\u00e4rckt, erquickt, ja unterh\u00e4lt und nehret,", "tokens": ["Sie", "nach\u00b7l\u00e4sst", ",", "st\u00e4rckt", ",", "er\u00b7quickt", ",", "ja", "un\u00b7ter\u00b7h\u00e4lt", "und", "neh\u00b7ret", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "$,", "VVPP", "$,", "ADV", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und sie an Kr\u00e4ffter nicht ersch\u00f6pft, noch sonst beschweret:", "tokens": ["Und", "sie", "an", "Kr\u00e4ff\u00b7ter", "nicht", "er\u00b7sch\u00f6pft", ",", "noch", "sonst", "be\u00b7schwe\u00b7ret", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPR", "NN", "PTKNEG", "VVPP", "$,", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ja das, was Anfangs blo\u00df nur gr\u00fcn,", "tokens": ["Ja", "das", ",", "was", "An\u00b7fangs", "blo\u00df", "nur", "gr\u00fcn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PDS", "$,", "PRELS", "ADV", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Und eine Farbe schien,", "tokens": ["Und", "ei\u00b7ne", "Far\u00b7be", "schien", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Hat solch unzehliche Ver\u00e4nderung, da\u00df man", "tokens": ["Hat", "solch", "un\u00b7zeh\u00b7li\u00b7che", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", ",", "da\u00df", "man"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "$,", "KOUS", "PIS"], "meter": "+-++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.11": {"text": "Sie nie genug bewundern kann.", "tokens": ["Sie", "nie", "ge\u00b7nug", "be\u00b7wun\u00b7dern", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "An allen Orten ist es gr\u00fcn: Allein", "tokens": ["An", "al\u00b7len", "Or\u00b7ten", "ist", "es", "gr\u00fcn", ":", "Al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "PPER", "ADJD", "$.", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Ist es an einem wol dasselbe? Nein!", "tokens": ["Ist", "es", "an", "ei\u00b7nem", "wol", "das\u00b7sel\u00b7be", "?", "Nein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ART", "ADV", "PDAT", "$.", "PTKANT", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Kein\u2019 einzige von allen f\u00e4rbet sich", "tokens": ["Kein'", "ein\u00b7zi\u00b7ge", "von", "al\u00b7len", "f\u00e4r\u00b7bet", "sich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "ADJA", "APPR", "PIS", "VVFIN", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Wie sich die andre f\u00e4rbt: und dieser Unterscheid,", "tokens": ["Wie", "sich", "die", "and\u00b7re", "f\u00e4rbt", ":", "und", "die\u00b7ser", "Un\u00b7ter\u00b7scheid", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "PIS", "VVFIN", "$.", "KON", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Der so verwunderlich,", "tokens": ["Der", "so", "ver\u00b7wun\u00b7der\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.17": {"text": "Da\u00df keine Menschen-Kunst noch Flei\u00df", "tokens": ["Da\u00df", "kei\u00b7ne", "Men\u00b7schen\u00b7Kunst", "noch", "Flei\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PIAT", "NN", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Die Mannichfaltigkeit", "tokens": ["Die", "Man\u00b7nich\u00b7fal\u00b7tig\u00b7keit"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.19": {"text": "Begreiffen, weniger sie nachzuahmen wei\u00df,", "tokens": ["Be\u00b7greif\u00b7fen", ",", "we\u00b7ni\u00b7ger", "sie", "nach\u00b7zu\u00b7ah\u00b7men", "wei\u00df", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUS", "PPER", "VVIZU", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Wird noch in iedem Kraut,", "tokens": ["Wird", "noch", "in", "ie\u00b7dem", "Kraut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.21": {"text": "Auf mehr als eine Art, geschaut:", "tokens": ["Auf", "mehr", "als", "ei\u00b7ne", "Art", ",", "ge\u00b7schaut", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "KOKOM", "ART", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.22": {"text": "Indem sowol, wann sie zuerst entstehn,", "tokens": ["In\u00b7dem", "so\u00b7wol", ",", "wann", "sie", "zu\u00b7erst", "ent\u00b7stehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "$,", "PWAV", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.23": {"text": "Als auch wann sie hernach im Wachsthum weiter gehn;", "tokens": ["Als", "auch", "wann", "sie", "her\u00b7nach", "im", "Wach\u00b7sthum", "wei\u00b7ter", "gehn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PWAV", "PPER", "ADV", "APPRART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Nicht minder wann sie reiff, an ihnen", "tokens": ["Nicht", "min\u00b7der", "wann", "sie", "reiff", ",", "an", "ih\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PTKNEG", "ADV", "PWAV", "PPER", "VVFIN", "$,", "APPR", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Ein\u2019 unterschiedne Art vom gr\u00fcnen,", "tokens": ["Ein'", "un\u00b7ter\u00b7schied\u00b7ne", "Art", "vom", "gr\u00fc\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.26": {"text": "Die alle sch\u00f6n und lieblich sind, zu sehn.", "tokens": ["Die", "al\u00b7le", "sch\u00f6n", "und", "lieb\u00b7lich", "sind", ",", "zu", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "PIS", "ADJD", "KON", "ADJD", "VAFIN", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Man kan dieselbe Meng\u2019 und Unterscheid entdecken", "tokens": ["Man", "kan", "die\u00b7sel\u00b7be", "Meng'", "und", "Un\u00b7ter\u00b7scheid", "ent\u00b7de\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PDAT", "NN", "KON", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In der Figur der Pflantzen, auch im schmecken,", "tokens": ["In", "der", "Fi\u00b7gur", "der", "Pflant\u00b7zen", ",", "auch", "im", "schme\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,", "ADV", "APPRART", "VVINF", "$,"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Und im Geruch derselben, eben auch", "tokens": ["Und", "im", "Ge\u00b7ruch", "der\u00b7sel\u00b7ben", ",", "e\u00b7ben", "auch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPRART", "NN", "PDAT", "$,", "ADV", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "In ihrer Nahrungs-Krafft, nicht minder im Gebrauch", "tokens": ["In", "ih\u00b7rer", "Nah\u00b7rungs\u00b7Krafft", ",", "nicht", "min\u00b7der", "im", "Ge\u00b7brauch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "$,", "PTKNEG", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zur edlen Artzeney.", "tokens": ["Zur", "ed\u00b7len", "Art\u00b7ze\u00b7ney", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Von den unzehligen will ich nur einerley", "tokens": ["Von", "den", "un\u00b7zeh\u00b7li\u00b7gen", "will", "ich", "nur", "ei\u00b7ner\u00b7ley"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "VMFIN", "PPER", "ADV", "PIAT"], "meter": "+-+--++--+-+", "measure": "asklepiade"}, "line.7": {"text": "Zu mein-und deiner Lehr erwegen:", "tokens": ["Zu", "mein\u00b7\u00b7und", "dei\u00b7ner", "Lehr", "er\u00b7we\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wenn GOTT der Sch\u00f6pfer nicht dem Heu,", "tokens": ["Wenn", "GoTT", "der", "Sch\u00f6p\u00b7fer", "nicht", "dem", "Heu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ART", "NN", "PTKNEG", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So gar dem trockenen, auch wenn es alt, den Segen", "tokens": ["So", "gar", "dem", "tro\u00b7cke\u00b7nen", ",", "auch", "wenn", "es", "alt", ",", "den", "Se\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "$,", "ADV", "KOUS", "PPER", "ADJD", "$,", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und eine Nahrungs-Krafft, f\u00fcr Ochsen, Pferd\u2019 und K\u00fch,", "tokens": ["Und", "ei\u00b7ne", "Nah\u00b7rungs\u00b7Krafft", ",", "f\u00fcr", "Och\u00b7sen", ",", "Pferd'", "und", "K\u00fch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und ander grosses Vieh", "tokens": ["Und", "an\u00b7der", "gros\u00b7ses", "Vieh"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADJD", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "So reichlich beygelegt; wie f\u00fcnd\u2019 ein Ackers-Mann,", "tokens": ["So", "reich\u00b7lich", "bey\u00b7ge\u00b7legt", ";", "wie", "f\u00fcnd'", "ein", "A\u00b7cker\u00b7sMann", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "$.", "PWAV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ja selbst der reichste Mensch, ein Futter doch f\u00fcr sie?", "tokens": ["Ja", "selbst", "der", "reichs\u00b7te", "Mensch", ",", "ein", "Fut\u00b7ter", "doch", "f\u00fcr", "sie", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "ART", "ADJA", "NN", "$,", "ART", "NN", "ADV", "APPR", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Da sie so gro\u00df, und da sie, blo\u00df allein", "tokens": ["Da", "sie", "so", "gro\u00df", ",", "und", "da", "sie", ",", "blo\u00df", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "$,", "KON", "KOUS", "PPER", "$,", "ADV", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Durch ihre St\u00e4rck, uns n\u00fctzlich seyn.", "tokens": ["Durch", "ih\u00b7re", "St\u00e4rck", ",", "uns", "n\u00fctz\u00b7lich", "seyn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "PPER", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Wenn man im Gegentheil von einem Menschen wollte,", "tokens": ["Wenn", "man", "im", "Ge\u00b7gen\u00b7theil", "von", "ei\u00b7nem", "Men\u00b7schen", "woll\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "APPR", "ART", "NN", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Da\u00df er auf gleiche Weis\u2019, als sie, sich nehren sollte;", "tokens": ["Da\u00df", "er", "auf", "glei\u00b7che", "Weis'", ",", "als", "sie", ",", "sich", "neh\u00b7ren", "soll\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "$,", "KOUS", "PPER", "$,", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wie? oder wenn man ihm (da ihm das Heu zu k\u00e4uen,", "tokens": ["Wie", "?", "o\u00b7der", "wenn", "man", "ihm", "(", "da", "ihm", "das", "Heu", "zu", "k\u00e4u\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "KON", "KOUS", "PIS", "PPER", "$(", "KOUS", "PPER", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Noch solches zu verd\u00e4uen", "tokens": ["Noch", "sol\u00b7ches", "zu", "ver\u00b7d\u00e4u\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PIS", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.13": {"text": "Nicht m\u00f6glich) aus dem Heu und Stroh die Krafft", "tokens": ["Nicht", "m\u00f6g\u00b7lich", ")", "aus", "dem", "Heu", "und", "Stroh", "die", "Krafft"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADJD", "$(", "APPR", "ART", "NN", "KON", "NN", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "In etwan einem Safft\u2019", "tokens": ["In", "et\u00b7wan", "ei\u00b7nem", "Safft'"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADV", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.15": {"text": "Heraus zu ziehn, und ihm zu geben", "tokens": ["He\u00b7raus", "zu", "ziehn", ",", "und", "ihm", "zu", "ge\u00b7ben"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "PTKZU", "VVINF", "$,", "KON", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Bem\u00fchet w\u00e4re; sprecht, sollt\u2019 dieses ihm sein Leben", "tokens": ["Be\u00b7m\u00fc\u00b7het", "w\u00e4\u00b7re", ";", "sprecht", ",", "sollt'", "die\u00b7ses", "ihm", "sein", "Le\u00b7ben"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "VAFIN", "$.", "VVFIN", "$,", "VMFIN", "PDAT", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wol zu erhalten f\u00e4hig seyn?", "tokens": ["Wol", "zu", "er\u00b7hal\u00b7ten", "f\u00e4\u00b7hig", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "ADJD", "VAINF", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.18": {"text": "Die\u00df truckne Gras dient anderm Vieh dennoch,", "tokens": ["Die\u00df", "truck\u00b7ne", "Gras", "dient", "an\u00b7derm", "Vieh", "den\u00b7noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "VVFIN", "PIS", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "Die Euter, die von Milch so \u00fcberfl\u00fcssig quillen,", "tokens": ["Die", "Eu\u00b7ter", ",", "die", "von", "Milch", "so", "\u00fc\u00b7berf\u00b7l\u00fcs\u00b7sig", "quil\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "NN", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Des Tages zweymahl anzuf\u00fcllen:", "tokens": ["Des", "Ta\u00b7ges", "zwey\u00b7mahl", "an\u00b7zu\u00b7f\u00fcl\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.21": {"text": "Womit sich doch", "tokens": ["Wo\u00b7mit", "sich", "doch"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PRF", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.22": {"text": "Und zwar stat andrer Speis\u2019 allein,", "tokens": ["Und", "zwar", "stat", "an\u00b7drer", "Speis'", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Viel tausend Menschen-Kinder nehren.", "tokens": ["Viel", "tau\u00b7send", "Men\u00b7schen\u00b7Kin\u00b7der", "neh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Betrachtete man doch, dem weisen GOTT zu Ehren,", "tokens": ["Be\u00b7trach\u00b7te\u00b7te", "man", "doch", ",", "dem", "wei\u00b7sen", "GoTT", "zu", "Eh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "$,", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die\u00df Wunder, welches man sonst leider nicht betrachtet,", "tokens": ["Die\u00df", "Wun\u00b7der", ",", "wel\u00b7ches", "man", "sonst", "lei\u00b7der", "nicht", "be\u00b7trach\u00b7tet", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "$,", "PRELS", "PIS", "ADV", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und, durch Gewohnheit, kaum des denckens w\u00fcrdig achtet;", "tokens": ["Und", ",", "durch", "Ge\u00b7wohn\u00b7heit", ",", "kaum", "des", "den\u00b7ckens", "w\u00fcr\u00b7dig", "ach\u00b7tet", ";"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "APPR", "NN", "$,", "ADV", "ART", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Man w\u00fcrd, o grosser GOTT, dich weis\u2019 und gro\u00df zu", "tokens": ["Man", "w\u00fcrd", ",", "o", "gros\u00b7ser", "GoTT", ",", "dich", "weis'", "und", "gro\u00df", "zu"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "$,", "FM", "ADJA", "NN", "$,", "PPER", "VVFIN", "KON", "ADJD", "PTKZU"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Sich nicht ers\u00e4ttigen, sich nicht erm\u00fcden k\u00f6nnen.", "tokens": ["Sich", "nicht", "er\u00b7s\u00e4t\u00b7ti\u00b7gen", ",", "sich", "nicht", "er\u00b7m\u00fc\u00b7den", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKNEG", "VVINF", "$,", "PRF", "PTKNEG", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}