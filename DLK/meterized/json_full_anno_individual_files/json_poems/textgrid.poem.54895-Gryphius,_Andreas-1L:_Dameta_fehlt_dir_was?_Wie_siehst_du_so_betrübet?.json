{"textgrid.poem.54895": {"metadata": {"author": {"name": "Gryphius, Andreas", "birth": "N.A.", "death": "N.A."}, "title": "1L: Dameta fehlt dir was? Wie siehst du so betr\u00fcbet?", "genre": "verse", "period": "N.A.", "pub_year": 1640, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dameta fehlt dir was? Wie siehst du so betr\u00fcbet?", "tokens": ["Da\u00b7me\u00b7ta", "fehlt", "dir", "was", "?", "Wie", "siehst", "du", "so", "be\u00b7tr\u00fc\u00b7bet", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PIS", "$.", "PWAV", "VVFIN", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Mir fehlt nur mehr denn viel: Mich d\u00fcnckt ich sey verliebet:", "tokens": ["Mir", "fehlt", "nur", "mehr", "denn", "viel", ":", "Mich", "d\u00fcnckt", "ich", "sey", "ver\u00b7lie\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADV", "ADV", "$.", "PPER", "VVFIN", "PPER", "VAFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Das lasse Pan nicht zu! welch Ubel steckt dich an!", "tokens": ["Das", "las\u00b7se", "Pan", "nicht", "zu", "!", "welch", "U\u00b7bel", "steckt", "dich", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "PTKVZ", "$.", "PWAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Der mu\u00df ein Unmensch seyn/ der nun nicht lieben kan!", "tokens": ["Der", "mu\u00df", "ein", "Un\u00b7mensch", "seyn", "/", "der", "nun", "nicht", "lie\u00b7ben", "kan", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "NN", "VAINF", "$(", "ART", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Di\u00df Wort schmeckt lauter Gifft/ die greifft dir nach dem Hertzen.", "tokens": ["Di\u00df", "Wort", "schmeckt", "lau\u00b7ter", "Gifft", "/", "die", "greifft", "dir", "nach", "dem", "Hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VVFIN", "PIAT", "NN", "$(", "ART", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Mein Thyrsis ich vergeh' in bitter-s\u00fcssen Schmertzen.", "tokens": ["Mein", "Thyr\u00b7sis", "ich", "ver\u00b7geh'", "in", "bit\u00b7ter\u00b7s\u00fcs\u00b7sen", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Sprich Aertzt' um Mittel an: Vers\u00e4ume keine Zeit:", "tokens": ["Sprich", "A\u00b7ertzt'", "um", "Mit\u00b7tel", "an", ":", "Ver\u00b7s\u00e4u\u00b7me", "kei\u00b7ne", "Zeit", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "APPR", "NN", "PTKVZ", "$.", "NN", "PIAT", "NN", "$."], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}}, "stanza.8": {"line.1": {"text": "Die Kr\u00e4uter lindern nicht der Plagen Hefftigkeit.", "tokens": ["Die", "Kr\u00e4u\u00b7ter", "lin\u00b7dern", "nicht", "der", "Pla\u00b7gen", "Heff\u00b7tig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "So wilt du sonder Rath in deinem Wahnwitz sterben?", "tokens": ["So", "wilt", "du", "son\u00b7der", "Rath", "in", "dei\u00b7nem", "Wahn\u00b7witz", "ster\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Ein Mittel wei\u00df ich noch/ ach w\u00e4r' es zu erwerben.", "tokens": ["Ein", "Mit\u00b7tel", "wei\u00df", "ich", "noch", "/", "ach", "w\u00e4r'", "es", "zu", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "$(", "XY", "VAFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Zw\u00f6lff Schaafe setz' ich drauf/ wo ich dich retten kan:", "tokens": ["Zw\u00f6lff", "Schaa\u00b7fe", "setz'", "ich", "drauf", "/", "wo", "ich", "dich", "ret\u00b7ten", "kan", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "PPER", "PAV", "$(", "PWAV", "PPER", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Ach! blickte Charis mich nur etwas freundlich an.", "tokens": ["Ach", "!", "blick\u00b7te", "Cha\u00b7ris", "mich", "nur", "et\u00b7was", "freund\u00b7lich", "an", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "NE", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Di\u00df Mittel ist f\u00fcrwahr weit \u00e4rger als dein Leiden.", "tokens": ["Di\u00df", "Mit\u00b7tel", "ist", "f\u00fcr\u00b7wahr", "weit", "\u00e4r\u00b7ger", "als", "dein", "Lei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "KOKOM", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Hilfft Charis nicht/ so mu\u00df ich Welt und Leben meiden.", "tokens": ["Hilfft", "Cha\u00b7ris", "nicht", "/", "so", "mu\u00df", "ich", "Welt", "und", "Le\u00b7ben", "mei\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PTKNEG", "$(", "ADV", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Begehrst du denn ein Weib ein lebend Creutz ins Haus:", "tokens": ["Be\u00b7gehrst", "du", "denn", "ein", "Weib", "ein", "le\u00b7bend", "Creutz", "ins", "Haus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "ART", "ADJD", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Di\u00df Creutz alleine jagt die b\u00f6sen Geister aus.", "tokens": ["Di\u00df", "Creutz", "al\u00b7lei\u00b7ne", "jagt", "die", "b\u00f6\u00b7sen", "Geis\u00b7ter", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "ADV", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "Dafern man gl\u00e4ubt da\u00df Arg mit Argen zu vertreiben:", "tokens": ["Da\u00b7fern", "man", "gl\u00e4ubt", "da\u00df", "Arg", "mit", "Ar\u00b7gen", "zu", "ver\u00b7trei\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "KOUS", "NE", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Was gut/ gesellet sich: Arg mu\u00df alleine bleiben.", "tokens": ["Was", "gut", "/", "ge\u00b7sel\u00b7let", "sich", ":", "Arg", "mu\u00df", "al\u00b7lei\u00b7ne", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "$(", "VVFIN", "PRF", "$.", "NE", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.19": {"line.1": {"text": "Eh ich ein Weib begehr'/ eh w\u00fcntsch' ich mir den Tod/", "tokens": ["Eh", "ich", "ein", "Weib", "be\u00b7gehr'", "/", "eh", "w\u00fcnt\u00b7sch'", "ich", "mir", "den", "Tod", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$(", "KOUS", "VVFIN", "PPER", "PPER", "ART", "NN", "$("], "meter": "+--+-+--+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.20": {"line.1": {"text": "Und ich find' ohne Weib mich in der h\u00f6chsten Noth.", "tokens": ["Und", "ich", "find'", "oh\u00b7ne", "Weib", "mich", "in", "der", "h\u00f6chs\u00b7ten", "Noth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "NN", "PRF", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Wie schwer ists/ wenn man sol der Jungfern Gunst erbitten:", "tokens": ["Wie", "schwer", "ists", "/", "wenn", "man", "sol", "der", "Jung\u00b7fern", "Gunst", "er\u00b7bit\u00b7ten", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "$(", "KOUS", "PIS", "VMFIN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Je fester eine Burg/ ie st\u00e4rcker sie bestritten:", "tokens": ["Je", "fes\u00b7ter", "ei\u00b7ne", "Burg", "/", "ie", "st\u00e4r\u00b7cker", "sie", "be\u00b7strit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$(", "ADV", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.23": {"line.1": {"text": "Was hilfft es wenn man sie bestritten sonder Frucht!", "tokens": ["Was", "hilfft", "es", "wenn", "man", "sie", "be\u00b7strit\u00b7ten", "son\u00b7der", "Frucht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "KOUS", "PIS", "PPER", "VVFIN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.24": {"line.1": {"text": "Man f\u00e4ngt die Hinde nicht/ als auf gejagter Flucht.", "tokens": ["Man", "f\u00e4ngt", "die", "Hin\u00b7de", "nicht", "/", "als", "auf", "ge\u00b7jag\u00b7ter", "Flucht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "PTKNEG", "$(", "KOKOM", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.25": {"line.1": {"text": "Solt' ich so lange Zeit der stoltzen G\u00fcte dienen:", "tokens": ["Solt'", "ich", "so", "lan\u00b7ge", "Zeit", "der", "stolt\u00b7zen", "G\u00fc\u00b7te", "die\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADJA", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.26": {"line.1": {"text": "Solt' ich/ wenn ich verliebt zu schlaffen mich erk\u00fchnen.", "tokens": ["Solt'", "ich", "/", "wenn", "ich", "ver\u00b7liebt", "zu", "schlaf\u00b7fen", "mich", "er\u00b7k\u00fch\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$(", "KOUS", "PPER", "VVPP", "PTKZU", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.27": {"line.1": {"text": "Wo aber denckst du hin: Hier taugt dein Singen nicht/", "tokens": ["Wo", "a\u00b7ber", "denckst", "du", "hin", ":", "Hier", "taugt", "dein", "Sin\u00b7gen", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$.", "ADV", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.28": {"line.1": {"text": "Die Musen geben mir was andern noch gebricht.", "tokens": ["Die", "Mu\u00b7sen", "ge\u00b7ben", "mir", "was", "an\u00b7dern", "noch", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PRELS", "PIS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.29": {"line.1": {"text": "Ach/ armer/ ach/ hier gilt kein Juncker von der Feder:", "tokens": ["Ach", "/", "ar\u00b7mer", "/", "ach", "/", "hier", "gilt", "kein", "Jun\u00b7cker", "von", "der", "Fe\u00b7der", ":"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$(", "ADJA", "$(", "XY", "$(", "ADV", "VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.30": {"line.1": {"text": "Man legt nach langem Krieg das Eisen von dem Leder.", "tokens": ["Man", "legt", "nach", "lan\u00b7gem", "Krieg", "das", "Ei\u00b7sen", "von", "dem", "Le\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ADJA", "NN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.31": {"line.1": {"text": "Die Sp\u00f6rner Klingen nicht wie Peruaner Gold:", "tokens": ["Die", "Sp\u00f6r\u00b7ner", "Klin\u00b7gen", "nicht", "wie", "Pe\u00b7ru\u00b7a\u00b7ner", "Gold", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKNEG", "KOKOM", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.32": {"line.1": {"text": "Di\u00df zehlt mein Daphnis nicht: Doch werd' ihm", "tokens": ["Di\u00df", "zehlt", "mein", "Daph\u00b7nis", "nicht", ":", "Doch", "werd'", "ihm"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$.", "KON", "VAFIN", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Ja ", "tokens": ["Ja"], "token_info": ["word"], "pos": ["PTKANT"], "meter": "+", "measure": "single.up"}}, "stanza.34": {"line.1": {"text": "Es geh mir/ wie es geh: Ich wil es einmahl wagen.", "tokens": ["Es", "geh", "mir", "/", "wie", "es", "geh", ":", "Ich", "wil", "es", "ein\u00b7mahl", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$(", "PWAV", "PPER", "VVFIN", "$.", "PPER", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.35": {"line.1": {"text": "Wofern du Wagen hast die mit vier Rossen gehn:", "tokens": ["Wo\u00b7fern", "du", "Wa\u00b7gen", "hast", "die", "mit", "vier", "Ros\u00b7sen", "gehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VAFIN", "ART", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.36": {"line.1": {"text": "Als must' ein iedes Hau\u00df voll Woll' und Leinwand stehn.", "tokens": ["Als", "must'", "ein", "ie\u00b7des", "Hau\u00df", "voll", "Woll'", "und", "Lein\u00b7wand", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VMFIN", "ART", "PIAT", "NN", "ADJD", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.37": {"line.1": {"text": "Wo aber zielst du hin/ nach Osten oder Westen?", "tokens": ["Wo", "a\u00b7ber", "zielst", "du", "hin", "/", "nach", "Os\u00b7ten", "o\u00b7der", "Wes\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$(", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.38": {"line.1": {"text": "Ich wehle hier und dar/ und ziele nach der besten.", "tokens": ["Ich", "weh\u00b7le", "hier", "und", "dar", "/", "und", "zie\u00b7le", "nach", "der", "bes\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "KON", "PTKVZ", "$(", "KON", "VVFIN", "APPR", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.39": {"line.1": {"text": "Der besten wie du meinst/ doch wehle mit bedacht:", "tokens": ["Der", "bes\u00b7ten", "wie", "du", "meinst", "/", "doch", "weh\u00b7le", "mit", "be\u00b7dacht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "KOKOM", "PPER", "ADV", "$(", "ADV", "VVFIN", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.40": {"line.1": {"text": "Die mich bey Tag' erquickt/ und fr\u00f6lich sey bey Nacht.", "tokens": ["Die", "mich", "bey", "Tag'", "er\u00b7quickt", "/", "und", "fr\u00f6\u00b7lich", "sey", "bey", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVPP", "$(", "KON", "ADJD", "VAFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.41": {"line.1": {"text": "Du sitzest unten an/ wo sie von h\u00f6herm Blute:", "tokens": ["Du", "sit\u00b7zest", "un\u00b7ten", "an", "/", "wo", "sie", "von", "h\u00f6\u00b7herm", "Blu\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$(", "PWAV", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.42": {"line.1": {"text": "Ich liebe die/ von der ich nicht den Wahn vermuthe.", "tokens": ["Ich", "lie\u00b7be", "die", "/", "von", "der", "ich", "nicht", "den", "Wahn", "ver\u00b7mu\u00b7the", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "$(", "APPR", "PRELS", "PPER", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.43": {"line.1": {"text": "Die man f\u00fcr sch\u00f6ne sch\u00e4tzt: Kennt ihrer Farben Prei\u00df:", "tokens": ["Die", "man", "f\u00fcr", "sch\u00f6\u00b7ne", "sch\u00e4tzt", ":", "Kennt", "ih\u00b7rer", "Far\u00b7ben", "Prei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "APPR", "ADJA", "VVFIN", "$.", "VVFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.44": {"line.1": {"text": "Seh' ich was he\u00dflichs an/ so schwitz' ich kalten Schwei\u00df.", "tokens": ["Seh'", "ich", "was", "he\u00df\u00b7lichs", "an", "/", "so", "schwitz'", "ich", "kal\u00b7ten", "Schwei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "VVFIN", "PTKVZ", "$(", "ADV", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.45": {"line.1": {"text": "Geberden k\u00f6nnen offt was h\u00e4\u00dflich/ sch\u00f6ne machen:", "tokens": ["Ge\u00b7ber\u00b7den", "k\u00f6n\u00b7nen", "offt", "was", "h\u00e4\u00df\u00b7lich", "/", "sch\u00f6\u00b7ne", "ma\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "PWS", "ADJD", "$(", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.46": {"line.1": {"text": "Die h\u00e4\u00dflich/ w\u00fcrde sich bey mir nicht sch\u00f6ne lachen", "tokens": ["Die", "h\u00e4\u00df\u00b7lich", "/", "w\u00fcr\u00b7de", "sich", "bey", "mir", "nicht", "sch\u00f6\u00b7ne", "la\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "$(", "VAFIN", "PRF", "APPR", "PPER", "PTKNEG", "ADJA", "VVINF"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.47": {"line.1": {"text": "Ich fragte mehr vor mich nach frommer Eltern Kind:", "tokens": ["Ich", "frag\u00b7te", "mehr", "vor", "mich", "nach", "from\u00b7mer", "El\u00b7tern", "Kind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PRF", "APPR", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.48": {"line.1": {"text": "W\u00e4r' es nicht selber fromm: Ist jener fromm-seyn/ Wind.", "tokens": ["W\u00e4r'", "es", "nicht", "sel\u00b7ber", "fromm", ":", "Ist", "je\u00b7ner", "from\u00b7mseyn", "/", "Wind", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "$.", "VAFIN", "PDAT", "NN", "$(", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.49": {"line.1": {"text": "Die mu\u00df ja z\u00fcchtig seyn/ die z\u00fcchtig ist gezeuget:", "tokens": ["Die", "mu\u00df", "ja", "z\u00fcch\u00b7tig", "seyn", "/", "die", "z\u00fcch\u00b7tig", "ist", "ge\u00b7zeu\u00b7get", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ADJD", "VAINF", "$(", "ART", "ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.50": {"line.1": {"text": "Mein wehrter Hertzens-Freund/ auch diese Regel treuget.", "tokens": ["Mein", "wehr\u00b7ter", "Hert\u00b7zens\u00b7Freund", "/", "auch", "die\u00b7se", "Re\u00b7gel", "treu\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "ADV", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.51": {"line.1": {"text": "Du lobst denn die man hat verzettelt auf dem Heu:", "tokens": ["Du", "lobst", "denn", "die", "man", "hat", "ver\u00b7zet\u00b7telt", "auf", "dem", "Heu", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "PIS", "VAFIN", "VVPP", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.52": {"line.1": {"text": "Die gibt f\u00fcr gleiches Vieh ein' angenehme Streu.", "tokens": ["Die", "gibt", "f\u00fcr", "glei\u00b7ches", "Vieh", "ein'", "an\u00b7ge\u00b7neh\u00b7me", "Streu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.53": {"line.1": {"text": "Die Jungfern k\u00f6nnen ietzt wol ander unterstreuen:", "tokens": ["Die", "Jung\u00b7fern", "k\u00f6n\u00b7nen", "ietzt", "wol", "an\u00b7der", "un\u00b7ter\u00b7streu\u00b7en", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.54": {"line.1": {"text": "Die zuviel unterstreut wird endlich Ochsen treuen.", "tokens": ["Die", "zu\u00b7viel", "un\u00b7ter\u00b7streut", "wird", "end\u00b7lich", "Och\u00b7sen", "treu\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVPP", "VAFIN", "ADV", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.55": {"line.1": {"text": "Die Weissen sind offt sieh wenn sie nicht stets purgiert:", "tokens": ["Die", "Weis\u00b7sen", "sind", "offt", "sieh", "wenn", "sie", "nicht", "stets", "pur\u00b7giert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "KOUS", "PPER", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}, "stanza.56": {"line.1": {"text": "Bey schwartzen w\u00fcrde mir viel Seiffen-Geld verschmiert.", "tokens": ["Bey", "schwart\u00b7zen", "w\u00fcr\u00b7de", "mir", "viel", "Seif\u00b7fen\u00b7Geld", "ver\u00b7schmiert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.57": {"line.1": {"text": "Ein Leib der braun und starck kan starcke P\u00fcffe tragen:", "tokens": ["Ein", "Leib", "der", "braun", "und", "starck", "kan", "star\u00b7cke", "P\u00fcf\u00b7fe", "tra\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJD", "KON", "NN", "VMFIN", "VVFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.58": {"line.1": {"text": "Du denckst ich werde mich/ als wie mit Hunden schlagen?", "tokens": ["Du", "denckst", "ich", "wer\u00b7de", "mich", "/", "als", "wie", "mit", "Hun\u00b7den", "schla\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VAFIN", "PPER", "$(", "KOUS", "KOKOM", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.59": {"line.1": {"text": "Kein Esel/ Glock und Weib sind sonder Schl\u00e4ge gut:", "tokens": ["Kein", "E\u00b7sel", "/", "Glock", "und", "Weib", "sind", "son\u00b7der", "Schl\u00e4\u00b7ge", "gut", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$(", "NN", "KON", "NN", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.60": {"line.1": {"text": "Der mu\u00df ein Esel seyn/ wer tobt auf Frauen-Blut.", "tokens": ["Der", "mu\u00df", "ein", "E\u00b7sel", "seyn", "/", "wer", "tobt", "auf", "Frau\u00b7en\u00b7Blut", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "NN", "VAINF", "$(", "PWS", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.61": {"line.1": {"text": "Nimm eine zarte denn/ die darffst du nicht ber\u00fchren.", "tokens": ["Nimm", "ei\u00b7ne", "zar\u00b7te", "denn", "/", "die", "darffst", "du", "nicht", "be\u00b7r\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ADJA", "KON", "$(", "ART", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.62": {"line.1": {"text": "N\u00fcrnberger Gut l\u00e4st sich auf alle M\u00e4rckte f\u00fchren.", "tokens": ["N\u00fcrn\u00b7ber\u00b7ger", "Gut", "l\u00e4st", "sich", "auf", "al\u00b7le", "M\u00e4rck\u00b7te", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "PRF", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "+--+---+-+-+-", "measure": "dactylic.di.plus"}}, "stanza.63": {"line.1": {"text": "Ich hielte viel von der die Tugend schweigen lehrt.", "tokens": ["Ich", "hiel\u00b7te", "viel", "von", "der", "die", "Tu\u00b7gend", "schwei\u00b7gen", "lehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PRELS", "ART", "NN", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.64": {"line.1": {"text": "Was n\u00fctzte mir der Block der keinmahl wird geh\u00f6rt.", "tokens": ["Was", "n\u00fctz\u00b7te", "mir", "der", "Block", "der", "kein\u00b7mahl", "wird", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ART", "NN", "ART", "ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.65": {"line.1": {"text": "Ich kenne manche wohl/ die gantze Monden brummen.", "tokens": ["Ich", "ken\u00b7ne", "man\u00b7che", "wohl", "/", "die", "gant\u00b7ze", "Mon\u00b7den", "brum\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ADV", "$(", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.66": {"line.1": {"text": "Ich kenne manche wohl/ die auf ein Jahr verstummen.", "tokens": ["Ich", "ken\u00b7ne", "man\u00b7che", "wohl", "/", "die", "auf", "ein", "Jahr", "ver\u00b7stum\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ADV", "$(", "ART", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.67": {"line.1": {"text": "Nimm einen Hasel-Ast/ der ist daf\u00fcr bewehrt/", "tokens": ["Nimm", "ei\u00b7nen", "Ha\u00b7sel\u00b7Ast", "/", "der", "ist", "da\u00b7f\u00fcr", "be\u00b7wehrt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$(", "ART", "VAFIN", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.68": {"line.1": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}}, "stanza.69": {"line.1": {"text": "So kan sie zu dem Schatz das Besemgeld erspahren.", "tokens": ["So", "kan", "sie", "zu", "dem", "Schatz", "das", "Be\u00b7sem\u00b7geld", "er\u00b7spah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.70": {"line.1": {"text": "Ihr Geld wird ohne di\u00df f\u00fcr tausend Teuffel fahren.", "tokens": ["Ihr", "Geld", "wird", "oh\u00b7ne", "di\u00df", "f\u00fcr", "tau\u00b7send", "Teuf\u00b7fel", "fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "PDS", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.71": {"line.1": {"text": "Wir kommen von dem Zweck. Man sagt/ wer h\u00e4lt der hegt.", "tokens": ["Wir", "kom\u00b7men", "von", "dem", "Zweck", ".", "Man", "sagt", "/", "wer", "h\u00e4lt", "der", "hegt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$.", "PIS", "VVFIN", "$(", "PWS", "VVFIN", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.72": {"line.1": {"text": "Nur da\u00df man nicht der Magd vors Brodt drey Schl\u00f6sser legt.", "tokens": ["Nur", "da\u00df", "man", "nicht", "der", "Magd", "vors", "Brodt", "drey", "Schl\u00f6s\u00b7ser", "legt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PIS", "PTKNEG", "ART", "NN", "APPRART", "NN", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.73": {"line.1": {"text": "Soll eine Reiche dich mit ihren G\u00fclden laben?", "tokens": ["Soll", "ei\u00b7ne", "Rei\u00b7che", "dich", "mit", "ih\u00b7ren", "G\u00fcl\u00b7den", "la\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NE", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.74": {"line.1": {"text": "Die sch\u00e4tz' ich dir vergn\u00fcgt/ und mich allein wil haben.", "tokens": ["Die", "sch\u00e4tz'", "ich", "dir", "ver\u00b7gn\u00fcgt", "/", "und", "mich", "al\u00b7lein", "wil", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PPER", "VVPP", "$(", "KON", "PPER", "ADV", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.75": {"line.1": {"text": "Die Weisheit kommt was hoch! Soll sie denn lustig seyn?", "tokens": ["Die", "Weis\u00b7heit", "kommt", "was", "hoch", "!", "Soll", "sie", "denn", "lus\u00b7tig", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PWS", "ADJD", "$.", "VMFIN", "PPER", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.76": {"line.1": {"text": "Ein immer-traurig Weib ist wie versaurter Wein.", "tokens": ["Ein", "im\u00b7mer\u00b7trau\u00b7rig", "Weib", "ist", "wie", "ver\u00b7saur\u00b7ter", "Wein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "VAFIN", "KOKOM", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.77": {"line.1": {"text": "Ich kenne/ die verstehn/ wie die Claviere klingen/", "tokens": ["Ich", "ken\u00b7ne", "/", "die", "ver\u00b7stehn", "/", "wie", "die", "Cla\u00b7vie\u00b7re", "klin\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "ART", "VVINF", "$(", "KOKOM", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.78": {"line.1": {"text": "Die k\u00f6nnen mit der Zeit ein Ninno Josephs singen.", "tokens": ["Die", "k\u00f6n\u00b7nen", "mit", "der", "Zeit", "ein", "Nin\u00b7no", "Jo\u00b7se\u00b7phs", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "APPR", "ART", "NN", "ART", "NN", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}}, "stanza.79": {"line.1": {"text": "Und denen Aretin und ", "tokens": ["Und", "de\u00b7nen", "A\u00b7re\u00b7tin", "und"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PDS", "NN", "KON"], "meter": "-+-+---", "measure": "unknown.measure.di"}}, "stanza.80": {"line.1": {"text": "Die sind zu klug/ mein Freund/ vor mich und meinen Stand.", "tokens": ["Die", "sind", "zu", "klug", "/", "mein", "Freund", "/", "vor", "mich", "und", "mei\u00b7nen", "Stand", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKA", "ADJD", "$(", "PPOSAT", "NN", "$(", "APPR", "PPER", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.81": {"line.1": {"text": "D\u00fcrfft' ein' auf gute Treu sich dir wol selbst anbieten?", "tokens": ["D\u00fcrfft'", "ein'", "auf", "gu\u00b7te", "Treu", "sich", "dir", "wol", "selbst", "an\u00b7bie\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "ADJA", "NN", "PRF", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "---+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.82": {"line.1": {"text": "Der w\u00e4re wol der Kopff gespalten in der Mitten.", "tokens": ["Der", "w\u00e4\u00b7re", "wol", "der", "Kopff", "ge\u00b7spal\u00b7ten", "in", "der", "Mit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN", "VVPP", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.83": {"line.1": {"text": "Was Rath denn/ wenn sie dir stets ihren Kram versagt.", "tokens": ["Was", "Rath", "denn", "/", "wenn", "sie", "dir", "stets", "ih\u00b7ren", "Kram", "ver\u00b7sagt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "KON", "$(", "KOUS", "PPER", "PPER", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.84": {"line.1": {"text": "Denn Liese gute Nacht und Sylvia gefragt.", "tokens": ["Denn", "Lie\u00b7se", "gu\u00b7te", "Nacht", "und", "Syl\u00b7via", "ge\u00b7fragt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJA", "NN", "KON", "NE", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.85": {"line.1": {"text": "Wenn sich der Zeug verliegt/ pflegt man bald lo\u00dfzuschlagen.", "tokens": ["Wenn", "sich", "der", "Zeug", "ver\u00b7liegt", "/", "pflegt", "man", "bald", "lo\u00df\u00b7zu\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "VVPP", "$(", "VVFIN", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.86": {"line.1": {"text": "Die zu viel Jahre zehlt/ wei\u00df gar zu viel zu sagen.", "tokens": ["Die", "zu", "viel", "Jah\u00b7re", "zehlt", "/", "wei\u00df", "gar", "zu", "viel", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "VVFIN", "$(", "VVFIN", "ADV", "PTKA", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.87": {"line.1": {"text": "Was wei\u00df ein Kindisch Kind/ das noch mit Tocken laufft.", "tokens": ["Was", "wei\u00df", "ein", "Kin\u00b7disch", "Kind", "/", "das", "noch", "mit", "To\u00b7cken", "laufft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "NN", "$(", "PDS", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.88": {"line.1": {"text": "Was unreiff acht ich nicht/ was faul wird nicht verkaufft.", "tokens": ["Was", "un\u00b7reiff", "acht", "ich", "nicht", "/", "was", "faul", "wird", "nicht", "ver\u00b7kaufft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "CARD", "PPER", "PTKNEG", "$(", "PWS", "ADJD", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.89": {"line.1": {"text": "Bey Wittben findet man bestellte K\u00fcch' und Keller.", "tokens": ["Bey", "Witt\u00b7ben", "fin\u00b7det", "man", "be\u00b7stell\u00b7te", "K\u00fc\u00b7ch'", "und", "Kel\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}}, "stanza.90": {"line.1": {"text": "Man freyt die Wittben wol/ man freyt auch ihre Heller.", "tokens": ["Man", "freyt", "die", "Witt\u00b7ben", "wol", "/", "man", "freyt", "auch", "ih\u00b7re", "Hel\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "ADV", "$(", "PIS", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.91": {"line.1": {"text": "Wird iemand drum verdacht? Sie sitzen warm und fest.", "tokens": ["Wird", "ie\u00b7mand", "drum", "ver\u00b7dacht", "?", "Sie", "sit\u00b7zen", "warm", "und", "fest", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PAV", "VVPP", "$.", "PPER", "VVFIN", "ADJD", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.92": {"line.1": {"text": "Zu fest auch wol f\u00fcr mich/ die erste Treu die best.", "tokens": ["Zu", "fest", "auch", "wol", "f\u00fcr", "mich", "/", "die", "ers\u00b7te", "Treu", "die", "best", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "ADV", "ADV", "APPR", "PPER", "$(", "ART", "ADJA", "NN", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}, "stanza.93": {"line.1": {"text": "Sie leben bey Verstand/ und haben was erfahren.", "tokens": ["Sie", "le\u00b7ben", "bey", "Ver\u00b7stand", "/", "und", "ha\u00b7ben", "was", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$(", "KON", "VAFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.94": {"line.1": {"text": "Gott woll' uns f\u00fcr und f\u00fcr vor dem Verstand bewahren.", "tokens": ["Gott", "woll'", "uns", "f\u00fcr", "und", "f\u00fcr", "vor", "dem", "Ver\u00b7stand", "be\u00b7wah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "APPR", "KON", "APPR", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.95": {"line.1": {"text": "So taugt dir keine nicht die ihren Mann beklagt?", "tokens": ["So", "taugt", "dir", "kei\u00b7ne", "nicht", "die", "ih\u00b7ren", "Mann", "be\u00b7klagt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "PTKNEG", "ART", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.96": {"line.1": {"text": "Sie ehr' ich. Doch ich lieb' ein unbefleckte Magd.", "tokens": ["Sie", "ehr'", "ich", ".", "Doch", "ich", "lieb'", "ein", "un\u00b7be\u00b7fleck\u00b7te", "Magd", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "KON", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.97": {"line.1": {"text": "Nimm was du wilst. Ich wil die Zeit allein vertreiben.", "tokens": ["Nimm", "was", "du", "wilst", ".", "Ich", "wil", "die", "Zeit", "al\u00b7lein", "ver\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PWS", "PPER", "VMFIN", "$.", "PPER", "VMFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.98": {"line.1": {"text": "So wilst du f\u00fcr und f\u00fcr ein Vesper-Knecht verbleiben?", "tokens": ["So", "wilst", "du", "f\u00fcr", "und", "f\u00fcr", "ein", "Ves\u00b7per\u00b7Knecht", "ver\u00b7blei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "KON", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.99": {"line.1": {"text": "Wer einsam ist/ vertreibt die Zeit in h\u00f6chster Ruh.", "tokens": ["Wer", "ein\u00b7sam", "ist", "/", "ver\u00b7treibt", "die", "Zeit", "in", "h\u00f6chs\u00b7ter", "Ruh", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$(", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.100": {"line.1": {"text": "Wer so verschimmelt bringt die Zeit gar \u00fcbel zu.", "tokens": ["Wer", "so", "ver\u00b7schim\u00b7melt", "bringt", "die", "Zeit", "gar", "\u00fc\u00b7bel", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVPP", "VVFIN", "ART", "NN", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.101": {"line.1": {"text": "Verschimmel' ich/ so putzt mich ab mit Flederwischen.", "tokens": ["Ver\u00b7schim\u00b7mel'", "ich", "/", "so", "putzt", "mich", "ab", "mit", "Fle\u00b7der\u00b7wi\u00b7schen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$(", "ADV", "VVFIN", "PPER", "PTKVZ", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.102": {"line.1": {"text": "Wohl mir/ wenn die mir lieb wird meinen Brand erfrischen.", "tokens": ["Wohl", "mir", "/", "wenn", "die", "mir", "lieb", "wird", "mei\u00b7nen", "Brand", "er\u00b7fri\u00b7schen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$(", "KOUS", "ART", "PPER", "ADJD", "VAFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.103": {"line.1": {"text": "Man glaubt da\u00df um die Zeit der heissen Sonnen-wende", "tokens": ["Man", "glaubt", "da\u00df", "um", "die", "Zeit", "der", "heis\u00b7sen", "Son\u00b7nen\u00b7wen\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "KOUS", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Bl\u00e4tter gr\u00fcne Tracht an B\u00e4umen um-sich-kehr/", "tokens": ["Der", "Bl\u00e4t\u00b7ter", "gr\u00fc\u00b7ne", "Tracht", "an", "B\u00e4u\u00b7men", "um\u00b7sich\u00b7kehr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPR", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr seht/ da\u00df eine Frau itzt Leid vor Lust versende/", "tokens": ["Ihr", "seht", "/", "da\u00df", "ei\u00b7ne", "Frau", "itzt", "Leid", "vor", "Lust", "ver\u00b7sen\u00b7de", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KOUS", "ART", "NN", "ADV", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und Profin euer Hertz vor R\u00fcels Grab begehr.", "tokens": ["Und", "Pro\u00b7fin", "eu\u00b7er", "Hertz", "vor", "R\u00fcels", "Grab", "be\u00b7gehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPOSAT", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Drum kehrt den Schlu\u00df euch um/ zu leben stets allein/", "tokens": ["Drum", "kehrt", "den", "Schlu\u00df", "euch", "um", "/", "zu", "le\u00b7ben", "stets", "al\u00b7lein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "PPER", "APPR", "$(", "PTKZU", "VVINF", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und sucht/ mein Freund/ itzt Mann/ doch Vater bald zu seyn.", "tokens": ["Und", "sucht", "/", "mein", "Freund", "/", "itzt", "Mann", "/", "doch", "Va\u00b7ter", "bald", "zu", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "PPOSAT", "NN", "$(", "ADV", "NN", "$(", "ADV", "NN", "ADV", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.104": {"line.1": {"text": "Dameta fehlt dir was? Wie siehst du so betr\u00fcbet?", "tokens": ["Da\u00b7me\u00b7ta", "fehlt", "dir", "was", "?", "Wie", "siehst", "du", "so", "be\u00b7tr\u00fc\u00b7bet", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PIS", "$.", "PWAV", "VVFIN", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.105": {"line.1": {"text": "Mir fehlt nur mehr denn viel: Mich d\u00fcnckt ich sey verliebet:", "tokens": ["Mir", "fehlt", "nur", "mehr", "denn", "viel", ":", "Mich", "d\u00fcnckt", "ich", "sey", "ver\u00b7lie\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADV", "ADV", "$.", "PPER", "VVFIN", "PPER", "VAFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.106": {"line.1": {"text": "Das lasse Pan nicht zu! welch Ubel steckt dich an!", "tokens": ["Das", "las\u00b7se", "Pan", "nicht", "zu", "!", "welch", "U\u00b7bel", "steckt", "dich", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "PTKVZ", "$.", "PWAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.107": {"line.1": {"text": "Der mu\u00df ein Unmensch seyn/ der nun nicht lieben kan!", "tokens": ["Der", "mu\u00df", "ein", "Un\u00b7mensch", "seyn", "/", "der", "nun", "nicht", "lie\u00b7ben", "kan", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "NN", "VAINF", "$(", "ART", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.108": {"line.1": {"text": "Di\u00df Wort schmeckt lauter Gifft/ die greifft dir nach dem Hertzen.", "tokens": ["Di\u00df", "Wort", "schmeckt", "lau\u00b7ter", "Gifft", "/", "die", "greifft", "dir", "nach", "dem", "Hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VVFIN", "PIAT", "NN", "$(", "ART", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.109": {"line.1": {"text": "Mein Thyrsis ich vergeh' in bitter-s\u00fcssen Schmertzen.", "tokens": ["Mein", "Thyr\u00b7sis", "ich", "ver\u00b7geh'", "in", "bit\u00b7ter\u00b7s\u00fcs\u00b7sen", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.110": {"line.1": {"text": "Sprich Aertzt' um Mittel an: Vers\u00e4ume keine Zeit:", "tokens": ["Sprich", "A\u00b7ertzt'", "um", "Mit\u00b7tel", "an", ":", "Ver\u00b7s\u00e4u\u00b7me", "kei\u00b7ne", "Zeit", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "APPR", "NN", "PTKVZ", "$.", "NN", "PIAT", "NN", "$."], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}}, "stanza.111": {"line.1": {"text": "Die Kr\u00e4uter lindern nicht der Plagen Hefftigkeit.", "tokens": ["Die", "Kr\u00e4u\u00b7ter", "lin\u00b7dern", "nicht", "der", "Pla\u00b7gen", "Heff\u00b7tig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.112": {"line.1": {"text": "So wilt du sonder Rath in deinem Wahnwitz sterben?", "tokens": ["So", "wilt", "du", "son\u00b7der", "Rath", "in", "dei\u00b7nem", "Wahn\u00b7witz", "ster\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.113": {"line.1": {"text": "Ein Mittel wei\u00df ich noch/ ach w\u00e4r' es zu erwerben.", "tokens": ["Ein", "Mit\u00b7tel", "wei\u00df", "ich", "noch", "/", "ach", "w\u00e4r'", "es", "zu", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "$(", "XY", "VAFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.114": {"line.1": {"text": "Zw\u00f6lff Schaafe setz' ich drauf/ wo ich dich retten kan:", "tokens": ["Zw\u00f6lff", "Schaa\u00b7fe", "setz'", "ich", "drauf", "/", "wo", "ich", "dich", "ret\u00b7ten", "kan", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "PPER", "PAV", "$(", "PWAV", "PPER", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.115": {"line.1": {"text": "Ach! blickte Charis mich nur etwas freundlich an.", "tokens": ["Ach", "!", "blick\u00b7te", "Cha\u00b7ris", "mich", "nur", "et\u00b7was", "freund\u00b7lich", "an", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "NE", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.116": {"line.1": {"text": "Di\u00df Mittel ist f\u00fcrwahr weit \u00e4rger als dein Leiden.", "tokens": ["Di\u00df", "Mit\u00b7tel", "ist", "f\u00fcr\u00b7wahr", "weit", "\u00e4r\u00b7ger", "als", "dein", "Lei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "KOKOM", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.117": {"line.1": {"text": "Hilfft Charis nicht/ so mu\u00df ich Welt und Leben meiden.", "tokens": ["Hilfft", "Cha\u00b7ris", "nicht", "/", "so", "mu\u00df", "ich", "Welt", "und", "Le\u00b7ben", "mei\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PTKNEG", "$(", "ADV", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.118": {"line.1": {"text": "Begehrst du denn ein Weib ein lebend Creutz ins Haus:", "tokens": ["Be\u00b7gehrst", "du", "denn", "ein", "Weib", "ein", "le\u00b7bend", "Creutz", "ins", "Haus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "ART", "ADJD", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.119": {"line.1": {"text": "Di\u00df Creutz alleine jagt die b\u00f6sen Geister aus.", "tokens": ["Di\u00df", "Creutz", "al\u00b7lei\u00b7ne", "jagt", "die", "b\u00f6\u00b7sen", "Geis\u00b7ter", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "ADV", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.120": {"line.1": {"text": "Dafern man gl\u00e4ubt da\u00df Arg mit Argen zu vertreiben:", "tokens": ["Da\u00b7fern", "man", "gl\u00e4ubt", "da\u00df", "Arg", "mit", "Ar\u00b7gen", "zu", "ver\u00b7trei\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "KOUS", "NE", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.121": {"line.1": {"text": "Was gut/ gesellet sich: Arg mu\u00df alleine bleiben.", "tokens": ["Was", "gut", "/", "ge\u00b7sel\u00b7let", "sich", ":", "Arg", "mu\u00df", "al\u00b7lei\u00b7ne", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "$(", "VVFIN", "PRF", "$.", "NE", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.122": {"line.1": {"text": "Eh ich ein Weib begehr'/ eh w\u00fcntsch' ich mir den Tod/", "tokens": ["Eh", "ich", "ein", "Weib", "be\u00b7gehr'", "/", "eh", "w\u00fcnt\u00b7sch'", "ich", "mir", "den", "Tod", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$(", "KOUS", "VVFIN", "PPER", "PPER", "ART", "NN", "$("], "meter": "+--+-+--+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.123": {"line.1": {"text": "Und ich find' ohne Weib mich in der h\u00f6chsten Noth.", "tokens": ["Und", "ich", "find'", "oh\u00b7ne", "Weib", "mich", "in", "der", "h\u00f6chs\u00b7ten", "Noth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "NN", "PRF", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.124": {"line.1": {"text": "Wie schwer ists/ wenn man sol der Jungfern Gunst erbitten:", "tokens": ["Wie", "schwer", "ists", "/", "wenn", "man", "sol", "der", "Jung\u00b7fern", "Gunst", "er\u00b7bit\u00b7ten", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "$(", "KOUS", "PIS", "VMFIN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.125": {"line.1": {"text": "Je fester eine Burg/ ie st\u00e4rcker sie bestritten:", "tokens": ["Je", "fes\u00b7ter", "ei\u00b7ne", "Burg", "/", "ie", "st\u00e4r\u00b7cker", "sie", "be\u00b7strit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$(", "ADV", "ADJD", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.126": {"line.1": {"text": "Was hilfft es wenn man sie bestritten sonder Frucht!", "tokens": ["Was", "hilfft", "es", "wenn", "man", "sie", "be\u00b7strit\u00b7ten", "son\u00b7der", "Frucht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "KOUS", "PIS", "PPER", "VVFIN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.127": {"line.1": {"text": "Man f\u00e4ngt die Hinde nicht/ als auf gejagter Flucht.", "tokens": ["Man", "f\u00e4ngt", "die", "Hin\u00b7de", "nicht", "/", "als", "auf", "ge\u00b7jag\u00b7ter", "Flucht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "PTKNEG", "$(", "KOKOM", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.128": {"line.1": {"text": "Solt' ich so lange Zeit der stoltzen G\u00fcte dienen:", "tokens": ["Solt'", "ich", "so", "lan\u00b7ge", "Zeit", "der", "stolt\u00b7zen", "G\u00fc\u00b7te", "die\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADJA", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.129": {"line.1": {"text": "Solt' ich/ wenn ich verliebt zu schlaffen mich erk\u00fchnen.", "tokens": ["Solt'", "ich", "/", "wenn", "ich", "ver\u00b7liebt", "zu", "schlaf\u00b7fen", "mich", "er\u00b7k\u00fch\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$(", "KOUS", "PPER", "VVPP", "PTKZU", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.130": {"line.1": {"text": "Wo aber denckst du hin: Hier taugt dein Singen nicht/", "tokens": ["Wo", "a\u00b7ber", "denckst", "du", "hin", ":", "Hier", "taugt", "dein", "Sin\u00b7gen", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$.", "ADV", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.131": {"line.1": {"text": "Die Musen geben mir was andern noch gebricht.", "tokens": ["Die", "Mu\u00b7sen", "ge\u00b7ben", "mir", "was", "an\u00b7dern", "noch", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PRELS", "PIS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.132": {"line.1": {"text": "Ach/ armer/ ach/ hier gilt kein Juncker von der Feder:", "tokens": ["Ach", "/", "ar\u00b7mer", "/", "ach", "/", "hier", "gilt", "kein", "Jun\u00b7cker", "von", "der", "Fe\u00b7der", ":"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$(", "ADJA", "$(", "XY", "$(", "ADV", "VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.133": {"line.1": {"text": "Man legt nach langem Krieg das Eisen von dem Leder.", "tokens": ["Man", "legt", "nach", "lan\u00b7gem", "Krieg", "das", "Ei\u00b7sen", "von", "dem", "Le\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ADJA", "NN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.134": {"line.1": {"text": "Die Sp\u00f6rner Klingen nicht wie Peruaner Gold:", "tokens": ["Die", "Sp\u00f6r\u00b7ner", "Klin\u00b7gen", "nicht", "wie", "Pe\u00b7ru\u00b7a\u00b7ner", "Gold", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKNEG", "KOKOM", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.135": {"line.1": {"text": "Di\u00df zehlt mein Daphnis nicht: Doch werd' ihm", "tokens": ["Di\u00df", "zehlt", "mein", "Daph\u00b7nis", "nicht", ":", "Doch", "werd'", "ihm"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$.", "KON", "VAFIN", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.136": {"line.1": {"text": "Ja ", "tokens": ["Ja"], "token_info": ["word"], "pos": ["PTKANT"], "meter": "+", "measure": "single.up"}}, "stanza.137": {"line.1": {"text": "Es geh mir/ wie es geh: Ich wil es einmahl wagen.", "tokens": ["Es", "geh", "mir", "/", "wie", "es", "geh", ":", "Ich", "wil", "es", "ein\u00b7mahl", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$(", "PWAV", "PPER", "VVFIN", "$.", "PPER", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.138": {"line.1": {"text": "Wofern du Wagen hast die mit vier Rossen gehn:", "tokens": ["Wo\u00b7fern", "du", "Wa\u00b7gen", "hast", "die", "mit", "vier", "Ros\u00b7sen", "gehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VAFIN", "ART", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.139": {"line.1": {"text": "Als must' ein iedes Hau\u00df voll Woll' und Leinwand stehn.", "tokens": ["Als", "must'", "ein", "ie\u00b7des", "Hau\u00df", "voll", "Woll'", "und", "Lein\u00b7wand", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VMFIN", "ART", "PIAT", "NN", "ADJD", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.140": {"line.1": {"text": "Wo aber zielst du hin/ nach Osten oder Westen?", "tokens": ["Wo", "a\u00b7ber", "zielst", "du", "hin", "/", "nach", "Os\u00b7ten", "o\u00b7der", "Wes\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$(", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.141": {"line.1": {"text": "Ich wehle hier und dar/ und ziele nach der besten.", "tokens": ["Ich", "weh\u00b7le", "hier", "und", "dar", "/", "und", "zie\u00b7le", "nach", "der", "bes\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "KON", "PTKVZ", "$(", "KON", "VVFIN", "APPR", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.142": {"line.1": {"text": "Der besten wie du meinst/ doch wehle mit bedacht:", "tokens": ["Der", "bes\u00b7ten", "wie", "du", "meinst", "/", "doch", "weh\u00b7le", "mit", "be\u00b7dacht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "KOKOM", "PPER", "ADV", "$(", "ADV", "VVFIN", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.143": {"line.1": {"text": "Die mich bey Tag' erquickt/ und fr\u00f6lich sey bey Nacht.", "tokens": ["Die", "mich", "bey", "Tag'", "er\u00b7quickt", "/", "und", "fr\u00f6\u00b7lich", "sey", "bey", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVPP", "$(", "KON", "ADJD", "VAFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.144": {"line.1": {"text": "Du sitzest unten an/ wo sie von h\u00f6herm Blute:", "tokens": ["Du", "sit\u00b7zest", "un\u00b7ten", "an", "/", "wo", "sie", "von", "h\u00f6\u00b7herm", "Blu\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$(", "PWAV", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.145": {"line.1": {"text": "Ich liebe die/ von der ich nicht den Wahn vermuthe.", "tokens": ["Ich", "lie\u00b7be", "die", "/", "von", "der", "ich", "nicht", "den", "Wahn", "ver\u00b7mu\u00b7the", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "$(", "APPR", "PRELS", "PPER", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.146": {"line.1": {"text": "Die man f\u00fcr sch\u00f6ne sch\u00e4tzt: Kennt ihrer Farben Prei\u00df:", "tokens": ["Die", "man", "f\u00fcr", "sch\u00f6\u00b7ne", "sch\u00e4tzt", ":", "Kennt", "ih\u00b7rer", "Far\u00b7ben", "Prei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "APPR", "ADJA", "VVFIN", "$.", "VVFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.147": {"line.1": {"text": "Seh' ich was he\u00dflichs an/ so schwitz' ich kalten Schwei\u00df.", "tokens": ["Seh'", "ich", "was", "he\u00df\u00b7lichs", "an", "/", "so", "schwitz'", "ich", "kal\u00b7ten", "Schwei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "VVFIN", "PTKVZ", "$(", "ADV", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.148": {"line.1": {"text": "Geberden k\u00f6nnen offt was h\u00e4\u00dflich/ sch\u00f6ne machen:", "tokens": ["Ge\u00b7ber\u00b7den", "k\u00f6n\u00b7nen", "offt", "was", "h\u00e4\u00df\u00b7lich", "/", "sch\u00f6\u00b7ne", "ma\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "PWS", "ADJD", "$(", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.149": {"line.1": {"text": "Die h\u00e4\u00dflich/ w\u00fcrde sich bey mir nicht sch\u00f6ne lachen", "tokens": ["Die", "h\u00e4\u00df\u00b7lich", "/", "w\u00fcr\u00b7de", "sich", "bey", "mir", "nicht", "sch\u00f6\u00b7ne", "la\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "$(", "VAFIN", "PRF", "APPR", "PPER", "PTKNEG", "ADJA", "VVINF"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.150": {"line.1": {"text": "Ich fragte mehr vor mich nach frommer Eltern Kind:", "tokens": ["Ich", "frag\u00b7te", "mehr", "vor", "mich", "nach", "from\u00b7mer", "El\u00b7tern", "Kind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PRF", "APPR", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.151": {"line.1": {"text": "W\u00e4r' es nicht selber fromm: Ist jener fromm-seyn/ Wind.", "tokens": ["W\u00e4r'", "es", "nicht", "sel\u00b7ber", "fromm", ":", "Ist", "je\u00b7ner", "from\u00b7mseyn", "/", "Wind", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "$.", "VAFIN", "PDAT", "NN", "$(", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.152": {"line.1": {"text": "Die mu\u00df ja z\u00fcchtig seyn/ die z\u00fcchtig ist gezeuget:", "tokens": ["Die", "mu\u00df", "ja", "z\u00fcch\u00b7tig", "seyn", "/", "die", "z\u00fcch\u00b7tig", "ist", "ge\u00b7zeu\u00b7get", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ADJD", "VAINF", "$(", "ART", "ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.153": {"line.1": {"text": "Mein wehrter Hertzens-Freund/ auch diese Regel treuget.", "tokens": ["Mein", "wehr\u00b7ter", "Hert\u00b7zens\u00b7Freund", "/", "auch", "die\u00b7se", "Re\u00b7gel", "treu\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "ADV", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.154": {"line.1": {"text": "Du lobst denn die man hat verzettelt auf dem Heu:", "tokens": ["Du", "lobst", "denn", "die", "man", "hat", "ver\u00b7zet\u00b7telt", "auf", "dem", "Heu", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "PIS", "VAFIN", "VVPP", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.155": {"line.1": {"text": "Die gibt f\u00fcr gleiches Vieh ein' angenehme Streu.", "tokens": ["Die", "gibt", "f\u00fcr", "glei\u00b7ches", "Vieh", "ein'", "an\u00b7ge\u00b7neh\u00b7me", "Streu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.156": {"line.1": {"text": "Die Jungfern k\u00f6nnen ietzt wol ander unterstreuen:", "tokens": ["Die", "Jung\u00b7fern", "k\u00f6n\u00b7nen", "ietzt", "wol", "an\u00b7der", "un\u00b7ter\u00b7streu\u00b7en", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.157": {"line.1": {"text": "Die zuviel unterstreut wird endlich Ochsen treuen.", "tokens": ["Die", "zu\u00b7viel", "un\u00b7ter\u00b7streut", "wird", "end\u00b7lich", "Och\u00b7sen", "treu\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVPP", "VAFIN", "ADV", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.158": {"line.1": {"text": "Die Weissen sind offt sieh wenn sie nicht stets purgiert:", "tokens": ["Die", "Weis\u00b7sen", "sind", "offt", "sieh", "wenn", "sie", "nicht", "stets", "pur\u00b7giert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "KOUS", "PPER", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}, "stanza.159": {"line.1": {"text": "Bey schwartzen w\u00fcrde mir viel Seiffen-Geld verschmiert.", "tokens": ["Bey", "schwart\u00b7zen", "w\u00fcr\u00b7de", "mir", "viel", "Seif\u00b7fen\u00b7Geld", "ver\u00b7schmiert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.160": {"line.1": {"text": "Ein Leib der braun und starck kan starcke P\u00fcffe tragen:", "tokens": ["Ein", "Leib", "der", "braun", "und", "starck", "kan", "star\u00b7cke", "P\u00fcf\u00b7fe", "tra\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJD", "KON", "NN", "VMFIN", "VVFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.161": {"line.1": {"text": "Du denckst ich werde mich/ als wie mit Hunden schlagen?", "tokens": ["Du", "denckst", "ich", "wer\u00b7de", "mich", "/", "als", "wie", "mit", "Hun\u00b7den", "schla\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VAFIN", "PPER", "$(", "KOUS", "KOKOM", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.162": {"line.1": {"text": "Kein Esel/ Glock und Weib sind sonder Schl\u00e4ge gut:", "tokens": ["Kein", "E\u00b7sel", "/", "Glock", "und", "Weib", "sind", "son\u00b7der", "Schl\u00e4\u00b7ge", "gut", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$(", "NN", "KON", "NN", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.163": {"line.1": {"text": "Der mu\u00df ein Esel seyn/ wer tobt auf Frauen-Blut.", "tokens": ["Der", "mu\u00df", "ein", "E\u00b7sel", "seyn", "/", "wer", "tobt", "auf", "Frau\u00b7en\u00b7Blut", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "NN", "VAINF", "$(", "PWS", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.164": {"line.1": {"text": "Nimm eine zarte denn/ die darffst du nicht ber\u00fchren.", "tokens": ["Nimm", "ei\u00b7ne", "zar\u00b7te", "denn", "/", "die", "darffst", "du", "nicht", "be\u00b7r\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ADJA", "KON", "$(", "ART", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.165": {"line.1": {"text": "N\u00fcrnberger Gut l\u00e4st sich auf alle M\u00e4rckte f\u00fchren.", "tokens": ["N\u00fcrn\u00b7ber\u00b7ger", "Gut", "l\u00e4st", "sich", "auf", "al\u00b7le", "M\u00e4rck\u00b7te", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "PRF", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "+--+---+-+-+-", "measure": "dactylic.di.plus"}}, "stanza.166": {"line.1": {"text": "Ich hielte viel von der die Tugend schweigen lehrt.", "tokens": ["Ich", "hiel\u00b7te", "viel", "von", "der", "die", "Tu\u00b7gend", "schwei\u00b7gen", "lehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PRELS", "ART", "NN", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.167": {"line.1": {"text": "Was n\u00fctzte mir der Block der keinmahl wird geh\u00f6rt.", "tokens": ["Was", "n\u00fctz\u00b7te", "mir", "der", "Block", "der", "kein\u00b7mahl", "wird", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ART", "NN", "ART", "ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.168": {"line.1": {"text": "Ich kenne manche wohl/ die gantze Monden brummen.", "tokens": ["Ich", "ken\u00b7ne", "man\u00b7che", "wohl", "/", "die", "gant\u00b7ze", "Mon\u00b7den", "brum\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ADV", "$(", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.169": {"line.1": {"text": "Ich kenne manche wohl/ die auf ein Jahr verstummen.", "tokens": ["Ich", "ken\u00b7ne", "man\u00b7che", "wohl", "/", "die", "auf", "ein", "Jahr", "ver\u00b7stum\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ADV", "$(", "ART", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.170": {"line.1": {"text": "Nimm einen Hasel-Ast/ der ist daf\u00fcr bewehrt/", "tokens": ["Nimm", "ei\u00b7nen", "Ha\u00b7sel\u00b7Ast", "/", "der", "ist", "da\u00b7f\u00fcr", "be\u00b7wehrt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$(", "ART", "VAFIN", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.171": {"line.1": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}}, "stanza.172": {"line.1": {"text": "So kan sie zu dem Schatz das Besemgeld erspahren.", "tokens": ["So", "kan", "sie", "zu", "dem", "Schatz", "das", "Be\u00b7sem\u00b7geld", "er\u00b7spah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.173": {"line.1": {"text": "Ihr Geld wird ohne di\u00df f\u00fcr tausend Teuffel fahren.", "tokens": ["Ihr", "Geld", "wird", "oh\u00b7ne", "di\u00df", "f\u00fcr", "tau\u00b7send", "Teuf\u00b7fel", "fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "PDS", "APPR", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.174": {"line.1": {"text": "Wir kommen von dem Zweck. Man sagt/ wer h\u00e4lt der hegt.", "tokens": ["Wir", "kom\u00b7men", "von", "dem", "Zweck", ".", "Man", "sagt", "/", "wer", "h\u00e4lt", "der", "hegt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$.", "PIS", "VVFIN", "$(", "PWS", "VVFIN", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.175": {"line.1": {"text": "Nur da\u00df man nicht der Magd vors Brodt drey Schl\u00f6sser legt.", "tokens": ["Nur", "da\u00df", "man", "nicht", "der", "Magd", "vors", "Brodt", "drey", "Schl\u00f6s\u00b7ser", "legt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PIS", "PTKNEG", "ART", "NN", "APPRART", "NN", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.176": {"line.1": {"text": "Soll eine Reiche dich mit ihren G\u00fclden laben?", "tokens": ["Soll", "ei\u00b7ne", "Rei\u00b7che", "dich", "mit", "ih\u00b7ren", "G\u00fcl\u00b7den", "la\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NE", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.177": {"line.1": {"text": "Die sch\u00e4tz' ich dir vergn\u00fcgt/ und mich allein wil haben.", "tokens": ["Die", "sch\u00e4tz'", "ich", "dir", "ver\u00b7gn\u00fcgt", "/", "und", "mich", "al\u00b7lein", "wil", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PPER", "VVPP", "$(", "KON", "PPER", "ADV", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.178": {"line.1": {"text": "Die Weisheit kommt was hoch! Soll sie denn lustig seyn?", "tokens": ["Die", "Weis\u00b7heit", "kommt", "was", "hoch", "!", "Soll", "sie", "denn", "lus\u00b7tig", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PWS", "ADJD", "$.", "VMFIN", "PPER", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.179": {"line.1": {"text": "Ein immer-traurig Weib ist wie versaurter Wein.", "tokens": ["Ein", "im\u00b7mer\u00b7trau\u00b7rig", "Weib", "ist", "wie", "ver\u00b7saur\u00b7ter", "Wein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "VAFIN", "KOKOM", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.180": {"line.1": {"text": "Ich kenne/ die verstehn/ wie die Claviere klingen/", "tokens": ["Ich", "ken\u00b7ne", "/", "die", "ver\u00b7stehn", "/", "wie", "die", "Cla\u00b7vie\u00b7re", "klin\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "ART", "VVINF", "$(", "KOKOM", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.181": {"line.1": {"text": "Die k\u00f6nnen mit der Zeit ein Ninno Josephs singen.", "tokens": ["Die", "k\u00f6n\u00b7nen", "mit", "der", "Zeit", "ein", "Nin\u00b7no", "Jo\u00b7se\u00b7phs", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "APPR", "ART", "NN", "ART", "NN", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}}, "stanza.182": {"line.1": {"text": "Und denen Aretin und ", "tokens": ["Und", "de\u00b7nen", "A\u00b7re\u00b7tin", "und"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PDS", "NN", "KON"], "meter": "-+-+---", "measure": "unknown.measure.di"}}, "stanza.183": {"line.1": {"text": "Die sind zu klug/ mein Freund/ vor mich und meinen Stand.", "tokens": ["Die", "sind", "zu", "klug", "/", "mein", "Freund", "/", "vor", "mich", "und", "mei\u00b7nen", "Stand", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKA", "ADJD", "$(", "PPOSAT", "NN", "$(", "APPR", "PPER", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.184": {"line.1": {"text": "D\u00fcrfft' ein' auf gute Treu sich dir wol selbst anbieten?", "tokens": ["D\u00fcrfft'", "ein'", "auf", "gu\u00b7te", "Treu", "sich", "dir", "wol", "selbst", "an\u00b7bie\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "APPR", "ADJA", "NN", "PRF", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "---+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.185": {"line.1": {"text": "Der w\u00e4re wol der Kopff gespalten in der Mitten.", "tokens": ["Der", "w\u00e4\u00b7re", "wol", "der", "Kopff", "ge\u00b7spal\u00b7ten", "in", "der", "Mit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN", "VVPP", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.186": {"line.1": {"text": "Was Rath denn/ wenn sie dir stets ihren Kram versagt.", "tokens": ["Was", "Rath", "denn", "/", "wenn", "sie", "dir", "stets", "ih\u00b7ren", "Kram", "ver\u00b7sagt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "KON", "$(", "KOUS", "PPER", "PPER", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.187": {"line.1": {"text": "Denn Liese gute Nacht und Sylvia gefragt.", "tokens": ["Denn", "Lie\u00b7se", "gu\u00b7te", "Nacht", "und", "Syl\u00b7via", "ge\u00b7fragt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJA", "NN", "KON", "NE", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.188": {"line.1": {"text": "Wenn sich der Zeug verliegt/ pflegt man bald lo\u00dfzuschlagen.", "tokens": ["Wenn", "sich", "der", "Zeug", "ver\u00b7liegt", "/", "pflegt", "man", "bald", "lo\u00df\u00b7zu\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "VVPP", "$(", "VVFIN", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.189": {"line.1": {"text": "Die zu viel Jahre zehlt/ wei\u00df gar zu viel zu sagen.", "tokens": ["Die", "zu", "viel", "Jah\u00b7re", "zehlt", "/", "wei\u00df", "gar", "zu", "viel", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "VVFIN", "$(", "VVFIN", "ADV", "PTKA", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.190": {"line.1": {"text": "Was wei\u00df ein Kindisch Kind/ das noch mit Tocken laufft.", "tokens": ["Was", "wei\u00df", "ein", "Kin\u00b7disch", "Kind", "/", "das", "noch", "mit", "To\u00b7cken", "laufft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "NN", "$(", "PDS", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.191": {"line.1": {"text": "Was unreiff acht ich nicht/ was faul wird nicht verkaufft.", "tokens": ["Was", "un\u00b7reiff", "acht", "ich", "nicht", "/", "was", "faul", "wird", "nicht", "ver\u00b7kaufft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "CARD", "PPER", "PTKNEG", "$(", "PWS", "ADJD", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.192": {"line.1": {"text": "Bey Wittben findet man bestellte K\u00fcch' und Keller.", "tokens": ["Bey", "Witt\u00b7ben", "fin\u00b7det", "man", "be\u00b7stell\u00b7te", "K\u00fc\u00b7ch'", "und", "Kel\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}}, "stanza.193": {"line.1": {"text": "Man freyt die Wittben wol/ man freyt auch ihre Heller.", "tokens": ["Man", "freyt", "die", "Witt\u00b7ben", "wol", "/", "man", "freyt", "auch", "ih\u00b7re", "Hel\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "ADV", "$(", "PIS", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.194": {"line.1": {"text": "Wird iemand drum verdacht? Sie sitzen warm und fest.", "tokens": ["Wird", "ie\u00b7mand", "drum", "ver\u00b7dacht", "?", "Sie", "sit\u00b7zen", "warm", "und", "fest", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PAV", "VVPP", "$.", "PPER", "VVFIN", "ADJD", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.195": {"line.1": {"text": "Zu fest auch wol f\u00fcr mich/ die erste Treu die best.", "tokens": ["Zu", "fest", "auch", "wol", "f\u00fcr", "mich", "/", "die", "ers\u00b7te", "Treu", "die", "best", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "ADV", "ADV", "APPR", "PPER", "$(", "ART", "ADJA", "NN", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}, "stanza.196": {"line.1": {"text": "Sie leben bey Verstand/ und haben was erfahren.", "tokens": ["Sie", "le\u00b7ben", "bey", "Ver\u00b7stand", "/", "und", "ha\u00b7ben", "was", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$(", "KON", "VAFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.197": {"line.1": {"text": "Gott woll' uns f\u00fcr und f\u00fcr vor dem Verstand bewahren.", "tokens": ["Gott", "woll'", "uns", "f\u00fcr", "und", "f\u00fcr", "vor", "dem", "Ver\u00b7stand", "be\u00b7wah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "APPR", "KON", "APPR", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.198": {"line.1": {"text": "So taugt dir keine nicht die ihren Mann beklagt?", "tokens": ["So", "taugt", "dir", "kei\u00b7ne", "nicht", "die", "ih\u00b7ren", "Mann", "be\u00b7klagt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "PTKNEG", "ART", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.199": {"line.1": {"text": "Sie ehr' ich. Doch ich lieb' ein unbefleckte Magd.", "tokens": ["Sie", "ehr'", "ich", ".", "Doch", "ich", "lieb'", "ein", "un\u00b7be\u00b7fleck\u00b7te", "Magd", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "KON", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.200": {"line.1": {"text": "Nimm was du wilst. Ich wil die Zeit allein vertreiben.", "tokens": ["Nimm", "was", "du", "wilst", ".", "Ich", "wil", "die", "Zeit", "al\u00b7lein", "ver\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PWS", "PPER", "VMFIN", "$.", "PPER", "VMFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.201": {"line.1": {"text": "So wilst du f\u00fcr und f\u00fcr ein Vesper-Knecht verbleiben?", "tokens": ["So", "wilst", "du", "f\u00fcr", "und", "f\u00fcr", "ein", "Ves\u00b7per\u00b7Knecht", "ver\u00b7blei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "KON", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.202": {"line.1": {"text": "Wer einsam ist/ vertreibt die Zeit in h\u00f6chster Ruh.", "tokens": ["Wer", "ein\u00b7sam", "ist", "/", "ver\u00b7treibt", "die", "Zeit", "in", "h\u00f6chs\u00b7ter", "Ruh", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$(", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.203": {"line.1": {"text": "Wer so verschimmelt bringt die Zeit gar \u00fcbel zu.", "tokens": ["Wer", "so", "ver\u00b7schim\u00b7melt", "bringt", "die", "Zeit", "gar", "\u00fc\u00b7bel", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVPP", "VVFIN", "ART", "NN", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.204": {"line.1": {"text": "Verschimmel' ich/ so putzt mich ab mit Flederwischen.", "tokens": ["Ver\u00b7schim\u00b7mel'", "ich", "/", "so", "putzt", "mich", "ab", "mit", "Fle\u00b7der\u00b7wi\u00b7schen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$(", "ADV", "VVFIN", "PPER", "PTKVZ", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.205": {"line.1": {"text": "Wohl mir/ wenn die mir lieb wird meinen Brand erfrischen.", "tokens": ["Wohl", "mir", "/", "wenn", "die", "mir", "lieb", "wird", "mei\u00b7nen", "Brand", "er\u00b7fri\u00b7schen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$(", "KOUS", "ART", "PPER", "ADJD", "VAFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.206": {"line.1": {"text": "Man glaubt da\u00df um die Zeit der heissen Sonnen-wende", "tokens": ["Man", "glaubt", "da\u00df", "um", "die", "Zeit", "der", "heis\u00b7sen", "Son\u00b7nen\u00b7wen\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "KOUS", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Bl\u00e4tter gr\u00fcne Tracht an B\u00e4umen um-sich-kehr/", "tokens": ["Der", "Bl\u00e4t\u00b7ter", "gr\u00fc\u00b7ne", "Tracht", "an", "B\u00e4u\u00b7men", "um\u00b7sich\u00b7kehr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPR", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr seht/ da\u00df eine Frau itzt Leid vor Lust versende/", "tokens": ["Ihr", "seht", "/", "da\u00df", "ei\u00b7ne", "Frau", "itzt", "Leid", "vor", "Lust", "ver\u00b7sen\u00b7de", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KOUS", "ART", "NN", "ADV", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und Profin euer Hertz vor R\u00fcels Grab begehr.", "tokens": ["Und", "Pro\u00b7fin", "eu\u00b7er", "Hertz", "vor", "R\u00fcels", "Grab", "be\u00b7gehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPOSAT", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Drum kehrt den Schlu\u00df euch um/ zu leben stets allein/", "tokens": ["Drum", "kehrt", "den", "Schlu\u00df", "euch", "um", "/", "zu", "le\u00b7ben", "stets", "al\u00b7lein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "PPER", "APPR", "$(", "PTKZU", "VVINF", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und sucht/ mein Freund/ itzt Mann/ doch Vater bald zu seyn.", "tokens": ["Und", "sucht", "/", "mein", "Freund", "/", "itzt", "Mann", "/", "doch", "Va\u00b7ter", "bald", "zu", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "PPOSAT", "NN", "$(", "ADV", "NN", "$(", "ADV", "NN", "ADV", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}