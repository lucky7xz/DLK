{"textgrid.poem.46175": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Paranesisch, bacchisch und satyrisches Gem\u00fc\u00df", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Weil nu der luft ganz ungest\u00fcm", "tokens": ["Weil", "nu", "der", "luft", "ganz", "un\u00b7ge\u00b7st\u00fcm"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mit schnee und regen sich vermischet", "tokens": ["mit", "schnee", "und", "re\u00b7gen", "sich", "ver\u00b7mi\u00b7schet"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "ADJA", "PRF", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und nu der wind mit nichten stum", "tokens": ["und", "nu", "der", "wind", "mit", "nich\u00b7ten", "stum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "das erdreich gleichsam seifend waschet;", "tokens": ["das", "er\u00b7dreich", "gleich\u00b7sam", "sei\u00b7fend", "wa\u00b7schet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so lasset uns auch, liebe freind,", "tokens": ["so", "las\u00b7set", "uns", "auch", ",", "lie\u00b7be", "freind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "was sprachen wir auch immer reden,", "tokens": ["was", "spra\u00b7chen", "wir", "auch", "im\u00b7mer", "re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "den tisch bedecken zu der stund", "tokens": ["den", "tisch", "be\u00b7de\u00b7cken", "zu", "der", "stund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "mit flaschen, schunken, k\u00e4s und fladen.", "tokens": ["mit", "fla\u00b7schen", ",", "schun\u00b7ken", ",", "k\u00e4s", "und", "fla\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "$,", "VVFIN", "$,", "ADJA", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Bring her die gl\u00e4ser und schenk ein.", "tokens": ["Bring", "her", "die", "gl\u00e4\u00b7ser", "und", "schenk", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "KON", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "wem kan zu drinken doch misfallen?", "tokens": ["wem", "kan", "zu", "drin\u00b7ken", "doch", "mis\u00b7fal\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PTKZU", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der wein hat gleichsam den rock an,", "tokens": ["der", "wein", "hat", "gleich\u00b7sam", "den", "rock", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "alsbald wir ihn in ein glas f\u00fcllen:", "tokens": ["als\u00b7bald", "wir", "ihn", "in", "ein", "glas", "f\u00fcl\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "jedoch das rein christallin glas", "tokens": ["je\u00b7doch", "das", "rein", "chris\u00b7tal\u00b7lin", "glas"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "NE", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "des weins leib, nicht die farb bedecket,", "tokens": ["des", "weins", "leib", ",", "nicht", "die", "farb", "be\u00b7de\u00b7cket", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PTKNEG", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "also, o wunder \u00fcbergro\u00df!", "tokens": ["al\u00b7so", ",", "o", "wun\u00b7der", "\u00fc\u00b7ber\u00b7gro\u00df", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "FM", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "den wein und unser aug erquicket.", "tokens": ["den", "wein", "und", "un\u00b7ser", "aug", "er\u00b7quic\u00b7ket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Dan er kaum rinnet aus dem loch", "tokens": ["Dan", "er", "kaum", "rin\u00b7net", "aus", "dem", "loch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "der schwangern kanten oder flaschen,", "tokens": ["der", "schwan\u00b7gern", "kan\u00b7ten", "o\u00b7der", "fla\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVINF", "VMFIN", "KON", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df wie er schmollet, ich auch lach,", "tokens": ["da\u00df", "wie", "er", "schmol\u00b7let", ",", "ich", "auch", "lach", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "PPER", "VVFIN", "$,", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "begehrend mich bald zu erfrischen:", "tokens": ["be\u00b7geh\u00b7rend", "mich", "bald", "zu", "er\u00b7fri\u00b7schen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "halt ich ihn dan in meiner hand,", "tokens": ["halt", "ich", "ihn", "dan", "in", "mei\u00b7ner", "hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das aus dem glas er werd gefreiet,", "tokens": ["das", "aus", "dem", "glas", "er", "werd", "ge\u00b7frei\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "VVFIN", "PPER", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "merk ich, da\u00df er mein herz und mund,", "tokens": ["merk", "ich", ",", "da\u00df", "er", "mein", "herz", "und", "mund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "eh da\u00df ich ihn versuch, erfreuet.", "tokens": ["eh", "da\u00df", "ich", "ihn", "ver\u00b7such", ",", "er\u00b7freu\u00b7et", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "PPER", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Darum wer doppeltes gut will", "tokens": ["Da\u00b7rum", "wer", "dop\u00b7pel\u00b7tes", "gut", "will"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "PWS", "ADV", "ADJD", "VMFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "anschauen, riechen, schmecken, sp\u00fcren,", "tokens": ["an\u00b7schau\u00b7en", ",", "rie\u00b7chen", ",", "schme\u00b7cken", ",", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der mu\u00df nu einen becher voll", "tokens": ["der", "mu\u00df", "nu", "ei\u00b7nen", "be\u00b7cher", "voll"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "ADV", "ART", "ADJA", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "des edlen rebensafts nicht sparen,", "tokens": ["des", "ed\u00b7len", "re\u00b7bens\u00b7afts", "nicht", "spa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so nem ein jeder sein gesch\u00fctz", "tokens": ["so", "nem", "ein", "je\u00b7der", "sein", "ge\u00b7sch\u00fctz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "PIAT", "PPOSAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "und eh wir es zugleich hinrichten,", "tokens": ["und", "eh", "wir", "es", "zu\u00b7gleich", "hin\u00b7rich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "mu\u00df er mit mir den reichen schatz", "tokens": ["mu\u00df", "er", "mit", "mir", "den", "rei\u00b7chen", "schatz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "PPER", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "zu loben, singend nicht verachten.", "tokens": ["zu", "lo\u00b7ben", ",", "sin\u00b7gend", "nicht", "ver\u00b7ach\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "ADJD", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Dieweil nu dises ein Rheinwein", "tokens": ["Die\u00b7weil", "nu", "di\u00b7ses", "ein", "Rhein\u00b7wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PDAT", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "oder dem Rheinwein zu vergleichen,", "tokens": ["o\u00b7der", "dem", "Rhein\u00b7wein", "zu", "ver\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "so schenk ihn in den becher ein,", "tokens": ["so", "schenk", "ihn", "in", "den", "be\u00b7cher", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "ihn mit gold noch mehr zu bereichen.", "tokens": ["ihn", "mit", "gold", "noch", "mehr", "zu", "be\u00b7rei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "NN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "er ist klar, lieblich, frisch und reich,", "tokens": ["er", "ist", "klar", ",", "lieb\u00b7lich", ",", "frisch", "und", "reich", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "ADJD", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "darum mu\u00df er herum passieren:", "tokens": ["da\u00b7rum", "mu\u00df", "er", "he\u00b7rum", "pas\u00b7sie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "APZR", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "da\u00df keiner m\u00f6g die zeit verlieren.", "tokens": ["da\u00df", "kei\u00b7ner", "m\u00f6g", "die", "zeit", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ist jener rotwein ein Franzos,", "tokens": ["Ist", "je\u00b7ner", "rot\u00b7wein", "ein", "Fran\u00b7zos", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so thut er wol, zu uns zu kommen;", "tokens": ["so", "thut", "er", "wol", ",", "zu", "uns", "zu", "kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "er l\u00e4chelt wie ein rote ros", "tokens": ["er", "l\u00e4\u00b7chelt", "wie", "ein", "ro\u00b7te", "ros"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und wird von uns gern angenommen.", "tokens": ["und", "wird", "von", "uns", "gern", "an\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ich h\u00f6r nicht mehr des winds get\u00f6s,", "tokens": ["ich", "h\u00f6r", "nicht", "mehr", "des", "winds", "ge\u00b7t\u00f6s", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "sither wir mit dem wein parlieren.", "tokens": ["si\u00b7ther", "wir", "mit", "dem", "wein", "par\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so lasset uns all garaussieren.", "tokens": ["so", "las\u00b7set", "uns", "all", "ga\u00b7raus\u00b7sie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ho! wein her, den uns das Welschland", "tokens": ["Ho", "!", "wein", "her", ",", "den", "uns", "das", "Wel\u00b7schland"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NN", "PTKVZ", "$,", "PRELS", "PPER", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "ohn des Bapst sig und segen sendet.", "tokens": ["ohn", "des", "Bapst", "sig", "und", "se\u00b7gen", "sen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "KON", "VVINF", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "ein schalen voll in meiner hand", "tokens": ["ein", "scha\u00b7len", "voll", "in", "mei\u00b7ner", "hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJD", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "davon, wird bald wol angewendet:", "tokens": ["da\u00b7von", ",", "wird", "bald", "wol", "an\u00b7ge\u00b7wen\u00b7det", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "die farb ist angenehm, ich sih,", "tokens": ["die", "farb", "ist", "an\u00b7ge\u00b7nehm", ",", "ich", "sih", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und sein geruch thut excellieren:", "tokens": ["und", "sein", "ge\u00b7ruch", "thut", "ex\u00b7cel\u00b7lie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "er kan nicht dan euch aggradieren.", "tokens": ["er", "kan", "nicht", "dan", "euch", "ag\u00b7gra\u00b7die\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ein ander Welschland wei\u00df ich noch,", "tokens": ["Ein", "an\u00b7der", "Wel\u00b7schland", "wei\u00df", "ich", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da man auch zechend fr\u00f6lich lebet", "tokens": ["da", "man", "auch", "ze\u00b7chend", "fr\u00f6\u00b7lich", "le\u00b7bet"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "ADJD", "VVFIN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "mit brot und k\u00e4s und ohn den koch,", "tokens": ["mit", "brot", "und", "k\u00e4s", "und", "ohn", "den", "koch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "KON", "APPR", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "schier Schweizer gleich, nach ehren strebet:", "tokens": ["schier", "Schwei\u00b7zer", "gleich", ",", "nach", "eh\u00b7ren", "stre\u00b7bet", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "$,", "APPR", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "reich her das volle kr\u00e4uslein da,", "tokens": ["reich", "her", "das", "vol\u00b7le", "kr\u00e4us\u00b7lein", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "es gilt den herren und den frauen", "tokens": ["es", "gilt", "den", "her\u00b7ren", "und", "den", "frau\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so das ist artlich ", "tokens": ["so", "das", "ist", "art\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PDS", "VAFIN", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Ist Engelland schon ohn Weinwachs,", "tokens": ["Ist", "En\u00b7gel\u00b7land", "schon", "ohn", "Wein\u00b7wachs", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "APPR", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "hat man doch gute wein darinnen,", "tokens": ["hat", "man", "doch", "gu\u00b7te", "wein", "da\u00b7rin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "ADJA", "NN", "ADV", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "und mancher drinket als ein Sachs,", "tokens": ["und", "man\u00b7cher", "drin\u00b7ket", "als", "ein", "Sachs", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wan er die schlacht gern wolt gewinnen:", "tokens": ["wan", "er", "die", "schlacht", "gern", "wolt", "ge\u00b7win\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "VVFIN", "ADV", "VMFIN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "drink mir ein glas des besten zu,", "tokens": ["drink", "mir", "ein", "glas", "des", "bes\u00b7ten", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ART", "ADJA", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "mit welchem die insuln prachtieren,", "tokens": ["mit", "wel\u00b7chem", "die", "in\u00b7suln", "prach\u00b7tie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "kan ein wein disen surpassieren?", "tokens": ["kan", "ein", "wein", "di\u00b7sen", "sur\u00b7pas\u00b7sie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVINF", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.10": {"line.1": {"text": "Die Niederteutsche, frische fisch,", "tokens": ["Die", "Nie\u00b7der\u00b7teut\u00b7sche", ",", "fri\u00b7sche", "fisch", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "die lang gern hinder dem tisch sitzen,", "tokens": ["die", "lang", "gern", "hin\u00b7der", "dem", "tisch", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "APPR", "ART", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "lieben den wein, der stark und frisch,", "tokens": ["lie\u00b7ben", "den", "wein", ",", "der", "stark", "und", "frisch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "PRELS", "ADJD", "KON", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "und zechen gern, bis da\u00df sie schwitzen:", "tokens": ["und", "ze\u00b7chen", "gern", ",", "bis", "da\u00df", "sie", "schwit\u00b7zen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "$,", "KON", "KOUS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so gib auch ihrentwegen nu", "tokens": ["so", "gib", "auch", "ih\u00b7rent\u00b7we\u00b7gen", "nu"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "ADV", "PPOSAT", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "den gro\u00dfen kelch, damit zu zehren", "tokens": ["den", "gro\u00b7\u00dfen", "kelch", ",", "da\u00b7mit", "zu", "zeh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PAV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so: dises hei\u00dfet recht laveeren.", "tokens": ["so", ":", "di\u00b7ses", "hei\u00b7\u00dfet", "recht", "la\u00b7vee\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PDS", "VVFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Seid ihr den Spaniern hie feind,", "tokens": ["Seid", "ihr", "den", "Spa\u00b7ni\u00b7ern", "hie", "feind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "so langsam ihrer zu gedenken?", "tokens": ["so", "lang\u00b7sam", "ih\u00b7rer", "zu", "ge\u00b7den\u00b7ken", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "seind sie doch aller l\u00e4nder freind,", "tokens": ["seind", "sie", "doch", "al\u00b7ler", "l\u00e4n\u00b7der", "freind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wan sie den wein schon nicht verschenken.", "tokens": ["wan", "sie", "den", "wein", "schon", "nicht", "ver\u00b7schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "gib ihres weins das gl\u00e4slein da,", "tokens": ["gib", "ih\u00b7res", "weins", "das", "gl\u00e4s\u00b7lein", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "ADJA", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "damit ich besser m\u00f6g hablieren.", "tokens": ["da\u00b7mit", "ich", "bes\u00b7ser", "m\u00f6g", "hab\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "wer will mag ", "tokens": ["wer", "will", "mag"], "token_info": ["word", "word", "word"], "pos": ["PWS", "VMFIN", "VMFIN"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.12": {"line.1": {"text": "In Irland war ich auch einmal", "tokens": ["In", "Ir\u00b7land", "war", "ich", "auch", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VAFIN", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und sah dort manche ding verwirren,", "tokens": ["und", "sah", "dort", "man\u00b7che", "ding", "ver\u00b7wir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "doch wissend wol die rechte wahl,", "tokens": ["doch", "wis\u00b7send", "wol", "die", "rech\u00b7te", "wahl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "lie\u00df ich mich billich nicht verirren.", "tokens": ["lie\u00df", "ich", "mich", "bil\u00b7lich", "nicht", "ver\u00b7ir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.5": {"text": "schenk ein ein wenig Usquebagh,", "tokens": ["schenk", "ein", "ein", "we\u00b7nig", "Us\u00b7queb\u00b7agh", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "in Irland \u00fcberal geliebet:", "tokens": ["in", "Ir\u00b7land", "\u00fc\u00b7be\u00b7ral", "ge\u00b7lie\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so, dieses hei\u00dfet wol ge\u00fcbet.", "tokens": ["so", ",", "die\u00b7ses", "hei\u00b7\u00dfet", "wol", "ge\u00b7\u00fc\u00b7bet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PDS", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "H\u00f6r ich nicht Fratzen, den dickkopf,", "tokens": ["H\u00f6r", "ich", "nicht", "Frat\u00b7zen", ",", "den", "dick\u00b7kopf", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "PPER", "PTKNEG", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der witzlos jederman will lehren?", "tokens": ["der", "witz\u00b7los", "je\u00b7der\u00b7man", "will", "leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welcher ein recht grober knopf", "tokens": ["und", "wel\u00b7cher", "ein", "recht", "gro\u00b7ber", "knopf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ohn sich selbs niemand sunst will ehren?", "tokens": ["ohn", "sich", "selbs", "nie\u00b7mand", "sunst", "will", "eh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "ADV", "PIS", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "es gilt hie sechs, in einem suff,", "tokens": ["es", "gilt", "hie", "sechs", ",", "in", "ei\u00b7nem", "suff", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "CARD", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "herr Fratz, ihr m\u00fcsset das aussaufen,", "tokens": ["herr", "Fratz", ",", "ihr", "m\u00fcs\u00b7set", "das", "aus\u00b7sau\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PPER", "VMFIN", "PDS", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "es gilt Fratz Curly Murly Buff", "tokens": ["es", "gilt", "Fratz", "Cur\u00b7ly", "Mur\u00b7ly", "Buff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NE", "NE", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "bis alle fallen \u00fcbern haufen.", "tokens": ["bis", "al\u00b7le", "fal\u00b7len", "\u00fc\u00b7bern", "hau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Ich glaub, ihr liebe ", "tokens": ["Ich", "glaub", ",", "ihr", "lie\u00b7be"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "da\u00df ihr das Latein gar verschworen", "tokens": ["da\u00df", "ihr", "das", "La\u00b7tein", "gar", "ver\u00b7schwo\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und auch das griechisch, als ich sih,", "tokens": ["und", "auch", "das", "grie\u00b7chisch", ",", "als", "ich", "sih", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJD", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ist nu verachtet und verloren:", "tokens": ["ist", "nu", "ver\u00b7ach\u00b7tet", "und", "ver\u00b7lo\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVFIN", "KON", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "doch weil ein christliches r\u00e4uschlein", "tokens": ["doch", "weil", "ein", "christ\u00b7li\u00b7ches", "r\u00e4usc\u00b7hlein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "nicht kan, ", "tokens": ["nicht", "kan", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "bring ich euch, ", "tokens": ["bring", "ich", "euch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "und wolt euch jetz nicht gern turbieren.", "tokens": ["und", "wolt", "euch", "jetz", "nicht", "gern", "tur\u00b7bie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Ho! herr Fratz, was bedeuten doch", "tokens": ["Ho", "!", "herr", "Fratz", ",", "was", "be\u00b7deu\u00b7ten", "doch"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "$.", "NN", "NN", "$,", "PWS", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schmorotzer, blacken und bacchanten,", "tokens": ["schmo\u00b7rot\u00b7zer", ",", "bla\u00b7cken", "und", "bac\u00b7chan\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "VVINF", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "die so verhasset von dem koch", "tokens": ["die", "so", "ver\u00b7has\u00b7set", "von", "dem", "koch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "als schulf\u00fcchs, penal und pedanten?", "tokens": ["als", "schul\u00b7f\u00fcchs", ",", "pe\u00b7nal", "und", "pe\u00b7dan\u00b7ten", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "$,", "ADJD", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "warum darf ohn ein narrenkapp,", "tokens": ["wa\u00b7rum", "darf", "ohn", "ein", "nar\u00b7ren\u00b7kapp", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "APPR", "ART", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ein narr halb welsch und halb teutsch glotzen?", "tokens": ["ein", "narr", "halb", "welsch", "und", "halb", "teutsch", "glot\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "ADJD", "KON", "ADJD", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "warum doch will ein jeder lapp", "tokens": ["wa\u00b7rum", "doch", "will", "ein", "je\u00b7der", "lapp"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "f\u00fcr gut teutsch ", "tokens": ["f\u00fcr", "gut", "teutsch"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJD", "ADJD"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.16": {"line.1": {"text": "Ist es nicht eines bl\u00f6den hirns", "tokens": ["Ist", "es", "nicht", "ei\u00b7nes", "bl\u00f6\u00b7den", "hirns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PTKNEG", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und eines hasenkopfs merkzeichen,", "tokens": ["und", "ei\u00b7nes", "ha\u00b7sen\u00b7kopfs", "merk\u00b7zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der wol wert eines langen horns", "tokens": ["der", "wol", "wert", "ei\u00b7nes", "lan\u00b7gen", "horns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADJD", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und gar nicht wert mit uns zu zechen?", "tokens": ["und", "gar", "nicht", "wert", "mit", "uns", "zu", "ze\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADJD", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "mit uns, die wir dem guten wein", "tokens": ["mit", "uns", ",", "die", "wir", "dem", "gu\u00b7ten", "wein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "$,", "PRELS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "allein zu ehren welsch gegecket,", "tokens": ["al\u00b7lein", "zu", "eh\u00b7ren", "welsch", "ge\u00b7ge\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "und doch mit gr\u00f6\u00dferm flei\u00df und wohn", "tokens": ["und", "doch", "mit", "gr\u00f6\u00b7\u00dferm", "flei\u00df", "und", "wohn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "in der welt gro\u00dfes buch gegucket?", "tokens": ["in", "der", "welt", "gro\u00b7\u00dfes", "buch", "ge\u00b7gu\u00b7cket", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Gl\u00fcck zu, du ohn ein g gesell,", "tokens": ["Gl\u00fcck", "zu", ",", "du", "ohn", "ein", "g", "ge\u00b7sell", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,", "PPER", "APPR", "ART", "NN", "ADJD", "$,"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "hat mich der ", "tokens": ["hat", "mich", "der"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "er glaub mir, da\u00df dem ", "tokens": ["er", "glaub", "mir", ",", "da\u00df", "dem"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "ich aufwarten in wenig stunden;", "tokens": ["ich", "auf\u00b7war\u00b7ten", "in", "we\u00b7nig", "stun\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "dan ", "tokens": ["dan"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "so hat er weit gevoyagieret.", "tokens": ["so", "hat", "er", "weit", "ge\u00b7vo\u00b7ya\u00b7gie\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "der teufel hol euch, ohn ein n", "tokens": ["der", "teu\u00b7fel", "hol", "euch", ",", "ohn", "ein", "n"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KOUI", "ART", "XY"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "herr Hans, weil ihr uns all vexieret.", "tokens": ["herr", "Hans", ",", "weil", "ihr", "uns", "all", "ve\u00b7xie\u00b7ret", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "KOUS", "PPER", "PRF", "PIAT", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Wer teutsch ist, der red auch gut teutsch,", "tokens": ["Wer", "teutsch", "ist", ",", "der", "red", "auch", "gut", "teutsch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$,", "PRELS", "VVFIN", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "wie der Welsch will gut welsch parlieren:", "tokens": ["wie", "der", "Welsch", "will", "gut", "welsch", "par\u00b7lie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VMFIN", "ADJD", "ADJD", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "zu fu\u00df geh, wer ohn pferd und gutsch,", "tokens": ["zu", "fu\u00df", "geh", ",", "wer", "ohn", "pferd", "und", "gutsch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "PTKVZ", "VVFIN", "$,", "PWS", "APPR", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und wer ein narr, kan nicht vil lehren.", "tokens": ["und", "wer", "ein", "narr", ",", "kan", "nicht", "vil", "leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "ADJD", "$,", "VMFIN", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so sprechet nu ein urtheil aus,", "tokens": ["so", "spre\u00b7chet", "nu", "ein", "ur\u00b7theil", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und m\u00e4niglich mag es wol h\u00f6ren,", "tokens": ["und", "m\u00e4\u00b7nig\u00b7lich", "mag", "es", "wol", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.7": {"text": "gleich ist ein halbwelschteutscher has", "tokens": ["gleich", "ist", "ein", "halb\u00b7wel\u00b7schteut\u00b7scher", "has"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "den angestrichnen kranken huren.", "tokens": ["den", "an\u00b7ge\u00b7strich\u00b7nen", "kran\u00b7ken", "hu\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Und gleich wie der ein schwein, gans, kalb,", "tokens": ["Und", "gleich", "wie", "der", "ein", "schwein", ",", "gans", ",", "kalb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "ART", "ART", "ADJD", "$,", "ADJA", "$,", "ADJD", "$,"], "meter": "-----+-+", "measure": "unknown.measure.di"}, "line.2": {"text": "der gut und b\u00f6sen wein vermischet,", "tokens": ["der", "gut", "und", "b\u00f6\u00b7sen", "wein", "ver\u00b7mi\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "so dem geh\u00f6rt ein narrenkolb,", "tokens": ["so", "dem", "ge\u00b7h\u00f6rt", "ein", "nar\u00b7ren\u00b7kolb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VVFIN", "ART", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der teutsch und welsch zusammenwaschet;", "tokens": ["der", "teutsch", "und", "welsch", "zu\u00b7sam\u00b7men\u00b7wa\u00b7schet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "sein hirn und red seind gelb, wei\u00df, schwarz,", "tokens": ["sein", "hirn", "und", "red", "seind", "gelb", ",", "wei\u00df", ",", "schwarz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "VVFIN", "VAFIN", "ADJD", "$,", "VVFIN", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "gr\u00fcn, rot und blau, ein schneiderk\u00fcssen,", "tokens": ["gr\u00fcn", ",", "rot", "und", "blau", ",", "ein", "schnei\u00b7der\u00b7k\u00fcs\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "KON", "ADJD", "$,", "ART", "ADJA", "$,"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "ein alter schurz, ein lahmer scherz", "tokens": ["ein", "al\u00b7ter", "schurz", ",", "ein", "lah\u00b7mer", "scherz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJD", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und ganz unw\u00fcrdig mehrer bossen.", "tokens": ["und", "ganz", "un\u00b7w\u00fcr\u00b7dig", "meh\u00b7rer", "bos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "PIAT", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Kom, schenkend das glas wider ein,", "tokens": ["Kom", ",", "schen\u00b7kend", "das", "glas", "wi\u00b7der", "ein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADJD", "PDS", "VVFIN", "APPR", "ART", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "uns des lusts wider zu begaben,", "tokens": ["uns", "des", "lusts", "wi\u00b7der", "zu", "be\u00b7ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df drinkend, singend, redend rein,", "tokens": ["da\u00df", "drin\u00b7kend", ",", "sin\u00b7gend", ",", "re\u00b7dend", "rein", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "$,", "ADJD", "$,", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wir uns und ander auch erlaben.", "tokens": ["wir", "uns", "und", "an\u00b7der", "auch", "er\u00b7la\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "KON", "ADJD", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "doch drink wer will; ich hab zu vil;", "tokens": ["doch", "drink", "wer", "will", ";", "ich", "hab", "zu", "vil", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PWS", "VMFIN", "$.", "PPER", "VAFIN", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wer will mag danzen, drinken, springen,", "tokens": ["wer", "will", "mag", "dan\u00b7zen", ",", "drin\u00b7ken", ",", "sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWS", "VMFIN", "VMFIN", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "frei bleibet jedem alles spil,", "tokens": ["frei", "blei\u00b7bet", "je\u00b7dem", "al\u00b7les", "spil", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PIAT", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und wer will mag nu mit mir singen.", "tokens": ["und", "wer", "will", "mag", "nu", "mit", "mir", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VMFIN", "VMFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Frisch auf, frisch auf, seid wol zu mut!", "tokens": ["Frisch", "auf", ",", "frisch", "auf", ",", "seid", "wol", "zu", "mut", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$,", "ADJD", "PTKVZ", "$,", "VAFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "herum das gl\u00e4slein bald mu\u00df fahren:", "tokens": ["he\u00b7rum", "das", "gl\u00e4s\u00b7lein", "bald", "mu\u00df", "fah\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "b\u00f6s ist das wetter, der wein gut,", "tokens": ["b\u00f6s", "ist", "das", "wet\u00b7ter", ",", "der", "wein", "gut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "$,", "PRELS", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und ihrer keines nu zu sparen.", "tokens": ["und", "ih\u00b7rer", "kei\u00b7nes", "nu", "zu", "spa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "PIS", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "der wein sparet zwar die witz", "tokens": ["der", "wein", "spa\u00b7ret", "zwar", "die", "witz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "mit nichten,", "tokens": ["mit", "nich\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "PIS", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "weil er mit zu starker hitz", "tokens": ["weil", "er", "mit", "zu", "star\u00b7ker", "hitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "APPR", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "dr\u00fccknet unser dichten.", "tokens": ["dr\u00fcck\u00b7net", "un\u00b7ser", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.22": {"line.1": {"text": "Ich wei\u00df zwar wol noch wa ich bin,", "tokens": ["Ich", "wei\u00df", "zwar", "wol", "noch", "wa", "ich", "bin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADV", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "darf aber wol f\u00fcr etlich schw\u00f6ren,", "tokens": ["darf", "a\u00b7ber", "wol", "f\u00fcr", "et\u00b7lich", "schw\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "APPR", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df sie sehr gern ihr herz und sin", "tokens": ["da\u00df", "sie", "sehr", "gern", "ihr", "herz", "und", "sin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "all tag verbausen und verzehren.", "tokens": ["all", "tag", "ver\u00b7bau\u00b7sen", "und", "ver\u00b7zeh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "KON", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "bleibt ihr verstand ohn wein", "tokens": ["bleibt", "ihr", "ver\u00b7stand", "ohn", "wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "dahinden", "tokens": ["da\u00b7hin\u00b7den"], "token_info": ["word"], "pos": ["PAV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "k\u00f6nden sie als stock und stein,", "tokens": ["k\u00f6n\u00b7den", "sie", "als", "stock", "und", "stein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "KOUS", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "drinkend ihn nicht finden.", "tokens": ["drin\u00b7kend", "ihn", "nicht", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.23": {"line.1": {"text": "Sih da, wie weis der ", "tokens": ["Sih", "da", ",", "wie", "weis", "der"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "$,", "PWAV", "ADJD", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "sich under uns alhie erzeiget!", "tokens": ["sich", "un\u00b7der", "uns", "al\u00b7hie", "er\u00b7zei\u00b7get", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "KON", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "er bei\u00df mir doch auf dise nu\u00df!", "tokens": ["er", "bei\u00df", "mir", "doch", "auf", "di\u00b7se", "nu\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "sprach Fratz, mit drinken nicht geschweiget.", "tokens": ["sprach", "Fratz", ",", "mit", "drin\u00b7ken", "nicht", "ge\u00b7schwei\u00b7get", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "APPR", "VVFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "selbs reimen,", "tokens": ["selbs", "rei\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "besser dan ihr, ja dan du,", "tokens": ["bes\u00b7ser", "dan", "ihr", ",", "ja", "dan", "du", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "PPER", "$,", "ADV", "ADV", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "und das loch verleimen.", "tokens": ["und", "das", "loch", "ver\u00b7lei\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.24": {"line.1": {"text": "Ich hab die l\u00e4nder diser welt", "tokens": ["Ich", "hab", "die", "l\u00e4n\u00b7der", "di\u00b7ser", "welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schon vil jahr her gedurchmarschieret,", "tokens": ["schon", "vil", "jahr", "her", "ge\u00b7durch\u00b7mar\u00b7schie\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "APZR", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und hab auch ", "tokens": ["und", "hab", "auch"], "token_info": ["word", "word", "word"], "pos": ["KON", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "dan all ihr Welsche, verspendieret;", "tokens": ["dan", "all", "ihr", "Wel\u00b7sche", ",", "ver\u00b7spen\u00b7die\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PIAT", "PPOSAT", "NN", "$,", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "kan ich aber nicht vil welsch", "tokens": ["kan", "ich", "a\u00b7ber", "nicht", "vil", "welsch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "parlieren,", "tokens": ["par\u00b7lie\u00b7ren", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "so kan ich doch, gar nicht falsch,", "tokens": ["so", "kan", "ich", "doch", ",", "gar", "nicht", "falsch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "$,", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "meinen becher leeren.", "tokens": ["mei\u00b7nen", "be\u00b7cher", "lee\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.25": {"line.1": {"text": "Ihr herren, ich brauch keine list,", "tokens": ["Ihr", "her\u00b7ren", ",", "ich", "brauch", "kei\u00b7ne", "list", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "ich drink und hab nichts zu bedenken;", "tokens": ["ich", "drink", "und", "hab", "nichts", "zu", "be\u00b7den\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "KON", "VAFIN", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "zu drinken ist all mein lust,", "tokens": ["zu", "drin\u00b7ken", "ist", "all", "mein", "lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "PIAT", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "es gilt, und solt mir keiner danken.", "tokens": ["es", "gilt", ",", "und", "solt", "mir", "kei\u00b7ner", "dan\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wil dan eurer keiner mir", "tokens": ["wil", "dan", "eu\u00b7rer", "kei\u00b7ner", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "PPOSAT", "PIS", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "antworten,", "tokens": ["ant\u00b7wor\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "sollet ihr auch, bis ich mehr", "tokens": ["sol\u00b7let", "ihr", "auch", ",", "bis", "ich", "mehr"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "$,", "KOUS", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "euch hofiere, warten.", "tokens": ["euch", "ho\u00b7fie\u00b7re", ",", "war\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.26": {"line.1": {"text": "Wie oft hab ich mit einem wort", "tokens": ["Wie", "oft", "hab", "ich", "mit", "ei\u00b7nem", "wort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verjaget manche dolle katzen?", "tokens": ["ver\u00b7ja\u00b7get", "man\u00b7che", "dol\u00b7le", "kat\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wie oft hab ich mit meinem schwert", "tokens": ["wie", "oft", "hab", "ich", "mit", "mei\u00b7nem", "schwert"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "zerhacket manchen dollen kauzen?", "tokens": ["zer\u00b7ha\u00b7cket", "man\u00b7chen", "dol\u00b7len", "kau\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dise faust hat so vil blut", "tokens": ["di\u00b7se", "faust", "hat", "so", "vil", "blut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "VAFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "vergossen,", "tokens": ["ver\u00b7gos\u00b7sen", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "da\u00df ohn blut kein stein, baum, blat,", "tokens": ["da\u00df", "ohn", "blut", "kein", "stein", ",", "baum", ",", "blat", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PIAT", "NN", "$,", "ADV", "$,", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "keine w\u00e4ld, feld, gassen.", "tokens": ["kei\u00b7ne", "w\u00e4ld", ",", "feld", ",", "gas\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PIAT", "NN", "$,", "NN", "$,", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.27": {"line.1": {"text": "So bin ich auch oft auf dem meer", "tokens": ["So", "bin", "ich", "auch", "oft", "auf", "dem", "meer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "schier in der sonnen selbs ersoffen:", "tokens": ["schier", "in", "der", "son\u00b7nen", "selbs", "er\u00b7sof\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "daher ich auch schwarz als ein mohr", "tokens": ["da\u00b7her", "ich", "auch", "schwarz", "als", "ein", "mohr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "ADV", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "hat mit der Venus oft zu schaffen:", "tokens": ["hat", "mit", "der", "Ve\u00b7nus", "oft", "zu", "schaf\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "und durch manchen hei\u00dfen schmatz", "tokens": ["und", "durch", "man\u00b7chen", "hei\u00b7\u00dfen", "schmatz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "verliebet", "tokens": ["ver\u00b7lie\u00b7bet"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "hat der Proserpina schmotz", "tokens": ["hat", "der", "Pro\u00b7ser\u00b7pi\u00b7na", "schmotz"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NE", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "oft mein herz erlabet.", "tokens": ["oft", "mein", "herz", "er\u00b7la\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.28": {"line.1": {"text": "Was hat sie unter ihrem belz,", "tokens": ["Was", "hat", "sie", "un\u00b7ter", "ih\u00b7rem", "belz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df sie sich lie\u00df so gern aufsch\u00fcrzen?", "tokens": ["da\u00df", "sie", "sich", "lie\u00df", "so", "gern", "auf\u00b7sch\u00fcr\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ich wei\u00df nicht was f\u00fcr Plutons bolz,", "tokens": ["ich", "wei\u00df", "nicht", "was", "f\u00fcr", "Plu\u00b7tons", "bolz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PRELS", "APPR", "NE", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der pflag gar teufelisch zu scherzen.", "tokens": ["der", "pflag", "gar", "teu\u00b7fe\u00b7lisch", "zu", "scher\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ha! er ist ein arger fuchs", "tokens": ["ha", "!", "er", "ist", "ein", "ar\u00b7ger", "fuchs"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "ohn zweifel,", "tokens": ["ohn", "zwei\u00b7fel", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "er ist alles \u00fcbels ", "tokens": ["er", "ist", "al\u00b7les", "\u00fc\u00b7bels"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIS", "ADV"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "und ein rechter teufel.", "tokens": ["und", "ein", "rech\u00b7ter", "teu\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.29": {"line.1": {"text": "Er hat zwei h\u00f6rner als ein ochs", "tokens": ["Er", "hat", "zwei", "h\u00f6r\u00b7ner", "als", "ein", "ochs"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "CARD", "ADJA", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seine seufzen seind feurflammen,", "tokens": ["und", "sei\u00b7ne", "seuf\u00b7zen", "seind", "feur\u00b7flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dem dunder gleich ist seine ", "tokens": ["dem", "dun\u00b7der", "gleich", "ist", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADV", "VAFIN", "PPOSAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "weil er von aller strahlen stammen:", "tokens": ["weil", "er", "von", "al\u00b7ler", "strah\u00b7len", "stam\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "seine augen, wan es ", "tokens": ["sei\u00b7ne", "au\u00b7gen", ",", "wan", "es"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "klar brennen:", "tokens": ["klar", "bren\u00b7nen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "ist es tag, so ist er ", "tokens": ["ist", "es", "tag", ",", "so", "ist", "er"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NN", "$,", "ADV", "VAFIN", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "finsternus zu nennen.", "tokens": ["fins\u00b7ter\u00b7nus", "zu", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "PTKZU", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.30": {"line.1": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.2": {"text": "dabei mein herz an sie gedenket,", "tokens": ["da\u00b7bei", "mein", "herz", "an", "sie", "ge\u00b7den\u00b7ket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dieweil zuvor die h\u00fcbsche ", "tokens": ["die\u00b7weil", "zu\u00b7vor", "die", "h\u00fcb\u00b7sche"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "dieselbig ihr aus lieb geschenket:", "tokens": ["die\u00b7sel\u00b7big", "ihr", "aus", "lieb", "ge\u00b7schen\u00b7ket", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wie er, hab ich mit ihr f\u00fcchs", "tokens": ["wie", "er", ",", "hab", "ich", "mit", "ihr", "f\u00fcchs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "$,", "VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "gejaget,", "tokens": ["ge\u00b7ja\u00b7get", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "war es regen oder ", "tokens": ["war", "es", "re\u00b7gen", "o\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADJA", "KON"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "hab ich es gewaget.", "tokens": ["hab", "ich", "es", "ge\u00b7wa\u00b7get", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.31": {"line.1": {"text": "Gleichwie ein doppelt klare ", "tokens": ["Gleich\u00b7wie", "ein", "dop\u00b7pelt", "kla\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJD", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die anblick ihrer augen leuchten:", "tokens": ["die", "an\u00b7blick", "ih\u00b7rer", "au\u00b7gen", "leuch\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "vor ihrem man ein T\u00fcrk, ein ", "tokens": ["vor", "ih\u00b7rem", "man", "ein", "T\u00fcrk", ",", "ein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "PIS", "ART", "NN", "$,", "ART"], "meter": "-+---+-", "measure": "dactylic.init"}, "line.4": {"text": "mu\u00df zittern, stinken und bald beichten:", "tokens": ["mu\u00df", "zit\u00b7tern", ",", "stin\u00b7ken", "und", "bald", "beich\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$,", "VVFIN", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ihre magd, die wie ein dachs", "tokens": ["ih\u00b7re", "magd", ",", "die", "wie", "ein", "dachs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "KOKOM", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "sich bucket,", "tokens": ["sich", "bu\u00b7cket", ","], "token_info": ["word", "word", "punct"], "pos": ["PRF", "VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "war ursach, da\u00df sich ", "tokens": ["war", "ur\u00b7sach", ",", "da\u00df", "sich"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VAFIN", "ADJD", "$,", "KOUS", "PRF"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "zwischen uns oft ducket.", "tokens": ["zwi\u00b7schen", "uns", "oft", "du\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.32": {"line.1": {"text": "Wer ist begirig ihres specks,", "tokens": ["Wer", "ist", "be\u00b7gi\u00b7rig", "ih\u00b7res", "specks", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem will ich bald ein bi\u00dflein schneiden:", "tokens": ["dem", "will", "ich", "bald", "ein", "bi\u00df\u00b7lein", "schnei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "ART", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "sehr gro\u00df ist ihrer grillen ", "tokens": ["sehr", "gro\u00df", "ist", "ih\u00b7rer", "gril\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "die kont ich lieber, dan euch, leiden:", "tokens": ["die", "kont", "ich", "lie\u00b7ber", ",", "dan", "euch", ",", "lei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "$,", "ADV", "PPER", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dan ich mag nicht euers dr ...", "tokens": ["dan", "ich", "mag", "nicht", "eu\u00b7ers", "dr", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VMFIN", "PTKNEG", "PPOSAT", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "vergessen:", "tokens": ["ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "drink, da drink, das ist das ", "tokens": ["drink", ",", "da", "drink", ",", "das", "ist", "das"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "$,", "KOUS", "ADJD", "$,", "PDS", "VAFIN", "ART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "welcher nicht will essen.", "tokens": ["wel\u00b7cher", "nicht", "will", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "VMFIN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.33": {"line.1": {"text": "F\u00fcr meine witz ist hie kein ", "tokens": ["F\u00fcr", "mei\u00b7ne", "witz", "ist", "hie", "kein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ADV", "PIAT"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "f\u00fcr mein gesicht kein liecht zu sehen;", "tokens": ["f\u00fcr", "mein", "ge\u00b7sicht", "kein", "liecht", "zu", "se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVPP", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "f\u00fcr meine hand kein kelch, kein ", "tokens": ["f\u00fcr", "mei\u00b7ne", "hand", "kein", "kelch", ",", "kein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIAT", "NN", "$,", "PIAT"], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.4": {"text": "f\u00fcr meine f\u00fc\u00df kein stand zu stehen.", "tokens": ["f\u00fcr", "mei\u00b7ne", "f\u00fc\u00df", "kein", "stand", "zu", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "PIAT", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ho! wer hat mich bei dem haar", "tokens": ["ho", "!", "wer", "hat", "mich", "bei", "dem", "haar"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "PWS", "VAFIN", "PRF", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "geraufet?", "tokens": ["ger\u00b7au\u00b7fet", "?"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "mord, raub! raub, mord! o gefahr", "tokens": ["mord", ",", "raub", "!", "raub", ",", "mord", "!", "o", "ge\u00b7fahr"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["NE", "$,", "VVFIN", "$.", "VVFIN", "$,", "NE", "$.", "FM", "NN"], "meter": "+-----+", "measure": "dactylic.init"}, "line.8": {"text": "alles rund umlaufet.", "tokens": ["al\u00b7les", "rund", "um\u00b7lau\u00b7fet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.34": {"line.1": {"text": "Ach wie kam ich in dises schif?", "tokens": ["Ach", "wie", "kam", "ich", "in", "di\u00b7ses", "schif", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "VVFIN", "PPER", "APPR", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "es grauset mir, ich kan nicht schwimmen.", "tokens": ["es", "grau\u00b7set", "mir", ",", "ich", "kan", "nicht", "schwim\u00b7men", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "hilf, hilf! ein seil, sto\u00df oder grif;", "tokens": ["hilf", ",", "hilf", "!", "ein", "seil", ",", "sto\u00df", "o\u00b7der", "grif", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "VVIMP", "$.", "ART", "NN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ach weh! nu hab ich auch das grimmen.", "tokens": ["ach", "weh", "!", "nu", "hab", "ich", "auch", "das", "grim\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "$.", "ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "alles leider ist umsunst!", "tokens": ["al\u00b7les", "lei\u00b7der", "ist", "um\u00b7sunst", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "VAFIN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "wir sinken.", "tokens": ["wir", "sin\u00b7ken", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "was? ja wol in diser brunst", "tokens": ["was", "?", "ja", "wol", "in", "di\u00b7ser", "brunst"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "$.", "ADV", "ADV", "APPR", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "brennen wir und stinken.", "tokens": ["bren\u00b7nen", "wir", "und", "stin\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.35": {"line.1": {"text": "Ho! helfet! reichet das geschirr!", "tokens": ["Ho", "!", "hel\u00b7fet", "!", "rei\u00b7chet", "das", "ge\u00b7schirr", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "$.", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "es ist umsunst! es ist geschehen!", "tokens": ["es", "ist", "um\u00b7sunst", "!", "es", "ist", "ge\u00b7sche\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$.", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ich bin ganz na\u00df! ich bin ganz d\u00fcrr,", "tokens": ["ich", "bin", "ganz", "na\u00df", "!", "ich", "bin", "ganz", "d\u00fcrr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$.", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "stum, lahm, kan ich nichts h\u00f6ren, sehen.", "tokens": ["stum", ",", "lahm", ",", "kan", "ich", "nichts", "h\u00f6\u00b7ren", ",", "se\u00b7hen", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "PTKVZ", "$,", "VMFIN", "PPER", "PIS", "VVINF", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ach die hagelstein, blitz, strahl", "tokens": ["ach", "die", "ha\u00b7gel\u00b7stein", ",", "blitz", ",", "strahl"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "ART", "NN", "$,", "VVIMP", "$,", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und dunder,", "tokens": ["und", "dun\u00b7der", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "ADJA", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "kommend auf mich auf einmal,", "tokens": ["kom\u00b7mend", "auf", "mich", "auf", "ein\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PRF", "APPR", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "schlagen mich hinunder.", "tokens": ["schla\u00b7gen", "mich", "hin\u00b7un\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.36": {"line.1": {"text": "Wa ist mein fu\u00df, wa meine stirn?", "tokens": ["Wa", "ist", "mein", "fu\u00df", ",", "wa", "mei\u00b7ne", "stirn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "PTKVZ", "$,", "XY", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "oh, mein kopf walzet auf der erden!", "tokens": ["oh", ",", "mein", "kopf", "wal\u00b7zet", "auf", "der", "er\u00b7den", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "halt! ich verlier sunst all mein hirn.", "tokens": ["halt", "!", "ich", "ver\u00b7lier", "sunst", "all", "mein", "hirn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PPER", "ADJD", "ADV", "PIAT", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "was wird doch endlich aus mir werden?", "tokens": ["was", "wird", "doch", "end\u00b7lich", "aus", "mir", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "APPR", "PPER", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ist keine hilf in dieser not", "tokens": ["ist", "kei\u00b7ne", "hilf", "in", "die\u00b7ser", "not"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "zu haben?", "tokens": ["zu", "ha\u00b7ben", "?"], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VAINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "got erbarm es! ich bin tod", "tokens": ["got", "er\u00b7barm", "es", "!", "ich", "bin", "tod"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "ADJD", "PPER", "$.", "PPER", "VAFIN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "und auch schon begraben.", "tokens": ["und", "auch", "schon", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.37": {"line.1": {"text": "Der volle narr, der w\u00fcste fratz", "tokens": ["Der", "vol\u00b7le", "narr", ",", "der", "w\u00fcs\u00b7te", "fratz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so voll besoffen als geschossen,", "tokens": ["so", "voll", "be\u00b7sof\u00b7fen", "als", "ge\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "KOKOM", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "hat als ein stinkend nasser ratz", "tokens": ["hat", "als", "ein", "stin\u00b7kend", "nas\u00b7ser", "ratz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "KOKOM", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sein abenteuer nu beschlossen.", "tokens": ["sein", "a\u00b7bent\u00b7eu\u00b7er", "nu", "be\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und zu ged\u00e4chtnus seiner that", "tokens": ["und", "zu", "ge\u00b7d\u00e4cht\u00b7nus", "sei\u00b7ner", "that"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "soll er hie seine grabschrift sehen,", "tokens": ["soll", "er", "hie", "sei\u00b7ne", "grab\u00b7schrift", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.7": {"text": "wan von dem rausch der grob unflat", "tokens": ["wan", "von", "dem", "rausch", "der", "grob", "un\u00b7flat"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "soll wider wachend auferstehen:", "tokens": ["soll", "wi\u00b7der", "wa\u00b7chend", "auf\u00b7er\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Fratz liget under dieser bank,", "tokens": ["Fratz", "li\u00b7get", "un\u00b7der", "die\u00b7ser", "bank", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "PDS", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "an leib und seel sehr w\u00fcst besudelt,", "tokens": ["an", "leib", "und", "seel", "sehr", "w\u00fcst", "be\u00b7su\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der mancherlei gedrank, gestank", "tokens": ["der", "man\u00b7cher\u00b7lei", "ge\u00b7drank", ",", "ge\u00b7stank"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "PIAT", "NN", "$,", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und sprach vermischet und verhudelt.", "tokens": ["und", "sprach", "ver\u00b7mi\u00b7schet", "und", "ver\u00b7hu\u00b7delt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ach leser, w\u00fcnsch, da\u00df ihm, dir, mir", "tokens": ["Ach", "le\u00b7ser", ",", "w\u00fcnsch", ",", "da\u00df", "ihm", ",", "dir", ",", "mir"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["ITJ", "ITJ", "$,", "ADJD", "$,", "KOUS", "PPER", "$,", "PPER", "$,", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "got das gedeihen wolle geben,", "tokens": ["got", "das", "ge\u00b7dei\u00b7hen", "wol\u00b7le", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "VVINF", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "da\u00df unser jeder, nach geb\u00fchr,", "tokens": ["da\u00df", "un\u00b7ser", "je\u00b7der", ",", "nach", "ge\u00b7b\u00fchr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "PIS", "$,", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "m\u00f6g besser reden, drinken, leben.", "tokens": ["m\u00f6g", "bes\u00b7ser", "re\u00b7den", ",", "drin\u00b7ken", ",", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VMFIN", "ADJD", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Weil nu der luft ganz ungest\u00fcm", "tokens": ["Weil", "nu", "der", "luft", "ganz", "un\u00b7ge\u00b7st\u00fcm"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mit schnee und regen sich vermischet", "tokens": ["mit", "schnee", "und", "re\u00b7gen", "sich", "ver\u00b7mi\u00b7schet"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "ADJA", "PRF", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und nu der wind mit nichten stum", "tokens": ["und", "nu", "der", "wind", "mit", "nich\u00b7ten", "stum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "das erdreich gleichsam seifend waschet;", "tokens": ["das", "er\u00b7dreich", "gleich\u00b7sam", "sei\u00b7fend", "wa\u00b7schet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so lasset uns auch, liebe freind,", "tokens": ["so", "las\u00b7set", "uns", "auch", ",", "lie\u00b7be", "freind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "was sprachen wir auch immer reden,", "tokens": ["was", "spra\u00b7chen", "wir", "auch", "im\u00b7mer", "re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "den tisch bedecken zu der stund", "tokens": ["den", "tisch", "be\u00b7de\u00b7cken", "zu", "der", "stund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "mit flaschen, schunken, k\u00e4s und fladen.", "tokens": ["mit", "fla\u00b7schen", ",", "schun\u00b7ken", ",", "k\u00e4s", "und", "fla\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "$,", "VVFIN", "$,", "ADJA", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "Bring her die gl\u00e4ser und schenk ein.", "tokens": ["Bring", "her", "die", "gl\u00e4\u00b7ser", "und", "schenk", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "KON", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "wem kan zu drinken doch misfallen?", "tokens": ["wem", "kan", "zu", "drin\u00b7ken", "doch", "mis\u00b7fal\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PTKZU", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der wein hat gleichsam den rock an,", "tokens": ["der", "wein", "hat", "gleich\u00b7sam", "den", "rock", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "alsbald wir ihn in ein glas f\u00fcllen:", "tokens": ["als\u00b7bald", "wir", "ihn", "in", "ein", "glas", "f\u00fcl\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "jedoch das rein christallin glas", "tokens": ["je\u00b7doch", "das", "rein", "chris\u00b7tal\u00b7lin", "glas"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "NE", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "des weins leib, nicht die farb bedecket,", "tokens": ["des", "weins", "leib", ",", "nicht", "die", "farb", "be\u00b7de\u00b7cket", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PTKNEG", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "also, o wunder \u00fcbergro\u00df!", "tokens": ["al\u00b7so", ",", "o", "wun\u00b7der", "\u00fc\u00b7ber\u00b7gro\u00df", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "FM", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "den wein und unser aug erquicket.", "tokens": ["den", "wein", "und", "un\u00b7ser", "aug", "er\u00b7quic\u00b7ket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Dan er kaum rinnet aus dem loch", "tokens": ["Dan", "er", "kaum", "rin\u00b7net", "aus", "dem", "loch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "der schwangern kanten oder flaschen,", "tokens": ["der", "schwan\u00b7gern", "kan\u00b7ten", "o\u00b7der", "fla\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVINF", "VMFIN", "KON", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df wie er schmollet, ich auch lach,", "tokens": ["da\u00df", "wie", "er", "schmol\u00b7let", ",", "ich", "auch", "lach", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "PPER", "VVFIN", "$,", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "begehrend mich bald zu erfrischen:", "tokens": ["be\u00b7geh\u00b7rend", "mich", "bald", "zu", "er\u00b7fri\u00b7schen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "halt ich ihn dan in meiner hand,", "tokens": ["halt", "ich", "ihn", "dan", "in", "mei\u00b7ner", "hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "das aus dem glas er werd gefreiet,", "tokens": ["das", "aus", "dem", "glas", "er", "werd", "ge\u00b7frei\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "VVFIN", "PPER", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "merk ich, da\u00df er mein herz und mund,", "tokens": ["merk", "ich", ",", "da\u00df", "er", "mein", "herz", "und", "mund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "eh da\u00df ich ihn versuch, erfreuet.", "tokens": ["eh", "da\u00df", "ich", "ihn", "ver\u00b7such", ",", "er\u00b7freu\u00b7et", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "PPER", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Darum wer doppeltes gut will", "tokens": ["Da\u00b7rum", "wer", "dop\u00b7pel\u00b7tes", "gut", "will"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "PWS", "ADV", "ADJD", "VMFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "anschauen, riechen, schmecken, sp\u00fcren,", "tokens": ["an\u00b7schau\u00b7en", ",", "rie\u00b7chen", ",", "schme\u00b7cken", ",", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der mu\u00df nu einen becher voll", "tokens": ["der", "mu\u00df", "nu", "ei\u00b7nen", "be\u00b7cher", "voll"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "ADV", "ART", "ADJA", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "des edlen rebensafts nicht sparen,", "tokens": ["des", "ed\u00b7len", "re\u00b7bens\u00b7afts", "nicht", "spa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so nem ein jeder sein gesch\u00fctz", "tokens": ["so", "nem", "ein", "je\u00b7der", "sein", "ge\u00b7sch\u00fctz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "PIAT", "PPOSAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "und eh wir es zugleich hinrichten,", "tokens": ["und", "eh", "wir", "es", "zu\u00b7gleich", "hin\u00b7rich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "mu\u00df er mit mir den reichen schatz", "tokens": ["mu\u00df", "er", "mit", "mir", "den", "rei\u00b7chen", "schatz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "PPER", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "zu loben, singend nicht verachten.", "tokens": ["zu", "lo\u00b7ben", ",", "sin\u00b7gend", "nicht", "ver\u00b7ach\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "ADJD", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Dieweil nu dises ein Rheinwein", "tokens": ["Die\u00b7weil", "nu", "di\u00b7ses", "ein", "Rhein\u00b7wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PDAT", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "oder dem Rheinwein zu vergleichen,", "tokens": ["o\u00b7der", "dem", "Rhein\u00b7wein", "zu", "ver\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "so schenk ihn in den becher ein,", "tokens": ["so", "schenk", "ihn", "in", "den", "be\u00b7cher", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "ihn mit gold noch mehr zu bereichen.", "tokens": ["ihn", "mit", "gold", "noch", "mehr", "zu", "be\u00b7rei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "NN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "er ist klar, lieblich, frisch und reich,", "tokens": ["er", "ist", "klar", ",", "lieb\u00b7lich", ",", "frisch", "und", "reich", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "ADJD", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "darum mu\u00df er herum passieren:", "tokens": ["da\u00b7rum", "mu\u00df", "er", "he\u00b7rum", "pas\u00b7sie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "APZR", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "da\u00df keiner m\u00f6g die zeit verlieren.", "tokens": ["da\u00df", "kei\u00b7ner", "m\u00f6g", "die", "zeit", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Ist jener rotwein ein Franzos,", "tokens": ["Ist", "je\u00b7ner", "rot\u00b7wein", "ein", "Fran\u00b7zos", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so thut er wol, zu uns zu kommen;", "tokens": ["so", "thut", "er", "wol", ",", "zu", "uns", "zu", "kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "er l\u00e4chelt wie ein rote ros", "tokens": ["er", "l\u00e4\u00b7chelt", "wie", "ein", "ro\u00b7te", "ros"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und wird von uns gern angenommen.", "tokens": ["und", "wird", "von", "uns", "gern", "an\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ich h\u00f6r nicht mehr des winds get\u00f6s,", "tokens": ["ich", "h\u00f6r", "nicht", "mehr", "des", "winds", "ge\u00b7t\u00f6s", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "sither wir mit dem wein parlieren.", "tokens": ["si\u00b7ther", "wir", "mit", "dem", "wein", "par\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so lasset uns all garaussieren.", "tokens": ["so", "las\u00b7set", "uns", "all", "ga\u00b7raus\u00b7sie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Ho! wein her, den uns das Welschland", "tokens": ["Ho", "!", "wein", "her", ",", "den", "uns", "das", "Wel\u00b7schland"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NN", "PTKVZ", "$,", "PRELS", "PPER", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "ohn des Bapst sig und segen sendet.", "tokens": ["ohn", "des", "Bapst", "sig", "und", "se\u00b7gen", "sen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "KON", "VVINF", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "ein schalen voll in meiner hand", "tokens": ["ein", "scha\u00b7len", "voll", "in", "mei\u00b7ner", "hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJD", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "davon, wird bald wol angewendet:", "tokens": ["da\u00b7von", ",", "wird", "bald", "wol", "an\u00b7ge\u00b7wen\u00b7det", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "die farb ist angenehm, ich sih,", "tokens": ["die", "farb", "ist", "an\u00b7ge\u00b7nehm", ",", "ich", "sih", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und sein geruch thut excellieren:", "tokens": ["und", "sein", "ge\u00b7ruch", "thut", "ex\u00b7cel\u00b7lie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "er kan nicht dan euch aggradieren.", "tokens": ["er", "kan", "nicht", "dan", "euch", "ag\u00b7gra\u00b7die\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Ein ander Welschland wei\u00df ich noch,", "tokens": ["Ein", "an\u00b7der", "Wel\u00b7schland", "wei\u00df", "ich", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da man auch zechend fr\u00f6lich lebet", "tokens": ["da", "man", "auch", "ze\u00b7chend", "fr\u00f6\u00b7lich", "le\u00b7bet"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "ADJD", "VVFIN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "mit brot und k\u00e4s und ohn den koch,", "tokens": ["mit", "brot", "und", "k\u00e4s", "und", "ohn", "den", "koch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "KON", "APPR", "ART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "schier Schweizer gleich, nach ehren strebet:", "tokens": ["schier", "Schwei\u00b7zer", "gleich", ",", "nach", "eh\u00b7ren", "stre\u00b7bet", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "$,", "APPR", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "reich her das volle kr\u00e4uslein da,", "tokens": ["reich", "her", "das", "vol\u00b7le", "kr\u00e4us\u00b7lein", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "es gilt den herren und den frauen", "tokens": ["es", "gilt", "den", "her\u00b7ren", "und", "den", "frau\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so das ist artlich ", "tokens": ["so", "das", "ist", "art\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PDS", "VAFIN", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.47": {"line.1": {"text": "Ist Engelland schon ohn Weinwachs,", "tokens": ["Ist", "En\u00b7gel\u00b7land", "schon", "ohn", "Wein\u00b7wachs", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "APPR", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "hat man doch gute wein darinnen,", "tokens": ["hat", "man", "doch", "gu\u00b7te", "wein", "da\u00b7rin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "ADJA", "NN", "ADV", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "und mancher drinket als ein Sachs,", "tokens": ["und", "man\u00b7cher", "drin\u00b7ket", "als", "ein", "Sachs", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wan er die schlacht gern wolt gewinnen:", "tokens": ["wan", "er", "die", "schlacht", "gern", "wolt", "ge\u00b7win\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "VVFIN", "ADV", "VMFIN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "drink mir ein glas des besten zu,", "tokens": ["drink", "mir", "ein", "glas", "des", "bes\u00b7ten", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ART", "ADJA", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "mit welchem die insuln prachtieren,", "tokens": ["mit", "wel\u00b7chem", "die", "in\u00b7suln", "prach\u00b7tie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "kan ein wein disen surpassieren?", "tokens": ["kan", "ein", "wein", "di\u00b7sen", "sur\u00b7pas\u00b7sie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVINF", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.48": {"line.1": {"text": "Die Niederteutsche, frische fisch,", "tokens": ["Die", "Nie\u00b7der\u00b7teut\u00b7sche", ",", "fri\u00b7sche", "fisch", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "die lang gern hinder dem tisch sitzen,", "tokens": ["die", "lang", "gern", "hin\u00b7der", "dem", "tisch", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "APPR", "ART", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "lieben den wein, der stark und frisch,", "tokens": ["lie\u00b7ben", "den", "wein", ",", "der", "stark", "und", "frisch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "PRELS", "ADJD", "KON", "ADJD", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "und zechen gern, bis da\u00df sie schwitzen:", "tokens": ["und", "ze\u00b7chen", "gern", ",", "bis", "da\u00df", "sie", "schwit\u00b7zen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "$,", "KON", "KOUS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so gib auch ihrentwegen nu", "tokens": ["so", "gib", "auch", "ih\u00b7rent\u00b7we\u00b7gen", "nu"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "ADV", "PPOSAT", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "den gro\u00dfen kelch, damit zu zehren", "tokens": ["den", "gro\u00b7\u00dfen", "kelch", ",", "da\u00b7mit", "zu", "zeh\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PAV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so: dises hei\u00dfet recht laveeren.", "tokens": ["so", ":", "di\u00b7ses", "hei\u00b7\u00dfet", "recht", "la\u00b7vee\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PDS", "VVFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Seid ihr den Spaniern hie feind,", "tokens": ["Seid", "ihr", "den", "Spa\u00b7ni\u00b7ern", "hie", "feind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "so langsam ihrer zu gedenken?", "tokens": ["so", "lang\u00b7sam", "ih\u00b7rer", "zu", "ge\u00b7den\u00b7ken", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "seind sie doch aller l\u00e4nder freind,", "tokens": ["seind", "sie", "doch", "al\u00b7ler", "l\u00e4n\u00b7der", "freind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wan sie den wein schon nicht verschenken.", "tokens": ["wan", "sie", "den", "wein", "schon", "nicht", "ver\u00b7schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "gib ihres weins das gl\u00e4slein da,", "tokens": ["gib", "ih\u00b7res", "weins", "das", "gl\u00e4s\u00b7lein", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "ADJA", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "damit ich besser m\u00f6g hablieren.", "tokens": ["da\u00b7mit", "ich", "bes\u00b7ser", "m\u00f6g", "hab\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "wer will mag ", "tokens": ["wer", "will", "mag"], "token_info": ["word", "word", "word"], "pos": ["PWS", "VMFIN", "VMFIN"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.50": {"line.1": {"text": "In Irland war ich auch einmal", "tokens": ["In", "Ir\u00b7land", "war", "ich", "auch", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VAFIN", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und sah dort manche ding verwirren,", "tokens": ["und", "sah", "dort", "man\u00b7che", "ding", "ver\u00b7wir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "doch wissend wol die rechte wahl,", "tokens": ["doch", "wis\u00b7send", "wol", "die", "rech\u00b7te", "wahl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "lie\u00df ich mich billich nicht verirren.", "tokens": ["lie\u00df", "ich", "mich", "bil\u00b7lich", "nicht", "ver\u00b7ir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.5": {"text": "schenk ein ein wenig Usquebagh,", "tokens": ["schenk", "ein", "ein", "we\u00b7nig", "Us\u00b7queb\u00b7agh", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "in Irland \u00fcberal geliebet:", "tokens": ["in", "Ir\u00b7land", "\u00fc\u00b7be\u00b7ral", "ge\u00b7lie\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "so, dieses hei\u00dfet wol ge\u00fcbet.", "tokens": ["so", ",", "die\u00b7ses", "hei\u00b7\u00dfet", "wol", "ge\u00b7\u00fc\u00b7bet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PDS", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "H\u00f6r ich nicht Fratzen, den dickkopf,", "tokens": ["H\u00f6r", "ich", "nicht", "Frat\u00b7zen", ",", "den", "dick\u00b7kopf", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "PPER", "PTKNEG", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "der witzlos jederman will lehren?", "tokens": ["der", "witz\u00b7los", "je\u00b7der\u00b7man", "will", "leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welcher ein recht grober knopf", "tokens": ["und", "wel\u00b7cher", "ein", "recht", "gro\u00b7ber", "knopf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ohn sich selbs niemand sunst will ehren?", "tokens": ["ohn", "sich", "selbs", "nie\u00b7mand", "sunst", "will", "eh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "ADV", "PIS", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "es gilt hie sechs, in einem suff,", "tokens": ["es", "gilt", "hie", "sechs", ",", "in", "ei\u00b7nem", "suff", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "CARD", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "herr Fratz, ihr m\u00fcsset das aussaufen,", "tokens": ["herr", "Fratz", ",", "ihr", "m\u00fcs\u00b7set", "das", "aus\u00b7sau\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PPER", "VMFIN", "PDS", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "es gilt Fratz Curly Murly Buff", "tokens": ["es", "gilt", "Fratz", "Cur\u00b7ly", "Mur\u00b7ly", "Buff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NE", "NE", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "bis alle fallen \u00fcbern haufen.", "tokens": ["bis", "al\u00b7le", "fal\u00b7len", "\u00fc\u00b7bern", "hau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Ich glaub, ihr liebe ", "tokens": ["Ich", "glaub", ",", "ihr", "lie\u00b7be"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "da\u00df ihr das Latein gar verschworen", "tokens": ["da\u00df", "ihr", "das", "La\u00b7tein", "gar", "ver\u00b7schwo\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und auch das griechisch, als ich sih,", "tokens": ["und", "auch", "das", "grie\u00b7chisch", ",", "als", "ich", "sih", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJD", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ist nu verachtet und verloren:", "tokens": ["ist", "nu", "ver\u00b7ach\u00b7tet", "und", "ver\u00b7lo\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVFIN", "KON", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "doch weil ein christliches r\u00e4uschlein", "tokens": ["doch", "weil", "ein", "christ\u00b7li\u00b7ches", "r\u00e4usc\u00b7hlein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "nicht kan, ", "tokens": ["nicht", "kan", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "bring ich euch, ", "tokens": ["bring", "ich", "euch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "und wolt euch jetz nicht gern turbieren.", "tokens": ["und", "wolt", "euch", "jetz", "nicht", "gern", "tur\u00b7bie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Ho! herr Fratz, was bedeuten doch", "tokens": ["Ho", "!", "herr", "Fratz", ",", "was", "be\u00b7deu\u00b7ten", "doch"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "$.", "NN", "NN", "$,", "PWS", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schmorotzer, blacken und bacchanten,", "tokens": ["schmo\u00b7rot\u00b7zer", ",", "bla\u00b7cken", "und", "bac\u00b7chan\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "VVINF", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "die so verhasset von dem koch", "tokens": ["die", "so", "ver\u00b7has\u00b7set", "von", "dem", "koch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "als schulf\u00fcchs, penal und pedanten?", "tokens": ["als", "schul\u00b7f\u00fcchs", ",", "pe\u00b7nal", "und", "pe\u00b7dan\u00b7ten", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "$,", "ADJD", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "warum darf ohn ein narrenkapp,", "tokens": ["wa\u00b7rum", "darf", "ohn", "ein", "nar\u00b7ren\u00b7kapp", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "APPR", "ART", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ein narr halb welsch und halb teutsch glotzen?", "tokens": ["ein", "narr", "halb", "welsch", "und", "halb", "teutsch", "glot\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "ADJD", "KON", "ADJD", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "warum doch will ein jeder lapp", "tokens": ["wa\u00b7rum", "doch", "will", "ein", "je\u00b7der", "lapp"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "f\u00fcr gut teutsch ", "tokens": ["f\u00fcr", "gut", "teutsch"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJD", "ADJD"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.54": {"line.1": {"text": "Ist es nicht eines bl\u00f6den hirns", "tokens": ["Ist", "es", "nicht", "ei\u00b7nes", "bl\u00f6\u00b7den", "hirns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PTKNEG", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und eines hasenkopfs merkzeichen,", "tokens": ["und", "ei\u00b7nes", "ha\u00b7sen\u00b7kopfs", "merk\u00b7zei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der wol wert eines langen horns", "tokens": ["der", "wol", "wert", "ei\u00b7nes", "lan\u00b7gen", "horns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADJD", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und gar nicht wert mit uns zu zechen?", "tokens": ["und", "gar", "nicht", "wert", "mit", "uns", "zu", "ze\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADJD", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "mit uns, die wir dem guten wein", "tokens": ["mit", "uns", ",", "die", "wir", "dem", "gu\u00b7ten", "wein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "$,", "PRELS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "allein zu ehren welsch gegecket,", "tokens": ["al\u00b7lein", "zu", "eh\u00b7ren", "welsch", "ge\u00b7ge\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "und doch mit gr\u00f6\u00dferm flei\u00df und wohn", "tokens": ["und", "doch", "mit", "gr\u00f6\u00b7\u00dferm", "flei\u00df", "und", "wohn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "in der welt gro\u00dfes buch gegucket?", "tokens": ["in", "der", "welt", "gro\u00b7\u00dfes", "buch", "ge\u00b7gu\u00b7cket", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Gl\u00fcck zu, du ohn ein g gesell,", "tokens": ["Gl\u00fcck", "zu", ",", "du", "ohn", "ein", "g", "ge\u00b7sell", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,", "PPER", "APPR", "ART", "NN", "ADJD", "$,"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "hat mich der ", "tokens": ["hat", "mich", "der"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "er glaub mir, da\u00df dem ", "tokens": ["er", "glaub", "mir", ",", "da\u00df", "dem"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "ich aufwarten in wenig stunden;", "tokens": ["ich", "auf\u00b7war\u00b7ten", "in", "we\u00b7nig", "stun\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "dan ", "tokens": ["dan"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "so hat er weit gevoyagieret.", "tokens": ["so", "hat", "er", "weit", "ge\u00b7vo\u00b7ya\u00b7gie\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "der teufel hol euch, ohn ein n", "tokens": ["der", "teu\u00b7fel", "hol", "euch", ",", "ohn", "ein", "n"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "KOUI", "ART", "XY"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "herr Hans, weil ihr uns all vexieret.", "tokens": ["herr", "Hans", ",", "weil", "ihr", "uns", "all", "ve\u00b7xie\u00b7ret", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "KOUS", "PPER", "PRF", "PIAT", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Wer teutsch ist, der red auch gut teutsch,", "tokens": ["Wer", "teutsch", "ist", ",", "der", "red", "auch", "gut", "teutsch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$,", "PRELS", "VVFIN", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "wie der Welsch will gut welsch parlieren:", "tokens": ["wie", "der", "Welsch", "will", "gut", "welsch", "par\u00b7lie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VMFIN", "ADJD", "ADJD", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "zu fu\u00df geh, wer ohn pferd und gutsch,", "tokens": ["zu", "fu\u00df", "geh", ",", "wer", "ohn", "pferd", "und", "gutsch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "PTKVZ", "VVFIN", "$,", "PWS", "APPR", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und wer ein narr, kan nicht vil lehren.", "tokens": ["und", "wer", "ein", "narr", ",", "kan", "nicht", "vil", "leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "ADJD", "$,", "VMFIN", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "so sprechet nu ein urtheil aus,", "tokens": ["so", "spre\u00b7chet", "nu", "ein", "ur\u00b7theil", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und m\u00e4niglich mag es wol h\u00f6ren,", "tokens": ["und", "m\u00e4\u00b7nig\u00b7lich", "mag", "es", "wol", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.7": {"text": "gleich ist ein halbwelschteutscher has", "tokens": ["gleich", "ist", "ein", "halb\u00b7wel\u00b7schteut\u00b7scher", "has"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "den angestrichnen kranken huren.", "tokens": ["den", "an\u00b7ge\u00b7strich\u00b7nen", "kran\u00b7ken", "hu\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Und gleich wie der ein schwein, gans, kalb,", "tokens": ["Und", "gleich", "wie", "der", "ein", "schwein", ",", "gans", ",", "kalb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "ART", "ART", "ADJD", "$,", "ADJA", "$,", "ADJD", "$,"], "meter": "-----+-+", "measure": "unknown.measure.di"}, "line.2": {"text": "der gut und b\u00f6sen wein vermischet,", "tokens": ["der", "gut", "und", "b\u00f6\u00b7sen", "wein", "ver\u00b7mi\u00b7schet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "so dem geh\u00f6rt ein narrenkolb,", "tokens": ["so", "dem", "ge\u00b7h\u00f6rt", "ein", "nar\u00b7ren\u00b7kolb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VVFIN", "ART", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der teutsch und welsch zusammenwaschet;", "tokens": ["der", "teutsch", "und", "welsch", "zu\u00b7sam\u00b7men\u00b7wa\u00b7schet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "sein hirn und red seind gelb, wei\u00df, schwarz,", "tokens": ["sein", "hirn", "und", "red", "seind", "gelb", ",", "wei\u00df", ",", "schwarz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "VVFIN", "VAFIN", "ADJD", "$,", "VVFIN", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "gr\u00fcn, rot und blau, ein schneiderk\u00fcssen,", "tokens": ["gr\u00fcn", ",", "rot", "und", "blau", ",", "ein", "schnei\u00b7der\u00b7k\u00fcs\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "KON", "ADJD", "$,", "ART", "ADJA", "$,"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "ein alter schurz, ein lahmer scherz", "tokens": ["ein", "al\u00b7ter", "schurz", ",", "ein", "lah\u00b7mer", "scherz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJD", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und ganz unw\u00fcrdig mehrer bossen.", "tokens": ["und", "ganz", "un\u00b7w\u00fcr\u00b7dig", "meh\u00b7rer", "bos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "PIAT", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Kom, schenkend das glas wider ein,", "tokens": ["Kom", ",", "schen\u00b7kend", "das", "glas", "wi\u00b7der", "ein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADJD", "PDS", "VVFIN", "APPR", "ART", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "uns des lusts wider zu begaben,", "tokens": ["uns", "des", "lusts", "wi\u00b7der", "zu", "be\u00b7ga\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df drinkend, singend, redend rein,", "tokens": ["da\u00df", "drin\u00b7kend", ",", "sin\u00b7gend", ",", "re\u00b7dend", "rein", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "$,", "ADJD", "$,", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wir uns und ander auch erlaben.", "tokens": ["wir", "uns", "und", "an\u00b7der", "auch", "er\u00b7la\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "KON", "ADJD", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "doch drink wer will; ich hab zu vil;", "tokens": ["doch", "drink", "wer", "will", ";", "ich", "hab", "zu", "vil", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PWS", "VMFIN", "$.", "PPER", "VAFIN", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wer will mag danzen, drinken, springen,", "tokens": ["wer", "will", "mag", "dan\u00b7zen", ",", "drin\u00b7ken", ",", "sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWS", "VMFIN", "VMFIN", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "frei bleibet jedem alles spil,", "tokens": ["frei", "blei\u00b7bet", "je\u00b7dem", "al\u00b7les", "spil", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PIAT", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und wer will mag nu mit mir singen.", "tokens": ["und", "wer", "will", "mag", "nu", "mit", "mir", "sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VMFIN", "VMFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Frisch auf, frisch auf, seid wol zu mut!", "tokens": ["Frisch", "auf", ",", "frisch", "auf", ",", "seid", "wol", "zu", "mut", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$,", "ADJD", "PTKVZ", "$,", "VAFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "herum das gl\u00e4slein bald mu\u00df fahren:", "tokens": ["he\u00b7rum", "das", "gl\u00e4s\u00b7lein", "bald", "mu\u00df", "fah\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "b\u00f6s ist das wetter, der wein gut,", "tokens": ["b\u00f6s", "ist", "das", "wet\u00b7ter", ",", "der", "wein", "gut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "$,", "PRELS", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und ihrer keines nu zu sparen.", "tokens": ["und", "ih\u00b7rer", "kei\u00b7nes", "nu", "zu", "spa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "PIS", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "der wein sparet zwar die witz", "tokens": ["der", "wein", "spa\u00b7ret", "zwar", "die", "witz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "mit nichten,", "tokens": ["mit", "nich\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "PIS", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "weil er mit zu starker hitz", "tokens": ["weil", "er", "mit", "zu", "star\u00b7ker", "hitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "APPR", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "dr\u00fccknet unser dichten.", "tokens": ["dr\u00fcck\u00b7net", "un\u00b7ser", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.60": {"line.1": {"text": "Ich wei\u00df zwar wol noch wa ich bin,", "tokens": ["Ich", "wei\u00df", "zwar", "wol", "noch", "wa", "ich", "bin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADV", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "darf aber wol f\u00fcr etlich schw\u00f6ren,", "tokens": ["darf", "a\u00b7ber", "wol", "f\u00fcr", "et\u00b7lich", "schw\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "APPR", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df sie sehr gern ihr herz und sin", "tokens": ["da\u00df", "sie", "sehr", "gern", "ihr", "herz", "und", "sin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "all tag verbausen und verzehren.", "tokens": ["all", "tag", "ver\u00b7bau\u00b7sen", "und", "ver\u00b7zeh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "KON", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "bleibt ihr verstand ohn wein", "tokens": ["bleibt", "ihr", "ver\u00b7stand", "ohn", "wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "dahinden", "tokens": ["da\u00b7hin\u00b7den"], "token_info": ["word"], "pos": ["PAV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "k\u00f6nden sie als stock und stein,", "tokens": ["k\u00f6n\u00b7den", "sie", "als", "stock", "und", "stein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "KOUS", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "drinkend ihn nicht finden.", "tokens": ["drin\u00b7kend", "ihn", "nicht", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.61": {"line.1": {"text": "Sih da, wie weis der ", "tokens": ["Sih", "da", ",", "wie", "weis", "der"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "$,", "PWAV", "ADJD", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "sich under uns alhie erzeiget!", "tokens": ["sich", "un\u00b7der", "uns", "al\u00b7hie", "er\u00b7zei\u00b7get", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "KON", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "er bei\u00df mir doch auf dise nu\u00df!", "tokens": ["er", "bei\u00df", "mir", "doch", "auf", "di\u00b7se", "nu\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "sprach Fratz, mit drinken nicht geschweiget.", "tokens": ["sprach", "Fratz", ",", "mit", "drin\u00b7ken", "nicht", "ge\u00b7schwei\u00b7get", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "APPR", "VVFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "selbs reimen,", "tokens": ["selbs", "rei\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "besser dan ihr, ja dan du,", "tokens": ["bes\u00b7ser", "dan", "ihr", ",", "ja", "dan", "du", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "PPER", "$,", "ADV", "ADV", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "und das loch verleimen.", "tokens": ["und", "das", "loch", "ver\u00b7lei\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.62": {"line.1": {"text": "Ich hab die l\u00e4nder diser welt", "tokens": ["Ich", "hab", "die", "l\u00e4n\u00b7der", "di\u00b7ser", "welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "schon vil jahr her gedurchmarschieret,", "tokens": ["schon", "vil", "jahr", "her", "ge\u00b7durch\u00b7mar\u00b7schie\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "APZR", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und hab auch ", "tokens": ["und", "hab", "auch"], "token_info": ["word", "word", "word"], "pos": ["KON", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "dan all ihr Welsche, verspendieret;", "tokens": ["dan", "all", "ihr", "Wel\u00b7sche", ",", "ver\u00b7spen\u00b7die\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PIAT", "PPOSAT", "NN", "$,", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "kan ich aber nicht vil welsch", "tokens": ["kan", "ich", "a\u00b7ber", "nicht", "vil", "welsch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "parlieren,", "tokens": ["par\u00b7lie\u00b7ren", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "so kan ich doch, gar nicht falsch,", "tokens": ["so", "kan", "ich", "doch", ",", "gar", "nicht", "falsch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "$,", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "meinen becher leeren.", "tokens": ["mei\u00b7nen", "be\u00b7cher", "lee\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.63": {"line.1": {"text": "Ihr herren, ich brauch keine list,", "tokens": ["Ihr", "her\u00b7ren", ",", "ich", "brauch", "kei\u00b7ne", "list", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "ich drink und hab nichts zu bedenken;", "tokens": ["ich", "drink", "und", "hab", "nichts", "zu", "be\u00b7den\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "KON", "VAFIN", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "zu drinken ist all mein lust,", "tokens": ["zu", "drin\u00b7ken", "ist", "all", "mein", "lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "PIAT", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "es gilt, und solt mir keiner danken.", "tokens": ["es", "gilt", ",", "und", "solt", "mir", "kei\u00b7ner", "dan\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wil dan eurer keiner mir", "tokens": ["wil", "dan", "eu\u00b7rer", "kei\u00b7ner", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "PPOSAT", "PIS", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "antworten,", "tokens": ["ant\u00b7wor\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "sollet ihr auch, bis ich mehr", "tokens": ["sol\u00b7let", "ihr", "auch", ",", "bis", "ich", "mehr"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "$,", "KOUS", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "euch hofiere, warten.", "tokens": ["euch", "ho\u00b7fie\u00b7re", ",", "war\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.64": {"line.1": {"text": "Wie oft hab ich mit einem wort", "tokens": ["Wie", "oft", "hab", "ich", "mit", "ei\u00b7nem", "wort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verjaget manche dolle katzen?", "tokens": ["ver\u00b7ja\u00b7get", "man\u00b7che", "dol\u00b7le", "kat\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wie oft hab ich mit meinem schwert", "tokens": ["wie", "oft", "hab", "ich", "mit", "mei\u00b7nem", "schwert"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "zerhacket manchen dollen kauzen?", "tokens": ["zer\u00b7ha\u00b7cket", "man\u00b7chen", "dol\u00b7len", "kau\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dise faust hat so vil blut", "tokens": ["di\u00b7se", "faust", "hat", "so", "vil", "blut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "VAFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "vergossen,", "tokens": ["ver\u00b7gos\u00b7sen", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "da\u00df ohn blut kein stein, baum, blat,", "tokens": ["da\u00df", "ohn", "blut", "kein", "stein", ",", "baum", ",", "blat", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PIAT", "NN", "$,", "ADV", "$,", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "keine w\u00e4ld, feld, gassen.", "tokens": ["kei\u00b7ne", "w\u00e4ld", ",", "feld", ",", "gas\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PIAT", "NN", "$,", "NN", "$,", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.65": {"line.1": {"text": "So bin ich auch oft auf dem meer", "tokens": ["So", "bin", "ich", "auch", "oft", "auf", "dem", "meer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "schier in der sonnen selbs ersoffen:", "tokens": ["schier", "in", "der", "son\u00b7nen", "selbs", "er\u00b7sof\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "daher ich auch schwarz als ein mohr", "tokens": ["da\u00b7her", "ich", "auch", "schwarz", "als", "ein", "mohr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "ADV", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "hat mit der Venus oft zu schaffen:", "tokens": ["hat", "mit", "der", "Ve\u00b7nus", "oft", "zu", "schaf\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "und durch manchen hei\u00dfen schmatz", "tokens": ["und", "durch", "man\u00b7chen", "hei\u00b7\u00dfen", "schmatz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "verliebet", "tokens": ["ver\u00b7lie\u00b7bet"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "hat der Proserpina schmotz", "tokens": ["hat", "der", "Pro\u00b7ser\u00b7pi\u00b7na", "schmotz"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NE", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "oft mein herz erlabet.", "tokens": ["oft", "mein", "herz", "er\u00b7la\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.66": {"line.1": {"text": "Was hat sie unter ihrem belz,", "tokens": ["Was", "hat", "sie", "un\u00b7ter", "ih\u00b7rem", "belz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df sie sich lie\u00df so gern aufsch\u00fcrzen?", "tokens": ["da\u00df", "sie", "sich", "lie\u00df", "so", "gern", "auf\u00b7sch\u00fcr\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ich wei\u00df nicht was f\u00fcr Plutons bolz,", "tokens": ["ich", "wei\u00df", "nicht", "was", "f\u00fcr", "Plu\u00b7tons", "bolz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PRELS", "APPR", "NE", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "der pflag gar teufelisch zu scherzen.", "tokens": ["der", "pflag", "gar", "teu\u00b7fe\u00b7lisch", "zu", "scher\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ha! er ist ein arger fuchs", "tokens": ["ha", "!", "er", "ist", "ein", "ar\u00b7ger", "fuchs"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "ohn zweifel,", "tokens": ["ohn", "zwei\u00b7fel", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "er ist alles \u00fcbels ", "tokens": ["er", "ist", "al\u00b7les", "\u00fc\u00b7bels"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIS", "ADV"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "und ein rechter teufel.", "tokens": ["und", "ein", "rech\u00b7ter", "teu\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.67": {"line.1": {"text": "Er hat zwei h\u00f6rner als ein ochs", "tokens": ["Er", "hat", "zwei", "h\u00f6r\u00b7ner", "als", "ein", "ochs"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "CARD", "ADJA", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und seine seufzen seind feurflammen,", "tokens": ["und", "sei\u00b7ne", "seuf\u00b7zen", "seind", "feur\u00b7flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dem dunder gleich ist seine ", "tokens": ["dem", "dun\u00b7der", "gleich", "ist", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADV", "VAFIN", "PPOSAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "weil er von aller strahlen stammen:", "tokens": ["weil", "er", "von", "al\u00b7ler", "strah\u00b7len", "stam\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "seine augen, wan es ", "tokens": ["sei\u00b7ne", "au\u00b7gen", ",", "wan", "es"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "klar brennen:", "tokens": ["klar", "bren\u00b7nen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "ist es tag, so ist er ", "tokens": ["ist", "es", "tag", ",", "so", "ist", "er"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NN", "$,", "ADV", "VAFIN", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "finsternus zu nennen.", "tokens": ["fins\u00b7ter\u00b7nus", "zu", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "PTKZU", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.68": {"line.1": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.2": {"text": "dabei mein herz an sie gedenket,", "tokens": ["da\u00b7bei", "mein", "herz", "an", "sie", "ge\u00b7den\u00b7ket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dieweil zuvor die h\u00fcbsche ", "tokens": ["die\u00b7weil", "zu\u00b7vor", "die", "h\u00fcb\u00b7sche"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "dieselbig ihr aus lieb geschenket:", "tokens": ["die\u00b7sel\u00b7big", "ihr", "aus", "lieb", "ge\u00b7schen\u00b7ket", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wie er, hab ich mit ihr f\u00fcchs", "tokens": ["wie", "er", ",", "hab", "ich", "mit", "ihr", "f\u00fcchs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "$,", "VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "gejaget,", "tokens": ["ge\u00b7ja\u00b7get", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "war es regen oder ", "tokens": ["war", "es", "re\u00b7gen", "o\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADJA", "KON"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "hab ich es gewaget.", "tokens": ["hab", "ich", "es", "ge\u00b7wa\u00b7get", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.69": {"line.1": {"text": "Gleichwie ein doppelt klare ", "tokens": ["Gleich\u00b7wie", "ein", "dop\u00b7pelt", "kla\u00b7re"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJD", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "die anblick ihrer augen leuchten:", "tokens": ["die", "an\u00b7blick", "ih\u00b7rer", "au\u00b7gen", "leuch\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "vor ihrem man ein T\u00fcrk, ein ", "tokens": ["vor", "ih\u00b7rem", "man", "ein", "T\u00fcrk", ",", "ein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "PIS", "ART", "NN", "$,", "ART"], "meter": "-+---+-", "measure": "dactylic.init"}, "line.4": {"text": "mu\u00df zittern, stinken und bald beichten:", "tokens": ["mu\u00df", "zit\u00b7tern", ",", "stin\u00b7ken", "und", "bald", "beich\u00b7ten", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$,", "VVFIN", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ihre magd, die wie ein dachs", "tokens": ["ih\u00b7re", "magd", ",", "die", "wie", "ein", "dachs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "KOKOM", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "sich bucket,", "tokens": ["sich", "bu\u00b7cket", ","], "token_info": ["word", "word", "punct"], "pos": ["PRF", "VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "war ursach, da\u00df sich ", "tokens": ["war", "ur\u00b7sach", ",", "da\u00df", "sich"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VAFIN", "ADJD", "$,", "KOUS", "PRF"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "zwischen uns oft ducket.", "tokens": ["zwi\u00b7schen", "uns", "oft", "du\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.70": {"line.1": {"text": "Wer ist begirig ihres specks,", "tokens": ["Wer", "ist", "be\u00b7gi\u00b7rig", "ih\u00b7res", "specks", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dem will ich bald ein bi\u00dflein schneiden:", "tokens": ["dem", "will", "ich", "bald", "ein", "bi\u00df\u00b7lein", "schnei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "ART", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "sehr gro\u00df ist ihrer grillen ", "tokens": ["sehr", "gro\u00df", "ist", "ih\u00b7rer", "gril\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "die kont ich lieber, dan euch, leiden:", "tokens": ["die", "kont", "ich", "lie\u00b7ber", ",", "dan", "euch", ",", "lei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "$,", "ADV", "PPER", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "dan ich mag nicht euers dr ...", "tokens": ["dan", "ich", "mag", "nicht", "eu\u00b7ers", "dr", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VMFIN", "PTKNEG", "PPOSAT", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "vergessen:", "tokens": ["ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "drink, da drink, das ist das ", "tokens": ["drink", ",", "da", "drink", ",", "das", "ist", "das"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "$,", "KOUS", "ADJD", "$,", "PDS", "VAFIN", "ART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "welcher nicht will essen.", "tokens": ["wel\u00b7cher", "nicht", "will", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "VMFIN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.71": {"line.1": {"text": "F\u00fcr meine witz ist hie kein ", "tokens": ["F\u00fcr", "mei\u00b7ne", "witz", "ist", "hie", "kein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ADV", "PIAT"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "f\u00fcr mein gesicht kein liecht zu sehen;", "tokens": ["f\u00fcr", "mein", "ge\u00b7sicht", "kein", "liecht", "zu", "se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVPP", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "f\u00fcr meine hand kein kelch, kein ", "tokens": ["f\u00fcr", "mei\u00b7ne", "hand", "kein", "kelch", ",", "kein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIAT", "NN", "$,", "PIAT"], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.4": {"text": "f\u00fcr meine f\u00fc\u00df kein stand zu stehen.", "tokens": ["f\u00fcr", "mei\u00b7ne", "f\u00fc\u00df", "kein", "stand", "zu", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "PIAT", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ho! wer hat mich bei dem haar", "tokens": ["ho", "!", "wer", "hat", "mich", "bei", "dem", "haar"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "PWS", "VAFIN", "PRF", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "geraufet?", "tokens": ["ger\u00b7au\u00b7fet", "?"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "mord, raub! raub, mord! o gefahr", "tokens": ["mord", ",", "raub", "!", "raub", ",", "mord", "!", "o", "ge\u00b7fahr"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["NE", "$,", "VVFIN", "$.", "VVFIN", "$,", "NE", "$.", "FM", "NN"], "meter": "+-----+", "measure": "dactylic.init"}, "line.8": {"text": "alles rund umlaufet.", "tokens": ["al\u00b7les", "rund", "um\u00b7lau\u00b7fet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.72": {"line.1": {"text": "Ach wie kam ich in dises schif?", "tokens": ["Ach", "wie", "kam", "ich", "in", "di\u00b7ses", "schif", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "VVFIN", "PPER", "APPR", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "es grauset mir, ich kan nicht schwimmen.", "tokens": ["es", "grau\u00b7set", "mir", ",", "ich", "kan", "nicht", "schwim\u00b7men", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "hilf, hilf! ein seil, sto\u00df oder grif;", "tokens": ["hilf", ",", "hilf", "!", "ein", "seil", ",", "sto\u00df", "o\u00b7der", "grif", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "VVIMP", "$.", "ART", "NN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "ach weh! nu hab ich auch das grimmen.", "tokens": ["ach", "weh", "!", "nu", "hab", "ich", "auch", "das", "grim\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "$.", "ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "alles leider ist umsunst!", "tokens": ["al\u00b7les", "lei\u00b7der", "ist", "um\u00b7sunst", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "VAFIN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "wir sinken.", "tokens": ["wir", "sin\u00b7ken", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "was? ja wol in diser brunst", "tokens": ["was", "?", "ja", "wol", "in", "di\u00b7ser", "brunst"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "$.", "ADV", "ADV", "APPR", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "brennen wir und stinken.", "tokens": ["bren\u00b7nen", "wir", "und", "stin\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.73": {"line.1": {"text": "Ho! helfet! reichet das geschirr!", "tokens": ["Ho", "!", "hel\u00b7fet", "!", "rei\u00b7chet", "das", "ge\u00b7schirr", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "$.", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "es ist umsunst! es ist geschehen!", "tokens": ["es", "ist", "um\u00b7sunst", "!", "es", "ist", "ge\u00b7sche\u00b7hen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$.", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ich bin ganz na\u00df! ich bin ganz d\u00fcrr,", "tokens": ["ich", "bin", "ganz", "na\u00df", "!", "ich", "bin", "ganz", "d\u00fcrr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$.", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "stum, lahm, kan ich nichts h\u00f6ren, sehen.", "tokens": ["stum", ",", "lahm", ",", "kan", "ich", "nichts", "h\u00f6\u00b7ren", ",", "se\u00b7hen", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "PTKVZ", "$,", "VMFIN", "PPER", "PIS", "VVINF", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ach die hagelstein, blitz, strahl", "tokens": ["ach", "die", "ha\u00b7gel\u00b7stein", ",", "blitz", ",", "strahl"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "ART", "NN", "$,", "VVIMP", "$,", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "und dunder,", "tokens": ["und", "dun\u00b7der", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "ADJA", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "kommend auf mich auf einmal,", "tokens": ["kom\u00b7mend", "auf", "mich", "auf", "ein\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PRF", "APPR", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "schlagen mich hinunder.", "tokens": ["schla\u00b7gen", "mich", "hin\u00b7un\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.74": {"line.1": {"text": "Wa ist mein fu\u00df, wa meine stirn?", "tokens": ["Wa", "ist", "mein", "fu\u00df", ",", "wa", "mei\u00b7ne", "stirn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "PTKVZ", "$,", "XY", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "oh, mein kopf walzet auf der erden!", "tokens": ["oh", ",", "mein", "kopf", "wal\u00b7zet", "auf", "der", "er\u00b7den", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "halt! ich verlier sunst all mein hirn.", "tokens": ["halt", "!", "ich", "ver\u00b7lier", "sunst", "all", "mein", "hirn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PPER", "ADJD", "ADV", "PIAT", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "was wird doch endlich aus mir werden?", "tokens": ["was", "wird", "doch", "end\u00b7lich", "aus", "mir", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "APPR", "PPER", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ist keine hilf in dieser not", "tokens": ["ist", "kei\u00b7ne", "hilf", "in", "die\u00b7ser", "not"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "zu haben?", "tokens": ["zu", "ha\u00b7ben", "?"], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VAINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "got erbarm es! ich bin tod", "tokens": ["got", "er\u00b7barm", "es", "!", "ich", "bin", "tod"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "ADJD", "PPER", "$.", "PPER", "VAFIN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "und auch schon begraben.", "tokens": ["und", "auch", "schon", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.75": {"line.1": {"text": "Der volle narr, der w\u00fcste fratz", "tokens": ["Der", "vol\u00b7le", "narr", ",", "der", "w\u00fcs\u00b7te", "fratz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so voll besoffen als geschossen,", "tokens": ["so", "voll", "be\u00b7sof\u00b7fen", "als", "ge\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "KOKOM", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "hat als ein stinkend nasser ratz", "tokens": ["hat", "als", "ein", "stin\u00b7kend", "nas\u00b7ser", "ratz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "KOKOM", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sein abenteuer nu beschlossen.", "tokens": ["sein", "a\u00b7bent\u00b7eu\u00b7er", "nu", "be\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "und zu ged\u00e4chtnus seiner that", "tokens": ["und", "zu", "ge\u00b7d\u00e4cht\u00b7nus", "sei\u00b7ner", "that"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "soll er hie seine grabschrift sehen,", "tokens": ["soll", "er", "hie", "sei\u00b7ne", "grab\u00b7schrift", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.7": {"text": "wan von dem rausch der grob unflat", "tokens": ["wan", "von", "dem", "rausch", "der", "grob", "un\u00b7flat"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "soll wider wachend auferstehen:", "tokens": ["soll", "wi\u00b7der", "wa\u00b7chend", "auf\u00b7er\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.76": {"line.1": {"text": "Fratz liget under dieser bank,", "tokens": ["Fratz", "li\u00b7get", "un\u00b7der", "die\u00b7ser", "bank", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "PDS", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "an leib und seel sehr w\u00fcst besudelt,", "tokens": ["an", "leib", "und", "seel", "sehr", "w\u00fcst", "be\u00b7su\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der mancherlei gedrank, gestank", "tokens": ["der", "man\u00b7cher\u00b7lei", "ge\u00b7drank", ",", "ge\u00b7stank"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "PIAT", "NN", "$,", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und sprach vermischet und verhudelt.", "tokens": ["und", "sprach", "ver\u00b7mi\u00b7schet", "und", "ver\u00b7hu\u00b7delt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ach leser, w\u00fcnsch, da\u00df ihm, dir, mir", "tokens": ["Ach", "le\u00b7ser", ",", "w\u00fcnsch", ",", "da\u00df", "ihm", ",", "dir", ",", "mir"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["ITJ", "ITJ", "$,", "ADJD", "$,", "KOUS", "PPER", "$,", "PPER", "$,", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "got das gedeihen wolle geben,", "tokens": ["got", "das", "ge\u00b7dei\u00b7hen", "wol\u00b7le", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "VVINF", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "da\u00df unser jeder, nach geb\u00fchr,", "tokens": ["da\u00df", "un\u00b7ser", "je\u00b7der", ",", "nach", "ge\u00b7b\u00fchr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "PIS", "$,", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "m\u00f6g besser reden, drinken, leben.", "tokens": ["m\u00f6g", "bes\u00b7ser", "re\u00b7den", ",", "drin\u00b7ken", ",", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VMFIN", "ADJD", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}