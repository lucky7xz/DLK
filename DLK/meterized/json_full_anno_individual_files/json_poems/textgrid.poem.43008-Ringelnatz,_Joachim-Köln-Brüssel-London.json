{"textgrid.poem.43008": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "K\u00f6ln-Br\u00fcssel-London", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ach, mir war seltsam. Nach dem Start erwog ich,", "tokens": ["Ach", ",", "mir", "war", "selt\u00b7sam", ".", "Nach", "dem", "Start", "er\u00b7wog", "ich", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PPER", "VAFIN", "ADJD", "$.", "APPR", "ART", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ob's komisch sei, wenn man sentimental", "tokens": ["Ob's", "ko\u00b7misch", "sei", ",", "wenn", "man", "sen\u00b7ti\u00b7men\u00b7tal"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJD", "VAFIN", "$,", "KOUS", "PIS", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Denkt. Ach, zum ersten Male \u00fcberflog ich \u2013", "tokens": ["Denkt", ".", "Ach", ",", "zum", "ers\u00b7ten", "Ma\u00b7le", "\u00fc\u00b7berf\u00b7log", "ich", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ITJ", "$,", "APPRART", "ADJA", "NN", "VVFIN", "PPER", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ein ehemaliger Seemann \u2013 den Kanal.", "tokens": ["Ein", "e\u00b7hem\u00b7a\u00b7li\u00b7ger", "See\u00b7mann", "\u2013", "den", "Ka\u00b7nal", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Im Sonnenwetter, das wir anfangs hatten,", "tokens": ["Im", "Son\u00b7nen\u00b7wet\u00b7ter", ",", "das", "wir", "an\u00b7fangs", "hat\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PRELS", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sah ich zur Erde. Lautlos eilend schlich", "tokens": ["Sah", "ich", "zur", "Er\u00b7de", ".", "Laut\u00b7los", "ei\u00b7lend", "schlich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "$.", "NN", "ADJD", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Tief unter uns, doch mit uns, unser Schatten.", "tokens": ["Tief", "un\u00b7ter", "uns", ",", "doch", "mit", "uns", ",", "un\u00b7ser", "Schat\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "$,", "ADV", "APPR", "PPER", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und ich ward traurig, als er pl\u00f6tzlich wich.", "tokens": ["Und", "ich", "ward", "trau\u00b7rig", ",", "als", "er", "pl\u00f6tz\u00b7lich", "wich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADJD", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Und Br\u00fcssel dann. Ein kurzer Aufenthalt.", "tokens": ["Und", "Br\u00fcs\u00b7sel", "dann", ".", "Ein", "kur\u00b7zer", "Auf\u00b7ent\u00b7halt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "$.", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ich hab als Sieger dort einmal gelitten,", "tokens": ["Ich", "hab", "als", "Sie\u00b7ger", "dort", "ein\u00b7mal", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KOUS", "NN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Krieg. Ich habe dort nichts abzubitten.", "tokens": ["Im", "Krieg", ".", "Ich", "ha\u00b7be", "dort", "nichts", "ab\u00b7zu\u00b7bit\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "PPER", "VAFIN", "ADV", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und doch: Es \u00fcberlief mich kalt.", "tokens": ["Und", "doch", ":", "Es", "\u00fc\u00b7berl\u00b7ief", "mich", "kalt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und weiter ging's, durch wechselvolle H\u00f6hen,", "tokens": ["Und", "wei\u00b7ter", "ging's", ",", "durch", "wech\u00b7sel\u00b7vol\u00b7le", "H\u00f6\u00b7hen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nunmehr durch Grau und schwere Hagelb\u00f6en.", "tokens": ["Nun\u00b7mehr", "durch", "Grau", "und", "schwe\u00b7re", "Ha\u00b7gel\u00b7b\u00f6en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Doch mich betrank's. Wie lange war es her,", "tokens": ["Doch", "mich", "be\u00b7trank'", "s.", "Wie", "lan\u00b7ge", "war", "es", "her", ","], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "VVIMP", "PWAV", "ADV", "VAFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df ich zur See ging?! \u2013 Segelschiff und Meer! \u2013", "tokens": ["Da\u00df", "ich", "zur", "See", "ging", "?!", "\u2013", "Se\u00b7gel\u00b7schiff", "und", "Meer", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VVFIN", "$.", "$(", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Und als nun fern, dann n\u00e4her der Kanal", "tokens": ["Und", "als", "nun", "fern", ",", "dann", "n\u00e4\u00b7her", "der", "Ka\u00b7nal"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ADV", "ADJD", "$,", "ADV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Auftauchte, ich die K\u00fcste \u00fcberschwebte,", "tokens": ["Auf\u00b7tauch\u00b7te", ",", "ich", "die", "K\u00fcs\u00b7te", "\u00fc\u00b7bersc\u00b7hweb\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "War's, da\u00df ich nun zum zweiten erstenmal", "tokens": ["Wa\u00b7r's", ",", "da\u00df", "ich", "nun", "zum", "zwei\u00b7ten", "ers\u00b7ten\u00b7mal"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Stolz, ehrlich staunend Globetrot erlebte.", "tokens": ["Stolz", ",", "ehr\u00b7lich", "stau\u00b7nend", "Glo\u00b7be\u00b7trot", "er\u00b7leb\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Die Ufer unsres Kontinents entschwanden.", "tokens": ["Die", "U\u00b7fer", "uns\u00b7res", "Kon\u00b7ti\u00b7nents", "ent\u00b7schwan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zwei Dampfer sah ich, die mit ihren Wellen", "tokens": ["Zwei", "Damp\u00b7fer", "sah", "ich", ",", "die", "mit", "ih\u00b7ren", "Wel\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "PPER", "$,", "PRELS", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Scheinbar ganz still, wie starrgefroren standen.", "tokens": ["Schein\u00b7bar", "ganz", "still", ",", "wie", "starr\u00b7ge\u00b7fro\u00b7ren", "stan\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "ADJD", "$,", "PWAV", "ADJD", "VVFIN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Dann brach die Sonne durch und wies mit hellen,", "tokens": ["Dann", "brach", "die", "Son\u00b7ne", "durch", "und", "wies", "mit", "hel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "KON", "VVFIN", "APPR", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Vergn\u00fcgten Fingern auf das Inselland", "tokens": ["Ver\u00b7gn\u00fcg\u00b7ten", "Fin\u00b7gern", "auf", "das", "In\u00b7sel\u00b7land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und auf zwei Flieger. Diese zogen", "tokens": ["Und", "auf", "zwei", "Flie\u00b7ger", ".", "Die\u00b7se", "zo\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPR", "CARD", "NN", "$.", "PDS", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An uns vorbei, als wir den Kuchenrand", "tokens": ["An", "uns", "vor\u00b7bei", ",", "als", "wir", "den", "Ku\u00b7chen\u00b7rand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "PTKVZ", "$,", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Von Englands K\u00fcste \u00fcberflogen.", "tokens": ["Von", "En\u00b7glands", "K\u00fcs\u00b7te", "\u00fc\u00b7berf\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Land unter uns. Bis sich vom Flugplatz Croydon", "tokens": ["Land", "un\u00b7ter", "uns", ".", "Bis", "sich", "vom", "Flug\u00b7platz", "Croy\u00b7don"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PPER", "$.", "APPR", "PRF", "APPRART", "NN", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Blinkauf, blinkab ein Winkefeuer zeigte.", "tokens": ["Blin\u00b7kauf", ",", "blin\u00b7kab", "ein", "Win\u00b7ke\u00b7feu\u00b7er", "zeig\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "ART", "NN", "VVFIN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Als dann sich unser Kahn zur Landung neigte,", "tokens": ["Als", "dann", "sich", "un\u00b7ser", "Kahn", "zur", "Lan\u00b7dung", "neig\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PRF", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wie brannte ich auf lang entbehrte Freuden.", "tokens": ["Wie", "brann\u00b7te", "ich", "auf", "lang", "ent\u00b7behr\u00b7te", "Freu\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Ach, mir war seltsam. Nach dem Start erwog ich,", "tokens": ["Ach", ",", "mir", "war", "selt\u00b7sam", ".", "Nach", "dem", "Start", "er\u00b7wog", "ich", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PPER", "VAFIN", "ADJD", "$.", "APPR", "ART", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ob's komisch sei, wenn man sentimental", "tokens": ["Ob's", "ko\u00b7misch", "sei", ",", "wenn", "man", "sen\u00b7ti\u00b7men\u00b7tal"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJD", "VAFIN", "$,", "KOUS", "PIS", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Denkt. Ach, zum ersten Male \u00fcberflog ich \u2013", "tokens": ["Denkt", ".", "Ach", ",", "zum", "ers\u00b7ten", "Ma\u00b7le", "\u00fc\u00b7berf\u00b7log", "ich", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ITJ", "$,", "APPRART", "ADJA", "NN", "VVFIN", "PPER", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ein ehemaliger Seemann \u2013 den Kanal.", "tokens": ["Ein", "e\u00b7hem\u00b7a\u00b7li\u00b7ger", "See\u00b7mann", "\u2013", "den", "Ka\u00b7nal", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Im Sonnenwetter, das wir anfangs hatten,", "tokens": ["Im", "Son\u00b7nen\u00b7wet\u00b7ter", ",", "das", "wir", "an\u00b7fangs", "hat\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PRELS", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sah ich zur Erde. Lautlos eilend schlich", "tokens": ["Sah", "ich", "zur", "Er\u00b7de", ".", "Laut\u00b7los", "ei\u00b7lend", "schlich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "$.", "NN", "ADJD", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Tief unter uns, doch mit uns, unser Schatten.", "tokens": ["Tief", "un\u00b7ter", "uns", ",", "doch", "mit", "uns", ",", "un\u00b7ser", "Schat\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "$,", "ADV", "APPR", "PPER", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und ich ward traurig, als er pl\u00f6tzlich wich.", "tokens": ["Und", "ich", "ward", "trau\u00b7rig", ",", "als", "er", "pl\u00f6tz\u00b7lich", "wich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADJD", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Und Br\u00fcssel dann. Ein kurzer Aufenthalt.", "tokens": ["Und", "Br\u00fcs\u00b7sel", "dann", ".", "Ein", "kur\u00b7zer", "Auf\u00b7ent\u00b7halt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "$.", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ich hab als Sieger dort einmal gelitten,", "tokens": ["Ich", "hab", "als", "Sie\u00b7ger", "dort", "ein\u00b7mal", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KOUS", "NN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Krieg. Ich habe dort nichts abzubitten.", "tokens": ["Im", "Krieg", ".", "Ich", "ha\u00b7be", "dort", "nichts", "ab\u00b7zu\u00b7bit\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "PPER", "VAFIN", "ADV", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und doch: Es \u00fcberlief mich kalt.", "tokens": ["Und", "doch", ":", "Es", "\u00fc\u00b7berl\u00b7ief", "mich", "kalt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Und weiter ging's, durch wechselvolle H\u00f6hen,", "tokens": ["Und", "wei\u00b7ter", "ging's", ",", "durch", "wech\u00b7sel\u00b7vol\u00b7le", "H\u00f6\u00b7hen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nunmehr durch Grau und schwere Hagelb\u00f6en.", "tokens": ["Nun\u00b7mehr", "durch", "Grau", "und", "schwe\u00b7re", "Ha\u00b7gel\u00b7b\u00f6en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Doch mich betrank's. Wie lange war es her,", "tokens": ["Doch", "mich", "be\u00b7trank'", "s.", "Wie", "lan\u00b7ge", "war", "es", "her", ","], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "VVIMP", "PWAV", "ADV", "VAFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df ich zur See ging?! \u2013 Segelschiff und Meer! \u2013", "tokens": ["Da\u00df", "ich", "zur", "See", "ging", "?!", "\u2013", "Se\u00b7gel\u00b7schiff", "und", "Meer", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VVFIN", "$.", "$(", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Und als nun fern, dann n\u00e4her der Kanal", "tokens": ["Und", "als", "nun", "fern", ",", "dann", "n\u00e4\u00b7her", "der", "Ka\u00b7nal"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ADV", "ADJD", "$,", "ADV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Auftauchte, ich die K\u00fcste \u00fcberschwebte,", "tokens": ["Auf\u00b7tauch\u00b7te", ",", "ich", "die", "K\u00fcs\u00b7te", "\u00fc\u00b7bersc\u00b7hweb\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "War's, da\u00df ich nun zum zweiten erstenmal", "tokens": ["Wa\u00b7r's", ",", "da\u00df", "ich", "nun", "zum", "zwei\u00b7ten", "ers\u00b7ten\u00b7mal"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Stolz, ehrlich staunend Globetrot erlebte.", "tokens": ["Stolz", ",", "ehr\u00b7lich", "stau\u00b7nend", "Glo\u00b7be\u00b7trot", "er\u00b7leb\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Die Ufer unsres Kontinents entschwanden.", "tokens": ["Die", "U\u00b7fer", "uns\u00b7res", "Kon\u00b7ti\u00b7nents", "ent\u00b7schwan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zwei Dampfer sah ich, die mit ihren Wellen", "tokens": ["Zwei", "Damp\u00b7fer", "sah", "ich", ",", "die", "mit", "ih\u00b7ren", "Wel\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "PPER", "$,", "PRELS", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Scheinbar ganz still, wie starrgefroren standen.", "tokens": ["Schein\u00b7bar", "ganz", "still", ",", "wie", "starr\u00b7ge\u00b7fro\u00b7ren", "stan\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "ADJD", "$,", "PWAV", "ADJD", "VVFIN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Dann brach die Sonne durch und wies mit hellen,", "tokens": ["Dann", "brach", "die", "Son\u00b7ne", "durch", "und", "wies", "mit", "hel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "KON", "VVFIN", "APPR", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Vergn\u00fcgten Fingern auf das Inselland", "tokens": ["Ver\u00b7gn\u00fcg\u00b7ten", "Fin\u00b7gern", "auf", "das", "In\u00b7sel\u00b7land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und auf zwei Flieger. Diese zogen", "tokens": ["Und", "auf", "zwei", "Flie\u00b7ger", ".", "Die\u00b7se", "zo\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPR", "CARD", "NN", "$.", "PDS", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An uns vorbei, als wir den Kuchenrand", "tokens": ["An", "uns", "vor\u00b7bei", ",", "als", "wir", "den", "Ku\u00b7chen\u00b7rand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "PTKVZ", "$,", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Von Englands K\u00fcste \u00fcberflogen.", "tokens": ["Von", "En\u00b7glands", "K\u00fcs\u00b7te", "\u00fc\u00b7berf\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Land unter uns. Bis sich vom Flugplatz Croydon", "tokens": ["Land", "un\u00b7ter", "uns", ".", "Bis", "sich", "vom", "Flug\u00b7platz", "Croy\u00b7don"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PPER", "$.", "APPR", "PRF", "APPRART", "NN", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Blinkauf, blinkab ein Winkefeuer zeigte.", "tokens": ["Blin\u00b7kauf", ",", "blin\u00b7kab", "ein", "Win\u00b7ke\u00b7feu\u00b7er", "zeig\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "ART", "NN", "VVFIN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Als dann sich unser Kahn zur Landung neigte,", "tokens": ["Als", "dann", "sich", "un\u00b7ser", "Kahn", "zur", "Lan\u00b7dung", "neig\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PRF", "PPOSAT", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wie brannte ich auf lang entbehrte Freuden.", "tokens": ["Wie", "brann\u00b7te", "ich", "auf", "lang", "ent\u00b7behr\u00b7te", "Freu\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}