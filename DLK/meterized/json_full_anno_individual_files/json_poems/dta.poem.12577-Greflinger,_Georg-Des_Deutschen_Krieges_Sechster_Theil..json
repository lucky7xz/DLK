{"dta.poem.12577": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Sechster Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Sag an/ Geschicht-G\u00f6ttin/ die Rede zu erwei-\ntern/", "tokens": ["Sag", "an", "/", "Ge\u00b7schicht\u00b7G\u00f6t\u00b7tin", "/", "die", "Re\u00b7de", "zu", "er\u00b7wei", "tern", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$(", "NN", "$(", "ART", "NN", "APPR", "TRUNC", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gieng nun nicht alle Macht der Luthrischen zu", "tokens": ["Gieng", "nun", "nicht", "al\u00b7le", "Macht", "der", "Luth\u00b7ri\u00b7schen", "zu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PTKNEG", "PIAT", "NN", "ART", "NN", "PTKZU"], "meter": "+--+-+-+--+", "measure": "iambic.penta.invert"}, "line.3": {"text": "scheitern", "tokens": ["schei\u00b7tern"], "token_info": ["word"], "pos": ["VVINF"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Wie dieses H\u00e4upt erlag? War noch wol jemand da", "tokens": ["Wie", "die\u00b7ses", "H\u00e4upt", "er\u00b7lag", "?", "War", "noch", "wol", "je\u00b7mand", "da"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PDAT", "NN", "VVFIN", "$.", "VAFIN", "ADV", "ADV", "PIS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der seines Hertzens war/ und Rom bestritte? Ja/", "tokens": ["Der", "sei\u00b7nes", "Hert\u00b7zens", "war", "/", "und", "Rom", "be\u00b7strit\u00b7te", "?", "Ja", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VAFIN", "$(", "KON", "NE", "VVFIN", "$.", "PTKANT", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nicht einer/ sondern mehr. Das Blut aus jhm geflossen", "tokens": ["Nicht", "ei\u00b7ner", "/", "son\u00b7dern", "mehr", ".", "Das", "Blut", "aus", "jhm", "ge\u00b7flos\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ART", "$(", "KON", "ADV", "$.", "ART", "NN", "APPR", "PPER", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "War Pelikanen-Blut/ aus welchem viel entsprossen/", "tokens": ["War", "Pe\u00b7li\u00b7ka\u00b7nen\u00b7Blut", "/", "aus", "wel\u00b7chem", "viel", "ent\u00b7spros\u00b7sen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$(", "APPR", "PRELS", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die seines Hertzens sind. So bald di\u00df H\u00e4upt erlag/", "tokens": ["Die", "sei\u00b7nes", "Hert\u00b7zens", "sind", ".", "So", "bald", "di\u00df", "H\u00e4upt", "er\u00b7lag", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VAFIN", "$.", "ADV", "ADV", "PDS", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Hielt Axel Oxenstern/ sein Kantzler/ einen Tag/", "tokens": ["Hielt", "A\u00b7xel", "O\u00b7xens\u00b7tern", "/", "sein", "Kantz\u00b7ler", "/", "ei\u00b7nen", "Tag", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "$(", "PPOSAT", "NN", "$(", "ART", "NN", "$("], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "Zu rathen/ auf was Art den Gliedern bey zu stehen/", "tokens": ["Zu", "ra\u00b7then", "/", "auf", "was", "Art", "den", "Glie\u00b7dern", "bey", "zu", "ste\u00b7hen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "APPR", "PRELS", "NN", "ART", "NN", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "und dieser Beystand ist hernach also geschehen:", "tokens": ["und", "die\u00b7ser", "Beys\u00b7tand", "ist", "her\u00b7nach", "al\u00b7so", "ge\u00b7sche\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "NN", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Herr Axel Oxenstirn war derer Waffen Hand/", "tokens": ["Herr", "A\u00b7xel", "O\u00b7xens\u00b7tirn", "war", "de\u00b7rer", "Waf\u00b7fen", "Hand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "NE", "VAFIN", "PDS", "NN", "NN", "$("], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "und theilte die Armee halb an den Weserstrand/", "tokens": ["und", "theil\u00b7te", "die", "Ar\u00b7mee", "halb", "an", "den", "We\u00b7ser\u00b7strand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Die der begl\u00fcckte F\u00fcrst von L\u00fcneburg regierte/", "tokens": ["Die", "der", "be\u00b7gl\u00fcck\u00b7te", "F\u00fcrst", "von", "L\u00fc\u00b7ne\u00b7burg", "re\u00b7gier\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Halb in das Oberland/ die Hertzog Bernhard f\u00fchrte.", "tokens": ["Halb", "in", "das", "O\u00b7ber\u00b7land", "/", "die", "Hert\u00b7zog", "Bern\u00b7hard", "f\u00fchr\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$(", "ART", "NE", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Wie hat Georg/ der F\u00fcrst von L\u00fcneburg/ gekriegt?", "tokens": ["Wie", "hat", "Ge\u00b7org", "/", "der", "F\u00fcrst", "von", "L\u00fc\u00b7ne\u00b7burg", "/", "ge\u00b7kriegt", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "VAFIN", "NE", "$(", "ART", "NN", "APPR", "NE", "$(", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Er hatte sich kaum recht zur Weser hingef\u00fcgt/", "tokens": ["Er", "hat\u00b7te", "sich", "kaum", "recht", "zur", "We\u00b7ser", "hin\u00b7ge\u00b7f\u00fcgt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "ADJD", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Bekam er seinen Feind/ den Gron\u00dffeld/ zu Gesichte/", "tokens": ["Be\u00b7kam", "er", "sei\u00b7nen", "Feind", "/", "den", "Gron\u00df\u00b7feld", "/", "zu", "Ge\u00b7sich\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$(", "ART", "NN", "$(", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "und schlug bey Rinteln jhm zw\u00f6lff hunder Mann zu nichte/", "tokens": ["und", "schlug", "bey", "Rin\u00b7teln", "jhm", "zw\u00f6lff", "hun\u00b7der", "Mann", "zu", "nich\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "PPER", "CARD", "ADJA", "NN", "APPR", "PIS", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Belagerte darauf das Hameln/ welchen Ort", "tokens": ["Be\u00b7la\u00b7ger\u00b7te", "da\u00b7rauf", "das", "Ha\u00b7meln", "/", "wel\u00b7chen", "Ort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PAV", "ART", "NN", "$(", "PWAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Merode retten wolt\u2019/ er schlug auch diesen fort.", "tokens": ["Me\u00b7ro\u00b7de", "ret\u00b7ten", "wolt'", "/", "er", "schlug", "auch", "die\u00b7sen", "fort", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVINF", "VMFIN", "$(", "PPER", "VVFIN", "ADV", "PDAT", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Merode kam noch eins mit einem gro\u00dfen Hauffen/", "tokens": ["Me\u00b7ro\u00b7de", "kam", "noch", "eins", "mit", "ei\u00b7nem", "gro\u00b7\u00dfen", "Hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "PIS", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Bey zw\u00f6lffmal tausend Mann sich um den Ort zu rauffen/", "tokens": ["Bey", "zw\u00f6lff\u00b7mal", "tau\u00b7send", "Mann", "sich", "um", "den", "Ort", "zu", "rauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "PRF", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Der Hertzog war bereit/ und nahm das Hessen-Heer", "tokens": ["Der", "Hert\u00b7zog", "war", "be\u00b7reit", "/", "und", "nahm", "das", "Hes\u00b7sen\u00b7Heer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NE", "VAFIN", "ADJD", "$(", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Zu H\u00fclffe/ schlug darauf den halben Theil und mehr", "tokens": ["Zu", "H\u00fclf\u00b7fe", "/", "schlug", "da\u00b7rauf", "den", "hal\u00b7ben", "Theil", "und", "mehr"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$(", "VVFIN", "PAV", "ART", "ADJA", "NN", "KON", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Bey sieben tausend Mann/ mit sampt dem H\u00e4upt Merode/", "tokens": ["Bey", "sie\u00b7ben", "tau\u00b7send", "Mann", "/", "mit", "sampt", "dem", "H\u00e4upt", "Me\u00b7ro\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "$(", "APPR", "APPR", "ART", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Nicht weit von Hameln ab ", "tokens": ["Nicht", "weit", "von", "Ha\u00b7meln", "ab"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADJD", "APPR", "NN", "PTKVZ"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.28": {"text": "Di\u00df war das dritte Jahr/ das Rom zur Trauer trieb/", "tokens": ["Di\u00df", "war", "das", "drit\u00b7te", "Jahr", "/", "das", "Rom", "zur", "Trau\u00b7er", "trieb", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$(", "ART", "NE", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "und war die dritte Schlacht/ in der sein Hoffen blieb.", "tokens": ["und", "war", "die", "drit\u00b7te", "Schlacht", "/", "in", "der", "sein", "Hof\u00b7fen", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJA", "NN", "$(", "APPR", "ART", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Wie tapfer sich der F\u00fcrst hierinnen hab\u2019 erwiesen/", "tokens": ["Wie", "tap\u00b7fer", "sich", "der", "F\u00fcrst", "hie\u00b7rin\u00b7nen", "hab'", "er\u00b7wie\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PRF", "ART", "NN", "PAV", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Wird jhm in seiner Grufft sehr r\u00fchmlich nach gepriesen/", "tokens": ["Wird", "jhm", "in", "sei\u00b7ner", "Grufft", "sehr", "r\u00fchm\u00b7lich", "nach", "ge\u00b7prie\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPOSAT", "NN", "ADV", "ADJD", "APPR", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Also that auch der Held Kniphausen/ der der Schar/", "tokens": ["Al\u00b7so", "that", "auch", "der", "Held", "Kni\u00b7phau\u00b7sen", "/", "der", "der", "Schar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "NN", "$(", "ART", "ART", "NN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.33": {"text": "Die dieser tapfre F\u00fcrst bekam/ Feld Marschall war.", "tokens": ["Die", "die\u00b7ser", "tapf\u00b7re", "F\u00fcrst", "be\u00b7kam", "/", "Feld", "Mar\u00b7schall", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "ADJA", "NN", "VVFIN", "$(", "NN", "NE", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Der He\u00df hatt\u2019 auch sein Lob/ der bald nach diesem schlagen", "tokens": ["Der", "He\u00df", "hatt'", "auch", "sein", "Lob", "/", "der", "bald", "nach", "die\u00b7sem", "schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADV", "PPOSAT", "NN", "$(", "ART", "ADV", "APPR", "PDAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Sich wieder r\u00fcckwerts zog/ worauf nach wenig Tagen", "tokens": ["Sich", "wie\u00b7der", "r\u00fcck\u00b7werts", "zog", "/", "wo\u00b7rauf", "nach", "we\u00b7nig", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "ADV", "ADV", "VVFIN", "$(", "PWAV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Das Hamelu \u00fcbergieng/ de\u00dfgleichen O\u00dfnabr\u00fcck/", "tokens": ["Das", "Ha\u00b7me\u00b7lu", "\u00fc\u00b7berg\u00b7ieng", "/", "de\u00df\u00b7glei\u00b7chen", "O\u00df\u00b7na\u00b7br\u00fcck", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "$(", "ADJA", "NN", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.37": {"text": "und dessen gantzes Stifft. Wie leichtlich bringt das Gl\u00fcck", "tokens": ["und", "des\u00b7sen", "gant\u00b7zes", "Stifft", ".", "Wie", "leicht\u00b7lich", "bringt", "das", "Gl\u00fcck"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PRELAT", "ADJA", "NN", "$.", "PWAV", "ADJD", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "Den hoch/ den andern tieff/ wie einen leichten Pallen.", "tokens": ["Den", "hoch", "/", "den", "an\u00b7dern", "tieff", "/", "wie", "ei\u00b7nen", "leich\u00b7ten", "Pal\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "$(", "ART", "ADJA", "NN", "$(", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Der Bischoff dieses Stiffts wolt\u2019 auf die Schweden fallen/", "tokens": ["Der", "Bi\u00b7schoff", "die\u00b7ses", "Stiffts", "wolt'", "auf", "die", "Schwe\u00b7den", "fal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PDAT", "NN", "VMFIN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Merode hatt\u2019 jhn mit/ das Blat schlug aber um/", "tokens": ["Me\u00b7ro\u00b7de", "hatt'", "jhn", "mit", "/", "das", "Blat", "schlug", "a\u00b7ber", "um", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "APPR", "$(", "ART", "NN", "VVFIN", "ADV", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Da\u00df er befallen wurd\u2019 und all sein Bischoffthum.", "tokens": ["Da\u00df", "er", "be\u00b7fal\u00b7len", "wurd'", "und", "all", "sein", "Bi\u00b7schofft\u00b7hum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAFIN", "KON", "PIAT", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "Hierzwischen gieng der Held von Weymar auf die Fran-", "tokens": ["Hier\u00b7zwi\u00b7schen", "gieng", "der", "Held", "von", "Wey\u00b7mar", "auf", "die", "Fran"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ART", "NN", "APPR", "NE", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "und schlosse Cronach/ Stadt und Schlo\u00df in enge Schran-", "tokens": ["und", "schlos\u00b7se", "Cro\u00b7nach", "/", "Stadt", "und", "Schlo\u00df", "in", "en\u00b7ge", "Schran"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "$(", "NN", "KON", "NN", "APPR", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Eroberte die Stadt/ die in die Flammen fiel/", "tokens": ["Er\u00b7o\u00b7ber\u00b7te", "die", "Stadt", "/", "die", "in", "die", "Flam\u00b7men", "fiel", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$(", "ART", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Das Schlo\u00df erwehrte sich/ worauf des F\u00fcrsten Ziel", "tokens": ["Das", "Schlo\u00df", "er\u00b7wehr\u00b7te", "sich", "/", "wo\u00b7rauf", "des", "F\u00fcrs\u00b7ten", "Ziel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "$(", "PWAV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Zum gro\u00dfen Bamberg war/ ein Platz voll M\u00f6nch und", "tokens": ["Zum", "gro\u00b7\u00dfen", "Bam\u00b7berg", "war", "/", "ein", "Platz", "voll", "M\u00f6nch", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VAFIN", "$(", "ART", "NN", "ADJD", "NN", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.47": {"text": "Pfaffen/", "tokens": ["Pfaf\u00b7fen", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.48": {"text": "Es wurde bald besiegt/ und alles was in Waffen", "tokens": ["Es", "wur\u00b7de", "bald", "be\u00b7siegt", "/", "und", "al\u00b7les", "was", "in", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$(", "KON", "PIS", "PWS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Gefunden/ todt gemacht/ di\u00df stundt\u2019 anch H\u00f6chstadt au\u00df/", "tokens": ["Ge\u00b7fun\u00b7den", "/", "todt", "ge\u00b7macht", "/", "di\u00df", "stundt'", "anch", "H\u00f6chs\u00b7tadt", "au\u00df", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADJD", "VVPP", "$(", "PDS", "VVFIN", "PIAT", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "und kam in solchen Brand/ da\u00df nicht ein einig Hau\u00df", "tokens": ["und", "kam", "in", "sol\u00b7chen", "Brand", "/", "da\u00df", "nicht", "ein", "ei\u00b7nig", "Hau\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "$(", "KOUS", "PTKNEG", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "unengez\u00fcndet blieb/ doch ist das Schlo\u00df geblieben/", "tokens": ["un\u00b7en\u00b7ge\u00b7z\u00fcn\u00b7det", "blieb", "/", "doch", "ist", "das", "Schlo\u00df", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$(", "ADV", "VAFIN", "ART", "NN", "VVPP", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.52": {"text": "Worzu die Schwedischen auch alles nieder hieben/", "tokens": ["Wor\u00b7zu", "die", "Schwe\u00b7di\u00b7schen", "auch", "al\u00b7les", "nie\u00b7der", "hie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADV", "PIS", "PTKVZ", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.53": {"text": "Was in den Waffen war. Als dieses war gethan/", "tokens": ["Was", "in", "den", "Waf\u00b7fen", "war", ".", "Als", "die\u00b7ses", "war", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VAFIN", "$.", "KOUS", "PDS", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Da wolte man mit Ernst der Vestung Forchheim an.", "tokens": ["Da", "wol\u00b7te", "man", "mit", "Ernst", "der", "Ves\u00b7tung", "Forch\u00b7heim", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "APPR", "NE", "ART", "NN", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Es trug sich aber zu/ da\u00df von des Feindes Scharen/", "tokens": ["Es", "trug", "sich", "a\u00b7ber", "zu", "/", "da\u00df", "von", "des", "Fein\u00b7des", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "PTKZU", "$(", "KOUS", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Die in der Ober-Pfaltz zu der Besatzung waren/", "tokens": ["Die", "in", "der", "O\u00b7ber\u00b7Pfaltz", "zu", "der", "Be\u00b7sat\u00b7zung", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "APPR", "ART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Ein weiter Streiff geschah/ da\u00df mancher Schweden-Hauff", "tokens": ["Ein", "wei\u00b7ter", "Streiff", "ge\u00b7schah", "/", "da\u00df", "man\u00b7cher", "Schwe\u00b7den\u00b7Hauff"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$(", "KOUS", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Hiedurch zu scheitern gieng. Di\u00df trieb den F\u00fcrsten auff/", "tokens": ["Hie\u00b7durch", "zu", "schei\u00b7tern", "gieng", ".", "Di\u00df", "trieb", "den", "F\u00fcrs\u00b7ten", "auff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PTKZU", "VVINF", "VVFIN", "$.", "PDS", "VVFIN", "ART", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "Da\u00df er das Forchheim lie\u00df/ sein Heer zusammen brachte/", "tokens": ["Da\u00df", "er", "das", "Forch\u00b7heim", "lie\u00df", "/", "sein", "Heer", "zu\u00b7sam\u00b7men", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$(", "PPOSAT", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Zu Bamberg Mustrung hielt/ und sich darwider machte/", "tokens": ["Zu", "Bam\u00b7berg", "Must\u00b7rung", "hielt", "/", "und", "sich", "dar\u00b7wi\u00b7der", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VVFIN", "$(", "KON", "PRF", "PAV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "Wo sein Herr Bruder jhm/ F\u00fcrst Wilholm/ Beystand that.", "tokens": ["Wo", "sein", "Herr", "Bru\u00b7der", "jhm", "/", "F\u00fcrst", "Wil\u00b7holm", "/", "Beys\u00b7tand", "that", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "NN", "PPER", "$(", "NN", "NE", "$(", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.62": {"text": "Damit so gieng er lo\u00df/ und wurde der Croat", "tokens": ["Da\u00b7mit", "so", "gieng", "er", "lo\u00df", "/", "und", "wur\u00b7de", "der", "Croat"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$(", "KON", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.63": {"text": "um Amberg/ Eger/ Cam/ und wo die R\u00e4uber lagen/", "tokens": ["um", "Am\u00b7berg", "/", "E\u00b7ger", "/", "Cam", "/", "und", "wo", "die", "R\u00e4u\u00b7ber", "la\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "NE", "$(", "NE", "$(", "KON", "PWAV", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.64": {"text": "Durch dieses Schweden-Volck so sauber aufgeschlagen/", "tokens": ["Durch", "die\u00b7ses", "Schwe\u00b7den\u00b7Volck", "so", "sau\u00b7ber", "auf\u00b7ge\u00b7schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "ADV", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Da\u00df nichts im Felde blieb. Ein mehrers that man nicht.", "tokens": ["Da\u00df", "nichts", "im", "Fel\u00b7de", "blieb", ".", "Ein", "meh\u00b7rers", "that", "man", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVFIN", "$.", "ART", "PIS", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.66": {"text": "Und wurde nun der Zug nach Donawerth gericht/", "tokens": ["Und", "wur\u00b7de", "nun", "der", "Zug", "nach", "Do\u00b7na\u00b7werth", "ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ART", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.67": {"text": "Mit dem Gustavus Horn/ der die Bannirsche Scharen/", "tokens": ["Mit", "dem", "Gus\u00b7ta\u00b7vus", "Horn", "/", "der", "die", "Ban\u00b7nir\u00b7sche", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NE", "NE", "$(", "ART", "ART", "ADJA", "NN", "$("], "meter": "--+--+--+--+-", "measure": "anapaest.tetra.plus"}, "line.68": {"text": "Die durch viel Streit und Zug sehr abgemattet waren/", "tokens": ["Die", "durch", "viel", "Streit", "und", "Zug", "sehr", "ab\u00b7ge\u00b7mat\u00b7tet", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "KON", "NN", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "An seinen Hauffen zog/ daselbst gesamter Hand", "tokens": ["An", "sei\u00b7nen", "Hauf\u00b7fen", "zog", "/", "da\u00b7selbst", "ge\u00b7sam\u00b7ter", "Hand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "PAV", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Auf B\u00e4yern lo\u00df zu gehn/ dieweil durch dieses Land", "tokens": ["Auf", "B\u00e4\u00b7yern", "lo\u00df", "zu", "gehn", "/", "die\u00b7weil", "durch", "die\u00b7ses", "Land"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKVZ", "PTKZU", "VVINF", "$(", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "und dessen Nachbarschafft viel Volck wurd aufgetrieben/", "tokens": ["und", "des\u00b7sen", "Nach\u00b7bar\u00b7schafft", "viel", "Volck", "wurd", "auf\u00b7ge\u00b7trie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRELAT", "NN", "PIAT", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Auch aus Jtalien sehr viel darzu beschrieben/", "tokens": ["Auch", "aus", "Jta\u00b7li\u00b7en", "sehr", "viel", "dar\u00b7zu", "be\u00b7schrie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "ADV", "ADV", "PAV", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.73": {"text": "Das ", "tokens": ["Das"], "token_info": ["word"], "pos": ["PDS"], "meter": "-", "measure": "single.down"}, "line.74": {"text": "und war Graf Altring nun der Beyerischen Macht", "tokens": ["und", "war", "Graf", "A\u00b7ltring", "nun", "der", "Be\u00b7ye\u00b7ri\u00b7schen", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "NE", "NE", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.75": {"text": "Zum H\u00e4upte vorgesetzt/ wie Tylli war gewesen.", "tokens": ["Zum", "H\u00e4up\u00b7te", "vor\u00b7ge\u00b7setzt", "/", "wie", "Tyl\u00b7li", "war", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$(", "KOKOM", "NE", "VAFIN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.76": {"text": "Damit man dieses Volck nicht m\u00f6cht\u2019 in Hauffen lesen/", "tokens": ["Da\u00b7mit", "man", "die\u00b7ses", "Volck", "nicht", "m\u00f6cht'", "in", "Hauf\u00b7fen", "le\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PDAT", "NN", "PTKNEG", "VMFIN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Verh\u00e4ufft- und einten sich die Schweden allerseits", "tokens": ["Ver\u00b7h\u00e4ufft", "und", "ein\u00b7ten", "sich", "die", "Schwe\u00b7den", "al\u00b7ler\u00b7seits"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["TRUNC", "KON", "VVFIN", "PRF", "ART", "NE", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.78": {"text": "und hofften solches Volck vermittelst eines Streits", "tokens": ["und", "hoff\u00b7ten", "sol\u00b7ches", "Volck", "ver\u00b7mit\u00b7telst", "ei\u00b7nes", "Streits"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PIAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.79": {"text": "Zu trennen/ oder denn in B\u00e4yern einzuhalten/", "tokens": ["Zu", "tren\u00b7nen", "/", "o\u00b7der", "denn", "in", "B\u00e4\u00b7yern", "ein\u00b7zu\u00b7hal\u00b7ten", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "ADV", "APPR", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "We\u00dfwegen Jan de Werth/ der Amberg zu verwalten", "tokens": ["We\u00df\u00b7we\u00b7gen", "Jan", "de", "Werth", "/", "der", "Am\u00b7berg", "zu", "ver\u00b7wal\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NE", "NE", "NE", "$(", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "und gute V\u00f6lcker hatt\u2019/ in jhren R\u00fccken gieng/", "tokens": ["und", "gu\u00b7te", "V\u00f6l\u00b7cker", "hatt'", "/", "in", "jhren", "R\u00fc\u00b7cken", "gieng", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "$(", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.82": {"text": "Den aber Bernhards Volck zum \u00fcbelsten emfieng/", "tokens": ["Den", "a\u00b7ber", "Bern\u00b7hards", "Volck", "zum", "\u00fc\u00b7bels\u00b7ten", "em\u00b7fi\u00b7eng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "NE", "NN", "APPRART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.83": {"text": "und jhm vier hundert schlug/ jhn selber auch verwundte/", "tokens": ["und", "jhm", "vier", "hun\u00b7dert", "schlug", "/", "jhn", "sel\u00b7ber", "auch", "ver\u00b7wund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "CARD", "CARD", "VVFIN", "$(", "PPER", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "Da\u00df er es lange Zeit mit gro\u00dfem Weh empfundte.", "tokens": ["Da\u00df", "er", "es", "lan\u00b7ge", "Zeit", "mit", "gro\u00b7\u00dfem", "Weh", "emp\u00b7fund\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "In dem di\u00df hier verlieff gieng Hertzog Wilhelms Schar", "tokens": ["In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "gieng", "Hert\u00b7zog", "Wil\u00b7helms", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "PDS", "ADV", "VVFIN", "VVFIN", "NE", "NE", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "Auf W\u00fcrtzburg/ und gewann mit Macht was widrig war/", "tokens": ["Auf", "W\u00fcrtz\u00b7burg", "/", "und", "ge\u00b7wann", "mit", "Macht", "was", "wid\u00b7rig", "war", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KON", "VVFIN", "APPR", "NN", "PWS", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.87": {"text": "Bernhardus aber brach indessen tief in B\u00e4yern/", "tokens": ["Bern\u00b7har\u00b7dus", "a\u00b7ber", "brach", "in\u00b7des\u00b7sen", "tief", "in", "B\u00e4\u00b7yern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "ADV", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "und gab/ de\u00dfgleichen Horn/ daselbst nicht viel zu feyern.", "tokens": ["und", "gab", "/", "de\u00df\u00b7glei\u00b7chen", "Horn", "/", "da\u00b7selbst", "nicht", "viel", "zu", "fey\u00b7ern", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "ADV", "NE", "$(", "PAV", "PTKNEG", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Er machte fliehens gnug/ nahm Landsberg st\u00fcrmend ein/", "tokens": ["Er", "mach\u00b7te", "flie\u00b7hens", "gnug", "/", "nahm", "Lands\u00b7berg", "st\u00fcr\u00b7mend", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$(", "VVFIN", "NE", "ADJD", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Zwung M\u00fcnchen unter sich/ es wurd\u2019 auch Neuburg sein/", "tokens": ["Zwung", "M\u00fcn\u00b7chen", "un\u00b7ter", "sich", "/", "es", "wurd'", "auch", "Neu\u00b7burg", "sein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "APPR", "PRF", "$(", "PPER", "VAFIN", "ADV", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "Das Dachau/ Eich\u2019 und mehr. In diesem guten lauffen", "tokens": ["Das", "Dac\u00b7hau", "/", "Eich'", "und", "mehr", ".", "In", "die\u00b7sem", "gu\u00b7ten", "lauf\u00b7fen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "NN", "KON", "ADV", "$.", "APPR", "PDAT", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "Bracht Albrecht Wallenstein ein gro\u00dfes Volck zu hauffen/", "tokens": ["Bracht", "Al\u00b7brecht", "Wal\u00b7len\u00b7stein", "ein", "gro\u00b7\u00dfes", "Volck", "zu", "hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.93": {"text": "Den B\u00e4yern Hilff zu thun. Es war die Schuld und", "tokens": ["Den", "B\u00e4\u00b7yern", "Hilff", "zu", "thun", ".", "Es", "war", "die", "Schuld", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$.", "PPER", "VAFIN", "ART", "NN", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.94": {"text": "Pflicht.", "tokens": ["Pflicht", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+", "measure": "single.up"}, "line.95": {"text": "De\u00dfwegen s\u00e4umte sich der Hertzog Bernhard nicht/", "tokens": ["De\u00df\u00b7we\u00b7gen", "s\u00e4um\u00b7te", "sich", "der", "Hert\u00b7zog", "Bern\u00b7hard", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PRF", "ART", "NE", "NE", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "(dann die Gelegenheit wischt leichtlich aus den H\u00e4nden/", "tokens": ["(", "dann", "die", "Ge\u00b7le\u00b7gen\u00b7heit", "wischt", "leicht\u00b7lich", "aus", "den", "H\u00e4n\u00b7den", "/"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ART", "NN", "VVFIN", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "und pflegt dem tausenden nicht wieder um zu wenden.)", "tokens": ["und", "pflegt", "dem", "tau\u00b7sen\u00b7den", "nicht", "wie\u00b7der", "um", "zu", "wen\u00b7den", ".", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "PTKNEG", "ADV", "KOUI", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Er that so viel es kunt\u2019 in solcher Eile seyn/", "tokens": ["Er", "that", "so", "viel", "es", "kunt'", "in", "sol\u00b7cher", "Ei\u00b7le", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PPER", "VMFIN", "APPR", "PIAT", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.99": {"text": "Nahm Eichstadt/ Stifft und Stadt in seine Schatzung ein.", "tokens": ["Nahm", "Eich\u00b7stadt", "/", "Stifft", "und", "Stadt", "in", "sei\u00b7ne", "Schat\u00b7zung", "ein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$(", "NN", "KON", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Verf\u00fcgte sich daranf nach Donawerth zu r\u00fccke/", "tokens": ["Ver\u00b7f\u00fcg\u00b7te", "sich", "da\u00b7ranf", "nach", "Do\u00b7na\u00b7werth", "zu", "r\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PAV", "APPR", "NN", "PTKZU", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Mit solchem guten Zug und unverf\u00e4lschtem Gl\u00fccke/", "tokens": ["Mit", "sol\u00b7chem", "gu\u00b7ten", "Zug", "und", "un\u00b7ver\u00b7f\u00e4lschtem", "Gl\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.102": {"text": "Jm Hertzen wohl vergn\u00fcgt. ", "tokens": ["Jm", "Hert\u00b7zen", "wohl", "ver\u00b7gn\u00fcgt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.103": {"text": "Von Helden Franckfurt zu/ wo Axel Oxenstern", "tokens": ["Von", "Hel\u00b7den", "Fran\u00b7ck\u00b7furt", "zu", "/", "wo", "A\u00b7xel", "O\u00b7xens\u00b7tern"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "NE", "PTKZU", "$(", "PWAV", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Aus Schwedischer Gewalt jhn seiner M\u00fch ergetzte/", "tokens": ["Aus", "Schwe\u00b7di\u00b7scher", "Ge\u00b7walt", "jhn", "sei\u00b7ner", "M\u00fch", "er\u00b7getz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "und \u00fcbers Franckenland zu einem F\u00fcrsten setzte/", "tokens": ["und", "\u00fc\u00b7bers", "Fran\u00b7cken\u00b7land", "zu", "ei\u00b7nem", "F\u00fcrs\u00b7ten", "setz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Bey welches au\u00dfen-seyn Gustavus Horn dem Feind", "tokens": ["Bey", "wel\u00b7ches", "au\u00b7\u00dfen\u00b7seyn", "Gus\u00b7ta\u00b7vus", "Horn", "dem", "Feind"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "NE", "NE", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Ein wachend Auge gab/ und einmahl unvermeynt", "tokens": ["Ein", "wa\u00b7chend", "Au\u00b7ge", "gab", "/", "und", "ein\u00b7mahl", "un\u00b7ver\u00b7meynt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "VVFIN", "$(", "KON", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "Bey Neuburg/ welche Stadt vom Altring gantz umgeben/", "tokens": ["Bey", "Neu\u00b7burg", "/", "wel\u00b7che", "Stadt", "vom", "A\u00b7ltring", "gantz", "um\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "PWAT", "NN", "APPRART", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "und grob beschossen war/ dreyhundert von dem Leben", "tokens": ["und", "grob", "be\u00b7schos\u00b7sen", "war", "/", "drey\u00b7hun\u00b7dert", "von", "dem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVPP", "VAFIN", "$(", "CARD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "Zur k\u00fclen Erden hieb/ di\u00df rettete die Stadt/", "tokens": ["Zur", "k\u00fc\u00b7len", "Er\u00b7den", "hieb", "/", "di\u00df", "ret\u00b7te\u00b7te", "die", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$(", "PDS", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Worauf er auch Neumarckt gar bald in H\u00e4nden hatt\u2019.", "tokens": ["Wo\u00b7rauf", "er", "auch", "Neu\u00b7marckt", "gar", "bald", "in", "H\u00e4n\u00b7den", "hatt'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "VVFIN", "ADV", "ADV", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "In dem er di\u00df betrieb/ kam Hertzog Bernhard wieder/", "tokens": ["In", "dem", "er", "di\u00df", "be\u00b7trieb", "/", "kam", "Hert\u00b7zog", "Bern\u00b7hard", "wie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "PDS", "VVFIN", "$(", "VVFIN", "NE", "NE", "ADV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "und brachte gro\u00dfes Geld/ das unter seine Glieder", "tokens": ["und", "brach\u00b7te", "gro\u00b7\u00dfes", "Geld", "/", "das", "un\u00b7ter", "sei\u00b7ne", "Glie\u00b7der"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADJA", "NN", "$(", "ART", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Solt\u2019 au\u00dfgetheilet seyn. Geld ist des Kriegers Muth/", "tokens": ["Solt'", "au\u00df\u00b7ge\u00b7thei\u00b7let", "seyn", ".", "Geld", "ist", "des", "Krie\u00b7gers", "Muth", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVPP", "VAINF", "$.", "NN", "VAFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "Hierf\u00fcr verkaufft er auch sein Leben oder Blut.", "tokens": ["Hier\u00b7f\u00fcr", "ver\u00b7kaufft", "er", "auch", "sein", "Le\u00b7ben", "o\u00b7der", "Blut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Man zahlt und musterte das gantze Heer der Schweden.", "tokens": ["Man", "zahlt", "und", "mus\u00b7ter\u00b7te", "das", "gant\u00b7ze", "Heer", "der", "Schwe\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "ART", "ADJA", "NN", "ART", "NE", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.117": {"text": "Was nun geredet ist/ werd\u2019 ich nicht offte reden/", "tokens": ["Was", "nun", "ge\u00b7re\u00b7det", "ist", "/", "werd'", "ich", "nicht", "off\u00b7te", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVPP", "VAFIN", "$(", "VAFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "Denn es nicht offt geschah/ da\u00df man Bezahlung gab.", "tokens": ["Denn", "es", "nicht", "offt", "ge\u00b7schah", "/", "da\u00df", "man", "Be\u00b7zah\u00b7lung", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PTKNEG", "ADV", "VVFIN", "$(", "KOUS", "PIS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.119": {"text": "Nun sonderte sich Horn vom Hertzog Bernhard ab/", "tokens": ["Nun", "son\u00b7der\u00b7te", "sich", "Horn", "vom", "Hert\u00b7zog", "Bern\u00b7hard", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "NE", "APPRART", "NE", "NE", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "und zog nach Costnitz zu/ ein mehrers zu besiegen/", "tokens": ["und", "zog", "nach", "Cost\u00b7nitz", "zu", "/", "ein", "meh\u00b7rers", "zu", "be\u00b7sie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "PTKZU", "$(", "ART", "PIS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Der Hertzog aber blieb zu Donawerth beligen/", "tokens": ["Der", "Hert\u00b7zog", "a\u00b7ber", "blieb", "zu", "Do\u00b7na\u00b7werth", "be\u00b7li\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "VVFIN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Zu sehen/ was sein Feind gesinnet war. Es kam/", "tokens": ["Zu", "se\u00b7hen", "/", "was", "sein", "Feind", "ge\u00b7sin\u00b7net", "war", ".", "Es", "kam", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPOSAT", "NN", "VVPP", "VAFIN", "$.", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.123": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.124": {"text": "und ihm 12000. Mann ge\u00fcbter Krieger bachte/", "tokens": ["und", "ihm", "12000.", "Mann", "ge\u00b7\u00fcb\u00b7ter", "Krie\u00b7ger", "bach\u00b7te", "/"], "token_info": ["word", "word", "ordinal", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJA", "NN", "ADJA", "NN", "VVFIN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.125": {"text": "Worauf auch ", "tokens": ["Wo\u00b7rauf", "auch"], "token_info": ["word", "word"], "pos": ["PAV", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.126": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.127": {"text": "Auf drey\u00dfig tausend Mann gesch\u00e4tzet wurd und mehr.", "tokens": ["Auf", "drey\u00b7\u00dfig", "tau\u00b7send", "Mann", "ge\u00b7sch\u00e4t\u00b7zet", "wurd", "und", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "VVPP", "VAFIN", "KON", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Di\u00df trieb den Hertzog an das seine zu bewachen/", "tokens": ["Di\u00df", "trieb", "den", "Hert\u00b7zog", "an", "das", "sei\u00b7ne", "zu", "be\u00b7wa\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NE", "APPR", "ART", "PPOSAT", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "und sich mit Horns Armee und andern starck zu machen/", "tokens": ["und", "sich", "mit", "Horns", "Ar\u00b7mee", "und", "an\u00b7dern", "starck", "zu", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "NE", "NN", "KON", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Dann er war zimlich schwach/ die ", "tokens": ["Dann", "er", "war", "zim\u00b7lich", "schwach", "/", "die"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "PPER", "VAFIN", "ADV", "ADJD", "$(", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.131": {"text": "Er hatte kurtz zuvor nach Mei\u00dfen eine Schar", "tokens": ["Er", "hat\u00b7te", "kurtz", "zu\u00b7vor", "nach", "Mei\u00b7\u00dfen", "ei\u00b7ne", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "ADV", "APPR", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.132": {"text": "Von funftzig hundert Mann dem Bruder zugeschicket/", "tokens": ["Von", "funft\u00b7zig", "hun\u00b7dert", "Mann", "dem", "Bru\u00b7der", "zu\u00b7ge\u00b7schi\u00b7cket", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "Dem Holcki/ der das Land hatt\u2019 \u00fcberall bedr\u00fccket/", "tokens": ["Dem", "Hol\u00b7cki", "/", "der", "das", "Land", "hatt'", "\u00fc\u00b7be\u00b7rall", "be\u00b7dr\u00fc\u00b7cket", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "ART", "NN", "VAFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.134": {"text": "Den Widerstand zu thun. Dargegen zog jhm nu", "tokens": ["Den", "Wi\u00b7der\u00b7stand", "zu", "thun", ".", "Dar\u00b7ge\u00b7gen", "zog", "jhm", "nu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$.", "PAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Der Held Gustavus Horn bey Paling wieder zu/", "tokens": ["Der", "Held", "Gus\u00b7ta\u00b7vus", "Horn", "bey", "Pa\u00b7ling", "wie\u00b7der", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "NE", "APPR", "NE", "ADV", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.136": {"text": "Auch Pfaltzgraf Christian von Birckenfeld/ der immer", "tokens": ["Auch", "Pfaltz\u00b7graf", "Chris\u00b7ti\u00b7an", "von", "Bir\u00b7cken\u00b7feld", "/", "der", "im\u00b7mer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "NE", "NE", "APPR", "NE", "$(", "ART", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "Jm Elsas standhafft war/ und nimmermehr nicht schlimmer", "tokens": ["Jm", "El\u00b7sas", "stand\u00b7hafft", "war", "/", "und", "nim\u00b7mer\u00b7mehr", "nicht", "schlim\u00b7mer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NE", "VVFIN", "VAFIN", "$(", "KON", "ADV", "PTKNEG", "ADJD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.138": {"text": "Als gl\u00fccklich kriegete. Sie waren alle drey", "tokens": ["Als", "gl\u00fcck\u00b7lich", "krie\u00b7ge\u00b7te", ".", "Sie", "wa\u00b7ren", "al\u00b7le", "drey"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ADJD", "VVFIN", "$.", "PPER", "VAFIN", "PIAT", "CARD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.139": {"text": "Von 40000, Mann/ versucht/ behertzt dabey.", "tokens": ["Von", "40000", ",", "Mann", "/", "ver\u00b7sucht", "/", "be\u00b7hertzt", "da\u00b7bey", "."], "token_info": ["word", "number", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "CARD", "$,", "NN", "$(", "VVPP", "$(", "VVFIN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.140": {"text": "Hielt also dieses Schwerdt das ander in der Scheiden/", "tokens": ["Hielt", "al\u00b7so", "die\u00b7ses", "Schwerdt", "das", "an\u00b7der", "in", "der", "Schei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PDAT", "NN", "ART", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.141": {"text": "Da\u00df aber ", "tokens": ["Da\u00df", "a\u00b7ber"], "token_info": ["word", "word"], "pos": ["KOUS", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.142": {"text": "Als wich\u2019 er/ zog er sich nach Elsas/ Brysach da", "tokens": ["Als", "wich'", "er", "/", "zog", "er", "sich", "nach", "El\u00b7sas", "/", "Bry\u00b7sach", "da"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "VVFIN", "PPER", "$(", "VVFIN", "PPER", "PRF", "APPR", "NE", "$(", "NE", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Zu retten/ welches er in gro\u00dfen N\u00f6then sah/", "tokens": ["Zu", "ret\u00b7ten", "/", "wel\u00b7ches", "er", "in", "gro\u00b7\u00dfen", "N\u00f6\u00b7then", "sah", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPER", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "Die jhr der Rheingraf gab/ den man den Otto nennte.", "tokens": ["Die", "jhr", "der", "Rhein\u00b7graf", "gab", "/", "den", "man", "den", "Ot\u00b7to", "nenn\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NE", "VVFIN", "$(", "ART", "PIS", "ART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "So m\u00e4chtig ", "tokens": ["So", "m\u00e4ch\u00b7tig"], "token_info": ["word", "word"], "pos": ["ADV", "ADJD"], "meter": "-+-", "measure": "amphibrach.single"}, "line.146": {"text": "So eilends folgten jhm der Horn und Pfaltzgraf nach/", "tokens": ["So", "ei\u00b7lends", "folg\u00b7ten", "jhm", "der", "Horn", "und", "Pfaltz\u00b7graf", "nach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "ART", "NN", "KON", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "und hielten seine Macht zu Tag und Nacht sehr wach.", "tokens": ["und", "hiel\u00b7ten", "sei\u00b7ne", "Macht", "zu", "Tag", "und", "Nacht", "sehr", "wach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "In dem der ", "tokens": ["In", "dem", "der"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.149": {"text": "Gieng Bernhard wiederum zu rucke nach den Beyern/", "tokens": ["Gieng", "Bern\u00b7hard", "wie\u00b7de\u00b7rum", "zu", "ru\u00b7cke", "nach", "den", "Be\u00b7yern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "PTKZU", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.150": {"text": "und \u00fcberrumpelte das Kellheim/ welchen Ort", "tokens": ["und", "\u00fc\u00b7berr\u00b7um\u00b7pel\u00b7te", "das", "Kell\u00b7heim", "/", "wel\u00b7chen", "Ort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "$(", "PWAT", "NN"], "meter": "-+--+--+---+", "measure": "amphibrach.tri.plus"}, "line.151": {"text": "Er starck bemannen lie\u00df/ und dann in eyle fort", "tokens": ["Er", "starck", "be\u00b7man\u00b7nen", "lie\u00df", "/", "und", "dann", "in", "ey\u00b7le", "fort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADJD", "VVINF", "VVFIN", "$(", "KON", "ADV", "APPR", "NN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "Vor Regenspurg gerieth. Es wurd ", "tokens": ["Vor", "Re\u00b7gen\u00b7spurg", "ge\u00b7rieth", ".", "Es", "wurd"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "$.", "PPER", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.153": {"text": "Begehrt/ bedraut/ beringt/ beschossen und gewonnen.", "tokens": ["Be\u00b7gehrt", "/", "be\u00b7draut", "/", "be\u00b7ringt", "/", "be\u00b7schos\u00b7sen", "und", "ge\u00b7won\u00b7nen", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "ADJD", "$(", "VVFIN", "$(", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Gantz Wien erschrack hierob. Es war bi\u00dfher sein Damm/", "tokens": ["Gantz", "Wi\u00b7en", "er\u00b7schrack", "hier\u00b7ob", ".", "Es", "war", "bi\u00df\u00b7her", "sein", "Damm", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "ADV", "$.", "PPER", "VAFIN", "ADV", "PPOSAT", "NN", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.155": {"text": "Hier folgten Straubingen/ Burcklengenfeld und Camm/", "tokens": ["Hier", "folg\u00b7ten", "Strau\u00b7bin\u00b7gen", "/", "Burck\u00b7len\u00b7gen\u00b7feld", "und", "Camm", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.156": {"text": "Fiizhofen/ Deckendorf/ rc. Erschlug auch tausend Bauern/", "tokens": ["Fiiz\u00b7ho\u00b7fen", "/", "De\u00b7cken\u00b7dorf", "/", "rc", ".", "Er\u00b7schlug", "auch", "tau\u00b7send", "Bau\u00b7ern", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$.", "VVFIN", "ADV", "CARD", "NN", "$("], "meter": "---+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.157": {"text": "und tausend noch darzu die sich wie W\u00e4ll\u2019 und Mauern", "tokens": ["und", "tau\u00b7send", "noch", "dar\u00b7zu", "die", "sich", "wie", "W\u00e4ll'", "und", "Mau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "ADV", "PAV", "PRELS", "PRF", "KOKOM", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "Jhm widerlegeten. Nach diesem warff er Stauff", "tokens": ["Jhm", "wi\u00b7der\u00b7le\u00b7ge\u00b7ten", ".", "Nach", "die\u00b7sem", "warff", "er", "Stauff"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "APPR", "PDAT", "VVFIN", "PPER", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.159": {"text": "Ein Felsen-festes Schlo\u00df zur freyen Lufft hinauff.", "tokens": ["Ein", "Fel\u00b7sen\u00b7fes\u00b7tes", "Schlo\u00df", "zur", "frey\u00b7en", "Lufft", "hin\u00b7auff", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Durch des Salpeters Macht kan man die Felsen spr\u00e4ngen/", "tokens": ["Durch", "des", "Sal\u00b7pe\u00b7ters", "Macht", "kan", "man", "die", "Fel\u00b7sen", "spr\u00e4n\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VMFIN", "PIS", "ART", "NN", "VVFIN", "$("], "meter": "+-++-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.161": {"text": "Es wolte durchs Gesch\u00fctz die Donau gro\u00df bedr\u00e4ngen/", "tokens": ["Es", "wol\u00b7te", "durchs", "Ge\u00b7sch\u00fctz", "die", "Do\u00b7nau", "gro\u00df", "be\u00b7dr\u00e4n\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPRART", "NN", "ART", "NE", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Da kriegt\u2019 es solchen Lohn Di\u00df trieb den Beyer-Herrn", "tokens": ["Da", "kriegt'", "es", "sol\u00b7chen", "Lohn", "Di\u00df", "trieb", "den", "Beyer\u00b7Herrn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "PDS", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.163": {"text": "Den hohen Bergen zu/ weil weder Gl\u00fcck noch Stern", "tokens": ["Den", "ho\u00b7hen", "Ber\u00b7gen", "zu", "/", "weil", "we\u00b7der", "Gl\u00fcck", "noch", "Stern"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "$(", "KOUS", "KON", "NN", "ADV", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "F\u00fcr jhn in Beyern war/ dann Bernhard sich zur Jser", "tokens": ["F\u00fcr", "jhn", "in", "Be\u00b7yern", "war", "/", "dann", "Bern\u00b7hard", "sich", "zur", "Jser"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NN", "VAFIN", "$(", "ADV", "NE", "PRF", "APPRART", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.165": {"text": "und bi\u00df an Passau gab. Jmmittelst schickte dieser", "tokens": ["und", "bi\u00df", "an", "Pas\u00b7sau", "gab", ".", "Jm\u00b7mit\u00b7telst", "schick\u00b7te", "die\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "NN", "VVFIN", "$.", "ADV", "VVFIN", "PDAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "Der jhm bey L\u00fctzen wich/ der Albrecht Wallenstein/", "tokens": ["Der", "jhm", "bey", "L\u00fct\u00b7zen", "wich", "/", "der", "Al\u00b7brecht", "Wal\u00b7len\u00b7stein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NE", "VVFIN", "$(", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "Acht tausend Mann vor Camm. Sie kamen/ doch nicht", "tokens": ["Acht", "tau\u00b7send", "Mann", "vor", "Camm", ".", "Sie", "ka\u00b7men", "/", "doch", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["CARD", "CARD", "NN", "APPR", "NN", "$.", "PPER", "VVFIN", "$(", "ADV", "PTKNEG"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.168": {"text": "ein.", "tokens": ["ein", "."], "token_info": ["word", "punct"], "pos": ["PTKVZ", "$."], "meter": "-", "measure": "single.down"}, "line.169": {"text": "Tnpadel/ der es hielt/ der spielte starck dagegen/", "tokens": ["Tn\u00b7pa\u00b7del", "/", "der", "es", "hielt", "/", "der", "spiel\u00b7te", "starck", "da\u00b7ge\u00b7gen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "PRELS", "PPER", "VVFIN", "$(", "ART", "ADJA", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "und schlug zwey hundert Mann. Ein mehrers zu erlegen", "tokens": ["und", "schlug", "zwey", "hun\u00b7dert", "Mann", ".", "Ein", "meh\u00b7rers", "zu", "er\u00b7le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "CARD", "CARD", "NN", "$.", "ART", "PIS", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Kam Hertzog Bernhard selbst. Er kam so bald nicht an/", "tokens": ["Kam", "Hert\u00b7zog", "Bern\u00b7hard", "selbst", ".", "Er", "kam", "so", "bald", "nicht", "an", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "ADV", "$.", "PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Gieng Gallas wieder durch/ und nahm die nechste Bahn", "tokens": ["Gieng", "Gal\u00b7las", "wie\u00b7der", "durch", "/", "und", "nahm", "die", "nechs\u00b7te", "Bahn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "ADV", "APPR", "$(", "KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.173": {"text": "Nach seinem Pilsen zu/ woher er war gekommen.", "tokens": ["Nach", "sei\u00b7nem", "Pil\u00b7sen", "zu", "/", "wo\u00b7her", "er", "war", "ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "$(", "PWAV", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.174": {"text": "Was hat dann ", "tokens": ["Was", "hat", "dann"], "token_info": ["word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.175": {"text": "Es kamen beyde Theil in Beyern wieder an/", "tokens": ["Es", "ka\u00b7men", "bey\u00b7de", "Theil", "in", "Be\u00b7yern", "wie\u00b7der", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "APPR", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.176": {"text": "und wachte ", "tokens": ["und", "wach\u00b7te"], "token_info": ["word", "word"], "pos": ["KON", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.177": {"text": "Zum felsichten Tyrol nicht kriegte. Mitlerweile", "tokens": ["Zum", "fel\u00b7sich\u00b7ten", "Ty\u00b7rol", "nicht", "krieg\u00b7te", ".", "Mit\u00b7ler\u00b7wei\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPRART", "NN", "NE", "PTKNEG", "VVFIN", "$.", "NN"], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.178": {"text": "Wurd er und tausend mehr von jhm dem Tod zu theile/", "tokens": ["Wurd", "er", "und", "tau\u00b7send", "mehr", "von", "jhm", "dem", "Tod", "zu", "thei\u00b7le", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "KON", "CARD", "ADV", "APPR", "PPER", "ART", "NN", "PTKZU", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Durch Pestilentz und Schwerdt. Der Bauer schlug dar-", "tokens": ["Durch", "Pes\u00b7ti\u00b7lentz", "und", "Schwerdt", ".", "Der", "Bau\u00b7er", "schlug", "dar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "$.", "ART", "NN", "VVFIN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.180": {"text": "Die Kranckheit schonte nicht/ damit zerstob sein Hauff/", "tokens": ["Die", "Kran\u00b7ck\u00b7heit", "schon\u00b7te", "nicht", "/", "da\u00b7mit", "zer\u00b7stob", "sein", "Hauff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "$(", "PAV", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.181": {"text": "Jhn nahm die Kranckheit weg. Anitzo will ich reden", "tokens": ["Jhn", "nahm", "die", "Kran\u00b7ck\u00b7heit", "weg", ".", "A\u00b7nit\u00b7zo", "will", "ich", "re\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$.", "NE", "VMFIN", "PPER", "VVINF"], "meter": "+--+--+-+-+-+-", "measure": "elegiambus"}, "line.182": {"text": "Was sonsten anders mehr bey K\u00e4ysrischen und Schweden", "tokens": ["Was", "sons\u00b7ten", "an\u00b7ders", "mehr", "bey", "K\u00e4y\u00b7sri\u00b7schen", "und", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "ADV", "ADV", "APPR", "NN", "KON", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.183": {"text": "In andern L\u00e4ndern sey geschehn. Ott Ludewich/", "tokens": ["In", "an\u00b7dern", "L\u00e4n\u00b7dern", "sey", "ge\u00b7schehn", ".", "Ott", "Lu\u00b7de\u00b7wich", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "VVPP", "$.", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Der Rheingraf/ der sehr starck von Deutsch- und Schweden", "tokens": ["Der", "Rhein\u00b7graf", "/", "der", "sehr", "starck", "von", "Deut\u00b7sch", "und", "Schwe\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NE", "$(", "ART", "ADV", "ADJD", "APPR", "TRUNC", "KON", "NE"], "meter": "-+---+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.185": {"text": "sich", "tokens": ["sich"], "token_info": ["word"], "pos": ["PRF"], "meter": "-", "measure": "single.down"}, "line.186": {"text": "Jm edlen Elsas hielt/ hatt\u2019 auch nicht wenig Gl\u00fccke/", "tokens": ["Jm", "ed\u00b7len", "El\u00b7sas", "hielt", "/", "hatt'", "auch", "nicht", "we\u00b7nig", "Gl\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NE", "VVFIN", "$(", "VAFIN", "ADV", "PTKNEG", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Wie eins ein Bauerschwarm viel gro\u00dfe M\u00f6rderst\u00fccke", "tokens": ["Wie", "eins", "ein", "Bau\u00b7er\u00b7schwarm", "viel", "gro\u00b7\u00dfe", "M\u00f6r\u00b7der\u00b7st\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ART", "NN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Bey seinem Volcke that/ da drung er unter sie", "tokens": ["Bey", "sei\u00b7nem", "Vol\u00b7cke", "that", "/", "da", "drung", "er", "un\u00b7ter", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "ADV", "VVFIN", "PPER", "APPR", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "und schlug 4000, todt. Die armen Esel die/", "tokens": ["und", "schlug", "4000", ",", "todt", ".", "Die", "ar\u00b7men", "E\u00b7sel", "die", "/"], "token_info": ["word", "word", "number", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "CARD", "$,", "ADJD", "$.", "ART", "ADJA", "NN", "ART", "$("], "meter": "-++-+-+-+", "measure": "unknown.measure.penta"}, "line.190": {"text": "Sind immer in dem Wahn/ wann sie nur dick zu hauffen", "tokens": ["Sind", "im\u00b7mer", "in", "dem", "Wahn", "/", "wann", "sie", "nur", "dick", "zu", "hauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "ART", "NN", "$(", "PWAV", "PPER", "ADV", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.191": {"text": "und wolbeflegelt sind/ so m\u00fcsse Mars entlauffen.", "tokens": ["und", "wol\u00b7be\u00b7fle\u00b7gelt", "sind", "/", "so", "m\u00fcs\u00b7se", "Mars", "ent\u00b7lauf\u00b7fen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "$(", "ADV", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Es trifft gar selten zu. Das Schwerdt ist viel zu stoltz/", "tokens": ["Es", "trifft", "gar", "sel\u00b7ten", "zu", ".", "Das", "Schwerdt", "ist", "viel", "zu", "stoltz", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "PTKVZ", "$.", "ART", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "So d\u00fcnn und kurtz es scheint/ da\u00df es dem groben Holtz", "tokens": ["So", "d\u00fcnn", "und", "kurtz", "es", "scheint", "/", "da\u00df", "es", "dem", "gro\u00b7ben", "Holtz"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "KON", "ADJD", "PPER", "VVFIN", "$(", "KOUS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Jm freyen Felde weich\u2019. Er schlug auch andre Scharen/", "tokens": ["Jm", "frey\u00b7en", "Fel\u00b7de", "weich'", ".", "Er", "schlug", "auch", "and\u00b7re", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Die jhm von Brysach au\u00df in seinem Nacken waren.", "tokens": ["Die", "jhm", "von", "Bry\u00b7sach", "au\u00df", "in", "sei\u00b7nem", "Na\u00b7cken", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NE", "APPR", "APPR", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "Wobey des Orts Regent von ", "tokens": ["Wo\u00b7bey", "des", "Orts", "Re\u00b7gent", "von"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "NN", "APPR"], "meter": "-+-++--", "measure": "unknown.measure.tri"}, "line.197": {"text": "Sein Leben hat verlohrn/ zu fr\u00fch/ auch nicht zu fr\u00fch.", "tokens": ["Sein", "Le\u00b7ben", "hat", "ver\u00b7lohrn", "/", "zu", "fr\u00fch", "/", "auch", "nicht", "zu", "fr\u00fch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "VVPP", "$(", "PTKA", "ADJD", "$(", "ADV", "PTKNEG", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.198": {"text": "Er war geschickt genug/ doch unbegl\u00fcckt in Thaten/", "tokens": ["Er", "war", "ge\u00b7schickt", "ge\u00b7nug", "/", "doch", "un\u00b7be\u00b7gl\u00fcckt", "in", "Tha\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "ADV", "$(", "ADV", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Hergegen hatt\u2019 es hier dem Otto wolgerathen/", "tokens": ["Her\u00b7ge\u00b7gen", "hatt'", "es", "hier", "dem", "Ot\u00b7to", "wol\u00b7ge\u00b7ra\u00b7then", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "NE", "NE", "$("], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.200": {"text": "Der Reinfeld/ ", "tokens": ["Der", "Re\u00b7in\u00b7feld", "/"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.201": {"text": "und fort dem Lothringer bey zehen hundert Mannn", "tokens": ["und", "fort", "dem", "Loth\u00b7rin\u00b7ger", "bey", "ze\u00b7hen", "hun\u00b7dert", "Mannn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PTKVZ", "ART", "NN", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.202": {"text": "Bey Pfaffenhofen schlug/ auch gar in Lothring streiffte/", "tokens": ["Bey", "Pfaf\u00b7fen\u00b7ho\u00b7fen", "schlug", "/", "auch", "gar", "in", "Loth\u00b7ring", "streiff\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$(", "ADV", "ADV", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Wo zwischen Franckreich auch viel Volcks zusammen h\u00e4uff-", "tokens": ["Wo", "zwi\u00b7schen", "Fran\u00b7ck\u00b7reich", "auch", "viel", "Volcks", "zu\u00b7sam\u00b7men", "h\u00e4uff"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "NE", "ADV", "PIAT", "NN", "ADV", "TRUNC"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.204": {"text": "und Nanzy \u00fcberkam/ wodurch der Lothringer", "tokens": ["und", "Nan\u00b7zy", "\u00fc\u00b7ber\u00b7kam", "/", "wo\u00b7durch", "der", "Loth\u00b7rin\u00b7ger"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NE", "VVFIN", "$(", "PWAV", "ART", "NN"], "meter": "-+-+-+-+-++-", "measure": "unknown.measure.hexa"}, "line.205": {"text": "Des Landes ohnig wurd/ und ferner hin und her", "tokens": ["Des", "Lan\u00b7des", "oh\u00b7nig", "wurd", "/", "und", "fer\u00b7ner", "hin", "und", "her"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "VAFIN", "$(", "KON", "ADV", "PTKVZ", "KON", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Als ein Vertriebner zog. So gl\u00fccklich Otto siegte/", "tokens": ["Als", "ein", "Ver\u00b7trieb\u00b7ner", "zog", ".", "So", "gl\u00fcck\u00b7lich", "Ot\u00b7to", "sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$.", "ADV", "ADJD", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "So gl\u00fccklich Baudis auch im Stiffte C\u00f6llen kriegte/", "tokens": ["So", "gl\u00fcck\u00b7lich", "Bau\u00b7dis", "auch", "im", "Stiff\u00b7te", "C\u00f6l\u00b7len", "krieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NE", "ADV", "APPRART", "NN", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "Besiegte Hammerstein/ nahm D\u00fctsch in seine Hand/", "tokens": ["Be\u00b7sieg\u00b7te", "Ham\u00b7mer\u00b7stein", "/", "nahm", "D\u00fctsch", "in", "sei\u00b7ne", "Hand", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "$(", "VVFIN", "NE", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Gab Goldenau zum Raub/ Remagen in den Brand/", "tokens": ["Gab", "Gol\u00b7de\u00b7nau", "zum", "Raub", "/", "Re\u00b7ma\u00b7gen", "in", "den", "Brand", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "APPRART", "NN", "$(", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "und L\u00fcntz in gro\u00dfe Noth. Demselben abznwehren/", "tokens": ["und", "L\u00fcntz", "in", "gro\u00b7\u00dfe", "Noth", ".", "Dem\u00b7sel\u00b7ben", "abzn\u00b7weh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "ADJA", "NN", "$.", "NN", "VVIZU", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.211": {"text": "Sah man ein Spannisch Heer sich jhm entgegen kehren.", "tokens": ["Sah", "man", "ein", "Span\u00b7nisch", "Heer", "sich", "jhm", "ent\u00b7ge\u00b7gen", "keh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "NN", "PRF", "PPER", "PTKVZ", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "We\u00dfwegen sich der F\u00fcrst von Birckenfeld erhob/", "tokens": ["We\u00df\u00b7we\u00b7gen", "sich", "der", "F\u00fcrst", "von", "Bir\u00b7cken\u00b7feld", "er\u00b7hob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Dem Baudis Hilff zu thun/ hiemit lag Baudis ob/", "tokens": ["Dem", "Bau\u00b7dis", "Hilff", "zu", "thun", "/", "hie\u00b7mit", "lag", "Bau\u00b7dis", "ob", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "PAV", "VVFIN", "NE", "KOUS", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Nahm ferners Sayin ein und Nordburg an der Eyffel/", "tokens": ["Nahm", "fer\u00b7ners", "Sa\u00b7yin", "ein", "und", "Nord\u00b7burg", "an", "der", "Eyf\u00b7fel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "PTKVZ", "KON", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "De\u00dfgleichen Montabour. Er h\u00e4tte sonder Zweiffel", "tokens": ["De\u00df\u00b7glei\u00b7chen", "Mon\u00b7ta\u00b7bour", ".", "Er", "h\u00e4t\u00b7te", "son\u00b7der", "Zweif\u00b7fel"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "PPER", "VAFIN", "ADV", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Roch weiter fort ger\u00fcckt/ wann jhn Celada nicht/", "tokens": ["Roch", "wei\u00b7ter", "fort", "ge\u00b7r\u00fcckt", "/", "wann", "jhn", "Ce\u00b7la\u00b7da", "nicht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKVZ", "VVPP", "$(", "PWAV", "PPER", "NE", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Der mit 12000. Man jhm in das Angesicht", "tokens": ["Der", "mit", "12000.", "Man", "jhm", "in", "das", "An\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "ordinal", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "PIS", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.218": {"text": "Zu ziehen r\u00fcstig war/ h\u00e4tt\u2019 einen Wall gegeben.", "tokens": ["Zu", "zie\u00b7hen", "r\u00fcs\u00b7tig", "war", "/", "h\u00e4tt'", "ei\u00b7nen", "Wall", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "ADJD", "VAFIN", "$(", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.219": {"text": "Zu dem war keine Hilff aus Deutschland zu erheben/", "tokens": ["Zu", "dem", "war", "kei\u00b7ne", "Hilff", "aus", "Deutschland", "zu", "er\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VAFIN", "PIAT", "NN", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.220": {"text": "Weil man des Volckes da sehr hoch bed\u00fcrfftig war/", "tokens": ["Weil", "man", "des", "Vol\u00b7ckes", "da", "sehr", "hoch", "be\u00b7d\u00fcrff\u00b7tig", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "ADV", "ADV", "ADJD", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.221": {"text": "Dann Albrecht Wallenstein mit einer gro\u00dfen Schar", "tokens": ["Dann", "Al\u00b7brecht", "Wal\u00b7len\u00b7stein", "mit", "ei\u00b7ner", "gro\u00b7\u00dfen", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "NE", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Gantz neu zu Felde gieng. Desselben Macht zu d\u00e4mpffen/", "tokens": ["Gantz", "neu", "zu", "Fel\u00b7de", "gieng", ".", "Des\u00b7sel\u00b7ben", "Macht", "zu", "d\u00e4mpf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "VVFIN", "$.", "PDAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "und auch die Spanische zugleich recht zu bek\u00e4mpffen/", "tokens": ["und", "auch", "die", "Spa\u00b7ni\u00b7sche", "zu\u00b7gleich", "recht", "zu", "be\u00b7k\u00e4mpf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.224": {"text": "War nun kein m\u00f6glich thun. Derhalben sinnte man", "tokens": ["War", "nun", "kein", "m\u00f6g\u00b7lich", "thun", ".", "Der\u00b7hal\u00b7ben", "sinn\u00b7te", "man"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PIAT", "ADJD", "VVINF", "$.", "PDS", "VVFIN", "PIS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "Den fernern Streit mit C\u00f6lln dem Haus\u2019 ", "tokens": ["Den", "fer\u00b7nern", "Streit", "mit", "C\u00f6lln", "dem", "Haus'"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.226": {"text": "Und kam des Baudis Volck hierauf nach Nieder-Sach-\nsen/", "tokens": ["Und", "kam", "des", "Bau\u00b7dis", "Volck", "hier\u00b7auf", "nach", "Nie\u00b7der\u00b7Sach", "sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "PAV", "APPR", "TRUNC", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.227": {"text": "Woselbst der Held Bannier ein Heer im vollem wachsen", "tokens": ["Wo\u00b7selbst", "der", "Held", "Ban\u00b7nier", "ein", "Heer", "im", "vol\u00b7lem", "wach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "NE", "ART", "NN", "APPRART", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.228": {"text": "Auf frischen Beinen hatt. Er/ Baudis/ aber nahm", "tokens": ["Auf", "fri\u00b7schen", "Bei\u00b7nen", "hatt", ".", "Er", "/", "Bau\u00b7dis", "/", "a\u00b7ber", "nahm"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$.", "PPER", "$(", "NE", "$(", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "Den Abscheid/ bi\u00df der Fied aus Prag zum Kriege kam/", "tokens": ["Den", "Ab\u00b7scheid", "/", "bi\u00df", "der", "Fied", "aus", "Prag", "zum", "Krie\u00b7ge", "kam", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "APPR", "ART", "NN", "APPR", "NE", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.230": {"text": "Da fieng er wieder an Cur-Sachsen beyzustehen/", "tokens": ["Da", "fi\u00b7eng", "er", "wie\u00b7der", "an", "Cur\u00b7Sach\u00b7sen", "bey\u00b7zu\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "VVINF", "$("], "meter": "--+-+-+-+-+-+-", "measure": "anapaest.init"}, "line.231": {"text": "und auf das Schweden-Heer sehr feindlich lo\u00df zu gehen/", "tokens": ["und", "auf", "das", "Schwe\u00b7den\u00b7Heer", "sehr", "feind\u00b7lich", "lo\u00df", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADV", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "Wie endlich folgen soll. Was that Cur-Sachsen nu?", "tokens": ["Wie", "end\u00b7lich", "fol\u00b7gen", "soll", ".", "Was", "that", "Cur\u00b7Sach\u00b7sen", "nu", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVINF", "VMFIN", "$.", "PWS", "VVFIN", "NE", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Was that Cur-Brandenburg? Nun will ich diesen zu.", "tokens": ["Was", "that", "Cur\u00b7Bran\u00b7den\u00b7burg", "?", "Nun", "will", "ich", "die\u00b7sen", "zu", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NE", "$.", "ADV", "VMFIN", "PPER", "PDS", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "So bald die gro\u00dfe Schlacht bey L\u00fctzen war geschehen/", "tokens": ["So", "bald", "die", "gro\u00b7\u00dfe", "Schlacht", "bey", "L\u00fct\u00b7zen", "war", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "APPR", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Begunten beyde Theil in Schlesien zu gehen/", "tokens": ["Be\u00b7gun\u00b7ten", "bey\u00b7de", "Theil", "in", "Schle\u00b7si\u00b7en", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "Mit de\u00df von Thurn und dann des Tubadls Schweden-", "tokens": ["Mit", "de\u00df", "von", "Thurn", "und", "dann", "des", "Tu\u00b7badls", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NE", "KON", "ADV", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.237": {"text": "Sich st\u00e4rcker au\u00dfzuthun/ und wurden jhrer mehr", "tokens": ["Sich", "st\u00e4r\u00b7cker", "au\u00df\u00b7zu\u00b7thun", "/", "und", "wur\u00b7den", "jhrer", "mehr"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "VVINF", "$(", "KON", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.238": {"text": "Als zwantzig tausend Mann/ die alle streitbahr waren.", "tokens": ["Als", "zwant\u00b7zig", "tau\u00b7send", "Mann", "/", "die", "al\u00b7le", "streit\u00b7bahr", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "CARD", "NN", "$(", "ART", "PIAT", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "Weil Carel Hannibal von Dohna gro\u00dfe Scharen", "tokens": ["Weil", "Ca\u00b7rel", "Han\u00b7ni\u00b7bal", "von", "Doh\u00b7na", "gro\u00b7\u00dfe", "Scha\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "NE", "APPR", "NE", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "Von zehen tausend Mann aus Pohlen herwertz bracht\u2019/", "tokens": ["Von", "ze\u00b7hen", "tau\u00b7send", "Mann", "aus", "Poh\u00b7len", "her\u00b7wertz", "bracht'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "APPR", "NE", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Als nahmen diese vier denselben Zug in acht", "tokens": ["Als", "nah\u00b7men", "die\u00b7se", "vier", "den\u00b7sel\u00b7ben", "Zug", "in", "acht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "VVFIN", "PDAT", "CARD", "PDAT", "NN", "APPR", "CARD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "und hieben tausend Mann bey Grota zu der Erden/", "tokens": ["und", "hie\u00b7ben", "tau\u00b7send", "Mann", "bey", "Gro\u00b7ta", "zu", "der", "Er\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "CARD", "NN", "APPR", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Auch halb so viel bey Brieg derselben lo\u00df zu werden.", "tokens": ["Auch", "halb", "so", "viel", "bey", "Brieg", "der\u00b7sel\u00b7ben", "lo\u00df", "zu", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "ADV", "APPR", "NN", "PDAT", "ADJD", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.244": {"text": "Das gantze Heer zerstob. Hierauf so galt es Brieg/", "tokens": ["Das", "gant\u00b7ze", "Heer", "zer\u00b7stob", ".", "Hier\u00b7auf", "so", "galt", "es", "Brieg", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$.", "PAV", "ADV", "VVFIN", "PPER", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.245": {"text": "und folgends Franckenstein. Ein au\u00dfgelesner Krieg/", "tokens": ["und", "fol\u00b7gends", "Fran\u00b7cken\u00b7stein", ".", "Ein", "au\u00df\u00b7ge\u00b7les\u00b7ner", "Krieg", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$.", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.246": {"text": "Wo stets gesieget wird. Sie wurden beyd\u2019 er\u00f6bert/", "tokens": ["Wo", "stets", "ge\u00b7sie\u00b7get", "wird", ".", "Sie", "wur\u00b7den", "beyd'", "er\u00b7\u00f6\u00b7bert", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVPP", "VAFIN", "$.", "PPER", "VAFIN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "und wurde Franckensteins Besatzung so gest\u00f6bert/", "tokens": ["und", "wur\u00b7de", "Fran\u00b7cken\u00b7steins", "Be\u00b7sat\u00b7zung", "so", "ge\u00b7st\u00f6\u00b7bert", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "NE", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "Da\u00df nicht der dritt\u2019 entkam/ zwey hundert blieben todt/", "tokens": ["Da\u00df", "nicht", "der", "dritt'", "ent\u00b7kam", "/", "zwey", "hun\u00b7dert", "blie\u00b7ben", "todt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKNEG", "ART", "ADJA", "VVFIN", "$(", "CARD", "CARD", "VVFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Ein andre Feinds Parthey hatt\u2019 auch nicht wenig Noth/", "tokens": ["Ein", "and\u00b7re", "Feinds", "Par\u00b7they", "hatt'", "auch", "nicht", "we\u00b7nig", "Noth", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VAFIN", "ADV", "PTKNEG", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Die Hulderich/ der Printz aus Dennemarck/ bestritte/", "tokens": ["Die", "Hul\u00b7de\u00b7rich", "/", "der", "Printz", "aus", "Den\u00b7ne\u00b7marck", "/", "be\u00b7strit\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NE", "$(", "ART", "NN", "APPR", "NN", "$(", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.251": {"text": "Der aber bald hernach selbst auch den Tod erlitte:", "tokens": ["Der", "a\u00b7ber", "bald", "her\u00b7nach", "selbst", "auch", "den", "Tod", "er\u00b7lit\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADV", "ADV", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Man bat jhn auf ein Wort zur feindlichen Parthey/", "tokens": ["Man", "bat", "jhn", "auf", "ein", "Wort", "zur", "feind\u00b7li\u00b7chen", "Par\u00b7they", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "Er kam und scheute nichts/ war aller Sorgen frey/", "tokens": ["Er", "kam", "und", "scheu\u00b7te", "nichts", "/", "war", "al\u00b7ler", "Sor\u00b7gen", "frey", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "PIS", "$(", "VAFIN", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Weil man um diese Zeit von einem Frieden sagte.", "tokens": ["Weil", "man", "um", "die\u00b7se", "Zeit", "von", "ei\u00b7nem", "Frie\u00b7den", "sag\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PDAT", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.255": {"text": "So bald er aber sich zu r\u00fccke wieder wagte/", "tokens": ["So", "bald", "er", "a\u00b7ber", "sich", "zu", "r\u00fc\u00b7cke", "wie\u00b7der", "wag\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADV", "PRF", "PTKZU", "VVFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Bekam er einen Schu\u00df/ da\u00df er vom Pferde fiel", "tokens": ["Be\u00b7kam", "er", "ei\u00b7nen", "Schu\u00df", "/", "da\u00df", "er", "vom", "Pfer\u00b7de", "fiel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "$(", "KOUS", "PPER", "APPRART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "und seinen Geist verlohr. Der gerne zancken wil", "tokens": ["und", "sei\u00b7nen", "Geist", "ver\u00b7lohr", ".", "Der", "ger\u00b7ne", "zan\u00b7cken", "wil"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "ART", "ADV", "VVINF", "VMFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Mag ein Verr\u00e4ther-St\u00fcck aus diesem Morde machen/", "tokens": ["Mag", "ein", "Ver\u00b7r\u00e4ther\u00b7St\u00fcck", "aus", "die\u00b7sem", "Mor\u00b7de", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "APPR", "PDAT", "NN", "VVINF", "$("], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}, "line.259": {"text": "Ich aber la\u00df es seyn/ und handle meine Sachen.", "tokens": ["Ich", "a\u00b7ber", "la\u00df", "es", "seyn", "/", "und", "hand\u00b7le", "mei\u00b7ne", "Sa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVIMP", "PPER", "VAINF", "$(", "KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "Nur dieses sag ich noch: Ach es war schad um dich/", "tokens": ["Nur", "die\u00b7ses", "sag", "ich", "noch", ":", "Ach", "es", "war", "schad", "um", "dich", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VVFIN", "PPER", "ADV", "$.", "ITJ", "PPER", "VAFIN", "ADJD", "APPR", "PPER", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.261": {"text": "Du K\u00f6niglicher Printz/ du tapfrer Hulderich!", "tokens": ["Du", "K\u00f6\u00b7nig\u00b7li\u00b7cher", "Printz", "/", "du", "tapf\u00b7rer", "Hul\u00b7de\u00b7rich", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$(", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Nach dem die S\u00e4chsische so guten Fortgang kriegten/", "tokens": ["Nach", "dem", "die", "S\u00e4ch\u00b7si\u00b7sche", "so", "gu\u00b7ten", "Fort\u00b7gang", "krieg\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "ADV", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "und nechst den Schwedischen bald di\u00df/ bald das befiegten/", "tokens": ["und", "nechst", "den", "Schwe\u00b7di\u00b7schen", "bald", "di\u00df", "/", "bald", "das", "be\u00b7fieg\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PDS", "$(", "ADV", "ART", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Ermanneten sie sich in drey\u00dfig tausend Mann/", "tokens": ["Er\u00b7man\u00b7ne\u00b7ten", "sie", "sich", "in", "drey\u00b7\u00dfig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "und boten jhrem Feind ein offnes Treffen an.", "tokens": ["und", "bo\u00b7ten", "jhrem", "Feind", "ein", "off\u00b7nes", "Tref\u00b7fen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.266": {"text": "Er aber/ Wallenstein/ von 40000. Knechten", "tokens": ["Er", "a\u00b7ber", "/", "Wal\u00b7len\u00b7stein", "/", "von", "40000.", "Knech\u00b7ten"], "token_info": ["word", "word", "punct", "word", "punct", "word", "ordinal", "word"], "pos": ["PPER", "ADV", "$(", "NN", "$(", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.267": {"text": "und mehrern starck/ hielt ein und hatte zu dem Fechten", "tokens": ["und", "meh\u00b7rern", "starck", "/", "hielt", "ein", "und", "hat\u00b7te", "zu", "dem", "Fech\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "$(", "VVFIN", "PTKVZ", "KON", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Jm Felde wenig Lust/ weil jhn die L\u00fctzner Schlacht", "tokens": ["Jm", "Fel\u00b7de", "we\u00b7nig", "Lust", "/", "weil", "jhn", "die", "L\u00fctz\u00b7ner", "Schlacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "PIAT", "NN", "$(", "KOUS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.269": {"text": "und Hertzog Bernhards Art hatt\u2019 ernstlich scheu gemacht.", "tokens": ["und", "Hert\u00b7zog", "Bern\u00b7hards", "Art", "hatt'", "ernst\u00b7lich", "scheu", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "NN", "VAFIN", "ADJD", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Als man jhm Stahl entbot\u2019/ entbot\u2019 er g\u00f6ldnen Frieden/", "tokens": ["Als", "man", "jhm", "Stahl", "ent\u00b7bot'", "/", "ent\u00b7bot'", "er", "g\u00f6ld\u00b7nen", "Frie\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "NN", "VVFIN", "$(", "VVFIN", "PPER", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Lie\u00df aber mitlerzeit viel grimme Waffen schmieden/", "tokens": ["Lie\u00df", "a\u00b7ber", "mit\u00b7ler\u00b7zeit", "viel", "grim\u00b7me", "Waf\u00b7fen", "schmie\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PIAT", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "und unter Holckens Hand bey zehen tausend Mann", "tokens": ["und", "un\u00b7ter", "Hol\u00b7ckens", "Hand", "bey", "ze\u00b7hen", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "NN", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Gantz Mei\u00dfen \u00fcberziehn. Di\u00df trieb Cur Sachsen an/", "tokens": ["Gantz", "Mei\u00b7\u00dfen", "\u00fc\u00b7ber\u00b7ziehn", ".", "Di\u00df", "trieb", "Cur", "Sach\u00b7sen", "an", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVINF", "$.", "PDS", "VVFIN", "NE", "NE", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "Jhr Volck aus Schlesien in Mei\u00dfen einzubringen/", "tokens": ["Ihr", "Volck", "aus", "Schle\u00b7si\u00b7en", "in", "Mei\u00b7\u00dfen", "ein\u00b7zu\u00b7brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "NN", "APPR", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "Des Holckens seiner Macht den Vorthel abzuspringen.", "tokens": ["Des", "Hol\u00b7ckens", "sei\u00b7ner", "Macht", "den", "Vor\u00b7thel", "ab\u00b7zu\u00b7sprin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "Wie nun die Schwedischen allein/ fiel Wallenstein", "tokens": ["Wie", "nun", "die", "Schwe\u00b7di\u00b7schen", "al\u00b7lein", "/", "fiel", "Wal\u00b7len\u00b7stein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "ADV", "$(", "VVFIN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Denselben als ein B\u00e4hr und Fuchs bey Steinau ein/ ", "tokens": ["Den\u00b7sel\u00b7ben", "als", "ein", "B\u00e4hr", "und", "Fuchs", "bey", "Stei\u00b7nau", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "KOUS", "ART", "NN", "KON", "NE", "APPR", "NE", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.278": {"text": "und schlug 2000. Mann/ nahm beyde Generalen/", "tokens": ["und", "schlug", "2000", ".", "Mann", "/", "nahm", "bey\u00b7de", "Ge\u00b7ne\u00b7ra\u00b7len", "/"], "token_info": ["word", "word", "number", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "CARD", "$.", "NN", "$(", "VVFIN", "PIAT", "NN", "$("], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.279": {"text": "Den Thurn und Tubald weg. Di\u00df solte das bezahlen/", "tokens": ["Den", "Thurn", "und", "Tu\u00b7bald", "weg", ".", "Di\u00df", "sol\u00b7te", "das", "be\u00b7zah\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "PTKVZ", "$.", "PDS", "VMFIN", "PDS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "Was jhm bey L\u00fctzen wnrd\u2019/ und war der erste Sieg/", "tokens": ["Was", "jhm", "bey", "L\u00fct\u00b7zen", "wnrd'", "/", "und", "war", "der", "ers\u00b7te", "Sieg", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NN", "VAFIN", "$(", "KON", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Den dieser Wallenstein in allem Schweden-Krieg", "tokens": ["Den", "die\u00b7ser", "Wal\u00b7len\u00b7stein", "in", "al\u00b7lem", "Schwe\u00b7den\u00b7Krieg"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PDAT", "NN", "APPR", "PIS", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Erhalten hat/ der Mann/ vor dem gantz Deutschland zagte/", "tokens": ["Er\u00b7hal\u00b7ten", "hat", "/", "der", "Mann", "/", "vor", "dem", "gantz", "Deutschland", "zag\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "$(", "ART", "NN", "$(", "APPR", "ART", "ADV", "NE", "VVFIN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.283": {"text": "Wann er zu rasen kam/ und nach den Zeptern fragte.", "tokens": ["Wann", "er", "zu", "ra\u00b7sen", "kam", "/", "und", "nach", "den", "Zep\u00b7tern", "frag\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PTKZU", "VVINF", "VVFIN", "$(", "KON", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Hierauf so muste nun Graf Man\u00dffeld von jhm hin", "tokens": ["Hier\u00b7auf", "so", "mus\u00b7te", "nun", "Graf", "Man\u00df\u00b7feld", "von", "jhm", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VMFIN", "ADV", "NE", "NE", "APPR", "PPER", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "und mit zehn tausend Mann Cur-Brandenburg beziehn/", "tokens": ["und", "mit", "zehn", "tau\u00b7send", "Mann", "Cur\u00b7Bran\u00b7den\u00b7burg", "be\u00b7ziehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "CARD", "NN", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.286": {"text": "Wie Holck\u2019 im Mei\u00dfen that/ da\u00df also diesen Landen", "tokens": ["Wie", "Holck'", "im", "Mei\u00b7\u00dfen", "that", "/", "da\u00df", "al\u00b7so", "die\u00b7sen", "Lan\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NE", "APPRART", "NN", "VVFIN", "$(", "KOUS", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.287": {"text": "Viel ", "tokens": ["Viel"], "token_info": ["word"], "pos": ["ADV"], "meter": "-", "measure": "single.down"}, "line.288": {"text": "Entrei\u00dfet/ tobet er/ wie man es hier vernahm/", "tokens": ["Ent\u00b7rei\u00b7\u00dfet", "/", "to\u00b7bet", "er", "/", "wie", "man", "es", "hier", "ver\u00b7nahm", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "VVFIN", "PPER", "$(", "PWAV", "PIS", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.289": {"text": "Als der von Wallenstein bey Steinau Lufft bekahm.", "tokens": ["Als", "der", "von", "Wal\u00b7len\u00b7stein", "bey", "Stei\u00b7nau", "Lufft", "be\u00b7kahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "APPR", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Damit Cur-Brandenburg dem Feinde was gewachsen", "tokens": ["Da\u00b7mit", "Cur\u00b7Bran\u00b7den\u00b7burg", "dem", "Fein\u00b7de", "was", "ge\u00b7wach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "NE", "ART", "NN", "PWS", "VAPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Jm Felde m\u00f6chte seyn/ berieff es von Cur-Sachsen", "tokens": ["Jm", "Fel\u00b7de", "m\u00f6ch\u00b7te", "seyn", "/", "be\u00b7rieff", "es", "von", "Cur\u00b7Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VMFIN", "VAINF", "$(", "VVFIN", "PPER", "APPR", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Jhr Volck/ und warb darzu was zu bekommen war/", "tokens": ["Ihr", "Volck", "/", "und", "warb", "dar\u00b7zu", "was", "zu", "be\u00b7kom\u00b7men", "war", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "KON", "VVFIN", "PAV", "PIS", "PTKZU", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Bot auch viel Land-Volck auf. Derselben gro\u00dfe Schar", "tokens": ["Bot", "auch", "viel", "Lan\u00b7dVolck", "auf", ".", "Der\u00b7sel\u00b7ben", "gro\u00b7\u00dfe", "Schar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PIAT", "NN", "PTKVZ", "$.", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Zu mehren/ und den Feind in seinen Platz zu treiben/", "tokens": ["Zu", "meh\u00b7ren", "/", "und", "den", "Feind", "in", "sei\u00b7nen", "Platz", "zu", "trei\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Zog Bannier m\u00e4chtig auf/ der Held/ von dem man schrei-", "tokens": ["Zog", "Ban\u00b7nier", "m\u00e4ch\u00b7tig", "auf", "/", "der", "Held", "/", "von", "dem", "man", "schrei"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "NE", "ADJD", "PTKVZ", "$(", "ART", "NN", "$(", "APPR", "PRELS", "PIS", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "und gro\u00dfes sagen wird/ so lang als Deutschland steht/", "tokens": ["und", "gro\u00b7\u00dfes", "sa\u00b7gen", "wird", "/", "so", "lang", "als", "Deutschland", "steht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "VVINF", "VAFIN", "$(", "ADV", "ADJD", "KOKOM", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.297": {"text": "und seines K\u00f6nigs Lob aus unserm Munde geht.", "tokens": ["und", "sei\u00b7nes", "K\u00f6\u00b7nigs", "Lob", "aus", "un\u00b7serm", "Mun\u00b7de", "geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "In dem man r\u00fcstig war den Feinden zu begegnen/", "tokens": ["In", "dem", "man", "r\u00fcs\u00b7tig", "war", "den", "Fein\u00b7den", "zu", "be\u00b7geg\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIS", "ADJD", "VAFIN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.299": {"text": "und bat/ da\u00df GOtt der HEer die Waffen m\u00f6chte segnen/", "tokens": ["und", "bat", "/", "da\u00df", "Gott", "der", "Heer", "die", "Waf\u00b7fen", "m\u00f6ch\u00b7te", "seg\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "KOUS", "NN", "ART", "NN", "ART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Dann Er/ der HErr/ giebt Sieg/ kam eine Zeitung ein/", "tokens": ["Dann", "Er", "/", "der", "Herr", "/", "giebt", "Sieg", "/", "kam", "ei\u00b7ne", "Zei\u00b7tung", "ein", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$(", "ART", "NN", "$(", "VVFIN", "NN", "$(", "VVFIN", "ART", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Es h\u00e4tte Gordons Spie\u00df den gro\u00dfen Wallenstein", "tokens": ["Es", "h\u00e4t\u00b7te", "Gor\u00b7dons", "Spie\u00df", "den", "gro\u00b7\u00dfen", "Wal\u00b7len\u00b7stein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "NE", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.302": {"text": "In Eger ", "tokens": ["In", "E\u00b7ger"], "token_info": ["word", "word"], "pos": ["APPR", "NE"], "meter": "-+-", "measure": "amphibrach.single"}, "line.303": {"text": "und mehrern l\u00fcstern war. Di\u00df kriegte der zu Lohne/", "tokens": ["und", "meh\u00b7rern", "l\u00fcs\u00b7tern", "war", ".", "Di\u00df", "krieg\u00b7te", "der", "zu", "Loh\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VAFIN", "$.", "PDS", "VVFIN", "ART", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Der den Gesalbten schlug. Nicht b\u00e4sser hatt\u2019 es Holck/", "tokens": ["Der", "den", "Ge\u00b7salb\u00b7ten", "schlug", ".", "Nicht", "b\u00e4s\u00b7ser", "hatt'", "es", "Holck", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVFIN", "$.", "PTKNEG", "ADJD", "VAFIN", "PPER", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.305": {"text": "Den eine strenge Pest mit seinem halben Volck", "tokens": ["Den", "ei\u00b7ne", "stren\u00b7ge", "Pest", "mit", "sei\u00b7nem", "hal\u00b7ben", "Volck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "ADJA", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "In Mei\u00dfen nieder warff/ also/ da\u00df alles Mei\u00dfen", "tokens": ["In", "Mei\u00b7\u00dfen", "nie\u00b7der", "warff", "/", "al\u00b7so", "/", "da\u00df", "al\u00b7les", "Mei\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "PTKVZ", "VVFIN", "$(", "ADV", "$(", "KOUS", "PIS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Vom Feinde ledig wurd\u2019/ eh jemand jhn zu schmei\u00dfen", "tokens": ["Vom", "Fein\u00b7de", "le\u00b7dig", "wurd'", "/", "eh", "je\u00b7mand", "jhn", "zu", "schmei\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "ADJD", "VAFIN", "$(", "KOUS", "PIS", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Vor seine Stirne kam. In dem di\u00df hier verlieff/", "tokens": ["Vor", "sei\u00b7ne", "Stir\u00b7ne", "kam", ".", "In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "APPR", "ART", "PDS", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Auch Hertzog Bernhard weit um sich in B\u00e4yern griff/", "tokens": ["Auch", "Hert\u00b7zog", "Bern\u00b7hard", "weit", "um", "sich", "in", "B\u00e4\u00b7yern", "griff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NE", "ADJD", "APPR", "PRF", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.310": {"text": "Da\u00df sich das meiste Volck in B\u00f6h\u00e4im must erheben/", "tokens": ["Da\u00df", "sich", "das", "meis\u00b7te", "Volck", "in", "B\u00f6\u00b7h\u00e4im", "must", "er\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "ADJA", "NN", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.311": {"text": "Darwider Dienst zu thun/ das nach des Albrechts Leben", "tokens": ["Dar\u00b7wi\u00b7der", "Dienst", "zu", "thun", "/", "das", "nach", "des", "Al\u00b7brechts", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PAV", "NN", "PTKZU", "VVINF", "$(", "PDS", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.312": {"text": "Des K\u00e4ysers erster Sohn/ der Dritte Ferdinand/", "tokens": ["Des", "K\u00e4y\u00b7sers", "ers\u00b7ter", "Sohn", "/", "der", "Drit\u00b7te", "Fer\u00b7di\u00b7nand", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "In seinem Z\u00e4umen hatt\u2019/ ein Herr/ dem alles Land", "tokens": ["In", "sei\u00b7nem", "Z\u00e4u\u00b7men", "hatt'", "/", "ein", "Herr", "/", "dem", "al\u00b7les", "Land"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "$(", "ART", "NN", "$(", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.314": {"text": "Ob seiner G\u00fctigkeit soll zu Gehorsam stehen/", "tokens": ["Ob", "sei\u00b7ner", "G\u00fc\u00b7tig\u00b7keit", "soll", "zu", "Ge\u00b7hor\u00b7sam", "ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "VMFIN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "War Man\u00dffeld gantz allein und leicht zu \u00fcbergehen.", "tokens": ["War", "Man\u00df\u00b7feld", "gantz", "al\u00b7lein", "und", "leicht", "zu", "\u00fc\u00b7ber\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "ADV", "KON", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.316": {"text": "Man trieb jhn aus der Marck und nahm ihm wieder ab", "tokens": ["Man", "trieb", "jhn", "aus", "der", "Marck", "und", "nahm", "ihm", "wie\u00b7der", "ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "KON", "VVFIN", "PPER", "ADV", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.317": {"text": "Was er gewonnen hatt\u2019/ also/ da\u00df er den Stab", "tokens": ["Was", "er", "ge\u00b7won\u00b7nen", "hatt'", "/", "al\u00b7so", "/", "da\u00df", "er", "den", "Stab"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "VVPP", "VAFIN", "$(", "ADV", "$(", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.318": {"text": "Nach Schlesien und fort nach B\u00f6h\u00e4im muste setzen/", "tokens": ["Nach", "Schle\u00b7si\u00b7en", "und", "fort", "nach", "B\u00f6\u00b7h\u00e4im", "mus\u00b7te", "set\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "PTKVZ", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "Hierzwischen hatte sich der Arnheim mit dem G\u00f6tzen", "tokens": ["Hier\u00b7zwi\u00b7schen", "hat\u00b7te", "sich", "der", "Arn\u00b7heim", "mit", "dem", "G\u00f6t\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PRF", "ART", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.320": {"text": "Bey L\u00fcgnitz ", "tokens": ["Bey", "L\u00fcg\u00b7nitz"], "token_info": ["word", "word"], "pos": ["APPR", "NE"], "meter": "-+-", "measure": "amphibrach.single"}, "line.321": {"text": "Jm Felde ligen lie\u00df/ vom Leben abgethan.", "tokens": ["Jm", "Fel\u00b7de", "li\u00b7gen", "lie\u00df", "/", "vom", "Le\u00b7ben", "ab\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVINF", "VVFIN", "$(", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.322": {"text": "Worauf Cur-Sachsen sich mit Bannern einig machte/", "tokens": ["Wo\u00b7rauf", "Cur\u00b7Sach\u00b7sen", "sich", "mit", "Ban\u00b7nern", "ei\u00b7nig", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "NN", "PRF", "APPR", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.323": {"text": "und 40000. Mann vor Prag in B\u00f6h\u00e4im brachte/", "tokens": ["und", "40000.", "Mann", "vor", "Prag", "in", "B\u00f6\u00b7h\u00e4im", "brach\u00b7te", "/"], "token_info": ["word", "ordinal", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NE", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.324": {"text": "Demselben ob zu seyn/ man schlo\u00df/ bescho\u00df es auch/", "tokens": ["Dem\u00b7sel\u00b7ben", "ob", "zu", "seyn", "/", "man", "schlo\u00df", "/", "be\u00b7scho\u00df", "es", "auch", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "KOUS", "PTKZU", "VAINF", "$(", "PIS", "VVFIN", "$(", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.325": {"text": "und forderte den Feind nach Schwedischem Gebrauch", "tokens": ["und", "for\u00b7der\u00b7te", "den", "Feind", "nach", "Schwe\u00b7di\u00b7schem", "Ge\u00b7brauch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.326": {"text": "(ein Volck das gerne schl\u00e4gt) ins freye Feld zu schlagen.", "tokens": ["(", "ein", "Volck", "das", "ger\u00b7ne", "schl\u00e4gt", ")", "ins", "frey\u00b7e", "Feld", "zu", "schla\u00b7gen", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "ART", "ADV", "VVFIN", "$(", "APPRART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.327": {"text": "Weil aber niemand kam/ und es bey hei\u00dfen Tagen", "tokens": ["Weil", "a\u00b7ber", "nie\u00b7mand", "kam", "/", "und", "es", "bey", "hei\u00b7\u00dfen", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PIS", "VVFIN", "$(", "KON", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.328": {"text": "Nicht gut zu Felde war/ dann es an Lufft und Bach", "tokens": ["Nicht", "gut", "zu", "Fel\u00b7de", "war", "/", "dann", "es", "an", "Lufft", "und", "Bach"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADJD", "APPR", "NN", "VAFIN", "$(", "ADV", "PPER", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.329": {"text": "Vor Menschen und vor Vieh und andern mehr gebrach/", "tokens": ["Vor", "Men\u00b7schen", "und", "vor", "Vieh", "und", "an\u00b7dern", "mehr", "ge\u00b7brach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "KON", "PIS", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "Verlie\u00df man diesen Ort und gieng nach andern Pl\u00e4tzen/", "tokens": ["Ver\u00b7lie\u00df", "man", "die\u00b7sen", "Ort", "und", "gieng", "nach", "an\u00b7dern", "Pl\u00e4t\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PDAT", "NN", "KON", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Die dieses Reich vermag/ dieselbige zu sch\u00e4tzen.", "tokens": ["Die", "die\u00b7ses", "Reich", "ver\u00b7mag", "/", "die\u00b7sel\u00b7bi\u00b7ge", "zu", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "VVFIN", "$(", "PDS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.332": {"text": "Was in dem tieffsten war/ das muste vor den Schein/", "tokens": ["Was", "in", "dem", "tieffs\u00b7ten", "war", "/", "das", "mus\u00b7te", "vor", "den", "Schein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "ADJA", "VAFIN", "$(", "PDS", "VMFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.333": {"text": "Vor Kriegern kan noch Brod/ noch Gold verborgen seyn.", "tokens": ["Vor", "Krie\u00b7gern", "kan", "noch", "Brod", "/", "noch", "Gold", "ver\u00b7bor\u00b7gen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VMFIN", "ADV", "NN", "$(", "ADV", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Jhr Stahl gr\u00e4bt alles durch/ was ist hiervon zu sagen/", "tokens": ["Ihr", "Stahl", "gr\u00e4bt", "al\u00b7les", "durch", "/", "was", "ist", "hier\u00b7von", "zu", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PIS", "APPR", "$(", "PWS", "VAFIN", "PAV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Der Landsknecht ist ein Mensch/ und hat dergleichen Ma-", "tokens": ["Der", "Lands\u00b7knecht", "ist", "ein", "Mensch", "/", "und", "hat", "derg\u00b7lei\u00b7chen", "Ma"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$(", "KON", "VAFIN", "PIS", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.336": {"text": "Als ich und du/ und der/ er frist kein Haberstroh.", "tokens": ["Als", "ich", "und", "du", "/", "und", "der", "/", "er", "frist", "kein", "Ha\u00b7bers\u00b7troh", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "KON", "PPER", "$(", "KON", "ART", "$(", "PPER", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "Ein jeder suchet Geld/ der Landsknecht eben so.", "tokens": ["Ein", "je\u00b7der", "su\u00b7chet", "Geld", "/", "der", "Lands\u00b7knecht", "e\u00b7ben", "so", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "NN", "$(", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.338": {"text": "Wir lassen dieses Heer in B\u00f6h\u00e4im Beuthen machen/", "tokens": ["Wir", "las\u00b7sen", "die\u00b7ses", "Heer", "in", "B\u00f6\u00b7h\u00e4im", "Beu\u00b7then", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "APPR", "NE", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.339": {"text": "und gehen weiter fort/ zu sehen/ wie die Sachen", "tokens": ["und", "ge\u00b7hen", "wei\u00b7ter", "fort", "/", "zu", "se\u00b7hen", "/", "wie", "die", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "PTKVZ", "$(", "PTKZU", "VVINF", "$(", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "In Nieder-Sachsen seyn. O wunderbahrer Krieg!", "tokens": ["In", "Nie\u00b7der\u00b7Sach\u00b7sen", "seyn", ".", "O", "wun\u00b7der\u00b7bah\u00b7rer", "Krieg", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAINF", "$.", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.341": {"text": "Ist auch Bestand dabey/ da immer Gl\u00fcck und Sieg", "tokens": ["Ist", "auch", "Be\u00b7stand", "da\u00b7bey", "/", "da", "im\u00b7mer", "Gl\u00fcck", "und", "Sieg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "NN", "PAV", "$(", "ADV", "ADV", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.342": {"text": "Auf einer Seiten ist? Was unl\u00e4ngst Prag erlitte/", "tokens": ["Auf", "ei\u00b7ner", "Sei\u00b7ten", "ist", "?", "Was", "un\u00b7l\u00e4ngst", "Prag", "er\u00b7lit\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "$.", "PWS", "ADV", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.343": {"text": "Erlitt\u2019 auch Hildesheim/ das Hertzog G\u00f6rg bestritte.", "tokens": ["Er\u00b7litt'", "auch", "Hil\u00b7des\u00b7heim", "/", "das", "Hert\u00b7zog", "G\u00f6rg", "be\u00b7strit\u00b7te", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "NE", "$(", "ART", "NE", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.344": {"text": "Dem Orte Schutz zu thun/ zog General Geleen", "tokens": ["Dem", "Or\u00b7te", "Schutz", "zu", "thun", "/", "zog", "Ge\u00b7ne\u00b7ral", "Ge\u00b7leen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "VVFIN", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.345": {"text": "Mit vielem Beystand auf. King hatt\u2019 ihn kaum ersehn/", "tokens": ["Mit", "vie\u00b7lem", "Beys\u00b7tand", "auf", ".", "King", "hatt'", "ihn", "kaum", "er\u00b7sehn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "PTKVZ", "$.", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.346": {"text": "King/ ein ber\u00fchmter Held/ da zog er ihm entgegen/", "tokens": ["King", "/", "ein", "be\u00b7r\u00fchm\u00b7ter", "Held", "/", "da", "zog", "er", "ihm", "ent\u00b7ge\u00b7gen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ART", "ADJA", "NN", "$(", "ADV", "VVFIN", "PPER", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.347": {"text": "und schlug desselben Volck/ da\u00df solches auf den Wegen", "tokens": ["und", "schlug", "des\u00b7sel\u00b7ben", "Volck", "/", "da\u00df", "sol\u00b7ches", "auf", "den", "We\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PDAT", "NN", "$(", "KOUS", "PIS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.348": {"text": "Wie Holtz zersplittert lag. Worauf Geleen mit Rach", "tokens": ["Wie", "Holtz", "zer\u00b7split\u00b7tert", "lag", ".", "Wo\u00b7rauf", "Ge\u00b7leen", "mit", "Rach"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "VVPP", "VVFIN", "$.", "PAV", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.349": {"text": "Das H\u00f6chster \u00fcberfiel/ und alles was was er wach", "tokens": ["Das", "H\u00f6chs\u00b7ter", "\u00fc\u00b7berf\u00b7iel", "/", "und", "al\u00b7les", "was", "was", "er", "wach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$(", "KON", "PIS", "PWS", "PWS", "PPER", "VVFIN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.350": {"text": "und wehrsam fund/ erschlug. Ein mehrers zu verh\u00fctten/", "tokens": ["und", "wehr\u00b7sam", "fund", "/", "er\u00b7schlug", ".", "Ein", "meh\u00b7rers", "zu", "ver\u00b7h\u00fct\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "$(", "VVFIN", "$.", "ART", "PIS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.351": {"text": "Zog Hertzog G\u00f6rg auf jhn/ und schlug nach seinen Sitten", "tokens": ["Zog", "Hert\u00b7zog", "G\u00f6rg", "auf", "jhn", "/", "und", "schlug", "nach", "sei\u00b7nen", "Sit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "NE", "NE", "APPR", "PPER", "$(", "KON", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.352": {"text": "Denselben abermahls bey Boldern in die Flucht/", "tokens": ["Den\u00b7sel\u00b7ben", "a\u00b7ber\u00b7mahls", "bey", "Bol\u00b7dern", "in", "die", "Flucht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "ADV", "APPR", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.353": {"text": "und nahm ihm Hamm darzu/ di\u00df war die Sieges-Frucht.", "tokens": ["und", "nahm", "ihm", "Hamm", "dar\u00b7zu", "/", "di\u00df", "war", "die", "Sie\u00b7ges\u00b7Frucht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "NN", "PAV", "$(", "PDS", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.354": {"text": "Geleen zog nochmahls auf das Hildesheim zu freyen/", "tokens": ["Ge\u00b7leen", "zog", "noch\u00b7mahls", "auf", "das", "Hil\u00b7des\u00b7heim", "zu", "frey\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.355": {"text": "und lie\u00df sich aber mals bey Steuerwald zerstreuen/", "tokens": ["und", "lie\u00df", "sich", "a\u00b7ber", "mals", "bey", "Steu\u00b7er\u00b7wald", "zer\u00b7streu\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "ADV", "APPR", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.356": {"text": "Das ihm Herr Andersohn/ ein Mann aus Schweden/ that.", "tokens": ["Das", "ihm", "Herr", "An\u00b7der\u00b7sohn", "/", "ein", "Mann", "aus", "Schwe\u00b7den", "/", "that", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "NN", "NE", "$(", "ART", "NN", "APPR", "NE", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.357": {"text": "Er kam zum vierdten mahl/ die h\u00f6chst-bedr\u00e4ngte Stadt", "tokens": ["Er", "kam", "zum", "vierd\u00b7ten", "mahl", "/", "die", "h\u00f6chst\u00b7be\u00b7dr\u00e4ng\u00b7te", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "ADJA", "ADV", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.358": {"text": "Zu retten/ welches jhn zum vierdten mahl zerstreute/", "tokens": ["Zu", "ret\u00b7ten", "/", "wel\u00b7ches", "jhn", "zum", "vierd\u00b7ten", "mahl", "zer\u00b7streu\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPER", "APPRART", "ADJA", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.359": {"text": "Dann man bey Sachsenst\u00e4tt\u2019 ihm viel darnieder meyhte/", "tokens": ["Dann", "man", "bey", "Sach\u00b7sen\u00b7st\u00e4tt'", "ihm", "viel", "dar\u00b7nie\u00b7der", "meyh\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "APPR", "NE", "PPER", "ADV", "PAV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.360": {"text": "Da\u00df er entfliehen must. Hierauf gieng Hildesheim/", "tokens": ["Da\u00df", "er", "ent\u00b7flie\u00b7hen", "must", ".", "Hier\u00b7auf", "gieng", "Hil\u00b7des\u00b7heim", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "VMFIN", "$.", "PAV", "VVFIN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.361": {"text": "Von allen H\u00fclffen blo\u00df/ in Schwedisches Gez\u00e4um/", "tokens": ["Von", "al\u00b7len", "H\u00fclf\u00b7fen", "blo\u00df", "/", "in", "Schwe\u00b7di\u00b7sches", "Ge\u00b7z\u00e4um", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADV", "$(", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.362": {"text": "Ein Ort von gro\u00dfem Werth. Anitzo will ich sehen", "tokens": ["Ein", "Ort", "von", "gro\u00b7\u00dfem", "Werth", ".", "A\u00b7nit\u00b7zo", "will", "ich", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$.", "NE", "VMFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.363": {"text": "Wie es dem Bernhard mag/ dem tapfern Bernhard/ gehen.", "tokens": ["Wie", "es", "dem", "Bern\u00b7hard", "mag", "/", "dem", "tap\u00b7fern", "Bern\u00b7hard", "/", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NE", "VMFIN", "$(", "ART", "ADJA", "NE", "$(", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.364": {"text": "Wir haben vor gedacht/ wie er der sch\u00f6nen Stadt/", "tokens": ["Wir", "ha\u00b7ben", "vor", "ge\u00b7dacht", "/", "wie", "er", "der", "sch\u00f6\u00b7nen", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "VVPP", "$(", "PWAV", "PPER", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.365": {"text": "Die von dem Regenflu\u00df und Burg den Nahmen hat/", "tokens": ["Die", "von", "dem", "Re\u00b7gen\u00b7flu\u00df", "und", "Burg", "den", "Nah\u00b7men", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "KON", "NN", "ART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.366": {"text": "Geschwind hab obgesiegt/ und Stauff zur Lufft gespr\u00e4nget/", "tokens": ["Ge\u00b7schwind", "hab", "ob\u00b7ge\u00b7siegt", "/", "und", "Stauff", "zur", "Lufft", "ge\u00b7spr\u00e4n\u00b7get", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVPP", "$(", "KON", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.367": {"text": "Nun aber wurd ein Schlo\u00df von seiner Macht bedr\u00e4nget/", "tokens": ["Nun", "a\u00b7ber", "wurd", "ein", "Schlo\u00df", "von", "sei\u00b7ner", "Macht", "be\u00b7dr\u00e4n\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.368": {"text": "Das Wiltzbnrg wird genannt. Der Beyer that Entsatz/", "tokens": ["Das", "Wiltzbnrg", "wird", "ge\u00b7nannt", ".", "Der", "Be\u00b7yer", "that", "Ent\u00b7satz", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$.", "ART", "NN", "VVFIN", "NN", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.369": {"text": "Es blieben aber wol drey hundert auf dem Platz/", "tokens": ["Es", "blie\u00b7ben", "a\u00b7ber", "wol", "drey", "hun\u00b7dert", "auf", "dem", "Platz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "CARD", "CARD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.370": {"text": "und tausend im Verhafft. Ein gleiches ist den Scharen/", "tokens": ["und", "tau\u00b7send", "im", "Ver\u00b7hafft", ".", "Ein", "glei\u00b7ches", "ist", "den", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPRART", "NN", "$.", "ART", "ADJA", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.371": {"text": "Die man Croaten nennt/ bey Eger wieder fahren/", "tokens": ["Die", "man", "Croa\u00b7ten", "nennt", "/", "bey", "E\u00b7ger", "wie\u00b7der", "fah\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "NN", "VVFIN", "$(", "APPR", "NE", "ADV", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.372": {"text": "Di\u00df hatten Ros\u2019 und Karpf/ zween Obersten/ gethan.", "tokens": ["Di\u00df", "hat\u00b7ten", "Ros'", "und", "Karpf", "/", "zween", "O\u00b7bers\u00b7ten", "/", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "KON", "NN", "$(", "VVFIN", "NN", "$(", "VVPP", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.373": {"text": "Nach diesem wolte man dem Cronach wieder an/", "tokens": ["Nach", "die\u00b7sem", "wol\u00b7te", "man", "dem", "Cro\u00b7nach", "wie\u00b7der", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VMFIN", "PIS", "ART", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.374": {"text": "Wurd aber bald gewehrt/ weil Ferdinand/ der Dritte/", "tokens": ["Wurd", "a\u00b7ber", "bald", "ge\u00b7wehrt", "/", "weil", "Fer\u00b7di\u00b7nand", "/", "der", "Drit\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "VVPP", "$(", "KOUS", "NE", "$(", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.375": {"text": "Des K\u00e4ysers gro\u00dfer Sohn/ sein Regenfpurg bestritte/", "tokens": ["Des", "K\u00e4y\u00b7sers", "gro\u00b7\u00dfer", "Sohn", "/", "sein", "Re\u00b7gen\u00b7fpurg", "be\u00b7strit\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$(", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.376": {"text": "und 60000. starck/ mit Altrings seiner Schar/", "tokens": ["und", "60000.", "starck", "/", "mit", "A\u00b7ltrings", "sei\u00b7ner", "Schar", "/"], "token_info": ["word", "ordinal", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$(", "APPR", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.377": {"text": "Die wieder muhtig gieng/ daf\u00fcr in Waffen war. ", "tokens": ["Die", "wie\u00b7der", "muh\u00b7tig", "gieng", "/", "da\u00b7f\u00fcr", "in", "Waf\u00b7fen", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VVFIN", "$(", "PAV", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.378": {"text": "Darum der gute F\u00fcrst/ vor solcher M\u00e4nge Waffen", "tokens": ["Da\u00b7rum", "der", "gu\u00b7te", "F\u00fcrst", "/", "vor", "sol\u00b7cher", "M\u00e4n\u00b7ge", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "$(", "APPR", "PIAT", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.379": {"text": "Der vielgeliebten Stadt gew\u00fcndschten Schutz zu schaffen/", "tokens": ["Der", "viel\u00b7ge\u00b7lieb\u00b7ten", "Stadt", "ge\u00b7w\u00fcnd\u00b7schten", "Schutz", "zu", "schaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.380": {"text": "Sehr wach und eufrig war/ auch 20000. Mann", "tokens": ["Sehr", "wach", "und", "euf\u00b7rig", "war", "/", "auch", "20000.", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "ordinal", "word"], "pos": ["ADV", "ADJD", "KON", "ADJD", "VAFIN", "$(", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.381": {"text": "In einen Hauffen bracht\u2019 und wagte sich hinan/", "tokens": ["In", "ei\u00b7nen", "Hauf\u00b7fen", "bracht'", "und", "wag\u00b7te", "sich", "hi\u00b7nan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "KON", "VVFIN", "PRF", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.382": {"text": "Der Stadt Entsatz zu thun/ must\u2019 aber wieder weichen/", "tokens": ["Der", "Stadt", "Ent\u00b7satz", "zu", "thun", "/", "must'", "a\u00b7ber", "wie\u00b7der", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "VMFIN", "ADV", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.383": {"text": "Dann es war seine Macht der andern nicht zu gleichen/", "tokens": ["Dann", "es", "war", "sei\u00b7ne", "Macht", "der", "an\u00b7dern", "nicht", "zu", "glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PPOSAT", "NN", "ART", "ADJA", "PTKNEG", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.384": {"text": "Woraus Gustavus Horn jhm bald zu H\u00fclffe kam/", "tokens": ["Wo\u00b7raus", "Gus\u00b7ta\u00b7vus", "Horn", "jhm", "bald", "zu", "H\u00fclf\u00b7fe", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "PPER", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.385": {"text": "und einen andern Zug mit jhm in B\u00e4yern nahm.", "tokens": ["und", "ei\u00b7nen", "an\u00b7dern", "Zug", "mit", "jhm", "in", "B\u00e4\u00b7yern", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "APPR", "PPER", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.386": {"text": "In welchem Zuge sie gantz Eicha flammicht machten/", "tokens": ["In", "wel\u00b7chem", "Zu\u00b7ge", "sie", "gantz", "Eic\u00b7ha", "flam\u00b7micht", "mach\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "PPER", "ADV", "NN", "VVFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.387": {"text": "Nach diesem Freysingen um Geld und Fahrni\u00df brachten.", "tokens": ["Nach", "die\u00b7sem", "Frey\u00b7sin\u00b7gen", "um", "Geld", "und", "Fahr\u00b7ni\u00df", "brach\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-++--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.388": {"text": "Auf solches gieng der Zug auf Landshut/ welche Stadt", "tokens": ["Auf", "sol\u00b7ches", "gieng", "der", "Zug", "auf", "Lands\u00b7hut", "/", "wel\u00b7che", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIS", "VVFIN", "ART", "NN", "APPR", "NE", "$(", "PWAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.389": {"text": "Den h\u00f6chsten Thurm bey uns nechst dem zu Stra\u00dfburg hat.", "tokens": ["Den", "h\u00f6chs\u00b7ten", "Thurm", "bey", "uns", "nechst", "dem", "zu", "Stra\u00df\u00b7burg", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PPER", "VVFIN", "ART", "APPR", "NE", "VAFIN", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.390": {"text": "Sie wurd\u2019 in Ernst bedr\u00e4ut die Waffen abzulegen/", "tokens": ["Sie", "wurd'", "in", "Ernst", "be\u00b7dr\u00e4ut", "die", "Waf\u00b7fen", "ab\u00b7zu\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NE", "VVFIN", "ART", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.391": {"text": "Sie aber scho\u00df in Ernst jhr Kraut und Loth dargegen/", "tokens": ["Sie", "a\u00b7ber", "scho\u00df", "in", "Ernst", "jhr", "Kraut", "und", "Loth", "dar\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "APPR", "NE", "PPOSAT", "NN", "KON", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.392": {"text": "Damit gieng alle Macht in einem Sturm darauff/", "tokens": ["Da\u00b7mit", "gieng", "al\u00b7le", "Macht", "in", "ei\u00b7nem", "Sturm", "dar\u00b7auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.393": {"text": "Hierob kam Altringer und sein ergrimter Hauff/", "tokens": ["Hier\u00b7ob", "kam", "A\u00b7ltrin\u00b7ger", "und", "sein", "er\u00b7grim\u00b7ter", "Hauff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "KON", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.394": {"text": "Der Stadt Entsatz zu thun/ sah aber mit betr\u00fcben/", "tokens": ["Der", "Stadt", "Ent\u00b7satz", "zu", "thun", "/", "sah", "a\u00b7ber", "mit", "be\u00b7tr\u00fc\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "VVFIN", "ADV", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.395": {"text": "Wie jene dort und da die Stadt darnieder hieben.", "tokens": ["Wie", "je\u00b7ne", "dort", "und", "da", "die", "Stadt", "dar\u00b7nie\u00b7der", "hie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDS", "ADV", "KON", "ADV", "ART", "NN", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.396": {"text": "Es gieng erb\u00e4rmlich zu/ da waren Schwerdt und Glut/", "tokens": ["Es", "gieng", "er\u00b7b\u00e4rm\u00b7lich", "zu", "/", "da", "wa\u00b7ren", "Schwerdt", "und", "Glut", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKZU", "$(", "ADV", "VAFIN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.397": {"text": "Da war auch \u00fcber di\u00df des Jsers strenge Flut/", "tokens": ["Da", "war", "auch", "\u00fc\u00b7ber", "di\u00df", "des", "Jsers", "stren\u00b7ge", "Flut", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "APPR", "PDS", "ART", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.398": {"text": "Die allesamt der Stadt ihr Volck sehr hoch bedr\u00e4ngten/", "tokens": ["Die", "al\u00b7le\u00b7samt", "der", "Stadt", "ihr", "Volck", "sehr", "hoch", "be\u00b7dr\u00e4ng\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "PPOSAT", "NN", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.399": {"text": "und \u00fcber tausend Mann ers\u00e4ufften/ w\u00fcrgt- und s\u00e4ngten.", "tokens": ["und", "\u00fc\u00b7ber", "tau\u00b7send", "Mann", "er\u00b7s\u00e4uff\u00b7ten", "/", "w\u00fcrg\u00b7t", "und", "s\u00e4ng\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "NN", "VVFIN", "$(", "TRUNC", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.400": {"text": "Der in der Stadt verblieb verdorb durch seinen Feind/", "tokens": ["Der", "in", "der", "Stadt", "ver\u00b7blieb", "ver\u00b7dorb", "durch", "sei\u00b7nen", "Feind", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADJD", "VVFIN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.401": {"text": "Der sich ins Feld begab/ verdorb durch seinen Freind.", "tokens": ["Der", "sich", "ins", "Feld", "be\u00b7gab", "/", "ver\u00b7dorb", "durch", "sei\u00b7nen", "Freind", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPRART", "NN", "VVFIN", "$(", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.402": {"text": "Dann wie die Bayrischen die Stadt verlohren sahen/", "tokens": ["Dann", "wie", "die", "Bay\u00b7ri\u00b7schen", "die", "Stadt", "ver\u00b7loh\u00b7ren", "sa\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "NN", "ART", "NN", "VVPP", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.403": {"text": "und keiner sich dem Feind in solcher dorffte nahen/", "tokens": ["und", "kei\u00b7ner", "sich", "dem", "Feind", "in", "sol\u00b7cher", "dorff\u00b7te", "na\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PRF", "ART", "NN", "APPR", "PIAT", "ADJA", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.404": {"text": "Da thaten sie als Feind. Was jener \u00fcberlie\u00df/", "tokens": ["Da", "tha\u00b7ten", "sie", "als", "Feind", ".", "Was", "je\u00b7ner", "\u00fc\u00b7ber\u00b7lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KOUS", "NN", "$.", "PWS", "PDS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.405": {"text": "Das nahmen diese weg. Der Altring scho\u00df und stie\u00df/", "tokens": ["Das", "nah\u00b7men", "die\u00b7se", "weg", ".", "Der", "A\u00b7ltring", "scho\u00df", "und", "stie\u00df", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PDS", "PTKVZ", "$.", "ART", "NN", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.406": {"text": "In Meynung seinem Volck hierinnen abzuwehren/", "tokens": ["In", "Mey\u00b7nung", "sei\u00b7nem", "Volck", "hie\u00b7rin\u00b7nen", "ab\u00b7zu\u00b7weh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPOSAT", "NN", "PAV", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.407": {"text": "Umsonst/ es muste sich die Stadt zu Grunde kehren/", "tokens": ["Um\u00b7sonst", "/", "es", "mus\u00b7te", "sich", "die", "Stadt", "zu", "Grun\u00b7de", "keh\u00b7ren", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PPER", "VMFIN", "PRF", "ART", "NN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.408": {"text": "und Altring selber mit.", "tokens": ["und", "A\u00b7ltring", "sel\u00b7ber", "mit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.409": {"text": "Da\u00df jhm den weisen Geist aus seinem C\u00f6rper nahm.", "tokens": ["Da\u00df", "jhm", "den", "wei\u00b7sen", "Geist", "aus", "sei\u00b7nem", "C\u00f6r\u00b7per", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.410": {"text": "Er war ein kluger Herr/ hatt\u2019 aber wenig Gl\u00fccke/", "tokens": ["Er", "war", "ein", "klu\u00b7ger", "Herr", "/", "hatt'", "a\u00b7ber", "we\u00b7nig", "Gl\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$(", "VAFIN", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.411": {"text": "und gieng manch guter Raht in seiner That zu r\u00fccke.", "tokens": ["und", "gieng", "manch", "gu\u00b7ter", "Raht", "in", "sei\u00b7ner", "That", "zu", "r\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "ADJA", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.412": {"text": "Nach diesem allen gieng die B\u00e4yerische Schar", "tokens": ["Nach", "die\u00b7sem", "al\u00b7len", "gieng", "die", "B\u00e4\u00b7ye\u00b7ri\u00b7sche", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "PIS", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.413": {"text": "Zur Donau wieder hin/ woher sie kommen war/", "tokens": ["Zur", "Do\u00b7nau", "wie\u00b7der", "hin", "/", "wo\u00b7her", "sie", "kom\u00b7men", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NE", "ADV", "PTKVZ", "$(", "PWAV", "PPER", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.414": {"text": "Das gute Regenspurg noch weiter zu bedr\u00fccken.", "tokens": ["Das", "gu\u00b7te", "Re\u00b7gen\u00b7spurg", "noch", "wei\u00b7ter", "zu", "be\u00b7dr\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.415": {"text": "Die Schweden folgten nach und waren stets im R\u00fccken/", "tokens": ["Die", "Schwe\u00b7den", "folg\u00b7ten", "nach", "und", "wa\u00b7ren", "stets", "im", "R\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "APPR", "KON", "VAFIN", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.416": {"text": "Vermeynten auch der Stadt in Gut und Bluts-Gefahr", "tokens": ["Ver\u00b7meyn\u00b7ten", "auch", "der", "Stadt", "in", "Gut", "und", "Bluts\u00b7Ge\u00b7fahr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.417": {"text": "Noch eins Entsatz zu thun. Wie aber Altrings Schar", "tokens": ["Noch", "eins", "Ent\u00b7satz", "zu", "thun", ".", "Wie", "a\u00b7ber", "A\u00b7ltrings", "Schar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "CARD", "NN", "PTKZU", "VVINF", "$.", "PWAV", "ADV", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.418": {"text": "Zu spat vor Landhut kam/ kam diese gleicher ma\u00dfen", "tokens": ["Zu", "spat", "vor", "Land\u00b7hut", "kam", "/", "kam", "die\u00b7se", "glei\u00b7cher", "ma\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKA", "ADJD", "APPR", "NN", "VVFIN", "$(", "VVFIN", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.419": {"text": "Zu spat vor Regenspurg/ dann es in allen Stra\u00dfen", "tokens": ["Zu", "spat", "vor", "Re\u00b7gen\u00b7spurg", "/", "dann", "es", "in", "al\u00b7len", "Stra\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKA", "ADJD", "APPR", "NN", "$(", "ADV", "PPER", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.420": {"text": "Daselbst schon K\u00e4ysrisch war. Es gieng am Pulver ab/", "tokens": ["Da\u00b7selbst", "schon", "K\u00e4y\u00b7srisch", "war", ".", "Es", "gieng", "am", "Pul\u00b7ver", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "VAFIN", "$.", "PPER", "VVFIN", "APPRART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.421": {"text": "und dieser Mangel zwung die Stadt ", "tokens": ["und", "die\u00b7ser", "Man\u00b7gel", "zwung", "die", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PDAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.422": {"text": "Sie hat vier hundert und auch f\u00fcnf und sechtzig F\u00e4lle", "tokens": ["Sie", "hat", "vier", "hun\u00b7dert", "und", "auch", "f\u00fcnf", "und", "secht\u00b7zig", "F\u00e4l\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "CARD", "CARD", "KON", "ADV", "CARD", "KON", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.423": {"text": "Aus jhr ins K\u00e4ysers Heer und dessen f\u00e4ste St\u00e4lle", "tokens": ["Aus", "jhr", "ins", "K\u00e4y\u00b7sers", "Heer", "und", "des\u00b7sen", "f\u00e4s\u00b7te", "St\u00e4l\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "APPRART", "NN", "NN", "KON", "PDS", "VVFIN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.424": {"text": "Mit Sieg und Ruhm gethan. Sie wurde sieben mahl", "tokens": ["Mit", "Sieg", "und", "Ruhm", "ge\u00b7than", ".", "Sie", "wur\u00b7de", "sie\u00b7ben", "mahl"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "$.", "PPER", "VAFIN", "CARD", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.425": {"text": "Mit gro\u00dfer Macht gest\u00fcrmt/ da\u00df eine gro\u00dfe Zahl", "tokens": ["Mit", "gro\u00b7\u00dfer", "Macht", "ge\u00b7st\u00fcrmt", "/", "da\u00df", "ei\u00b7ne", "gro\u00b7\u00dfe", "Zahl"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$(", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.426": {"text": "Von ihren Feinden blieb/ dann sie den sieben St\u00fcrmen", "tokens": ["Von", "ih\u00b7ren", "Fein\u00b7den", "blieb", "/", "dann", "sie", "den", "sie\u00b7ben", "St\u00fcr\u00b7men"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "ADV", "PPER", "ART", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.427": {"text": "Von jhren Mauren au\u00df/ Pasteyen/ Schantz- und Th\u00fcrmen", "tokens": ["Von", "jhren", "Mau\u00b7ren", "au\u00df", "/", "Pas\u00b7te\u00b7yen", "/", "Schant\u00b7z", "und", "Th\u00fcr\u00b7men"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$(", "NN", "$(", "TRUNC", "KON", "NN"], "meter": "--+--+--+--+-", "measure": "anapaest.tetra.plus"}, "line.428": {"text": "Erschr\u00f6cklich widerstundt\u2019. Jhr Kriegs-H\u00e4upt war Lars", "tokens": ["Er\u00b7schr\u00f6ck\u00b7lich", "wi\u00b7der\u00b7stundt'", ".", "Ihr", "Kriegs\u00b7H\u00e4upt", "war", "Lars"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "PTKVZ", "$.", "PPOSAT", "NN", "VAFIN", "NN"], "meter": "-+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.429": {"text": "Kag/", "tokens": ["Kag", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+", "measure": "single.up"}, "line.430": {"text": "Ein Held/ den auch sein Feind nicht anders nennen mag/", "tokens": ["Ein", "Held", "/", "den", "auch", "sein", "Feind", "nicht", "an\u00b7ders", "nen\u00b7nen", "mag", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "ADV", "PPOSAT", "NN", "PTKNEG", "ADV", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.431": {"text": "Als einen tapfern Mann. Es sey hierbey geschrieben/", "tokens": ["Als", "ei\u00b7nen", "tap\u00b7fern", "Mann", ".", "Es", "sey", "hier\u00b7bey", "ge\u00b7schrie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.432": {"text": "Da\u00df achtmal tausend Mann vor Regenspurg geblieben/", "tokens": ["Da\u00df", "acht\u00b7mal", "tau\u00b7send", "Mann", "vor", "Re\u00b7gen\u00b7spurg", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "CARD", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.433": {"text": "und umgekommen seyn/ und da\u00df ein hundert St\u00fcck", "tokens": ["und", "um\u00b7ge\u00b7kom\u00b7men", "seyn", "/", "und", "da\u00df", "ein", "hun\u00b7dert", "St\u00fcck"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "VAINF", "$(", "KON", "KOUS", "ART", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.434": {"text": "Erschr\u00f6cklich gro\u00df auf sie und jhre feste Br\u00fcck/", "tokens": ["Er\u00b7schr\u00f6ck\u00b7lich", "gro\u00df", "auf", "sie", "und", "jhre", "fes\u00b7te", "Br\u00fcck", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "APPR", "PPER", "KON", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.435": {"text": "Auf der drey Th\u00fcrme stehn/ die viel zu schaffen gaben/", "tokens": ["Auf", "der", "drey", "Th\u00fcr\u00b7me", "stehn", "/", "die", "viel", "zu", "schaf\u00b7fen", "ga\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "CARD", "NN", "VVINF", "$(", "ART", "PIS", "PTKZU", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.436": {"text": "Mit Donnder-gleichen Macht auf sie gehagelt haben.", "tokens": ["Mit", "Donn\u00b7der\u00b7glei\u00b7chen", "Macht", "auf", "sie", "ge\u00b7ha\u00b7gelt", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "APPR", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.437": {"text": "Da\u00df funfzehn tausend Sch\u00fc\u00df aus St\u00fccken auf die Stadt/", "tokens": ["Da\u00df", "funf\u00b7zehn", "tau\u00b7send", "Sch\u00fc\u00df", "aus", "St\u00fc\u00b7cken", "auf", "die", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "CARD", "NN", "APPR", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.438": {"text": "So lang sie Widerstand von jhren Wercken that/", "tokens": ["So", "lang", "sie", "Wi\u00b7der\u00b7stand", "von", "jhren", "Wer\u00b7cken", "that", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.439": {"text": "Geschehen seyn/ ist wahr/ und w\u00fcrdig zu bem\u00e4rcken.", "tokens": ["Ge\u00b7sche\u00b7hen", "seyn", "/", "ist", "wahr", "/", "und", "w\u00fcr\u00b7dig", "zu", "be\u00b7m\u00e4r\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAINF", "$(", "VAFIN", "ADJD", "$(", "KON", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.440": {"text": "Und wird derselben Mahl an Br\u00fccken/ Th\u00fcrm- und Wer-\ncken", "tokens": ["Und", "wird", "der\u00b7sel\u00b7ben", "Mahl", "an", "Br\u00fc\u00b7cken", "/", "Th\u00fcr\u00b7m", "und", "Wer", "cken"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PDAT", "NN", "APPR", "NN", "$(", "TRUNC", "KON", "TRUNC", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.441": {"text": "Nicht au\u00df gel\u00f6schet seyn. Wie Hertzog Bernhard sah", "tokens": ["Nicht", "au\u00df", "ge\u00b7l\u00f6\u00b7schet", "seyn", ".", "Wie", "Hert\u00b7zog", "Bern\u00b7hard", "sah"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKNEG", "PTKVZ", "VVPP", "VAINF", "$.", "PWAV", "NE", "NE", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.442": {"text": "Da\u00df sein Entsatz zu spat/ und war er noch so nah/", "tokens": ["Da\u00df", "sein", "Ent\u00b7satz", "zu", "spat", "/", "und", "war", "er", "noch", "so", "nah", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKZU", "VVFIN", "$(", "KON", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.443": {"text": "Gieng er nach Augspurg zu/ woselbst er seine Scharen", "tokens": ["Gieng", "er", "nach", "Augs\u00b7purg", "zu", "/", "wo\u00b7selbst", "er", "sei\u00b7ne", "Scha\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NE", "PTKZU", "$(", "PWAV", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.444": {"text": "Zertheilte. Die dem Horn in dem Gebiete waren/", "tokens": ["Zer\u00b7theil\u00b7te", ".", "Die", "dem", "Horn", "in", "dem", "Ge\u00b7bie\u00b7te", "wa\u00b7ren", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ART", "ART", "NN", "APPR", "ART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.445": {"text": "Verreisten nach Tyrol/ der Spannischen Armee/", "tokens": ["Ver\u00b7reis\u00b7ten", "nach", "Ty\u00b7rol", "/", "der", "Span\u00b7ni\u00b7schen", "Ar\u00b7mee", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.446": {"text": "Die der Toledo bracht\u2019 und damals in der N\u00e4h", "tokens": ["Die", "der", "To\u00b7le\u00b7do", "bracht'", "und", "da\u00b7mals", "in", "der", "N\u00e4h"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NE", "VVFIN", "KON", "ADV", "APPR", "ART", "NN"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.447": {"text": "Auf Deutschland lo\u00df zu gehn/ sich w\u00fcrcklich lie\u00df vermercken/", "tokens": ["Auf", "Deutschland", "lo\u00df", "zu", "gehn", "/", "sich", "w\u00fcrck\u00b7lich", "lie\u00df", "ver\u00b7mer\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PTKVZ", "PTKZU", "VVINF", "$(", "PRF", "ADJD", "VVFIN", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.448": {"text": "Des K\u00e4ysers seine Macht ein mehres zu verst\u00e4rcken/", "tokens": ["Des", "K\u00e4y\u00b7sers", "sei\u00b7ne", "Macht", "ein", "meh\u00b7res", "zu", "ver\u00b7st\u00e4r\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "ART", "PIS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.449": {"text": "Ein Dorn und Damm zu seyn. Bernhardus aber gieng", "tokens": ["Ein", "Dorn", "und", "Damm", "zu", "seyn", ".", "Bern\u00b7har\u00b7dus", "a\u00b7ber", "gieng"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "KON", "NN", "PTKZU", "VAINF", "$.", "NE", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.450": {"text": "Nach Lauingen/ woselbst er b\u00f6se Post empfieng/", "tokens": ["Nach", "Lau\u00b7in\u00b7gen", "/", "wo\u00b7selbst", "er", "b\u00f6\u00b7se", "Post", "emp\u00b7fi\u00b7eng", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "PWAV", "PPER", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.451": {"text": "Da\u00df Ferdinands Gewalt nach Donawerth und weiter", "tokens": ["Da\u00df", "Fer\u00b7di\u00b7nands", "Ge\u00b7walt", "nach", "Do\u00b7na\u00b7werth", "und", "wei\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "NN", "APPR", "NN", "KON", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.452": {"text": "Zu gehen eilete/ wie dann 6000. Reiter/", "tokens": ["Zu", "ge\u00b7hen", "ei\u00b7le\u00b7te", "/", "wie", "dann", "6000.", "Rei\u00b7ter", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "ordinal", "word", "punct"], "pos": ["PTKZU", "VVINF", "VVFIN", "$(", "KOKOM", "ADV", "ADJA", "NN", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.453": {"text": "Die man Croaten nennt/ mit Rauben/ Mord und Brand", "tokens": ["Die", "man", "Croa\u00b7ten", "nennt", "/", "mit", "Rau\u00b7ben", "/", "Mord", "und", "Brand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "PIS", "NN", "VVFIN", "$(", "APPR", "NN", "$(", "NN", "KON", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.454": {"text": "und tausend ", "tokens": ["und", "tau\u00b7send"], "token_info": ["word", "word"], "pos": ["KON", "CARD"], "meter": "-+-", "measure": "amphibrach.single"}, "line.455": {"text": "und auch an Donawerth gantz unversehens waren/", "tokens": ["und", "auch", "an", "Do\u00b7na\u00b7werth", "gantz", "un\u00b7ver\u00b7se\u00b7hens", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "ADV", "ADV", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.456": {"text": "Hatt\u2019 also Bernhard sich hier vieles zu befahren/", "tokens": ["Hatt'", "al\u00b7so", "Bern\u00b7hard", "sich", "hier", "vie\u00b7les", "zu", "be\u00b7fah\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NE", "PRF", "ADV", "PIS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.457": {"text": "Zumahl weil Donawerth in Feindes H\u00e4nde kam.", "tokens": ["Zu\u00b7mahl", "weil", "Do\u00b7na\u00b7werth", "in", "Fein\u00b7des", "H\u00e4n\u00b7de", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "NN", "APPR", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.458": {"text": "Daher er abermals den Horn zu H\u00fclffe nahm/", "tokens": ["Da\u00b7her", "er", "a\u00b7ber\u00b7mals", "den", "Horn", "zu", "H\u00fclf\u00b7fe", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "ART", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.459": {"text": "Den Einbruch seines Feinds mit Macht zu unterbrechen/", "tokens": ["Den", "Ein\u00b7bruch", "sei\u00b7nes", "Feinds", "mit", "Macht", "zu", "un\u00b7ter\u00b7bre\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.460": {"text": "Der schon bey N\u00f6rdlingen sein Lager abzustechen/", "tokens": ["Der", "schon", "bey", "N\u00f6rd\u00b7lin\u00b7gen", "sein", "La\u00b7ger", "ab\u00b7zu\u00b7ste\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NN", "PPOSAT", "NN", "VVIZU", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.461": {"text": "und dessen Herr zu seyn/ sehr gro\u00df gesch\u00e4fftig war.", "tokens": ["und", "des\u00b7sen", "Herr", "zu", "seyn", "/", "sehr", "gro\u00df", "ge\u00b7sch\u00e4ff\u00b7tig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRELAT", "NN", "PTKZU", "VAINF", "$(", "ADV", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.462": {"text": "Dem allen vor zu seyn/ kam Hertzog Bernhards Schar/", "tokens": ["Dem", "al\u00b7len", "vor", "zu", "seyn", "/", "kam", "Hert\u00b7zog", "Bern\u00b7hards", "Schar", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "APPR", "PTKZU", "VAINF", "$(", "VVFIN", "NE", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.463": {"text": "Des Horn/ des Birckenfelds/ des Durlachs seine Hauf-", "tokens": ["Des", "Horn", "/", "des", "Bir\u00b7cken\u00b7felds", "/", "des", "Dur\u00b7lachs", "sei\u00b7ne", "Hauf"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "ART", "NN", "$(", "ART", "NN", "PPOSAT", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.464": {"text": "und des von W\u00fcrtenbergs/ sich um den Ort zu rauffen/", "tokens": ["und", "des", "von", "W\u00fcr\u00b7ten\u00b7bergs", "/", "sich", "um", "den", "Ort", "zu", "rauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "APPR", "NE", "$(", "PRF", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.465": {"text": "Hier wa: Lars Kag/ hier war der nunmehr Schwedsche Kratz/", "tokens": ["Hier", "wa", ":", "Lars", "Kag", "/", "hier", "war", "der", "nun\u00b7mehr", "Schwed\u00b7sche", "Kratz", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$.", "NE", "NE", "$(", "ADV", "VAFIN", "ART", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.466": {"text": "und bracht ein jeglicher viel V\u00f6lcker auf den Platz/", "tokens": ["und", "bracht", "ein", "jeg\u00b7li\u00b7cher", "viel", "V\u00f6l\u00b7cker", "auf", "den", "Platz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "PIAT", "PIAT", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.467": {"text": "Zu sterben/ oder dann mit Ehren obzuligen.", "tokens": ["Zu", "ster\u00b7ben", "/", "o\u00b7der", "dann", "mit", "Eh\u00b7ren", "ob\u00b7zu\u00b7li\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.468": {"text": "Es lie\u00df sich anfangs auch sehr herrlich mit den Siegen", "tokens": ["Es", "lie\u00df", "sich", "an\u00b7fangs", "auch", "sehr", "herr\u00b7lich", "mit", "den", "Sie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "ADV", "ADV", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.469": {"text": "Bey diesen Helden an/ wie dann bey Popfingen/", "tokens": ["Bey", "die\u00b7sen", "Hel\u00b7den", "an", "/", "wie", "dann", "bey", "Pop\u00b7fin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "PTKVZ", "$(", "KOKOM", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.470": {"text": "Gem\u00fcnd und anderswo um N\u00f6rdlingen geschehn.", "tokens": ["Ge\u00b7m\u00fcnd", "und", "an\u00b7ders\u00b7wo", "um", "N\u00f6rd\u00b7lin\u00b7gen", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.471": {"text": "Und sind bey Popfingen ein tausend Mann geblieben/", "tokens": ["Und", "sind", "bey", "Pop\u00b7fin\u00b7gen", "ein", "tau\u00b7send", "Mann", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "NN", "ART", "CARD", "NN", "VVPP", "$("], "meter": "-+-++--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.472": {"text": "Die Hertzog Bernhards Volck mit Macht hat aufgerie-", "tokens": ["Die", "Hert\u00b7zog", "Bern\u00b7hards", "Volck", "mit", "Macht", "hat", "auf\u00b7ge\u00b7rie"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NE", "NE", "NN", "APPR", "NN", "VAFIN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.473": {"text": "Ein tausend bey Gem\u00fcnd/ bey Aalen halb so viel.", "tokens": ["Ein", "tau\u00b7send", "bey", "Ge\u00b7m\u00fcnd", "/", "bey", "Aa\u00b7len", "halb", "so", "viel", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "APPR", "NN", "$(", "APPR", "NN", "ADJD", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.474": {"text": "Ich schweige wie gestreng der Ros\u2019 aus D\u00fcnckelspiel/", "tokens": ["Ich", "schwei\u00b7ge", "wie", "ge\u00b7streng", "der", "Ros'", "aus", "D\u00fcn\u00b7ckel\u00b7spiel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ADJD", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.475": {"text": "Canofsky/ Commendant in Buchorn und bey Giengen/", "tokens": ["Ca\u00b7nofs\u00b7ky", "/", "Com\u00b7men\u00b7dant", "in", "Bu\u00b7chorn", "und", "bey", "Gien\u00b7gen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NN", "APPR", "NN", "KON", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.476": {"text": "Der Oberst Milneckhau\u00df/ des K\u00e4ysers Volck empfiengen.", "tokens": ["Der", "O\u00b7berst", "Mil\u00b7neck\u00b7hau\u00df", "/", "des", "K\u00e4y\u00b7sers", "Volck", "emp\u00b7fi\u00b7en\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$(", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.477": {"text": "In dem man beyderseits viel scharmuzierens trieb/", "tokens": ["In", "dem", "man", "bey\u00b7der\u00b7seits", "viel", "schar\u00b7mu\u00b7zie\u00b7rens", "trieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIS", "ADV", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.478": {"text": "und sonder gro\u00dfe Frucht viel Volcks darnieder hieb/", "tokens": ["und", "son\u00b7der", "gro\u00b7\u00dfe", "Frucht", "viel", "Volcks", "dar\u00b7nie\u00b7der", "hieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "NN", "PIAT", "NN", "PAV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.479": {"text": "Kam ", "tokens": ["Kam"], "token_info": ["word"], "pos": ["NE"], "meter": "-", "measure": "single.down"}, "line.480": {"text": "Des K\u00e4ysers Hilf zu thun/ die vor sehr m\u00e4chtig waren/", "tokens": ["Des", "K\u00e4y\u00b7sers", "Hilf", "zu", "thun", "/", "die", "vor", "sehr", "m\u00e4ch\u00b7tig", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "PTKZU", "VVINF", "$(", "ART", "APPR", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.481": {"text": "Der eine brachte mehr als sieben tausend Mann/", "tokens": ["Der", "ei\u00b7ne", "brach\u00b7te", "mehr", "als", "sie\u00b7ben", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PIS", "KOKOM", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.482": {"text": "Der ander fast so viel zu drey\u00dfig tausend an.", "tokens": ["Der", "an\u00b7der", "fast", "so", "viel", "zu", "drey\u00b7\u00dfig", "tau\u00b7send", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "ADV", "ADV", "APPR", "CARD", "CARD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.483": {"text": "Nach dem sie N\u00f6rdlingen zur h\u00f6chsten Sorge brachten/", "tokens": ["Nach", "dem", "sie", "N\u00f6rd\u00b7lin\u00b7gen", "zur", "h\u00f6chs\u00b7ten", "Sor\u00b7ge", "brach\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-++--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.484": {"text": "und die Bel\u00e4gerte viel Feuer-Zeichen machten/", "tokens": ["und", "die", "Be\u00b7l\u00e4\u00b7ger\u00b7te", "viel", "Feu\u00b7er\u00b7Zei\u00b7chen", "mach\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.485": {"text": "Als Zeichen jhrer Noth und suchten also Schirm/", "tokens": ["Als", "Zei\u00b7chen", "jhrer", "Noth", "und", "such\u00b7ten", "al\u00b7so", "Schirm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPOSAT", "NN", "KON", "VVFIN", "ADV", "NN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.486": {"text": "Zog Hertzog Bernhard an das m\u00e4chtige Gest\u00fcrm/", "tokens": ["Zog", "Hert\u00b7zog", "Bern\u00b7hard", "an", "das", "m\u00e4ch\u00b7ti\u00b7ge", "Ge\u00b7st\u00fcrm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NE", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.487": {"text": "Zu \u00e4ndern und zugleich an eine Schlacht zu gehen/", "tokens": ["Zu", "\u00e4n\u00b7dern", "und", "zu\u00b7gleich", "an", "ei\u00b7ne", "Schlacht", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "KON", "ADV", "APPR", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.488": {"text": "Sie ist auch bald darauf ", "tokens": ["Sie", "ist", "auch", "bald", "da\u00b7rauf"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.489": {"text": "In welcher Ferdinand den Lorber-Krantz empfieng/", "tokens": ["In", "wel\u00b7cher", "Fer\u00b7di\u00b7nand", "den", "Lor\u00b7ber\u00b7Krantz", "emp\u00b7fi\u00b7eng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.490": {"text": "und seines Gegners Macht fast gantz zu scheitern gieng.", "tokens": ["und", "sei\u00b7nes", "Geg\u00b7ners", "Macht", "fast", "gantz", "zu", "schei\u00b7tern", "gieng", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "ADV", "ADV", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.491": {"text": "Die gantze Reuterey wurd in die Flucht getrieben/", "tokens": ["Die", "gant\u00b7ze", "Reu\u00b7te\u00b7rey", "wurd", "in", "die", "Flucht", "ge\u00b7trie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.492": {"text": "Das Fu\u00df-Volck aber wurd\u2019 erb\u00e4mlich aufgerieben.", "tokens": ["Das", "Fu\u00df\u00b7Volck", "a\u00b7ber", "wurd'", "er\u00b7b\u00e4m\u00b7lich", "auf\u00b7ge\u00b7rie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.493": {"text": "Durch Bley und Stahl erw\u00fcrgt. Das m\u00e4tzeln war so", "tokens": ["Durch", "Bley", "und", "Stahl", "er\u00b7w\u00fcrgt", ".", "Das", "m\u00e4t\u00b7zeln", "war", "so"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "$.", "ART", "NN", "VAFIN", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.494": {"text": "gro\u00df/", "tokens": ["gro\u00df", "/"], "token_info": ["word", "punct"], "pos": ["ADJD", "$("], "meter": "-", "measure": "single.down"}, "line.495": {"text": "(vorau\u00df von dieser Schar/ die aus der H\u00f6llen Scho\u00df", "tokens": ["(", "vor\u00b7au\u00df", "von", "die\u00b7ser", "Schar", "/", "die", "aus", "der", "H\u00f6l\u00b7len", "Scho\u00df"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "APPR", "PDAT", "NN", "$(", "ART", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.496": {"text": "Entsprossen ist/ ein Volck/ das sonder allen Zweifel", "tokens": ["Ent\u00b7spros\u00b7sen", "ist", "/", "ein", "Volck", "/", "das", "son\u00b7der", "al\u00b7len", "Zwei\u00b7fel"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$(", "ART", "NN", "$(", "ART", "ADJA", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.497": {"text": "Nichts anders ist und bleibt als Mensch-gestallte Teufel/", "tokens": ["Nichts", "an\u00b7ders", "ist", "und", "bleibt", "als", "Men\u00b7schge\u00b7stall\u00b7te", "Teu\u00b7fel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "VAFIN", "KON", "VVFIN", "KOKOM", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.498": {"text": "Di\u00df ", "tokens": ["Di\u00df"], "token_info": ["word"], "pos": ["PDS"], "meter": "+", "measure": "single.up"}, "line.499": {"text": "Das M\u00e4tzeln war so gro\u00df/ da\u00df K\u00f6nig Ferdinand/", "tokens": ["Das", "M\u00e4t\u00b7zeln", "war", "so", "gro\u00df", "/", "da\u00df", "K\u00f6\u00b7nig", "Fer\u00b7di\u00b7nand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$(", "KOUS", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.500": {"text": "Der fromme Ferdinand und Sieger muste sagen:", "tokens": ["Der", "from\u00b7me", "Fer\u00b7di\u00b7nand", "und", "Sie\u00b7ger", "mus\u00b7te", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.501": {"text": "H\u00f6rt auf/ es ist genug/ ihr habt genug erschlagen.", "tokens": ["H\u00f6rt", "auf", "/", "es", "ist", "ge\u00b7nug", "/", "ihr", "habt", "ge\u00b7nug", "er\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "$(", "PPER", "VAFIN", "ADV", "$(", "PPER", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.502": {"text": "Ach freylich gar genug/ ein zehen tausend Mann/", "tokens": ["Ach", "frey\u00b7lich", "gar", "ge\u00b7nug", "/", "ein", "ze\u00b7hen", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "ADV", "ADV", "$(", "ART", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.503": {"text": "Worbey der Sieges-Held sehr billich sagen kan:", "tokens": ["Wor\u00b7bey", "der", "Sie\u00b7ges\u00b7Held", "sehr", "bil\u00b7lich", "sa\u00b7gen", "kan", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.504": {"text": "Es ist genug/ h\u00f6rt auf/ die Menschen umzubringen/", "tokens": ["Es", "ist", "ge\u00b7nug", "/", "h\u00f6rt", "auf", "/", "die", "Men\u00b7schen", "um\u00b7zu\u00b7brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$(", "VVFIN", "APPR", "$(", "ART", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.505": {"text": "Worauf 5000. Mann und mehr gefangen giengen/", "tokens": ["Wo\u00b7rauf", "5000.", "Mann", "und", "mehr", "ge\u00b7fan\u00b7gen", "gien\u00b7gen", "/"], "token_info": ["word", "ordinal", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJA", "NN", "KON", "ADV", "ADJD", "VVFIN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.506": {"text": "Worbey Gustavus Horn/ Graf Kratz und Rosenstein/", "tokens": ["Wor\u00b7bey", "Gus\u00b7ta\u00b7vus", "Horn", "/", "Graf", "Kratz", "und", "Ro\u00b7sen\u00b7stein", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "$(", "NE", "NE", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.507": {"text": "Mit vielen Gro\u00dfen mehr verhafft gewesen seyn.", "tokens": ["Mit", "vie\u00b7len", "Gro\u00b7\u00dfen", "mehr", "ver\u00b7hafft", "ge\u00b7we\u00b7sen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADV", "VVPP", "VAPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.508": {"text": "Hier fielen achtzig St\u00fcck und zehen tausend Pferde", "tokens": ["Hier", "fie\u00b7len", "acht\u00b7zig", "St\u00fcck", "und", "ze\u00b7hen", "tau\u00b7send", "Pfer\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "CARD", "NN", "KON", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.509": {"text": "Den K\u00e4ysrischen anheim, Wie ich berichtet werde/", "tokens": ["Den", "K\u00e4y\u00b7sri\u00b7schen", "an\u00b7heim", ",", "Wie", "ich", "be\u00b7rich\u00b7tet", "wer\u00b7de", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$,", "PWAV", "PPER", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.510": {"text": "Hat dieser gro\u00dfe Sieg dem Sieger mehrers nicht", "tokens": ["Hat", "die\u00b7ser", "gro\u00b7\u00dfe", "Sieg", "dem", "Sie\u00b7ger", "meh\u00b7rers", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PDAT", "ADJA", "NN", "ART", "NN", "ADV", "PTKNEG"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.511": {"text": "Als funfzehn hundert Mann vom Leben hingericht.", "tokens": ["Als", "funf\u00b7zehn", "hun\u00b7dert", "Mann", "vom", "Le\u00b7ben", "hin\u00b7ge\u00b7richt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "CARD", "NN", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.512": {"text": "Was diese Schlacht f\u00fcr Noth den L\u00e4ndern hab erreget", "tokens": ["Was", "die\u00b7se", "Schlacht", "f\u00fcr", "Noth", "den", "L\u00e4n\u00b7dern", "hab", "er\u00b7re\u00b7get"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PDAT", "NN", "APPR", "NN", "ART", "NN", "VAFIN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.513": {"text": "Von Hunger/ Mord und Pest/ als es zu kommen pfleget/", "tokens": ["Von", "Hun\u00b7ger", "/", "Mord", "und", "Pest", "/", "als", "es", "zu", "kom\u00b7men", "pfle\u00b7get", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "NN", "KON", "NN", "$(", "KOUS", "PPER", "PTKZU", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.514": {"text": "Wo Mars das Zepter hat/ kan nicht beschrieben seyn/", "tokens": ["Wo", "Mars", "das", "Zep\u00b7ter", "hat", "/", "kan", "nicht", "be\u00b7schrie\u00b7ben", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ART", "NN", "VAFIN", "$(", "VMFIN", "PTKNEG", "VVPP", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.515": {"text": "und darum halt ich auch mit meiner Feder ein.", "tokens": ["und", "da\u00b7rum", "halt", "ich", "auch", "mit", "mei\u00b7ner", "Fe\u00b7der", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}