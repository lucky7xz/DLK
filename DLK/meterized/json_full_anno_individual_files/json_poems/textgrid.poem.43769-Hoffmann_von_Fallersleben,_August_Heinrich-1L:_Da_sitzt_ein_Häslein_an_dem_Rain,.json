{"textgrid.poem.43769": {"metadata": {"author": {"name": "Hoffmann von Fallersleben, August Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: Da sitzt ein H\u00e4slein an dem Rain,", "genre": "verse", "period": "N.A.", "pub_year": 1836, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Da sitzt ein H\u00e4slein an dem Rain,", "tokens": ["Da", "sitzt", "ein", "H\u00e4s\u00b7lein", "an", "dem", "Rain", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und keinem J\u00e4ger f\u00e4llt das ein,", "tokens": ["Und", "kei\u00b7nem", "J\u00e4\u00b7ger", "f\u00e4llt", "das", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PDS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sitzt und denket an sein Endchen,", "tokens": ["Sitzt", "und", "den\u00b7ket", "an", "sein", "End\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und es schreibt sein Testamentchen,", "tokens": ["Und", "es", "schreibt", "sein", "Tes\u00b7ta\u00b7ment\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Schreibt's und weint, schreibt's und weint.", "tokens": ["Schreibt's", "und", "weint", ",", "schreibt's", "und", "weint", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "KON", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.2": {"line.1": {"text": "Ich H\u00e4slein bin ein Waisenkind:", "tokens": ["Ich", "H\u00e4s\u00b7lein", "bin", "ein", "Wai\u00b7sen\u00b7kind", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was hilft's, da\u00df ich so gut gesinnt?", "tokens": ["Was", "hilft's", ",", "da\u00df", "ich", "so", "gut", "ge\u00b7sinnt", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Keinem thu' ich Leid hienieden,", "tokens": ["Kei\u00b7nem", "thu'", "ich", "Leid", "hien\u00b7ie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ach, ich lasse ja zufrieden", "tokens": ["Ach", ",", "ich", "las\u00b7se", "ja", "zu\u00b7frie\u00b7den"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "PPER", "VVFIN", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Jedermann, Jedermann.", "tokens": ["Je\u00b7der\u00b7mann", ",", "Je\u00b7der\u00b7mann", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PIS", "$,", "PIS", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.3": {"line.1": {"text": "Ich lasse Weizen Weizen sein,", "tokens": ["Ich", "las\u00b7se", "Wei\u00b7zen", "Wei\u00b7zen", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und trink' auch keinen Tropfen Wein;", "tokens": ["Und", "trink'", "auch", "kei\u00b7nen", "Trop\u00b7fen", "Wein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nur da\u00df ich mal aus dem Kohle", "tokens": ["Nur", "da\u00df", "ich", "mal", "aus", "dem", "Koh\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "ADV", "APPR", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Hie und da ein Bl\u00e4ttchen hole,", "tokens": ["Hie", "und", "da", "ein", "Bl\u00e4tt\u00b7chen", "ho\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Nicht wie's thut Ochs und Kuh.", "tokens": ["Nicht", "wie's", "thut", "Ochs", "und", "Kuh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Mein Klagen aber wenig frommt.", "tokens": ["Mein", "Kla\u00b7gen", "a\u00b7ber", "we\u00b7nig", "frommt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "O weh, der b\u00f6se J\u00e4ger kommt;", "tokens": ["O", "weh", ",", "der", "b\u00f6\u00b7se", "J\u00e4\u00b7ger", "kommt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Kaum da\u00df er mich hat gesehen,", "tokens": ["Kaum", "da\u00df", "er", "mich", "hat", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PPER", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist es schon um mich geschehen", "tokens": ["Ist", "es", "schon", "um", "mich", "ge\u00b7sche\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "PPER", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Und er schie\u00dft, piff, paff, puff.", "tokens": ["Und", "er", "schie\u00dft", ",", "piff", ",", "paff", ",", "puff", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,", "NE", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.5": {"line.1": {"text": "Gebraten werd' ich dann zuletzt", "tokens": ["Ge\u00b7bra\u00b7ten", "werd'", "ich", "dann", "zu\u00b7letzt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Tischgenossen vorgesetzt:", "tokens": ["Den", "Tischge\u00b7nos\u00b7sen", "vor\u00b7ge\u00b7setzt", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbfrisch gegessen und getrunken!", "tokens": ["\u00bb", "frisch", "ge\u00b7ges\u00b7sen", "und", "ge\u00b7trun\u00b7ken", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "VVPP", "KON", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schont nicht Lampen, den Hallunken!", "tokens": ["Schont", "nicht", "Lam\u00b7pen", ",", "den", "Hal\u00b7lun\u00b7ken", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "NN", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Schont ihn nicht! Schont ihn nicht!\u00ab", "tokens": ["Schont", "ihn", "nicht", "!", "Schont", "ihn", "nicht", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "VVFIN", "PPER", "PTKNEG", "$.", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.6": {"line.1": {"text": "Da sitzt ein H\u00e4slein an dem Rain,", "tokens": ["Da", "sitzt", "ein", "H\u00e4s\u00b7lein", "an", "dem", "Rain", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und keinem J\u00e4ger f\u00e4llt das ein,", "tokens": ["Und", "kei\u00b7nem", "J\u00e4\u00b7ger", "f\u00e4llt", "das", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PDS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sitzt und denket an sein Endchen,", "tokens": ["Sitzt", "und", "den\u00b7ket", "an", "sein", "End\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und es schreibt sein Testamentchen,", "tokens": ["Und", "es", "schreibt", "sein", "Tes\u00b7ta\u00b7ment\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Schreibt's und weint, schreibt's und weint.", "tokens": ["Schreibt's", "und", "weint", ",", "schreibt's", "und", "weint", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "KON", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.7": {"line.1": {"text": "Ich H\u00e4slein bin ein Waisenkind:", "tokens": ["Ich", "H\u00e4s\u00b7lein", "bin", "ein", "Wai\u00b7sen\u00b7kind", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was hilft's, da\u00df ich so gut gesinnt?", "tokens": ["Was", "hilft's", ",", "da\u00df", "ich", "so", "gut", "ge\u00b7sinnt", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Keinem thu' ich Leid hienieden,", "tokens": ["Kei\u00b7nem", "thu'", "ich", "Leid", "hien\u00b7ie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ach, ich lasse ja zufrieden", "tokens": ["Ach", ",", "ich", "las\u00b7se", "ja", "zu\u00b7frie\u00b7den"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "PPER", "VVFIN", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Jedermann, Jedermann.", "tokens": ["Je\u00b7der\u00b7mann", ",", "Je\u00b7der\u00b7mann", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["PIS", "$,", "PIS", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.8": {"line.1": {"text": "Ich lasse Weizen Weizen sein,", "tokens": ["Ich", "las\u00b7se", "Wei\u00b7zen", "Wei\u00b7zen", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und trink' auch keinen Tropfen Wein;", "tokens": ["Und", "trink'", "auch", "kei\u00b7nen", "Trop\u00b7fen", "Wein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nur da\u00df ich mal aus dem Kohle", "tokens": ["Nur", "da\u00df", "ich", "mal", "aus", "dem", "Koh\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "ADV", "APPR", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Hie und da ein Bl\u00e4ttchen hole,", "tokens": ["Hie", "und", "da", "ein", "Bl\u00e4tt\u00b7chen", "ho\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Nicht wie's thut Ochs und Kuh.", "tokens": ["Nicht", "wie's", "thut", "Ochs", "und", "Kuh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Mein Klagen aber wenig frommt.", "tokens": ["Mein", "Kla\u00b7gen", "a\u00b7ber", "we\u00b7nig", "frommt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "O weh, der b\u00f6se J\u00e4ger kommt;", "tokens": ["O", "weh", ",", "der", "b\u00f6\u00b7se", "J\u00e4\u00b7ger", "kommt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Kaum da\u00df er mich hat gesehen,", "tokens": ["Kaum", "da\u00df", "er", "mich", "hat", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PPER", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist es schon um mich geschehen", "tokens": ["Ist", "es", "schon", "um", "mich", "ge\u00b7sche\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "PPER", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Und er schie\u00dft, piff, paff, puff.", "tokens": ["Und", "er", "schie\u00dft", ",", "piff", ",", "paff", ",", "puff", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,", "NE", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.10": {"line.1": {"text": "Gebraten werd' ich dann zuletzt", "tokens": ["Ge\u00b7bra\u00b7ten", "werd'", "ich", "dann", "zu\u00b7letzt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Tischgenossen vorgesetzt:", "tokens": ["Den", "Tischge\u00b7nos\u00b7sen", "vor\u00b7ge\u00b7setzt", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbfrisch gegessen und getrunken!", "tokens": ["\u00bb", "frisch", "ge\u00b7ges\u00b7sen", "und", "ge\u00b7trun\u00b7ken", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "VVPP", "KON", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schont nicht Lampen, den Hallunken!", "tokens": ["Schont", "nicht", "Lam\u00b7pen", ",", "den", "Hal\u00b7lun\u00b7ken", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "NN", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Schont ihn nicht! Schont ihn nicht!\u00ab", "tokens": ["Schont", "ihn", "nicht", "!", "Schont", "ihn", "nicht", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "VVFIN", "PPER", "PTKNEG", "$.", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}}}}}