{"textgrid.poem.34986": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "3", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbmein Lehrer, mein Aristoteles,", "tokens": ["\u00bb", "mein", "Leh\u00b7rer", ",", "mein", "A\u00b7ris\u00b7to\u00b7te\u00b7les", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Der war zuerst ein Pf\u00e4ffchen", "tokens": ["Der", "war", "zu\u00b7erst", "ein", "Pf\u00e4ff\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von der franz\u00f6sischen Kolonie,", "tokens": ["Von", "der", "fran\u00b7z\u00f6\u00b7si\u00b7schen", "Ko\u00b7lo\u00b7nie", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und trug ein wei\u00dfes Beffchen.", "tokens": ["Und", "trug", "ein", "wei\u00b7\u00dfes", "Beff\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Er hat nachher als Philosoph", "tokens": ["Er", "hat", "nach\u00b7her", "als", "Phi\u00b7lo\u00b7soph"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "KOUS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vermittelt die Extreme,", "tokens": ["Ver\u00b7mit\u00b7telt", "die", "Ext\u00b7re\u00b7me", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "-+--+--", "measure": "iambic.di.relaxed"}, "line.3": {"text": "Und leider Gottes! hat er mich", "tokens": ["Und", "lei\u00b7der", "Got\u00b7tes", "!", "hat", "er", "mich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "NN", "$.", "VAFIN", "PPER", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erzogen nach seinem Systeme.", "tokens": ["Er\u00b7zo\u00b7gen", "nach", "sei\u00b7nem", "Sys\u00b7te\u00b7me", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Ich ward ein Zwitter, ein Mittelding,", "tokens": ["Ich", "ward", "ein", "Zwit\u00b7ter", ",", "ein", "Mit\u00b7tel\u00b7ding", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das weder Fleisch noch Fisch ist,", "tokens": ["Das", "we\u00b7der", "Fleisch", "noch", "Fisch", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "KON", "NN", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das von den Extremen unserer Zeit", "tokens": ["Das", "von", "den", "Ext\u00b7re\u00b7men", "un\u00b7se\u00b7rer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ein n\u00e4rrisches Gemisch ist.", "tokens": ["Ein", "n\u00e4r\u00b7ri\u00b7sches", "Ge\u00b7misch", "ist", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.4": {"line.1": {"text": "Ich bin nicht schlecht, ich bin nicht gut,", "tokens": ["Ich", "bin", "nicht", "schlecht", ",", "ich", "bin", "nicht", "gut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht dumm und nicht gescheute,", "tokens": ["Nicht", "dumm", "und", "nicht", "ge\u00b7scheu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "KON", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und wenn ich gestern vorw\u00e4rts ging,", "tokens": ["Und", "wenn", "ich", "ge\u00b7stern", "vor\u00b7w\u00e4rts", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "So geh ich r\u00fcckw\u00e4rts heute;", "tokens": ["So", "geh", "ich", "r\u00fcck\u00b7w\u00e4rts", "heu\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Ein aufgekl\u00e4rter Obskurant,", "tokens": ["Ein", "auf\u00b7ge\u00b7kl\u00e4r\u00b7ter", "Obs\u00b7ku\u00b7rant", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und weder Hengst noch Stute!", "tokens": ["Und", "we\u00b7der", "Hengst", "noch", "Stu\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KON", "NN", "ADV", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ja, ich begeistre mich zugleich", "tokens": ["Ja", ",", "ich", "be\u00b7geist\u00b7re", "mich", "zu\u00b7gleich"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr Sophokles und die Knute.", "tokens": ["F\u00fcr", "So\u00b7phok\u00b7les", "und", "die", "Knu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Herr Jesus ist meine Zuversicht,", "tokens": ["Herr", "Je\u00b7sus", "ist", "mei\u00b7ne", "Zu\u00b7ver\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Doch auch den Bacchus nehme", "tokens": ["Doch", "auch", "den", "Bac\u00b7chus", "neh\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NE", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich mir zum Tr\u00f6ster, vermittelnd stets", "tokens": ["Ich", "mir", "zum", "Tr\u00f6s\u00b7ter", ",", "ver\u00b7mit\u00b7telnd", "stets"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "PPER", "APPRART", "NN", "$,", "VVPP", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die beiden G\u00f6tterextreme.\u00ab", "tokens": ["Die", "bei\u00b7den", "G\u00f6t\u00b7ter\u00b7ext\u00b7re\u00b7me", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}}, "stanza.7": {"line.1": {"text": "\u00bbmein Lehrer, mein Aristoteles,", "tokens": ["\u00bb", "mein", "Leh\u00b7rer", ",", "mein", "A\u00b7ris\u00b7to\u00b7te\u00b7les", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Der war zuerst ein Pf\u00e4ffchen", "tokens": ["Der", "war", "zu\u00b7erst", "ein", "Pf\u00e4ff\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von der franz\u00f6sischen Kolonie,", "tokens": ["Von", "der", "fran\u00b7z\u00f6\u00b7si\u00b7schen", "Ko\u00b7lo\u00b7nie", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und trug ein wei\u00dfes Beffchen.", "tokens": ["Und", "trug", "ein", "wei\u00b7\u00dfes", "Beff\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Er hat nachher als Philosoph", "tokens": ["Er", "hat", "nach\u00b7her", "als", "Phi\u00b7lo\u00b7soph"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "KOUS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vermittelt die Extreme,", "tokens": ["Ver\u00b7mit\u00b7telt", "die", "Ext\u00b7re\u00b7me", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "-+--+--", "measure": "iambic.di.relaxed"}, "line.3": {"text": "Und leider Gottes! hat er mich", "tokens": ["Und", "lei\u00b7der", "Got\u00b7tes", "!", "hat", "er", "mich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "NN", "$.", "VAFIN", "PPER", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Erzogen nach seinem Systeme.", "tokens": ["Er\u00b7zo\u00b7gen", "nach", "sei\u00b7nem", "Sys\u00b7te\u00b7me", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Ich ward ein Zwitter, ein Mittelding,", "tokens": ["Ich", "ward", "ein", "Zwit\u00b7ter", ",", "ein", "Mit\u00b7tel\u00b7ding", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das weder Fleisch noch Fisch ist,", "tokens": ["Das", "we\u00b7der", "Fleisch", "noch", "Fisch", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "KON", "NN", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das von den Extremen unserer Zeit", "tokens": ["Das", "von", "den", "Ext\u00b7re\u00b7men", "un\u00b7se\u00b7rer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ein n\u00e4rrisches Gemisch ist.", "tokens": ["Ein", "n\u00e4r\u00b7ri\u00b7sches", "Ge\u00b7misch", "ist", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Ich bin nicht schlecht, ich bin nicht gut,", "tokens": ["Ich", "bin", "nicht", "schlecht", ",", "ich", "bin", "nicht", "gut", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht dumm und nicht gescheute,", "tokens": ["Nicht", "dumm", "und", "nicht", "ge\u00b7scheu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "KON", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und wenn ich gestern vorw\u00e4rts ging,", "tokens": ["Und", "wenn", "ich", "ge\u00b7stern", "vor\u00b7w\u00e4rts", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "So geh ich r\u00fcckw\u00e4rts heute;", "tokens": ["So", "geh", "ich", "r\u00fcck\u00b7w\u00e4rts", "heu\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Ein aufgekl\u00e4rter Obskurant,", "tokens": ["Ein", "auf\u00b7ge\u00b7kl\u00e4r\u00b7ter", "Obs\u00b7ku\u00b7rant", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und weder Hengst noch Stute!", "tokens": ["Und", "we\u00b7der", "Hengst", "noch", "Stu\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KON", "NN", "ADV", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ja, ich begeistre mich zugleich", "tokens": ["Ja", ",", "ich", "be\u00b7geist\u00b7re", "mich", "zu\u00b7gleich"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr Sophokles und die Knute.", "tokens": ["F\u00fcr", "So\u00b7phok\u00b7les", "und", "die", "Knu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Herr Jesus ist meine Zuversicht,", "tokens": ["Herr", "Je\u00b7sus", "ist", "mei\u00b7ne", "Zu\u00b7ver\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Doch auch den Bacchus nehme", "tokens": ["Doch", "auch", "den", "Bac\u00b7chus", "neh\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NE", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich mir zum Tr\u00f6ster, vermittelnd stets", "tokens": ["Ich", "mir", "zum", "Tr\u00f6s\u00b7ter", ",", "ver\u00b7mit\u00b7telnd", "stets"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "PPER", "APPRART", "NN", "$,", "VVPP", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die beiden G\u00f6tterextreme.\u00ab", "tokens": ["Die", "bei\u00b7den", "G\u00f6t\u00b7ter\u00b7ext\u00b7re\u00b7me", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}}}}}