{"textgrid.poem.34962": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.85", "nl:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Superkargo Mynheer van Koek", "tokens": ["Der", "Su\u00b7per\u00b7kar\u00b7go", "My\u00b7nheer", "van", "Ko\u00b7ek"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "NE", "NE"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sitzt rechnend in seiner Kaj\u00fcte;", "tokens": ["Sitzt", "rech\u00b7nend", "in", "sei\u00b7ner", "Ka\u00b7j\u00fc\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Er kalkuliert der Ladung Betrag", "tokens": ["Er", "kal\u00b7ku\u00b7liert", "der", "La\u00b7dung", "Be\u00b7trag"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und die probabeln Profite.", "tokens": ["Und", "die", "pro\u00b7ba\u00b7beln", "Pro\u00b7fi\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbder Gummi ist gut, der Pfeffer ist gut,", "tokens": ["\u00bb", "der", "Gum\u00b7mi", "ist", "gut", ",", "der", "Pfef\u00b7fer", "ist", "gut", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NE", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dreihundert S\u00e4cke und F\u00e4sser;", "tokens": ["Drei\u00b7hun\u00b7dert", "S\u00e4\u00b7cke", "und", "F\u00e4s\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Ich habe Goldstaub und Elfenbein \u2013", "tokens": ["Ich", "ha\u00b7be", "Gold\u00b7staub", "und", "El\u00b7fen\u00b7bein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die schwarze Ware ist besser.", "tokens": ["Die", "schwar\u00b7ze", "Wa\u00b7re", "ist", "bes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Sechshundert Neger tauschte ich ein", "tokens": ["Sechs\u00b7hun\u00b7dert", "Ne\u00b7ger", "tauschte", "ich", "ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "PPER", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Spottwohlfeil am Senegalflusse.", "tokens": ["Spott\u00b7wohl\u00b7feil", "am", "Se\u00b7ne\u00b7gal\u00b7flus\u00b7se", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Das Fleisch ist hart, die Sehnen sind stramm,", "tokens": ["Das", "Fleisch", "ist", "hart", ",", "die", "Seh\u00b7nen", "sind", "stramm", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wie Eisen vom besten Gusse.", "tokens": ["Wie", "Ei\u00b7sen", "vom", "bes\u00b7ten", "Gus\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Ich hab zum Tausche Branntewein,", "tokens": ["Ich", "hab", "zum", "Tau\u00b7sche", "Brann\u00b7te\u00b7wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Glasperlen und Stahlzeug gegeben;", "tokens": ["Glas\u00b7per\u00b7len", "und", "Stahl\u00b7zeug", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Gewinne daran achthundert Prozent,", "tokens": ["Ge\u00b7win\u00b7ne", "da\u00b7ran", "acht\u00b7hun\u00b7dert", "Pro\u00b7zent", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PAV", "CARD", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bleibt mir die H\u00e4lfte am Leben.", "tokens": ["Bleibt", "mir", "die", "H\u00e4lf\u00b7te", "am", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPRART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.5": {"line.1": {"text": "Bleiben mir Neger dreihundert nur", "tokens": ["Blei\u00b7ben", "mir", "Ne\u00b7ger", "drei\u00b7hun\u00b7dert", "nur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "NN", "VVFIN", "ADV"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Im Hafen von Rio-Janeiro,", "tokens": ["Im", "Ha\u00b7fen", "von", "Rio\u00b7Ja\u00b7nei\u00b7ro", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NE", "$,"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Zahlt dort mir hundert Dukaten per St\u00fcck", "tokens": ["Zahlt", "dort", "mir", "hun\u00b7dert", "Du\u00b7ka\u00b7ten", "per", "St\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PPER", "CARD", "NN", "APPR", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Das Haus Gonzales Perreiro.\u00ab", "tokens": ["Das", "Haus", "Gon\u00b7za\u00b7les", "Per\u00b7rei\u00b7ro", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "NE", "NE", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Da pl\u00f6tzlich wird Mynheer van Koek", "tokens": ["Da", "pl\u00f6tz\u00b7lich", "wird", "My\u00b7nheer", "van", "Ko\u00b7ek"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "NE", "NE", "NE"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Aus seinen Gedanken gerissen;", "tokens": ["Aus", "sei\u00b7nen", "Ge\u00b7dan\u00b7ken", "ge\u00b7ris\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Der Schiffschirurgius tritt herein,", "tokens": ["Der", "Schiff\u00b7schi\u00b7rur\u00b7gius", "tritt", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Doktor van der Smissen.", "tokens": ["Der", "Dok\u00b7tor", "van", "der", "Smis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Das ist eine klapperd\u00fcrre Figur,", "tokens": ["Das", "ist", "ei\u00b7ne", "klap\u00b7per\u00b7d\u00fcr\u00b7re", "Fi\u00b7gur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Nase voll roter Warzen \u2013", "tokens": ["Die", "Na\u00b7se", "voll", "ro\u00b7ter", "War\u00b7zen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "ADJA", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbnun, Wasserfeldscherer\u00ab, ruft van Koek,", "tokens": ["\u00bb", "nun", ",", "Was\u00b7ser\u00b7feld\u00b7sche\u00b7rer", "\u00ab", ",", "ruft", "van", "Ko\u00b7ek", ","], "token_info": ["punct", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NN", "$(", "$,", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "\u00bbwie geht's meinen lieben Schwarzen?\u00ab", "tokens": ["\u00bb", "wie", "geht's", "mei\u00b7nen", "lie\u00b7ben", "Schwar\u00b7zen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPOSAT", "ADJA", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Der Doktor dankt der Nachfrage und spricht:", "tokens": ["Der", "Dok\u00b7tor", "dankt", "der", "Nach\u00b7fra\u00b7ge", "und", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbich bin zu melden gekommen,", "tokens": ["\u00bb", "ich", "bin", "zu", "mel\u00b7den", "ge\u00b7kom\u00b7men", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKZU", "VVINF", "VVPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df heute nacht die Sterblichkeit", "tokens": ["Da\u00df", "heu\u00b7te", "nacht", "die", "Sterb\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bedeutend zugenommen.", "tokens": ["Be\u00b7deu\u00b7tend", "zu\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Im Durchschnitt starben t\u00e4glich zwei,", "tokens": ["Im", "Durch\u00b7schnitt", "star\u00b7ben", "t\u00e4g\u00b7lich", "zwei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ADJD", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch heute starben sieben,", "tokens": ["Doch", "heu\u00b7te", "star\u00b7ben", "sie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "CARD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vier M\u00e4nner, drei Frauen \u2013 Ich hab den Verlust", "tokens": ["Vier", "M\u00e4n\u00b7ner", ",", "drei", "Frau\u00b7en", "\u2013", "Ich", "hab", "den", "Ver\u00b7lust"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["CARD", "NN", "$,", "CARD", "NN", "$(", "PPER", "VAFIN", "ART", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Sogleich in die Kladde geschrieben.", "tokens": ["Sog\u00b7leich", "in", "die", "Klad\u00b7de", "ge\u00b7schrie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.10": {"line.1": {"text": "Ich inspizierte die Leichen genau;", "tokens": ["Ich", "ins\u00b7pi\u00b7zier\u00b7te", "die", "Lei\u00b7chen", "ge\u00b7nau", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Denn diese Schelme stellen", "tokens": ["Denn", "die\u00b7se", "Schel\u00b7me", "stel\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PDAT", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sich manchmal tot, damit man sie", "tokens": ["Sich", "manch\u00b7mal", "tot", ",", "da\u00b7mit", "man", "sie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PRF", "ADV", "ADJD", "$,", "KOUS", "PIS", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hinabwirft in die Wellen.", "tokens": ["Hin\u00b7ab\u00b7wirft", "in", "die", "Wel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Ich nahm den Toten die Eisen ab;", "tokens": ["Ich", "nahm", "den", "To\u00b7ten", "die", "Ei\u00b7sen", "ab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und wie ich gew\u00f6hnlich tue,", "tokens": ["Und", "wie", "ich", "ge\u00b7w\u00f6hn\u00b7lich", "tue", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADJD", "VVFIN", "$,"], "meter": "----+-+", "measure": "unknown.measure.di"}, "line.3": {"text": "Ich lie\u00df die Leichen werfen ins Meer", "tokens": ["Ich", "lie\u00df", "die", "Lei\u00b7chen", "wer\u00b7fen", "ins", "Meer"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Des Morgens in der Fruhe.", "tokens": ["Des", "Mor\u00b7gens", "in", "der", "Fru\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Es schossen alsbald hervor aus der Flut", "tokens": ["Es", "schos\u00b7sen", "als\u00b7bald", "her\u00b7vor", "aus", "der", "Flut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "APPR", "ART", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Haifische, ganze Heere,", "tokens": ["Hai\u00b7fi\u00b7sche", ",", "gan\u00b7ze", "Hee\u00b7re", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Sie lieben so sehr das Negerfleisch;", "tokens": ["Sie", "lie\u00b7ben", "so", "sehr", "das", "Ne\u00b7ger\u00b7fleisch", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das sind meine Pension\u00e4re.", "tokens": ["Das", "sind", "mei\u00b7ne", "Pen\u00b7si\u00b7o\u00b7n\u00e4\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.13": {"line.1": {"text": "Sie folgten unseres Schiffes Spur,", "tokens": ["Sie", "folg\u00b7ten", "un\u00b7se\u00b7res", "Schif\u00b7fes", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seit wir verlassen die K\u00fcste;", "tokens": ["Seit", "wir", "ver\u00b7las\u00b7sen", "die", "K\u00fcs\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Die Bestien wittern den Leichengeruch", "tokens": ["Die", "Be\u00b7sti\u00b7en", "wit\u00b7tern", "den", "Lei\u00b7chen\u00b7ge\u00b7ruch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit schnupperndem Fra\u00dfgel\u00fcste.", "tokens": ["Mit", "schnup\u00b7pern\u00b7dem", "Fra\u00df\u00b7ge\u00b7l\u00fcs\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.14": {"line.1": {"text": "Es ist possierlich anzusehn,", "tokens": ["Es", "ist", "pos\u00b7sier\u00b7lich", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVIZU", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sie nach den Toten schnappen!", "tokens": ["Wie", "sie", "nach", "den", "To\u00b7ten", "schnap\u00b7pen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die fa\u00dft den Kopf, die fa\u00dft das Bein,", "tokens": ["Die", "fa\u00dft", "den", "Kopf", ",", "die", "fa\u00dft", "das", "Bein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$,", "PRELS", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die andern schlucken die Lappen.", "tokens": ["Die", "an\u00b7dern", "schlu\u00b7cken", "die", "Lap\u00b7pen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Ist alles verschlungen, dann tummeln sie sich", "tokens": ["Ist", "al\u00b7les", "ver\u00b7schlun\u00b7gen", ",", "dann", "tum\u00b7meln", "sie", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "VVINF", "$,", "ADV", "VVFIN", "PPER", "PRF"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Vergn\u00fcgt um des Schiffes Planken", "tokens": ["Ver\u00b7gn\u00fcgt", "um", "des", "Schif\u00b7fes", "Plan\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und glotzen mich an, als wollten sie", "tokens": ["Und", "glot\u00b7zen", "mich", "an", ",", "als", "woll\u00b7ten", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "VMFIN", "PPER"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sich f\u00fcr das Fr\u00fchst\u00fcck bedanken.\u00ab", "tokens": ["Sich", "f\u00fcr", "das", "Fr\u00fch\u00b7st\u00fcck", "be\u00b7dan\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.16": {"line.1": {"text": "Doch seufzend f\u00e4llt ihm in die Red'", "tokens": ["Doch", "seuf\u00b7zend", "f\u00e4llt", "ihm", "in", "die", "Red'"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Van Koek: \u00bbWie kann ich lindern", "tokens": ["Van", "Ko\u00b7ek", ":", "\u00bb", "Wie", "kann", "ich", "lin\u00b7dern"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "$.", "$(", "PWAV", "VMFIN", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das \u00dcbel? wie kann ich die Progression", "tokens": ["Das", "\u00dc\u00b7bel", "?", "wie", "kann", "ich", "die", "Pro\u00b7gres\u00b7si\u00b7on"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$.", "PWAV", "VMFIN", "PPER", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Der Sterblichkeit verhindern?\u00ab", "tokens": ["Der", "Sterb\u00b7lich\u00b7keit", "ver\u00b7hin\u00b7dern", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Der Doktor erwidert: \u00bbDurch eigne Schuld", "tokens": ["Der", "Dok\u00b7tor", "er\u00b7wi\u00b7dert", ":", "\u00bb", "Durch", "eig\u00b7ne", "Schuld"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "$.", "$(", "APPR", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sind viele Schwarze gestorben;", "tokens": ["Sind", "vie\u00b7le", "Schwar\u00b7ze", "ge\u00b7stor\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ihr schlechter Odem hat die Luft", "tokens": ["Ihr", "schlech\u00b7ter", "O\u00b7dem", "hat", "die", "Luft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Im Schiffsraum so sehr verdorben.", "tokens": ["Im", "Schiffs\u00b7raum", "so", "sehr", "ver\u00b7dor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Auch starben viele durch Melancholie,", "tokens": ["Auch", "star\u00b7ben", "vie\u00b7le", "durch", "Me\u00b7lan\u00b7cho\u00b7lie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dieweil sie sich t\u00f6dlich langweilen;", "tokens": ["Die\u00b7weil", "sie", "sich", "t\u00f6d\u00b7lich", "lang\u00b7wei\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Durch etwas Luft, Musik und Tanz", "tokens": ["Durch", "et\u00b7was", "Luft", ",", "Mu\u00b7sik", "und", "Tanz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "$,", "NN", "KON", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "L\u00e4\u00dft sich die Krankheit heilen.\u00ab", "tokens": ["L\u00e4\u00dft", "sich", "die", "Krank\u00b7heit", "hei\u00b7len", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Da ruft van Koek: \u00bbEin guter Rat!", "tokens": ["Da", "ruft", "van", "Ko\u00b7ek", ":", "\u00bb", "Ein", "gu\u00b7ter", "Rat", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "NE", "$.", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mein teurer Wasserfeldscherer", "tokens": ["Mein", "teu\u00b7rer", "Was\u00b7ser\u00b7feld\u00b7sche\u00b7rer"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Ist klug wie Aristoteles,", "tokens": ["Ist", "klug", "wie", "A\u00b7ris\u00b7to\u00b7te\u00b7les", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "KOKOM", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Des Alexanders Lehrer.", "tokens": ["Des", "A\u00b7lex\u00b7an\u00b7ders", "Leh\u00b7rer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NE", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Der Pr\u00e4sident der Soziet\u00e4t", "tokens": ["Der", "Pr\u00e4\u00b7si\u00b7dent", "der", "So\u00b7zie\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tulpenveredlung im Delfte", "tokens": ["Der", "Tul\u00b7pen\u00b7ve\u00b7red\u00b7lung", "im", "Delf\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ist sehr gescheit, doch hat er nicht", "tokens": ["Ist", "sehr", "ge\u00b7scheit", ",", "doch", "hat", "er", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADJD", "$,", "ADV", "VAFIN", "PPER", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von Eurem Verstande die H\u00e4lfte.", "tokens": ["Von", "Eu\u00b7rem", "Ver\u00b7stan\u00b7de", "die", "H\u00e4lf\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.21": {"line.1": {"text": "Musik! Musik! Die Schwarzen soll'n", "tokens": ["Mu\u00b7sik", "!", "Mu\u00b7sik", "!", "Die", "Schwar\u00b7zen", "soll'n"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NN", "$.", "ART", "NN", "VMFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hier auf dem Verdecke tanzen.", "tokens": ["Hier", "auf", "dem", "Ver\u00b7de\u00b7cke", "tan\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Und wer sich beim Hopsen nicht am\u00fcsiert,", "tokens": ["Und", "wer", "sich", "beim", "Hop\u00b7sen", "nicht", "a\u00b7m\u00fc\u00b7siert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PRF", "APPRART", "NN", "PTKNEG", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Den soll die Peitsche kuranzen.\u00ab", "tokens": ["Den", "soll", "die", "Peit\u00b7sche", "ku\u00b7ran\u00b7zen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.22": {"line.1": {"text": "Der Superkargo Mynheer van Koek", "tokens": ["Der", "Su\u00b7per\u00b7kar\u00b7go", "My\u00b7nheer", "van", "Ko\u00b7ek"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "NE", "NE"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sitzt rechnend in seiner Kaj\u00fcte;", "tokens": ["Sitzt", "rech\u00b7nend", "in", "sei\u00b7ner", "Ka\u00b7j\u00fc\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Er kalkuliert der Ladung Betrag", "tokens": ["Er", "kal\u00b7ku\u00b7liert", "der", "La\u00b7dung", "Be\u00b7trag"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und die probabeln Profite.", "tokens": ["Und", "die", "pro\u00b7ba\u00b7beln", "Pro\u00b7fi\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "\u00bbder Gummi ist gut, der Pfeffer ist gut,", "tokens": ["\u00bb", "der", "Gum\u00b7mi", "ist", "gut", ",", "der", "Pfef\u00b7fer", "ist", "gut", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NE", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dreihundert S\u00e4cke und F\u00e4sser;", "tokens": ["Drei\u00b7hun\u00b7dert", "S\u00e4\u00b7cke", "und", "F\u00e4s\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Ich habe Goldstaub und Elfenbein \u2013", "tokens": ["Ich", "ha\u00b7be", "Gold\u00b7staub", "und", "El\u00b7fen\u00b7bein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die schwarze Ware ist besser.", "tokens": ["Die", "schwar\u00b7ze", "Wa\u00b7re", "ist", "bes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "Sechshundert Neger tauschte ich ein", "tokens": ["Sechs\u00b7hun\u00b7dert", "Ne\u00b7ger", "tauschte", "ich", "ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "PPER", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Spottwohlfeil am Senegalflusse.", "tokens": ["Spott\u00b7wohl\u00b7feil", "am", "Se\u00b7ne\u00b7gal\u00b7flus\u00b7se", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Das Fleisch ist hart, die Sehnen sind stramm,", "tokens": ["Das", "Fleisch", "ist", "hart", ",", "die", "Seh\u00b7nen", "sind", "stramm", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wie Eisen vom besten Gusse.", "tokens": ["Wie", "Ei\u00b7sen", "vom", "bes\u00b7ten", "Gus\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.25": {"line.1": {"text": "Ich hab zum Tausche Branntewein,", "tokens": ["Ich", "hab", "zum", "Tau\u00b7sche", "Brann\u00b7te\u00b7wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Glasperlen und Stahlzeug gegeben;", "tokens": ["Glas\u00b7per\u00b7len", "und", "Stahl\u00b7zeug", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Gewinne daran achthundert Prozent,", "tokens": ["Ge\u00b7win\u00b7ne", "da\u00b7ran", "acht\u00b7hun\u00b7dert", "Pro\u00b7zent", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PAV", "CARD", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bleibt mir die H\u00e4lfte am Leben.", "tokens": ["Bleibt", "mir", "die", "H\u00e4lf\u00b7te", "am", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPRART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.26": {"line.1": {"text": "Bleiben mir Neger dreihundert nur", "tokens": ["Blei\u00b7ben", "mir", "Ne\u00b7ger", "drei\u00b7hun\u00b7dert", "nur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "NN", "VVFIN", "ADV"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Im Hafen von Rio-Janeiro,", "tokens": ["Im", "Ha\u00b7fen", "von", "Rio\u00b7Ja\u00b7nei\u00b7ro", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NE", "$,"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Zahlt dort mir hundert Dukaten per St\u00fcck", "tokens": ["Zahlt", "dort", "mir", "hun\u00b7dert", "Du\u00b7ka\u00b7ten", "per", "St\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PPER", "CARD", "NN", "APPR", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Das Haus Gonzales Perreiro.\u00ab", "tokens": ["Das", "Haus", "Gon\u00b7za\u00b7les", "Per\u00b7rei\u00b7ro", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "NE", "NE", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Da pl\u00f6tzlich wird Mynheer van Koek", "tokens": ["Da", "pl\u00f6tz\u00b7lich", "wird", "My\u00b7nheer", "van", "Ko\u00b7ek"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "NE", "NE", "NE"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Aus seinen Gedanken gerissen;", "tokens": ["Aus", "sei\u00b7nen", "Ge\u00b7dan\u00b7ken", "ge\u00b7ris\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Der Schiffschirurgius tritt herein,", "tokens": ["Der", "Schiff\u00b7schi\u00b7rur\u00b7gius", "tritt", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Doktor van der Smissen.", "tokens": ["Der", "Dok\u00b7tor", "van", "der", "Smis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.28": {"line.1": {"text": "Das ist eine klapperd\u00fcrre Figur,", "tokens": ["Das", "ist", "ei\u00b7ne", "klap\u00b7per\u00b7d\u00fcr\u00b7re", "Fi\u00b7gur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Nase voll roter Warzen \u2013", "tokens": ["Die", "Na\u00b7se", "voll", "ro\u00b7ter", "War\u00b7zen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "ADJA", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbnun, Wasserfeldscherer\u00ab, ruft van Koek,", "tokens": ["\u00bb", "nun", ",", "Was\u00b7ser\u00b7feld\u00b7sche\u00b7rer", "\u00ab", ",", "ruft", "van", "Ko\u00b7ek", ","], "token_info": ["punct", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NN", "$(", "$,", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "\u00bbwie geht's meinen lieben Schwarzen?\u00ab", "tokens": ["\u00bb", "wie", "geht's", "mei\u00b7nen", "lie\u00b7ben", "Schwar\u00b7zen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPOSAT", "ADJA", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Der Doktor dankt der Nachfrage und spricht:", "tokens": ["Der", "Dok\u00b7tor", "dankt", "der", "Nach\u00b7fra\u00b7ge", "und", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbich bin zu melden gekommen,", "tokens": ["\u00bb", "ich", "bin", "zu", "mel\u00b7den", "ge\u00b7kom\u00b7men", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PTKZU", "VVINF", "VVPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df heute nacht die Sterblichkeit", "tokens": ["Da\u00df", "heu\u00b7te", "nacht", "die", "Sterb\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bedeutend zugenommen.", "tokens": ["Be\u00b7deu\u00b7tend", "zu\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.30": {"line.1": {"text": "Im Durchschnitt starben t\u00e4glich zwei,", "tokens": ["Im", "Durch\u00b7schnitt", "star\u00b7ben", "t\u00e4g\u00b7lich", "zwei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ADJD", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch heute starben sieben,", "tokens": ["Doch", "heu\u00b7te", "star\u00b7ben", "sie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "CARD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vier M\u00e4nner, drei Frauen \u2013 Ich hab den Verlust", "tokens": ["Vier", "M\u00e4n\u00b7ner", ",", "drei", "Frau\u00b7en", "\u2013", "Ich", "hab", "den", "Ver\u00b7lust"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["CARD", "NN", "$,", "CARD", "NN", "$(", "PPER", "VAFIN", "ART", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Sogleich in die Kladde geschrieben.", "tokens": ["Sog\u00b7leich", "in", "die", "Klad\u00b7de", "ge\u00b7schrie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.31": {"line.1": {"text": "Ich inspizierte die Leichen genau;", "tokens": ["Ich", "ins\u00b7pi\u00b7zier\u00b7te", "die", "Lei\u00b7chen", "ge\u00b7nau", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Denn diese Schelme stellen", "tokens": ["Denn", "die\u00b7se", "Schel\u00b7me", "stel\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PDAT", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sich manchmal tot, damit man sie", "tokens": ["Sich", "manch\u00b7mal", "tot", ",", "da\u00b7mit", "man", "sie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PRF", "ADV", "ADJD", "$,", "KOUS", "PIS", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hinabwirft in die Wellen.", "tokens": ["Hin\u00b7ab\u00b7wirft", "in", "die", "Wel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.32": {"line.1": {"text": "Ich nahm den Toten die Eisen ab;", "tokens": ["Ich", "nahm", "den", "To\u00b7ten", "die", "Ei\u00b7sen", "ab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und wie ich gew\u00f6hnlich tue,", "tokens": ["Und", "wie", "ich", "ge\u00b7w\u00f6hn\u00b7lich", "tue", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADJD", "VVFIN", "$,"], "meter": "----+-+", "measure": "unknown.measure.di"}, "line.3": {"text": "Ich lie\u00df die Leichen werfen ins Meer", "tokens": ["Ich", "lie\u00df", "die", "Lei\u00b7chen", "wer\u00b7fen", "ins", "Meer"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Des Morgens in der Fruhe.", "tokens": ["Des", "Mor\u00b7gens", "in", "der", "Fru\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "Es schossen alsbald hervor aus der Flut", "tokens": ["Es", "schos\u00b7sen", "als\u00b7bald", "her\u00b7vor", "aus", "der", "Flut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "APPR", "ART", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Haifische, ganze Heere,", "tokens": ["Hai\u00b7fi\u00b7sche", ",", "gan\u00b7ze", "Hee\u00b7re", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Sie lieben so sehr das Negerfleisch;", "tokens": ["Sie", "lie\u00b7ben", "so", "sehr", "das", "Ne\u00b7ger\u00b7fleisch", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das sind meine Pension\u00e4re.", "tokens": ["Das", "sind", "mei\u00b7ne", "Pen\u00b7si\u00b7o\u00b7n\u00e4\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.34": {"line.1": {"text": "Sie folgten unseres Schiffes Spur,", "tokens": ["Sie", "folg\u00b7ten", "un\u00b7se\u00b7res", "Schif\u00b7fes", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seit wir verlassen die K\u00fcste;", "tokens": ["Seit", "wir", "ver\u00b7las\u00b7sen", "die", "K\u00fcs\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Die Bestien wittern den Leichengeruch", "tokens": ["Die", "Be\u00b7sti\u00b7en", "wit\u00b7tern", "den", "Lei\u00b7chen\u00b7ge\u00b7ruch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit schnupperndem Fra\u00dfgel\u00fcste.", "tokens": ["Mit", "schnup\u00b7pern\u00b7dem", "Fra\u00df\u00b7ge\u00b7l\u00fcs\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.35": {"line.1": {"text": "Es ist possierlich anzusehn,", "tokens": ["Es", "ist", "pos\u00b7sier\u00b7lich", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVIZU", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sie nach den Toten schnappen!", "tokens": ["Wie", "sie", "nach", "den", "To\u00b7ten", "schnap\u00b7pen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die fa\u00dft den Kopf, die fa\u00dft das Bein,", "tokens": ["Die", "fa\u00dft", "den", "Kopf", ",", "die", "fa\u00dft", "das", "Bein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$,", "PRELS", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die andern schlucken die Lappen.", "tokens": ["Die", "an\u00b7dern", "schlu\u00b7cken", "die", "Lap\u00b7pen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.36": {"line.1": {"text": "Ist alles verschlungen, dann tummeln sie sich", "tokens": ["Ist", "al\u00b7les", "ver\u00b7schlun\u00b7gen", ",", "dann", "tum\u00b7meln", "sie", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "VVINF", "$,", "ADV", "VVFIN", "PPER", "PRF"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Vergn\u00fcgt um des Schiffes Planken", "tokens": ["Ver\u00b7gn\u00fcgt", "um", "des", "Schif\u00b7fes", "Plan\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und glotzen mich an, als wollten sie", "tokens": ["Und", "glot\u00b7zen", "mich", "an", ",", "als", "woll\u00b7ten", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "VMFIN", "PPER"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sich f\u00fcr das Fr\u00fchst\u00fcck bedanken.\u00ab", "tokens": ["Sich", "f\u00fcr", "das", "Fr\u00fch\u00b7st\u00fcck", "be\u00b7dan\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.37": {"line.1": {"text": "Doch seufzend f\u00e4llt ihm in die Red'", "tokens": ["Doch", "seuf\u00b7zend", "f\u00e4llt", "ihm", "in", "die", "Red'"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Van Koek: \u00bbWie kann ich lindern", "tokens": ["Van", "Ko\u00b7ek", ":", "\u00bb", "Wie", "kann", "ich", "lin\u00b7dern"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "$.", "$(", "PWAV", "VMFIN", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das \u00dcbel? wie kann ich die Progression", "tokens": ["Das", "\u00dc\u00b7bel", "?", "wie", "kann", "ich", "die", "Pro\u00b7gres\u00b7si\u00b7on"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$.", "PWAV", "VMFIN", "PPER", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Der Sterblichkeit verhindern?\u00ab", "tokens": ["Der", "Sterb\u00b7lich\u00b7keit", "ver\u00b7hin\u00b7dern", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.38": {"line.1": {"text": "Der Doktor erwidert: \u00bbDurch eigne Schuld", "tokens": ["Der", "Dok\u00b7tor", "er\u00b7wi\u00b7dert", ":", "\u00bb", "Durch", "eig\u00b7ne", "Schuld"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "$.", "$(", "APPR", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sind viele Schwarze gestorben;", "tokens": ["Sind", "vie\u00b7le", "Schwar\u00b7ze", "ge\u00b7stor\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ihr schlechter Odem hat die Luft", "tokens": ["Ihr", "schlech\u00b7ter", "O\u00b7dem", "hat", "die", "Luft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Im Schiffsraum so sehr verdorben.", "tokens": ["Im", "Schiffs\u00b7raum", "so", "sehr", "ver\u00b7dor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.39": {"line.1": {"text": "Auch starben viele durch Melancholie,", "tokens": ["Auch", "star\u00b7ben", "vie\u00b7le", "durch", "Me\u00b7lan\u00b7cho\u00b7lie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dieweil sie sich t\u00f6dlich langweilen;", "tokens": ["Die\u00b7weil", "sie", "sich", "t\u00f6d\u00b7lich", "lang\u00b7wei\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Durch etwas Luft, Musik und Tanz", "tokens": ["Durch", "et\u00b7was", "Luft", ",", "Mu\u00b7sik", "und", "Tanz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "$,", "NN", "KON", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "L\u00e4\u00dft sich die Krankheit heilen.\u00ab", "tokens": ["L\u00e4\u00dft", "sich", "die", "Krank\u00b7heit", "hei\u00b7len", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.40": {"line.1": {"text": "Da ruft van Koek: \u00bbEin guter Rat!", "tokens": ["Da", "ruft", "van", "Ko\u00b7ek", ":", "\u00bb", "Ein", "gu\u00b7ter", "Rat", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "NE", "$.", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mein teurer Wasserfeldscherer", "tokens": ["Mein", "teu\u00b7rer", "Was\u00b7ser\u00b7feld\u00b7sche\u00b7rer"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Ist klug wie Aristoteles,", "tokens": ["Ist", "klug", "wie", "A\u00b7ris\u00b7to\u00b7te\u00b7les", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "KOKOM", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Des Alexanders Lehrer.", "tokens": ["Des", "A\u00b7lex\u00b7an\u00b7ders", "Leh\u00b7rer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NE", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.41": {"line.1": {"text": "Der Pr\u00e4sident der Soziet\u00e4t", "tokens": ["Der", "Pr\u00e4\u00b7si\u00b7dent", "der", "So\u00b7zie\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tulpenveredlung im Delfte", "tokens": ["Der", "Tul\u00b7pen\u00b7ve\u00b7red\u00b7lung", "im", "Delf\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ist sehr gescheit, doch hat er nicht", "tokens": ["Ist", "sehr", "ge\u00b7scheit", ",", "doch", "hat", "er", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADJD", "$,", "ADV", "VAFIN", "PPER", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von Eurem Verstande die H\u00e4lfte.", "tokens": ["Von", "Eu\u00b7rem", "Ver\u00b7stan\u00b7de", "die", "H\u00e4lf\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.42": {"line.1": {"text": "Musik! Musik! Die Schwarzen soll'n", "tokens": ["Mu\u00b7sik", "!", "Mu\u00b7sik", "!", "Die", "Schwar\u00b7zen", "soll'n"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NN", "$.", "ART", "NN", "VMFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hier auf dem Verdecke tanzen.", "tokens": ["Hier", "auf", "dem", "Ver\u00b7de\u00b7cke", "tan\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Und wer sich beim Hopsen nicht am\u00fcsiert,", "tokens": ["Und", "wer", "sich", "beim", "Hop\u00b7sen", "nicht", "a\u00b7m\u00fc\u00b7siert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PRF", "APPRART", "NN", "PTKNEG", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Den soll die Peitsche kuranzen.\u00ab", "tokens": ["Den", "soll", "die", "Peit\u00b7sche", "ku\u00b7ran\u00b7zen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}}}}}