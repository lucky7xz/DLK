{"textgrid.poem.65377": {"metadata": {"author": {"name": "M\u00fcller, Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "Der Seehund", "genre": "verse", "period": "N.A.", "pub_year": 1810, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn uns ein Seehund die Aale zerbissen,", "tokens": ["Wenn", "uns", "ein", "See\u00b7hund", "die", "Aa\u00b7le", "zer\u00b7bis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn er die Netz' uns in St\u00fccke gerissen,", "tokens": ["Wenn", "er", "die", "Netz'", "uns", "in", "St\u00fc\u00b7cke", "ge\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "---+--+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Rotten wir all' uns zusammen zur Jagd \u2013", "tokens": ["Rot\u00b7ten", "wir", "all'", "uns", "zu\u00b7sam\u00b7men", "zur", "Jagd", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PIS", "PPER", "ADV", "APPRART", "NN", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Seehund, du R\u00e4uber, jetzt nimm dich in Acht!", "tokens": ["See\u00b7hund", ",", "du", "R\u00e4u\u00b7ber", ",", "jetzt", "nimm", "dich", "in", "Acht", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "NN", "$,", "ADV", "VVIMP", "PPER", "APPR", "CARD", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.2": {"line.1": {"text": "Ach, und wer hat uns die Herzen zerrissen?", "tokens": ["Ach", ",", "und", "wer", "hat", "uns", "die", "Her\u00b7zen", "zer\u00b7ris\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "KON", "PWS", "VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Ach, und wer hat uns die Freuden zerbissen?", "tokens": ["Ach", ",", "und", "wer", "hat", "uns", "die", "Freu\u00b7den", "zer\u00b7bis\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "KON", "PWS", "VAFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Ob wir sie kennen? Wer kennte sie nicht?", "tokens": ["Ob", "wir", "sie", "ken\u00b7nen", "?", "Wer", "kenn\u00b7te", "sie", "nicht", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVINF", "$.", "PWS", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Br\u00fcder, wann halten mit der wir Gericht?", "tokens": ["Br\u00fc\u00b7der", ",", "wann", "hal\u00b7ten", "mit", "der", "wir", "Ge\u00b7richt", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWAV", "VVFIN", "APPR", "PRELS", "PPER", "NN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.3": {"line.1": {"text": "Seht doch, da k\u00f6mmt sie ja selber gegangen:", "tokens": ["Seht", "doch", ",", "da", "k\u00f6mmt", "sie", "ja", "sel\u00b7ber", "ge\u00b7gan\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "ADV", "VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "K\u00f6nnten sie halten und k\u00f6nnten sie fangen.", "tokens": ["K\u00f6nn\u00b7ten", "sie", "hal\u00b7ten", "und", "k\u00f6nn\u00b7ten", "sie", "fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "VVINF", "KON", "VMFIN", "PPER", "VVFIN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.3": {"text": "L\u00e4uft in die Fall' uns die R\u00e4uberin hier,", "tokens": ["L\u00e4uft", "in", "die", "Fall'", "uns", "die", "R\u00e4u\u00b7be\u00b7rin", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "PPER", "ART", "NN", "ADV", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Br\u00fcder, was machen wir jetzo mit ihr?", "tokens": ["Br\u00fc\u00b7der", ",", "was", "ma\u00b7chen", "wir", "jet\u00b7zo", "mit", "ihr", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWS", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.4": {"line.1": {"text": "Machen ihr Platz unter Neigen und Nicken,", "tokens": ["Ma\u00b7chen", "ihr", "Platz", "un\u00b7ter", "Nei\u00b7gen", "und", "Ni\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Schleichen ihr nach mit sch\u00fcchternen Blicken,", "tokens": ["Schlei\u00b7chen", "ihr", "nach", "mit", "sch\u00fcch\u00b7ter\u00b7nen", "Bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "APPR", "ADJA", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Gucken uns an und sagen geschwind:", "tokens": ["Gu\u00b7cken", "uns", "an", "und", "sa\u00b7gen", "ge\u00b7schwind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "KON", "VVFIN", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "'s ist doch ein liebes, ein herziges Kind!", "tokens": ["'s", "ist", "doch", "ein", "lie\u00b7bes", ",", "ein", "her\u00b7zi\u00b7ges", "Kind", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "$,", "ART", "ADJA", "NN", "$."], "meter": "+---+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Wenn uns ein Seehund die Aale zerbissen,", "tokens": ["Wenn", "uns", "ein", "See\u00b7hund", "die", "Aa\u00b7le", "zer\u00b7bis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn er die Netz' uns in St\u00fccke gerissen,", "tokens": ["Wenn", "er", "die", "Netz'", "uns", "in", "St\u00fc\u00b7cke", "ge\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "---+--+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Rotten wir all' uns zusammen zur Jagd \u2013", "tokens": ["Rot\u00b7ten", "wir", "all'", "uns", "zu\u00b7sam\u00b7men", "zur", "Jagd", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PIS", "PPER", "ADV", "APPRART", "NN", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Seehund, du R\u00e4uber, jetzt nimm dich in Acht!", "tokens": ["See\u00b7hund", ",", "du", "R\u00e4u\u00b7ber", ",", "jetzt", "nimm", "dich", "in", "Acht", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "NN", "$,", "ADV", "VVIMP", "PPER", "APPR", "CARD", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.6": {"line.1": {"text": "Ach, und wer hat uns die Herzen zerrissen?", "tokens": ["Ach", ",", "und", "wer", "hat", "uns", "die", "Her\u00b7zen", "zer\u00b7ris\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "KON", "PWS", "VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Ach, und wer hat uns die Freuden zerbissen?", "tokens": ["Ach", ",", "und", "wer", "hat", "uns", "die", "Freu\u00b7den", "zer\u00b7bis\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "KON", "PWS", "VAFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Ob wir sie kennen? Wer kennte sie nicht?", "tokens": ["Ob", "wir", "sie", "ken\u00b7nen", "?", "Wer", "kenn\u00b7te", "sie", "nicht", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVINF", "$.", "PWS", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Br\u00fcder, wann halten mit der wir Gericht?", "tokens": ["Br\u00fc\u00b7der", ",", "wann", "hal\u00b7ten", "mit", "der", "wir", "Ge\u00b7richt", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWAV", "VVFIN", "APPR", "PRELS", "PPER", "NN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.7": {"line.1": {"text": "Seht doch, da k\u00f6mmt sie ja selber gegangen:", "tokens": ["Seht", "doch", ",", "da", "k\u00f6mmt", "sie", "ja", "sel\u00b7ber", "ge\u00b7gan\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "ADV", "VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "K\u00f6nnten sie halten und k\u00f6nnten sie fangen.", "tokens": ["K\u00f6nn\u00b7ten", "sie", "hal\u00b7ten", "und", "k\u00f6nn\u00b7ten", "sie", "fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "VVINF", "KON", "VMFIN", "PPER", "VVFIN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.3": {"text": "L\u00e4uft in die Fall' uns die R\u00e4uberin hier,", "tokens": ["L\u00e4uft", "in", "die", "Fall'", "uns", "die", "R\u00e4u\u00b7be\u00b7rin", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "PPER", "ART", "NN", "ADV", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Br\u00fcder, was machen wir jetzo mit ihr?", "tokens": ["Br\u00fc\u00b7der", ",", "was", "ma\u00b7chen", "wir", "jet\u00b7zo", "mit", "ihr", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWS", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.8": {"line.1": {"text": "Machen ihr Platz unter Neigen und Nicken,", "tokens": ["Ma\u00b7chen", "ihr", "Platz", "un\u00b7ter", "Nei\u00b7gen", "und", "Ni\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Schleichen ihr nach mit sch\u00fcchternen Blicken,", "tokens": ["Schlei\u00b7chen", "ihr", "nach", "mit", "sch\u00fcch\u00b7ter\u00b7nen", "Bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "APPR", "ADJA", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Gucken uns an und sagen geschwind:", "tokens": ["Gu\u00b7cken", "uns", "an", "und", "sa\u00b7gen", "ge\u00b7schwind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "KON", "VVFIN", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "'s ist doch ein liebes, ein herziges Kind!", "tokens": ["'s", "ist", "doch", "ein", "lie\u00b7bes", ",", "ein", "her\u00b7zi\u00b7ges", "Kind", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "$,", "ART", "ADJA", "NN", "$."], "meter": "+---+--+--+", "measure": "trochaic.tetra.relaxed"}}}}}