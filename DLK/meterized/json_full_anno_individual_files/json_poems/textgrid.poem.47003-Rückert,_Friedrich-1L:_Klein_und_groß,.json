{"textgrid.poem.47003": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: Klein und gro\u00df,", "genre": "verse", "period": "N.A.", "pub_year": 1827, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Klein und gro\u00df,", "tokens": ["Klein", "und", "gro\u00df", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Gro\u00df und klein,", "tokens": ["Gro\u00df", "und", "klein", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Soll hier blo\u00df", "tokens": ["Soll", "hier", "blo\u00df"], "token_info": ["word", "word", "word"], "pos": ["VMFIN", "ADV", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Eines sein.", "tokens": ["Ei\u00b7nes", "sein", "."], "token_info": ["word", "word", "punct"], "pos": ["PIS", "VAINF", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Gro\u00df- und Klein-", "tokens": ["Gro\u00df", "und", "Klein"], "token_info": ["word", "word", "word"], "pos": ["TRUNC", "KON", "TRUNC"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Deutsch, herbei,", "tokens": ["Deutsch", ",", "her\u00b7bei", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "PTKVZ", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Da\u00df hier ein", "tokens": ["Da\u00df", "hier", "ein"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "ADV", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Deutschland sei.", "tokens": ["Deutschland", "sei", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "VAFIN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.3": {"line.1": {"text": "Einig so,", "tokens": ["Ei\u00b7nig", "so", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Eins im Feld,", "tokens": ["Eins", "im", "Feld", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Stehn wir froh", "tokens": ["Stehn", "wir", "froh"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Einer Welt.", "tokens": ["Ei\u00b7ner", "Welt", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Klein und gro\u00df,", "tokens": ["Klein", "und", "gro\u00df", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Gro\u00df und klein,", "tokens": ["Gro\u00df", "und", "klein", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Soll hier blo\u00df", "tokens": ["Soll", "hier", "blo\u00df"], "token_info": ["word", "word", "word"], "pos": ["VMFIN", "ADV", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Eines sein.", "tokens": ["Ei\u00b7nes", "sein", "."], "token_info": ["word", "word", "punct"], "pos": ["PIS", "VAINF", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Gro\u00df- und Klein-", "tokens": ["Gro\u00df", "und", "Klein"], "token_info": ["word", "word", "word"], "pos": ["TRUNC", "KON", "TRUNC"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Deutsch, herbei,", "tokens": ["Deutsch", ",", "her\u00b7bei", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "PTKVZ", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Da\u00df hier ein", "tokens": ["Da\u00df", "hier", "ein"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "ADV", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Deutschland sei.", "tokens": ["Deutschland", "sei", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "VAFIN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.6": {"line.1": {"text": "Einig so,", "tokens": ["Ei\u00b7nig", "so", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Eins im Feld,", "tokens": ["Eins", "im", "Feld", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Stehn wir froh", "tokens": ["Stehn", "wir", "froh"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPER", "ADJD"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Einer Welt.", "tokens": ["Ei\u00b7ner", "Welt", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}