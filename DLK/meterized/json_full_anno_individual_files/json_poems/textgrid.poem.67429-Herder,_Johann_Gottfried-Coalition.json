{"textgrid.poem.67429": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "Coalition", "genre": "verse", "period": "N.A.", "pub_year": 1792, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbpolitisch Lied, ein b\u00f6ses, b\u00f6ses Lied!\u00ab", "tokens": ["\u00bb", "po\u00b7li\u00b7tisch", "Lied", ",", "ein", "b\u00f6\u00b7ses", ",", "b\u00f6\u00b7ses", "Lied", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "NN", "$,", "ART", "ADJA", "$,", "ADJA", "NN", "$.", "$("], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "So sagt das Sprichwort; und Du willst, o Freund,", "tokens": ["So", "sagt", "das", "Sprich\u00b7wort", ";", "und", "Du", "willst", ",", "o", "Freund", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "KON", "PPER", "VMFIN", "$,", "FM", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df dichtend unsre Nation sogar", "tokens": ["Da\u00df", "dich\u00b7tend", "uns\u00b7re", "Na\u00b7tion", "so\u00b7gar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Politisire? H\u00f6r ein M\u00e4rchen an,", "tokens": ["Po\u00b7li\u00b7ti\u00b7si\u00b7re", "?", "H\u00f6r", "ein", "M\u00e4r\u00b7chen", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NE", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Was ein politisch Wort, ein blo\u00dfes Wort,", "tokens": ["Was", "ein", "po\u00b7li\u00b7tisch", "Wort", ",", "ein", "blo\u00b7\u00dfes", "Wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "F\u00fcr mancherlei Besinnung dem Gem\u00fcth", "tokens": ["F\u00fcr", "man\u00b7cher\u00b7lei", "Be\u00b7sin\u00b7nung", "dem", "Ge\u00b7m\u00fcth"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Nur ", "tokens": ["Nur"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Bekannt: man wiegte vor nicht langer Zeit", "tokens": ["Be\u00b7kannt", ":", "man", "wieg\u00b7te", "vor", "nicht", "lan\u00b7ger", "Zeit"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$.", "PIS", "VVFIN", "APPR", "PTKNEG", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Die Kinder mit coa-coalisirt", "tokens": ["Die", "Kin\u00b7der", "mit", "coa\u00b7coali\u00b7sirt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.10": {"text": "In sanftern Schlaf. Das junge Fr\u00e4ulein fragte", "tokens": ["In", "sanf\u00b7tern", "Schlaf", ".", "Das", "jun\u00b7ge", "Fr\u00e4u\u00b7lein", "frag\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$.", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Die gn\u00e4dige Mama: \u00bbWas machen jetzt", "tokens": ["Die", "gn\u00e4\u00b7di\u00b7ge", "Ma\u00b7ma", ":", "\u00bb", "Was", "ma\u00b7chen", "jetzt"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "PWS", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Die gn\u00e4d'gen Tanten, die coalisirten", "tokens": ["Die", "gn\u00e4d'\u00b7gen", "Tan\u00b7ten", ",", "die", "coa\u00b7lis\u00b7ir\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Puissancen wol?\u00ab Der Informator h\u00f6rte", "tokens": ["Pu\u00b7is\u00b7san\u00b7cen", "wol", "?", "\u00ab", "Der", "In\u00b7for\u00b7ma\u00b7tor", "h\u00f6r\u00b7te"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NN", "ADV", "$.", "$(", "ART", "NN", "VVFIN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.14": {"text": "Das Wort mit Aerger: \u00bbWahrer Sol\u00f6cism!", "tokens": ["Das", "Wort", "mit", "A\u00b7er\u00b7ger", ":", "\u00bb", "Wah\u00b7rer", "So\u00b7l\u00f6\u00b7cism", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$.", "$(", "ADJA", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "(soll's ja so hei\u00dfen) einzig ", "tokens": ["(", "soll's", "ja", "so", "hei\u00b7\u00dfen", ")", "ein\u00b7zig"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word"], "pos": ["$(", "VMFIN", "ADV", "ADV", "VVINF", "$(", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.16": {"text": "Und nicht ", "tokens": ["Und", "nicht"], "token_info": ["word", "word"], "pos": ["KON", "PTKNEG"], "meter": "-+", "measure": "iambic.single"}, "line.17": {"text": "Erfand das Wort, als ob die ganze Welt", "tokens": ["Er\u00b7fand", "das", "Wort", ",", "als", "ob", "die", "gan\u00b7ze", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "KOKOM", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.18": {"text": "F\u00fcr ihn ", "tokens": ["F\u00fcr", "ihn"], "token_info": ["word", "word"], "pos": ["APPR", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.19": {"text": "Antwortete der Secretarius,", "tokens": ["Ant\u00b7wor\u00b7te\u00b7te", "der", "Se\u00b7cre\u00b7ta\u00b7rius", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "+--++-+-+", "measure": "dactylic.init"}, "line.20": {"text": "\u00bbder stolze ", "tokens": ["\u00bb", "der", "stol\u00b7ze"], "token_info": ["punct", "word", "word"], "pos": ["$(", "ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.21": {"text": "Entgegen seinem Rath nichts mehr bedeute", "tokens": ["Ent\u00b7ge\u00b7gen", "sei\u00b7nem", "Rath", "nichts", "mehr", "be\u00b7deu\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.22": {"text": "Als eine Reichstags-Coalition.", "tokens": ["Als", "ei\u00b7ne", "Reichs\u00b7tags\u00b7Coali\u00b7ti\u00b7on", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Sie sangen ja den zweiten Psalm!\u00ab \u00bbWoher", "tokens": ["Sie", "san\u00b7gen", "ja", "den", "zwei\u00b7ten", "Psalm", "!", "\u00ab", "\u00bb", "Wo\u00b7her"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$.", "$(", "$(", "PWAV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.24": {"text": "Es stamme,\u00ab sprach der Informator, \u00bbfremd", "tokens": ["Es", "stam\u00b7me", ",", "\u00ab", "sprach", "der", "In\u00b7for\u00b7ma\u00b7tor", ",", "\u00bb", "fremd"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "$(", "VVFIN", "ART", "NN", "$,", "$(", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.25": {"text": "Ist es und tauget nicht. Sonst nannte man's", "tokens": ["Ist", "es", "und", "tau\u00b7get", "nicht", ".", "Sonst", "nann\u00b7te", "man's"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "KON", "VVFIN", "PTKNEG", "$.", "ADV", "VVFIN", "PIS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.26": {"text": "Es hie\u00df auch ", "tokens": ["Es", "hie\u00df", "auch"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.27": {"text": "Die Allianz. Doch das ", "tokens": ["Die", "Al\u00b7li\u00b7anz", ".", "Doch", "das"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$.", "KON", "PDS"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.28": {"text": "Ich schlug das Buch der Richter auf, wie B\u00e4ume", "tokens": ["Ich", "schlug", "das", "Buch", "der", "Rich\u00b7ter", "auf", ",", "wie", "B\u00e4u\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "PTKVZ", "$,", "PWAV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.29": {"text": "Sich um die Allianz und Monarchie", "tokens": ["Sich", "um", "die", "Al\u00b7li\u00b7anz", "und", "Mon\u00b7ar\u00b7chie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Besprachen: \u00bbSoll ich meinen s\u00fc\u00dfen Most", "tokens": ["Be\u00b7spra\u00b7chen", ":", "\u00bb", "Soll", "ich", "mei\u00b7nen", "s\u00fc\u00b7\u00dfen", "Most"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "$(", "VMFIN", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.31": {"text": "Aufgeben?\u00ab sprach der Weinstock. \u00bbUnd soll ich", "tokens": ["Auf\u00b7ge\u00b7ben", "?", "\u00ab", "sprach", "der", "Wein\u00b7stock", ".", "\u00bb", "Und", "soll", "ich"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NN", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "KON", "VMFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.32": {"text": "Aus meiner Wurzel treten, da\u00df ich mich", "tokens": ["Aus", "mei\u00b7ner", "Wur\u00b7zel", "tre\u00b7ten", ",", "da\u00df", "ich", "mich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$,", "KOUS", "PPER", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.33": {"text": "Coalisire?\u00ab sprach die Ceder. \u00bbSchlage", "tokens": ["Coa\u00b7li\u00b7si\u00b7re", "?", "\u00ab", "sprach", "die", "Ce\u00b7der", ".", "\u00bb", "Schla\u00b7ge"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["NE", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.34": {"text": "Der Herr nur den Propheten Daniel", "tokens": ["Der", "Herr", "nur", "den", "Pro\u00b7phe\u00b7ten", "Da\u00b7ni\u00b7el"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "ART", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.35": {"text": "Und Esra sammt der Offenbarung auf!", "tokens": ["Und", "Es\u00b7ra", "sammt", "der", "Of\u00b7fen\u00b7ba\u00b7rung", "auf", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.36": {"text": "Da findet er so manches sch\u00f6ne Bild", "tokens": ["Da", "fin\u00b7det", "er", "so", "man\u00b7ches", "sch\u00f6\u00b7ne", "Bild"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.37": {"text": "Coalisirter M\u00e4chte: Adler, Leu", "tokens": ["Coa\u00b7lis\u00b7ir\u00b7ter", "M\u00e4ch\u00b7te", ":", "Ad\u00b7ler", ",", "Leu"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["ADJA", "NN", "$.", "NN", "$,", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.38": {"text": "Und Lamm und Greif; es giebt ein sch\u00f6nes Kupfer!\u00ab", "tokens": ["Und", "Lamm", "und", "Greif", ";", "es", "giebt", "ein", "sch\u00f6\u00b7nes", "Kup\u00b7fer", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "NN", "KON", "NN", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.39": {"text": "Die gn\u00e4d'ge Tante sprach's. \u00bbVerzeihung!\u00ab bat", "tokens": ["Die", "gn\u00e4d'\u00b7ge", "Tan\u00b7te", "sprach'", "s.", "\u00bb", "Ver\u00b7zei\u00b7hung", "!", "\u00ab", "bat"], "token_info": ["word", "word", "word", "word", "abbreviation", "punct", "word", "punct", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "NE", "$(", "NN", "$.", "$(", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.40": {"text": "Ein stattlicher Notarius; \u00bballhier", "tokens": ["Ein", "statt\u00b7li\u00b7cher", "No\u00b7ta\u00b7rius", ";", "\u00bb", "all\u00b7hier"], "token_info": ["word", "word", "word", "punct", "punct", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.41": {"text": "Gilt nicht die Bibel. In ", "tokens": ["Gilt", "nicht", "die", "Bi\u00b7bel", ".", "In"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PTKNEG", "ART", "NN", "$.", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.42": {"text": "Entscheiden wir; wir sind ", "tokens": ["Ent\u00b7schei\u00b7den", "wir", ";", "wir", "sind"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "$.", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.43": {"text": "\u00bbso lange darfst Du Deines Landes Baum", "tokens": ["\u00bb", "so", "lan\u00b7ge", "darfst", "Du", "Dei\u00b7nes", "Lan\u00b7des", "Baum"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "ADV", "VMFIN", "PPER", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.44": {"text": "Und Kruste von dem Meinigen zur\u00fcck-", "tokens": ["Und", "Krus\u00b7te", "von", "dem", "Mei\u00b7ni\u00b7gen", "zu\u00b7r\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "APPR", "ART", "PPOSS", "TRUNC"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.45": {"text": "Begehren, als sie mit dem Boden noch", "tokens": ["Be\u00b7geh\u00b7ren", ",", "als", "sie", "mit", "dem", "Bo\u00b7den", "noch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUS", "PPER", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.46": {"text": "Nicht ", "tokens": ["Nicht"], "token_info": ["word"], "pos": ["PTKNEG"], "meter": "+", "measure": "single.up"}, "line.47": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.48": {"text": "\u00bbund gar ", "tokens": ["\u00bb", "und", "gar"], "token_info": ["punct", "word", "word"], "pos": ["$(", "KON", "ADV"], "meter": "-+", "measure": "iambic.single"}, "line.49": {"text": "Bestimmt!\u00ab sprach ein Geheimer Rath; \u00bbdie Kruste,", "tokens": ["Be\u00b7stimmt", "!", "\u00ab", "sprach", "ein", "Ge\u00b7hei\u00b7mer", "Rath", ";", "\u00bb", "die", "Krus\u00b7te", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["VVPP", "$.", "$(", "VVFIN", "ART", "ADJA", "NN", "$.", "$(", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.50": {"text": "Der Baum ", "tokens": ["Der", "Baum"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.51": {"text": "So hei\u00dft es ", "tokens": ["So", "hei\u00dft", "es"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.52": {"text": "Ein B\u00fcndni\u00df, hei\u00dft's ", "tokens": ["Ein", "B\u00fcn\u00b7dni\u00df", ",", "hei\u00dft's"], "token_info": ["word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "NE"], "meter": "-+-+", "measure": "iambic.di"}, "line.53": {"text": "Coalisiren Cabinette sich,", "tokens": ["Coa\u00b7li\u00b7si\u00b7ren", "Ca\u00b7bi\u00b7net\u00b7te", "sich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PRF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.54": {"text": "So folgt darauf ", "tokens": ["So", "folgt", "da\u00b7rauf"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PAV"], "meter": "-+-+", "measure": "iambic.di"}, "line.55": {"text": "Der fremden Erdenkruste ", "tokens": ["Der", "frem\u00b7den", "Er\u00b7den\u00b7krus\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.56": {"text": "Ein angenehmer Actus.\u00ab Endlich ward", "tokens": ["Ein", "an\u00b7ge\u00b7neh\u00b7mer", "Ac\u00b7tus", ".", "\u00ab", "End\u00b7lich", "ward"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV", "VAFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.57": {"text": "Dem Herrn des Hauses dieser Tummelplatz", "tokens": ["Dem", "Herrn", "des", "Hau\u00b7ses", "die\u00b7ser", "Tum\u00b7mel\u00b7platz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "PDAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.58": {"text": "Zu eng. \u00bbIch d\u00e4chte, Jedermann von uns", "tokens": ["Zu", "eng", ".", "\u00bb", "Ich", "d\u00e4ch\u00b7te", ",", "Je\u00b7der\u00b7mann", "von", "uns"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKA", "ADJD", "$.", "$(", "PPER", "VVFIN", "$,", "PIS", "APPR", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.59": {"text": "Coalescirt' und coalirte nur", "tokens": ["Coale\u00b7scirt'", "und", "coa\u00b7lir\u00b7te", "nur"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "VVFIN", "ADV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.60": {"text": "Zuerst mit sich und seiner Kruste.\u00ab \u00bbDas", "tokens": ["Zu\u00b7erst", "mit", "sich", "und", "sei\u00b7ner", "Krus\u00b7te", ".", "\u00ab", "\u00bb", "Das"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["ADV", "APPR", "PRF", "KON", "PPOSAT", "NN", "$.", "$(", "$(", "ART"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.61": {"text": "Ist's eben, gn\u00e4d'ger Herr,\u00ab sprach ein Statist-", "tokens": ["Ist's", "e\u00b7ben", ",", "gn\u00e4d'\u00b7ger", "Herr", ",", "\u00ab", "sprach", "ein", "Sta\u00b7tist"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NE", "ADV", "$,", "ADJA", "NN", "$,", "$(", "VVFIN", "ART", "TRUNC"], "meter": "-+-+-++-+-", "measure": "unknown.measure.penta"}, "line.62": {"text": "Iker, der ", "tokens": ["I\u00b7ker", ",", "der"], "token_info": ["word", "punct", "word"], "pos": ["NE", "$,", "PRELS"], "meter": "+--", "measure": "dactylic.init"}, "line.63": {"text": "Geleget hatte. \u00bbAls vor Jahren ich", "tokens": ["Ge\u00b7le\u00b7get", "hat\u00b7te", ".", "\u00bb", "Als", "vor", "Jah\u00b7ren", "ich"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$.", "$(", "KOUS", "APPR", "NN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.64": {"text": "Mit meinem jungen Herrn auf Reisen war,", "tokens": ["Mit", "mei\u00b7nem", "jun\u00b7gen", "Herrn", "auf", "Rei\u00b7sen", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "APPR", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.65": {"text": "Da fiel mir auf der letzten Station", "tokens": ["Da", "fiel", "mir", "auf", "der", "letz\u00b7ten", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.66": {"text": "In Frankreich an der Grenze schwer es auf,", "tokens": ["In", "Fran\u00b7kreich", "an", "der", "Gren\u00b7ze", "schwer", "es", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "ADJD", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.67": {"text": "Wie Alles dort so bald coalescire.", "tokens": ["Wie", "Al\u00b7les", "dort", "so", "bald", "coa\u00b7le\u00b7sci\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADV", "ADV", "ADV", "NE", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.68": {"text": "Vor wenig Jahren waren Hennegau", "tokens": ["Vor", "we\u00b7nig", "Jah\u00b7ren", "wa\u00b7ren", "Hen\u00b7ne\u00b7gau"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.69": {"text": "Und Flandern fl\u00e4misch, Lothringen war deutsch;", "tokens": ["Und", "Flan\u00b7dern", "fl\u00e4\u00b7misch", ",", "Loth\u00b7rin\u00b7gen", "war", "deutsch", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJD", "$,", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.70": {"text": "Und jetzt ist bis zur letzten Station", "tokens": ["Und", "jetzt", "ist", "bis", "zur", "letz\u00b7ten", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.71": {"text": "Alles franz\u00f6sisch, um- und umgewandt,", "tokens": ["Al\u00b7les", "fran\u00b7z\u00f6\u00b7sisch", ",", "um", "und", "um\u00b7ge\u00b7wandt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "$,", "TRUNC", "KON", "VVPP", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.72": {"text": "Bekleibet, neugeschaffen, coalirt.\u00ab", "tokens": ["Be\u00b7klei\u00b7bet", ",", "neu\u00b7ge\u00b7schaf\u00b7fen", ",", "coa\u00b7lirt", ".", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["VVFIN", "$,", "ADJA", "$,", "VVPP", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.73": {"text": "Und dicht daneben h\u00e4ngt, an Wulst und Leib", "tokens": ["Und", "dicht", "da\u00b7ne\u00b7ben", "h\u00e4ngt", ",", "an", "Wulst", "und", "Leib"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "PAV", "VVFIN", "$,", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.74": {"text": "Und Sprach' und Sitten gleich, das Brabant an,", "tokens": ["Und", "Sprach'", "und", "Sit\u00b7ten", "gleich", ",", "das", "Bra\u00b7bant", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "ADV", "$,", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.75": {"text": "Das Deutschland! \u00bbWie ", "tokens": ["Das", "Deutschland", "!", "\u00bb", "Wie"], "token_info": ["word", "word", "punct", "punct", "word"], "pos": ["ART", "NN", "$.", "$(", "PWAV"], "meter": "+-+", "measure": "trochaic.di"}, "line.76": {"text": "Fragt' ich mich selbst, \u00bbund wie ", "tokens": ["Fragt'", "ich", "mich", "selbst", ",", "\u00bb", "und", "wie"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "$,", "$(", "KON", "PWAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.77": {"text": "Es sich Provinzen, die's incorporirt?", "tokens": ["Es", "sich", "Pro\u00b7vin\u00b7zen", ",", "die's", "in\u00b7cor\u00b7po\u00b7rirt", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "PRF", "NN", "$,", "FM.la", "FM.la", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.78": {"text": "Ein schweres Staatsproblem!\u00ab Hier sehen Sie", "tokens": ["Ein", "schwe\u00b7res", "Staats\u00b7prob\u00b7lem", "!", "\u00ab", "Hier", "se\u00b7hen", "Sie"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.79": {"text": "Die gro\u00dfe L\u00e4nderkarte. Ostw\u00e4rts dort", "tokens": ["Die", "gro\u00b7\u00dfe", "L\u00e4n\u00b7der\u00b7kar\u00b7te", ".", "Ost\u00b7w\u00e4rts", "dort"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "ADV", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.80": {"text": "Das ungeheure Kaiserthum Gro\u00df-Tschni,", "tokens": ["Das", "un\u00b7ge\u00b7heu\u00b7re", "Kai\u00b7ser\u00b7thum", "Gro\u00df\u00b7\u00b7Tschni", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.81": {"text": "Tschong-Ku, Tschong-Hoa! Leider nennen wir's", "tokens": ["Tschong\u00b7Ku", ",", "Tschong\u00b7Hoa", "!", "Lei\u00b7der", "nen\u00b7nen", "wir's"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "NE", "$.", "NN", "VVINF", "VAFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.82": {"text": "Mit falschem Namen ", "tokens": ["Mit", "fal\u00b7schem", "Na\u00b7men"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.83": {"text": "Mit seinen tausendundvierhundertzwei-", "tokens": ["Mit", "sei\u00b7nen", "tau\u00b7send\u00b7und\u00b7vier\u00b7hun\u00b7dertzwei"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "TRUNC"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.84": {"text": "Undvierzig Str\u00f6men, vielen Br\u00fccken und", "tokens": ["Und\u00b7vier\u00b7zig", "Str\u00f6\u00b7men", ",", "vie\u00b7len", "Br\u00fc\u00b7cken", "und"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "NN", "$,", "PIAT", "NN", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.85": {"text": "Zweitausend Bergen, hundertneunundvierzig", "tokens": ["Zweit\u00b7au\u00b7send", "Ber\u00b7gen", ",", "hun\u00b7dert\u00b7neun\u00b7und\u00b7vier\u00b7zig"], "token_info": ["word", "word", "punct", "word"], "pos": ["CARD", "NN", "$,", "CARD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.86": {"text": "Millionen und sechshundertzweiundsechigtausend Menschen,", "tokens": ["Mil\u00b7lion\u00b7en", "und", "sechs\u00b7hun\u00b7dert\u00b7zwei\u00b7und\u00b7se\u00b7chig\u00b7tau\u00b7send", "Men\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+--+-+-+-+-", "measure": "amphibrach.tri.plus"}, "line.87": {"text": "Dort von der Mauer bis nach Canton zu,", "tokens": ["Dort", "von", "der", "Mau\u00b7er", "bis", "nach", "Can\u00b7ton", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "APPR", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.88": {"text": "Ja bis nach Lao-Tschua, Cotschin-Tschina,", "tokens": ["Ja", "bis", "nach", "Lao\u00b7\u00b7T\u00b7schua", ",", "Cot\u00b7schin\u00b7\u00b7T\u00b7schi\u00b7na", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PTKANT", "KON", "APPR", "NE", "$,", "NE", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.89": {"text": "Cambotscha, Tunkin, ist, wie ", "tokens": ["Cam\u00b7bot\u00b7scha", ",", "Tun\u00b7kin", ",", "ist", ",", "wie"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "NE", "$,", "VAFIN", "$,", "PWAV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.90": {"text": "Mit seinem Boden trefflich coalirt.", "tokens": ["Mit", "sei\u00b7nem", "Bo\u00b7den", "treff\u00b7lich", "coa\u00b7lirt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.91": {"text": "Ein jeder Mandarin hat seinen Platz", "tokens": ["Ein", "je\u00b7der", "Man\u00b7da\u00b7rin", "hat", "sei\u00b7nen", "Platz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.92": {"text": "Und seine Feder. Kommt ein fremder Lord,", "tokens": ["Und", "sei\u00b7ne", "Fe\u00b7der", ".", "Kommt", "ein", "frem\u00b7der", "Lord", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$.", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.93": {"text": "Mit Freudenfeuern f\u00fchrt man ihn hinein,", "tokens": ["Mit", "Freu\u00b7den\u00b7feu\u00b7ern", "f\u00fchrt", "man", "ihn", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.94": {"text": "Und bald hinaus, da\u00df er nicht coalire.", "tokens": ["Und", "bald", "hin\u00b7aus", ",", "da\u00df", "er", "nicht", "coa\u00b7li\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APZR", "$,", "KOUS", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.95": {"text": "Dagegen Hindostan, das arme Land,", "tokens": ["Da\u00b7ge\u00b7gen", "Hin\u00b7dos\u00b7tan", ",", "das", "ar\u00b7me", "Land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.96": {"text": "Ist elend coalirt. Bramanen, Schattri,", "tokens": ["Ist", "e\u00b7lend", "coa\u00b7lirt", ".", "Bra\u00b7ma\u00b7nen", ",", "Schat\u00b7tri", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$.", "NN", "$,", "NE", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.97": {"text": "Banjanen, Schutter, und die Fremden gar,", "tokens": ["Ban\u00b7ja\u00b7nen", ",", "Schut\u00b7ter", ",", "und", "die", "Frem\u00b7den", "gar", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "KON", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.98": {"text": "Seiken, Dschaten, Gebern und Afghanen,", "tokens": ["Sei\u00b7ken", ",", "Dscha\u00b7ten", ",", "Ge\u00b7bern", "und", "Af\u00b7gha\u00b7nen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.99": {"text": "Mongolen, Juden, Perser, Araber,", "tokens": ["Mon\u00b7go\u00b7len", ",", "Ju\u00b7den", ",", "Per\u00b7ser", ",", "A\u00b7ra\u00b7ber", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "---+-+--+-", "measure": "iambic.tri.relaxed"}, "line.100": {"text": "Und Europ\u00e4er aller Art, Maratten,", "tokens": ["Und", "Eu\u00b7ro\u00b7p\u00e4\u00b7er", "al\u00b7ler", "Art", ",", "Ma\u00b7rat\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "PIAT", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.101": {"text": "Rasbatten; darum geht's den guten Hindus", "tokens": ["Ras\u00b7bat\u00b7ten", ";", "da\u00b7rum", "geht's", "den", "gu\u00b7ten", "Hin\u00b7dus"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "PAV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.102": {"text": "Auch so erb\u00e4rmlich. \u2013 Nun spazieren Sie", "tokens": ["Auch", "so", "er\u00b7b\u00e4rm\u00b7lich", ".", "\u2013", "Nun", "spa\u00b7zie\u00b7ren", "Sie"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "$.", "$(", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.103": {"text": "Von den Fuchsinseln bis nach Kexholm hin;", "tokens": ["Von", "den", "Fuchs\u00b7in\u00b7seln", "bis", "nach", "Kex\u00b7holm", "hin", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "APPR", "NE", "PTKVZ", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.104": {"text": "Wie h\u00e4ngt's zusammen! Samojeden und", "tokens": ["Wie", "h\u00e4ngt's", "zu\u00b7sam\u00b7men", "!", "Sa\u00b7mo\u00b7je\u00b7den", "und"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VVFIN", "PTKVZ", "$.", "NN", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.105": {"text": "Tungusen, Tatern, Kamtschadalen; da", "tokens": ["Tun\u00b7gu\u00b7sen", ",", "Ta\u00b7tern", ",", "Kamt\u00b7scha\u00b7da\u00b7len", ";", "da"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "$.", "KOUS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.106": {"text": "Lebt Jeder, wie er will, wenn er nur Pelze", "tokens": ["Lebt", "Je\u00b7der", ",", "wie", "er", "will", ",", "wenn", "er", "nur", "Pel\u00b7ze"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "$,", "PWAV", "PPER", "VMFIN", "$,", "KOUS", "PPER", "ADV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.107": {"text": "Und seinen Rubel giebt. \u2013 Das arme Polen,", "tokens": ["Und", "sei\u00b7nen", "Ru\u00b7bel", "giebt", ".", "\u2013", "Das", "ar\u00b7me", "Po\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "$(", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.108": {"text": "Warum denn ward's zertheilt? Es war mit sich", "tokens": ["Wa\u00b7rum", "denn", "ward's", "zer\u00b7theilt", "?", "Es", "war", "mit", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "VVPP", "$.", "PPER", "VAFIN", "APPR", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.109": {"text": "Nicht coalirt; drum schnitt man es entzwei;", "tokens": ["Nicht", "coa\u00b7lirt", ";", "drum", "schnitt", "man", "es", "ent\u00b7zwei", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$.", "PAV", "VVFIN", "PIS", "PPER", "PTKVZ", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.110": {"text": "Nun wachsen seine St\u00fccke neu und frisch", "tokens": ["Nun", "wach\u00b7sen", "sei\u00b7ne", "St\u00fc\u00b7cke", "neu", "und", "frisch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.111": {"text": "Zusammen durch die Cur der Sympathie.", "tokens": ["Zu\u00b7sam\u00b7men", "durch", "die", "Cur", "der", "Sym\u00b7pa\u00b7thie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.112": {"text": "\u00bbdas gro\u00dfe Deutschland (warum liegt es doch", "tokens": ["\u00bb", "das", "gro\u00b7\u00dfe", "Deutschland", "(", "wa\u00b7rum", "liegt", "es", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$(", "PWAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.113": {"text": "So nah an Polen?), Holland, Engeland", "tokens": ["So", "nah", "an", "Po\u00b7len", "?", ")", ",", "Hol\u00b7land", ",", "En\u00b7ge\u00b7land"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "punct", "word"], "pos": ["ADV", "ADJD", "APPR", "NE", "$.", "$(", "$,", "NE", "$,", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.114": {"text": "Mit Schottland, Irland, Caledonien,", "tokens": ["Mit", "Schott\u00b7land", ",", "Ir\u00b7land", ",", "Ca\u00b7le\u00b7do\u00b7ni\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.115": {"text": "Italien und Griechenland, T\u00fcrkei", "tokens": ["I\u00b7ta\u00b7li\u00b7en", "und", "Grie\u00b7chen\u00b7land", ",", "T\u00fcr\u00b7kei"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "KON", "NE", "$,", "NN"], "meter": "+--+-+-+++", "measure": "iambic.hexa.invert"}, "line.116": {"text": "Und Walachei und Moldau \u2013\u00ab \u00bbIst's denn noch", "tokens": ["Und", "Wa\u00b7la\u00b7chei", "und", "Mol\u00b7dau", "\u2013", "\u00ab", "\u00bb", "Ist's", "denn", "noch"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NE", "$(", "$(", "$(", "NE", "KON", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.117": {"text": "Nicht aus?\u00ab rief der Baron. \u2013 \u00bbDas Beste kommt", "tokens": ["Nicht", "aus", "?", "\u00ab", "rief", "der", "Ba\u00b7ron", ".", "\u2013", "\u00bb", "Das", "Bes\u00b7te", "kommt"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word"], "pos": ["PTKNEG", "PTKVZ", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "$(", "ART", "NN", "VVFIN"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.118": {"text": "Anjetzt. \u2013 Nun treten Sie in Frankreich ein!", "tokens": ["An\u00b7jetzt", ".", "\u2013", "Nun", "tre\u00b7ten", "Sie", "in", "Fran\u00b7kreich", "ein", "!"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "$(", "ADV", "VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.119": {"text": "Da weht franz\u00f6sische Luft; da essen sie", "tokens": ["Da", "weht", "fran\u00b7z\u00f6\u00b7si\u00b7sche", "Luft", ";", "da", "es\u00b7sen", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "$.", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.120": {"text": "Und trinken, jauchzen, reden, singen ganz", "tokens": ["Und", "trin\u00b7ken", ",", "jauch\u00b7zen", ",", "re\u00b7den", ",", "sin\u00b7gen", "ganz"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "$,", "VVINF", "$,", "VVINF", "$,", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.121": {"text": "Franz\u00f6sisch. Schon das Kind in Mutterleib,", "tokens": ["Fran\u00b7z\u00f6\u00b7sisch", ".", "Schon", "das", "Kind", "in", "Mut\u00b7ter\u00b7leib", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ADV", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.122": {"text": "Ich glaub', es denkt und spricht franz\u00f6sisch. Selbst", "tokens": ["Ich", "glaub'", ",", "es", "denkt", "und", "spricht", "fran\u00b7z\u00f6\u00b7sisch", ".", "Selbst"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "KON", "VVFIN", "ADJD", "$.", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.123": {"text": "Latein und Griech'sch spricht man franz\u00f6sisch aus,", "tokens": ["La\u00b7tein", "und", "Griech'sch", "spricht", "man", "fran\u00b7z\u00f6\u00b7sisch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "PIS", "ADJD", "PTKVZ", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.124": {"text": "Und Alles mit Geschmack. Sie ziehn den Fremden", "tokens": ["Und", "Al\u00b7les", "mit", "Ge\u00b7schmack", ".", "Sie", "ziehn", "den", "Frem\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIS", "APPR", "NN", "$.", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.125": {"text": "So an sich, da\u00df er mit coalescirt.", "tokens": ["So", "an", "sich", ",", "da\u00df", "er", "mit", "coa\u00b7le\u00b7scirt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRF", "$,", "KOUS", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.126": {"text": "Oft hab' ich dran gedacht, warum denn Griechen", "tokens": ["Oft", "hab'", "ich", "dran", "ge\u00b7dacht", ",", "wa\u00b7rum", "denn", "Grie\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "VVPP", "$,", "PWAV", "ADV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.127": {"text": "Und R\u00f6mer auch nicht so zusammenwuchsen.", "tokens": ["Und", "R\u00f6\u00b7mer", "auch", "nicht", "so", "zu\u00b7sam\u00b7men\u00b7wuch\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.128": {"text": "Was half den Griechen ihr Ach\u00e4erbund,", "tokens": ["Was", "half", "den", "Grie\u00b7chen", "ihr", "A\u00b7ch\u00e4er\u00b7bund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.129": {"text": "Ihr Panionium, Amphiktyonenhof,", "tokens": ["Ihr", "Pa\u00b7ni\u00b7o\u00b7ni\u00b7um", ",", "Am\u00b7phik\u00b7ty\u00b7on\u00b7en\u00b7hof", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Ihr Pan\u00e4tolium? Was halfen den", "tokens": ["Ihr", "Pa\u00b7n\u00e4\u00b7to\u00b7li\u00b7um", "?", "Was", "hal\u00b7fen", "den"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$.", "PWS", "VVFIN", "ART"], "meter": "+-+--+-+--", "measure": "trochaic.tetra.relaxed"}, "line.131": {"text": "Etruriern die Lucumonen? was", "tokens": ["E\u00b7tru\u00b7ri\u00b7ern", "die", "Lu\u00b7cu\u00b7mo\u00b7nen", "?", "was"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ADV", "ART", "NN", "$.", "PWS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.132": {"text": "Den R\u00f6mern ihr ", "tokens": ["Den", "R\u00f6\u00b7mern", "ihr"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "PPOSAT"], "meter": "-+-+", "measure": "iambic.di"}, "line.133": {"text": "Den Celtiberiern \u2013\u00ab \u00bbIst's noch nicht aus?", "tokens": ["Den", "Cel\u00b7ti\u00b7be\u00b7ri\u00b7ern", "\u2013", "\u00ab", "\u00bb", "Ist's", "noch", "nicht", "aus", "?"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "$(", "$(", "NE", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.134": {"text": "Da seh' der Herr die sieben Pfeile auf", "tokens": ["Da", "seh'", "der", "Herr", "die", "sie\u00b7ben", "Pfei\u00b7le", "auf"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.135": {"text": "Holl\u00e4ndischen Ducaten mit der Aufschrift", "tokens": ["Hol\u00b7l\u00e4n\u00b7di\u00b7schen", "Du\u00b7ca\u00b7ten", "mit", "der", "Auf\u00b7schrift"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.136": {"text": "Im Golde des Ducaten coalirt!\u00ab", "tokens": ["Im", "Gol\u00b7de", "des", "Du\u00b7ca\u00b7ten", "coa\u00b7lirt", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.137": {"text": "\u00bbnun so coalisir' Er denn!\u00ab \u00bbEr wird,\u00ab", "tokens": ["\u00bb", "nun", "so", "coali\u00b7sir'", "Er", "denn", "!", "\u00ab", "\u00bb", "Er", "wird", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADV", "VVFIN", "PPER", "ADV", "$.", "$(", "$(", "PPER", "VAFIN", "$,", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.138": {"text": "Antwortete der Arzt, der bis dahin", "tokens": ["Ant\u00b7wor\u00b7te\u00b7te", "der", "Arzt", ",", "der", "bis", "da\u00b7hin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "PRELS", "APPR", "PAV"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.139": {"text": "Geschwiegen hatte, \u00bbjetzt erz\u00e4hlen, wie", "tokens": ["Ge\u00b7schwie\u00b7gen", "hat\u00b7te", ",", "\u00bb", "jetzt", "er\u00b7z\u00e4h\u00b7len", ",", "wie"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word"], "pos": ["VVPP", "VAFIN", "$,", "$(", "ADV", "VVINF", "$,", "PWAV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.140": {"text": "Man die in Eins Gewachsenen curirt.", "tokens": ["Man", "die", "in", "Eins", "Ge\u00b7wach\u00b7se\u00b7nen", "cu\u00b7rirt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.141": {"text": "Dem Einen Schnupftobak, der Andre niest;", "tokens": ["Dem", "Ei\u00b7nen", "Schnupf\u00b7to\u00b7bak", ",", "der", "And\u00b7re", "niest", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "$,", "PRELS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.142": {"text": "Purgirt den Einen \u2013 denn wie Haller sagt,", "tokens": ["Pur\u00b7girt", "den", "Ei\u00b7nen", "\u2013", "denn", "wie", "Hal\u00b7ler", "sagt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ART", "$(", "KON", "PWAV", "NE", "VVFIN", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.143": {"text": "Kommt's bei in Eins Gewachsnen nicht auf K\u00f6pfe", "tokens": ["Kommt's", "bei", "in", "Eins", "Ge\u00b7wachs\u00b7nen", "nicht", "auf", "K\u00f6p\u00b7fe"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "APPR", "ART", "NN", "PTKNEG", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.144": {"text": "Und M\u00e4gen an, sie sind ", "tokens": ["Und", "M\u00e4\u00b7gen", "an", ",", "sie", "sind"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KON", "NN", "PTKVZ", "$,", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.145": {"text": "\u00bbnicht also!\u00ab sprach ein Casuist; \u00bbnach ", "tokens": ["\u00bb", "nicht", "al\u00b7so", "!", "\u00ab", "sprach", "ein", "Ca\u00b7su\u00b7ist", ";", "\u00bb", "nach"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["$(", "PTKNEG", "ADV", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "APPR"], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.146": {"text": "Wird ein ", "tokens": ["Wird", "ein"], "token_info": ["word", "word"], "pos": ["VAFIN", "ART"], "meter": "-+", "measure": "iambic.single"}, "line.147": {"text": "Da viel zu herzen?\u00ab Der Baron", "tokens": ["Da", "viel", "zu", "her\u00b7zen", "?", "\u00ab", "Der", "Ba\u00b7ron"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["KOUS", "PIS", "PTKZU", "VVINF", "$.", "$(", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.148": {"text": "War dieses Streites m\u00fcde. \u00bbSeht, Ihr Herrn,", "tokens": ["War", "die\u00b7ses", "Strei\u00b7tes", "m\u00fc\u00b7de", ".", "\u00bb", "Seht", ",", "Ihr", "Herrn", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADJD", "$.", "$(", "VVFIN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.149": {"text": "Ihr selber seid in Euern Meinungen,", "tokens": ["Ihr", "sel\u00b7ber", "seid", "in", "Eu\u00b7ern", "Mei\u00b7nun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.150": {"text": "Ein Wort betreffend, weder coalirt,", "tokens": ["Ein", "Wort", "be\u00b7tref\u00b7fend", ",", "we\u00b7der", "coa\u00b7lirt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "KON", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.151": {"text": "Noch wollt Ihr Euch coalisiren; und", "tokens": ["Noch", "wollt", "Ihr", "Euch", "coa\u00b7li\u00b7si\u00b7ren", ";", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "VVINF", "$.", "KON"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.152": {"text": "Coalisirt die Welt? Nutzlose M\u00fch!", "tokens": ["Coa\u00b7li\u00b7sirt", "die", "Welt", "?", "Nutz\u00b7lo\u00b7se", "M\u00fch", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$.", "NE", "NE", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.153": {"text": "Sei Jeder erst mit seinem Stand und Land", "tokens": ["Sei", "Je\u00b7der", "erst", "mit", "sei\u00b7nem", "Stand", "und", "Land"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.154": {"text": "Und Haus und Hof und Weib und Kind und Amt", "tokens": ["Und", "Haus", "und", "Hof", "und", "Weib", "und", "Kind", "und", "Amt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "KON", "NN", "KON", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.155": {"text": "Und Pflicht, ja mit sich selbst recht coalirt;", "tokens": ["Und", "Pflicht", ",", "ja", "mit", "sich", "selbst", "recht", "coa\u00b7lirt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "ADV", "APPR", "PRF", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.156": {"text": "Er wird Tschin-Tschin vergessen. Lerne doch,", "tokens": ["Er", "wird", "Tschin\u00b7\u00b7T\u00b7schin", "ver\u00b7ges\u00b7sen", ".", "Ler\u00b7ne", "doch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "VVPP", "$.", "NN", "ADV", "$,"], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.157": {"text": "Was Euch der Haushahn in der Fibel sagt,", "tokens": ["Was", "Euch", "der", "Haus\u00b7hahn", "in", "der", "Fi\u00b7bel", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.158": {"text": "Ein Jeder seine Lection, so steht", "tokens": ["Ein", "Je\u00b7der", "sei\u00b7ne", "Lec\u00b7ti\u00b7on", ",", "so", "steht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PIS", "PPOSAT", "NN", "$,", "ADV", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.159": {"text": "Es wohl in Hause, Stadt und Land und Welt.\u00ab \u2013", "tokens": ["Es", "wohl", "in", "Hau\u00b7se", ",", "Stadt", "und", "Land", "und", "Welt", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "ADV", "APPR", "NN", "$,", "NN", "KON", "NN", "KON", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.160": {"text": "Sieh, Freund, so spricht die deutsche Politik", "tokens": ["Sieh", ",", "Freund", ",", "so", "spricht", "die", "deut\u00b7sche", "Po\u00b7li\u00b7tik"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$,", "NN", "$,", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.161": {"text": "Vom Fernsten immer und vom Weitesten,", "tokens": ["Vom", "Ferns\u00b7ten", "im\u00b7mer", "und", "vom", "Wei\u00b7tes\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "KON", "APPRART", "NN", "$,"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.162": {"text": "Nur nicht von sich. Und lohnt es wol der M\u00fch,", "tokens": ["Nur", "nicht", "von", "sich", ".", "Und", "lohnt", "es", "wol", "der", "M\u00fch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "APPR", "PRF", "$.", "KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.163": {"text": "Die Musen mit dem Wuste zu entweihn?", "tokens": ["Die", "Mu\u00b7sen", "mit", "dem", "Wus\u00b7te", "zu", "ent\u00b7weihn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.164": {"text": "Verbannt aus Deutschland ist die Politik;", "tokens": ["Ver\u00b7bannt", "aus", "Deutschland", "ist", "die", "Po\u00b7li\u00b7tik", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NE", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.165": {"text": "Verbannet sei nur nicht die Menschlichkeit!", "tokens": ["Ver\u00b7ban\u00b7net", "sei", "nur", "nicht", "die", "Menschlich\u00b7keit", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ADV", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbpolitisch Lied, ein b\u00f6ses, b\u00f6ses Lied!\u00ab", "tokens": ["\u00bb", "po\u00b7li\u00b7tisch", "Lied", ",", "ein", "b\u00f6\u00b7ses", ",", "b\u00f6\u00b7ses", "Lied", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "NN", "$,", "ART", "ADJA", "$,", "ADJA", "NN", "$.", "$("], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "So sagt das Sprichwort; und Du willst, o Freund,", "tokens": ["So", "sagt", "das", "Sprich\u00b7wort", ";", "und", "Du", "willst", ",", "o", "Freund", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "KON", "PPER", "VMFIN", "$,", "FM", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df dichtend unsre Nation sogar", "tokens": ["Da\u00df", "dich\u00b7tend", "uns\u00b7re", "Na\u00b7tion", "so\u00b7gar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Politisire? H\u00f6r ein M\u00e4rchen an,", "tokens": ["Po\u00b7li\u00b7ti\u00b7si\u00b7re", "?", "H\u00f6r", "ein", "M\u00e4r\u00b7chen", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NE", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Was ein politisch Wort, ein blo\u00dfes Wort,", "tokens": ["Was", "ein", "po\u00b7li\u00b7tisch", "Wort", ",", "ein", "blo\u00b7\u00dfes", "Wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "F\u00fcr mancherlei Besinnung dem Gem\u00fcth", "tokens": ["F\u00fcr", "man\u00b7cher\u00b7lei", "Be\u00b7sin\u00b7nung", "dem", "Ge\u00b7m\u00fcth"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Nur ", "tokens": ["Nur"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Bekannt: man wiegte vor nicht langer Zeit", "tokens": ["Be\u00b7kannt", ":", "man", "wieg\u00b7te", "vor", "nicht", "lan\u00b7ger", "Zeit"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$.", "PIS", "VVFIN", "APPR", "PTKNEG", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Die Kinder mit coa-coalisirt", "tokens": ["Die", "Kin\u00b7der", "mit", "coa\u00b7coali\u00b7sirt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.10": {"text": "In sanftern Schlaf. Das junge Fr\u00e4ulein fragte", "tokens": ["In", "sanf\u00b7tern", "Schlaf", ".", "Das", "jun\u00b7ge", "Fr\u00e4u\u00b7lein", "frag\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$.", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Die gn\u00e4dige Mama: \u00bbWas machen jetzt", "tokens": ["Die", "gn\u00e4\u00b7di\u00b7ge", "Ma\u00b7ma", ":", "\u00bb", "Was", "ma\u00b7chen", "jetzt"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "PWS", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Die gn\u00e4d'gen Tanten, die coalisirten", "tokens": ["Die", "gn\u00e4d'\u00b7gen", "Tan\u00b7ten", ",", "die", "coa\u00b7lis\u00b7ir\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Puissancen wol?\u00ab Der Informator h\u00f6rte", "tokens": ["Pu\u00b7is\u00b7san\u00b7cen", "wol", "?", "\u00ab", "Der", "In\u00b7for\u00b7ma\u00b7tor", "h\u00f6r\u00b7te"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NN", "ADV", "$.", "$(", "ART", "NN", "VVFIN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.14": {"text": "Das Wort mit Aerger: \u00bbWahrer Sol\u00f6cism!", "tokens": ["Das", "Wort", "mit", "A\u00b7er\u00b7ger", ":", "\u00bb", "Wah\u00b7rer", "So\u00b7l\u00f6\u00b7cism", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$.", "$(", "ADJA", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "(soll's ja so hei\u00dfen) einzig ", "tokens": ["(", "soll's", "ja", "so", "hei\u00b7\u00dfen", ")", "ein\u00b7zig"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word"], "pos": ["$(", "VMFIN", "ADV", "ADV", "VVINF", "$(", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.16": {"text": "Und nicht ", "tokens": ["Und", "nicht"], "token_info": ["word", "word"], "pos": ["KON", "PTKNEG"], "meter": "-+", "measure": "iambic.single"}, "line.17": {"text": "Erfand das Wort, als ob die ganze Welt", "tokens": ["Er\u00b7fand", "das", "Wort", ",", "als", "ob", "die", "gan\u00b7ze", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "KOKOM", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.18": {"text": "F\u00fcr ihn ", "tokens": ["F\u00fcr", "ihn"], "token_info": ["word", "word"], "pos": ["APPR", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.19": {"text": "Antwortete der Secretarius,", "tokens": ["Ant\u00b7wor\u00b7te\u00b7te", "der", "Se\u00b7cre\u00b7ta\u00b7rius", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "+--++-+-+", "measure": "dactylic.init"}, "line.20": {"text": "\u00bbder stolze ", "tokens": ["\u00bb", "der", "stol\u00b7ze"], "token_info": ["punct", "word", "word"], "pos": ["$(", "ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.21": {"text": "Entgegen seinem Rath nichts mehr bedeute", "tokens": ["Ent\u00b7ge\u00b7gen", "sei\u00b7nem", "Rath", "nichts", "mehr", "be\u00b7deu\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.22": {"text": "Als eine Reichstags-Coalition.", "tokens": ["Als", "ei\u00b7ne", "Reichs\u00b7tags\u00b7Coali\u00b7ti\u00b7on", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Sie sangen ja den zweiten Psalm!\u00ab \u00bbWoher", "tokens": ["Sie", "san\u00b7gen", "ja", "den", "zwei\u00b7ten", "Psalm", "!", "\u00ab", "\u00bb", "Wo\u00b7her"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$.", "$(", "$(", "PWAV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.24": {"text": "Es stamme,\u00ab sprach der Informator, \u00bbfremd", "tokens": ["Es", "stam\u00b7me", ",", "\u00ab", "sprach", "der", "In\u00b7for\u00b7ma\u00b7tor", ",", "\u00bb", "fremd"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "$(", "VVFIN", "ART", "NN", "$,", "$(", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.25": {"text": "Ist es und tauget nicht. Sonst nannte man's", "tokens": ["Ist", "es", "und", "tau\u00b7get", "nicht", ".", "Sonst", "nann\u00b7te", "man's"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "KON", "VVFIN", "PTKNEG", "$.", "ADV", "VVFIN", "PIS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.26": {"text": "Es hie\u00df auch ", "tokens": ["Es", "hie\u00df", "auch"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.27": {"text": "Die Allianz. Doch das ", "tokens": ["Die", "Al\u00b7li\u00b7anz", ".", "Doch", "das"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$.", "KON", "PDS"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.28": {"text": "Ich schlug das Buch der Richter auf, wie B\u00e4ume", "tokens": ["Ich", "schlug", "das", "Buch", "der", "Rich\u00b7ter", "auf", ",", "wie", "B\u00e4u\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "PTKVZ", "$,", "PWAV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.29": {"text": "Sich um die Allianz und Monarchie", "tokens": ["Sich", "um", "die", "Al\u00b7li\u00b7anz", "und", "Mon\u00b7ar\u00b7chie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Besprachen: \u00bbSoll ich meinen s\u00fc\u00dfen Most", "tokens": ["Be\u00b7spra\u00b7chen", ":", "\u00bb", "Soll", "ich", "mei\u00b7nen", "s\u00fc\u00b7\u00dfen", "Most"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "$(", "VMFIN", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.31": {"text": "Aufgeben?\u00ab sprach der Weinstock. \u00bbUnd soll ich", "tokens": ["Auf\u00b7ge\u00b7ben", "?", "\u00ab", "sprach", "der", "Wein\u00b7stock", ".", "\u00bb", "Und", "soll", "ich"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NN", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "KON", "VMFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.32": {"text": "Aus meiner Wurzel treten, da\u00df ich mich", "tokens": ["Aus", "mei\u00b7ner", "Wur\u00b7zel", "tre\u00b7ten", ",", "da\u00df", "ich", "mich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$,", "KOUS", "PPER", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.33": {"text": "Coalisire?\u00ab sprach die Ceder. \u00bbSchlage", "tokens": ["Coa\u00b7li\u00b7si\u00b7re", "?", "\u00ab", "sprach", "die", "Ce\u00b7der", ".", "\u00bb", "Schla\u00b7ge"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["NE", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.34": {"text": "Der Herr nur den Propheten Daniel", "tokens": ["Der", "Herr", "nur", "den", "Pro\u00b7phe\u00b7ten", "Da\u00b7ni\u00b7el"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "ART", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.35": {"text": "Und Esra sammt der Offenbarung auf!", "tokens": ["Und", "Es\u00b7ra", "sammt", "der", "Of\u00b7fen\u00b7ba\u00b7rung", "auf", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.36": {"text": "Da findet er so manches sch\u00f6ne Bild", "tokens": ["Da", "fin\u00b7det", "er", "so", "man\u00b7ches", "sch\u00f6\u00b7ne", "Bild"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.37": {"text": "Coalisirter M\u00e4chte: Adler, Leu", "tokens": ["Coa\u00b7lis\u00b7ir\u00b7ter", "M\u00e4ch\u00b7te", ":", "Ad\u00b7ler", ",", "Leu"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["ADJA", "NN", "$.", "NN", "$,", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.38": {"text": "Und Lamm und Greif; es giebt ein sch\u00f6nes Kupfer!\u00ab", "tokens": ["Und", "Lamm", "und", "Greif", ";", "es", "giebt", "ein", "sch\u00f6\u00b7nes", "Kup\u00b7fer", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "NN", "KON", "NN", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.39": {"text": "Die gn\u00e4d'ge Tante sprach's. \u00bbVerzeihung!\u00ab bat", "tokens": ["Die", "gn\u00e4d'\u00b7ge", "Tan\u00b7te", "sprach'", "s.", "\u00bb", "Ver\u00b7zei\u00b7hung", "!", "\u00ab", "bat"], "token_info": ["word", "word", "word", "word", "abbreviation", "punct", "word", "punct", "punct", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "NE", "$(", "NN", "$.", "$(", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.40": {"text": "Ein stattlicher Notarius; \u00bballhier", "tokens": ["Ein", "statt\u00b7li\u00b7cher", "No\u00b7ta\u00b7rius", ";", "\u00bb", "all\u00b7hier"], "token_info": ["word", "word", "word", "punct", "punct", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.41": {"text": "Gilt nicht die Bibel. In ", "tokens": ["Gilt", "nicht", "die", "Bi\u00b7bel", ".", "In"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PTKNEG", "ART", "NN", "$.", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.42": {"text": "Entscheiden wir; wir sind ", "tokens": ["Ent\u00b7schei\u00b7den", "wir", ";", "wir", "sind"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "$.", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.43": {"text": "\u00bbso lange darfst Du Deines Landes Baum", "tokens": ["\u00bb", "so", "lan\u00b7ge", "darfst", "Du", "Dei\u00b7nes", "Lan\u00b7des", "Baum"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "ADV", "VMFIN", "PPER", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.44": {"text": "Und Kruste von dem Meinigen zur\u00fcck-", "tokens": ["Und", "Krus\u00b7te", "von", "dem", "Mei\u00b7ni\u00b7gen", "zu\u00b7r\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "APPR", "ART", "PPOSS", "TRUNC"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.45": {"text": "Begehren, als sie mit dem Boden noch", "tokens": ["Be\u00b7geh\u00b7ren", ",", "als", "sie", "mit", "dem", "Bo\u00b7den", "noch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUS", "PPER", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.46": {"text": "Nicht ", "tokens": ["Nicht"], "token_info": ["word"], "pos": ["PTKNEG"], "meter": "+", "measure": "single.up"}, "line.47": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.48": {"text": "\u00bbund gar ", "tokens": ["\u00bb", "und", "gar"], "token_info": ["punct", "word", "word"], "pos": ["$(", "KON", "ADV"], "meter": "-+", "measure": "iambic.single"}, "line.49": {"text": "Bestimmt!\u00ab sprach ein Geheimer Rath; \u00bbdie Kruste,", "tokens": ["Be\u00b7stimmt", "!", "\u00ab", "sprach", "ein", "Ge\u00b7hei\u00b7mer", "Rath", ";", "\u00bb", "die", "Krus\u00b7te", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["VVPP", "$.", "$(", "VVFIN", "ART", "ADJA", "NN", "$.", "$(", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.50": {"text": "Der Baum ", "tokens": ["Der", "Baum"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.51": {"text": "So hei\u00dft es ", "tokens": ["So", "hei\u00dft", "es"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.52": {"text": "Ein B\u00fcndni\u00df, hei\u00dft's ", "tokens": ["Ein", "B\u00fcn\u00b7dni\u00df", ",", "hei\u00dft's"], "token_info": ["word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "NE"], "meter": "-+-+", "measure": "iambic.di"}, "line.53": {"text": "Coalisiren Cabinette sich,", "tokens": ["Coa\u00b7li\u00b7si\u00b7ren", "Ca\u00b7bi\u00b7net\u00b7te", "sich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PRF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.54": {"text": "So folgt darauf ", "tokens": ["So", "folgt", "da\u00b7rauf"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PAV"], "meter": "-+-+", "measure": "iambic.di"}, "line.55": {"text": "Der fremden Erdenkruste ", "tokens": ["Der", "frem\u00b7den", "Er\u00b7den\u00b7krus\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.56": {"text": "Ein angenehmer Actus.\u00ab Endlich ward", "tokens": ["Ein", "an\u00b7ge\u00b7neh\u00b7mer", "Ac\u00b7tus", ".", "\u00ab", "End\u00b7lich", "ward"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV", "VAFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.57": {"text": "Dem Herrn des Hauses dieser Tummelplatz", "tokens": ["Dem", "Herrn", "des", "Hau\u00b7ses", "die\u00b7ser", "Tum\u00b7mel\u00b7platz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "PDAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.58": {"text": "Zu eng. \u00bbIch d\u00e4chte, Jedermann von uns", "tokens": ["Zu", "eng", ".", "\u00bb", "Ich", "d\u00e4ch\u00b7te", ",", "Je\u00b7der\u00b7mann", "von", "uns"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKA", "ADJD", "$.", "$(", "PPER", "VVFIN", "$,", "PIS", "APPR", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.59": {"text": "Coalescirt' und coalirte nur", "tokens": ["Coale\u00b7scirt'", "und", "coa\u00b7lir\u00b7te", "nur"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "VVFIN", "ADV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.60": {"text": "Zuerst mit sich und seiner Kruste.\u00ab \u00bbDas", "tokens": ["Zu\u00b7erst", "mit", "sich", "und", "sei\u00b7ner", "Krus\u00b7te", ".", "\u00ab", "\u00bb", "Das"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word"], "pos": ["ADV", "APPR", "PRF", "KON", "PPOSAT", "NN", "$.", "$(", "$(", "ART"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.61": {"text": "Ist's eben, gn\u00e4d'ger Herr,\u00ab sprach ein Statist-", "tokens": ["Ist's", "e\u00b7ben", ",", "gn\u00e4d'\u00b7ger", "Herr", ",", "\u00ab", "sprach", "ein", "Sta\u00b7tist"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["NE", "ADV", "$,", "ADJA", "NN", "$,", "$(", "VVFIN", "ART", "TRUNC"], "meter": "-+-+-++-+-", "measure": "unknown.measure.penta"}, "line.62": {"text": "Iker, der ", "tokens": ["I\u00b7ker", ",", "der"], "token_info": ["word", "punct", "word"], "pos": ["NE", "$,", "PRELS"], "meter": "+--", "measure": "dactylic.init"}, "line.63": {"text": "Geleget hatte. \u00bbAls vor Jahren ich", "tokens": ["Ge\u00b7le\u00b7get", "hat\u00b7te", ".", "\u00bb", "Als", "vor", "Jah\u00b7ren", "ich"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$.", "$(", "KOUS", "APPR", "NN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.64": {"text": "Mit meinem jungen Herrn auf Reisen war,", "tokens": ["Mit", "mei\u00b7nem", "jun\u00b7gen", "Herrn", "auf", "Rei\u00b7sen", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "APPR", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.65": {"text": "Da fiel mir auf der letzten Station", "tokens": ["Da", "fiel", "mir", "auf", "der", "letz\u00b7ten", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.66": {"text": "In Frankreich an der Grenze schwer es auf,", "tokens": ["In", "Fran\u00b7kreich", "an", "der", "Gren\u00b7ze", "schwer", "es", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "ADJD", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.67": {"text": "Wie Alles dort so bald coalescire.", "tokens": ["Wie", "Al\u00b7les", "dort", "so", "bald", "coa\u00b7le\u00b7sci\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADV", "ADV", "ADV", "NE", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.68": {"text": "Vor wenig Jahren waren Hennegau", "tokens": ["Vor", "we\u00b7nig", "Jah\u00b7ren", "wa\u00b7ren", "Hen\u00b7ne\u00b7gau"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.69": {"text": "Und Flandern fl\u00e4misch, Lothringen war deutsch;", "tokens": ["Und", "Flan\u00b7dern", "fl\u00e4\u00b7misch", ",", "Loth\u00b7rin\u00b7gen", "war", "deutsch", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJD", "$,", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.70": {"text": "Und jetzt ist bis zur letzten Station", "tokens": ["Und", "jetzt", "ist", "bis", "zur", "letz\u00b7ten", "Sta\u00b7ti\u00b7on"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.71": {"text": "Alles franz\u00f6sisch, um- und umgewandt,", "tokens": ["Al\u00b7les", "fran\u00b7z\u00f6\u00b7sisch", ",", "um", "und", "um\u00b7ge\u00b7wandt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "$,", "TRUNC", "KON", "VVPP", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.72": {"text": "Bekleibet, neugeschaffen, coalirt.\u00ab", "tokens": ["Be\u00b7klei\u00b7bet", ",", "neu\u00b7ge\u00b7schaf\u00b7fen", ",", "coa\u00b7lirt", ".", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["VVFIN", "$,", "ADJA", "$,", "VVPP", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.73": {"text": "Und dicht daneben h\u00e4ngt, an Wulst und Leib", "tokens": ["Und", "dicht", "da\u00b7ne\u00b7ben", "h\u00e4ngt", ",", "an", "Wulst", "und", "Leib"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "PAV", "VVFIN", "$,", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.74": {"text": "Und Sprach' und Sitten gleich, das Brabant an,", "tokens": ["Und", "Sprach'", "und", "Sit\u00b7ten", "gleich", ",", "das", "Bra\u00b7bant", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "ADV", "$,", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.75": {"text": "Das Deutschland! \u00bbWie ", "tokens": ["Das", "Deutschland", "!", "\u00bb", "Wie"], "token_info": ["word", "word", "punct", "punct", "word"], "pos": ["ART", "NN", "$.", "$(", "PWAV"], "meter": "+-+", "measure": "trochaic.di"}, "line.76": {"text": "Fragt' ich mich selbst, \u00bbund wie ", "tokens": ["Fragt'", "ich", "mich", "selbst", ",", "\u00bb", "und", "wie"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "$,", "$(", "KON", "PWAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.77": {"text": "Es sich Provinzen, die's incorporirt?", "tokens": ["Es", "sich", "Pro\u00b7vin\u00b7zen", ",", "die's", "in\u00b7cor\u00b7po\u00b7rirt", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "PRF", "NN", "$,", "FM.la", "FM.la", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.78": {"text": "Ein schweres Staatsproblem!\u00ab Hier sehen Sie", "tokens": ["Ein", "schwe\u00b7res", "Staats\u00b7prob\u00b7lem", "!", "\u00ab", "Hier", "se\u00b7hen", "Sie"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "$(", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.79": {"text": "Die gro\u00dfe L\u00e4nderkarte. Ostw\u00e4rts dort", "tokens": ["Die", "gro\u00b7\u00dfe", "L\u00e4n\u00b7der\u00b7kar\u00b7te", ".", "Ost\u00b7w\u00e4rts", "dort"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "ADV", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.80": {"text": "Das ungeheure Kaiserthum Gro\u00df-Tschni,", "tokens": ["Das", "un\u00b7ge\u00b7heu\u00b7re", "Kai\u00b7ser\u00b7thum", "Gro\u00df\u00b7\u00b7Tschni", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.81": {"text": "Tschong-Ku, Tschong-Hoa! Leider nennen wir's", "tokens": ["Tschong\u00b7Ku", ",", "Tschong\u00b7Hoa", "!", "Lei\u00b7der", "nen\u00b7nen", "wir's"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "NE", "$.", "NN", "VVINF", "VAFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.82": {"text": "Mit falschem Namen ", "tokens": ["Mit", "fal\u00b7schem", "Na\u00b7men"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.83": {"text": "Mit seinen tausendundvierhundertzwei-", "tokens": ["Mit", "sei\u00b7nen", "tau\u00b7send\u00b7und\u00b7vier\u00b7hun\u00b7dertzwei"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "TRUNC"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.84": {"text": "Undvierzig Str\u00f6men, vielen Br\u00fccken und", "tokens": ["Und\u00b7vier\u00b7zig", "Str\u00f6\u00b7men", ",", "vie\u00b7len", "Br\u00fc\u00b7cken", "und"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "NN", "$,", "PIAT", "NN", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.85": {"text": "Zweitausend Bergen, hundertneunundvierzig", "tokens": ["Zweit\u00b7au\u00b7send", "Ber\u00b7gen", ",", "hun\u00b7dert\u00b7neun\u00b7und\u00b7vier\u00b7zig"], "token_info": ["word", "word", "punct", "word"], "pos": ["CARD", "NN", "$,", "CARD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.86": {"text": "Millionen und sechshundertzweiundsechigtausend Menschen,", "tokens": ["Mil\u00b7lion\u00b7en", "und", "sechs\u00b7hun\u00b7dert\u00b7zwei\u00b7und\u00b7se\u00b7chig\u00b7tau\u00b7send", "Men\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "CARD", "NN", "$,"], "meter": "-+--+--+-+-+-+-", "measure": "amphibrach.tri.plus"}, "line.87": {"text": "Dort von der Mauer bis nach Canton zu,", "tokens": ["Dort", "von", "der", "Mau\u00b7er", "bis", "nach", "Can\u00b7ton", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "APPR", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.88": {"text": "Ja bis nach Lao-Tschua, Cotschin-Tschina,", "tokens": ["Ja", "bis", "nach", "Lao\u00b7\u00b7T\u00b7schua", ",", "Cot\u00b7schin\u00b7\u00b7T\u00b7schi\u00b7na", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PTKANT", "KON", "APPR", "NE", "$,", "NE", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.89": {"text": "Cambotscha, Tunkin, ist, wie ", "tokens": ["Cam\u00b7bot\u00b7scha", ",", "Tun\u00b7kin", ",", "ist", ",", "wie"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "NE", "$,", "VAFIN", "$,", "PWAV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.90": {"text": "Mit seinem Boden trefflich coalirt.", "tokens": ["Mit", "sei\u00b7nem", "Bo\u00b7den", "treff\u00b7lich", "coa\u00b7lirt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.91": {"text": "Ein jeder Mandarin hat seinen Platz", "tokens": ["Ein", "je\u00b7der", "Man\u00b7da\u00b7rin", "hat", "sei\u00b7nen", "Platz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.92": {"text": "Und seine Feder. Kommt ein fremder Lord,", "tokens": ["Und", "sei\u00b7ne", "Fe\u00b7der", ".", "Kommt", "ein", "frem\u00b7der", "Lord", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$.", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.93": {"text": "Mit Freudenfeuern f\u00fchrt man ihn hinein,", "tokens": ["Mit", "Freu\u00b7den\u00b7feu\u00b7ern", "f\u00fchrt", "man", "ihn", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.94": {"text": "Und bald hinaus, da\u00df er nicht coalire.", "tokens": ["Und", "bald", "hin\u00b7aus", ",", "da\u00df", "er", "nicht", "coa\u00b7li\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APZR", "$,", "KOUS", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.95": {"text": "Dagegen Hindostan, das arme Land,", "tokens": ["Da\u00b7ge\u00b7gen", "Hin\u00b7dos\u00b7tan", ",", "das", "ar\u00b7me", "Land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.96": {"text": "Ist elend coalirt. Bramanen, Schattri,", "tokens": ["Ist", "e\u00b7lend", "coa\u00b7lirt", ".", "Bra\u00b7ma\u00b7nen", ",", "Schat\u00b7tri", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$.", "NN", "$,", "NE", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.97": {"text": "Banjanen, Schutter, und die Fremden gar,", "tokens": ["Ban\u00b7ja\u00b7nen", ",", "Schut\u00b7ter", ",", "und", "die", "Frem\u00b7den", "gar", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "KON", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.98": {"text": "Seiken, Dschaten, Gebern und Afghanen,", "tokens": ["Sei\u00b7ken", ",", "Dscha\u00b7ten", ",", "Ge\u00b7bern", "und", "Af\u00b7gha\u00b7nen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.99": {"text": "Mongolen, Juden, Perser, Araber,", "tokens": ["Mon\u00b7go\u00b7len", ",", "Ju\u00b7den", ",", "Per\u00b7ser", ",", "A\u00b7ra\u00b7ber", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "---+-+--+-", "measure": "iambic.tri.relaxed"}, "line.100": {"text": "Und Europ\u00e4er aller Art, Maratten,", "tokens": ["Und", "Eu\u00b7ro\u00b7p\u00e4\u00b7er", "al\u00b7ler", "Art", ",", "Ma\u00b7rat\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "PIAT", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.101": {"text": "Rasbatten; darum geht's den guten Hindus", "tokens": ["Ras\u00b7bat\u00b7ten", ";", "da\u00b7rum", "geht's", "den", "gu\u00b7ten", "Hin\u00b7dus"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "PAV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.102": {"text": "Auch so erb\u00e4rmlich. \u2013 Nun spazieren Sie", "tokens": ["Auch", "so", "er\u00b7b\u00e4rm\u00b7lich", ".", "\u2013", "Nun", "spa\u00b7zie\u00b7ren", "Sie"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "$.", "$(", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.103": {"text": "Von den Fuchsinseln bis nach Kexholm hin;", "tokens": ["Von", "den", "Fuchs\u00b7in\u00b7seln", "bis", "nach", "Kex\u00b7holm", "hin", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "APPR", "NE", "PTKVZ", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.104": {"text": "Wie h\u00e4ngt's zusammen! Samojeden und", "tokens": ["Wie", "h\u00e4ngt's", "zu\u00b7sam\u00b7men", "!", "Sa\u00b7mo\u00b7je\u00b7den", "und"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VVFIN", "PTKVZ", "$.", "NN", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.105": {"text": "Tungusen, Tatern, Kamtschadalen; da", "tokens": ["Tun\u00b7gu\u00b7sen", ",", "Ta\u00b7tern", ",", "Kamt\u00b7scha\u00b7da\u00b7len", ";", "da"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "$.", "KOUS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.106": {"text": "Lebt Jeder, wie er will, wenn er nur Pelze", "tokens": ["Lebt", "Je\u00b7der", ",", "wie", "er", "will", ",", "wenn", "er", "nur", "Pel\u00b7ze"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "$,", "PWAV", "PPER", "VMFIN", "$,", "KOUS", "PPER", "ADV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.107": {"text": "Und seinen Rubel giebt. \u2013 Das arme Polen,", "tokens": ["Und", "sei\u00b7nen", "Ru\u00b7bel", "giebt", ".", "\u2013", "Das", "ar\u00b7me", "Po\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "$(", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.108": {"text": "Warum denn ward's zertheilt? Es war mit sich", "tokens": ["Wa\u00b7rum", "denn", "ward's", "zer\u00b7theilt", "?", "Es", "war", "mit", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "VVPP", "$.", "PPER", "VAFIN", "APPR", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.109": {"text": "Nicht coalirt; drum schnitt man es entzwei;", "tokens": ["Nicht", "coa\u00b7lirt", ";", "drum", "schnitt", "man", "es", "ent\u00b7zwei", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$.", "PAV", "VVFIN", "PIS", "PPER", "PTKVZ", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.110": {"text": "Nun wachsen seine St\u00fccke neu und frisch", "tokens": ["Nun", "wach\u00b7sen", "sei\u00b7ne", "St\u00fc\u00b7cke", "neu", "und", "frisch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.111": {"text": "Zusammen durch die Cur der Sympathie.", "tokens": ["Zu\u00b7sam\u00b7men", "durch", "die", "Cur", "der", "Sym\u00b7pa\u00b7thie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.112": {"text": "\u00bbdas gro\u00dfe Deutschland (warum liegt es doch", "tokens": ["\u00bb", "das", "gro\u00b7\u00dfe", "Deutschland", "(", "wa\u00b7rum", "liegt", "es", "doch"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$(", "PWAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.113": {"text": "So nah an Polen?), Holland, Engeland", "tokens": ["So", "nah", "an", "Po\u00b7len", "?", ")", ",", "Hol\u00b7land", ",", "En\u00b7ge\u00b7land"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "punct", "word"], "pos": ["ADV", "ADJD", "APPR", "NE", "$.", "$(", "$,", "NE", "$,", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.114": {"text": "Mit Schottland, Irland, Caledonien,", "tokens": ["Mit", "Schott\u00b7land", ",", "Ir\u00b7land", ",", "Ca\u00b7le\u00b7do\u00b7ni\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.115": {"text": "Italien und Griechenland, T\u00fcrkei", "tokens": ["I\u00b7ta\u00b7li\u00b7en", "und", "Grie\u00b7chen\u00b7land", ",", "T\u00fcr\u00b7kei"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NE", "KON", "NE", "$,", "NN"], "meter": "+--+-+-+++", "measure": "iambic.hexa.invert"}, "line.116": {"text": "Und Walachei und Moldau \u2013\u00ab \u00bbIst's denn noch", "tokens": ["Und", "Wa\u00b7la\u00b7chei", "und", "Mol\u00b7dau", "\u2013", "\u00ab", "\u00bb", "Ist's", "denn", "noch"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NE", "$(", "$(", "$(", "NE", "KON", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.117": {"text": "Nicht aus?\u00ab rief der Baron. \u2013 \u00bbDas Beste kommt", "tokens": ["Nicht", "aus", "?", "\u00ab", "rief", "der", "Ba\u00b7ron", ".", "\u2013", "\u00bb", "Das", "Bes\u00b7te", "kommt"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word"], "pos": ["PTKNEG", "PTKVZ", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "$(", "ART", "NN", "VVFIN"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.118": {"text": "Anjetzt. \u2013 Nun treten Sie in Frankreich ein!", "tokens": ["An\u00b7jetzt", ".", "\u2013", "Nun", "tre\u00b7ten", "Sie", "in", "Fran\u00b7kreich", "ein", "!"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "$(", "ADV", "VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.119": {"text": "Da weht franz\u00f6sische Luft; da essen sie", "tokens": ["Da", "weht", "fran\u00b7z\u00f6\u00b7si\u00b7sche", "Luft", ";", "da", "es\u00b7sen", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "$.", "ADV", "VVFIN", "PPER"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.120": {"text": "Und trinken, jauchzen, reden, singen ganz", "tokens": ["Und", "trin\u00b7ken", ",", "jauch\u00b7zen", ",", "re\u00b7den", ",", "sin\u00b7gen", "ganz"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "$,", "VVINF", "$,", "VVINF", "$,", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.121": {"text": "Franz\u00f6sisch. Schon das Kind in Mutterleib,", "tokens": ["Fran\u00b7z\u00f6\u00b7sisch", ".", "Schon", "das", "Kind", "in", "Mut\u00b7ter\u00b7leib", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ADV", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.122": {"text": "Ich glaub', es denkt und spricht franz\u00f6sisch. Selbst", "tokens": ["Ich", "glaub'", ",", "es", "denkt", "und", "spricht", "fran\u00b7z\u00f6\u00b7sisch", ".", "Selbst"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "KON", "VVFIN", "ADJD", "$.", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.123": {"text": "Latein und Griech'sch spricht man franz\u00f6sisch aus,", "tokens": ["La\u00b7tein", "und", "Griech'sch", "spricht", "man", "fran\u00b7z\u00f6\u00b7sisch", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "PIS", "ADJD", "PTKVZ", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.124": {"text": "Und Alles mit Geschmack. Sie ziehn den Fremden", "tokens": ["Und", "Al\u00b7les", "mit", "Ge\u00b7schmack", ".", "Sie", "ziehn", "den", "Frem\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIS", "APPR", "NN", "$.", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.125": {"text": "So an sich, da\u00df er mit coalescirt.", "tokens": ["So", "an", "sich", ",", "da\u00df", "er", "mit", "coa\u00b7le\u00b7scirt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRF", "$,", "KOUS", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.126": {"text": "Oft hab' ich dran gedacht, warum denn Griechen", "tokens": ["Oft", "hab'", "ich", "dran", "ge\u00b7dacht", ",", "wa\u00b7rum", "denn", "Grie\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "VVPP", "$,", "PWAV", "ADV", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.127": {"text": "Und R\u00f6mer auch nicht so zusammenwuchsen.", "tokens": ["Und", "R\u00f6\u00b7mer", "auch", "nicht", "so", "zu\u00b7sam\u00b7men\u00b7wuch\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.128": {"text": "Was half den Griechen ihr Ach\u00e4erbund,", "tokens": ["Was", "half", "den", "Grie\u00b7chen", "ihr", "A\u00b7ch\u00e4er\u00b7bund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.129": {"text": "Ihr Panionium, Amphiktyonenhof,", "tokens": ["Ihr", "Pa\u00b7ni\u00b7o\u00b7ni\u00b7um", ",", "Am\u00b7phik\u00b7ty\u00b7on\u00b7en\u00b7hof", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Ihr Pan\u00e4tolium? Was halfen den", "tokens": ["Ihr", "Pa\u00b7n\u00e4\u00b7to\u00b7li\u00b7um", "?", "Was", "hal\u00b7fen", "den"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$.", "PWS", "VVFIN", "ART"], "meter": "+-+--+-+--", "measure": "trochaic.tetra.relaxed"}, "line.131": {"text": "Etruriern die Lucumonen? was", "tokens": ["E\u00b7tru\u00b7ri\u00b7ern", "die", "Lu\u00b7cu\u00b7mo\u00b7nen", "?", "was"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ADV", "ART", "NN", "$.", "PWS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.132": {"text": "Den R\u00f6mern ihr ", "tokens": ["Den", "R\u00f6\u00b7mern", "ihr"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "PPOSAT"], "meter": "-+-+", "measure": "iambic.di"}, "line.133": {"text": "Den Celtiberiern \u2013\u00ab \u00bbIst's noch nicht aus?", "tokens": ["Den", "Cel\u00b7ti\u00b7be\u00b7ri\u00b7ern", "\u2013", "\u00ab", "\u00bb", "Ist's", "noch", "nicht", "aus", "?"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "$(", "$(", "NE", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.134": {"text": "Da seh' der Herr die sieben Pfeile auf", "tokens": ["Da", "seh'", "der", "Herr", "die", "sie\u00b7ben", "Pfei\u00b7le", "auf"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.135": {"text": "Holl\u00e4ndischen Ducaten mit der Aufschrift", "tokens": ["Hol\u00b7l\u00e4n\u00b7di\u00b7schen", "Du\u00b7ca\u00b7ten", "mit", "der", "Auf\u00b7schrift"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.136": {"text": "Im Golde des Ducaten coalirt!\u00ab", "tokens": ["Im", "Gol\u00b7de", "des", "Du\u00b7ca\u00b7ten", "coa\u00b7lirt", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.137": {"text": "\u00bbnun so coalisir' Er denn!\u00ab \u00bbEr wird,\u00ab", "tokens": ["\u00bb", "nun", "so", "coali\u00b7sir'", "Er", "denn", "!", "\u00ab", "\u00bb", "Er", "wird", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADV", "VVFIN", "PPER", "ADV", "$.", "$(", "$(", "PPER", "VAFIN", "$,", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.138": {"text": "Antwortete der Arzt, der bis dahin", "tokens": ["Ant\u00b7wor\u00b7te\u00b7te", "der", "Arzt", ",", "der", "bis", "da\u00b7hin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "PRELS", "APPR", "PAV"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.139": {"text": "Geschwiegen hatte, \u00bbjetzt erz\u00e4hlen, wie", "tokens": ["Ge\u00b7schwie\u00b7gen", "hat\u00b7te", ",", "\u00bb", "jetzt", "er\u00b7z\u00e4h\u00b7len", ",", "wie"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word"], "pos": ["VVPP", "VAFIN", "$,", "$(", "ADV", "VVINF", "$,", "PWAV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.140": {"text": "Man die in Eins Gewachsenen curirt.", "tokens": ["Man", "die", "in", "Eins", "Ge\u00b7wach\u00b7se\u00b7nen", "cu\u00b7rirt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.141": {"text": "Dem Einen Schnupftobak, der Andre niest;", "tokens": ["Dem", "Ei\u00b7nen", "Schnupf\u00b7to\u00b7bak", ",", "der", "And\u00b7re", "niest", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "$,", "PRELS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.142": {"text": "Purgirt den Einen \u2013 denn wie Haller sagt,", "tokens": ["Pur\u00b7girt", "den", "Ei\u00b7nen", "\u2013", "denn", "wie", "Hal\u00b7ler", "sagt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ART", "$(", "KON", "PWAV", "NE", "VVFIN", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.143": {"text": "Kommt's bei in Eins Gewachsnen nicht auf K\u00f6pfe", "tokens": ["Kommt's", "bei", "in", "Eins", "Ge\u00b7wachs\u00b7nen", "nicht", "auf", "K\u00f6p\u00b7fe"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "APPR", "ART", "NN", "PTKNEG", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.144": {"text": "Und M\u00e4gen an, sie sind ", "tokens": ["Und", "M\u00e4\u00b7gen", "an", ",", "sie", "sind"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KON", "NN", "PTKVZ", "$,", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.145": {"text": "\u00bbnicht also!\u00ab sprach ein Casuist; \u00bbnach ", "tokens": ["\u00bb", "nicht", "al\u00b7so", "!", "\u00ab", "sprach", "ein", "Ca\u00b7su\u00b7ist", ";", "\u00bb", "nach"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word"], "pos": ["$(", "PTKNEG", "ADV", "$.", "$(", "VVFIN", "ART", "NN", "$.", "$(", "APPR"], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.146": {"text": "Wird ein ", "tokens": ["Wird", "ein"], "token_info": ["word", "word"], "pos": ["VAFIN", "ART"], "meter": "-+", "measure": "iambic.single"}, "line.147": {"text": "Da viel zu herzen?\u00ab Der Baron", "tokens": ["Da", "viel", "zu", "her\u00b7zen", "?", "\u00ab", "Der", "Ba\u00b7ron"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["KOUS", "PIS", "PTKZU", "VVINF", "$.", "$(", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.148": {"text": "War dieses Streites m\u00fcde. \u00bbSeht, Ihr Herrn,", "tokens": ["War", "die\u00b7ses", "Strei\u00b7tes", "m\u00fc\u00b7de", ".", "\u00bb", "Seht", ",", "Ihr", "Herrn", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADJD", "$.", "$(", "VVFIN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.149": {"text": "Ihr selber seid in Euern Meinungen,", "tokens": ["Ihr", "sel\u00b7ber", "seid", "in", "Eu\u00b7ern", "Mei\u00b7nun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.150": {"text": "Ein Wort betreffend, weder coalirt,", "tokens": ["Ein", "Wort", "be\u00b7tref\u00b7fend", ",", "we\u00b7der", "coa\u00b7lirt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "KON", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.151": {"text": "Noch wollt Ihr Euch coalisiren; und", "tokens": ["Noch", "wollt", "Ihr", "Euch", "coa\u00b7li\u00b7si\u00b7ren", ";", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "VVINF", "$.", "KON"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.152": {"text": "Coalisirt die Welt? Nutzlose M\u00fch!", "tokens": ["Coa\u00b7li\u00b7sirt", "die", "Welt", "?", "Nutz\u00b7lo\u00b7se", "M\u00fch", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$.", "NE", "NE", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.153": {"text": "Sei Jeder erst mit seinem Stand und Land", "tokens": ["Sei", "Je\u00b7der", "erst", "mit", "sei\u00b7nem", "Stand", "und", "Land"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.154": {"text": "Und Haus und Hof und Weib und Kind und Amt", "tokens": ["Und", "Haus", "und", "Hof", "und", "Weib", "und", "Kind", "und", "Amt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "KON", "NN", "KON", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.155": {"text": "Und Pflicht, ja mit sich selbst recht coalirt;", "tokens": ["Und", "Pflicht", ",", "ja", "mit", "sich", "selbst", "recht", "coa\u00b7lirt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "ADV", "APPR", "PRF", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.156": {"text": "Er wird Tschin-Tschin vergessen. Lerne doch,", "tokens": ["Er", "wird", "Tschin\u00b7\u00b7T\u00b7schin", "ver\u00b7ges\u00b7sen", ".", "Ler\u00b7ne", "doch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "VVPP", "$.", "NN", "ADV", "$,"], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.157": {"text": "Was Euch der Haushahn in der Fibel sagt,", "tokens": ["Was", "Euch", "der", "Haus\u00b7hahn", "in", "der", "Fi\u00b7bel", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.158": {"text": "Ein Jeder seine Lection, so steht", "tokens": ["Ein", "Je\u00b7der", "sei\u00b7ne", "Lec\u00b7ti\u00b7on", ",", "so", "steht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PIS", "PPOSAT", "NN", "$,", "ADV", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.159": {"text": "Es wohl in Hause, Stadt und Land und Welt.\u00ab \u2013", "tokens": ["Es", "wohl", "in", "Hau\u00b7se", ",", "Stadt", "und", "Land", "und", "Welt", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "ADV", "APPR", "NN", "$,", "NN", "KON", "NN", "KON", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.160": {"text": "Sieh, Freund, so spricht die deutsche Politik", "tokens": ["Sieh", ",", "Freund", ",", "so", "spricht", "die", "deut\u00b7sche", "Po\u00b7li\u00b7tik"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$,", "NN", "$,", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.161": {"text": "Vom Fernsten immer und vom Weitesten,", "tokens": ["Vom", "Ferns\u00b7ten", "im\u00b7mer", "und", "vom", "Wei\u00b7tes\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "KON", "APPRART", "NN", "$,"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.162": {"text": "Nur nicht von sich. Und lohnt es wol der M\u00fch,", "tokens": ["Nur", "nicht", "von", "sich", ".", "Und", "lohnt", "es", "wol", "der", "M\u00fch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "APPR", "PRF", "$.", "KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.163": {"text": "Die Musen mit dem Wuste zu entweihn?", "tokens": ["Die", "Mu\u00b7sen", "mit", "dem", "Wus\u00b7te", "zu", "ent\u00b7weihn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.164": {"text": "Verbannt aus Deutschland ist die Politik;", "tokens": ["Ver\u00b7bannt", "aus", "Deutschland", "ist", "die", "Po\u00b7li\u00b7tik", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NE", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.165": {"text": "Verbannet sei nur nicht die Menschlichkeit!", "tokens": ["Ver\u00b7ban\u00b7net", "sei", "nur", "nicht", "die", "Menschlich\u00b7keit", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ADV", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}