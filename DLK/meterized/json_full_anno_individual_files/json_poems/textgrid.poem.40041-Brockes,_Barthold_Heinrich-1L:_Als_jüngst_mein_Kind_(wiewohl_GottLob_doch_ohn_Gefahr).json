{"textgrid.poem.40041": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: Als j\u00fcngst mein Kind (wiewohl GottLob doch ohn Gefahr)", "genre": "verse", "period": "N.A.", "pub_year": 1713, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Als j\u00fcngst mein Kind (wiewohl GottLob doch ohn Gefahr)", "tokens": ["Als", "j\u00fcngst", "mein", "Kind", "(", "wie\u00b7wohl", "Gott", "Lob", "doch", "ohn", "Ge\u00b7fahr", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "NN", "$(", "KOUS", "NN", "NN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch einen Fall, am Haupt verletzet war,", "tokens": ["Durch", "ei\u00b7nen", "Fall", ",", "am", "Haupt", "ver\u00b7let\u00b7zet", "war", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "APPRART", "NN", "VVFIN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "So, da\u00df der Wund-Artzt ihm ein' Oeffnung machen muste;", "tokens": ["So", ",", "da\u00df", "der", "Wun\u00b7dArtzt", "ihm", "ein'", "Oeff\u00b7nung", "ma\u00b7chen", "mus\u00b7te", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "PPER", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bek\u00fcmmert' es sich nicht, weil von dem Schmertz,", "tokens": ["Be\u00b7k\u00fcm\u00b7mert'", "es", "sich", "nicht", ",", "weil", "von", "dem", "Schmertz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "PTKNEG", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der es betreffen sollt', sein unbesorgtes Hertz", "tokens": ["Der", "es", "be\u00b7tref\u00b7fen", "sollt'", ",", "sein", "un\u00b7be\u00b7sorg\u00b7tes", "Hertz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "PPER", "VVINF", "VMFIN", "$,", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nicht das geringste wuste.", "tokens": ["Nicht", "das", "ge\u00b7rings\u00b7te", "wus\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Der Schnitt geschahe denn: Drauf fing es zwar", "tokens": ["Der", "Schnitt", "ge\u00b7scha\u00b7he", "denn", ":", "Drauf", "fing", "es", "zwar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "$.", "PAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Den Augenblick erb\u00e4rmlich an zu weinen;", "tokens": ["Den", "Au\u00b7gen\u00b7blick", "er\u00b7b\u00e4rm\u00b7lich", "an", "zu", "wei\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Allein es sahe kaum das Gold", "tokens": ["Al\u00b7lein", "es", "sa\u00b7he", "kaum", "das", "Gold"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Von einer Zucker-Puppe scheinen,", "tokens": ["Von", "ei\u00b7ner", "Zu\u00b7cke\u00b7rPup\u00b7pe", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Als es auch schon getr\u00f6stet war;", "tokens": ["Als", "es", "auch", "schon", "ge\u00b7tr\u00f6s\u00b7tet", "war", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Die Thr\u00e4nen waren eh, als noch das Blut, gestillt.", "tokens": ["Die", "Thr\u00e4\u00b7nen", "wa\u00b7ren", "eh", ",", "als", "noch", "das", "Blut", ",", "ge\u00b7stillt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "KOUS", "$,", "KOUS", "ADV", "ART", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Das schien mir nun ein Lehr-reich Bild.", "tokens": ["Das", "schien", "mir", "nun", "ein", "Lehr\u00b7reich", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn erstlich folgt daraus der Schlu\u00df,", "tokens": ["Denn", "erst\u00b7lich", "folgt", "da\u00b7raus", "der", "Schlu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PAV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df wir uns Kummer und Verdru\u00df,", "tokens": ["Da\u00df", "wir", "uns", "Kum\u00b7mer", "und", "Ver\u00b7dru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Anstatt durch Dencken sie zu mindern und zu bessern,", "tokens": ["An\u00b7statt", "durch", "Den\u00b7cken", "sie", "zu", "min\u00b7dern", "und", "zu", "bes\u00b7sern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "VVFIN", "PPER", "APPR", "ADJA", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Durch Dencken nur noch mehren und vergr\u00f6ssern.", "tokens": ["Durch", "Den\u00b7cken", "nur", "noch", "meh\u00b7ren", "und", "ver\u00b7gr\u00f6s\u00b7sern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "ADV", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Man zieht die Plagen und die Pein,", "tokens": ["Man", "zieht", "die", "Pla\u00b7gen", "und", "die", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Die noch entfernt und erst zuk\u00fcnftig seyn,", "tokens": ["Die", "noch", "ent\u00b7fernt", "und", "erst", "zu\u00b7k\u00fcnf\u00b7tig", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "KON", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Im Dencken schon voraus herbey.", "tokens": ["Im", "Den\u00b7cken", "schon", "vo\u00b7raus", "her\u00b7bey", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Die Phantasey ist stets besch\u00e4fftiget und fertig,", "tokens": ["Die", "Phan\u00b7ta\u00b7sey", "ist", "stets", "be\u00b7sch\u00e4ff\u00b7ti\u00b7get", "und", "fer\u00b7tig", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Damit ein fernes Leid uns gegenw\u00e4rtig", "tokens": ["Da\u00b7mit", "ein", "fer\u00b7nes", "Leid", "uns", "ge\u00b7gen\u00b7w\u00e4r\u00b7tig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "PPER", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Und, eh mans f\u00fchlet, f\u00fchlbar sey.", "tokens": ["Und", ",", "eh", "mans", "f\u00fch\u00b7let", ",", "f\u00fchl\u00b7bar", "sey", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PIS", "VVFIN", "$,", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Erweget denn, geliebte Menschen, doch,", "tokens": ["Er\u00b7we\u00b7get", "denn", ",", "ge\u00b7lieb\u00b7te", "Men\u00b7schen", ",", "doch", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "ADJA", "NN", "$,", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wie gl\u00fccklich wir in diesem Stande noch,", "tokens": ["Wie", "gl\u00fcck\u00b7lich", "wir", "in", "die\u00b7sem", "Stan\u00b7de", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "APPR", "PDAT", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und wie wir Gott daf\u00fcr von Hertzen dancken m\u00fcssen,", "tokens": ["Und", "wie", "wir", "Gott", "da\u00b7f\u00fcr", "von", "Hert\u00b7zen", "dan\u00b7cken", "m\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "NN", "PAV", "APPR", "NN", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df Er, nach Seinem weisen Rath,", "tokens": ["Da\u00df", "Er", ",", "nach", "Sei\u00b7nem", "wei\u00b7sen", "Rath", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Uns das, was noch nicht ist, verborgen hat,", "tokens": ["Uns", "das", ",", "was", "noch", "nicht", "ist", ",", "ver\u00b7bor\u00b7gen", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "PDS", "$,", "PRELS", "ADV", "PTKNEG", "VAFIN", "$,", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und wir vom K\u00fcnftigen nichts wissen!", "tokens": ["Und", "wir", "vom", "K\u00fcnf\u00b7ti\u00b7gen", "nichts", "wis\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Die Wohlthat ist f\u00fcrwahr weit gr\u00f6sser, als man meynet,", "tokens": ["Die", "Wohlt\u00b7hat", "ist", "f\u00fcr\u00b7wahr", "weit", "gr\u00f6s\u00b7ser", ",", "als", "man", "mey\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "$,", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und herrlicher, als sie beym ersten Anblick scheinet.", "tokens": ["Und", "herr\u00b7li\u00b7cher", ",", "als", "sie", "beym", "ers\u00b7ten", "An\u00b7blick", "schei\u00b7net", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "$,", "KOUS", "PPER", "APPRART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn w\u00fcsten wir ein k\u00fcnftigs Gl\u00fcck vorher;", "tokens": ["Denn", "w\u00fcs\u00b7ten", "wir", "ein", "k\u00fcnf\u00b7tigs", "Gl\u00fcck", "vor\u00b7her", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "So w\u00fcrden wir in steter Unruh seyn:", "tokens": ["So", "w\u00fcr\u00b7den", "wir", "in", "ste\u00b7ter", "Un\u00b7ruh", "seyn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "Ein jeder Augenblick", "tokens": ["Ein", "je\u00b7der", "Au\u00b7gen\u00b7blick"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "W\u00fcrd' uns ein Zag, ein Tag ein Jahr-lang, w\u00e4hren.", "tokens": ["W\u00fcrd'", "uns", "ein", "Zag", ",", "ein", "Tag", "ein", "Jahr\u00b7lang", ",", "w\u00e4h\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "$,", "ART", "NN", "ART", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Hingegen w\u00fcrd' ein k\u00fcnftigs Ungel\u00fcck", "tokens": ["Hin\u00b7ge\u00b7gen", "w\u00fcrd'", "ein", "k\u00fcnf\u00b7tigs", "Un\u00b7ge\u00b7l\u00fcck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "---+-+-+-+", "measure": "zehnsilber"}, "line.14": {"text": "Uns mit stets gegenw\u00e4rt'ger Pein,", "tokens": ["Uns", "mit", "stets", "ge\u00b7gen\u00b7w\u00e4rt'\u00b7ger", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Durch eine schwartze Furcht, beschweren.", "tokens": ["Durch", "ei\u00b7ne", "schwart\u00b7ze", "Furcht", ",", "be\u00b7schwe\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Von meines Kindes Fall war die\u00df die erste Lehre.", "tokens": ["Von", "mei\u00b7nes", "Kin\u00b7des", "Fall", "war", "die\u00df", "die", "ers\u00b7te", "Leh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VAFIN", "PDS", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Die andre folget itzt: So wie das Kind die Schmertzen", "tokens": ["Die", "and\u00b7re", "fol\u00b7get", "itzt", ":", "So", "wie", "das", "Kind", "die", "Schmert\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "ADV", "$.", "ADV", "KOKOM", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch einen Vorwurf, der ihm lieb,", "tokens": ["Durch", "ei\u00b7nen", "Vor\u00b7wurf", ",", "der", "ihm", "lieb", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADJD", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.3": {"text": "Aus seinem Hirn und Hertzen,", "tokens": ["Aus", "sei\u00b7nem", "Hirn", "und", "Hert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und folglich wirklich von sich, trieb;", "tokens": ["Und", "folg\u00b7lich", "wirk\u00b7lich", "von", "sich", ",", "trieb", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "APPR", "PRF", "$,", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So m\u00f6chten wir uns wohl mit aller Kraft", "tokens": ["So", "m\u00f6ch\u00b7ten", "wir", "uns", "wohl", "mit", "al\u00b7ler", "Kraft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und allem Ernst dahin bem\u00fchen,", "tokens": ["Und", "al\u00b7lem", "Ernst", "da\u00b7hin", "be\u00b7m\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "NN", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Uns durch die eine Leidenschaft", "tokens": ["Uns", "durch", "die", "ei\u00b7ne", "Lei\u00b7den\u00b7schaft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "ART", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der andern zu entziehen!", "tokens": ["Der", "an\u00b7dern", "zu", "ent\u00b7zie\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Ach da\u00df wir uns doch \u00e4ndern m\u00f6chten,", "tokens": ["Ach", "da\u00df", "wir", "uns", "doch", "\u00e4n\u00b7dern", "m\u00f6ch\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "KOUS", "PPER", "PRF", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wann es etwa wiedrig geht,", "tokens": ["Und", "wann", "es", "et\u00b7wa", "wied\u00b7rig", "geht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit Ernst auf etwas anders d\u00e4chten,", "tokens": ["Mit", "Ernst", "auf", "et\u00b7was", "an\u00b7ders", "d\u00e4ch\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "PIS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil in Gedancken meist so Gl\u00fcck als Leid besteht!", "tokens": ["Weil", "in", "Ge\u00b7dan\u00b7cken", "meist", "so", "Gl\u00fcck", "als", "Leid", "be\u00b7steht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "ADV", "ADV", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Als j\u00fcngst mein Kind (wiewohl GottLob doch ohn Gefahr)", "tokens": ["Als", "j\u00fcngst", "mein", "Kind", "(", "wie\u00b7wohl", "Gott", "Lob", "doch", "ohn", "Ge\u00b7fahr", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "NN", "$(", "KOUS", "NN", "NN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch einen Fall, am Haupt verletzet war,", "tokens": ["Durch", "ei\u00b7nen", "Fall", ",", "am", "Haupt", "ver\u00b7let\u00b7zet", "war", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "APPRART", "NN", "VVFIN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "So, da\u00df der Wund-Artzt ihm ein' Oeffnung machen muste;", "tokens": ["So", ",", "da\u00df", "der", "Wun\u00b7dArtzt", "ihm", "ein'", "Oeff\u00b7nung", "ma\u00b7chen", "mus\u00b7te", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "PPER", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bek\u00fcmmert' es sich nicht, weil von dem Schmertz,", "tokens": ["Be\u00b7k\u00fcm\u00b7mert'", "es", "sich", "nicht", ",", "weil", "von", "dem", "Schmertz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "PTKNEG", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der es betreffen sollt', sein unbesorgtes Hertz", "tokens": ["Der", "es", "be\u00b7tref\u00b7fen", "sollt'", ",", "sein", "un\u00b7be\u00b7sorg\u00b7tes", "Hertz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "PPER", "VVINF", "VMFIN", "$,", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nicht das geringste wuste.", "tokens": ["Nicht", "das", "ge\u00b7rings\u00b7te", "wus\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Der Schnitt geschahe denn: Drauf fing es zwar", "tokens": ["Der", "Schnitt", "ge\u00b7scha\u00b7he", "denn", ":", "Drauf", "fing", "es", "zwar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "$.", "PAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Den Augenblick erb\u00e4rmlich an zu weinen;", "tokens": ["Den", "Au\u00b7gen\u00b7blick", "er\u00b7b\u00e4rm\u00b7lich", "an", "zu", "wei\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Allein es sahe kaum das Gold", "tokens": ["Al\u00b7lein", "es", "sa\u00b7he", "kaum", "das", "Gold"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Von einer Zucker-Puppe scheinen,", "tokens": ["Von", "ei\u00b7ner", "Zu\u00b7cke\u00b7rPup\u00b7pe", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Als es auch schon getr\u00f6stet war;", "tokens": ["Als", "es", "auch", "schon", "ge\u00b7tr\u00f6s\u00b7tet", "war", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Die Thr\u00e4nen waren eh, als noch das Blut, gestillt.", "tokens": ["Die", "Thr\u00e4\u00b7nen", "wa\u00b7ren", "eh", ",", "als", "noch", "das", "Blut", ",", "ge\u00b7stillt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "KOUS", "$,", "KOUS", "ADV", "ART", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Das schien mir nun ein Lehr-reich Bild.", "tokens": ["Das", "schien", "mir", "nun", "ein", "Lehr\u00b7reich", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn erstlich folgt daraus der Schlu\u00df,", "tokens": ["Denn", "erst\u00b7lich", "folgt", "da\u00b7raus", "der", "Schlu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PAV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df wir uns Kummer und Verdru\u00df,", "tokens": ["Da\u00df", "wir", "uns", "Kum\u00b7mer", "und", "Ver\u00b7dru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Anstatt durch Dencken sie zu mindern und zu bessern,", "tokens": ["An\u00b7statt", "durch", "Den\u00b7cken", "sie", "zu", "min\u00b7dern", "und", "zu", "bes\u00b7sern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "VVFIN", "PPER", "APPR", "ADJA", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Durch Dencken nur noch mehren und vergr\u00f6ssern.", "tokens": ["Durch", "Den\u00b7cken", "nur", "noch", "meh\u00b7ren", "und", "ver\u00b7gr\u00f6s\u00b7sern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "ADV", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Man zieht die Plagen und die Pein,", "tokens": ["Man", "zieht", "die", "Pla\u00b7gen", "und", "die", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Die noch entfernt und erst zuk\u00fcnftig seyn,", "tokens": ["Die", "noch", "ent\u00b7fernt", "und", "erst", "zu\u00b7k\u00fcnf\u00b7tig", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "KON", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Im Dencken schon voraus herbey.", "tokens": ["Im", "Den\u00b7cken", "schon", "vo\u00b7raus", "her\u00b7bey", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Die Phantasey ist stets besch\u00e4fftiget und fertig,", "tokens": ["Die", "Phan\u00b7ta\u00b7sey", "ist", "stets", "be\u00b7sch\u00e4ff\u00b7ti\u00b7get", "und", "fer\u00b7tig", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Damit ein fernes Leid uns gegenw\u00e4rtig", "tokens": ["Da\u00b7mit", "ein", "fer\u00b7nes", "Leid", "uns", "ge\u00b7gen\u00b7w\u00e4r\u00b7tig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "PPER", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Und, eh mans f\u00fchlet, f\u00fchlbar sey.", "tokens": ["Und", ",", "eh", "mans", "f\u00fch\u00b7let", ",", "f\u00fchl\u00b7bar", "sey", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PIS", "VVFIN", "$,", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Erweget denn, geliebte Menschen, doch,", "tokens": ["Er\u00b7we\u00b7get", "denn", ",", "ge\u00b7lieb\u00b7te", "Men\u00b7schen", ",", "doch", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "ADJA", "NN", "$,", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wie gl\u00fccklich wir in diesem Stande noch,", "tokens": ["Wie", "gl\u00fcck\u00b7lich", "wir", "in", "die\u00b7sem", "Stan\u00b7de", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "APPR", "PDAT", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und wie wir Gott daf\u00fcr von Hertzen dancken m\u00fcssen,", "tokens": ["Und", "wie", "wir", "Gott", "da\u00b7f\u00fcr", "von", "Hert\u00b7zen", "dan\u00b7cken", "m\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "NN", "PAV", "APPR", "NN", "VVINF", "VMINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df Er, nach Seinem weisen Rath,", "tokens": ["Da\u00df", "Er", ",", "nach", "Sei\u00b7nem", "wei\u00b7sen", "Rath", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Uns das, was noch nicht ist, verborgen hat,", "tokens": ["Uns", "das", ",", "was", "noch", "nicht", "ist", ",", "ver\u00b7bor\u00b7gen", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "PDS", "$,", "PRELS", "ADV", "PTKNEG", "VAFIN", "$,", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und wir vom K\u00fcnftigen nichts wissen!", "tokens": ["Und", "wir", "vom", "K\u00fcnf\u00b7ti\u00b7gen", "nichts", "wis\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Die Wohlthat ist f\u00fcrwahr weit gr\u00f6sser, als man meynet,", "tokens": ["Die", "Wohlt\u00b7hat", "ist", "f\u00fcr\u00b7wahr", "weit", "gr\u00f6s\u00b7ser", ",", "als", "man", "mey\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "$,", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und herrlicher, als sie beym ersten Anblick scheinet.", "tokens": ["Und", "herr\u00b7li\u00b7cher", ",", "als", "sie", "beym", "ers\u00b7ten", "An\u00b7blick", "schei\u00b7net", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "$,", "KOUS", "PPER", "APPRART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn w\u00fcsten wir ein k\u00fcnftigs Gl\u00fcck vorher;", "tokens": ["Denn", "w\u00fcs\u00b7ten", "wir", "ein", "k\u00fcnf\u00b7tigs", "Gl\u00fcck", "vor\u00b7her", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "So w\u00fcrden wir in steter Unruh seyn:", "tokens": ["So", "w\u00fcr\u00b7den", "wir", "in", "ste\u00b7ter", "Un\u00b7ruh", "seyn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "Ein jeder Augenblick", "tokens": ["Ein", "je\u00b7der", "Au\u00b7gen\u00b7blick"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "W\u00fcrd' uns ein Zag, ein Tag ein Jahr-lang, w\u00e4hren.", "tokens": ["W\u00fcrd'", "uns", "ein", "Zag", ",", "ein", "Tag", "ein", "Jahr\u00b7lang", ",", "w\u00e4h\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "$,", "ART", "NN", "ART", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Hingegen w\u00fcrd' ein k\u00fcnftigs Ungel\u00fcck", "tokens": ["Hin\u00b7ge\u00b7gen", "w\u00fcrd'", "ein", "k\u00fcnf\u00b7tigs", "Un\u00b7ge\u00b7l\u00fcck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "---+-+-+-+", "measure": "zehnsilber"}, "line.14": {"text": "Uns mit stets gegenw\u00e4rt'ger Pein,", "tokens": ["Uns", "mit", "stets", "ge\u00b7gen\u00b7w\u00e4rt'\u00b7ger", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Durch eine schwartze Furcht, beschweren.", "tokens": ["Durch", "ei\u00b7ne", "schwart\u00b7ze", "Furcht", ",", "be\u00b7schwe\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Von meines Kindes Fall war die\u00df die erste Lehre.", "tokens": ["Von", "mei\u00b7nes", "Kin\u00b7des", "Fall", "war", "die\u00df", "die", "ers\u00b7te", "Leh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VAFIN", "PDS", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Die andre folget itzt: So wie das Kind die Schmertzen", "tokens": ["Die", "and\u00b7re", "fol\u00b7get", "itzt", ":", "So", "wie", "das", "Kind", "die", "Schmert\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "ADV", "$.", "ADV", "KOKOM", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Durch einen Vorwurf, der ihm lieb,", "tokens": ["Durch", "ei\u00b7nen", "Vor\u00b7wurf", ",", "der", "ihm", "lieb", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADJD", "$,"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.3": {"text": "Aus seinem Hirn und Hertzen,", "tokens": ["Aus", "sei\u00b7nem", "Hirn", "und", "Hert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und folglich wirklich von sich, trieb;", "tokens": ["Und", "folg\u00b7lich", "wirk\u00b7lich", "von", "sich", ",", "trieb", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "APPR", "PRF", "$,", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So m\u00f6chten wir uns wohl mit aller Kraft", "tokens": ["So", "m\u00f6ch\u00b7ten", "wir", "uns", "wohl", "mit", "al\u00b7ler", "Kraft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und allem Ernst dahin bem\u00fchen,", "tokens": ["Und", "al\u00b7lem", "Ernst", "da\u00b7hin", "be\u00b7m\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "NN", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Uns durch die eine Leidenschaft", "tokens": ["Uns", "durch", "die", "ei\u00b7ne", "Lei\u00b7den\u00b7schaft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "ART", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der andern zu entziehen!", "tokens": ["Der", "an\u00b7dern", "zu", "ent\u00b7zie\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Ach da\u00df wir uns doch \u00e4ndern m\u00f6chten,", "tokens": ["Ach", "da\u00df", "wir", "uns", "doch", "\u00e4n\u00b7dern", "m\u00f6ch\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "KOUS", "PPER", "PRF", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wann es etwa wiedrig geht,", "tokens": ["Und", "wann", "es", "et\u00b7wa", "wied\u00b7rig", "geht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit Ernst auf etwas anders d\u00e4chten,", "tokens": ["Mit", "Ernst", "auf", "et\u00b7was", "an\u00b7ders", "d\u00e4ch\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "PIS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Weil in Gedancken meist so Gl\u00fcck als Leid besteht!", "tokens": ["Weil", "in", "Ge\u00b7dan\u00b7cken", "meist", "so", "Gl\u00fcck", "als", "Leid", "be\u00b7steht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "ADV", "ADV", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}