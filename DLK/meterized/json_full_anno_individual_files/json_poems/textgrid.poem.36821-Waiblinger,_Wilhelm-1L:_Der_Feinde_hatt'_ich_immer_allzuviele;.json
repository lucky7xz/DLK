{"textgrid.poem.36821": {"metadata": {"author": {"name": "Waiblinger, Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "1L: Der Feinde hatt' ich immer allzuviele;", "genre": "verse", "period": "N.A.", "pub_year": 1817, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Feinde hatt' ich immer allzuviele;", "tokens": ["Der", "Fein\u00b7de", "hatt'", "ich", "im\u00b7mer", "all\u00b7zu\u00b7vie\u00b7le", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "PIS", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Oft seh' ich sie, gleich zaubrischen Figuren,", "tokens": ["Oft", "seh'", "ich", "sie", ",", "gleich", "zaub\u00b7ri\u00b7schen", "Fi\u00b7gu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "$,", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Vor\u00fcberziehn im stillen Schattenspiele.", "tokens": ["Vor\u00b7\u00fc\u00b7ber\u00b7ziehn", "im", "stil\u00b7len", "Schat\u00b7ten\u00b7spie\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Ich habe viel, und wurde viel beleidigt,", "tokens": ["Ich", "ha\u00b7be", "viel", ",", "und", "wur\u00b7de", "viel", "be\u00b7lei\u00b7digt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$,", "KON", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich f\u00fchlte manchen Schmerz, und weckte manchen,", "tokens": ["Ich", "f\u00fchl\u00b7te", "man\u00b7chen", "Schmerz", ",", "und", "weck\u00b7te", "man\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "KON", "VVFIN", "PIS", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Oft hab' ich andre, wen'ge mich vertheidigt.", "tokens": ["Oft", "hab'", "ich", "and\u00b7re", ",", "wen'\u00b7ge", "mich", "ver\u00b7thei\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIS", "$,", "KOUS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Von wen'gen Herzen bin ich selbst geschieden,", "tokens": ["Von", "wen'\u00b7gen", "Her\u00b7zen", "bin", "ich", "selbst", "ge\u00b7schie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Bekennen mu\u00df ich, da\u00df die Lieben Theuern", "tokens": ["Be\u00b7ken\u00b7nen", "mu\u00df", "ich", ",", "da\u00df", "die", "Lie\u00b7ben", "Theu\u00b7ern"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VMFIN", "PPER", "$,", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Mich meist zuerst geflohen und gemieden.", "tokens": ["Mich", "meist", "zu\u00b7erst", "ge\u00b7flo\u00b7hen", "und", "ge\u00b7mie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Falsch war ich nie, so oft sie's auch mich hie\u00dfen,", "tokens": ["Falsch", "war", "ich", "nie", ",", "so", "oft", "sie's", "auch", "mich", "hie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "$,", "ADV", "ADV", "PIS", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich t\u00e4uschte nur, weil ich mich selbst get\u00e4uschet,", "tokens": ["Ich", "t\u00e4uschte", "nur", ",", "weil", "ich", "mich", "selbst", "ge\u00b7t\u00e4u\u00b7schet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "KOUS", "PPER", "PRF", "ADV", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Beweinte sie, die mich entt\u00e4uscht verlie\u00dfen.", "tokens": ["Be\u00b7wein\u00b7te", "sie", ",", "die", "mich", "ent\u00b7t\u00e4uscht", "ver\u00b7lie\u00b7\u00dfen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PRELS", "PPER", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Ein ewig Scheiden und ein ewig Lassen", "tokens": ["Ein", "e\u00b7wig", "Schei\u00b7den", "und", "ein", "e\u00b7wig", "Las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "KON", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "War so mein Leben, doch die alten Freunde", "tokens": ["War", "so", "mein", "Le\u00b7ben", ",", "doch", "die", "al\u00b7ten", "Freun\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$,", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Der Heimath sind's, die mich am meisten hassen.", "tokens": ["Der", "Hei\u00b7math", "sin\u00b7d's", ",", "die", "mich", "am", "meis\u00b7ten", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$,", "PRELS", "PRF", "PTKA", "PIS", "VVINF", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Kaum wei\u00df ich selber, wie es so gekommen,", "tokens": ["Kaum", "wei\u00df", "ich", "sel\u00b7ber", ",", "wie", "es", "so", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie h\u00e4tten Recht, fast sollte man es meinen,", "tokens": ["Sie", "h\u00e4t\u00b7ten", "Recht", ",", "fast", "soll\u00b7te", "man", "es", "mei\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "ADV", "VMFIN", "PIS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie sind die Bessern ja, sie sind die Frommen.", "tokens": ["Sie", "sind", "die", "Bes\u00b7sern", "ja", ",", "sie", "sind", "die", "From\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PTKANT", "$,", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Der Feinde hatt' ich immer allzuviele;", "tokens": ["Der", "Fein\u00b7de", "hatt'", "ich", "im\u00b7mer", "all\u00b7zu\u00b7vie\u00b7le", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "PIS", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Oft seh' ich sie, gleich zaubrischen Figuren,", "tokens": ["Oft", "seh'", "ich", "sie", ",", "gleich", "zaub\u00b7ri\u00b7schen", "Fi\u00b7gu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "$,", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Vor\u00fcberziehn im stillen Schattenspiele.", "tokens": ["Vor\u00b7\u00fc\u00b7ber\u00b7ziehn", "im", "stil\u00b7len", "Schat\u00b7ten\u00b7spie\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Ich habe viel, und wurde viel beleidigt,", "tokens": ["Ich", "ha\u00b7be", "viel", ",", "und", "wur\u00b7de", "viel", "be\u00b7lei\u00b7digt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$,", "KON", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich f\u00fchlte manchen Schmerz, und weckte manchen,", "tokens": ["Ich", "f\u00fchl\u00b7te", "man\u00b7chen", "Schmerz", ",", "und", "weck\u00b7te", "man\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "KON", "VVFIN", "PIS", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Oft hab' ich andre, wen'ge mich vertheidigt.", "tokens": ["Oft", "hab'", "ich", "and\u00b7re", ",", "wen'\u00b7ge", "mich", "ver\u00b7thei\u00b7digt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIS", "$,", "KOUS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Von wen'gen Herzen bin ich selbst geschieden,", "tokens": ["Von", "wen'\u00b7gen", "Her\u00b7zen", "bin", "ich", "selbst", "ge\u00b7schie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Bekennen mu\u00df ich, da\u00df die Lieben Theuern", "tokens": ["Be\u00b7ken\u00b7nen", "mu\u00df", "ich", ",", "da\u00df", "die", "Lie\u00b7ben", "Theu\u00b7ern"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VMFIN", "PPER", "$,", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Mich meist zuerst geflohen und gemieden.", "tokens": ["Mich", "meist", "zu\u00b7erst", "ge\u00b7flo\u00b7hen", "und", "ge\u00b7mie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Falsch war ich nie, so oft sie's auch mich hie\u00dfen,", "tokens": ["Falsch", "war", "ich", "nie", ",", "so", "oft", "sie's", "auch", "mich", "hie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "$,", "ADV", "ADV", "PIS", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich t\u00e4uschte nur, weil ich mich selbst get\u00e4uschet,", "tokens": ["Ich", "t\u00e4uschte", "nur", ",", "weil", "ich", "mich", "selbst", "ge\u00b7t\u00e4u\u00b7schet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "KOUS", "PPER", "PRF", "ADV", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Beweinte sie, die mich entt\u00e4uscht verlie\u00dfen.", "tokens": ["Be\u00b7wein\u00b7te", "sie", ",", "die", "mich", "ent\u00b7t\u00e4uscht", "ver\u00b7lie\u00b7\u00dfen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PRELS", "PPER", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Ein ewig Scheiden und ein ewig Lassen", "tokens": ["Ein", "e\u00b7wig", "Schei\u00b7den", "und", "ein", "e\u00b7wig", "Las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "KON", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "War so mein Leben, doch die alten Freunde", "tokens": ["War", "so", "mein", "Le\u00b7ben", ",", "doch", "die", "al\u00b7ten", "Freun\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "$,", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Der Heimath sind's, die mich am meisten hassen.", "tokens": ["Der", "Hei\u00b7math", "sin\u00b7d's", ",", "die", "mich", "am", "meis\u00b7ten", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$,", "PRELS", "PRF", "PTKA", "PIS", "VVINF", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Kaum wei\u00df ich selber, wie es so gekommen,", "tokens": ["Kaum", "wei\u00df", "ich", "sel\u00b7ber", ",", "wie", "es", "so", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie h\u00e4tten Recht, fast sollte man es meinen,", "tokens": ["Sie", "h\u00e4t\u00b7ten", "Recht", ",", "fast", "soll\u00b7te", "man", "es", "mei\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "ADV", "VMFIN", "PIS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie sind die Bessern ja, sie sind die Frommen.", "tokens": ["Sie", "sind", "die", "Bes\u00b7sern", "ja", ",", "sie", "sind", "die", "From\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PTKANT", "$,", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}