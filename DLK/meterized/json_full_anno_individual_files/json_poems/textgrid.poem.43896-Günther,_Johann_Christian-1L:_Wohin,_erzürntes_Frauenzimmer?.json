{"textgrid.poem.43896": {"metadata": {"author": {"name": "G\u00fcnther, Johann Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wohin, erz\u00fcrntes Frauenzimmer?", "genre": "verse", "period": "N.A.", "pub_year": 1709, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wohin, erz\u00fcrntes Frauenzimmer?", "tokens": ["Wo\u00b7hin", ",", "er\u00b7z\u00fcrn\u00b7tes", "Frau\u00b7en\u00b7zim\u00b7mer", "?"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PWAV", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wohin, vielleicht zu deiner Qual?", "tokens": ["Wo\u00b7hin", ",", "viel\u00b7leicht", "zu", "dei\u00b7ner", "Qual", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bisweilen hilft nicht allemahl,", "tokens": ["Bis\u00b7wei\u00b7len", "hilft", "nicht", "al\u00b7le\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und oft gedr\u00e4ut erschreckt nicht immer,", "tokens": ["Und", "oft", "ge\u00b7dr\u00e4ut", "er\u00b7schreckt", "nicht", "im\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Zu viel gestraft bringt wenig Reu;", "tokens": ["Zu", "viel", "ge\u00b7straft", "bringt", "we\u00b7nig", "Reu", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJD", "VVFIN", "PIS", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Bu\u00dfe mu\u00df die Strafe mindern,", "tokens": ["Die", "Bu\u00b7\u00dfe", "mu\u00df", "die", "Stra\u00b7fe", "min\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Sonst wird bey schl\u00e4gefaulen Kindern", "tokens": ["Sonst", "wird", "bey", "schl\u00e4\u00b7ge\u00b7fau\u00b7len", "Kin\u00b7dern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Furcht zu einer Raserey.", "tokens": ["Die", "Furcht", "zu", "ei\u00b7ner", "Ra\u00b7se\u00b7rey", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein allzu scharf gespannter Bogen", "tokens": ["Ein", "all\u00b7zu", "scharf", "ge\u00b7spann\u00b7ter", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PTKA", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rei\u00dft endlich Sehn und Draht entzwey.", "tokens": ["Rei\u00dft", "end\u00b7lich", "Sehn", "und", "Draht", "ent\u00b7zwey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVFIN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist dieses nicht mein Conterfey,", "tokens": ["Ist", "die\u00b7ses", "nicht", "mein", "Con\u00b7ter\u00b7fey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So sprich: Die Warheit hat gelogen.", "tokens": ["So", "sprich", ":", "Die", "War\u00b7heit", "hat", "ge\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dein Zorn geht etwas gar zu weit,", "tokens": ["Dein", "Zorn", "geht", "et\u00b7was", "gar", "zu", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "ADV", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dein Eifer weis von keinem Maa\u00dfe,", "tokens": ["Dein", "Ei\u00b7fer", "weis", "von", "kei\u00b7nem", "Maa\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKVZ", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Sein Glei\u00df sucht vor die Mittelstra\u00dfe", "tokens": ["Sein", "Glei\u00df", "sucht", "vor", "die", "Mit\u00b7tel\u00b7stra\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den Weg der Unbedachtsamkeit.", "tokens": ["Den", "Weg", "der", "Un\u00b7be\u00b7dacht\u00b7sam\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein Nebel schw\u00e4rzt der Augen Sterne", "tokens": ["Ein", "Ne\u00b7bel", "schw\u00e4rzt", "der", "Au\u00b7gen", "Ster\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und macht ein saures Angesicht,", "tokens": ["Und", "macht", "ein", "sau\u00b7res", "An\u00b7ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dein Fu\u00df h\u00f6rt meinen Zuruf nicht,", "tokens": ["Dein", "Fu\u00df", "h\u00f6rt", "mei\u00b7nen", "Zu\u00b7ruf", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Darmit er dich von mir entferne", "tokens": ["Dar\u00b7mit", "er", "dich", "von", "mir", "ent\u00b7fer\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "APPR", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.6": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.7": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.8": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}}, "stanza.4": {"line.1": {"text": "Wohin, erz\u00fcrntes Frauenzimmer?", "tokens": ["Wo\u00b7hin", ",", "er\u00b7z\u00fcrn\u00b7tes", "Frau\u00b7en\u00b7zim\u00b7mer", "?"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PWAV", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wohin, vielleicht zu deiner Qual?", "tokens": ["Wo\u00b7hin", ",", "viel\u00b7leicht", "zu", "dei\u00b7ner", "Qual", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bisweilen hilft nicht allemahl,", "tokens": ["Bis\u00b7wei\u00b7len", "hilft", "nicht", "al\u00b7le\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und oft gedr\u00e4ut erschreckt nicht immer,", "tokens": ["Und", "oft", "ge\u00b7dr\u00e4ut", "er\u00b7schreckt", "nicht", "im\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVFIN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Zu viel gestraft bringt wenig Reu;", "tokens": ["Zu", "viel", "ge\u00b7straft", "bringt", "we\u00b7nig", "Reu", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJD", "VVFIN", "PIS", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Bu\u00dfe mu\u00df die Strafe mindern,", "tokens": ["Die", "Bu\u00b7\u00dfe", "mu\u00df", "die", "Stra\u00b7fe", "min\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Sonst wird bey schl\u00e4gefaulen Kindern", "tokens": ["Sonst", "wird", "bey", "schl\u00e4\u00b7ge\u00b7fau\u00b7len", "Kin\u00b7dern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Furcht zu einer Raserey.", "tokens": ["Die", "Furcht", "zu", "ei\u00b7ner", "Ra\u00b7se\u00b7rey", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ein allzu scharf gespannter Bogen", "tokens": ["Ein", "all\u00b7zu", "scharf", "ge\u00b7spann\u00b7ter", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PTKA", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rei\u00dft endlich Sehn und Draht entzwey.", "tokens": ["Rei\u00dft", "end\u00b7lich", "Sehn", "und", "Draht", "ent\u00b7zwey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVFIN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist dieses nicht mein Conterfey,", "tokens": ["Ist", "die\u00b7ses", "nicht", "mein", "Con\u00b7ter\u00b7fey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So sprich: Die Warheit hat gelogen.", "tokens": ["So", "sprich", ":", "Die", "War\u00b7heit", "hat", "ge\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dein Zorn geht etwas gar zu weit,", "tokens": ["Dein", "Zorn", "geht", "et\u00b7was", "gar", "zu", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "ADV", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dein Eifer weis von keinem Maa\u00dfe,", "tokens": ["Dein", "Ei\u00b7fer", "weis", "von", "kei\u00b7nem", "Maa\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKVZ", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Sein Glei\u00df sucht vor die Mittelstra\u00dfe", "tokens": ["Sein", "Glei\u00df", "sucht", "vor", "die", "Mit\u00b7tel\u00b7stra\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Den Weg der Unbedachtsamkeit.", "tokens": ["Den", "Weg", "der", "Un\u00b7be\u00b7dacht\u00b7sam\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ein Nebel schw\u00e4rzt der Augen Sterne", "tokens": ["Ein", "Ne\u00b7bel", "schw\u00e4rzt", "der", "Au\u00b7gen", "Ster\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und macht ein saures Angesicht,", "tokens": ["Und", "macht", "ein", "sau\u00b7res", "An\u00b7ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dein Fu\u00df h\u00f6rt meinen Zuruf nicht,", "tokens": ["Dein", "Fu\u00df", "h\u00f6rt", "mei\u00b7nen", "Zu\u00b7ruf", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Darmit er dich von mir entferne", "tokens": ["Dar\u00b7mit", "er", "dich", "von", "mir", "ent\u00b7fer\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "APPR", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.6": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.7": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}, "line.8": {"text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .", "tokens": [".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$.", "$."]}}}}}