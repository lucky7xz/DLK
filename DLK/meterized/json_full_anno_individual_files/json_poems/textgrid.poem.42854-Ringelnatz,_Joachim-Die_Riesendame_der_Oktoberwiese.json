{"textgrid.poem.42854": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Die Riesendame der Oktoberwiese", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die Zeltwand spaltete sich weit,", "tokens": ["Die", "Zelt\u00b7wand", "spal\u00b7te\u00b7te", "sich", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eine ungeheure Glocke wuchtete", "tokens": ["Und", "ei\u00b7ne", "un\u00b7ge\u00b7heu\u00b7re", "Glo\u00b7cke", "wuch\u00b7te\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Herein. \u00bbEmmy, das gr\u00f6\u00dfte Wunder unsrer Zeit!\u00ab", "tokens": ["Her\u00b7ein", ".", "\u00bb", "Em\u00b7my", ",", "das", "gr\u00f6\u00df\u00b7te", "Wun\u00b7der", "uns\u00b7rer", "Zeit", "!", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKVZ", "$.", "$(", "NE", "$,", "ART", "ADJA", "NN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Dort, wo der H\u00e4ngerock am Halse buchtete,", "tokens": ["Dort", ",", "wo", "der", "H\u00e4n\u00b7ge\u00b7rock", "am", "Hal\u00b7se", "buch\u00b7te\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dort bot sich triefenden Quartanerl\u00fcsten", "tokens": ["Dort", "bot", "sich", "trie\u00b7fen\u00b7den", "Quar\u00b7ta\u00b7ner\u00b7l\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ADJA", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Die Lavamasse von alpinen Br\u00fcsten,", "tokens": ["Die", "La\u00b7va\u00b7mas\u00b7se", "von", "al\u00b7pi\u00b7nen", "Br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Die majest\u00e4tisch auseinanderflo\u00df.", "tokens": ["Die", "ma\u00b7jes\u00b7t\u00e4\u00b7tisch", "aus\u00b7ein\u00b7an\u00b7der\u00b7flo\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbemmy, der weibliche Kolo\u00df.\u00ab", "tokens": ["\u00bb", "em\u00b7my", ",", "der", "weib\u00b7li\u00b7che", "Ko\u00b7lo\u00df", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "$,", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Hilflose Vorderschinken hingen", "tokens": ["Hilf\u00b7lo\u00b7se", "Vor\u00b7der\u00b7schin\u00b7ken", "hin\u00b7gen"], "token_info": ["word", "word", "word"], "pos": ["NE", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Herunter, die in W\u00fcrstchen \u00fcbergingen.", "tokens": ["Her\u00b7un\u00b7ter", ",", "die", "in", "W\u00fcr\u00b7stchen", "\u00fc\u00b7ber\u00b7gin\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Und als sie langsam wendete: \u2013 Oho! \u2013", "tokens": ["Und", "als", "sie", "lang\u00b7sam", "wen\u00b7de\u00b7te", ":", "\u2013", "O\u00b7ho", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "KOUS", "PPER", "ADJD", "VVFIN", "$.", "$(", "ITJ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Da zeigte sich der Vollbegriff Popo", "tokens": ["Da", "zeig\u00b7te", "sich", "der", "Voll\u00b7be\u00b7griff", "Po\u00b7po"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "In schweren erzgego\u00dfnen Wolkenmassen.", "tokens": ["In", "schwe\u00b7ren", "erz\u00b7ge\u00b7go\u00df\u00b7nen", "Wol\u00b7ken\u00b7mas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "\u00bbnicht anfassen!\u00ab", "tokens": ["\u00bb", "nicht", "an\u00b7fas\u00b7sen", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "PTKNEG", "VVINF", "$.", "$("], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "Und fl\u00fcchtig unter hochgerafften Segeln", "tokens": ["Und", "fl\u00fcch\u00b7tig", "un\u00b7ter", "hoch\u00b7ge\u00b7raff\u00b7ten", "Se\u00b7geln"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Sah man der Oberschenkel S\u00e4ulenpracht.", "tokens": ["Sah", "man", "der", "O\u00b7ber\u00b7schen\u00b7kel", "S\u00e4u\u00b7len\u00b7pracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "NN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da war es aus. Da wurde gell gelacht.", "tokens": ["Da", "war", "es", "aus", ".", "Da", "wur\u00b7de", "gell", "ge\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKVZ", "$.", "ADV", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ich wu\u00dfte jeden Witz zu \u00fcberflegeln,", "tokens": ["Ich", "wu\u00df\u00b7te", "je\u00b7den", "Witz", "zu", "\u00fc\u00b7berf\u00b7le\u00b7geln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und jeder Beifall st\u00e4rkte meinen Schwung.", "tokens": ["Und", "je\u00b7der", "Bei\u00b7fall", "st\u00e4rk\u00b7te", "mei\u00b7nen", "Schwung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Die Dicke schwieg. Ich gab die Vorstellung.", "tokens": ["Die", "Di\u00b7cke", "schwieg", ".", "Ich", "gab", "die", "Vor\u00b7stel\u00b7lung", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.3": {"line.1": {"text": "Besonders lachten selbst recht runde Leute.", "tokens": ["Be\u00b7son\u00b7ders", "lach\u00b7ten", "selbst", "recht", "run\u00b7de", "Leu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich wartete, bis sich das Volk zerstreute.", "tokens": ["Ich", "war\u00b7te\u00b7te", ",", "bis", "sich", "das", "Volk", "zer\u00b7streu\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PRF", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Nacht war es worden. Emmy lie\u00df sich dort,", "tokens": ["Nacht", "war", "es", "wor\u00b7den", ".", "Em\u00b7my", "lie\u00df", "sich", "dort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "VAPP", "$.", "NE", "VVFIN", "PRF", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wo sie gestanden, dumpf zum Nachtmahl nieder.", "tokens": ["Wo", "sie", "ge\u00b7stan\u00b7den", ",", "dumpf", "zum", "Nacht\u00b7mahl", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVPP", "$,", "ADJD", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie schlang mit Gier, doch regte kaum die Glieder.", "tokens": ["Sie", "schlang", "mit", "Gier", ",", "doch", "reg\u00b7te", "kaum", "die", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,", "ADV", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "\u00bbsag, Emmy, w\u00fcrdest du ein gutes Wort,", "tokens": ["\u00bb", "sag", ",", "Em\u00b7my", ",", "w\u00fcr\u00b7dest", "du", "ein", "gu\u00b7tes", "Wort", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NE", "$,", "VAFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Das keinen Witz und keine Neugier hat,", "tokens": ["Das", "kei\u00b7nen", "Witz", "und", "kei\u00b7ne", "Neu\u00b7gier", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "KON", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Von einem, der dich tief betrauert, h\u00f6ren?\u00ab", "tokens": ["Von", "ei\u00b7nem", ",", "der", "dich", "tief", "be\u00b7trau\u00b7ert", ",", "h\u00f6\u00b7ren", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PRF", "ADJD", "VVPP", "$,", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Sie sah nicht auf. Sie nickte kurz und matt:", "tokens": ["Sie", "sah", "nicht", "auf", ".", "Sie", "nick\u00b7te", "kurz", "und", "matt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PTKVZ", "$.", "PPER", "VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbnur zu! Beim Essen kann mich gar nichts st\u00f6ren.\u00ab", "tokens": ["\u00bb", "nur", "zu", "!", "Beim", "Es\u00b7sen", "kann", "mich", "gar", "nichts", "st\u00f6\u00b7ren", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PTKVZ", "$.", "APPRART", "NN", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "\u00bbemmy! Du armes Wunderwerk der Zeit!", "tokens": ["\u00bb", "em\u00b7my", "!", "Du", "ar\u00b7mes", "Wun\u00b7der\u00b7werk", "der", "Zeit", "!"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$.", "PPER", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Du trittst dich selbst mit ordin\u00e4ren Reden,", "tokens": ["Du", "trittst", "dich", "selbst", "mit", "or\u00b7di\u00b7n\u00e4\u00b7ren", "Re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Mit eingelerntem hohlen Vortrag breit.", "tokens": ["Mit", "ein\u00b7ge\u00b7lern\u00b7tem", "hoh\u00b7len", "Vor\u00b7trag", "breit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Du l\u00e4\u00dft die schlimme Masse deines Fettes", "tokens": ["Du", "l\u00e4\u00dft", "die", "schlim\u00b7me", "Mas\u00b7se", "dei\u00b7nes", "Fet\u00b7tes"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Von jedem Buben, jeder Dirne kneten.", "tokens": ["Von", "je\u00b7dem", "Bu\u00b7ben", ",", "je\u00b7der", "Dir\u00b7ne", "kne\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Man kann den Scherz vom Umfang deines Bettes,", "tokens": ["Man", "kann", "den", "Scherz", "vom", "Um\u00b7fang", "dei\u00b7nes", "Bet\u00b7tes", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Der Badewanne bis zum Ekel spinnen.", "tokens": ["Der", "Ba\u00b7de\u00b7wan\u00b7ne", "bis", "zum", "E\u00b7kel", "spin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und so tat ich. Und konnte nicht von hinnen.", "tokens": ["Und", "so", "tat", "ich", ".", "Und", "konn\u00b7te", "nicht", "von", "hin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "$.", "KON", "VMFIN", "PTKNEG", "APPR", "ADV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Ich dachte mich besch\u00e4mt in dich hinein.", "tokens": ["Ich", "dach\u00b7te", "mich", "be\u00b7sch\u00e4mt", "in", "dich", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Es m\u00fc\u00dfte doch in dir, in deinem Leben", "tokens": ["Es", "m\u00fc\u00df\u00b7te", "doch", "in", "dir", ",", "in", "dei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "PPER", "$,", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Sich irgendwo das Schmerzgef\u00fchl ergeben:", "tokens": ["Sich", "ir\u00b7gend\u00b7wo", "das", "Schmerz\u00b7ge\u00b7f\u00fchl", "er\u00b7ge\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Ein Dasein lang nicht Mensch noch Tier zu sein.\u00ab", "tokens": ["Ein", "Da\u00b7sein", "lang", "nicht", "Mensch", "noch", "Tier", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "ADJD", "PTKNEG", "NN", "ADV", "ADJD", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Hier hielt ich inne, dachte zaghaft nach.", "tokens": ["Hier", "hielt", "ich", "in\u00b7ne", ",", "dach\u00b7te", "zag\u00b7haft", "nach", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Bis ein Ger\u00e4usch am Eingang unterbrach.", "tokens": ["Bis", "ein", "Ge\u00b7r\u00e4usch", "am", "Ein\u00b7gang", "un\u00b7ter\u00b7brach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Es nahte sich mit wohlgebornen Schritten", "tokens": ["Es", "nah\u00b7te", "sich", "mit", "wohl\u00b7ge\u00b7bor\u00b7nen", "Schrit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der Elefant vom Nachbarzelt", "tokens": ["Der", "E\u00b7le\u00b7fant", "vom", "Nach\u00b7bar\u00b7zelt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sagte: \u00bbEmmy, schwerste Frau der Welt,", "tokens": ["Und", "sag\u00b7te", ":", "\u00bb", "Em\u00b7my", ",", "schwers\u00b7te", "Frau", "der", "Welt", ","], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NE", "$,", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Darf ich um einen kleinen Beischlaf bitten?\u00ab", "tokens": ["Darf", "ich", "um", "ei\u00b7nen", "klei\u00b7nen", "Bei\u00b7schlaf", "bit\u00b7ten", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ART", "ADJA", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Diskret entweichend konnte ich noch h\u00f6ren:", "tokens": ["Dis\u00b7kret", "ent\u00b7wei\u00b7chend", "konn\u00b7te", "ich", "noch", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "\u00bbnur zu! Beim Essen kann mich gar nichts st\u00f6ren.\u00ab", "tokens": ["\u00bb", "nur", "zu", "!", "Beim", "Es\u00b7sen", "kann", "mich", "gar", "nichts", "st\u00f6\u00b7ren", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PTKVZ", "$.", "APPRART", "NN", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Die Zeltwand spaltete sich weit,", "tokens": ["Die", "Zelt\u00b7wand", "spal\u00b7te\u00b7te", "sich", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eine ungeheure Glocke wuchtete", "tokens": ["Und", "ei\u00b7ne", "un\u00b7ge\u00b7heu\u00b7re", "Glo\u00b7cke", "wuch\u00b7te\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Herein. \u00bbEmmy, das gr\u00f6\u00dfte Wunder unsrer Zeit!\u00ab", "tokens": ["Her\u00b7ein", ".", "\u00bb", "Em\u00b7my", ",", "das", "gr\u00f6\u00df\u00b7te", "Wun\u00b7der", "uns\u00b7rer", "Zeit", "!", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKVZ", "$.", "$(", "NE", "$,", "ART", "ADJA", "NN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Dort, wo der H\u00e4ngerock am Halse buchtete,", "tokens": ["Dort", ",", "wo", "der", "H\u00e4n\u00b7ge\u00b7rock", "am", "Hal\u00b7se", "buch\u00b7te\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dort bot sich triefenden Quartanerl\u00fcsten", "tokens": ["Dort", "bot", "sich", "trie\u00b7fen\u00b7den", "Quar\u00b7ta\u00b7ner\u00b7l\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ADJA", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Die Lavamasse von alpinen Br\u00fcsten,", "tokens": ["Die", "La\u00b7va\u00b7mas\u00b7se", "von", "al\u00b7pi\u00b7nen", "Br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Die majest\u00e4tisch auseinanderflo\u00df.", "tokens": ["Die", "ma\u00b7jes\u00b7t\u00e4\u00b7tisch", "aus\u00b7ein\u00b7an\u00b7der\u00b7flo\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbemmy, der weibliche Kolo\u00df.\u00ab", "tokens": ["\u00bb", "em\u00b7my", ",", "der", "weib\u00b7li\u00b7che", "Ko\u00b7lo\u00df", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "$,", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Hilflose Vorderschinken hingen", "tokens": ["Hilf\u00b7lo\u00b7se", "Vor\u00b7der\u00b7schin\u00b7ken", "hin\u00b7gen"], "token_info": ["word", "word", "word"], "pos": ["NE", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Herunter, die in W\u00fcrstchen \u00fcbergingen.", "tokens": ["Her\u00b7un\u00b7ter", ",", "die", "in", "W\u00fcr\u00b7stchen", "\u00fc\u00b7ber\u00b7gin\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Und als sie langsam wendete: \u2013 Oho! \u2013", "tokens": ["Und", "als", "sie", "lang\u00b7sam", "wen\u00b7de\u00b7te", ":", "\u2013", "O\u00b7ho", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "KOUS", "PPER", "ADJD", "VVFIN", "$.", "$(", "ITJ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Da zeigte sich der Vollbegriff Popo", "tokens": ["Da", "zeig\u00b7te", "sich", "der", "Voll\u00b7be\u00b7griff", "Po\u00b7po"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "In schweren erzgego\u00dfnen Wolkenmassen.", "tokens": ["In", "schwe\u00b7ren", "erz\u00b7ge\u00b7go\u00df\u00b7nen", "Wol\u00b7ken\u00b7mas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "\u00bbnicht anfassen!\u00ab", "tokens": ["\u00bb", "nicht", "an\u00b7fas\u00b7sen", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "PTKNEG", "VVINF", "$.", "$("], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "Und fl\u00fcchtig unter hochgerafften Segeln", "tokens": ["Und", "fl\u00fcch\u00b7tig", "un\u00b7ter", "hoch\u00b7ge\u00b7raff\u00b7ten", "Se\u00b7geln"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Sah man der Oberschenkel S\u00e4ulenpracht.", "tokens": ["Sah", "man", "der", "O\u00b7ber\u00b7schen\u00b7kel", "S\u00e4u\u00b7len\u00b7pracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "NN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da war es aus. Da wurde gell gelacht.", "tokens": ["Da", "war", "es", "aus", ".", "Da", "wur\u00b7de", "gell", "ge\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKVZ", "$.", "ADV", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ich wu\u00dfte jeden Witz zu \u00fcberflegeln,", "tokens": ["Ich", "wu\u00df\u00b7te", "je\u00b7den", "Witz", "zu", "\u00fc\u00b7berf\u00b7le\u00b7geln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und jeder Beifall st\u00e4rkte meinen Schwung.", "tokens": ["Und", "je\u00b7der", "Bei\u00b7fall", "st\u00e4rk\u00b7te", "mei\u00b7nen", "Schwung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Die Dicke schwieg. Ich gab die Vorstellung.", "tokens": ["Die", "Di\u00b7cke", "schwieg", ".", "Ich", "gab", "die", "Vor\u00b7stel\u00b7lung", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.10": {"line.1": {"text": "Besonders lachten selbst recht runde Leute.", "tokens": ["Be\u00b7son\u00b7ders", "lach\u00b7ten", "selbst", "recht", "run\u00b7de", "Leu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich wartete, bis sich das Volk zerstreute.", "tokens": ["Ich", "war\u00b7te\u00b7te", ",", "bis", "sich", "das", "Volk", "zer\u00b7streu\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PRF", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Nacht war es worden. Emmy lie\u00df sich dort,", "tokens": ["Nacht", "war", "es", "wor\u00b7den", ".", "Em\u00b7my", "lie\u00df", "sich", "dort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "VAPP", "$.", "NE", "VVFIN", "PRF", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wo sie gestanden, dumpf zum Nachtmahl nieder.", "tokens": ["Wo", "sie", "ge\u00b7stan\u00b7den", ",", "dumpf", "zum", "Nacht\u00b7mahl", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVPP", "$,", "ADJD", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie schlang mit Gier, doch regte kaum die Glieder.", "tokens": ["Sie", "schlang", "mit", "Gier", ",", "doch", "reg\u00b7te", "kaum", "die", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,", "ADV", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "\u00bbsag, Emmy, w\u00fcrdest du ein gutes Wort,", "tokens": ["\u00bb", "sag", ",", "Em\u00b7my", ",", "w\u00fcr\u00b7dest", "du", "ein", "gu\u00b7tes", "Wort", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NE", "$,", "VAFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Das keinen Witz und keine Neugier hat,", "tokens": ["Das", "kei\u00b7nen", "Witz", "und", "kei\u00b7ne", "Neu\u00b7gier", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "KON", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Von einem, der dich tief betrauert, h\u00f6ren?\u00ab", "tokens": ["Von", "ei\u00b7nem", ",", "der", "dich", "tief", "be\u00b7trau\u00b7ert", ",", "h\u00f6\u00b7ren", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PRF", "ADJD", "VVPP", "$,", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Sie sah nicht auf. Sie nickte kurz und matt:", "tokens": ["Sie", "sah", "nicht", "auf", ".", "Sie", "nick\u00b7te", "kurz", "und", "matt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PTKVZ", "$.", "PPER", "VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbnur zu! Beim Essen kann mich gar nichts st\u00f6ren.\u00ab", "tokens": ["\u00bb", "nur", "zu", "!", "Beim", "Es\u00b7sen", "kann", "mich", "gar", "nichts", "st\u00f6\u00b7ren", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PTKVZ", "$.", "APPRART", "NN", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "\u00bbemmy! Du armes Wunderwerk der Zeit!", "tokens": ["\u00bb", "em\u00b7my", "!", "Du", "ar\u00b7mes", "Wun\u00b7der\u00b7werk", "der", "Zeit", "!"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$.", "PPER", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Du trittst dich selbst mit ordin\u00e4ren Reden,", "tokens": ["Du", "trittst", "dich", "selbst", "mit", "or\u00b7di\u00b7n\u00e4\u00b7ren", "Re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Mit eingelerntem hohlen Vortrag breit.", "tokens": ["Mit", "ein\u00b7ge\u00b7lern\u00b7tem", "hoh\u00b7len", "Vor\u00b7trag", "breit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Du l\u00e4\u00dft die schlimme Masse deines Fettes", "tokens": ["Du", "l\u00e4\u00dft", "die", "schlim\u00b7me", "Mas\u00b7se", "dei\u00b7nes", "Fet\u00b7tes"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Von jedem Buben, jeder Dirne kneten.", "tokens": ["Von", "je\u00b7dem", "Bu\u00b7ben", ",", "je\u00b7der", "Dir\u00b7ne", "kne\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Man kann den Scherz vom Umfang deines Bettes,", "tokens": ["Man", "kann", "den", "Scherz", "vom", "Um\u00b7fang", "dei\u00b7nes", "Bet\u00b7tes", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Der Badewanne bis zum Ekel spinnen.", "tokens": ["Der", "Ba\u00b7de\u00b7wan\u00b7ne", "bis", "zum", "E\u00b7kel", "spin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und so tat ich. Und konnte nicht von hinnen.", "tokens": ["Und", "so", "tat", "ich", ".", "Und", "konn\u00b7te", "nicht", "von", "hin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "$.", "KON", "VMFIN", "PTKNEG", "APPR", "ADV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Ich dachte mich besch\u00e4mt in dich hinein.", "tokens": ["Ich", "dach\u00b7te", "mich", "be\u00b7sch\u00e4mt", "in", "dich", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Es m\u00fc\u00dfte doch in dir, in deinem Leben", "tokens": ["Es", "m\u00fc\u00df\u00b7te", "doch", "in", "dir", ",", "in", "dei\u00b7nem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "PPER", "$,", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Sich irgendwo das Schmerzgef\u00fchl ergeben:", "tokens": ["Sich", "ir\u00b7gend\u00b7wo", "das", "Schmerz\u00b7ge\u00b7f\u00fchl", "er\u00b7ge\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Ein Dasein lang nicht Mensch noch Tier zu sein.\u00ab", "tokens": ["Ein", "Da\u00b7sein", "lang", "nicht", "Mensch", "noch", "Tier", "zu", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "ADJD", "PTKNEG", "NN", "ADV", "ADJD", "PTKZU", "VAINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Hier hielt ich inne, dachte zaghaft nach.", "tokens": ["Hier", "hielt", "ich", "in\u00b7ne", ",", "dach\u00b7te", "zag\u00b7haft", "nach", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Bis ein Ger\u00e4usch am Eingang unterbrach.", "tokens": ["Bis", "ein", "Ge\u00b7r\u00e4usch", "am", "Ein\u00b7gang", "un\u00b7ter\u00b7brach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Es nahte sich mit wohlgebornen Schritten", "tokens": ["Es", "nah\u00b7te", "sich", "mit", "wohl\u00b7ge\u00b7bor\u00b7nen", "Schrit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der Elefant vom Nachbarzelt", "tokens": ["Der", "E\u00b7le\u00b7fant", "vom", "Nach\u00b7bar\u00b7zelt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sagte: \u00bbEmmy, schwerste Frau der Welt,", "tokens": ["Und", "sag\u00b7te", ":", "\u00bb", "Em\u00b7my", ",", "schwers\u00b7te", "Frau", "der", "Welt", ","], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NE", "$,", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Darf ich um einen kleinen Beischlaf bitten?\u00ab", "tokens": ["Darf", "ich", "um", "ei\u00b7nen", "klei\u00b7nen", "Bei\u00b7schlaf", "bit\u00b7ten", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ART", "ADJA", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Diskret entweichend konnte ich noch h\u00f6ren:", "tokens": ["Dis\u00b7kret", "ent\u00b7wei\u00b7chend", "konn\u00b7te", "ich", "noch", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "\u00bbnur zu! Beim Essen kann mich gar nichts st\u00f6ren.\u00ab", "tokens": ["\u00bb", "nur", "zu", "!", "Beim", "Es\u00b7sen", "kann", "mich", "gar", "nichts", "st\u00f6\u00b7ren", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PTKVZ", "$.", "APPRART", "NN", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}