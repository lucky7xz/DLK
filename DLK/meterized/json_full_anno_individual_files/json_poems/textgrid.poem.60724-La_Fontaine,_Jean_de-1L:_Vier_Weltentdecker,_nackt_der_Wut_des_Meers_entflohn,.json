{"textgrid.poem.60724": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Vier Weltentdecker, nackt der Wut des Meers entflohn,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Vier Weltentdecker, nackt der Wut des Meers entflohn,", "tokens": ["Vier", "Wel\u00b7tent\u00b7de\u00b7cker", ",", "nackt", "der", "Wut", "des", "Meers", "ent\u00b7flohn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "ADJD", "ART", "NN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Ein Kaufmann, ein Baron, ein Hirt, ein K\u00f6nigssohn,", "tokens": ["Ein", "Kauf\u00b7mann", ",", "ein", "Ba\u00b7ron", ",", "ein", "Hirt", ",", "ein", "K\u00f6\u00b7nigs\u00b7sohn", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von gleicher Armut alle jetzt wie Belisar,", "tokens": ["Von", "glei\u00b7cher", "Ar\u00b7mut", "al\u00b7le", "jetzt", "wie", "Be\u00b7li\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PIS", "ADV", "KOKOM", "NE", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Erbettelten zur Lindrung ihrer gro\u00dfen Not", "tokens": ["Er\u00b7bet\u00b7tel\u00b7ten", "zur", "Lind\u00b7rung", "ih\u00b7rer", "gro\u00b7\u00dfen", "Not"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Von fremden Leuten nun ihr Brot.", "tokens": ["Von", "frem\u00b7den", "Leu\u00b7ten", "nun", "ihr", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Zu sagen, welcher Zufall diese kleine Schar", "tokens": ["Zu", "sa\u00b7gen", ",", "wel\u00b7cher", "Zu\u00b7fall", "die\u00b7se", "klei\u00b7ne", "Schar"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$,", "PWAT", "NN", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Zusammenschlo\u00df, aus der ein jeder anders war,", "tokens": ["Zu\u00b7sam\u00b7men\u00b7schlo\u00df", ",", "aus", "der", "ein", "je\u00b7der", "an\u00b7ders", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "PRELS", "ART", "PIS", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Erscheint mir unn\u00fctz ganz und gar.", "tokens": ["Er\u00b7scheint", "mir", "un\u00b7n\u00fctz", "ganz", "und", "gar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ADV", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Sie setzten schlie\u00dflich sich bei einem Brunnen hin", "tokens": ["Sie", "setz\u00b7ten", "schlie\u00df\u00b7lich", "sich", "bei", "ei\u00b7nem", "Brun\u00b7nen", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "PRF", "APPR", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und hielten Rat, die armen Leute.", "tokens": ["Und", "hiel\u00b7ten", "Rat", ",", "die", "ar\u00b7men", "Leu\u00b7te", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NN", "$,", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Der Prinz beklagte, da\u00df die Gro\u00dfen oft die Beute", "tokens": ["Der", "Prinz", "be\u00b7klag\u00b7te", ",", "da\u00df", "die", "Gro\u00b7\u00dfen", "oft", "die", "Beu\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "KOUS", "ART", "NN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Des Ungl\u00fccks sind. Der Hirt dagegen sprach: \u00bbIch bin", "tokens": ["Des", "Un\u00b7gl\u00fccks", "sind", ".", "Der", "Hirt", "da\u00b7ge\u00b7gen", "sprach", ":", "\u00bb", "Ich", "bin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ART", "NN", "VAFIN", "$.", "ART", "NN", "PAV", "VVFIN", "$.", "$(", "PPER", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Der Meinung, jeder lasse eitles Jammern ruhn,", "tokens": ["Der", "Mei\u00b7nung", ",", "je\u00b7der", "las\u00b7se", "eit\u00b7les", "Jam\u00b7mern", "ruhn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PIS", "VVFIN", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Um besser das, was gegenw\u00e4rtig hilft, zu tun.", "tokens": ["Um", "bes\u00b7ser", "das", ",", "was", "ge\u00b7gen\u00b7w\u00e4r\u00b7tig", "hilft", ",", "zu", "tun", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUI", "ADJD", "PDS", "$,", "PRELS", "ADJD", "VVFIN", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein Klagen heilt; nur Arbeit schenkt Gewinn,", "tokens": ["Kein", "Kla\u00b7gen", "heilt", ";", "nur", "Ar\u00b7beit", "schenkt", "Ge\u00b7winn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$.", "ADV", "NN", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mit ihr verm\u00f6gen wir bis Rom zu ziehen!\u00ab", "tokens": ["Mit", "ihr", "ver\u00b7m\u00f6\u00b7gen", "wir", "bis", "Rom", "zu", "zie\u00b7hen", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PPER", "APPR", "NE", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "So sprach ein Hirt? \u2013 Was findet ihr darin?", "tokens": ["So", "sprach", "ein", "Hirt", "?", "\u2013", "Was", "fin\u00b7det", "ihr", "da\u00b7rin", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "PWS", "VVFIN", "PPER", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Meint ihr, gekr\u00f6nten H\u00e4uptern einzig habe", "tokens": ["Meint", "ihr", ",", "ge\u00b7kr\u00f6n\u00b7ten", "H\u00e4up\u00b7tern", "ein\u00b7zig", "ha\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "ADJA", "NN", "ADJD", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Der Himmel Weisheit und Verstand verliehen,", "tokens": ["Der", "Him\u00b7mel", "Weis\u00b7heit", "und", "Ver\u00b7stand", "ver\u00b7lie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und Hirten h\u00e4tten keine gr\u00f6\u00dfre Geistesgabe", "tokens": ["Und", "Hir\u00b7ten", "h\u00e4t\u00b7ten", "kei\u00b7ne", "gr\u00f6\u00df\u00b7re", "Geis\u00b7tes\u00b7ga\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VAFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Als ihre Schafe? \u2013 Nun, der Rat des Hirten", "tokens": ["Als", "ih\u00b7re", "Scha\u00b7fe", "?", "\u2013", "Nun", ",", "der", "Rat", "des", "Hir\u00b7ten"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "$.", "$(", "ADV", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Gefiel den andern drei,", "tokens": ["Ge\u00b7fiel", "den", "an\u00b7dern", "drei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "CARD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Die an Amerikas Gestade mit ihm irrten.", "tokens": ["Die", "an", "A\u00b7me\u00b7ri\u00b7kas", "Ge\u00b7sta\u00b7de", "mit", "ihm", "irr\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.12": {"text": "Der Kaufmann sprach, er sei", "tokens": ["Der", "Kauf\u00b7mann", "sprach", ",", "er", "sei"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "In Rechenkunst erprobt und wolle Stunden geben.", "tokens": ["In", "Re\u00b7chen\u00b7kunst", "er\u00b7probt", "und", "wol\u00b7le", "Stun\u00b7den", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "KON", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "\u00bbich lehre Politik,\u00ab versprach der K\u00f6nigssohn.", "tokens": ["\u00bb", "ich", "leh\u00b7re", "Po\u00b7li\u00b7tik", ",", "\u00ab", "ver\u00b7sprach", "der", "K\u00f6\u00b7nigs\u00b7sohn", "."], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "NN", "$,", "$(", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "\u00bbich kenne die Heraldik wie mein Leben,\u00ab", "tokens": ["\u00bb", "ich", "ken\u00b7ne", "die", "Her\u00b7al\u00b7dik", "wie", "mein", "Le\u00b7ben", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "NN", "KOKOM", "PPOSAT", "NN", "$,", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.16": {"text": "Erkl\u00e4rte eifrig der Baron;", "tokens": ["Er\u00b7kl\u00e4r\u00b7te", "eif\u00b7rig", "der", "Ba\u00b7ron", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "\u00bbich werde Sch\u00fcler nehmen gegen guten Lohn.\u00ab", "tokens": ["\u00bb", "ich", "wer\u00b7de", "Sch\u00fc\u00b7ler", "neh\u00b7men", "ge\u00b7gen", "gu\u00b7ten", "Lohn", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VAFIN", "NN", "VVFIN", "APPR", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Als ob man auch in jenen wilden Landen schon", "tokens": ["Als", "ob", "man", "auch", "in", "je\u00b7nen", "wil\u00b7den", "Lan\u00b7den", "schon"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "PIS", "ADV", "APPR", "PDAT", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Intresse h\u00e4tte f\u00fcr so leere Eitelkeit.", "tokens": ["I\u00b7ntres\u00b7se", "h\u00e4t\u00b7te", "f\u00fcr", "so", "lee\u00b7re", "Ei\u00b7tel\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.5": {"line.1": {"text": "Es sprach der Hirt: \u00bbRecht gut, doch leider nicht gescheit;", "tokens": ["Es", "sprach", "der", "Hirt", ":", "\u00bb", "Recht", "gut", ",", "doch", "lei\u00b7der", "nicht", "ge\u00b7scheit", ";"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$.", "$(", "NN", "ADJD", "$,", "ADV", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Des Monats drei\u00dfig Tage sind nicht schnell entflohn,", "tokens": ["Des", "Mo\u00b7nats", "drei\u00b7\u00dfig", "Ta\u00b7ge", "sind", "nicht", "schnell", "ent\u00b7flohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "CARD", "NN", "VAFIN", "PTKNEG", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wir k\u00f6nnen schwerlich bis zu eurem Zahltag fasten.", "tokens": ["Wir", "k\u00f6n\u00b7nen", "schwer\u00b7lich", "bis", "zu", "eu\u00b7rem", "Zahl\u00b7tag", "fas\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Die Hoffnung, die ihr zeigt, ist sch\u00f6n, doch liegt sie weit;", "tokens": ["Die", "Hoff\u00b7nung", ",", "die", "ihr", "zeigt", ",", "ist", "sch\u00f6n", ",", "doch", "liegt", "sie", "weit", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "$,", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wir d\u00fcrfen nicht so lange rasten.", "tokens": ["Wir", "d\u00fcr\u00b7fen", "nicht", "so", "lan\u00b7ge", "ras\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich habe Hunger w\u00e4hrenddessen.", "tokens": ["Ich", "ha\u00b7be", "Hun\u00b7ger", "w\u00e4h\u00b7rend\u00b7des\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sagt doch, mit welcher Sicherheit", "tokens": ["Sagt", "doch", ",", "mit", "wel\u00b7cher", "Si\u00b7cher\u00b7heit"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beschafft ihr morgen unser Mittagessen?", "tokens": ["Be\u00b7schafft", "ihr", "mor\u00b7gen", "un\u00b7ser", "Mit\u00b7ta\u00b7ges\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Und wer sorgt heute f\u00fcr das Abendessen?", "tokens": ["Und", "wer", "sorgt", "heu\u00b7te", "f\u00fcr", "das", "A\u00b7ben\u00b7des\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Denn darum handelt sich's vor allen Dingen.", "tokens": ["Denn", "da\u00b7rum", "han\u00b7delt", "sich's", "vor", "al\u00b7len", "Din\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PIS", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Es reicht mir eure Wissenschaft nicht hin,", "tokens": ["Es", "reicht", "mir", "eu\u00b7re", "Wis\u00b7sen\u00b7schaft", "nicht", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPOSAT", "NN", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Doch meine Hand soll Rettung bringen.\u00ab", "tokens": ["Doch", "mei\u00b7ne", "Hand", "soll", "Ret\u00b7tung", "brin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Er sprach's und ging zum Wald und machte Reisig dort,", "tokens": ["Er", "sprach's", "und", "ging", "zum", "Wald", "und", "mach\u00b7te", "Rei\u00b7sig", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "APPRART", "NN", "KON", "VVFIN", "NE", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Verkaufte dieses Tag f\u00fcr Tag, und sein Gewinn,", "tokens": ["Ver\u00b7kauf\u00b7te", "die\u00b7ses", "Tag", "f\u00fcr", "Tag", ",", "und", "sein", "Ge\u00b7winn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PDAT", "NN", "APPR", "NN", "$,", "KON", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "So klein er wohl auch war, verhinderte hinfort,", "tokens": ["So", "klein", "er", "wohl", "auch", "war", ",", "ver\u00b7hin\u00b7der\u00b7te", "hin\u00b7fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "ADV", "ADV", "VAFIN", "$,", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df jene, die gleich ihm nicht lang zu fasten wu\u00dften,", "tokens": ["Da\u00df", "je\u00b7ne", ",", "die", "gleich", "ihm", "nicht", "lang", "zu", "fas\u00b7ten", "wu\u00df\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "$,", "PRELS", "ADV", "PPER", "PTKNEG", "ADJD", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nicht schlie\u00dflich \u00bbdrunten\u00ab ihre Gaben \u00fcben mu\u00dften.", "tokens": ["Nicht", "schlie\u00df\u00b7lich", "\u00bb", "drun\u00b7ten", "\u00ab", "ih\u00b7re", "Ga\u00b7ben", "\u00fc\u00b7ben", "mu\u00df\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$(", "ADV", "$(", "PPOSAT", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Was dies Geschehnis lehrt? \u2013 Das Leben zu erhalten,", "tokens": ["Was", "dies", "Ge\u00b7scheh\u00b7nis", "lehrt", "?", "\u2013", "Das", "Le\u00b7ben", "zu", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDS", "NN", "VVFIN", "$.", "$(", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bedarf\u2019s an K\u00fcnsten wenig nur:", "tokens": ["Be\u00b7da\u00b7rf's", "an", "K\u00fcns\u00b7ten", "we\u00b7nig", "nur", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "ADV", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "La\u00dft nur die Gaben der Natur", "tokens": ["La\u00dft", "nur", "die", "Ga\u00b7ben", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "ADV", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und eure Hand als schnellste Helfer walten!", "tokens": ["Und", "eu\u00b7re", "Hand", "als", "schnells\u00b7te", "Hel\u00b7fer", "wal\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "KOUS", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Vier Weltentdecker, nackt der Wut des Meers entflohn,", "tokens": ["Vier", "Wel\u00b7tent\u00b7de\u00b7cker", ",", "nackt", "der", "Wut", "des", "Meers", "ent\u00b7flohn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "ADJD", "ART", "NN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Ein Kaufmann, ein Baron, ein Hirt, ein K\u00f6nigssohn,", "tokens": ["Ein", "Kauf\u00b7mann", ",", "ein", "Ba\u00b7ron", ",", "ein", "Hirt", ",", "ein", "K\u00f6\u00b7nigs\u00b7sohn", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von gleicher Armut alle jetzt wie Belisar,", "tokens": ["Von", "glei\u00b7cher", "Ar\u00b7mut", "al\u00b7le", "jetzt", "wie", "Be\u00b7li\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PIS", "ADV", "KOKOM", "NE", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Erbettelten zur Lindrung ihrer gro\u00dfen Not", "tokens": ["Er\u00b7bet\u00b7tel\u00b7ten", "zur", "Lind\u00b7rung", "ih\u00b7rer", "gro\u00b7\u00dfen", "Not"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Von fremden Leuten nun ihr Brot.", "tokens": ["Von", "frem\u00b7den", "Leu\u00b7ten", "nun", "ihr", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Zu sagen, welcher Zufall diese kleine Schar", "tokens": ["Zu", "sa\u00b7gen", ",", "wel\u00b7cher", "Zu\u00b7fall", "die\u00b7se", "klei\u00b7ne", "Schar"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$,", "PWAT", "NN", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Zusammenschlo\u00df, aus der ein jeder anders war,", "tokens": ["Zu\u00b7sam\u00b7men\u00b7schlo\u00df", ",", "aus", "der", "ein", "je\u00b7der", "an\u00b7ders", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "PRELS", "ART", "PIS", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Erscheint mir unn\u00fctz ganz und gar.", "tokens": ["Er\u00b7scheint", "mir", "un\u00b7n\u00fctz", "ganz", "und", "gar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ADV", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Sie setzten schlie\u00dflich sich bei einem Brunnen hin", "tokens": ["Sie", "setz\u00b7ten", "schlie\u00df\u00b7lich", "sich", "bei", "ei\u00b7nem", "Brun\u00b7nen", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "PRF", "APPR", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und hielten Rat, die armen Leute.", "tokens": ["Und", "hiel\u00b7ten", "Rat", ",", "die", "ar\u00b7men", "Leu\u00b7te", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NN", "$,", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Der Prinz beklagte, da\u00df die Gro\u00dfen oft die Beute", "tokens": ["Der", "Prinz", "be\u00b7klag\u00b7te", ",", "da\u00df", "die", "Gro\u00b7\u00dfen", "oft", "die", "Beu\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "KOUS", "ART", "NN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Des Ungl\u00fccks sind. Der Hirt dagegen sprach: \u00bbIch bin", "tokens": ["Des", "Un\u00b7gl\u00fccks", "sind", ".", "Der", "Hirt", "da\u00b7ge\u00b7gen", "sprach", ":", "\u00bb", "Ich", "bin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["ART", "NN", "VAFIN", "$.", "ART", "NN", "PAV", "VVFIN", "$.", "$(", "PPER", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Der Meinung, jeder lasse eitles Jammern ruhn,", "tokens": ["Der", "Mei\u00b7nung", ",", "je\u00b7der", "las\u00b7se", "eit\u00b7les", "Jam\u00b7mern", "ruhn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PIS", "VVFIN", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Um besser das, was gegenw\u00e4rtig hilft, zu tun.", "tokens": ["Um", "bes\u00b7ser", "das", ",", "was", "ge\u00b7gen\u00b7w\u00e4r\u00b7tig", "hilft", ",", "zu", "tun", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUI", "ADJD", "PDS", "$,", "PRELS", "ADJD", "VVFIN", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Kein Klagen heilt; nur Arbeit schenkt Gewinn,", "tokens": ["Kein", "Kla\u00b7gen", "heilt", ";", "nur", "Ar\u00b7beit", "schenkt", "Ge\u00b7winn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$.", "ADV", "NN", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mit ihr verm\u00f6gen wir bis Rom zu ziehen!\u00ab", "tokens": ["Mit", "ihr", "ver\u00b7m\u00f6\u00b7gen", "wir", "bis", "Rom", "zu", "zie\u00b7hen", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PPER", "APPR", "NE", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "So sprach ein Hirt? \u2013 Was findet ihr darin?", "tokens": ["So", "sprach", "ein", "Hirt", "?", "\u2013", "Was", "fin\u00b7det", "ihr", "da\u00b7rin", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "PWS", "VVFIN", "PPER", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Meint ihr, gekr\u00f6nten H\u00e4uptern einzig habe", "tokens": ["Meint", "ihr", ",", "ge\u00b7kr\u00f6n\u00b7ten", "H\u00e4up\u00b7tern", "ein\u00b7zig", "ha\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "ADJA", "NN", "ADJD", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Der Himmel Weisheit und Verstand verliehen,", "tokens": ["Der", "Him\u00b7mel", "Weis\u00b7heit", "und", "Ver\u00b7stand", "ver\u00b7lie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und Hirten h\u00e4tten keine gr\u00f6\u00dfre Geistesgabe", "tokens": ["Und", "Hir\u00b7ten", "h\u00e4t\u00b7ten", "kei\u00b7ne", "gr\u00f6\u00df\u00b7re", "Geis\u00b7tes\u00b7ga\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VAFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Als ihre Schafe? \u2013 Nun, der Rat des Hirten", "tokens": ["Als", "ih\u00b7re", "Scha\u00b7fe", "?", "\u2013", "Nun", ",", "der", "Rat", "des", "Hir\u00b7ten"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "$.", "$(", "ADV", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Gefiel den andern drei,", "tokens": ["Ge\u00b7fiel", "den", "an\u00b7dern", "drei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "CARD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Die an Amerikas Gestade mit ihm irrten.", "tokens": ["Die", "an", "A\u00b7me\u00b7ri\u00b7kas", "Ge\u00b7sta\u00b7de", "mit", "ihm", "irr\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.12": {"text": "Der Kaufmann sprach, er sei", "tokens": ["Der", "Kauf\u00b7mann", "sprach", ",", "er", "sei"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "PPER", "VAFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "In Rechenkunst erprobt und wolle Stunden geben.", "tokens": ["In", "Re\u00b7chen\u00b7kunst", "er\u00b7probt", "und", "wol\u00b7le", "Stun\u00b7den", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "KON", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "\u00bbich lehre Politik,\u00ab versprach der K\u00f6nigssohn.", "tokens": ["\u00bb", "ich", "leh\u00b7re", "Po\u00b7li\u00b7tik", ",", "\u00ab", "ver\u00b7sprach", "der", "K\u00f6\u00b7nigs\u00b7sohn", "."], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "NN", "$,", "$(", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "\u00bbich kenne die Heraldik wie mein Leben,\u00ab", "tokens": ["\u00bb", "ich", "ken\u00b7ne", "die", "Her\u00b7al\u00b7dik", "wie", "mein", "Le\u00b7ben", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "NN", "KOKOM", "PPOSAT", "NN", "$,", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.16": {"text": "Erkl\u00e4rte eifrig der Baron;", "tokens": ["Er\u00b7kl\u00e4r\u00b7te", "eif\u00b7rig", "der", "Ba\u00b7ron", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "\u00bbich werde Sch\u00fcler nehmen gegen guten Lohn.\u00ab", "tokens": ["\u00bb", "ich", "wer\u00b7de", "Sch\u00fc\u00b7ler", "neh\u00b7men", "ge\u00b7gen", "gu\u00b7ten", "Lohn", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VAFIN", "NN", "VVFIN", "APPR", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Als ob man auch in jenen wilden Landen schon", "tokens": ["Als", "ob", "man", "auch", "in", "je\u00b7nen", "wil\u00b7den", "Lan\u00b7den", "schon"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "PIS", "ADV", "APPR", "PDAT", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Intresse h\u00e4tte f\u00fcr so leere Eitelkeit.", "tokens": ["I\u00b7ntres\u00b7se", "h\u00e4t\u00b7te", "f\u00fcr", "so", "lee\u00b7re", "Ei\u00b7tel\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.16": {"line.1": {"text": "Es sprach der Hirt: \u00bbRecht gut, doch leider nicht gescheit;", "tokens": ["Es", "sprach", "der", "Hirt", ":", "\u00bb", "Recht", "gut", ",", "doch", "lei\u00b7der", "nicht", "ge\u00b7scheit", ";"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$.", "$(", "NN", "ADJD", "$,", "ADV", "ADV", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "Des Monats drei\u00dfig Tage sind nicht schnell entflohn,", "tokens": ["Des", "Mo\u00b7nats", "drei\u00b7\u00dfig", "Ta\u00b7ge", "sind", "nicht", "schnell", "ent\u00b7flohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "CARD", "NN", "VAFIN", "PTKNEG", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wir k\u00f6nnen schwerlich bis zu eurem Zahltag fasten.", "tokens": ["Wir", "k\u00f6n\u00b7nen", "schwer\u00b7lich", "bis", "zu", "eu\u00b7rem", "Zahl\u00b7tag", "fas\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Die Hoffnung, die ihr zeigt, ist sch\u00f6n, doch liegt sie weit;", "tokens": ["Die", "Hoff\u00b7nung", ",", "die", "ihr", "zeigt", ",", "ist", "sch\u00f6n", ",", "doch", "liegt", "sie", "weit", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "$,", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Wir d\u00fcrfen nicht so lange rasten.", "tokens": ["Wir", "d\u00fcr\u00b7fen", "nicht", "so", "lan\u00b7ge", "ras\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich habe Hunger w\u00e4hrenddessen.", "tokens": ["Ich", "ha\u00b7be", "Hun\u00b7ger", "w\u00e4h\u00b7rend\u00b7des\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sagt doch, mit welcher Sicherheit", "tokens": ["Sagt", "doch", ",", "mit", "wel\u00b7cher", "Si\u00b7cher\u00b7heit"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beschafft ihr morgen unser Mittagessen?", "tokens": ["Be\u00b7schafft", "ihr", "mor\u00b7gen", "un\u00b7ser", "Mit\u00b7ta\u00b7ges\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Und wer sorgt heute f\u00fcr das Abendessen?", "tokens": ["Und", "wer", "sorgt", "heu\u00b7te", "f\u00fcr", "das", "A\u00b7ben\u00b7des\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Denn darum handelt sich's vor allen Dingen.", "tokens": ["Denn", "da\u00b7rum", "han\u00b7delt", "sich's", "vor", "al\u00b7len", "Din\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PIS", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Es reicht mir eure Wissenschaft nicht hin,", "tokens": ["Es", "reicht", "mir", "eu\u00b7re", "Wis\u00b7sen\u00b7schaft", "nicht", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPOSAT", "NN", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Doch meine Hand soll Rettung bringen.\u00ab", "tokens": ["Doch", "mei\u00b7ne", "Hand", "soll", "Ret\u00b7tung", "brin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Er sprach's und ging zum Wald und machte Reisig dort,", "tokens": ["Er", "sprach's", "und", "ging", "zum", "Wald", "und", "mach\u00b7te", "Rei\u00b7sig", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "APPRART", "NN", "KON", "VVFIN", "NE", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Verkaufte dieses Tag f\u00fcr Tag, und sein Gewinn,", "tokens": ["Ver\u00b7kauf\u00b7te", "die\u00b7ses", "Tag", "f\u00fcr", "Tag", ",", "und", "sein", "Ge\u00b7winn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PDAT", "NN", "APPR", "NN", "$,", "KON", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "So klein er wohl auch war, verhinderte hinfort,", "tokens": ["So", "klein", "er", "wohl", "auch", "war", ",", "ver\u00b7hin\u00b7der\u00b7te", "hin\u00b7fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "ADV", "ADV", "VAFIN", "$,", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df jene, die gleich ihm nicht lang zu fasten wu\u00dften,", "tokens": ["Da\u00df", "je\u00b7ne", ",", "die", "gleich", "ihm", "nicht", "lang", "zu", "fas\u00b7ten", "wu\u00df\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "$,", "PRELS", "ADV", "PPER", "PTKNEG", "ADJD", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nicht schlie\u00dflich \u00bbdrunten\u00ab ihre Gaben \u00fcben mu\u00dften.", "tokens": ["Nicht", "schlie\u00df\u00b7lich", "\u00bb", "drun\u00b7ten", "\u00ab", "ih\u00b7re", "Ga\u00b7ben", "\u00fc\u00b7ben", "mu\u00df\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$(", "ADV", "$(", "PPOSAT", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Was dies Geschehnis lehrt? \u2013 Das Leben zu erhalten,", "tokens": ["Was", "dies", "Ge\u00b7scheh\u00b7nis", "lehrt", "?", "\u2013", "Das", "Le\u00b7ben", "zu", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDS", "NN", "VVFIN", "$.", "$(", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bedarf\u2019s an K\u00fcnsten wenig nur:", "tokens": ["Be\u00b7da\u00b7rf's", "an", "K\u00fcns\u00b7ten", "we\u00b7nig", "nur", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "ADV", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "La\u00dft nur die Gaben der Natur", "tokens": ["La\u00dft", "nur", "die", "Ga\u00b7ben", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "ADV", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und eure Hand als schnellste Helfer walten!", "tokens": ["Und", "eu\u00b7re", "Hand", "als", "schnells\u00b7te", "Hel\u00b7fer", "wal\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "KOUS", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}