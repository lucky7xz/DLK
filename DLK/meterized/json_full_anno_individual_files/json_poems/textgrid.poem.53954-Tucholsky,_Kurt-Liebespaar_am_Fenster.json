{"textgrid.poem.53954": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Liebespaar am Fenster", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dies ist ein Sonntag vormittag;", "tokens": ["Dies", "ist", "ein", "Sonn\u00b7tag", "vor\u00b7mit\u00b7tag", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wir lehnen so zum Spa\u00dfe", "tokens": ["wir", "leh\u00b7nen", "so", "zum", "Spa\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "leicht erm\u00fcdet zum Fenster hinaus", "tokens": ["leicht", "er\u00b7m\u00fc\u00b7det", "zum", "Fens\u00b7ter", "hin\u00b7aus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "APPRART", "NN", "APZR"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "und sehen auf die Stra\u00dfe.", "tokens": ["und", "se\u00b7hen", "auf", "die", "Stra\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die Sonne scheint. Das Leben rinnt.", "tokens": ["Die", "Son\u00b7ne", "scheint", ".", "Das", "Le\u00b7ben", "rinnt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein kleiner Hund, ein dickes Kind . . .", "tokens": ["Ein", "klei\u00b7ner", "Hund", ",", "ein", "di\u00b7ckes", "Kind", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wir haben uns gefunden", "tokens": ["Wir", "ha\u00b7ben", "uns", "ge\u00b7fun\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "f\u00fcr Tage, Wochen, Monate", "tokens": ["f\u00fcr", "Ta\u00b7ge", ",", "Wo\u00b7chen", ",", "Mo\u00b7na\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "und f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.2": {"line.1": {"text": "Ich, der Mann, denke mir nichts.", "tokens": ["Ich", ",", "der", "Mann", ",", "den\u00b7ke", "mir", "nichts", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "$,", "VVFIN", "PPER", "PIS", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Heut kann ich zu Hause bleiben,", "tokens": ["Heut", "kann", "ich", "zu", "Hau\u00b7se", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "heute geh ich nicht ins B\u00fcro \u2013", "tokens": ["heu\u00b7te", "geh", "ich", "nicht", "ins", "B\u00fc\u00b7ro", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "APPRART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": " . . . an die Steuer mu\u00df ich noch schreiben . . . .", "tokens": [".", ".", ".", "an", "die", "Steu\u00b7er", "mu\u00df", "ich", "noch", "schrei\u00b7ben", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "APPR", "ART", "NN", "VMFIN", "PPER", "ADV", "VVINF", "$.", "$.", "$.", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Wieviel Uhr? Ich wei\u00df nicht genau.", "tokens": ["Wie\u00b7viel", "Uhr", "?", "Ich", "wei\u00df", "nicht", "ge\u00b7nau", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$.", "PPER", "VVFIN", "PTKNEG", "ADJD", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Sie ist zu mir wie eine Frau,", "tokens": ["Sie", "ist", "zu", "mir", "wie", "ei\u00b7ne", "Frau", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "ich f\u00fchl mich ihr verbunden", "tokens": ["ich", "f\u00fchl", "mich", "ihr", "ver\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PPER", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "f\u00fcr Tage, Wochen, Monate", "tokens": ["f\u00fcr", "Ta\u00b7ge", ",", "Wo\u00b7chen", ",", "Mo\u00b7na\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "und f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.3": {"line.1": {"text": "Ich, die Frau, bin gern bei ihm.", "tokens": ["Ich", ",", "die", "Frau", ",", "bin", "gern", "bei", "ihm", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "$,", "VAFIN", "ADV", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Von Heiraten wird nicht gesprochen.", "tokens": ["Von", "Hei\u00b7ra\u00b7ten", "wird", "nicht", "ge\u00b7spro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Aber eines Tages will ich ihn mir", "tokens": ["A\u00b7ber", "ei\u00b7nes", "Ta\u00b7ges", "will", "ich", "ihn", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VMFIN", "PPER", "PPER", "PPER"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "ganz und gar unterjochen.", "tokens": ["ganz", "und", "gar", "un\u00b7ter\u00b7jo\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die Dicke, daneben auf ihrem Balkon,", "tokens": ["Die", "Di\u00b7cke", ",", "da\u00b7ne\u00b7ben", "auf", "ih\u00b7rem", "Bal\u00b7kon", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PAV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "gibt ihrem Kinde einen Bonbon", "tokens": ["gibt", "ih\u00b7rem", "Kin\u00b7de", "ei\u00b7nen", "Bon\u00b7bon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "ART", "NN"], "meter": "+--+-+---", "measure": "iambic.tri.invert"}, "line.7": {"text": "und spielt mit ihren Hunden . . .", "tokens": ["und", "spielt", "mit", "ih\u00b7ren", "Hun\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "So soll mein Leben auch einmal sein \u2013", "tokens": ["So", "soll", "mein", "Le\u00b7ben", "auch", "ein\u00b7mal", "sein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPOSAT", "NN", "ADV", "ADV", "VAINF", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "und nicht nur f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "nicht", "nur", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.4": {"line.1": {"text": "Von Kopf zu Kopf umflie\u00dft uns ein Strom;", "tokens": ["Von", "Kopf", "zu", "Kopf", "um\u00b7flie\u00dft", "uns", "ein", "Strom", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "noch sind wir ein Abenteuer.", "tokens": ["noch", "sind", "wir", "ein", "A\u00b7bent\u00b7eu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eines Tages trennen wir uns,", "tokens": ["Ei\u00b7nes", "Ta\u00b7ges", "tren\u00b7nen", "wir", "uns", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PPER", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "eine andere kommt . . . ein neuer . . .", "tokens": ["ei\u00b7ne", "an\u00b7de\u00b7re", "kommt", ".", ".", ".", "ein", "neu\u00b7er", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADJA", "VVFIN", "$.", "$.", "$.", "ART", "ADJA", "$.", "$.", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Oder wir bleiben f\u00fcr immer zusammen;", "tokens": ["O\u00b7der", "wir", "blei\u00b7ben", "f\u00fcr", "im\u00b7mer", "zu\u00b7sam\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADV", "PTKVZ", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "dann erl\u00f6schen die gro\u00dfen Flammen,", "tokens": ["dann", "er\u00b7l\u00f6\u00b7schen", "die", "gro\u00b7\u00dfen", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "Gewohnheit wird, was Liebe war.", "tokens": ["Ge\u00b7wohn\u00b7heit", "wird", ",", "was", "Lie\u00b7be", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "$,", "PWS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und nur in seltenen Sekunden", "tokens": ["Und", "nur", "in", "sel\u00b7te\u00b7nen", "Se\u00b7kun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "blitzt Erinnerung auf an ein sch\u00f6nes Jahr,", "tokens": ["blitzt", "E\u00b7rin\u00b7ne\u00b7rung", "auf", "an", "ein", "sch\u00f6\u00b7nes", "Jahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.10": {"text": "und an Stunden \u2013 an gl\u00fcckliche Stunden.", "tokens": ["und", "an", "Stun\u00b7den", "\u2013", "an", "gl\u00fcck\u00b7li\u00b7che", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "ADJA", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.5": {"line.1": {"text": "Dies ist ein Sonntag vormittag;", "tokens": ["Dies", "ist", "ein", "Sonn\u00b7tag", "vor\u00b7mit\u00b7tag", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wir lehnen so zum Spa\u00dfe", "tokens": ["wir", "leh\u00b7nen", "so", "zum", "Spa\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "leicht erm\u00fcdet zum Fenster hinaus", "tokens": ["leicht", "er\u00b7m\u00fc\u00b7det", "zum", "Fens\u00b7ter", "hin\u00b7aus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "APPRART", "NN", "APZR"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "und sehen auf die Stra\u00dfe.", "tokens": ["und", "se\u00b7hen", "auf", "die", "Stra\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die Sonne scheint. Das Leben rinnt.", "tokens": ["Die", "Son\u00b7ne", "scheint", ".", "Das", "Le\u00b7ben", "rinnt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein kleiner Hund, ein dickes Kind . . .", "tokens": ["Ein", "klei\u00b7ner", "Hund", ",", "ein", "di\u00b7ckes", "Kind", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wir haben uns gefunden", "tokens": ["Wir", "ha\u00b7ben", "uns", "ge\u00b7fun\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "f\u00fcr Tage, Wochen, Monate", "tokens": ["f\u00fcr", "Ta\u00b7ge", ",", "Wo\u00b7chen", ",", "Mo\u00b7na\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "und f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.6": {"line.1": {"text": "Ich, der Mann, denke mir nichts.", "tokens": ["Ich", ",", "der", "Mann", ",", "den\u00b7ke", "mir", "nichts", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "$,", "VVFIN", "PPER", "PIS", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Heut kann ich zu Hause bleiben,", "tokens": ["Heut", "kann", "ich", "zu", "Hau\u00b7se", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "heute geh ich nicht ins B\u00fcro \u2013", "tokens": ["heu\u00b7te", "geh", "ich", "nicht", "ins", "B\u00fc\u00b7ro", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "APPRART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": " . . . an die Steuer mu\u00df ich noch schreiben . . . .", "tokens": [".", ".", ".", "an", "die", "Steu\u00b7er", "mu\u00df", "ich", "noch", "schrei\u00b7ben", ".", ".", ".", "."], "token_info": ["punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$.", "$.", "$.", "APPR", "ART", "NN", "VMFIN", "PPER", "ADV", "VVINF", "$.", "$.", "$.", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Wieviel Uhr? Ich wei\u00df nicht genau.", "tokens": ["Wie\u00b7viel", "Uhr", "?", "Ich", "wei\u00df", "nicht", "ge\u00b7nau", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$.", "PPER", "VVFIN", "PTKNEG", "ADJD", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Sie ist zu mir wie eine Frau,", "tokens": ["Sie", "ist", "zu", "mir", "wie", "ei\u00b7ne", "Frau", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "ich f\u00fchl mich ihr verbunden", "tokens": ["ich", "f\u00fchl", "mich", "ihr", "ver\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PPER", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "f\u00fcr Tage, Wochen, Monate", "tokens": ["f\u00fcr", "Ta\u00b7ge", ",", "Wo\u00b7chen", ",", "Mo\u00b7na\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "und f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.7": {"line.1": {"text": "Ich, die Frau, bin gern bei ihm.", "tokens": ["Ich", ",", "die", "Frau", ",", "bin", "gern", "bei", "ihm", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "NN", "$,", "VAFIN", "ADV", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Von Heiraten wird nicht gesprochen.", "tokens": ["Von", "Hei\u00b7ra\u00b7ten", "wird", "nicht", "ge\u00b7spro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PTKNEG", "VVPP", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Aber eines Tages will ich ihn mir", "tokens": ["A\u00b7ber", "ei\u00b7nes", "Ta\u00b7ges", "will", "ich", "ihn", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VMFIN", "PPER", "PPER", "PPER"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "ganz und gar unterjochen.", "tokens": ["ganz", "und", "gar", "un\u00b7ter\u00b7jo\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die Dicke, daneben auf ihrem Balkon,", "tokens": ["Die", "Di\u00b7cke", ",", "da\u00b7ne\u00b7ben", "auf", "ih\u00b7rem", "Bal\u00b7kon", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PAV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "gibt ihrem Kinde einen Bonbon", "tokens": ["gibt", "ih\u00b7rem", "Kin\u00b7de", "ei\u00b7nen", "Bon\u00b7bon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "ART", "NN"], "meter": "+--+-+---", "measure": "iambic.tri.invert"}, "line.7": {"text": "und spielt mit ihren Hunden . . .", "tokens": ["und", "spielt", "mit", "ih\u00b7ren", "Hun\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "So soll mein Leben auch einmal sein \u2013", "tokens": ["So", "soll", "mein", "Le\u00b7ben", "auch", "ein\u00b7mal", "sein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPOSAT", "NN", "ADV", "ADV", "VAINF", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "und nicht nur f\u00fcr Stunden \u2013 f\u00fcr Stunden.", "tokens": ["und", "nicht", "nur", "f\u00fcr", "Stun\u00b7den", "\u2013", "f\u00fcr", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "APPR", "NN", "$(", "APPR", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.8": {"line.1": {"text": "Von Kopf zu Kopf umflie\u00dft uns ein Strom;", "tokens": ["Von", "Kopf", "zu", "Kopf", "um\u00b7flie\u00dft", "uns", "ein", "Strom", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "noch sind wir ein Abenteuer.", "tokens": ["noch", "sind", "wir", "ein", "A\u00b7bent\u00b7eu\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eines Tages trennen wir uns,", "tokens": ["Ei\u00b7nes", "Ta\u00b7ges", "tren\u00b7nen", "wir", "uns", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PPER", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "eine andere kommt . . . ein neuer . . .", "tokens": ["ei\u00b7ne", "an\u00b7de\u00b7re", "kommt", ".", ".", ".", "ein", "neu\u00b7er", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADJA", "VVFIN", "$.", "$.", "$.", "ART", "ADJA", "$.", "$.", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Oder wir bleiben f\u00fcr immer zusammen;", "tokens": ["O\u00b7der", "wir", "blei\u00b7ben", "f\u00fcr", "im\u00b7mer", "zu\u00b7sam\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADV", "PTKVZ", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "dann erl\u00f6schen die gro\u00dfen Flammen,", "tokens": ["dann", "er\u00b7l\u00f6\u00b7schen", "die", "gro\u00b7\u00dfen", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "Gewohnheit wird, was Liebe war.", "tokens": ["Ge\u00b7wohn\u00b7heit", "wird", ",", "was", "Lie\u00b7be", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "$,", "PWS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und nur in seltenen Sekunden", "tokens": ["Und", "nur", "in", "sel\u00b7te\u00b7nen", "Se\u00b7kun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "blitzt Erinnerung auf an ein sch\u00f6nes Jahr,", "tokens": ["blitzt", "E\u00b7rin\u00b7ne\u00b7rung", "auf", "an", "ein", "sch\u00f6\u00b7nes", "Jahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.10": {"text": "und an Stunden \u2013 an gl\u00fcckliche Stunden.", "tokens": ["und", "an", "Stun\u00b7den", "\u2013", "an", "gl\u00fcck\u00b7li\u00b7che", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$(", "APPR", "ADJA", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}}}}