{"textgrid.poem.53677": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Vorn an der Rampe", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Man ist sehr streng in unsern Orten,", "tokens": ["Man", "ist", "sehr", "streng", "in", "un\u00b7sern", "Or\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "dem Staatsanwalt kann nichts entgehn . . .", "tokens": ["dem", "Staats\u00b7an\u00b7walt", "kann", "nichts", "ent\u00b7gehn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "VVINF", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Drum sprech ich nur in halben Worten \u2013", "tokens": ["Drum", "sprech", "ich", "nur", "in", "hal\u00b7ben", "Wor\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wer Bildung hat, wird mir verstehn!", "tokens": ["wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.2": {"line.1": {"text": "Herr Edschmid, eins der gr\u00f6\u00dften Lichter,", "tokens": ["Herr", "E\u00b7dschmid", ",", "eins", "der", "gr\u00f6\u00df\u00b7ten", "Lich\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "PIS", "ART", "ADJA", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "tut stark an der Grammatik drehn.", "tokens": ["tut", "stark", "an", "der", "Gram\u00b7ma\u00b7tik", "drehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Doch gibts noch ville sch\u00f6nre Dichter . . .", "tokens": ["Doch", "gibts", "noch", "vil\u00b7le", "sch\u00f6n\u00b7re", "Dich\u00b7ter", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Sternheim kennt, wird mir verstehn.", "tokens": ["Wer", "Stern\u00b7heim", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein Sprichwort ist in jedem Falle", "tokens": ["Ein", "Sprich\u00b7wort", "ist", "in", "je\u00b7dem", "Fal\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "gut angebracht und wundersch\u00f6n:", "tokens": ["gut", "an\u00b7ge\u00b7bracht", "und", "wun\u00b7der\u00b7sch\u00f6n", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbdie Dummen werden niemals alle.\u00ab", "tokens": ["\u00bb", "die", "Dum\u00b7men", "wer\u00b7den", "nie\u00b7mals", "al\u00b7le", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "PIS", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Consols hat, wird mir verstehn.", "tokens": ["Wer", "Con\u00b7sols", "hat", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der falsche Wilhelm ziert die Haare", "tokens": ["Der", "fal\u00b7sche", "Wil\u00b7helm", "ziert", "die", "Haa\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der Damens, die auf B\u00e4lle gehn.", "tokens": ["der", "Da\u00b7mens", ",", "die", "auf", "B\u00e4l\u00b7le", "gehn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und doch ist Einfachheit das Wahre.", "tokens": ["Und", "doch", "ist", "Ein\u00b7fach\u00b7heit", "das", "Wah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Holzbock kennt, wird mir verstehn.", "tokens": ["Wer", "Holz\u00b7bock", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Wie keusch war fr\u00fcher die Soubrette!", "tokens": ["Wie", "keusch", "war", "fr\u00fc\u00b7her", "die", "Soub\u00b7ret\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ADJD", "ART", "NN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Heut kannste so viel Akte sehn \u2013!", "tokens": ["Heut", "kanns\u00b7te", "so", "viel", "Ak\u00b7te", "sehn", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VMFIN", "ADV", "PIAT", "NN", "VVINF", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich lieb nur noch in Balltoilette . . .", "tokens": ["Ich", "lieb", "nur", "noch", "in", "Ball\u00b7toi\u00b7let\u00b7te", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "ADJD", "ADV", "ADV", "APPR", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Cellyn kennt, wird mir verstehn.", "tokens": ["Wer", "Cel\u00b7lyn", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wenn einer eigne Ideen hat,", "tokens": ["Wenn", "ei\u00b7ner", "eig\u00b7ne", "I\u00b7deen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dann schreit bestimmt ein Knickebeen,", "tokens": ["dann", "schreit", "be\u00b7stimmt", "ein", "Kni\u00b7cke\u00b7be\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "da\u00df es das alles schon gesehn hat \u2013", "tokens": ["da\u00df", "es", "das", "al\u00b7les", "schon", "ge\u00b7sehn", "hat", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "PIS", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wer Je\u00dfner kennt, wird mir verstehn.", "tokens": ["wer", "Je\u00df\u00b7ner", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ein Frauenarzt, der nichts verratzt hat,", "tokens": ["Ein", "Frau\u00b7en\u00b7arzt", ",", "der", "nichts", "ver\u00b7ratzt", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "kann sich ne Villa kaufen gehn,", "tokens": ["kann", "sich", "ne", "Vil\u00b7la", "kau\u00b7fen", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "NE", "NE", "VVINF", "VVINF", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "weil er sein Geld zusammengekratzt hat.", "tokens": ["weil", "er", "sein", "Geld", "zu\u00b7sam\u00b7men\u00b7ge\u00b7kratzt", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wer Bildung hat, wird mir verstehn.", "tokens": ["Wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "Soll ich von Polletike singen", "tokens": ["Soll", "ich", "von", "Pol\u00b7le\u00b7ti\u00b7ke", "sin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und sagt S. J.: \u00bbNu mach mal een!\u00ab \u2013:", "tokens": ["und", "sagt", "S.", "J.", ":", "\u00bb", "Nu", "mach", "mal", "e\u00b7en", "!", "\u00ab", "\u2013", ":"], "token_info": ["word", "word", "abbreviation", "abbreviation", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "NN", "NE", "$.", "$(", "ADV", "VVFIN", "ADV", "VVPP", "$.", "$(", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "denk ich an G\u00f6tz von Berlichingen.", "tokens": ["denk", "ich", "an", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Bildung hat, wird mir verstehn \u2013!", "tokens": ["Wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$(", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.9": {"line.1": {"text": "Man ist sehr streng in unsern Orten,", "tokens": ["Man", "ist", "sehr", "streng", "in", "un\u00b7sern", "Or\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "dem Staatsanwalt kann nichts entgehn . . .", "tokens": ["dem", "Staats\u00b7an\u00b7walt", "kann", "nichts", "ent\u00b7gehn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "VVINF", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Drum sprech ich nur in halben Worten \u2013", "tokens": ["Drum", "sprech", "ich", "nur", "in", "hal\u00b7ben", "Wor\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wer Bildung hat, wird mir verstehn!", "tokens": ["wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.10": {"line.1": {"text": "Herr Edschmid, eins der gr\u00f6\u00dften Lichter,", "tokens": ["Herr", "E\u00b7dschmid", ",", "eins", "der", "gr\u00f6\u00df\u00b7ten", "Lich\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "PIS", "ART", "ADJA", "NN", "$,"], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.2": {"text": "tut stark an der Grammatik drehn.", "tokens": ["tut", "stark", "an", "der", "Gram\u00b7ma\u00b7tik", "drehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Doch gibts noch ville sch\u00f6nre Dichter . . .", "tokens": ["Doch", "gibts", "noch", "vil\u00b7le", "sch\u00f6n\u00b7re", "Dich\u00b7ter", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Sternheim kennt, wird mir verstehn.", "tokens": ["Wer", "Stern\u00b7heim", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Ein Sprichwort ist in jedem Falle", "tokens": ["Ein", "Sprich\u00b7wort", "ist", "in", "je\u00b7dem", "Fal\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "gut angebracht und wundersch\u00f6n:", "tokens": ["gut", "an\u00b7ge\u00b7bracht", "und", "wun\u00b7der\u00b7sch\u00f6n", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbdie Dummen werden niemals alle.\u00ab", "tokens": ["\u00bb", "die", "Dum\u00b7men", "wer\u00b7den", "nie\u00b7mals", "al\u00b7le", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "PIS", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Consols hat, wird mir verstehn.", "tokens": ["Wer", "Con\u00b7sols", "hat", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Der falsche Wilhelm ziert die Haare", "tokens": ["Der", "fal\u00b7sche", "Wil\u00b7helm", "ziert", "die", "Haa\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "der Damens, die auf B\u00e4lle gehn.", "tokens": ["der", "Da\u00b7mens", ",", "die", "auf", "B\u00e4l\u00b7le", "gehn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und doch ist Einfachheit das Wahre.", "tokens": ["Und", "doch", "ist", "Ein\u00b7fach\u00b7heit", "das", "Wah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Holzbock kennt, wird mir verstehn.", "tokens": ["Wer", "Holz\u00b7bock", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Wie keusch war fr\u00fcher die Soubrette!", "tokens": ["Wie", "keusch", "war", "fr\u00fc\u00b7her", "die", "Soub\u00b7ret\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ADJD", "ART", "NN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Heut kannste so viel Akte sehn \u2013!", "tokens": ["Heut", "kanns\u00b7te", "so", "viel", "Ak\u00b7te", "sehn", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VMFIN", "ADV", "PIAT", "NN", "VVINF", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich lieb nur noch in Balltoilette . . .", "tokens": ["Ich", "lieb", "nur", "noch", "in", "Ball\u00b7toi\u00b7let\u00b7te", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "ADJD", "ADV", "ADV", "APPR", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Cellyn kennt, wird mir verstehn.", "tokens": ["Wer", "Cel\u00b7lyn", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Wenn einer eigne Ideen hat,", "tokens": ["Wenn", "ei\u00b7ner", "eig\u00b7ne", "I\u00b7deen", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "dann schreit bestimmt ein Knickebeen,", "tokens": ["dann", "schreit", "be\u00b7stimmt", "ein", "Kni\u00b7cke\u00b7be\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "da\u00df es das alles schon gesehn hat \u2013", "tokens": ["da\u00df", "es", "das", "al\u00b7les", "schon", "ge\u00b7sehn", "hat", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "PIS", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wer Je\u00dfner kennt, wird mir verstehn.", "tokens": ["wer", "Je\u00df\u00b7ner", "kennt", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Ein Frauenarzt, der nichts verratzt hat,", "tokens": ["Ein", "Frau\u00b7en\u00b7arzt", ",", "der", "nichts", "ver\u00b7ratzt", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "kann sich ne Villa kaufen gehn,", "tokens": ["kann", "sich", "ne", "Vil\u00b7la", "kau\u00b7fen", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "NE", "NE", "VVINF", "VVINF", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "weil er sein Geld zusammengekratzt hat.", "tokens": ["weil", "er", "sein", "Geld", "zu\u00b7sam\u00b7men\u00b7ge\u00b7kratzt", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wer Bildung hat, wird mir verstehn.", "tokens": ["Wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.16": {"line.1": {"text": "Soll ich von Polletike singen", "tokens": ["Soll", "ich", "von", "Pol\u00b7le\u00b7ti\u00b7ke", "sin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und sagt S. J.: \u00bbNu mach mal een!\u00ab \u2013:", "tokens": ["und", "sagt", "S.", "J.", ":", "\u00bb", "Nu", "mach", "mal", "e\u00b7en", "!", "\u00ab", "\u2013", ":"], "token_info": ["word", "word", "abbreviation", "abbreviation", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "NN", "NE", "$.", "$(", "ADV", "VVFIN", "ADV", "VVPP", "$.", "$(", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "denk ich an G\u00f6tz von Berlichingen.", "tokens": ["denk", "ich", "an", "G\u00f6tz", "von", "Ber\u00b7li\u00b7chin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer Bildung hat, wird mir verstehn \u2013!", "tokens": ["Wer", "Bil\u00b7dung", "hat", ",", "wird", "mir", "ver\u00b7stehn", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VAFIN", "PPER", "VVINF", "$(", "$."], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}}}}