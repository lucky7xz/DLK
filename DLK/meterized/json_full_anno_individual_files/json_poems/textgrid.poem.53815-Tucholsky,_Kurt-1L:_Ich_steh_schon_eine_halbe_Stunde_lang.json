{"textgrid.poem.53815": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ich steh schon eine halbe Stunde lang", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich steh schon eine halbe Stunde lang", "tokens": ["Ich", "steh", "schon", "ei\u00b7ne", "hal\u00b7be", "Stun\u00b7de", "lang"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "vor diesem gef\u00fcllten Kleiderschrank.", "tokens": ["vor", "die\u00b7sem", "ge\u00b7f\u00fcll\u00b7ten", "Klei\u00b7der\u00b7schrank", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Was ziehe ich heute nachmittag an \u2013?", "tokens": ["Was", "zie\u00b7he", "ich", "heu\u00b7te", "nach\u00b7mit\u00b7tag", "an", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "NN", "PTKVZ", "$(", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.2": {"line.1": {"text": "Jedes Kleid erinnert mich . . .", "tokens": ["Je\u00b7des", "Kleid", "e\u00b7rin\u00b7nert", "mich", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "$.", "$.", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "also jedes erinnert mich an einen Mann.", "tokens": ["al\u00b7so", "je\u00b7des", "e\u00b7rin\u00b7nert", "mich", "an", "ei\u00b7nen", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "+-+----+-+-+", "measure": "unknown.measure.penta"}}, "stanza.3": {"line.1": {"text": "In diesem Sportkost\u00fcm ritt ich den Pony.", "tokens": ["In", "die\u00b7sem", "Sport\u00b7kos\u00b7t\u00fcm", "ritt", "ich", "den", "Po\u00b7ny", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "In diesem braunen k\u00fc\u00dfte mich Jonny.", "tokens": ["In", "die\u00b7sem", "brau\u00b7nen", "k\u00fc\u00df\u00b7te", "mich", "Jon\u00b7ny", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "VVFIN", "PPER", "NE", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das da hab ich an dem Abend getragen,", "tokens": ["Das", "da", "hab", "ich", "an", "dem", "A\u00b7bend", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "da kriegte Erich den Doktor am Kragen,", "tokens": ["da", "krieg\u00b7te", "E\u00b7rich", "den", "Dok\u00b7tor", "am", "Kra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "wegen frech . . .", "tokens": ["we\u00b7gen", "frech", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ADJD", "$.", "$.", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Hier go\u00df mir seinerzeit", "tokens": ["Hier", "go\u00df", "mir", "sei\u00b7ner\u00b7zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "der Assessor die So\u00dfe \u00fcbers Kleid", "tokens": ["der", "As\u00b7ses\u00b7sor", "die", "So\u00b7\u00dfe", "\u00fc\u00b7bers", "Kleid"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "und bewies mir hinterher klar und kalt,", "tokens": ["und", "be\u00b7wies", "mir", "hin\u00b7ter\u00b7her", "klar", "und", "kalt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "nach BGB sei das h\u00f6hre Gewalt.", "tokens": ["nach", "BgB", "sei", "das", "h\u00f6h\u00b7re", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.10": {"text": "Tolpatsch.", "tokens": ["Tol\u00b7patsch", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.4": {"line.1": {"text": "In dem . . . also das will ich vergessen . . .", "tokens": ["In", "dem", ".", ".", ".", "al\u00b7so", "das", "will", "ich", "ver\u00b7ges\u00b7sen", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "$.", "$.", "$.", "ADV", "PDS", "VMFIN", "PPER", "VVPP", "$.", "$.", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "da hab ich mit Joe im Auto gesessen \u2013", "tokens": ["da", "hab", "ich", "mit", "Joe", "im", "Au\u00b7to", "ge\u00b7ses\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NE", "APPRART", "NN", "VVPP", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "und so. Und in dem hat mir Fritz einen Antrag gemacht,", "tokens": ["und", "so", ".", "Und", "in", "dem", "hat", "mir", "Fritz", "ei\u00b7nen", "An\u00b7trag", "ge\u00b7macht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "KON", "APPR", "PDS", "VAFIN", "PPER", "NE", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "und ich habe ihn \u2013 leider \u2013 ausgelacht.", "tokens": ["und", "ich", "ha\u00b7be", "ihn", "\u2013", "lei\u00b7der", "\u2013", "aus\u00b7ge\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "$(", "ADV", "$(", "VVPP", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Dieses hier will ich \u00fcberhaupt nicht mehr sehn:", "tokens": ["Die\u00b7ses", "hier", "will", "ich", "\u00fc\u00b7ber\u00b7haupt", "nicht", "mehr", "sehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "in dem mu\u00dft ich zu dieser dummen Premiere gehn.", "tokens": ["in", "dem", "mu\u00dft", "ich", "zu", "die\u00b7ser", "dum\u00b7men", "Pre\u00b7mie\u00b7re", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VMFIN", "PPER", "APPR", "PDAT", "ADJA", "NN", "VVINF", "$."], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "Und das hier . . .? H\u00e4ngt das noch immer im Schranke . . .?", "tokens": ["Und", "das", "hier", ".", ".", ".", "?", "H\u00e4ngt", "das", "noch", "im\u00b7mer", "im", "Schran\u00b7ke", ".", ".", ".", "?"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KON", "PDS", "ADV", "$.", "$.", "$.", "$.", "VVFIN", "PDS", "ADV", "ADV", "APPRART", "NN", "$.", "$.", "$.", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Sekt macht keine Flecke \u2013? Na, ich danke \u2013!", "tokens": ["Sekt", "macht", "kei\u00b7ne", "Fle\u00b7cke", "\u2013", "?", "Na", ",", "ich", "dan\u00b7ke", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "PIAT", "NN", "$(", "$.", "ITJ", "$,", "PPER", "VVFIN", "$(", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.9": {"text": "Und den Mantel \u2013 ich will das nicht mehr wissen \u2013", "tokens": ["Und", "den", "Man\u00b7tel", "\u2013", "ich", "will", "das", "nicht", "mehr", "wis\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$(", "PPER", "VMFIN", "PDS", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "haben sie mir beim Sechstagerennen zerrissen!", "tokens": ["ha\u00b7ben", "sie", "mir", "beim", "Sechs\u00b7ta\u00b7ge\u00b7ren\u00b7nen", "zer\u00b7ris\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}}, "stanza.5": {"line.1": {"text": "Ich steh schon eine halbe Stunde lang", "tokens": ["Ich", "steh", "schon", "ei\u00b7ne", "hal\u00b7be", "Stun\u00b7de", "lang"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "vor diesem gef\u00fcllten Kleiderschrank:", "tokens": ["vor", "die\u00b7sem", "ge\u00b7f\u00fcll\u00b7ten", "Klei\u00b7der\u00b7schrank", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "das nackteste M\u00e4dchen in ganz Berlin.", "tokens": ["das", "nack\u00b7tes\u00b7te", "M\u00e4d\u00b7chen", "in", "ganz", "Ber\u00b7lin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ADV", "NE", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Wie man sieht:", "tokens": ["Wie", "man", "sieht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Ich habe nichts anzuziehn \u2013!", "tokens": ["Ich", "ha\u00b7be", "nichts", "an\u00b7zu\u00b7ziehn", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVIZU", "$(", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Ich steh schon eine halbe Stunde lang", "tokens": ["Ich", "steh", "schon", "ei\u00b7ne", "hal\u00b7be", "Stun\u00b7de", "lang"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "vor diesem gef\u00fcllten Kleiderschrank.", "tokens": ["vor", "die\u00b7sem", "ge\u00b7f\u00fcll\u00b7ten", "Klei\u00b7der\u00b7schrank", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Was ziehe ich heute nachmittag an \u2013?", "tokens": ["Was", "zie\u00b7he", "ich", "heu\u00b7te", "nach\u00b7mit\u00b7tag", "an", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "NN", "PTKVZ", "$(", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.8": {"line.1": {"text": "Jedes Kleid erinnert mich . . .", "tokens": ["Je\u00b7des", "Kleid", "e\u00b7rin\u00b7nert", "mich", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "$.", "$.", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "also jedes erinnert mich an einen Mann.", "tokens": ["al\u00b7so", "je\u00b7des", "e\u00b7rin\u00b7nert", "mich", "an", "ei\u00b7nen", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "+-+----+-+-+", "measure": "unknown.measure.penta"}}, "stanza.9": {"line.1": {"text": "In diesem Sportkost\u00fcm ritt ich den Pony.", "tokens": ["In", "die\u00b7sem", "Sport\u00b7kos\u00b7t\u00fcm", "ritt", "ich", "den", "Po\u00b7ny", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "In diesem braunen k\u00fc\u00dfte mich Jonny.", "tokens": ["In", "die\u00b7sem", "brau\u00b7nen", "k\u00fc\u00df\u00b7te", "mich", "Jon\u00b7ny", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "VVFIN", "PPER", "NE", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das da hab ich an dem Abend getragen,", "tokens": ["Das", "da", "hab", "ich", "an", "dem", "A\u00b7bend", "ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "da kriegte Erich den Doktor am Kragen,", "tokens": ["da", "krieg\u00b7te", "E\u00b7rich", "den", "Dok\u00b7tor", "am", "Kra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "wegen frech . . .", "tokens": ["we\u00b7gen", "frech", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ADJD", "$.", "$.", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Hier go\u00df mir seinerzeit", "tokens": ["Hier", "go\u00df", "mir", "sei\u00b7ner\u00b7zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "der Assessor die So\u00dfe \u00fcbers Kleid", "tokens": ["der", "As\u00b7ses\u00b7sor", "die", "So\u00b7\u00dfe", "\u00fc\u00b7bers", "Kleid"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "und bewies mir hinterher klar und kalt,", "tokens": ["und", "be\u00b7wies", "mir", "hin\u00b7ter\u00b7her", "klar", "und", "kalt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "nach BGB sei das h\u00f6hre Gewalt.", "tokens": ["nach", "BgB", "sei", "das", "h\u00f6h\u00b7re", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.10": {"text": "Tolpatsch.", "tokens": ["Tol\u00b7patsch", "."], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.10": {"line.1": {"text": "In dem . . . also das will ich vergessen . . .", "tokens": ["In", "dem", ".", ".", ".", "al\u00b7so", "das", "will", "ich", "ver\u00b7ges\u00b7sen", ".", ".", "."], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "$.", "$.", "$.", "ADV", "PDS", "VMFIN", "PPER", "VVPP", "$.", "$.", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "da hab ich mit Joe im Auto gesessen \u2013", "tokens": ["da", "hab", "ich", "mit", "Joe", "im", "Au\u00b7to", "ge\u00b7ses\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NE", "APPRART", "NN", "VVPP", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "und so. Und in dem hat mir Fritz einen Antrag gemacht,", "tokens": ["und", "so", ".", "Und", "in", "dem", "hat", "mir", "Fritz", "ei\u00b7nen", "An\u00b7trag", "ge\u00b7macht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "KON", "APPR", "PDS", "VAFIN", "PPER", "NE", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "und ich habe ihn \u2013 leider \u2013 ausgelacht.", "tokens": ["und", "ich", "ha\u00b7be", "ihn", "\u2013", "lei\u00b7der", "\u2013", "aus\u00b7ge\u00b7lacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "$(", "ADV", "$(", "VVPP", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Dieses hier will ich \u00fcberhaupt nicht mehr sehn:", "tokens": ["Die\u00b7ses", "hier", "will", "ich", "\u00fc\u00b7ber\u00b7haupt", "nicht", "mehr", "sehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "in dem mu\u00dft ich zu dieser dummen Premiere gehn.", "tokens": ["in", "dem", "mu\u00dft", "ich", "zu", "die\u00b7ser", "dum\u00b7men", "Pre\u00b7mie\u00b7re", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VMFIN", "PPER", "APPR", "PDAT", "ADJA", "NN", "VVINF", "$."], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "Und das hier . . .? H\u00e4ngt das noch immer im Schranke . . .?", "tokens": ["Und", "das", "hier", ".", ".", ".", "?", "H\u00e4ngt", "das", "noch", "im\u00b7mer", "im", "Schran\u00b7ke", ".", ".", ".", "?"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["KON", "PDS", "ADV", "$.", "$.", "$.", "$.", "VVFIN", "PDS", "ADV", "ADV", "APPRART", "NN", "$.", "$.", "$.", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Sekt macht keine Flecke \u2013? Na, ich danke \u2013!", "tokens": ["Sekt", "macht", "kei\u00b7ne", "Fle\u00b7cke", "\u2013", "?", "Na", ",", "ich", "dan\u00b7ke", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "PIAT", "NN", "$(", "$.", "ITJ", "$,", "PPER", "VVFIN", "$(", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.9": {"text": "Und den Mantel \u2013 ich will das nicht mehr wissen \u2013", "tokens": ["Und", "den", "Man\u00b7tel", "\u2013", "ich", "will", "das", "nicht", "mehr", "wis\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$(", "PPER", "VMFIN", "PDS", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "haben sie mir beim Sechstagerennen zerrissen!", "tokens": ["ha\u00b7ben", "sie", "mir", "beim", "Sechs\u00b7ta\u00b7ge\u00b7ren\u00b7nen", "zer\u00b7ris\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}}, "stanza.11": {"line.1": {"text": "Ich steh schon eine halbe Stunde lang", "tokens": ["Ich", "steh", "schon", "ei\u00b7ne", "hal\u00b7be", "Stun\u00b7de", "lang"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "vor diesem gef\u00fcllten Kleiderschrank:", "tokens": ["vor", "die\u00b7sem", "ge\u00b7f\u00fcll\u00b7ten", "Klei\u00b7der\u00b7schrank", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "das nackteste M\u00e4dchen in ganz Berlin.", "tokens": ["das", "nack\u00b7tes\u00b7te", "M\u00e4d\u00b7chen", "in", "ganz", "Ber\u00b7lin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ADV", "NE", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Wie man sieht:", "tokens": ["Wie", "man", "sieht", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Ich habe nichts anzuziehn \u2013!", "tokens": ["Ich", "ha\u00b7be", "nichts", "an\u00b7zu\u00b7ziehn", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVIZU", "$(", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}}}}