{"textgrid.poem.56954": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wir sa\u00dfen an zwei Tischen \u2013 wo? \u2013 im All ...", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir sa\u00dfen an zwei Tischen \u2013 wo? \u2013 im All ...", "tokens": ["Wir", "sa\u00b7\u00dfen", "an", "zwei", "Ti\u00b7schen", "\u2013", "wo", "?", "\u2013", "im", "All", "..."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "CARD", "NN", "$(", "PWAV", "$.", "$(", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wars Schenke, Stadt, Land, Stern \u2013 was tuts dazu!", "tokens": ["Wars", "Schen\u00b7ke", ",", "Stadt", ",", "Land", ",", "Stern", "\u2013", "was", "tuts", "da\u00b7zu", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$(", "PWS", "VVFIN", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Wir sa\u00dfen irgendwo im Reich des Lebens ...", "tokens": ["Wir", "sa\u00b7\u00dfen", "ir\u00b7gend\u00b7wo", "im", "Reich", "des", "Le\u00b7bens", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wir sa\u00dfen an zwei Tischen, hier und dort.", "tokens": ["Wir", "sa\u00b7\u00dfen", "an", "zwei", "Ti\u00b7schen", ",", "hier", "und", "dort", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "CARD", "NN", "$,", "ADV", "KON", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Und meine Seele brannte: Fremdes M\u00e4dchen,", "tokens": ["Und", "mei\u00b7ne", "See\u00b7le", "brann\u00b7te", ":", "Frem\u00b7des", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wenn ich in deine Augen dichten d\u00fcrfte \u2013", "tokens": ["wenn", "ich", "in", "dei\u00b7ne", "Au\u00b7gen", "dich\u00b7ten", "d\u00fcrf\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "ADJA", "VMFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wenn dieser k\u00f6nigliche Mund mich lohnte \u2013", "tokens": ["wenn", "die\u00b7ser", "k\u00f6\u00b7nig\u00b7li\u00b7che", "Mund", "mich", "lohn\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und diese k\u00f6nigliche Hand mich kr\u00f6nte \u2013!", "tokens": ["und", "die\u00b7se", "k\u00f6\u00b7nig\u00b7li\u00b7che", "Hand", "mich", "kr\u00f6n\u00b7te", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PDAT", "ADJA", "NN", "PPER", "VVFIN", "$(", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Und deine Seele brannte: Fremder J\u00fcngling,", "tokens": ["Und", "dei\u00b7ne", "See\u00b7le", "brann\u00b7te", ":", "Frem\u00b7der", "J\u00fcng\u00b7ling", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wer bist du, da\u00df du mich so tief erregest \u2013", "tokens": ["wer", "bist", "du", ",", "da\u00df", "du", "mich", "so", "tief", "er\u00b7re\u00b7gest", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "$,", "KOUS", "PPER", "PRF", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "da\u00df ich die Kniee dir umfassen m\u00f6chte \u2013", "tokens": ["da\u00df", "ich", "die", "Kni\u00b7ee", "dir", "um\u00b7fas\u00b7sen", "m\u00f6ch\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPER", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und sagen nichts als: Liebster, Liebster, Liebster \u2013!", "tokens": ["und", "sa\u00b7gen", "nichts", "als", ":", "Liebs\u00b7ter", ",", "Liebs\u00b7ter", ",", "Liebs\u00b7ter", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PIS", "KOKOM", "$.", "NN", "$,", "NN", "$,", "NN", "$(", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Und unsre Seelen schlugen fast zusammen.", "tokens": ["Und", "uns\u00b7re", "See\u00b7len", "schlu\u00b7gen", "fast", "zu\u00b7sam\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Doch jeder blieb an seinem starren Tisch \u2013", "tokens": ["Doch", "je\u00b7der", "blieb", "an", "sei\u00b7nem", "star\u00b7ren", "Tisch", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und stand zuletzt mit denen um ihn auf \u2013", "tokens": ["und", "stand", "zu\u00b7letzt", "mit", "de\u00b7nen", "um", "ihn", "auf", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PRELS", "APPR", "PPER", "APPR", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "und ging hinaus \u2013 und sahn uns nimmermehr.", "tokens": ["und", "ging", "hin\u00b7aus", "\u2013", "und", "sahn", "uns", "nim\u00b7mer\u00b7mehr", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$(", "KON", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Wir sa\u00dfen an zwei Tischen \u2013 wo? \u2013 im All ...", "tokens": ["Wir", "sa\u00b7\u00dfen", "an", "zwei", "Ti\u00b7schen", "\u2013", "wo", "?", "\u2013", "im", "All", "..."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "CARD", "NN", "$(", "PWAV", "$.", "$(", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wars Schenke, Stadt, Land, Stern \u2013 was tuts dazu!", "tokens": ["Wars", "Schen\u00b7ke", ",", "Stadt", ",", "Land", ",", "Stern", "\u2013", "was", "tuts", "da\u00b7zu", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$(", "PWS", "VVFIN", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Wir sa\u00dfen irgendwo im Reich des Lebens ...", "tokens": ["Wir", "sa\u00b7\u00dfen", "ir\u00b7gend\u00b7wo", "im", "Reich", "des", "Le\u00b7bens", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wir sa\u00dfen an zwei Tischen, hier und dort.", "tokens": ["Wir", "sa\u00b7\u00dfen", "an", "zwei", "Ti\u00b7schen", ",", "hier", "und", "dort", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "CARD", "NN", "$,", "ADV", "KON", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Und meine Seele brannte: Fremdes M\u00e4dchen,", "tokens": ["Und", "mei\u00b7ne", "See\u00b7le", "brann\u00b7te", ":", "Frem\u00b7des", "M\u00e4d\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wenn ich in deine Augen dichten d\u00fcrfte \u2013", "tokens": ["wenn", "ich", "in", "dei\u00b7ne", "Au\u00b7gen", "dich\u00b7ten", "d\u00fcrf\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "ADJA", "VMFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wenn dieser k\u00f6nigliche Mund mich lohnte \u2013", "tokens": ["wenn", "die\u00b7ser", "k\u00f6\u00b7nig\u00b7li\u00b7che", "Mund", "mich", "lohn\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und diese k\u00f6nigliche Hand mich kr\u00f6nte \u2013!", "tokens": ["und", "die\u00b7se", "k\u00f6\u00b7nig\u00b7li\u00b7che", "Hand", "mich", "kr\u00f6n\u00b7te", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PDAT", "ADJA", "NN", "PPER", "VVFIN", "$(", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Und deine Seele brannte: Fremder J\u00fcngling,", "tokens": ["Und", "dei\u00b7ne", "See\u00b7le", "brann\u00b7te", ":", "Frem\u00b7der", "J\u00fcng\u00b7ling", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wer bist du, da\u00df du mich so tief erregest \u2013", "tokens": ["wer", "bist", "du", ",", "da\u00df", "du", "mich", "so", "tief", "er\u00b7re\u00b7gest", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "$,", "KOUS", "PPER", "PRF", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "da\u00df ich die Kniee dir umfassen m\u00f6chte \u2013", "tokens": ["da\u00df", "ich", "die", "Kni\u00b7ee", "dir", "um\u00b7fas\u00b7sen", "m\u00f6ch\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPER", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "und sagen nichts als: Liebster, Liebster, Liebster \u2013!", "tokens": ["und", "sa\u00b7gen", "nichts", "als", ":", "Liebs\u00b7ter", ",", "Liebs\u00b7ter", ",", "Liebs\u00b7ter", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PIS", "KOKOM", "$.", "NN", "$,", "NN", "$,", "NN", "$(", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Und unsre Seelen schlugen fast zusammen.", "tokens": ["Und", "uns\u00b7re", "See\u00b7len", "schlu\u00b7gen", "fast", "zu\u00b7sam\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Doch jeder blieb an seinem starren Tisch \u2013", "tokens": ["Doch", "je\u00b7der", "blieb", "an", "sei\u00b7nem", "star\u00b7ren", "Tisch", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und stand zuletzt mit denen um ihn auf \u2013", "tokens": ["und", "stand", "zu\u00b7letzt", "mit", "de\u00b7nen", "um", "ihn", "auf", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PRELS", "APPR", "PPER", "APPR", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "und ging hinaus \u2013 und sahn uns nimmermehr.", "tokens": ["und", "ging", "hin\u00b7aus", "\u2013", "und", "sahn", "uns", "nim\u00b7mer\u00b7mehr", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "$(", "KON", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}