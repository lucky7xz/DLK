{"textgrid.poem.48753": {"metadata": {"author": {"name": "Fleming, Paul", "birth": "N.A.", "death": "N.A."}, "title": "40. Auf den Kosakenberg", "genre": "verse", "period": "N.A.", "pub_year": 1624, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Du durch die Laster selbst so weit berufner H\u00fcgel,", "tokens": ["Du", "durch", "die", "Las\u00b7ter", "selbst", "so", "weit", "be\u00b7ruf\u00b7ner", "H\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "ADV", "ADV", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "dem Ph\u00f6bus alles Haar hat um sein Haupt versengt,", "tokens": ["dem", "Ph\u00f6\u00b7bus", "al\u00b7les", "Haar", "hat", "um", "sein", "Haupt", "ver\u00b7sengt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "NN", "VAFIN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "da keine Dryas sich zu wohnen unterf\u00e4ngt,", "tokens": ["da", "kei\u00b7ne", "Dryas", "sich", "zu", "woh\u00b7nen", "un\u00b7ter\u00b7f\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "PRF", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "wie auch kein menschlichs Mensch, kein Wild und kein Gefl\u00fcgel,", "tokens": ["wie", "auch", "kein", "menschlichs", "Mensch", ",", "kein", "Wild", "und", "kein", "Ge\u00b7fl\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIAT", "ADJA", "NN", "$,", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.2": {"line.1": {"text": "halt' itzo noch nicht an der M\u00f6rder strenge Z\u00fcgel,", "tokens": ["halt'", "it\u00b7zo", "noch", "nicht", "an", "der", "M\u00f6r\u00b7der", "stren\u00b7ge", "Z\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "PTKNEG", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "die an dich ", "tokens": ["die", "an", "dich"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "der dieses Stromes Raub an seine Tannen h\u00e4ngt,", "tokens": ["der", "die\u00b7ses", "Stro\u00b7mes", "Raub", "an", "sei\u00b7ne", "Tan\u00b7nen", "h\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "la\u00df sie ziehn ab und zu mit freiem vollem B\u00fcgel.", "tokens": ["la\u00df", "sie", "ziehn", "ab", "und", "zu", "mit", "frei\u00b7em", "vol\u00b7lem", "B\u00fc\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "PTKVZ", "KON", "APPR", "APPR", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Vollf\u00fchrt der H\u00f6chste das, was er durch uns f\u00e4ngt an,", "tokens": ["Voll\u00b7f\u00fchrt", "der", "H\u00f6chs\u00b7te", "das", ",", "was", "er", "durch", "uns", "f\u00e4ngt", "an", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "PDS", "$,", "PWS", "PPER", "APPR", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "so soll bes\u00e4et stehn dein nie gepfl\u00fcgter Plan,", "tokens": ["so", "soll", "be\u00b7s\u00e4et", "stehn", "dein", "nie", "ge\u00b7pfl\u00fcg\u00b7ter", "Plan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADJD", "VVFIN", "PPOSAT", "ADV", "ADJA", "NN", "$,"], "meter": "-+----+-+-+", "measure": "dactylic.init"}, "line.3": {"text": "der Platz sein eine Stadt verwahrt mit Tor und Riegel.", "tokens": ["der", "Platz", "sein", "ei\u00b7ne", "Stadt", "ver\u00b7wahrt", "mit", "Tor", "und", "Rie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ART", "NN", "VVPP", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Alsdenn so la\u00dft uns sehn, was ein ", "tokens": ["Als\u00b7denn", "so", "la\u00dft", "uns", "sehn", ",", "was", "ein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "VVIMP", "PPER", "VVINF", "$,", "PRELS", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und ob die ", "tokens": ["und", "ob", "die"], "token_info": ["word", "word", "word"], "pos": ["KON", "KOUS", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Der Himmel gibt uns selbst hier\u00fcber Brief und Siegel.", "tokens": ["Der", "Him\u00b7mel", "gibt", "uns", "selbst", "hier\u00b7\u00fc\u00b7ber", "Brief", "und", "Sie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "PAV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Du durch die Laster selbst so weit berufner H\u00fcgel,", "tokens": ["Du", "durch", "die", "Las\u00b7ter", "selbst", "so", "weit", "be\u00b7ruf\u00b7ner", "H\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "ADV", "ADV", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "dem Ph\u00f6bus alles Haar hat um sein Haupt versengt,", "tokens": ["dem", "Ph\u00f6\u00b7bus", "al\u00b7les", "Haar", "hat", "um", "sein", "Haupt", "ver\u00b7sengt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PIAT", "NN", "VAFIN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "da keine Dryas sich zu wohnen unterf\u00e4ngt,", "tokens": ["da", "kei\u00b7ne", "Dryas", "sich", "zu", "woh\u00b7nen", "un\u00b7ter\u00b7f\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "PRF", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "wie auch kein menschlichs Mensch, kein Wild und kein Gefl\u00fcgel,", "tokens": ["wie", "auch", "kein", "menschlichs", "Mensch", ",", "kein", "Wild", "und", "kein", "Ge\u00b7fl\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIAT", "ADJA", "NN", "$,", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.6": {"line.1": {"text": "halt' itzo noch nicht an der M\u00f6rder strenge Z\u00fcgel,", "tokens": ["halt'", "it\u00b7zo", "noch", "nicht", "an", "der", "M\u00f6r\u00b7der", "stren\u00b7ge", "Z\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "PTKNEG", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "die an dich ", "tokens": ["die", "an", "dich"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "der dieses Stromes Raub an seine Tannen h\u00e4ngt,", "tokens": ["der", "die\u00b7ses", "Stro\u00b7mes", "Raub", "an", "sei\u00b7ne", "Tan\u00b7nen", "h\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "la\u00df sie ziehn ab und zu mit freiem vollem B\u00fcgel.", "tokens": ["la\u00df", "sie", "ziehn", "ab", "und", "zu", "mit", "frei\u00b7em", "vol\u00b7lem", "B\u00fc\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "PTKVZ", "KON", "APPR", "APPR", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Vollf\u00fchrt der H\u00f6chste das, was er durch uns f\u00e4ngt an,", "tokens": ["Voll\u00b7f\u00fchrt", "der", "H\u00f6chs\u00b7te", "das", ",", "was", "er", "durch", "uns", "f\u00e4ngt", "an", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "PDS", "$,", "PWS", "PPER", "APPR", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "so soll bes\u00e4et stehn dein nie gepfl\u00fcgter Plan,", "tokens": ["so", "soll", "be\u00b7s\u00e4et", "stehn", "dein", "nie", "ge\u00b7pfl\u00fcg\u00b7ter", "Plan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADJD", "VVFIN", "PPOSAT", "ADV", "ADJA", "NN", "$,"], "meter": "-+----+-+-+", "measure": "dactylic.init"}, "line.3": {"text": "der Platz sein eine Stadt verwahrt mit Tor und Riegel.", "tokens": ["der", "Platz", "sein", "ei\u00b7ne", "Stadt", "ver\u00b7wahrt", "mit", "Tor", "und", "Rie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ART", "NN", "VVPP", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Alsdenn so la\u00dft uns sehn, was ein ", "tokens": ["Als\u00b7denn", "so", "la\u00dft", "uns", "sehn", ",", "was", "ein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "VVIMP", "PPER", "VVINF", "$,", "PRELS", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und ob die ", "tokens": ["und", "ob", "die"], "token_info": ["word", "word", "word"], "pos": ["KON", "KOUS", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Der Himmel gibt uns selbst hier\u00fcber Brief und Siegel.", "tokens": ["Der", "Him\u00b7mel", "gibt", "uns", "selbst", "hier\u00b7\u00fc\u00b7ber", "Brief", "und", "Sie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "PAV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}