{"textgrid.poem.62814": {"metadata": {"author": {"name": "Pfeffel, Gottlieb Konrad", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Affe machte so viel Streiche", "genre": "verse", "period": "N.A.", "pub_year": 1765, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Affe machte so viel Streiche", "tokens": ["Ein", "Af\u00b7fe", "mach\u00b7te", "so", "viel", "Strei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So manche feine Schelmerey,", "tokens": ["So", "man\u00b7che", "fei\u00b7ne", "Schel\u00b7me\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df in dem ganzen K\u00f6nigreiche", "tokens": ["Da\u00df", "in", "dem", "gan\u00b7zen", "K\u00f6\u00b7nig\u00b7rei\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein Ruhm erscholl und selbst der Leu,", "tokens": ["Sein", "Ruhm", "er\u00b7scholl", "und", "selbst", "der", "Leu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "KON", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Freund der K\u00fcnste, zween Emiren", "tokens": ["Ein", "Freund", "der", "K\u00fcns\u00b7te", ",", "zween", "E\u00b7mi\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "$,", "VVFIN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Befahl, ihn auf die Burg zu f\u00fchren.", "tokens": ["Be\u00b7fahl", ",", "ihn", "auf", "die", "Burg", "zu", "f\u00fch\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der Gro\u00dfherr wollte fast zerplatzen,", "tokens": ["Der", "Gro\u00df\u00b7herr", "woll\u00b7te", "fast", "zer\u00b7plat\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als unser Gauckler vor ihn trat;", "tokens": ["Als", "un\u00b7ser", "Gauck\u00b7ler", "vor", "ihn", "trat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch tausend Schw\u00e4nke, tausend Fratzen", "tokens": ["Durch", "tau\u00b7send", "Schw\u00e4n\u00b7ke", ",", "tau\u00b7send", "Frat\u00b7zen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "CARD", "NN", "$,", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Erhielt er gleich den Rang als Rath;", "tokens": ["Er\u00b7hielt", "er", "gleich", "den", "Rang", "als", "Rath", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "KOUS", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und bald hernach durch Brief und Siegel", "tokens": ["Und", "bald", "her\u00b7nach", "durch", "Brief", "und", "Sie\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den Titel: Ritter Eulenspiegel.", "tokens": ["Den", "Ti\u00b7tel", ":", "Rit\u00b7ter", "Eu\u00b7len\u00b7spie\u00b7gel", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "NE", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Im Anfang trafen seine Possen", "tokens": ["Im", "An\u00b7fang", "tra\u00b7fen", "sei\u00b7ne", "Pos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den Sch\u00f6ps, den Esel und das Rind,", "tokens": ["Den", "Sch\u00f6ps", ",", "den", "E\u00b7sel", "und", "das", "Rind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Kleeblatt, dem des Sp\u00f6tters Glossen", "tokens": ["Ein", "Klee\u00b7blatt", ",", "dem", "des", "Sp\u00f6t\u00b7ters", "Glos\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von Alters her gewidmet sind.", "tokens": ["Von", "Al\u00b7ters", "her", "ge\u00b7wid\u00b7met", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APZR", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Allein sie schwiegen, oder machten", "tokens": ["Al\u00b7lein", "sie", "schwie\u00b7gen", ",", "o\u00b7der", "mach\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "PPER", "VVFIN", "$,", "KON", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Gar Choro mit, wenn andre lachten.", "tokens": ["Gar", "Cho\u00b7ro", "mit", ",", "wenn", "and\u00b7re", "lach\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "PTKVZ", "$,", "KOUS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Beyfall, der ihn warnen sollte,", "tokens": ["Der", "Bey\u00b7fall", ",", "der", "ihn", "war\u00b7nen", "soll\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Des K\u00f6nigs Gunst, berauschten ihn,", "tokens": ["Des", "K\u00f6\u00b7nigs", "Gunst", ",", "be\u00b7rauschten", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.3": {"text": "Indem er mehr noch gl\u00e4nzen wollte", "tokens": ["In\u00b7dem", "er", "mehr", "noch", "gl\u00e4n\u00b7zen", "woll\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVINF", "VMFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Verga\u00df sich unser Harlekin,", "tokens": ["Ver\u00b7ga\u00df", "sich", "un\u00b7ser", "Har\u00b7le\u00b7kin", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und \u00fcbte seine Neckereyen", "tokens": ["Und", "\u00fcb\u00b7te", "sei\u00b7ne", "Ne\u00b7cke\u00b7re\u00b7yen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "Am Tiger, Wolf und andern Beyen.", "tokens": ["Am", "Ti\u00b7ger", ",", "Wolf", "und", "an\u00b7dern", "Be\u00b7yen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "NE", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.5": {"line.1": {"text": "Nach einer Zeit von sieben Tagen", "tokens": ["Nach", "ei\u00b7ner", "Zeit", "von", "sie\u00b7ben", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War Meister Affe so beherzt,", "tokens": ["War", "Meis\u00b7ter", "Af\u00b7fe", "so", "be\u00b7herzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sich an den Leuen selbst zu wagen,", "tokens": ["Sich", "an", "den", "Leu\u00b7en", "selbst", "zu", "wa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und nun war seine Gunst verscherzt.", "tokens": ["Und", "nun", "war", "sei\u00b7ne", "Gunst", "ver\u00b7scherzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Majest\u00e4t, anstatt zu lachen,", "tokens": ["Die", "Ma\u00b7jes\u00b7t\u00e4t", ",", "an\u00b7statt", "zu", "la\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUI", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Befahl ihm den Proce\u00df zu machen.", "tokens": ["Be\u00b7fahl", "ihm", "den", "Pro\u00b7ce\u00df", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Bey Niedern, die dem Spotte weichen,", "tokens": ["Bey", "Nie\u00b7dern", ",", "die", "dem", "Spot\u00b7te", "wei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist er verbl\u00fcmte Tyranney:", "tokens": ["Ist", "er", "ver\u00b7bl\u00fcm\u00b7te", "Ty\u00b7ran\u00b7ney", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bey denen, die an Stand sich gleichen,", "tokens": ["Bey", "de\u00b7nen", ",", "die", "an", "Stand", "sich", "glei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "$,", "PRELS", "APPR", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist er ein Quell der Z\u00e4nkerey:", "tokens": ["Ist", "er", "ein", "Quell", "der", "Z\u00e4n\u00b7ke\u00b7rey", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Bey Gro\u00dfen ist er ein Verbrechen,", "tokens": ["Bey", "Gro\u00b7\u00dfen", "ist", "er", "ein", "Ver\u00b7bre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das sie mit ihren Blitzen r\u00e4chen.", "tokens": ["Das", "sie", "mit", "ih\u00b7ren", "Blit\u00b7zen", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ein Affe machte so viel Streiche", "tokens": ["Ein", "Af\u00b7fe", "mach\u00b7te", "so", "viel", "Strei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So manche feine Schelmerey,", "tokens": ["So", "man\u00b7che", "fei\u00b7ne", "Schel\u00b7me\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df in dem ganzen K\u00f6nigreiche", "tokens": ["Da\u00df", "in", "dem", "gan\u00b7zen", "K\u00f6\u00b7nig\u00b7rei\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein Ruhm erscholl und selbst der Leu,", "tokens": ["Sein", "Ruhm", "er\u00b7scholl", "und", "selbst", "der", "Leu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "KON", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Freund der K\u00fcnste, zween Emiren", "tokens": ["Ein", "Freund", "der", "K\u00fcns\u00b7te", ",", "zween", "E\u00b7mi\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "$,", "VVFIN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Befahl, ihn auf die Burg zu f\u00fchren.", "tokens": ["Be\u00b7fahl", ",", "ihn", "auf", "die", "Burg", "zu", "f\u00fch\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Der Gro\u00dfherr wollte fast zerplatzen,", "tokens": ["Der", "Gro\u00df\u00b7herr", "woll\u00b7te", "fast", "zer\u00b7plat\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als unser Gauckler vor ihn trat;", "tokens": ["Als", "un\u00b7ser", "Gauck\u00b7ler", "vor", "ihn", "trat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch tausend Schw\u00e4nke, tausend Fratzen", "tokens": ["Durch", "tau\u00b7send", "Schw\u00e4n\u00b7ke", ",", "tau\u00b7send", "Frat\u00b7zen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "CARD", "NN", "$,", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Erhielt er gleich den Rang als Rath;", "tokens": ["Er\u00b7hielt", "er", "gleich", "den", "Rang", "als", "Rath", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "KOUS", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und bald hernach durch Brief und Siegel", "tokens": ["Und", "bald", "her\u00b7nach", "durch", "Brief", "und", "Sie\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den Titel: Ritter Eulenspiegel.", "tokens": ["Den", "Ti\u00b7tel", ":", "Rit\u00b7ter", "Eu\u00b7len\u00b7spie\u00b7gel", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "NE", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Im Anfang trafen seine Possen", "tokens": ["Im", "An\u00b7fang", "tra\u00b7fen", "sei\u00b7ne", "Pos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den Sch\u00f6ps, den Esel und das Rind,", "tokens": ["Den", "Sch\u00f6ps", ",", "den", "E\u00b7sel", "und", "das", "Rind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Kleeblatt, dem des Sp\u00f6tters Glossen", "tokens": ["Ein", "Klee\u00b7blatt", ",", "dem", "des", "Sp\u00f6t\u00b7ters", "Glos\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von Alters her gewidmet sind.", "tokens": ["Von", "Al\u00b7ters", "her", "ge\u00b7wid\u00b7met", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APZR", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Allein sie schwiegen, oder machten", "tokens": ["Al\u00b7lein", "sie", "schwie\u00b7gen", ",", "o\u00b7der", "mach\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "PPER", "VVFIN", "$,", "KON", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Gar Choro mit, wenn andre lachten.", "tokens": ["Gar", "Cho\u00b7ro", "mit", ",", "wenn", "and\u00b7re", "lach\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "PTKVZ", "$,", "KOUS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Der Beyfall, der ihn warnen sollte,", "tokens": ["Der", "Bey\u00b7fall", ",", "der", "ihn", "war\u00b7nen", "soll\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Des K\u00f6nigs Gunst, berauschten ihn,", "tokens": ["Des", "K\u00f6\u00b7nigs", "Gunst", ",", "be\u00b7rauschten", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.3": {"text": "Indem er mehr noch gl\u00e4nzen wollte", "tokens": ["In\u00b7dem", "er", "mehr", "noch", "gl\u00e4n\u00b7zen", "woll\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVINF", "VMFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Verga\u00df sich unser Harlekin,", "tokens": ["Ver\u00b7ga\u00df", "sich", "un\u00b7ser", "Har\u00b7le\u00b7kin", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und \u00fcbte seine Neckereyen", "tokens": ["Und", "\u00fcb\u00b7te", "sei\u00b7ne", "Ne\u00b7cke\u00b7re\u00b7yen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "Am Tiger, Wolf und andern Beyen.", "tokens": ["Am", "Ti\u00b7ger", ",", "Wolf", "und", "an\u00b7dern", "Be\u00b7yen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "NE", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "Nach einer Zeit von sieben Tagen", "tokens": ["Nach", "ei\u00b7ner", "Zeit", "von", "sie\u00b7ben", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War Meister Affe so beherzt,", "tokens": ["War", "Meis\u00b7ter", "Af\u00b7fe", "so", "be\u00b7herzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sich an den Leuen selbst zu wagen,", "tokens": ["Sich", "an", "den", "Leu\u00b7en", "selbst", "zu", "wa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und nun war seine Gunst verscherzt.", "tokens": ["Und", "nun", "war", "sei\u00b7ne", "Gunst", "ver\u00b7scherzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Majest\u00e4t, anstatt zu lachen,", "tokens": ["Die", "Ma\u00b7jes\u00b7t\u00e4t", ",", "an\u00b7statt", "zu", "la\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUI", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Befahl ihm den Proce\u00df zu machen.", "tokens": ["Be\u00b7fahl", "ihm", "den", "Pro\u00b7ce\u00df", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Bey Niedern, die dem Spotte weichen,", "tokens": ["Bey", "Nie\u00b7dern", ",", "die", "dem", "Spot\u00b7te", "wei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist er verbl\u00fcmte Tyranney:", "tokens": ["Ist", "er", "ver\u00b7bl\u00fcm\u00b7te", "Ty\u00b7ran\u00b7ney", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bey denen, die an Stand sich gleichen,", "tokens": ["Bey", "de\u00b7nen", ",", "die", "an", "Stand", "sich", "glei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "$,", "PRELS", "APPR", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist er ein Quell der Z\u00e4nkerey:", "tokens": ["Ist", "er", "ein", "Quell", "der", "Z\u00e4n\u00b7ke\u00b7rey", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Bey Gro\u00dfen ist er ein Verbrechen,", "tokens": ["Bey", "Gro\u00b7\u00dfen", "ist", "er", "ein", "Ver\u00b7bre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das sie mit ihren Blitzen r\u00e4chen.", "tokens": ["Das", "sie", "mit", "ih\u00b7ren", "Blit\u00b7zen", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}