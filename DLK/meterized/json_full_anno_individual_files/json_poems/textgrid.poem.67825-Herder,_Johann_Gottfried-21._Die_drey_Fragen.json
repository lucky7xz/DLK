{"textgrid.poem.67825": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "21. Die drey Fragen", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es war ein Ritter, er reist durchs Land,", "tokens": ["Es", "war", "ein", "Rit\u00b7ter", ",", "er", "reist", "durchs", "Land", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er sucht ein Weib sich aus zur Hand.", "tokens": ["Er", "sucht", "ein", "Weib", "sich", "aus", "zur", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PRF", "APPR", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Er kam wohl vor ein'r Wittwe Th\u00fcr,", "tokens": ["Er", "kam", "wohl", "vor", "ein'r", "Witt\u00b7we", "Th\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Drei sch\u00f6ne T\u00f6chter trat'n herf\u00fcr.", "tokens": ["Drei", "sch\u00f6\u00b7ne", "T\u00f6ch\u00b7ter", "trat'n", "her\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Der Ritter, er sah, er sah sie lang;", "tokens": ["Der", "Rit\u00b7ter", ",", "er", "sah", ",", "er", "sah", "sie", "lang", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu w\u00e4hlen war ihm das Herz so bang.", "tokens": ["Zu", "w\u00e4h\u00b7len", "war", "ihm", "das", "Herz", "so", "bang", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Wer antwort't mir der Fragen drei,", "tokens": ["Wer", "ant\u00b7wort't", "mir", "der", "Fra\u00b7gen", "drei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ART", "NN", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu wissen, welch' die Meine sei?", "tokens": ["Zu", "wis\u00b7sen", ",", "welch'", "die", "Mei\u00b7ne", "sei", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PRELS", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "\u00bbleg vor, leg vor uns die Fragen drei,", "tokens": ["\u00bb", "leg", "vor", ",", "leg", "vor", "uns", "die", "Fra\u00b7gen", "drei", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "PTKVZ", "$,", "NE", "APPR", "PPER", "ART", "NN", "CARD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Zu wissen, welch' die Deine sey?\u00ab", "tokens": ["Zu", "wis\u00b7sen", ",", "welch'", "die", "Dei\u00b7ne", "sey", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PRELS", "ART", "PPOSAT", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "\u00bbo, was ist l\u00e4nger, als der Weg daher?", "tokens": ["\u00bb", "o", ",", "was", "ist", "l\u00e4n\u00b7ger", ",", "als", "der", "Weg", "da\u00b7her", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "$,", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Oder was ist tiefer, als das tiefe Meer?", "tokens": ["O\u00b7der", "was", "ist", "tie\u00b7fer", ",", "als", "das", "tie\u00b7fe", "Meer", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.7": {"line.1": {"text": "Oder was ist lauter, als das laute Horn?", "tokens": ["O\u00b7der", "was", "ist", "lau\u00b7ter", ",", "als", "das", "lau\u00b7te", "Horn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "PDS", "VVFIN", "NE", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Oder was ist sch\u00e4rfer, als der scharfe Dorn?", "tokens": ["O\u00b7der", "was", "ist", "sch\u00e4r\u00b7fer", ",", "als", "der", "schar\u00b7fe", "Dorn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.8": {"line.1": {"text": "Oder was ist gr\u00fcner, als gr\u00fcnes Gras?", "tokens": ["O\u00b7der", "was", "ist", "gr\u00fc\u00b7ner", ",", "als", "gr\u00fc\u00b7nes", "Gras", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ADJA", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Oder was ist schlimmer, als ein Weibsbild was?\u00ab", "tokens": ["O\u00b7der", "was", "ist", "schlim\u00b7mer", ",", "als", "ein", "Weibs\u00b7bild", "was", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PWS", "$.", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.9": {"line.1": {"text": "Die Erste, die Zweite sie sannen nach,", "tokens": ["Die", "Ers\u00b7te", ",", "die", "Zwei\u00b7te", "sie", "san\u00b7nen", "nach", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Die Dritte, die j\u00fcngste, die Sch\u00f6nste sprach:", "tokens": ["Die", "Drit\u00b7te", ",", "die", "j\u00fcngs\u00b7te", ",", "die", "Sch\u00f6ns\u00b7te", "sprach", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.10": {"line.1": {"text": "\u00bbo Lieb ist l\u00e4nger, als der Weg daher,", "tokens": ["\u00bb", "o", "Lieb", "ist", "l\u00e4n\u00b7ger", ",", "als", "der", "Weg", "da\u00b7her", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und H\u00f6ll ist tiefer, als das tiefe Meer.", "tokens": ["Und", "H\u00f6ll", "ist", "tie\u00b7fer", ",", "als", "das", "tie\u00b7fe", "Meer", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Und Donner ist lauter, als das laute Horn,", "tokens": ["Und", "Don\u00b7ner", "ist", "lau\u00b7ter", ",", "als", "das", "lau\u00b7te", "Horn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "PDS", "VVFIN", "NE", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und Hunger ist sch\u00e4rfer, als der scharfe Dorn.", "tokens": ["Und", "Hun\u00b7ger", "ist", "sch\u00e4r\u00b7fer", ",", "als", "der", "schar\u00b7fe", "Dorn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Und Gift ist gr\u00fcner als das gr\u00fcne Gras,", "tokens": ["Und", "Gift", "ist", "gr\u00fc\u00b7ner", "als", "das", "gr\u00fc\u00b7ne", "Gras", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und der Teufel ist \u00e4rger, als ein Weibsbild was.\u00ab", "tokens": ["Und", "der", "Teu\u00b7fel", "ist", "\u00e4r\u00b7ger", ",", "als", "ein", "Weibs\u00b7bild", "was", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PWS", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Kaum hatt sie die Fragen beantwort't so,", "tokens": ["Kaum", "hatt", "sie", "die", "Fra\u00b7gen", "be\u00b7ant\u00b7wort't", "so", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVFIN", "ADV", "$,"], "meter": "-+--+--+--", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Der Ritter, er eilt und w\u00e4hlt sie froh.", "tokens": ["Der", "Rit\u00b7ter", ",", "er", "eilt", "und", "w\u00e4hlt", "sie", "froh", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "KON", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Die Erste, die Zweite, sie sannen nach,", "tokens": ["Die", "Ers\u00b7te", ",", "die", "Zwei\u00b7te", ",", "sie", "san\u00b7nen", "nach", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Inde\u00df ihn'n jezt ein Freier gebrach.", "tokens": ["In\u00b7de\u00df", "ihn'n", "jezt", "ein", "Frei\u00b7er", "ge\u00b7brach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}}, "stanza.15": {"line.1": {"text": "Drum liebe M\u00e4dchen seyd auf der Hut,", "tokens": ["Drum", "lie\u00b7be", "M\u00e4d\u00b7chen", "seyd", "auf", "der", "Hut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NN", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Fr\u00e4gt euch ein Freier, antwortet gut.", "tokens": ["Fr\u00e4gt", "euch", "ein", "Frei\u00b7er", ",", "ant\u00b7wor\u00b7tet", "gut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "VVFIN", "ADJD", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.16": {"line.1": {"text": "Es war ein Ritter, er reist durchs Land,", "tokens": ["Es", "war", "ein", "Rit\u00b7ter", ",", "er", "reist", "durchs", "Land", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er sucht ein Weib sich aus zur Hand.", "tokens": ["Er", "sucht", "ein", "Weib", "sich", "aus", "zur", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PRF", "APPR", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Er kam wohl vor ein'r Wittwe Th\u00fcr,", "tokens": ["Er", "kam", "wohl", "vor", "ein'r", "Witt\u00b7we", "Th\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Drei sch\u00f6ne T\u00f6chter trat'n herf\u00fcr.", "tokens": ["Drei", "sch\u00f6\u00b7ne", "T\u00f6ch\u00b7ter", "trat'n", "her\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Der Ritter, er sah, er sah sie lang;", "tokens": ["Der", "Rit\u00b7ter", ",", "er", "sah", ",", "er", "sah", "sie", "lang", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu w\u00e4hlen war ihm das Herz so bang.", "tokens": ["Zu", "w\u00e4h\u00b7len", "war", "ihm", "das", "Herz", "so", "bang", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.19": {"line.1": {"text": "Wer antwort't mir der Fragen drei,", "tokens": ["Wer", "ant\u00b7wort't", "mir", "der", "Fra\u00b7gen", "drei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ART", "NN", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu wissen, welch' die Meine sei?", "tokens": ["Zu", "wis\u00b7sen", ",", "welch'", "die", "Mei\u00b7ne", "sei", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PRELS", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "\u00bbleg vor, leg vor uns die Fragen drei,", "tokens": ["\u00bb", "leg", "vor", ",", "leg", "vor", "uns", "die", "Fra\u00b7gen", "drei", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "PTKVZ", "$,", "NE", "APPR", "PPER", "ART", "NN", "CARD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Zu wissen, welch' die Deine sey?\u00ab", "tokens": ["Zu", "wis\u00b7sen", ",", "welch'", "die", "Dei\u00b7ne", "sey", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PRELS", "ART", "PPOSAT", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "\u00bbo, was ist l\u00e4nger, als der Weg daher?", "tokens": ["\u00bb", "o", ",", "was", "ist", "l\u00e4n\u00b7ger", ",", "als", "der", "Weg", "da\u00b7her", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "$,", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Oder was ist tiefer, als das tiefe Meer?", "tokens": ["O\u00b7der", "was", "ist", "tie\u00b7fer", ",", "als", "das", "tie\u00b7fe", "Meer", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.22": {"line.1": {"text": "Oder was ist lauter, als das laute Horn?", "tokens": ["O\u00b7der", "was", "ist", "lau\u00b7ter", ",", "als", "das", "lau\u00b7te", "Horn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "PDS", "VVFIN", "NE", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Oder was ist sch\u00e4rfer, als der scharfe Dorn?", "tokens": ["O\u00b7der", "was", "ist", "sch\u00e4r\u00b7fer", ",", "als", "der", "schar\u00b7fe", "Dorn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.23": {"line.1": {"text": "Oder was ist gr\u00fcner, als gr\u00fcnes Gras?", "tokens": ["O\u00b7der", "was", "ist", "gr\u00fc\u00b7ner", ",", "als", "gr\u00fc\u00b7nes", "Gras", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ADJA", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Oder was ist schlimmer, als ein Weibsbild was?\u00ab", "tokens": ["O\u00b7der", "was", "ist", "schlim\u00b7mer", ",", "als", "ein", "Weibs\u00b7bild", "was", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PWS", "$.", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.24": {"line.1": {"text": "Die Erste, die Zweite sie sannen nach,", "tokens": ["Die", "Ers\u00b7te", ",", "die", "Zwei\u00b7te", "sie", "san\u00b7nen", "nach", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Die Dritte, die j\u00fcngste, die Sch\u00f6nste sprach:", "tokens": ["Die", "Drit\u00b7te", ",", "die", "j\u00fcngs\u00b7te", ",", "die", "Sch\u00f6ns\u00b7te", "sprach", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.25": {"line.1": {"text": "\u00bbo Lieb ist l\u00e4nger, als der Weg daher,", "tokens": ["\u00bb", "o", "Lieb", "ist", "l\u00e4n\u00b7ger", ",", "als", "der", "Weg", "da\u00b7her", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und H\u00f6ll ist tiefer, als das tiefe Meer.", "tokens": ["Und", "H\u00f6ll", "ist", "tie\u00b7fer", ",", "als", "das", "tie\u00b7fe", "Meer", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.26": {"line.1": {"text": "Und Donner ist lauter, als das laute Horn,", "tokens": ["Und", "Don\u00b7ner", "ist", "lau\u00b7ter", ",", "als", "das", "lau\u00b7te", "Horn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "PDS", "VVFIN", "NE", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und Hunger ist sch\u00e4rfer, als der scharfe Dorn.", "tokens": ["Und", "Hun\u00b7ger", "ist", "sch\u00e4r\u00b7fer", ",", "als", "der", "schar\u00b7fe", "Dorn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.27": {"line.1": {"text": "Und Gift ist gr\u00fcner als das gr\u00fcne Gras,", "tokens": ["Und", "Gift", "ist", "gr\u00fc\u00b7ner", "als", "das", "gr\u00fc\u00b7ne", "Gras", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und der Teufel ist \u00e4rger, als ein Weibsbild was.\u00ab", "tokens": ["Und", "der", "Teu\u00b7fel", "ist", "\u00e4r\u00b7ger", ",", "als", "ein", "Weibs\u00b7bild", "was", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PWS", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.28": {"line.1": {"text": "Kaum hatt sie die Fragen beantwort't so,", "tokens": ["Kaum", "hatt", "sie", "die", "Fra\u00b7gen", "be\u00b7ant\u00b7wort't", "so", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVFIN", "ADV", "$,"], "meter": "-+--+--+--", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Der Ritter, er eilt und w\u00e4hlt sie froh.", "tokens": ["Der", "Rit\u00b7ter", ",", "er", "eilt", "und", "w\u00e4hlt", "sie", "froh", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "KON", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.29": {"line.1": {"text": "Die Erste, die Zweite, sie sannen nach,", "tokens": ["Die", "Ers\u00b7te", ",", "die", "Zwei\u00b7te", ",", "sie", "san\u00b7nen", "nach", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ART", "ADJA", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Inde\u00df ihn'n jezt ein Freier gebrach.", "tokens": ["In\u00b7de\u00df", "ihn'n", "jezt", "ein", "Frei\u00b7er", "ge\u00b7brach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}}, "stanza.30": {"line.1": {"text": "Drum liebe M\u00e4dchen seyd auf der Hut,", "tokens": ["Drum", "lie\u00b7be", "M\u00e4d\u00b7chen", "seyd", "auf", "der", "Hut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NN", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Fr\u00e4gt euch ein Freier, antwortet gut.", "tokens": ["Fr\u00e4gt", "euch", "ein", "Frei\u00b7er", ",", "ant\u00b7wor\u00b7tet", "gut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "VVFIN", "ADJD", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}}}}