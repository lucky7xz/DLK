{"textgrid.poem.47056": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "53.", "genre": "verse", "period": "N.A.", "pub_year": 1827, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Des Sommers, als ich unter bunten Scherzen", "tokens": ["Des", "Som\u00b7mers", ",", "als", "ich", "un\u00b7ter", "bun\u00b7ten", "Scher\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dich vor mir gaukeln sah in H\u00fctt' und Triften,", "tokens": ["Dich", "vor", "mir", "gau\u00b7keln", "sah", "in", "H\u00fctt'", "und", "Trif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPER", "VVINF", "VVFIN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Verga\u00df ich nicht, ein Denkbuch mir zu stiften,", "tokens": ["Ver\u00b7ga\u00df", "ich", "nicht", ",", "ein", "Denk\u00b7buch", "mir", "zu", "stif\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$,", "ART", "NN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Beschreibend manches Blatt von meinem Herzen.", "tokens": ["Be\u00b7schrei\u00b7bend", "man\u00b7ches", "Blatt", "von", "mei\u00b7nem", "Her\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PIAT", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Nun sitzend hier bei der Erinnerung Kerzen,", "tokens": ["Nun", "sit\u00b7zend", "hier", "bei", "der", "E\u00b7rin\u00b7ne\u00b7rung", "Ker\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Still bl\u00e4tternd in den aufgerollten Schriften;", "tokens": ["Still", "bl\u00e4t\u00b7ternd", "in", "den", "auf\u00b7ge\u00b7roll\u00b7ten", "Schrif\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So wie die Biene Honig saugt aus Giften,", "tokens": ["So", "wie", "die", "Bie\u00b7ne", "Ho\u00b7nig", "saugt", "aus", "Gif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "ADJA", "NN", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Saug' ich Erquickung selbst aus meinen Schmerzen.", "tokens": ["Saug'", "ich", "Er\u00b7quic\u00b7kung", "selbst", "aus", "mei\u00b7nen", "Schmer\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "NN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "O hier sind wunderbar verschlungne Chiffern,", "tokens": ["O", "hier", "sind", "wun\u00b7der\u00b7bar", "ver\u00b7schlung\u00b7ne", "Chif\u00b7fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und Amor, der die R\u00e4tsel zu entsiegeln", "tokens": ["Und", "A\u00b7mor", ",", "der", "die", "R\u00e4t\u00b7sel", "zu", "ent\u00b7sie\u00b7geln"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NE", "$,", "PRELS", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Bestellt ward, ist ein tr\u00fcg'rischer Dolmetscher.", "tokens": ["Be\u00b7stellt", "ward", ",", "ist", "ein", "tr\u00fcg'\u00b7ri\u00b7scher", "Dol\u00b7met\u00b7scher", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$,", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Was herb' daran ist, will er nicht entziffern,", "tokens": ["Was", "herb'", "da\u00b7ran", "ist", ",", "will", "er", "nicht", "ent\u00b7zif\u00b7fern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PAV", "VAFIN", "$,", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das S\u00fc\u00dfe aber wei\u00df er abzuspiegeln", "tokens": ["Das", "S\u00fc\u00b7\u00dfe", "a\u00b7ber", "wei\u00df", "er", "ab\u00b7zu\u00b7spie\u00b7geln"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVFIN", "PPER", "VVIZU"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So lieblich, da\u00df vor Luft zerschm\u00f6lzen Gletscher.", "tokens": ["So", "lieb\u00b7lich", ",", "da\u00df", "vor", "Luft", "zer\u00b7schm\u00f6l\u00b7zen", "Glet\u00b7scher", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Des Sommers, als ich unter bunten Scherzen", "tokens": ["Des", "Som\u00b7mers", ",", "als", "ich", "un\u00b7ter", "bun\u00b7ten", "Scher\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dich vor mir gaukeln sah in H\u00fctt' und Triften,", "tokens": ["Dich", "vor", "mir", "gau\u00b7keln", "sah", "in", "H\u00fctt'", "und", "Trif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPER", "VVINF", "VVFIN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Verga\u00df ich nicht, ein Denkbuch mir zu stiften,", "tokens": ["Ver\u00b7ga\u00df", "ich", "nicht", ",", "ein", "Denk\u00b7buch", "mir", "zu", "stif\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$,", "ART", "NN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Beschreibend manches Blatt von meinem Herzen.", "tokens": ["Be\u00b7schrei\u00b7bend", "man\u00b7ches", "Blatt", "von", "mei\u00b7nem", "Her\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PIAT", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Nun sitzend hier bei der Erinnerung Kerzen,", "tokens": ["Nun", "sit\u00b7zend", "hier", "bei", "der", "E\u00b7rin\u00b7ne\u00b7rung", "Ker\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Still bl\u00e4tternd in den aufgerollten Schriften;", "tokens": ["Still", "bl\u00e4t\u00b7ternd", "in", "den", "auf\u00b7ge\u00b7roll\u00b7ten", "Schrif\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So wie die Biene Honig saugt aus Giften,", "tokens": ["So", "wie", "die", "Bie\u00b7ne", "Ho\u00b7nig", "saugt", "aus", "Gif\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "ADJA", "NN", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Saug' ich Erquickung selbst aus meinen Schmerzen.", "tokens": ["Saug'", "ich", "Er\u00b7quic\u00b7kung", "selbst", "aus", "mei\u00b7nen", "Schmer\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "NN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "O hier sind wunderbar verschlungne Chiffern,", "tokens": ["O", "hier", "sind", "wun\u00b7der\u00b7bar", "ver\u00b7schlung\u00b7ne", "Chif\u00b7fern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und Amor, der die R\u00e4tsel zu entsiegeln", "tokens": ["Und", "A\u00b7mor", ",", "der", "die", "R\u00e4t\u00b7sel", "zu", "ent\u00b7sie\u00b7geln"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NE", "$,", "PRELS", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Bestellt ward, ist ein tr\u00fcg'rischer Dolmetscher.", "tokens": ["Be\u00b7stellt", "ward", ",", "ist", "ein", "tr\u00fcg'\u00b7ri\u00b7scher", "Dol\u00b7met\u00b7scher", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$,", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "Was herb' daran ist, will er nicht entziffern,", "tokens": ["Was", "herb'", "da\u00b7ran", "ist", ",", "will", "er", "nicht", "ent\u00b7zif\u00b7fern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PAV", "VAFIN", "$,", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das S\u00fc\u00dfe aber wei\u00df er abzuspiegeln", "tokens": ["Das", "S\u00fc\u00b7\u00dfe", "a\u00b7ber", "wei\u00df", "er", "ab\u00b7zu\u00b7spie\u00b7geln"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVFIN", "PPER", "VVIZU"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So lieblich, da\u00df vor Luft zerschm\u00f6lzen Gletscher.", "tokens": ["So", "lieb\u00b7lich", ",", "da\u00df", "vor", "Luft", "zer\u00b7schm\u00f6l\u00b7zen", "Glet\u00b7scher", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}