{"textgrid.poem.53546": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wenn nun die Ladung Korn und Fett", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn nun die Ladung Korn und Fett", "tokens": ["Wenn", "nun", "die", "La\u00b7dung", "Korn", "und", "Fett"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den Anfang macht zu besserm Leben,", "tokens": ["den", "An\u00b7fang", "macht", "zu", "bes\u00b7serm", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wenn Deutschland erst zu essen h\u00e4tt \u2013:", "tokens": ["wenn", "Deutschland", "erst", "zu", "es\u00b7sen", "h\u00e4tt", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NE", "ADV", "PTKZU", "VVINF", "VAFIN", "$(", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "mein Gott, was wird das alles geben!", "tokens": ["mein", "Gott", ",", "was", "wird", "das", "al\u00b7les", "ge\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VAFIN", "ART", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Zum Beispiel, der, der Schinken schiebt,", "tokens": ["Zum", "Bei\u00b7spiel", ",", "der", ",", "der", "Schin\u00b7ken", "schiebt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PRELS", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wird tiefbek\u00fcmmert ausverkaufen \u2013", "tokens": ["wird", "tief\u00b7be\u00b7k\u00fcm\u00b7mert", "aus\u00b7ver\u00b7kau\u00b7fen", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "man wird, weil es Vergeltung gibt,", "tokens": ["man", "wird", ",", "weil", "es", "Ver\u00b7gel\u00b7tung", "gibt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$,", "KOUS", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich nicht um seine Schinken raufen.", "tokens": ["sich", "nicht", "um", "sei\u00b7ne", "Schin\u00b7ken", "rau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKNEG", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und Tante Malchens Eierschrank?", "tokens": ["Und", "Tan\u00b7te", "Mal\u00b7chens", "Ei\u00b7er\u00b7schrank", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und Onkel Maxens Butterkammer?", "tokens": ["Und", "On\u00b7kel", "Ma\u00b7xens", "But\u00b7ter\u00b7kam\u00b7mer", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie ziehn sie die Gesichter lang!", "tokens": ["Wie", "ziehn", "sie", "die", "Ge\u00b7sich\u00b7ter", "lang", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In allen H\u00e4usern \u2013 welch ein Jammer!", "tokens": ["In", "al\u00b7len", "H\u00e4u\u00b7sern", "\u2013", "welch", "ein", "Jam\u00b7mer", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$(", "PWAT", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Im Kurse f\u00e4llt die Schl\u00e4chterfrau,", "tokens": ["Im", "Kur\u00b7se", "f\u00e4llt", "die", "Schl\u00e4ch\u00b7ter\u00b7frau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "das Butterfr\u00e4ulein gilt nur wenig,", "tokens": ["das", "But\u00b7ter\u00b7fr\u00e4u\u00b7lein", "gilt", "nur", "we\u00b7nig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIS", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der Kaufmann spricht nicht mehr so rauh \u2013", "tokens": ["der", "Kauf\u00b7mann", "spricht", "nicht", "mehr", "so", "rauh", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Halli! hallo! voll Freuden dehn ich", "tokens": ["Hal\u00b7li", "!", "hal\u00b7lo", "!", "voll", "Freu\u00b7den", "dehn", "ich"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NE", "$.", "ADJD", "NN", "VVFIN", "PPER"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "befreit die Knochen. D\u00e4mmert es?", "tokens": ["be\u00b7freit", "die", "Kno\u00b7chen", ".", "D\u00e4m\u00b7mert", "es", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$.", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dies Dasein war seit langen Jahren", "tokens": ["Dies", "Da\u00b7sein", "war", "seit", "lan\u00b7gen", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "NN", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "in Wahrheit ein bel\u00e4mmertes \u2013", "tokens": ["in", "Wahr\u00b7heit", "ein", "be\u00b7l\u00e4m\u00b7mer\u00b7tes", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "ADJA", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ach, wie wir einst so gl\u00fccklich waren!", "tokens": ["Ach", ",", "wie", "wir", "einst", "so", "gl\u00fcck\u00b7lich", "wa\u00b7ren", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PWAV", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Kommt wirklich Brot und Speck herein?", "tokens": ["Kommt", "wirk\u00b7lich", "Brot", "und", "Speck", "her\u00b7ein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich tanze einen frohen L\u00e4ndler.", "tokens": ["Ich", "tan\u00b7ze", "ei\u00b7nen", "fro\u00b7hen", "L\u00e4nd\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die gro\u00dfe Zeit wird wieder klein,", "tokens": ["Die", "gro\u00b7\u00dfe", "Zeit", "wird", "wie\u00b7der", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die gro\u00dfe Zeit der Gr\u00fcnkramh\u00e4ndler.", "tokens": ["die", "gro\u00b7\u00dfe", "Zeit", "der", "Gr\u00fcn\u00b7kram\u00b7h\u00e4nd\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Wenn nun die Ladung Korn und Fett", "tokens": ["Wenn", "nun", "die", "La\u00b7dung", "Korn", "und", "Fett"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "den Anfang macht zu besserm Leben,", "tokens": ["den", "An\u00b7fang", "macht", "zu", "bes\u00b7serm", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wenn Deutschland erst zu essen h\u00e4tt \u2013:", "tokens": ["wenn", "Deutschland", "erst", "zu", "es\u00b7sen", "h\u00e4tt", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NE", "ADV", "PTKZU", "VVINF", "VAFIN", "$(", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "mein Gott, was wird das alles geben!", "tokens": ["mein", "Gott", ",", "was", "wird", "das", "al\u00b7les", "ge\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VAFIN", "ART", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Zum Beispiel, der, der Schinken schiebt,", "tokens": ["Zum", "Bei\u00b7spiel", ",", "der", ",", "der", "Schin\u00b7ken", "schiebt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PRELS", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wird tiefbek\u00fcmmert ausverkaufen \u2013", "tokens": ["wird", "tief\u00b7be\u00b7k\u00fcm\u00b7mert", "aus\u00b7ver\u00b7kau\u00b7fen", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "man wird, weil es Vergeltung gibt,", "tokens": ["man", "wird", ",", "weil", "es", "Ver\u00b7gel\u00b7tung", "gibt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$,", "KOUS", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "sich nicht um seine Schinken raufen.", "tokens": ["sich", "nicht", "um", "sei\u00b7ne", "Schin\u00b7ken", "rau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKNEG", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Und Tante Malchens Eierschrank?", "tokens": ["Und", "Tan\u00b7te", "Mal\u00b7chens", "Ei\u00b7er\u00b7schrank", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und Onkel Maxens Butterkammer?", "tokens": ["Und", "On\u00b7kel", "Ma\u00b7xens", "But\u00b7ter\u00b7kam\u00b7mer", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie ziehn sie die Gesichter lang!", "tokens": ["Wie", "ziehn", "sie", "die", "Ge\u00b7sich\u00b7ter", "lang", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In allen H\u00e4usern \u2013 welch ein Jammer!", "tokens": ["In", "al\u00b7len", "H\u00e4u\u00b7sern", "\u2013", "welch", "ein", "Jam\u00b7mer", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$(", "PWAT", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Im Kurse f\u00e4llt die Schl\u00e4chterfrau,", "tokens": ["Im", "Kur\u00b7se", "f\u00e4llt", "die", "Schl\u00e4ch\u00b7ter\u00b7frau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "das Butterfr\u00e4ulein gilt nur wenig,", "tokens": ["das", "But\u00b7ter\u00b7fr\u00e4u\u00b7lein", "gilt", "nur", "we\u00b7nig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIS", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "der Kaufmann spricht nicht mehr so rauh \u2013", "tokens": ["der", "Kauf\u00b7mann", "spricht", "nicht", "mehr", "so", "rauh", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Halli! hallo! voll Freuden dehn ich", "tokens": ["Hal\u00b7li", "!", "hal\u00b7lo", "!", "voll", "Freu\u00b7den", "dehn", "ich"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NE", "$.", "ADJD", "NN", "VVFIN", "PPER"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "befreit die Knochen. D\u00e4mmert es?", "tokens": ["be\u00b7freit", "die", "Kno\u00b7chen", ".", "D\u00e4m\u00b7mert", "es", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$.", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dies Dasein war seit langen Jahren", "tokens": ["Dies", "Da\u00b7sein", "war", "seit", "lan\u00b7gen", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "NN", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "in Wahrheit ein bel\u00e4mmertes \u2013", "tokens": ["in", "Wahr\u00b7heit", "ein", "be\u00b7l\u00e4m\u00b7mer\u00b7tes", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "ADJA", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ach, wie wir einst so gl\u00fccklich waren!", "tokens": ["Ach", ",", "wie", "wir", "einst", "so", "gl\u00fcck\u00b7lich", "wa\u00b7ren", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PWAV", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Kommt wirklich Brot und Speck herein?", "tokens": ["Kommt", "wirk\u00b7lich", "Brot", "und", "Speck", "her\u00b7ein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich tanze einen frohen L\u00e4ndler.", "tokens": ["Ich", "tan\u00b7ze", "ei\u00b7nen", "fro\u00b7hen", "L\u00e4nd\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die gro\u00dfe Zeit wird wieder klein,", "tokens": ["Die", "gro\u00b7\u00dfe", "Zeit", "wird", "wie\u00b7der", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die gro\u00dfe Zeit der Gr\u00fcnkramh\u00e4ndler.", "tokens": ["die", "gro\u00b7\u00dfe", "Zeit", "der", "Gr\u00fcn\u00b7kram\u00b7h\u00e4nd\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}