{"textgrid.poem.42984": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Zu dir", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sie sprangen aus rasender Eisenbahn", "tokens": ["Sie", "spran\u00b7gen", "aus", "ra\u00b7sen\u00b7der", "Ei\u00b7sen\u00b7bahn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und haben sich gar nicht weh getan.", "tokens": ["Und", "ha\u00b7ben", "sich", "gar", "nicht", "weh", "ge\u00b7tan", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "ADV", "PTKNEG", "ADJD", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Sie wanderten \u00fcber Geleise,", "tokens": ["Sie", "wan\u00b7der\u00b7ten", "\u00fc\u00b7ber", "Ge\u00b7lei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Und wenn ein Zug sie \u00fcberfuhr,", "tokens": ["Und", "wenn", "ein", "Zug", "sie", "\u00fc\u00b7ber\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dann knirschte nichts. Sie lachten nur.", "tokens": ["Dann", "knirschte", "nichts", ".", "Sie", "lach\u00b7ten", "nur", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$.", "PPER", "VVFIN", "ADV", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und weiter ging die Reise.", "tokens": ["Und", "wei\u00b7ter", "ging", "die", "Rei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Sie schritten durch eine steinerne Wand,", "tokens": ["Sie", "schrit\u00b7ten", "durch", "ei\u00b7ne", "stei\u00b7ner\u00b7ne", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch Stacheldr\u00e4hte und W\u00fcstenbrand,", "tokens": ["Durch", "Sta\u00b7chel\u00b7dr\u00e4h\u00b7te", "und", "W\u00fcs\u00b7ten\u00b7brand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Durch Grenzverbote und Schranken", "tokens": ["Durch", "Grenz\u00b7ver\u00b7bo\u00b7te", "und", "Schran\u00b7ken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "KON", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und durch ein vorgehaltnes Gewehr,", "tokens": ["Und", "durch", "ein", "vor\u00b7ge\u00b7halt\u00b7nes", "Ge\u00b7wehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Durchzogen viele Meilen Meer. \u2013", "tokens": ["Durch\u00b7zo\u00b7gen", "vie\u00b7le", "Mei\u00b7len", "Meer", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Meine Gedanken. \u2013", "tokens": ["Mei\u00b7ne", "Ge\u00b7dan\u00b7ken", ".", "\u2013"], "token_info": ["word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "$.", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.5": {"line.1": {"text": "Ihr Kurs ging durch, ging nie vorbei.", "tokens": ["Ihr", "Kurs", "ging", "durch", ",", "ging", "nie", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKVZ", "$,", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und als sie dich erreichten,", "tokens": ["Und", "als", "sie", "dich", "er\u00b7reich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da zitterten sie und erbleichten", "tokens": ["Da", "zit\u00b7ter\u00b7ten", "sie", "und", "er\u00b7bleich\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VVINF"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Und f\u00fchlten sich doch unsagbar frei.", "tokens": ["Und", "f\u00fchl\u00b7ten", "sich", "doch", "un\u00b7sag\u00b7bar", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.6": {"line.1": {"text": "Sie sprangen aus rasender Eisenbahn", "tokens": ["Sie", "spran\u00b7gen", "aus", "ra\u00b7sen\u00b7der", "Ei\u00b7sen\u00b7bahn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und haben sich gar nicht weh getan.", "tokens": ["Und", "ha\u00b7ben", "sich", "gar", "nicht", "weh", "ge\u00b7tan", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "ADV", "PTKNEG", "ADJD", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Sie wanderten \u00fcber Geleise,", "tokens": ["Sie", "wan\u00b7der\u00b7ten", "\u00fc\u00b7ber", "Ge\u00b7lei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Und wenn ein Zug sie \u00fcberfuhr,", "tokens": ["Und", "wenn", "ein", "Zug", "sie", "\u00fc\u00b7ber\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dann knirschte nichts. Sie lachten nur.", "tokens": ["Dann", "knirschte", "nichts", ".", "Sie", "lach\u00b7ten", "nur", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$.", "PPER", "VVFIN", "ADV", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und weiter ging die Reise.", "tokens": ["Und", "wei\u00b7ter", "ging", "die", "Rei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Sie schritten durch eine steinerne Wand,", "tokens": ["Sie", "schrit\u00b7ten", "durch", "ei\u00b7ne", "stei\u00b7ner\u00b7ne", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch Stacheldr\u00e4hte und W\u00fcstenbrand,", "tokens": ["Durch", "Sta\u00b7chel\u00b7dr\u00e4h\u00b7te", "und", "W\u00fcs\u00b7ten\u00b7brand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Durch Grenzverbote und Schranken", "tokens": ["Durch", "Grenz\u00b7ver\u00b7bo\u00b7te", "und", "Schran\u00b7ken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "KON", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und durch ein vorgehaltnes Gewehr,", "tokens": ["Und", "durch", "ein", "vor\u00b7ge\u00b7halt\u00b7nes", "Ge\u00b7wehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Durchzogen viele Meilen Meer. \u2013", "tokens": ["Durch\u00b7zo\u00b7gen", "vie\u00b7le", "Mei\u00b7len", "Meer", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIAT", "NN", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Meine Gedanken. \u2013", "tokens": ["Mei\u00b7ne", "Ge\u00b7dan\u00b7ken", ".", "\u2013"], "token_info": ["word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "$.", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.10": {"line.1": {"text": "Ihr Kurs ging durch, ging nie vorbei.", "tokens": ["Ihr", "Kurs", "ging", "durch", ",", "ging", "nie", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKVZ", "$,", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und als sie dich erreichten,", "tokens": ["Und", "als", "sie", "dich", "er\u00b7reich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da zitterten sie und erbleichten", "tokens": ["Da", "zit\u00b7ter\u00b7ten", "sie", "und", "er\u00b7bleich\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VVINF"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Und f\u00fchlten sich doch unsagbar frei.", "tokens": ["Und", "f\u00fchl\u00b7ten", "sich", "doch", "un\u00b7sag\u00b7bar", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}}}}