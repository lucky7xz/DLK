{"dta.poem.20448": {"metadata": {"author": {"name": "Hofmann von Hofmannswaldau, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Vergn\u00fcgung sein selbst/ die man bey  \n der verachtung sch\u00f6pffen kan.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1695", "urn": "urn:nbn:de:kobv:b4-200905197751", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Was \u00fcberzieht mich vor ein wetter?", "tokens": ["Was", "\u00fc\u00b7berz\u00b7ieht", "mich", "vor", "ein", "wet\u00b7ter", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo st\u00fcrmet alle mi\u00dfgunst her?", "tokens": ["Wo", "st\u00fcr\u00b7met", "al\u00b7le", "mi\u00df\u00b7gunst", "her", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich bin ein ziel-zweck tausend sp\u00f6tter/", "tokens": ["Ich", "bin", "ein", "ziel\u00b7zweck", "tau\u00b7send", "sp\u00f6t\u00b7ter", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "CARD", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mich \u00fcberschwemmt ein tadel-meer.", "tokens": ["Mich", "\u00fc\u00b7bersc\u00b7hwemmt", "ein", "ta\u00b7del\u00b7meer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verleumdung speyet auff mich gallen/", "tokens": ["Ver\u00b7leum\u00b7dung", "spey\u00b7et", "auff", "mich", "gal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und haucht mit schwefel-dunst mich an;", "tokens": ["Und", "haucht", "mit", "schwe\u00b7fel\u00b7dunst", "mich", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ich soll der gantzen welt mi\u00dffallen/", "tokens": ["Ich", "soll", "der", "gant\u00b7zen", "welt", "mi\u00df\u00b7fal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ob keinen gleich ich was gethan.", "tokens": ["Ob", "kei\u00b7nen", "gleich", "ich", "was", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ADV", "PPER", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der eine tadelt das gebl\u00fcte/", "tokens": ["Der", "ei\u00b7ne", "ta\u00b7delt", "das", "ge\u00b7bl\u00fc\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "ART", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dem ist die herkunfft allzu klein;", "tokens": ["Dem", "ist", "die", "her\u00b7kunfft", "all\u00b7zu", "klein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der findet was in dem gem\u00fcthe;", "tokens": ["Der", "fin\u00b7det", "was", "in", "dem", "ge\u00b7m\u00fc\u00b7the", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "APPR", "ART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dem seynd die minen zu gemein;", "tokens": ["Dem", "seynd", "die", "mi\u00b7nen", "zu", "ge\u00b7mein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dem taugt der leib nicht; dems gesichte;", "tokens": ["Dem", "taugt", "der", "leib", "nicht", ";", "dems", "ge\u00b7sich\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "PTKNEG", "$.", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Dem mangelt reichthum und ein stand/", "tokens": ["Dem", "man\u00b7gelt", "reicht\u00b7hum", "und", "ein", "stand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "FM", "FM", "KON", "ART", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Bi\u00df endlich auch an dem ger\u00fcchte", "tokens": ["Bi\u00df", "end\u00b7lich", "auch", "an", "dem", "ge\u00b7r\u00fcch\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ADV", "APPR", "ART", "ADJA"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.8": {"text": "Ein laster-fleckmahl wird gebrannt.", "tokens": ["Ein", "las\u00b7ter\u00b7fleck\u00b7mahl", "wird", "ge\u00b7brannt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Frisch auff/ mein muth/ bey dem gewitter!", "tokens": ["Frisch", "auff", "/", "mein", "muth", "/", "bey", "dem", "ge\u00b7wit\u00b7ter", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "$(", "PPOSAT", "NN", "$(", "APPR", "ART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Du kennest/ wo dein hafen sey.", "tokens": ["Du", "ken\u00b7nest", "/", "wo", "dein", "ha\u00b7fen", "sey", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PWAV", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wer an dir sieht dergleichen splitter/", "tokens": ["Wer", "an", "dir", "sieht", "derg\u00b7lei\u00b7chen", "split\u00b7ter", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PPER", "VVFIN", "PIS", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist selbst nicht von dem balcken frey.", "tokens": ["Ist", "selbst", "nicht", "von", "dem", "bal\u00b7cken", "frey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verachtung kan kein mensch entkommen/", "tokens": ["Ver\u00b7ach\u00b7tung", "kan", "kein", "mensch", "ent\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Man stellt auch g\u00f6ttern fehler aus.", "tokens": ["Man", "stellt", "auch", "g\u00f6t\u00b7tern", "feh\u00b7ler", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wenn tugend du zum schutz genommen/", "tokens": ["Wenn", "tu\u00b7gend", "du", "zum", "schutz", "ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Behauptestu genug dein haus.", "tokens": ["Be\u00b7haup\u00b7tes\u00b7tu", "ge\u00b7nug", "dein", "haus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Bin ich kein hoher vom gebl\u00fcte/", "tokens": ["Bin", "ich", "kein", "ho\u00b7her", "vom", "ge\u00b7bl\u00fc\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "ADJA", "APPRART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bin ich doch auch der schlechste nicht.", "tokens": ["Bin", "ich", "doch", "auch", "der", "schlechs\u00b7te", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ART", "ADJA", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der adel stecket im gem\u00fcthe/", "tokens": ["Der", "a\u00b7del", "ste\u00b7cket", "im", "ge\u00b7m\u00fc\u00b7the", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wer wei\u00df ob mirs daran gebricht?", "tokens": ["Wer", "wei\u00df", "ob", "mirs", "da\u00b7ran", "ge\u00b7bricht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "KOUS", "NE", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die ersten seynd nur schlecht gebohren/", "tokens": ["Die", "ers\u00b7ten", "seynd", "nur", "schlecht", "ge\u00b7boh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ADV", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und schwungen sich durch kunst empor.", "tokens": ["Und", "schwun\u00b7gen", "sich", "durch", "kunst", "em\u00b7por", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wenn tugend ich mein hertz geschworen/", "tokens": ["Wenn", "tu\u00b7gend", "ich", "mein", "hertz", "ge\u00b7schwo\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "PPOSAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Was h\u00e4lt man die geburt mir vor?", "tokens": ["Was", "h\u00e4lt", "man", "die", "ge\u00b7burt", "mir", "vor", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PIS", "ART", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Hab ich gleich itzund keinen tittel/", "tokens": ["Hab", "ich", "gleich", "it\u00b7zund", "kei\u00b7nen", "tit\u00b7tel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wer wei\u00df/ was ich noch werden kan?", "tokens": ["Wer", "wei\u00df", "/", "was", "ich", "noch", "wer\u00b7den", "kan", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$(", "PWS", "PPER", "ADV", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der h\u00f6chste hat noch solche mittel/", "tokens": ["Der", "h\u00f6chs\u00b7te", "hat", "noch", "sol\u00b7che", "mit\u00b7tel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch welche Joseph ward ein mann.", "tokens": ["Durch", "wel\u00b7che", "Jo\u00b7se\u00b7ph", "ward", "ein", "mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "NE", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Was ich nicht bin/ das kan ich hoffen/", "tokens": ["Was", "ich", "nicht", "bin", "/", "das", "kan", "ich", "hof\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "VAFIN", "$(", "PDS", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wenn schwei\u00df mir auff den wangen wacht/", "tokens": ["Wenn", "schwei\u00df", "mir", "auff", "den", "wan\u00b7gen", "wacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "PPER", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Der ehren-tempel ist noch offen/", "tokens": ["Der", "eh\u00b7ren\u00b7tem\u00b7pel", "ist", "noch", "of\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und wird noch t\u00e4glich auffgemacht.", "tokens": ["Und", "wird", "noch", "t\u00e4g\u00b7lich", "auff\u00b7ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Auch armuth macht mich unerschrocken/", "tokens": ["Auch", "ar\u00b7muth", "macht", "mich", "un\u00b7er\u00b7schro\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wer sich vergn\u00fcgt/ ist allzu reich.", "tokens": ["Wer", "sich", "ver\u00b7gn\u00fcgt", "/", "ist", "all\u00b7zu", "reich", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "VVPP", "$(", "VAFIN", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Speist mich der himmel nur mit brocken/", "tokens": ["Speist", "mich", "der", "him\u00b7mel", "nur", "mit", "bro\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sind sie doch manna-k\u00f6rnern gleich.", "tokens": ["Sind", "sie", "doch", "man\u00b7na\u00b7k\u00f6r\u00b7nern", "gleich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIS", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nicht ieder kan in gold zerrinnen;", "tokens": ["Nicht", "ie\u00b7der", "kan", "in", "gold", "zer\u00b7rin\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Gnug/ da\u00df ich nicht darff betteln gehn.", "tokens": ["Gnug", "/", "da\u00df", "ich", "nicht", "darff", "bet\u00b7teln", "gehn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KOUS", "PPER", "PTKNEG", "VMFIN", "VVINF", "VVINF", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Und la\u00df ich keine seide spinnen/", "tokens": ["Und", "la\u00df", "ich", "kei\u00b7ne", "sei\u00b7de", "spin\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "PIAT", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ist unschuld auch in wolle sch\u00f6n.", "tokens": ["Ist", "un\u00b7schuld", "auch", "in", "wol\u00b7le", "sch\u00f6n", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "ADV", "APPR", "VMFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Befinden sp\u00f6tter an mir m\u00e4ngel/", "tokens": ["Be\u00b7fin\u00b7den", "sp\u00f6t\u00b7ter", "an", "mir", "m\u00e4n\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "PPER", "NE", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Welch mensch kan ohne mangel seyn?", "tokens": ["Welch", "mensch", "kan", "oh\u00b7ne", "man\u00b7gel", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "APPR", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es haben auch gefehlt die engel;", "tokens": ["Es", "ha\u00b7ben", "auch", "ge\u00b7fehlt", "die", "en\u00b7gel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und gold ist nicht von schlacken rein.", "tokens": ["Und", "gold", "ist", "nicht", "von", "schla\u00b7cken", "rein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "PTKNEG", "APPR", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die sonne selbst hat ihre flecken;", "tokens": ["Die", "son\u00b7ne", "selbst", "hat", "ih\u00b7re", "fle\u00b7cken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wie sieht man denn auff mich so scharff?", "tokens": ["Wie", "sieht", "man", "denn", "auff", "mich", "so", "scharff", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "ADV", "APPR", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Vielleicht werd ich sie noch bedecken/", "tokens": ["Viel\u00b7leicht", "werd", "ich", "sie", "noch", "be\u00b7de\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wenn mi\u00dfgunst der verh\u00fcllung darff.", "tokens": ["Wenn", "mi\u00df\u00b7gunst", "der", "ver\u00b7h\u00fcl\u00b7lung", "darff", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VMFIN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.8": {"line.1": {"text": "Verleumdet man zuletzt den namen;", "tokens": ["Ver\u00b7leum\u00b7det", "man", "zu\u00b7letzt", "den", "na\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Di\u00df ist ein pfeil/ der alle trifft.", "tokens": ["Di\u00df", "ist", "ein", "pfeil", "/", "der", "al\u00b7le", "trifft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$(", "ART", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der teuffel mischt stets guten saamen/", "tokens": ["Der", "teuf\u00b7fel", "mischt", "stets", "gu\u00b7ten", "saa\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die kr\u00f6te saugt aus rosen gifft.", "tokens": ["Die", "kr\u00f6\u00b7te", "saugt", "aus", "ro\u00b7sen", "gifft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich bin nicht besser als die liljen;", "tokens": ["Ich", "bin", "nicht", "bes\u00b7ser", "als", "die", "lil\u00b7jen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Auff die schmiern k\u00e4fer unflat hin;", "tokens": ["Auff", "die", "schmiern", "k\u00e4\u00b7fer", "un\u00b7flat", "hin", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Kein koth kan ihren schnee nicht tilgen/", "tokens": ["Kein", "koth", "kan", "ih\u00b7ren", "schnee", "nicht", "til\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPOSAT", "NN", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "So bleib ich gleichfalls/ was ich bin.", "tokens": ["So", "bleib", "ich", "gleich\u00b7falls", "/", "was", "ich", "bin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$(", "PWS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "La\u00df immer sp\u00f6tter seyn beflissen", "tokens": ["La\u00df", "im\u00b7mer", "sp\u00f6t\u00b7ter", "seyn", "be\u00b7flis\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVIMP", "ADV", "ADJD", "VAINF", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu schw\u00e4rtzen meinen kleinen ruhm;", "tokens": ["Zu", "schw\u00e4rt\u00b7zen", "mei\u00b7nen", "klei\u00b7nen", "ruhm", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die unschuld/ und ein gut gewissen", "tokens": ["Die", "un\u00b7schuld", "/", "und", "ein", "gut", "ge\u00b7wis\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "KON", "ART", "ADJD", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Verbleiben doch mein eigenthum.", "tokens": ["Ver\u00b7blei\u00b7ben", "doch", "mein", "ei\u00b7gen\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mit Joseph lach ich aller l\u00fcgen/", "tokens": ["Mit", "Jo\u00b7se\u00b7ph", "lach", "ich", "al\u00b7ler", "l\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "PPER", "PIAT", "VVFIN", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Mich kr\u00e4ncket kein verg\u00e4llter spruch.", "tokens": ["Mich", "kr\u00e4n\u00b7cket", "kein", "ver\u00b7g\u00e4ll\u00b7ter", "spruch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wer unterliegt/ wird endlich siegen/", "tokens": ["Wer", "un\u00b7ter\u00b7liegt", "/", "wird", "end\u00b7lich", "sie\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$(", "VAFIN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wenn mi\u00dfgunst trifft ihr eigen fluch.", "tokens": ["Wenn", "mi\u00df\u00b7gunst", "trifft", "ihr", "ei\u00b7gen", "fluch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Mein gl\u00fccke kan nicht immer schlaffen/", "tokens": ["Mein", "gl\u00fc\u00b7cke", "kan", "nicht", "im\u00b7mer", "schlaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Irus wird zuletzt erfreut/", "tokens": ["Ein", "I\u00b7rus", "wird", "zu\u00b7letzt", "er\u00b7freut", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein David bleibt nicht bey den schaafen;", "tokens": ["Ein", "Da\u00b7vid", "bleibt", "nicht", "bey", "den", "schaa\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "PTKNEG", "APPR", "ART", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vielleicht ver\u00e4ndert mich die zeit.", "tokens": ["Viel\u00b7leicht", "ver\u00b7\u00e4n\u00b7dert", "mich", "die", "zeit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich suche keine gunst bey allen/", "tokens": ["Ich", "su\u00b7che", "kei\u00b7ne", "gunst", "bey", "al\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "APPR", "PIAT", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Was acht ich/ was man von mir h\u00e4lt/", "tokens": ["Was", "acht", "ich", "/", "was", "man", "von", "mir", "h\u00e4lt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "CARD", "PPER", "$(", "PWS", "PIS", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Kan ich den klugen nur gefallen/", "tokens": ["Kan", "ich", "den", "klu\u00b7gen", "nur", "ge\u00b7fal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Mi\u00dffall ich willig aller welt.", "tokens": ["Mi\u00df\u00b7fall", "ich", "wil\u00b7lig", "al\u00b7ler", "welt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}