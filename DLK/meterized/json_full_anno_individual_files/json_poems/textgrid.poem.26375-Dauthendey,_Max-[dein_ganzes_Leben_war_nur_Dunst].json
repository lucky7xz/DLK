{"textgrid.poem.26375": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[dein ganzes Leben war nur Dunst]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dein ganzes Leben war nur Dunst,", "tokens": ["Dein", "gan\u00b7zes", "Le\u00b7ben", "war", "nur", "Dunst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Liebst du nicht stets mit edler Kunst.", "tokens": ["Liebst", "du", "nicht", "stets", "mit", "ed\u00b7ler", "Kunst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Und lieben sollst du vor dem Tode,", "tokens": ["Und", "lie\u00b7ben", "sollst", "du", "vor", "dem", "To\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das war von je pariser Mode.", "tokens": ["Das", "war", "von", "je", "pa\u00b7ri\u00b7ser", "Mo\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Die Stadt spricht ganz in meinem Sinn", "tokens": ["Die", "Stadt", "spricht", "ganz", "in", "mei\u00b7nem", "Sinn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und immer zog's mich zu ihr hin.", "tokens": ["Und", "im\u00b7mer", "zo\u00b7g's", "mich", "zu", "ihr", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Ehre ist mehr ein kaltes Feuer,", "tokens": ["Eh\u00b7re", "ist", "mehr", "ein", "kal\u00b7tes", "Feu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Nur Liebe, die w\u00e4rmt ungeheuer,", "tokens": ["Nur", "Lie\u00b7be", ",", "die", "w\u00e4rmt", "un\u00b7ge\u00b7heu\u00b7er", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "PRELS", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Geld gibt dem Leibe vieles Gl\u00fcck,", "tokens": ["Geld", "gibt", "dem", "Lei\u00b7be", "vie\u00b7les", "Gl\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "PIS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch nicht den h\u00f6chsten Augenblick.", "tokens": ["Doch", "nicht", "den", "h\u00f6chs\u00b7ten", "Au\u00b7gen\u00b7blick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Nur Liebe macht im Mark erbeben,", "tokens": ["Nur", "Lie\u00b7be", "macht", "im", "Mark", "er\u00b7be\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Deshalb soll jeder sie erleben.", "tokens": ["Des\u00b7halb", "soll", "je\u00b7der", "sie", "er\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Mir tanzten die Pariser Stra\u00dfen,", "tokens": ["Mir", "tanz\u00b7ten", "die", "Pa\u00b7ri\u00b7ser", "Stra\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Konnt' mich vor Freude nicht mehr lassen,", "tokens": ["Konnt'", "mich", "vor", "Freu\u00b7de", "nicht", "mehr", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPR", "NN", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wu\u00dfte, Frau K\u00f6nigin war da,", "tokens": ["Wu\u00df\u00b7te", ",", "Frau", "K\u00f6\u00b7ni\u00b7gin", "war", "da", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "NN", "VAFIN", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wenn ich sie selbst auch noch nicht sah.", "tokens": ["Wenn", "ich", "sie", "selbst", "auch", "noch", "nicht", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADV", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "W\u00fcnschte durch Mauern jetzt zu sehn", "tokens": ["W\u00fcnschte", "durch", "Mau\u00b7ern", "jetzt", "zu", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "NN", "ADV", "PTKZU", "VVINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und in den H\u00e4usern umzugehn.", "tokens": ["Und", "in", "den", "H\u00e4u\u00b7sern", "um\u00b7zu\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Doch dieses mu\u00dft' ich unterlassen", "tokens": ["Doch", "die\u00b7ses", "mu\u00dft'", "ich", "un\u00b7ter\u00b7las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "VMFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und mich beschr\u00e4nken auf die Stra\u00dfen.", "tokens": ["Und", "mich", "be\u00b7schr\u00e4n\u00b7ken", "auf", "die", "Stra\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Der Zufall spielt gar gern Verstecken,", "tokens": ["Der", "Zu\u00b7fall", "spielt", "gar", "gern", "Ver\u00b7ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mich tat er unverge\u00dflich necken.", "tokens": ["Mich", "tat", "er", "un\u00b7ver\u00b7ge\u00df\u00b7lich", "ne\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Auf einem Dampfboot auf der Seine,", "tokens": ["Auf", "ei\u00b7nem", "Dampf\u00b7boot", "auf", "der", "Sei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als ich an dem Gel\u00e4nder lehne,", "tokens": ["Als", "ich", "an", "dem", "Ge\u00b7l\u00e4n\u00b7der", "leh\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ein ander Boot kam mir entgegen,", "tokens": ["Ein", "an\u00b7der", "Boot", "kam", "mir", "ent\u00b7ge\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da naht \u00bbsie\u00ab wie ein goldner Segen.", "tokens": ["Da", "naht", "\u00bb", "sie", "\u00ab", "wie", "ein", "gold\u00b7ner", "Se\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "PPER", "$(", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Sie tr\u00e4gt ihr stolzestes Gesicht", "tokens": ["Sie", "tr\u00e4gt", "ihr", "stol\u00b7zes\u00b7tes", "Ge\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lebt allein und sieht mich nicht.", "tokens": ["Und", "lebt", "al\u00b7lein", "und", "sieht", "mich", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KON", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Ich z\u00e4hlte nicht einmal bis zwei,", "tokens": ["Ich", "z\u00e4hl\u00b7te", "nicht", "ein\u00b7mal", "bis", "zwei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da war das Boot mit ihr vorbei;", "tokens": ["Da", "war", "das", "Boot", "mit", "ihr", "vor\u00b7bei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Den Dampf tat ich von Grund aus hassen,", "tokens": ["Den", "Dampf", "tat", "ich", "von", "Grund", "aus", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "NN", "APPR", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt war ich wiederum verlassen.", "tokens": ["Jetzt", "war", "ich", "wie\u00b7de\u00b7rum", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Im Schlaf erschien mir dann die Seine", "tokens": ["Im", "Schlaf", "er\u00b7schien", "mir", "dann", "die", "Sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ADV", "ART", "PPOSAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie meiner Sehnsucht lange Tr\u00e4ne,", "tokens": ["Wie", "mei\u00b7ner", "Sehn\u00b7sucht", "lan\u00b7ge", "Tr\u00e4\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Und stets auf einem andern Schiff", "tokens": ["Und", "stets", "auf", "ei\u00b7nem", "an\u00b7dern", "Schiff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schwamm die vor\u00fcber, die ich rief.", "tokens": ["Schwamm", "die", "vor\u00b7\u00fc\u00b7ber", ",", "die", "ich", "rief", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ART", "PTKVZ", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Ich wurde nicht im Suchen lahm,", "tokens": ["Ich", "wur\u00b7de", "nicht", "im", "Su\u00b7chen", "lahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wiederum ein Zufall kam.", "tokens": ["Und", "wie\u00b7de\u00b7rum", "ein", "Zu\u00b7fall", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Kommt man in eine neue Stadt,", "tokens": ["Kommt", "man", "in", "ei\u00b7ne", "neu\u00b7e", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In der man ein paar Freunde hat,", "tokens": ["In", "der", "man", "ein", "paar", "Freun\u00b7de", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIS", "ART", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Geht man zu ihnen mal hinauf", "tokens": ["Geht", "man", "zu", "ih\u00b7nen", "mal", "hin\u00b7auf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "APPR", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sucht die lieben Freunde auf.", "tokens": ["Und", "sucht", "die", "lie\u00b7ben", "Freun\u00b7de", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Mein Freund war Maler von Beruf,", "tokens": ["Mein", "Freund", "war", "Ma\u00b7ler", "von", "Be\u00b7ruf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Am liebsten er die Nacktheit schuf.", "tokens": ["Am", "liebs\u00b7ten", "er", "die", "Nackt\u00b7heit", "schuf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "VVFIN", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Hab' vor den Bildern Platz genommen.", "tokens": ["Hab'", "vor", "den", "Bil\u00b7dern", "Platz", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er sprach: \u00bbDer Wein, der wird gleich kommen.\u00ab", "tokens": ["Er", "sprach", ":", "\u00bb", "Der", "Wein", ",", "der", "wird", "gleich", "kom\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "ART", "NN", "$,", "PRELS", "VAFIN", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Sein Modell warf den Mantel ab,", "tokens": ["Sein", "Mo\u00b7dell", "warf", "den", "Man\u00b7tel", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nackt stand sie da, wie Gott sie gab.", "tokens": ["Nackt", "stand", "sie", "da", ",", "wie", "Gott", "sie", "gab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWAV", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Den Wein tat kleiderlos sie kaufen,", "tokens": ["Den", "Wein", "tat", "klei\u00b7der\u00b7los", "sie", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mich tat es ganz hei\u00df \u00fcberlaufen.", "tokens": ["Mich", "tat", "es", "ganz", "hei\u00df", "\u00fc\u00b7berl\u00b7au\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Ich lobte sehr ihr blankes Haar.", "tokens": ["Ich", "lob\u00b7te", "sehr", "ihr", "blan\u00b7kes", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Freund rief: \u00bbEs ist sonderbar,", "tokens": ["Mein", "Freund", "rief", ":", "\u00bb", "Es", "ist", "son\u00b7der\u00b7bar", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Wie dieses Haar jetzt modisch wird!", "tokens": ["Wie", "die\u00b7ses", "Haar", "jetzt", "mo\u00b7disch", "wird", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDAT", "NN", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Noch st\u00e4rker hat es mich verwirrt", "tokens": ["Noch", "st\u00e4r\u00b7ker", "hat", "es", "mich", "ver\u00b7wirrt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "PRF", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Von einer Dame ", "tokens": ["Von", "ei\u00b7ner", "Da\u00b7me"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Wie eine K\u00f6nigin ist die,", "tokens": ["Wie", "ei\u00b7ne", "K\u00f6\u00b7ni\u00b7gin", "ist", "die", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VAFIN", "ART", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Ihr Haar ist eine hei\u00dfe Krone.\u00ab", "tokens": ["Ihr", "Haar", "ist", "ei\u00b7ne", "hei\u00b7\u00dfe", "Kro\u00b7ne", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich fragte zitternd, wo sie wohne.", "tokens": ["Ich", "frag\u00b7te", "zit\u00b7ternd", ",", "wo", "sie", "woh\u00b7ne", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVPP", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "\u00bbdort steht sie an dem Fenster eben!\u00ab", "tokens": ["\u00bb", "dort", "steht", "sie", "an", "dem", "Fens\u00b7ter", "e\u00b7ben", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Feuer f\u00fchlt' ich mich umgeben,", "tokens": ["Von", "Feu\u00b7er", "f\u00fchlt'", "ich", "mich", "um\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Frau K\u00f6nigin gleich rechter Hand", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "gleich", "rech\u00b7ter", "Hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NN", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im n\u00e4chsten Haus am Fenster stand.", "tokens": ["Im", "n\u00e4chs\u00b7ten", "Haus", "am", "Fens\u00b7ter", "stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Sie sah gerade auf die Uhr:", "tokens": ["Sie", "sah", "ge\u00b7ra\u00b7de", "auf", "die", "Uhr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbo Gott, w\u00e4r' ich ein Zeiger nur!", "tokens": ["\u00bb", "o", "Gott", ",", "w\u00e4r'", "ich", "ein", "Zei\u00b7ger", "nur", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "$,", "VAFIN", "PPER", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Ich w\u00fcrde ihre Blicke lenken,", "tokens": ["Ich", "w\u00fcr\u00b7de", "ih\u00b7re", "Bli\u00b7cke", "len\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "An mich m\u00fc\u00dfte sie st\u00fcndlich denken.\u00ab", "tokens": ["An", "mich", "m\u00fc\u00df\u00b7te", "sie", "st\u00fcnd\u00b7lich", "den\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VMFIN", "PPER", "ADJD", "VVINF", "$.", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.34": {"line.1": {"text": "Lange sprach ich kein lautes Wort,", "tokens": ["Lan\u00b7ge", "sprach", "ich", "kein", "lau\u00b7tes", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "O, ging' sie nie vom Fenster fort!", "tokens": ["O", ",", "ging'", "sie", "nie", "vom", "Fens\u00b7ter", "fort", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Nat\u00fcrlich mu\u00dfte sie dann gehn,", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "mu\u00df\u00b7te", "sie", "dann", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00df mich lahm und zweifelnd stehn.", "tokens": ["Und", "lie\u00df", "mich", "lahm", "und", "zwei\u00b7felnd", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Und als der helle Tag gewichen,", "tokens": ["Und", "als", "der", "hel\u00b7le", "Tag", "ge\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kam wie ein Kater ich geschlichen,", "tokens": ["Kam", "wie", "ein", "Ka\u00b7ter", "ich", "ge\u00b7schli\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Mein Mut, der wurde st\u00fcndlich tr\u00fcber,", "tokens": ["Mein", "Mut", ",", "der", "wur\u00b7de", "st\u00fcnd\u00b7lich", "tr\u00fc\u00b7ber", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sa\u00df ihrem Hause gegen\u00fcber", "tokens": ["Sa\u00df", "ih\u00b7rem", "Hau\u00b7se", "ge\u00b7gen\u00b7\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "APPO"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Auf einer Bank bei einem Zaun", "tokens": ["Auf", "ei\u00b7ner", "Bank", "bei", "ei\u00b7nem", "Zaun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und tat nur immer aufw\u00e4rts schaun.", "tokens": ["Und", "tat", "nur", "im\u00b7mer", "auf\u00b7w\u00e4rts", "schaun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Und blies sie aus den Lampenschein,", "tokens": ["Und", "blies", "sie", "aus", "den", "Lam\u00b7pen\u00b7schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schlief ich mit offnen Augen ein,", "tokens": ["Schlief", "ich", "mit", "off\u00b7nen", "Au\u00b7gen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "Schlief mich so g\u00f6ttlich nie mehr aus", "tokens": ["Schlief", "mich", "so", "g\u00f6tt\u00b7lich", "nie", "mehr", "aus"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie in den N\u00e4chten vor dem Haus.", "tokens": ["Wie", "in", "den", "N\u00e4ch\u00b7ten", "vor", "dem", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Sah, wie der Mond am Fenster leckte,", "tokens": ["Sah", ",", "wie", "der", "Mond", "am", "Fens\u00b7ter", "leck\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Schiefer von den D\u00e4chern deckte.", "tokens": ["Und", "Schie\u00b7fer", "von", "den", "D\u00e4\u00b7chern", "deck\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Zum Mond auf D\u00e4chern tanzt' Paris,", "tokens": ["Zum", "Mond", "auf", "D\u00e4\u00b7chern", "tanzt'", "Pa\u00b7ris", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "VVFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nachtwind die T\u00e4nzer vorw\u00e4rtsblies,", "tokens": ["Nacht\u00b7wind", "die", "T\u00e4n\u00b7zer", "vor\u00b7w\u00e4rts\u00b7blies", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "VVFIN", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Wenn M\u00e4nner die Jungfrauen k\u00fc\u00dften,", "tokens": ["Wenn", "M\u00e4n\u00b7ner", "die", "Jung\u00b7frau\u00b7en", "k\u00fc\u00df\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fuhren Raketen aus den Br\u00fcsten,", "tokens": ["Fuh\u00b7ren", "Ra\u00b7ke\u00b7ten", "aus", "den", "Br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.44": {"line.1": {"text": "Sah Ab\u00e4lard mit Helo\u00efsen", "tokens": ["Sah", "A\u00b7b\u00e4\u00b7lard", "mit", "He\u00b7lo\u00ef\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "NE", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der gro\u00dfen Lieb' gottvolle Riesen.", "tokens": ["Der", "gro\u00b7\u00dfen", "Lieb'", "gott\u00b7vol\u00b7le", "Rie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Zum Marterberg tanzt' man aufw\u00e4rts,", "tokens": ["Zum", "Mar\u00b7ter\u00b7berg", "tanzt'", "man", "auf\u00b7w\u00e4rts", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "ADV", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Rund um die Kirch \u00bbzum heil'gen Herz\u00ab,", "tokens": ["Rund", "um", "die", "Kirch", "\u00bb", "zum", "heil'\u00b7gen", "Herz", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$(", "APPRART", "ADJA", "NN", "$(", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.46": {"line.1": {"text": "Und Mann mit Weib zum Mond sich schwang,", "tokens": ["Und", "Mann", "mit", "Weib", "zum", "Mond", "sich", "schwang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NN", "APPRART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df still der Mond in Scherben sprang.", "tokens": ["Da\u00df", "still", "der", "Mond", "in", "Scher\u00b7ben", "sprang", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Sterne verpfiffen wie die Fl\u00f6ten,", "tokens": ["Ster\u00b7ne", "ver\u00b7pfif\u00b7fen", "wie", "die", "Fl\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "KOKOM", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kein Fr\u00fchrot kann die T\u00e4nzer t\u00f6ten,", "tokens": ["Kein", "Fr\u00fch\u00b7rot", "kann", "die", "T\u00e4n\u00b7zer", "t\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Schliefen wie Flaschen nach dem Mahl,", "tokens": ["Schlie\u00b7fen", "wie", "Fla\u00b7schen", "nach", "dem", "Mahl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kehrer kamen zum Stra\u00dfensaal.", "tokens": ["Keh\u00b7rer", "ka\u00b7men", "zum", "Stra\u00b7\u00dfen\u00b7saal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.49": {"line.1": {"text": "Es leb' die Lieb'! blieb's Losungswort,", "tokens": ["Es", "leb'", "die", "Lieb'", "!", "blie\u00b7b's", "Lo\u00b7sungs\u00b7wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Behutsam schob man Scherben fort. \u2013", "tokens": ["Be\u00b7hut\u00b7sam", "schob", "man", "Scher\u00b7ben", "fort", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "VVFIN", "PIS", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "So hielt ich nachts die Augen offen", "tokens": ["So", "hielt", "ich", "nachts", "die", "Au\u00b7gen", "of\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und tat verz\u00fcckt in Bildern hoffen.", "tokens": ["Und", "tat", "ver\u00b7z\u00fcckt", "in", "Bil\u00b7dern", "hof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Ich wagte nicht, zu ihr zu gehn,", "tokens": ["Ich", "wag\u00b7te", "nicht", ",", "zu", "ihr", "zu", "gehn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Aus Angst, sie k\u00f6nnt den R\u00fccken drehn,", "tokens": ["Aus", "Angst", ",", "sie", "k\u00f6nnt", "den", "R\u00fc\u00b7cken", "drehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Und sich f\u00fcr immer von mir wenden,", "tokens": ["Und", "sich", "f\u00fcr", "im\u00b7mer", "von", "mir", "wen\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und schn\u00f6de m\u00fc\u00dft' mein Herz verenden.", "tokens": ["Und", "schn\u00f6\u00b7de", "m\u00fc\u00dft'", "mein", "Herz", "ver\u00b7en\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VMFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Ich wartete den Zufall ab,", "tokens": ["Ich", "war\u00b7te\u00b7te", "den", "Zu\u00b7fall", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der sich zum drittenmal begab.", "tokens": ["Der", "sich", "zum", "drit\u00b7ten\u00b7mal", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPRART", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Dem Zufall mu\u00df ein Hoch ich bringen,", "tokens": ["Dem", "Zu\u00b7fall", "mu\u00df", "ein", "Hoch", "ich", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er ist es wert, ihn zu besingen.", "tokens": ["Er", "ist", "es", "wert", ",", "ihn", "zu", "be\u00b7sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Der Zufall fragt nicht wo, nicht wie,", "tokens": ["Der", "Zu\u00b7fall", "fragt", "nicht", "wo", ",", "nicht", "wie", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "PWAV", "$,", "PTKNEG", "KOKOM", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zerst\u00f6rt und bringt die Harmonie,", "tokens": ["Zer\u00b7st\u00f6rt", "und", "bringt", "die", "Har\u00b7mo\u00b7nie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Kann selbst in Mi\u00dfkredit nicht kommen,", "tokens": ["Kann", "selbst", "in", "Mi\u00df\u00b7kre\u00b7dit", "nicht", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn er sich l\u00e4cherlich benommen.", "tokens": ["Wenn", "er", "sich", "l\u00e4\u00b7cher\u00b7lich", "be\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Ich \u00c4rmster, ich kann nichts daf\u00fcr,", "tokens": ["Ich", "\u00c4rms\u00b7ter", ",", "ich", "kann", "nichts", "da\u00b7f\u00fcr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$,", "PPER", "VMFIN", "PIS", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ach, l\u00e4cherlich kam er zu mir.", "tokens": ["Ach", ",", "l\u00e4\u00b7cher\u00b7lich", "kam", "er", "zu", "mir", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "ADJD", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.58": {"line.1": {"text": "Wenn man es mal recht eilig hat,", "tokens": ["Wenn", "man", "es", "mal", "recht", "ei\u00b7lig", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gibt's Omnibusse in der Stadt.", "tokens": ["Gibt's", "Om\u00b7ni\u00b7bus\u00b7se", "in", "der", "Stadt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Ein Platz war n\u00e4mlich nur noch frei,", "tokens": ["Ein", "Platz", "war", "n\u00e4m\u00b7lich", "nur", "noch", "frei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau K\u00f6nigin sa\u00df dicht dabei,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "sa\u00df", "dicht", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "ADJD", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Ich lie\u00df mich ihr zur Seite nieder,", "tokens": ["Ich", "lie\u00df", "mich", "ihr", "zur", "Sei\u00b7te", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Empfahl dem Himmel meine Glieder.", "tokens": ["Emp\u00b7fahl", "dem", "Him\u00b7mel", "mei\u00b7ne", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Sie sah mich noch nicht vorderhand,", "tokens": ["Sie", "sah", "mich", "noch", "nicht", "vor\u00b7der\u00b7hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ich blieb ihr noch unbekannt.", "tokens": ["Und", "ich", "blieb", "ihr", "noch", "un\u00b7be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.62": {"line.1": {"text": "Ein Omnibus, der sch\u00fcttelt stark,", "tokens": ["Ein", "Om\u00b7ni\u00b7bus", ",", "der", "sch\u00fct\u00b7telt", "stark", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich f\u00fchlte mein Gehirn wie Quark,", "tokens": ["Ich", "f\u00fchl\u00b7te", "mein", "Ge\u00b7hirn", "wie", "Quark", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Da Schulter ich an Schulter sa\u00df", "tokens": ["Da", "Schul\u00b7ter", "ich", "an", "Schul\u00b7ter", "sa\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "PPER", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit ihr, die mir am Herzen fra\u00df.", "tokens": ["Mit", "ihr", ",", "die", "mir", "am", "Her\u00b7zen", "fra\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "PRELS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Ich f\u00fchlte bald, ich w\u00fcrde toll,", "tokens": ["Ich", "f\u00fchl\u00b7te", "bald", ",", "ich", "w\u00fcr\u00b7de", "toll", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Kopf brannte wie Alkohol,", "tokens": ["Mein", "Kopf", "brann\u00b7te", "wie", "Al\u00b7ko\u00b7hol", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Die Augen wuchsen gro\u00df wie R\u00e4der.", "tokens": ["Die", "Au\u00b7gen", "wuch\u00b7sen", "gro\u00df", "wie", "R\u00e4\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich glaub', ich werde Attent\u00e4ter,", "tokens": ["Ich", "glaub'", ",", "ich", "wer\u00b7de", "At\u00b7ten\u00b7t\u00e4\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Denn alles dr\u00e4ngt nach einem Ku\u00df,", "tokens": ["Denn", "al\u00b7les", "dr\u00e4ngt", "nach", "ei\u00b7nem", "Ku\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den ich jetzt endlich haben mu\u00df.", "tokens": ["Den", "ich", "jetzt", "end\u00b7lich", "ha\u00b7ben", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "F\u00fchlte Fieber in jedem Arm,", "tokens": ["F\u00fchl\u00b7te", "Fie\u00b7ber", "in", "je\u00b7dem", "Arm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Selbst meine Sohlen wurden warm.", "tokens": ["Selbst", "mei\u00b7ne", "Soh\u00b7len", "wur\u00b7den", "warm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Ich bin ganz j\u00e4hlings aufgesprungen", "tokens": ["Ich", "bin", "ganz", "j\u00e4h\u00b7lings", "auf\u00b7ge\u00b7sprun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und hab' Frau K\u00f6nigin umschlungen", "tokens": ["Und", "hab'", "Frau", "K\u00f6\u00b7ni\u00b7gin", "um\u00b7schlun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "NN", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "Und k\u00fcss' die Dame durch den Schleier,", "tokens": ["Und", "k\u00fcss'", "die", "Da\u00b7me", "durch", "den", "Schlei\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann erst war mir die Seele freier.", "tokens": ["Dann", "erst", "war", "mir", "die", "See\u00b7le", "frei\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}}, "stanza.70": {"line.1": {"text": "Sie schreit, bis sie mich schnell erkennt,", "tokens": ["Sie", "schreit", ",", "bis", "sie", "mich", "schnell", "er\u00b7kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "PRF", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch alles schon zusammenrennt,", "tokens": ["Doch", "al\u00b7les", "schon", "zu\u00b7sam\u00b7men\u00b7rennt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.71": {"line.1": {"text": "Man flieht, man ruft den Kondukteur,", "tokens": ["Man", "flieht", ",", "man", "ruft", "den", "Kon\u00b7duk\u00b7teur", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man kreischt: \u00bbEin Narr macht hier Malheur!\u00ab", "tokens": ["Man", "kreischt", ":", "\u00bb", "Ein", "Narr", "macht", "hier", "Mal\u00b7heur", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$.", "$(", "ART", "NN", "VVFIN", "ADV", "NN", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.72": {"line.1": {"text": "Man stoppt. Doch die Frau K\u00f6nigin", "tokens": ["Man", "stoppt", ".", "Doch", "die", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "$.", "KON", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sagt zu den Leuten obenhin:", "tokens": ["Sagt", "zu", "den", "Leu\u00b7ten", "o\u00b7ben\u00b7hin", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "\u00bbes ist ja weiter nichts geschehn", "tokens": ["\u00bb", "es", "ist", "ja", "wei\u00b7ter", "nichts", "ge\u00b7schehn"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VAFIN", "ADV", "ADV", "PIS", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als nur ein frohes Wiedersehn.\u00ab", "tokens": ["Als", "nur", "ein", "fro\u00b7hes", "Wie\u00b7der\u00b7sehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Sie ging dann gern mit mir spazieren,", "tokens": ["Sie", "ging", "dann", "gern", "mit", "mir", "spa\u00b7zie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sollt' sie zu sch\u00f6nen Bildern f\u00fchren.", "tokens": ["Sollt'", "sie", "zu", "sch\u00f6\u00b7nen", "Bil\u00b7dern", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Sie war noch rosenrot vom Ku\u00df", "tokens": ["Sie", "war", "noch", "ro\u00b7sen\u00b7rot", "vom", "Ku\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sprach nicht mehr vom Omnibus.", "tokens": ["Und", "sprach", "nicht", "mehr", "vom", "Om\u00b7ni\u00b7bus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.76": {"line.1": {"text": "Wenn Wangen sich wie Blumen zeigen,", "tokens": ["Wenn", "Wan\u00b7gen", "sich", "wie", "Blu\u00b7men", "zei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PRF", "KOKOM", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann platzt im Herzen bald das Schweigen.", "tokens": ["Dann", "platzt", "im", "Her\u00b7zen", "bald", "das", "Schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.77": {"line.1": {"text": "Und in den Louvregalerien", "tokens": ["Und", "in", "den", "Louv\u00b7re\u00b7ga\u00b7le\u00b7ri\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War's Wunschschlo\u00df der Frau K\u00f6nigin.", "tokens": ["Wa\u00b7r's", "Wunschsc\u00b7hlo\u00df", "der", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ART", "NN", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.78": {"line.1": {"text": "Die Welt herrlich um uns entstand,", "tokens": ["Die", "Welt", "herr\u00b7lich", "um", "uns", "ent\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "PPER", "VVFIN", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mit Lieb' gemalt auf Leinewand,", "tokens": ["Mit", "Lieb'", "ge\u00b7malt", "auf", "Lei\u00b7ne\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Wir sa\u00dfen still vor einem Bild", "tokens": ["Wir", "sa\u00b7\u00dfen", "still", "vor", "ei\u00b7nem", "Bild"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Mondpracht und doch seltsam wild,", "tokens": ["In", "Mond\u00b7pracht", "und", "doch", "selt\u00b7sam", "wild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Ein schwarz verzweifelt Ackerland,", "tokens": ["Ein", "schwarz", "ver\u00b7zwei\u00b7felt", "A\u00b7cker\u00b7land", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Wassergraben rechter Hand,", "tokens": ["Ein", "Was\u00b7ser\u00b7gra\u00b7ben", "rech\u00b7ter", "Hand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "Gemalt nach schwangerm Abendregen,", "tokens": ["Ge\u00b7malt", "nach", "schwan\u00b7germ", "A\u00b7bend\u00b7re\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Pf\u00fctzen noch auf allen Wegen;", "tokens": ["Und", "Pf\u00fct\u00b7zen", "noch", "auf", "al\u00b7len", "We\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.82": {"line.1": {"text": "In Wolken, die voll F\u00f6hn und na\u00df,", "tokens": ["In", "Wol\u00b7ken", ",", "die", "voll", "F\u00f6hn", "und", "na\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "ADJD", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Mond grell wie ein Blitzstrahl sa\u00df.", "tokens": ["Der", "Mond", "grell", "wie", "ein", "Blitz\u00b7strahl", "sa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.83": {"line.1": {"text": "\u00bbhier in dem Bilde wollen wir", "tokens": ["\u00bb", "hier", "in", "dem", "Bil\u00b7de", "wol\u00b7len", "wir"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "VMFIN", "PPER"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spazieren gehn,\u00ab sprach sie zu mir.", "tokens": ["Spa\u00b7zie\u00b7ren", "gehn", ",", "\u00ab", "sprach", "sie", "zu", "mir", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "$(", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Wir sa\u00dfen eng auf dem Sofa", "tokens": ["Wir", "sa\u00b7\u00dfen", "eng", "auf", "dem", "So\u00b7fa"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und gingen in die Landschaft da.", "tokens": ["Und", "gin\u00b7gen", "in", "die", "Land\u00b7schaft", "da", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.85": {"line.1": {"text": "Sie sprach so g\u00f6ttlich nebenbei,", "tokens": ["Sie", "sprach", "so", "g\u00f6tt\u00b7lich", "ne\u00b7ben\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und was sie sprach, war einerlei.", "tokens": ["Und", "was", "sie", "sprach", ",", "war", "ei\u00b7ner\u00b7lei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "PIS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "Ich f\u00fchlte es bei ihr sogleich:", "tokens": ["Ich", "f\u00fchl\u00b7te", "es", "bei", "ihr", "sog\u00b7leich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ja, ich und sie werden ein Reich.", "tokens": ["Ja", ",", "ich", "und", "sie", "wer\u00b7den", "ein", "Reich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "KON", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "----+--+", "measure": "iambic.di.chol"}}, "stanza.87": {"line.1": {"text": "Der Ku\u00df hat freier mich gemacht,", "tokens": ["Der", "Ku\u00df", "hat", "frei\u00b7er", "mich", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ich erz\u00e4hlte von der Nacht,", "tokens": ["Und", "ich", "er\u00b7z\u00e4hl\u00b7te", "von", "der", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.88": {"line.1": {"text": "Da\u00df ich ihr Fenster still besessen", "tokens": ["Da\u00df", "ich", "ihr", "Fens\u00b7ter", "still", "be\u00b7ses\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Sehnsucht t\u00e4t den Mond auffressen.", "tokens": ["Und", "Sehn\u00b7sucht", "t\u00e4t", "den", "Mond", "auf\u00b7fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.89": {"line.1": {"text": "Da tat der F\u00f6hnwind hei\u00df umgehen,", "tokens": ["Da", "tat", "der", "F\u00f6hn\u00b7wind", "hei\u00df", "um\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Louvre tat voll Schw\u00fcle stehen.", "tokens": ["Der", "Louv\u00b7re", "tat", "voll", "Schw\u00fc\u00b7le", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.90": {"line.1": {"text": "Mir war, als folgten uns aus Rahmen", "tokens": ["Mir", "war", ",", "als", "folg\u00b7ten", "uns", "aus", "Rah\u00b7men"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "KOKOM", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "All die gemalten Herrn und Damen.", "tokens": ["All", "die", "ge\u00b7mal\u00b7ten", "Herrn", "und", "Da\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.91": {"line.1": {"text": "Leute aus jeglichem Jahrhundert", "tokens": ["Leu\u00b7te", "aus", "jeg\u00b7li\u00b7chem", "Jahr\u00b7hun\u00b7dert"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "PIAT", "NN"], "meter": "+--+---+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sie haben K\u00f6nigin bewundert.", "tokens": ["Sie", "ha\u00b7ben", "K\u00f6\u00b7ni\u00b7gin", "be\u00b7wun\u00b7dert", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.92": {"line.1": {"text": "Sie konnte Tote zittern machen,", "tokens": ["Sie", "konn\u00b7te", "To\u00b7te", "zit\u00b7tern", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lieb' sprach zu ihr in allen Sprachen.", "tokens": ["Lieb'", "sprach", "zu", "ihr", "in", "al\u00b7len", "Spra\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.93": {"line.1": {"text": "Rubens und Rembrandt gl\u00fchten da,", "tokens": ["Ru\u00b7bens", "und", "Rem\u00b7brandt", "gl\u00fch\u00b7ten", "da", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "VVFIN", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sobald Frau K\u00f6nigin hinsah,", "tokens": ["So\u00b7bald", "Frau", "K\u00f6\u00b7ni\u00b7gin", "hin\u00b7sah", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.94": {"line.1": {"text": "Holbein und D\u00fcrer gr\u00fc\u00dften tief,", "tokens": ["Hol\u00b7bein", "und", "D\u00fc\u00b7rer", "gr\u00fc\u00df\u00b7ten", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihr Mund sanft: \u00bbMadonna\u00ab rief.", "tokens": ["Und", "ihr", "Mund", "sanft", ":", "\u00bb", "Ma\u00b7don\u00b7na", "\u00ab", "rief", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ADJD", "$.", "$(", "FM", "$(", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.95": {"line.1": {"text": "Weil man das Singen ja nicht sieht,", "tokens": ["Weil", "man", "das", "Sin\u00b7gen", "ja", "nicht", "sieht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "ADV", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sang K\u00f6nigin halblaut ein Lied,", "tokens": ["Sang", "K\u00f6\u00b7ni\u00b7gin", "hal\u00b7blaut", "ein", "Lied", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.96": {"line.1": {"text": "Lie\u00df wie ein Taschentuch es fallen", "tokens": ["Lie\u00df", "wie", "ein", "Ta\u00b7schen\u00b7tuch", "es", "fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In Huld als Dank ihren Vasallen.", "tokens": ["In", "Huld", "als", "Dank", "ih\u00b7ren", "Va\u00b7sal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KOKOM", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.97": {"line.1": {"text": "Und Milos Venus lud uns ein,", "tokens": ["Und", "Mi\u00b7los", "Ve\u00b7nus", "lud", "uns", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Marmor hatte Feuerschein,", "tokens": ["Ihr", "Mar\u00b7mor", "hat\u00b7te", "Feu\u00b7er\u00b7schein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.98": {"line.1": {"text": "Ihr Leib war wie ein Sonnenst\u00fcck,", "tokens": ["Ihr", "Leib", "war", "wie", "ein", "Son\u00b7nen\u00b7st\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es war ihr h\u00f6chster Augenblick.", "tokens": ["Es", "war", "ihr", "h\u00f6chs\u00b7ter", "Au\u00b7gen\u00b7blick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.99": {"line.1": {"text": "Denn einst, als man Paris beschossen,", "tokens": ["Denn", "einst", ",", "als", "man", "Pa\u00b7ris", "be\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "PIS", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat das die Venus schwer verdrossen,", "tokens": ["Hat", "das", "die", "Ve\u00b7nus", "schwer", "ver\u00b7dros\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.100": {"line.1": {"text": "Sie legte sich in eine Kist'.", "tokens": ["Sie", "leg\u00b7te", "sich", "in", "ei\u00b7ne", "Kist'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Versteckt in einer Fuhre Mist,", "tokens": ["Ver\u00b7steckt", "in", "ei\u00b7ner", "Fuh\u00b7re", "Mist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.101": {"line.1": {"text": "Lag sie in einer der Kasernen,", "tokens": ["Lag", "sie", "in", "ei\u00b7ner", "der", "Ka\u00b7ser\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis sich der Deutsche tat entfernen.", "tokens": ["Bis", "sich", "der", "Deut\u00b7sche", "tat", "ent\u00b7fer\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.102": {"line.1": {"text": "Sich rettend so aus den Gefahren", "tokens": ["Sich", "ret\u00b7tend", "so", "aus", "den", "Ge\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wartet sie jetzt auf Balthasaren.", "tokens": ["War\u00b7tet", "sie", "jetzt", "auf", "Balt\u00b7ha\u00b7sa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}}, "stanza.103": {"line.1": {"text": "Blank, und von Mist nicht einen Schimmer,", "tokens": ["Blank", ",", "und", "von", "Mist", "nicht", "ei\u00b7nen", "Schim\u00b7mer", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "APPR", "NN", "PTKNEG", "ART", "NN", "$,"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Steht sie im Louvrehinterzimmer.", "tokens": ["Steht", "sie", "im", "Louv\u00b7re\u00b7hin\u00b7ter\u00b7zim\u00b7mer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.104": {"line.1": {"text": "Und dann, an diesem Nachmittag,", "tokens": ["Und", "dann", ",", "an", "die\u00b7sem", "Nach\u00b7mit\u00b7tag", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sonne ihr am Nabel lag.", "tokens": ["Die", "Son\u00b7ne", "ihr", "am", "Na\u00b7bel", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.105": {"line.1": {"text": "Da kam der Balthasar auch hin", "tokens": ["Da", "kam", "der", "Balt\u00b7ha\u00b7sar", "auch", "hin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihm zur Seit' Frau K\u00f6nigin.", "tokens": ["Und", "ihm", "zur", "Seit'", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.106": {"line.1": {"text": "Ganz harmlos sagt der Balthasar:", "tokens": ["Ganz", "harm\u00b7los", "sagt", "der", "Balt\u00b7ha\u00b7sar", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbdie Venus ist mal sonderbar!", "tokens": ["\u00bb", "die", "Ve\u00b7nus", "ist", "mal", "son\u00b7der\u00b7bar", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.107": {"line.1": {"text": "Ich sage euch, da\u00df ihr es wi\u00dft,", "tokens": ["Ich", "sa\u00b7ge", "euch", ",", "da\u00df", "ihr", "es", "wi\u00dft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie hier nicht die Sch\u00f6nste ist.\u00ab", "tokens": ["Da\u00df", "sie", "hier", "nicht", "die", "Sch\u00f6ns\u00b7te", "ist", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "PTKNEG", "ART", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.108": {"line.1": {"text": "Und er sah nur Frau K\u00f6nigin", "tokens": ["Und", "er", "sah", "nur", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV", "NN", "NN"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und sah nicht mehr zur Venus hin.", "tokens": ["Und", "sah", "nicht", "mehr", "zur", "Ve\u00b7nus", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.109": {"line.1": {"text": "Als echte Venus freut sie sich,", "tokens": ["Als", "ech\u00b7te", "Ve\u00b7nus", "freut", "sie", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "VVFIN", "PPER", "PRF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sonn' sie sich vom Nabel strich", "tokens": ["Die", "Sonn'", "sie", "sich", "vom", "Na\u00b7bel", "strich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPER", "PRF", "APPRART", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.110": {"line.1": {"text": "Und legt sie auf das Goldhaupt hin", "tokens": ["Und", "legt", "sie", "auf", "das", "Gold\u00b7haupt", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Krone der Frau K\u00f6nigin.", "tokens": ["Als", "Kro\u00b7ne", "der", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.111": {"line.1": {"text": "Frau K\u00f6nigin hat nicht verneint,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "hat", "nicht", "ver\u00b7neint", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau Venus hat uns still vereint,", "tokens": ["Frau", "Ve\u00b7nus", "hat", "uns", "still", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.112": {"line.1": {"text": "Es waren sich die Herzen nah,", "tokens": ["Es", "wa\u00b7ren", "sich", "die", "Her\u00b7zen", "nah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als w\u00e4r' ich Vater, sie Mama,", "tokens": ["Als", "w\u00e4r'", "ich", "Va\u00b7ter", ",", "sie", "Ma\u00b7ma", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "NN", "$,", "PPER", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.113": {"line.1": {"text": "Sie dr\u00fcckte mir die Lippen zu", "tokens": ["Sie", "dr\u00fcck\u00b7te", "mir", "die", "Lip\u00b7pen", "zu"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ward noch sch\u00f6ner und sprach: \u00bbDu.\u00ab", "tokens": ["Und", "ward", "noch", "sch\u00f6\u00b7ner", "und", "sprach", ":", "\u00bb", "Du", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "KON", "VVFIN", "$.", "$(", "PPER", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.114": {"line.1": {"text": "Von den Gen\u00fcssen der Genu\u00df", "tokens": ["Von", "den", "Ge\u00b7n\u00fcs\u00b7sen", "der", "Ge\u00b7nu\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist so ein richtiger erster Ku\u00df,", "tokens": ["Ist", "so", "ein", "rich\u00b7ti\u00b7ger", "ers\u00b7ter", "Ku\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.115": {"line.1": {"text": "Es m\u00fcssen beide t\u00fcchtig wollen,", "tokens": ["Es", "m\u00fcs\u00b7sen", "bei\u00b7de", "t\u00fcch\u00b7tig", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "ADJD", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sch\u00f6pft man heftig aus dem Vollen.", "tokens": ["Dann", "sch\u00f6pft", "man", "hef\u00b7tig", "aus", "dem", "Vol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.116": {"line.1": {"text": "So hatt' ich es mir ausgedacht,", "tokens": ["So", "hatt'", "ich", "es", "mir", "aus\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch anders ist die Welt gemacht.", "tokens": ["Doch", "an\u00b7ders", "ist", "die", "Welt", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.117": {"line.1": {"text": "Auch ich hab' es erfahren m\u00fcssen:", "tokens": ["Auch", "ich", "hab'", "es", "er\u00b7fah\u00b7ren", "m\u00fcs\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PPER", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein keusches Weib kann noch nicht k\u00fcssen,", "tokens": ["Ein", "keu\u00b7sches", "Weib", "kann", "noch", "nicht", "k\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.118": {"line.1": {"text": "Sie kann die Lippen noch nicht stellen,", "tokens": ["Sie", "kann", "die", "Lip\u00b7pen", "noch", "nicht", "stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tut oft den andern Mund verfehlen,", "tokens": ["Tut", "oft", "den", "an\u00b7dern", "Mund", "ver\u00b7feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.119": {"line.1": {"text": "Sie stellt sich ungeschickt noch an,", "tokens": ["Sie", "stellt", "sich", "un\u00b7ge\u00b7schickt", "noch", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADJD", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man k\u00fc\u00dft statt Lippe oft den Zahn.", "tokens": ["Man", "k\u00fc\u00dft", "statt", "Lip\u00b7pe", "oft", "den", "Zahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NE", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.120": {"line.1": {"text": "Doch Liebe \u00fcbt das K\u00fcssen ein,", "tokens": ["Doch", "Lie\u00b7be", "\u00fcbt", "das", "K\u00fcs\u00b7sen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dunkel soll es dabei sein.", "tokens": ["Und", "dun\u00b7kel", "soll", "es", "da\u00b7bei", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VMFIN", "PPER", "PAV", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.121": {"line.1": {"text": "Wir fuhren weich in einem Wagen", "tokens": ["Wir", "fuh\u00b7ren", "weich", "in", "ei\u00b7nem", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00dfen durch Paris uns tragen.", "tokens": ["Und", "lie\u00b7\u00dfen", "durch", "Pa\u00b7ris", "uns", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.122": {"line.1": {"text": "Der Wagen war ein fliegend Haus,", "tokens": ["Der", "Wa\u00b7gen", "war", "ein", "flie\u00b7gend", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Drin \u00fcbten wir das K\u00fcssen aus.", "tokens": ["Drin", "\u00fcb\u00b7ten", "wir", "das", "K\u00fcs\u00b7sen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.123": {"line.1": {"text": "Man k\u00fc\u00dft sich, und man spricht kein Wort,", "tokens": ["Man", "k\u00fc\u00dft", "sich", ",", "und", "man", "spricht", "kein", "Wort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "$,", "KON", "PIS", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und denkt nicht, \u2013 man ist einfach fort.", "tokens": ["Und", "denkt", "nicht", ",", "\u2013", "man", "ist", "ein\u00b7fach", "fort", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "$(", "PIS", "VAFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.124": {"line.1": {"text": "Das Herz hat jahrelang gehastet,", "tokens": ["Das", "Herz", "hat", "jah\u00b7re\u00b7lang", "ge\u00b7has\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis es den Mund fand, wo es rastet;", "tokens": ["Bis", "es", "den", "Mund", "fand", ",", "wo", "es", "ras\u00b7tet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.125": {"line.1": {"text": "Es tat ja Tag und Nacht stets rennen,", "tokens": ["Es", "tat", "ja", "Tag", "und", "Nacht", "stets", "ren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "KON", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man kann's dem Herzen wirklich g\u00f6nnen.", "tokens": ["Man", "kann's", "dem", "Her\u00b7zen", "wirk\u00b7lich", "g\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.126": {"line.1": {"text": "Oft hab' ich dr\u00fcber nachgedacht,", "tokens": ["Oft", "hab'", "ich", "dr\u00fc\u00b7ber", "nach\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie doch das gute Herz es macht,", "tokens": ["Wie", "doch", "das", "gu\u00b7te", "Herz", "es", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.127": {"line.1": {"text": "Da\u00df immerfort es wachen kann,", "tokens": ["Da\u00df", "im\u00b7mer\u00b7fort", "es", "wa\u00b7chen", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Arbeitend stets von Jugend an.", "tokens": ["Ar\u00b7bei\u00b7tend", "stets", "von", "Ju\u00b7gend", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.128": {"line.1": {"text": "Nachts, wenn der ganze K\u00f6rper ruht,", "tokens": ["Nachts", ",", "wenn", "der", "gan\u00b7ze", "K\u00f6r\u00b7per", "ruht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sortiert es immer noch das Blut,", "tokens": ["Sor\u00b7tiert", "es", "im\u00b7mer", "noch", "das", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.129": {"line.1": {"text": "Der Muskel schafft oft hundert Jahr.", "tokens": ["Der", "Mus\u00b7kel", "schafft", "oft", "hun\u00b7dert", "Jahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "CARD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich find' es gar nicht sonderbar,", "tokens": ["Ich", "find'", "es", "gar", "nicht", "son\u00b7der\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.130": {"line.1": {"text": "Da\u00df er nach Ku\u00df und Liebe dr\u00e4ngt,", "tokens": ["Da\u00df", "er", "nach", "Ku\u00df", "und", "Lie\u00b7be", "dr\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn dieses ihm Erholung schenkt.", "tokens": ["Wenn", "die\u00b7ses", "ihm", "Er\u00b7ho\u00b7lung", "schenkt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.131": {"line.1": {"text": "O, st\u00f6re nie den Mensch, der k\u00fc\u00dft,", "tokens": ["O", ",", "st\u00f6\u00b7re", "nie", "den", "Mensch", ",", "der", "k\u00fc\u00dft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "ADV", "ART", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil das einfach unmenschlich ist!", "tokens": ["Weil", "das", "ein\u00b7fach", "un\u00b7menschlich", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.132": {"line.1": {"text": "Und in Paris ist man gew\u00f6hnt,", "tokens": ["Und", "in", "Pa\u00b7ris", "ist", "man", "ge\u00b7w\u00f6hnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "VAFIN", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man die Liebe jedem g\u00f6nnt.", "tokens": ["Da\u00df", "man", "die", "Lie\u00b7be", "je\u00b7dem", "g\u00f6nnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.133": {"line.1": {"text": "So k\u00fc\u00dften wir und waren fort,", "tokens": ["So", "k\u00fc\u00df\u00b7ten", "wir", "und", "wa\u00b7ren", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VAFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sogar noch am Platz ", "tokens": ["So\u00b7gar", "noch", "am", "Platz"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPRART", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.134": {"line.1": {"text": "Wo einst man k\u00f6pfte Nacht und Tag,", "tokens": ["Wo", "einst", "man", "k\u00f6pf\u00b7te", "Nacht", "und", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIS", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Pflaster mir voll K\u00fcsse lag.", "tokens": ["Das", "Pflas\u00b7ter", "mir", "voll", "K\u00fcs\u00b7se", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.135": {"line.1": {"text": "Laternen tanzten um uns her,", "tokens": ["La\u00b7ter\u00b7nen", "tanz\u00b7ten", "um", "uns", "her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als wenn der Platz die Milchstra\u00df' w\u00e4r'.", "tokens": ["Als", "wenn", "der", "Platz", "die", "Milch\u00b7stra\u00df'", "w\u00e4r'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.136": {"line.1": {"text": "Doch pl\u00f6tzlich blieb mein Kopf nicht heil,", "tokens": ["Doch", "pl\u00f6tz\u00b7lich", "blieb", "mein", "Kopf", "nicht", "heil", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPOSAT", "NN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Wort fiel schwer wie ein Fallbeil.", "tokens": ["Ein", "Wort", "fiel", "schwer", "wie", "ein", "Fall\u00b7beil", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.137": {"line.1": {"text": "Wo einst die Guillotine stand,", "tokens": ["Wo", "einst", "die", "Guil\u00b7lo\u00b7ti\u00b7ne", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Balzer sich ganz kopflos fand,", "tokens": ["Der", "Bal\u00b7zer", "sich", "ganz", "kopf\u00b7los", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.138": {"line.1": {"text": "Denn K\u00f6nigin sprach ahnungsvoll,", "tokens": ["Denn", "K\u00f6\u00b7ni\u00b7gin", "sprach", "ah\u00b7nungs\u00b7voll", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von \u00bbTreue\u00ab, die man halten soll:", "tokens": ["Von", "\u00bb", "Treu\u00b7e", "\u00ab", ",", "die", "man", "hal\u00b7ten", "soll", ":"], "token_info": ["word", "punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$(", "NN", "$(", "$,", "PRELS", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.139": {"line.1": {"text": "\u00bbdu, Balzer, dein will ich gern sein,", "tokens": ["\u00bb", "du", ",", "Bal\u00b7zer", ",", "dein", "will", "ich", "gern", "sein", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "$,", "NN", "$,", "PPOSAT", "VMFIN", "PPER", "ADV", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch fiele es dir jemals ein,", "tokens": ["Doch", "fie\u00b7le", "es", "dir", "je\u00b7mals", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.140": {"line.1": {"text": "Da\u00df du mich zum Betrug gew\u00e4hlt,", "tokens": ["Da\u00df", "du", "mich", "zum", "Be\u00b7trug", "ge\u00b7w\u00e4hlt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann glaub' ich nichts mehr auf der Welt.\u00ab", "tokens": ["Dann", "glaub'", "ich", "nichts", "mehr", "auf", "der", "Welt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "ADV", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.141": {"line.1": {"text": "Ich wei\u00df nicht, welch ein Blitz geschah,", "tokens": ["Ich", "wei\u00df", "nicht", ",", "welch", "ein", "Blitz", "ge\u00b7schah", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PWAT", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich die Zukunft pl\u00f6tzlich sah.", "tokens": ["Da\u00df", "ich", "die", "Zu\u00b7kunft", "pl\u00f6tz\u00b7lich", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.142": {"line.1": {"text": "Mitten in meines Blutes Saus", "tokens": ["Mit\u00b7ten", "in", "mei\u00b7nes", "Blu\u00b7tes", "Saus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wischte der Blitz den Kopf mir aus.", "tokens": ["Wischte", "der", "Blitz", "den", "Kopf", "mir", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.143": {"line.1": {"text": "Wer k\u00f6nnte es mit Ernst beschw\u00f6ren,", "tokens": ["Wer", "k\u00f6nn\u00b7te", "es", "mit", "Ernst", "be\u00b7schw\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "APPR", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ihn die Zeiten nie bet\u00f6ren?", "tokens": ["Da\u00df", "ihn", "die", "Zei\u00b7ten", "nie", "be\u00b7t\u00f6\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.144": {"line.1": {"text": "Wei\u00df man denn, wer man selber ist,", "tokens": ["Wei\u00df", "man", "denn", ",", "wer", "man", "sel\u00b7ber", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "$,", "PWS", "PIS", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Getaufter Heide, genannt Christ.", "tokens": ["Ge\u00b7tauf\u00b7ter", "Hei\u00b7de", ",", "ge\u00b7nannt", "Christ", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "VVPP", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.145": {"line.1": {"text": "Nie kann ich f\u00fcr mich garantieren,", "tokens": ["Nie", "kann", "ich", "f\u00fcr", "mich", "ga\u00b7ran\u00b7tie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben ist ein st\u00fcndlich Irren.", "tokens": ["Das", "Le\u00b7ben", "ist", "ein", "st\u00fcnd\u00b7lich", "Ir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.146": {"line.1": {"text": "Heut leg' als Christ ich mich zu Bett,", "tokens": ["Heut", "leg'", "als", "Christ", "ich", "mich", "zu", "Bett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KOUS", "NN", "PPER", "PRF", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und fr\u00fch bet' ich zu Mohammed.", "tokens": ["Und", "fr\u00fch", "bet'", "ich", "zu", "Mo\u00b7ham\u00b7med", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.147": {"line.1": {"text": "Denn immer blindhin rollt die Welt,", "tokens": ["Denn", "im\u00b7mer", "blind\u00b7hin", "rollt", "die", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kurz, nur die Seel' im Leib aush\u00e4lt.", "tokens": ["Kurz", ",", "nur", "die", "Seel'", "im", "Leib", "aus\u00b7h\u00e4lt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADV", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.148": {"line.1": {"text": "Ersch\u00fcttert hat mich, was sie sprach,", "tokens": ["Er\u00b7sch\u00fct\u00b7tert", "hat", "mich", ",", "was", "sie", "sprach", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPER", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es war der Liebe erstes \u00bbAch\u00ab.", "tokens": ["Es", "war", "der", "Lie\u00b7be", "ers\u00b7tes", "\u00bb", "Ach", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJA", "$(", "ITJ", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.149": {"line.1": {"text": "Man soll im Gl\u00fcck am Leid nicht r\u00fchren,", "tokens": ["Man", "soll", "im", "Gl\u00fcck", "am", "Leid", "nicht", "r\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPRART", "NN", "APPRART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht stets nach der Mechanik sp\u00fcren,", "tokens": ["Nicht", "stets", "nach", "der", "Me\u00b7cha\u00b7nik", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.150": {"line.1": {"text": "Puppen sind wir im Puppenhaus,", "tokens": ["Pup\u00b7pen", "sind", "wir", "im", "Pup\u00b7pen\u00b7haus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spielt man zu hart, l\u00e4uft S\u00e4gmehl 'raus.", "tokens": ["Spielt", "man", "zu", "hart", ",", "l\u00e4uft", "S\u00e4g\u00b7mehl", "'raus", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PTKA", "ADJD", "$,", "VVFIN", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.151": {"line.1": {"text": "Dein ganzes Leben war nur Dunst,", "tokens": ["Dein", "gan\u00b7zes", "Le\u00b7ben", "war", "nur", "Dunst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Liebst du nicht stets mit edler Kunst.", "tokens": ["Liebst", "du", "nicht", "stets", "mit", "ed\u00b7ler", "Kunst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.152": {"line.1": {"text": "Und lieben sollst du vor dem Tode,", "tokens": ["Und", "lie\u00b7ben", "sollst", "du", "vor", "dem", "To\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das war von je pariser Mode.", "tokens": ["Das", "war", "von", "je", "pa\u00b7ri\u00b7ser", "Mo\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.153": {"line.1": {"text": "Die Stadt spricht ganz in meinem Sinn", "tokens": ["Die", "Stadt", "spricht", "ganz", "in", "mei\u00b7nem", "Sinn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und immer zog's mich zu ihr hin.", "tokens": ["Und", "im\u00b7mer", "zo\u00b7g's", "mich", "zu", "ihr", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.154": {"line.1": {"text": "Ehre ist mehr ein kaltes Feuer,", "tokens": ["Eh\u00b7re", "ist", "mehr", "ein", "kal\u00b7tes", "Feu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Nur Liebe, die w\u00e4rmt ungeheuer,", "tokens": ["Nur", "Lie\u00b7be", ",", "die", "w\u00e4rmt", "un\u00b7ge\u00b7heu\u00b7er", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "PRELS", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.155": {"line.1": {"text": "Geld gibt dem Leibe vieles Gl\u00fcck,", "tokens": ["Geld", "gibt", "dem", "Lei\u00b7be", "vie\u00b7les", "Gl\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "PIS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch nicht den h\u00f6chsten Augenblick.", "tokens": ["Doch", "nicht", "den", "h\u00f6chs\u00b7ten", "Au\u00b7gen\u00b7blick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.156": {"line.1": {"text": "Nur Liebe macht im Mark erbeben,", "tokens": ["Nur", "Lie\u00b7be", "macht", "im", "Mark", "er\u00b7be\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Deshalb soll jeder sie erleben.", "tokens": ["Des\u00b7halb", "soll", "je\u00b7der", "sie", "er\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.157": {"line.1": {"text": "Mir tanzten die Pariser Stra\u00dfen,", "tokens": ["Mir", "tanz\u00b7ten", "die", "Pa\u00b7ri\u00b7ser", "Stra\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Konnt' mich vor Freude nicht mehr lassen,", "tokens": ["Konnt'", "mich", "vor", "Freu\u00b7de", "nicht", "mehr", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPR", "NN", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.158": {"line.1": {"text": "Wu\u00dfte, Frau K\u00f6nigin war da,", "tokens": ["Wu\u00df\u00b7te", ",", "Frau", "K\u00f6\u00b7ni\u00b7gin", "war", "da", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "NN", "VAFIN", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wenn ich sie selbst auch noch nicht sah.", "tokens": ["Wenn", "ich", "sie", "selbst", "auch", "noch", "nicht", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADV", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.159": {"line.1": {"text": "W\u00fcnschte durch Mauern jetzt zu sehn", "tokens": ["W\u00fcnschte", "durch", "Mau\u00b7ern", "jetzt", "zu", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "NN", "ADV", "PTKZU", "VVINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und in den H\u00e4usern umzugehn.", "tokens": ["Und", "in", "den", "H\u00e4u\u00b7sern", "um\u00b7zu\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.160": {"line.1": {"text": "Doch dieses mu\u00dft' ich unterlassen", "tokens": ["Doch", "die\u00b7ses", "mu\u00dft'", "ich", "un\u00b7ter\u00b7las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "VMFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und mich beschr\u00e4nken auf die Stra\u00dfen.", "tokens": ["Und", "mich", "be\u00b7schr\u00e4n\u00b7ken", "auf", "die", "Stra\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.161": {"line.1": {"text": "Der Zufall spielt gar gern Verstecken,", "tokens": ["Der", "Zu\u00b7fall", "spielt", "gar", "gern", "Ver\u00b7ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mich tat er unverge\u00dflich necken.", "tokens": ["Mich", "tat", "er", "un\u00b7ver\u00b7ge\u00df\u00b7lich", "ne\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.162": {"line.1": {"text": "Auf einem Dampfboot auf der Seine,", "tokens": ["Auf", "ei\u00b7nem", "Dampf\u00b7boot", "auf", "der", "Sei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als ich an dem Gel\u00e4nder lehne,", "tokens": ["Als", "ich", "an", "dem", "Ge\u00b7l\u00e4n\u00b7der", "leh\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.163": {"line.1": {"text": "Ein ander Boot kam mir entgegen,", "tokens": ["Ein", "an\u00b7der", "Boot", "kam", "mir", "ent\u00b7ge\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da naht \u00bbsie\u00ab wie ein goldner Segen.", "tokens": ["Da", "naht", "\u00bb", "sie", "\u00ab", "wie", "ein", "gold\u00b7ner", "Se\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "PPER", "$(", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.164": {"line.1": {"text": "Sie tr\u00e4gt ihr stolzestes Gesicht", "tokens": ["Sie", "tr\u00e4gt", "ihr", "stol\u00b7zes\u00b7tes", "Ge\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lebt allein und sieht mich nicht.", "tokens": ["Und", "lebt", "al\u00b7lein", "und", "sieht", "mich", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KON", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.165": {"line.1": {"text": "Ich z\u00e4hlte nicht einmal bis zwei,", "tokens": ["Ich", "z\u00e4hl\u00b7te", "nicht", "ein\u00b7mal", "bis", "zwei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da war das Boot mit ihr vorbei;", "tokens": ["Da", "war", "das", "Boot", "mit", "ihr", "vor\u00b7bei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.166": {"line.1": {"text": "Den Dampf tat ich von Grund aus hassen,", "tokens": ["Den", "Dampf", "tat", "ich", "von", "Grund", "aus", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "NN", "APPR", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt war ich wiederum verlassen.", "tokens": ["Jetzt", "war", "ich", "wie\u00b7de\u00b7rum", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.167": {"line.1": {"text": "Im Schlaf erschien mir dann die Seine", "tokens": ["Im", "Schlaf", "er\u00b7schien", "mir", "dann", "die", "Sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ADV", "ART", "PPOSAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie meiner Sehnsucht lange Tr\u00e4ne,", "tokens": ["Wie", "mei\u00b7ner", "Sehn\u00b7sucht", "lan\u00b7ge", "Tr\u00e4\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.168": {"line.1": {"text": "Und stets auf einem andern Schiff", "tokens": ["Und", "stets", "auf", "ei\u00b7nem", "an\u00b7dern", "Schiff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schwamm die vor\u00fcber, die ich rief.", "tokens": ["Schwamm", "die", "vor\u00b7\u00fc\u00b7ber", ",", "die", "ich", "rief", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ART", "PTKVZ", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.169": {"line.1": {"text": "Ich wurde nicht im Suchen lahm,", "tokens": ["Ich", "wur\u00b7de", "nicht", "im", "Su\u00b7chen", "lahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wiederum ein Zufall kam.", "tokens": ["Und", "wie\u00b7de\u00b7rum", "ein", "Zu\u00b7fall", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.170": {"line.1": {"text": "Kommt man in eine neue Stadt,", "tokens": ["Kommt", "man", "in", "ei\u00b7ne", "neu\u00b7e", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In der man ein paar Freunde hat,", "tokens": ["In", "der", "man", "ein", "paar", "Freun\u00b7de", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIS", "ART", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.171": {"line.1": {"text": "Geht man zu ihnen mal hinauf", "tokens": ["Geht", "man", "zu", "ih\u00b7nen", "mal", "hin\u00b7auf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "APPR", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sucht die lieben Freunde auf.", "tokens": ["Und", "sucht", "die", "lie\u00b7ben", "Freun\u00b7de", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.172": {"line.1": {"text": "Mein Freund war Maler von Beruf,", "tokens": ["Mein", "Freund", "war", "Ma\u00b7ler", "von", "Be\u00b7ruf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Am liebsten er die Nacktheit schuf.", "tokens": ["Am", "liebs\u00b7ten", "er", "die", "Nackt\u00b7heit", "schuf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "VVFIN", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.173": {"line.1": {"text": "Hab' vor den Bildern Platz genommen.", "tokens": ["Hab'", "vor", "den", "Bil\u00b7dern", "Platz", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er sprach: \u00bbDer Wein, der wird gleich kommen.\u00ab", "tokens": ["Er", "sprach", ":", "\u00bb", "Der", "Wein", ",", "der", "wird", "gleich", "kom\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "ART", "NN", "$,", "PRELS", "VAFIN", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.174": {"line.1": {"text": "Sein Modell warf den Mantel ab,", "tokens": ["Sein", "Mo\u00b7dell", "warf", "den", "Man\u00b7tel", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nackt stand sie da, wie Gott sie gab.", "tokens": ["Nackt", "stand", "sie", "da", ",", "wie", "Gott", "sie", "gab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWAV", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.175": {"line.1": {"text": "Den Wein tat kleiderlos sie kaufen,", "tokens": ["Den", "Wein", "tat", "klei\u00b7der\u00b7los", "sie", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mich tat es ganz hei\u00df \u00fcberlaufen.", "tokens": ["Mich", "tat", "es", "ganz", "hei\u00df", "\u00fc\u00b7berl\u00b7au\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.176": {"line.1": {"text": "Ich lobte sehr ihr blankes Haar.", "tokens": ["Ich", "lob\u00b7te", "sehr", "ihr", "blan\u00b7kes", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Freund rief: \u00bbEs ist sonderbar,", "tokens": ["Mein", "Freund", "rief", ":", "\u00bb", "Es", "ist", "son\u00b7der\u00b7bar", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.177": {"line.1": {"text": "Wie dieses Haar jetzt modisch wird!", "tokens": ["Wie", "die\u00b7ses", "Haar", "jetzt", "mo\u00b7disch", "wird", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDAT", "NN", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Noch st\u00e4rker hat es mich verwirrt", "tokens": ["Noch", "st\u00e4r\u00b7ker", "hat", "es", "mich", "ver\u00b7wirrt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "PRF", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.178": {"line.1": {"text": "Von einer Dame ", "tokens": ["Von", "ei\u00b7ner", "Da\u00b7me"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Wie eine K\u00f6nigin ist die,", "tokens": ["Wie", "ei\u00b7ne", "K\u00f6\u00b7ni\u00b7gin", "ist", "die", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VAFIN", "ART", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.179": {"line.1": {"text": "Ihr Haar ist eine hei\u00dfe Krone.\u00ab", "tokens": ["Ihr", "Haar", "ist", "ei\u00b7ne", "hei\u00b7\u00dfe", "Kro\u00b7ne", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich fragte zitternd, wo sie wohne.", "tokens": ["Ich", "frag\u00b7te", "zit\u00b7ternd", ",", "wo", "sie", "woh\u00b7ne", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVPP", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.180": {"line.1": {"text": "\u00bbdort steht sie an dem Fenster eben!\u00ab", "tokens": ["\u00bb", "dort", "steht", "sie", "an", "dem", "Fens\u00b7ter", "e\u00b7ben", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von Feuer f\u00fchlt' ich mich umgeben,", "tokens": ["Von", "Feu\u00b7er", "f\u00fchlt'", "ich", "mich", "um\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.181": {"line.1": {"text": "Frau K\u00f6nigin gleich rechter Hand", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "gleich", "rech\u00b7ter", "Hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NN", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im n\u00e4chsten Haus am Fenster stand.", "tokens": ["Im", "n\u00e4chs\u00b7ten", "Haus", "am", "Fens\u00b7ter", "stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.182": {"line.1": {"text": "Sie sah gerade auf die Uhr:", "tokens": ["Sie", "sah", "ge\u00b7ra\u00b7de", "auf", "die", "Uhr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbo Gott, w\u00e4r' ich ein Zeiger nur!", "tokens": ["\u00bb", "o", "Gott", ",", "w\u00e4r'", "ich", "ein", "Zei\u00b7ger", "nur", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "$,", "VAFIN", "PPER", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.183": {"line.1": {"text": "Ich w\u00fcrde ihre Blicke lenken,", "tokens": ["Ich", "w\u00fcr\u00b7de", "ih\u00b7re", "Bli\u00b7cke", "len\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "An mich m\u00fc\u00dfte sie st\u00fcndlich denken.\u00ab", "tokens": ["An", "mich", "m\u00fc\u00df\u00b7te", "sie", "st\u00fcnd\u00b7lich", "den\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VMFIN", "PPER", "ADJD", "VVINF", "$.", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.184": {"line.1": {"text": "Lange sprach ich kein lautes Wort,", "tokens": ["Lan\u00b7ge", "sprach", "ich", "kein", "lau\u00b7tes", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "O, ging' sie nie vom Fenster fort!", "tokens": ["O", ",", "ging'", "sie", "nie", "vom", "Fens\u00b7ter", "fort", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.185": {"line.1": {"text": "Nat\u00fcrlich mu\u00dfte sie dann gehn,", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "mu\u00df\u00b7te", "sie", "dann", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00df mich lahm und zweifelnd stehn.", "tokens": ["Und", "lie\u00df", "mich", "lahm", "und", "zwei\u00b7felnd", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.186": {"line.1": {"text": "Und als der helle Tag gewichen,", "tokens": ["Und", "als", "der", "hel\u00b7le", "Tag", "ge\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kam wie ein Kater ich geschlichen,", "tokens": ["Kam", "wie", "ein", "Ka\u00b7ter", "ich", "ge\u00b7schli\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.187": {"line.1": {"text": "Mein Mut, der wurde st\u00fcndlich tr\u00fcber,", "tokens": ["Mein", "Mut", ",", "der", "wur\u00b7de", "st\u00fcnd\u00b7lich", "tr\u00fc\u00b7ber", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sa\u00df ihrem Hause gegen\u00fcber", "tokens": ["Sa\u00df", "ih\u00b7rem", "Hau\u00b7se", "ge\u00b7gen\u00b7\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "APPO"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.188": {"line.1": {"text": "Auf einer Bank bei einem Zaun", "tokens": ["Auf", "ei\u00b7ner", "Bank", "bei", "ei\u00b7nem", "Zaun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und tat nur immer aufw\u00e4rts schaun.", "tokens": ["Und", "tat", "nur", "im\u00b7mer", "auf\u00b7w\u00e4rts", "schaun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.189": {"line.1": {"text": "Und blies sie aus den Lampenschein,", "tokens": ["Und", "blies", "sie", "aus", "den", "Lam\u00b7pen\u00b7schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schlief ich mit offnen Augen ein,", "tokens": ["Schlief", "ich", "mit", "off\u00b7nen", "Au\u00b7gen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.190": {"line.1": {"text": "Schlief mich so g\u00f6ttlich nie mehr aus", "tokens": ["Schlief", "mich", "so", "g\u00f6tt\u00b7lich", "nie", "mehr", "aus"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie in den N\u00e4chten vor dem Haus.", "tokens": ["Wie", "in", "den", "N\u00e4ch\u00b7ten", "vor", "dem", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.191": {"line.1": {"text": "Sah, wie der Mond am Fenster leckte,", "tokens": ["Sah", ",", "wie", "der", "Mond", "am", "Fens\u00b7ter", "leck\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Schiefer von den D\u00e4chern deckte.", "tokens": ["Und", "Schie\u00b7fer", "von", "den", "D\u00e4\u00b7chern", "deck\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.192": {"line.1": {"text": "Zum Mond auf D\u00e4chern tanzt' Paris,", "tokens": ["Zum", "Mond", "auf", "D\u00e4\u00b7chern", "tanzt'", "Pa\u00b7ris", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NN", "VVFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nachtwind die T\u00e4nzer vorw\u00e4rtsblies,", "tokens": ["Nacht\u00b7wind", "die", "T\u00e4n\u00b7zer", "vor\u00b7w\u00e4rts\u00b7blies", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "VVFIN", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}}, "stanza.193": {"line.1": {"text": "Wenn M\u00e4nner die Jungfrauen k\u00fc\u00dften,", "tokens": ["Wenn", "M\u00e4n\u00b7ner", "die", "Jung\u00b7frau\u00b7en", "k\u00fc\u00df\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fuhren Raketen aus den Br\u00fcsten,", "tokens": ["Fuh\u00b7ren", "Ra\u00b7ke\u00b7ten", "aus", "den", "Br\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.194": {"line.1": {"text": "Sah Ab\u00e4lard mit Helo\u00efsen", "tokens": ["Sah", "A\u00b7b\u00e4\u00b7lard", "mit", "He\u00b7lo\u00ef\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "NE", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der gro\u00dfen Lieb' gottvolle Riesen.", "tokens": ["Der", "gro\u00b7\u00dfen", "Lieb'", "gott\u00b7vol\u00b7le", "Rie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.195": {"line.1": {"text": "Zum Marterberg tanzt' man aufw\u00e4rts,", "tokens": ["Zum", "Mar\u00b7ter\u00b7berg", "tanzt'", "man", "auf\u00b7w\u00e4rts", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "ADV", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Rund um die Kirch \u00bbzum heil'gen Herz\u00ab,", "tokens": ["Rund", "um", "die", "Kirch", "\u00bb", "zum", "heil'\u00b7gen", "Herz", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$(", "APPRART", "ADJA", "NN", "$(", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.196": {"line.1": {"text": "Und Mann mit Weib zum Mond sich schwang,", "tokens": ["Und", "Mann", "mit", "Weib", "zum", "Mond", "sich", "schwang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NN", "APPRART", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df still der Mond in Scherben sprang.", "tokens": ["Da\u00df", "still", "der", "Mond", "in", "Scher\u00b7ben", "sprang", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.197": {"line.1": {"text": "Sterne verpfiffen wie die Fl\u00f6ten,", "tokens": ["Ster\u00b7ne", "ver\u00b7pfif\u00b7fen", "wie", "die", "Fl\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "KOKOM", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kein Fr\u00fchrot kann die T\u00e4nzer t\u00f6ten,", "tokens": ["Kein", "Fr\u00fch\u00b7rot", "kann", "die", "T\u00e4n\u00b7zer", "t\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.198": {"line.1": {"text": "Schliefen wie Flaschen nach dem Mahl,", "tokens": ["Schlie\u00b7fen", "wie", "Fla\u00b7schen", "nach", "dem", "Mahl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "NN", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Kehrer kamen zum Stra\u00dfensaal.", "tokens": ["Keh\u00b7rer", "ka\u00b7men", "zum", "Stra\u00b7\u00dfen\u00b7saal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.199": {"line.1": {"text": "Es leb' die Lieb'! blieb's Losungswort,", "tokens": ["Es", "leb'", "die", "Lieb'", "!", "blie\u00b7b's", "Lo\u00b7sungs\u00b7wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$.", "ADJA", "NN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Behutsam schob man Scherben fort. \u2013", "tokens": ["Be\u00b7hut\u00b7sam", "schob", "man", "Scher\u00b7ben", "fort", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "VVFIN", "PIS", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.200": {"line.1": {"text": "So hielt ich nachts die Augen offen", "tokens": ["So", "hielt", "ich", "nachts", "die", "Au\u00b7gen", "of\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und tat verz\u00fcckt in Bildern hoffen.", "tokens": ["Und", "tat", "ver\u00b7z\u00fcckt", "in", "Bil\u00b7dern", "hof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.201": {"line.1": {"text": "Ich wagte nicht, zu ihr zu gehn,", "tokens": ["Ich", "wag\u00b7te", "nicht", ",", "zu", "ihr", "zu", "gehn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Aus Angst, sie k\u00f6nnt den R\u00fccken drehn,", "tokens": ["Aus", "Angst", ",", "sie", "k\u00f6nnt", "den", "R\u00fc\u00b7cken", "drehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.202": {"line.1": {"text": "Und sich f\u00fcr immer von mir wenden,", "tokens": ["Und", "sich", "f\u00fcr", "im\u00b7mer", "von", "mir", "wen\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und schn\u00f6de m\u00fc\u00dft' mein Herz verenden.", "tokens": ["Und", "schn\u00f6\u00b7de", "m\u00fc\u00dft'", "mein", "Herz", "ver\u00b7en\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VMFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.203": {"line.1": {"text": "Ich wartete den Zufall ab,", "tokens": ["Ich", "war\u00b7te\u00b7te", "den", "Zu\u00b7fall", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der sich zum drittenmal begab.", "tokens": ["Der", "sich", "zum", "drit\u00b7ten\u00b7mal", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPRART", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.204": {"line.1": {"text": "Dem Zufall mu\u00df ein Hoch ich bringen,", "tokens": ["Dem", "Zu\u00b7fall", "mu\u00df", "ein", "Hoch", "ich", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er ist es wert, ihn zu besingen.", "tokens": ["Er", "ist", "es", "wert", ",", "ihn", "zu", "be\u00b7sin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.205": {"line.1": {"text": "Der Zufall fragt nicht wo, nicht wie,", "tokens": ["Der", "Zu\u00b7fall", "fragt", "nicht", "wo", ",", "nicht", "wie", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "PWAV", "$,", "PTKNEG", "KOKOM", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zerst\u00f6rt und bringt die Harmonie,", "tokens": ["Zer\u00b7st\u00f6rt", "und", "bringt", "die", "Har\u00b7mo\u00b7nie", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.206": {"line.1": {"text": "Kann selbst in Mi\u00dfkredit nicht kommen,", "tokens": ["Kann", "selbst", "in", "Mi\u00df\u00b7kre\u00b7dit", "nicht", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn er sich l\u00e4cherlich benommen.", "tokens": ["Wenn", "er", "sich", "l\u00e4\u00b7cher\u00b7lich", "be\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.207": {"line.1": {"text": "Ich \u00c4rmster, ich kann nichts daf\u00fcr,", "tokens": ["Ich", "\u00c4rms\u00b7ter", ",", "ich", "kann", "nichts", "da\u00b7f\u00fcr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$,", "PPER", "VMFIN", "PIS", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ach, l\u00e4cherlich kam er zu mir.", "tokens": ["Ach", ",", "l\u00e4\u00b7cher\u00b7lich", "kam", "er", "zu", "mir", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "ADJD", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.208": {"line.1": {"text": "Wenn man es mal recht eilig hat,", "tokens": ["Wenn", "man", "es", "mal", "recht", "ei\u00b7lig", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gibt's Omnibusse in der Stadt.", "tokens": ["Gibt's", "Om\u00b7ni\u00b7bus\u00b7se", "in", "der", "Stadt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.209": {"line.1": {"text": "Ein Platz war n\u00e4mlich nur noch frei,", "tokens": ["Ein", "Platz", "war", "n\u00e4m\u00b7lich", "nur", "noch", "frei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau K\u00f6nigin sa\u00df dicht dabei,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "sa\u00df", "dicht", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "ADJD", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.210": {"line.1": {"text": "Ich lie\u00df mich ihr zur Seite nieder,", "tokens": ["Ich", "lie\u00df", "mich", "ihr", "zur", "Sei\u00b7te", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Empfahl dem Himmel meine Glieder.", "tokens": ["Emp\u00b7fahl", "dem", "Him\u00b7mel", "mei\u00b7ne", "Glie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.211": {"line.1": {"text": "Sie sah mich noch nicht vorderhand,", "tokens": ["Sie", "sah", "mich", "noch", "nicht", "vor\u00b7der\u00b7hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ich blieb ihr noch unbekannt.", "tokens": ["Und", "ich", "blieb", "ihr", "noch", "un\u00b7be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.212": {"line.1": {"text": "Ein Omnibus, der sch\u00fcttelt stark,", "tokens": ["Ein", "Om\u00b7ni\u00b7bus", ",", "der", "sch\u00fct\u00b7telt", "stark", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich f\u00fchlte mein Gehirn wie Quark,", "tokens": ["Ich", "f\u00fchl\u00b7te", "mein", "Ge\u00b7hirn", "wie", "Quark", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.213": {"line.1": {"text": "Da Schulter ich an Schulter sa\u00df", "tokens": ["Da", "Schul\u00b7ter", "ich", "an", "Schul\u00b7ter", "sa\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "PPER", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit ihr, die mir am Herzen fra\u00df.", "tokens": ["Mit", "ihr", ",", "die", "mir", "am", "Her\u00b7zen", "fra\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "$,", "PRELS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.214": {"line.1": {"text": "Ich f\u00fchlte bald, ich w\u00fcrde toll,", "tokens": ["Ich", "f\u00fchl\u00b7te", "bald", ",", "ich", "w\u00fcr\u00b7de", "toll", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Kopf brannte wie Alkohol,", "tokens": ["Mein", "Kopf", "brann\u00b7te", "wie", "Al\u00b7ko\u00b7hol", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.215": {"line.1": {"text": "Die Augen wuchsen gro\u00df wie R\u00e4der.", "tokens": ["Die", "Au\u00b7gen", "wuch\u00b7sen", "gro\u00df", "wie", "R\u00e4\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich glaub', ich werde Attent\u00e4ter,", "tokens": ["Ich", "glaub'", ",", "ich", "wer\u00b7de", "At\u00b7ten\u00b7t\u00e4\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.216": {"line.1": {"text": "Denn alles dr\u00e4ngt nach einem Ku\u00df,", "tokens": ["Denn", "al\u00b7les", "dr\u00e4ngt", "nach", "ei\u00b7nem", "Ku\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den ich jetzt endlich haben mu\u00df.", "tokens": ["Den", "ich", "jetzt", "end\u00b7lich", "ha\u00b7ben", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.217": {"line.1": {"text": "F\u00fchlte Fieber in jedem Arm,", "tokens": ["F\u00fchl\u00b7te", "Fie\u00b7ber", "in", "je\u00b7dem", "Arm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Selbst meine Sohlen wurden warm.", "tokens": ["Selbst", "mei\u00b7ne", "Soh\u00b7len", "wur\u00b7den", "warm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.218": {"line.1": {"text": "Ich bin ganz j\u00e4hlings aufgesprungen", "tokens": ["Ich", "bin", "ganz", "j\u00e4h\u00b7lings", "auf\u00b7ge\u00b7sprun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und hab' Frau K\u00f6nigin umschlungen", "tokens": ["Und", "hab'", "Frau", "K\u00f6\u00b7ni\u00b7gin", "um\u00b7schlun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "NN", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.219": {"line.1": {"text": "Und k\u00fcss' die Dame durch den Schleier,", "tokens": ["Und", "k\u00fcss'", "die", "Da\u00b7me", "durch", "den", "Schlei\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann erst war mir die Seele freier.", "tokens": ["Dann", "erst", "war", "mir", "die", "See\u00b7le", "frei\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ART", "NN", "ADJD", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}}, "stanza.220": {"line.1": {"text": "Sie schreit, bis sie mich schnell erkennt,", "tokens": ["Sie", "schreit", ",", "bis", "sie", "mich", "schnell", "er\u00b7kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "PRF", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch alles schon zusammenrennt,", "tokens": ["Doch", "al\u00b7les", "schon", "zu\u00b7sam\u00b7men\u00b7rennt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.221": {"line.1": {"text": "Man flieht, man ruft den Kondukteur,", "tokens": ["Man", "flieht", ",", "man", "ruft", "den", "Kon\u00b7duk\u00b7teur", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man kreischt: \u00bbEin Narr macht hier Malheur!\u00ab", "tokens": ["Man", "kreischt", ":", "\u00bb", "Ein", "Narr", "macht", "hier", "Mal\u00b7heur", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$.", "$(", "ART", "NN", "VVFIN", "ADV", "NN", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.222": {"line.1": {"text": "Man stoppt. Doch die Frau K\u00f6nigin", "tokens": ["Man", "stoppt", ".", "Doch", "die", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "$.", "KON", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sagt zu den Leuten obenhin:", "tokens": ["Sagt", "zu", "den", "Leu\u00b7ten", "o\u00b7ben\u00b7hin", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.223": {"line.1": {"text": "\u00bbes ist ja weiter nichts geschehn", "tokens": ["\u00bb", "es", "ist", "ja", "wei\u00b7ter", "nichts", "ge\u00b7schehn"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VAFIN", "ADV", "ADV", "PIS", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als nur ein frohes Wiedersehn.\u00ab", "tokens": ["Als", "nur", "ein", "fro\u00b7hes", "Wie\u00b7der\u00b7sehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.224": {"line.1": {"text": "Sie ging dann gern mit mir spazieren,", "tokens": ["Sie", "ging", "dann", "gern", "mit", "mir", "spa\u00b7zie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sollt' sie zu sch\u00f6nen Bildern f\u00fchren.", "tokens": ["Sollt'", "sie", "zu", "sch\u00f6\u00b7nen", "Bil\u00b7dern", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.225": {"line.1": {"text": "Sie war noch rosenrot vom Ku\u00df", "tokens": ["Sie", "war", "noch", "ro\u00b7sen\u00b7rot", "vom", "Ku\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sprach nicht mehr vom Omnibus.", "tokens": ["Und", "sprach", "nicht", "mehr", "vom", "Om\u00b7ni\u00b7bus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.226": {"line.1": {"text": "Wenn Wangen sich wie Blumen zeigen,", "tokens": ["Wenn", "Wan\u00b7gen", "sich", "wie", "Blu\u00b7men", "zei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PRF", "KOKOM", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann platzt im Herzen bald das Schweigen.", "tokens": ["Dann", "platzt", "im", "Her\u00b7zen", "bald", "das", "Schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.227": {"line.1": {"text": "Und in den Louvregalerien", "tokens": ["Und", "in", "den", "Louv\u00b7re\u00b7ga\u00b7le\u00b7ri\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War's Wunschschlo\u00df der Frau K\u00f6nigin.", "tokens": ["Wa\u00b7r's", "Wunschsc\u00b7hlo\u00df", "der", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ART", "NN", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.228": {"line.1": {"text": "Die Welt herrlich um uns entstand,", "tokens": ["Die", "Welt", "herr\u00b7lich", "um", "uns", "ent\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "PPER", "VVFIN", "$,"], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mit Lieb' gemalt auf Leinewand,", "tokens": ["Mit", "Lieb'", "ge\u00b7malt", "auf", "Lei\u00b7ne\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.229": {"line.1": {"text": "Wir sa\u00dfen still vor einem Bild", "tokens": ["Wir", "sa\u00b7\u00dfen", "still", "vor", "ei\u00b7nem", "Bild"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Mondpracht und doch seltsam wild,", "tokens": ["In", "Mond\u00b7pracht", "und", "doch", "selt\u00b7sam", "wild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADV", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.230": {"line.1": {"text": "Ein schwarz verzweifelt Ackerland,", "tokens": ["Ein", "schwarz", "ver\u00b7zwei\u00b7felt", "A\u00b7cker\u00b7land", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Wassergraben rechter Hand,", "tokens": ["Ein", "Was\u00b7ser\u00b7gra\u00b7ben", "rech\u00b7ter", "Hand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.231": {"line.1": {"text": "Gemalt nach schwangerm Abendregen,", "tokens": ["Ge\u00b7malt", "nach", "schwan\u00b7germ", "A\u00b7bend\u00b7re\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Pf\u00fctzen noch auf allen Wegen;", "tokens": ["Und", "Pf\u00fct\u00b7zen", "noch", "auf", "al\u00b7len", "We\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.232": {"line.1": {"text": "In Wolken, die voll F\u00f6hn und na\u00df,", "tokens": ["In", "Wol\u00b7ken", ",", "die", "voll", "F\u00f6hn", "und", "na\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "ADJD", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Mond grell wie ein Blitzstrahl sa\u00df.", "tokens": ["Der", "Mond", "grell", "wie", "ein", "Blitz\u00b7strahl", "sa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.233": {"line.1": {"text": "\u00bbhier in dem Bilde wollen wir", "tokens": ["\u00bb", "hier", "in", "dem", "Bil\u00b7de", "wol\u00b7len", "wir"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "VMFIN", "PPER"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spazieren gehn,\u00ab sprach sie zu mir.", "tokens": ["Spa\u00b7zie\u00b7ren", "gehn", ",", "\u00ab", "sprach", "sie", "zu", "mir", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "$(", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.234": {"line.1": {"text": "Wir sa\u00dfen eng auf dem Sofa", "tokens": ["Wir", "sa\u00b7\u00dfen", "eng", "auf", "dem", "So\u00b7fa"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und gingen in die Landschaft da.", "tokens": ["Und", "gin\u00b7gen", "in", "die", "Land\u00b7schaft", "da", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.235": {"line.1": {"text": "Sie sprach so g\u00f6ttlich nebenbei,", "tokens": ["Sie", "sprach", "so", "g\u00f6tt\u00b7lich", "ne\u00b7ben\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und was sie sprach, war einerlei.", "tokens": ["Und", "was", "sie", "sprach", ",", "war", "ei\u00b7ner\u00b7lei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "PIS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.236": {"line.1": {"text": "Ich f\u00fchlte es bei ihr sogleich:", "tokens": ["Ich", "f\u00fchl\u00b7te", "es", "bei", "ihr", "sog\u00b7leich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ja, ich und sie werden ein Reich.", "tokens": ["Ja", ",", "ich", "und", "sie", "wer\u00b7den", "ein", "Reich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "KON", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "----+--+", "measure": "iambic.di.chol"}}, "stanza.237": {"line.1": {"text": "Der Ku\u00df hat freier mich gemacht,", "tokens": ["Der", "Ku\u00df", "hat", "frei\u00b7er", "mich", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ich erz\u00e4hlte von der Nacht,", "tokens": ["Und", "ich", "er\u00b7z\u00e4hl\u00b7te", "von", "der", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.238": {"line.1": {"text": "Da\u00df ich ihr Fenster still besessen", "tokens": ["Da\u00df", "ich", "ihr", "Fens\u00b7ter", "still", "be\u00b7ses\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Sehnsucht t\u00e4t den Mond auffressen.", "tokens": ["Und", "Sehn\u00b7sucht", "t\u00e4t", "den", "Mond", "auf\u00b7fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.239": {"line.1": {"text": "Da tat der F\u00f6hnwind hei\u00df umgehen,", "tokens": ["Da", "tat", "der", "F\u00f6hn\u00b7wind", "hei\u00df", "um\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Louvre tat voll Schw\u00fcle stehen.", "tokens": ["Der", "Louv\u00b7re", "tat", "voll", "Schw\u00fc\u00b7le", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.240": {"line.1": {"text": "Mir war, als folgten uns aus Rahmen", "tokens": ["Mir", "war", ",", "als", "folg\u00b7ten", "uns", "aus", "Rah\u00b7men"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "KOKOM", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "All die gemalten Herrn und Damen.", "tokens": ["All", "die", "ge\u00b7mal\u00b7ten", "Herrn", "und", "Da\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.241": {"line.1": {"text": "Leute aus jeglichem Jahrhundert", "tokens": ["Leu\u00b7te", "aus", "jeg\u00b7li\u00b7chem", "Jahr\u00b7hun\u00b7dert"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "PIAT", "NN"], "meter": "+--+---+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sie haben K\u00f6nigin bewundert.", "tokens": ["Sie", "ha\u00b7ben", "K\u00f6\u00b7ni\u00b7gin", "be\u00b7wun\u00b7dert", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.242": {"line.1": {"text": "Sie konnte Tote zittern machen,", "tokens": ["Sie", "konn\u00b7te", "To\u00b7te", "zit\u00b7tern", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lieb' sprach zu ihr in allen Sprachen.", "tokens": ["Lieb'", "sprach", "zu", "ihr", "in", "al\u00b7len", "Spra\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPOSAT", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.243": {"line.1": {"text": "Rubens und Rembrandt gl\u00fchten da,", "tokens": ["Ru\u00b7bens", "und", "Rem\u00b7brandt", "gl\u00fch\u00b7ten", "da", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "VVFIN", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sobald Frau K\u00f6nigin hinsah,", "tokens": ["So\u00b7bald", "Frau", "K\u00f6\u00b7ni\u00b7gin", "hin\u00b7sah", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.244": {"line.1": {"text": "Holbein und D\u00fcrer gr\u00fc\u00dften tief,", "tokens": ["Hol\u00b7bein", "und", "D\u00fc\u00b7rer", "gr\u00fc\u00df\u00b7ten", "tief", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihr Mund sanft: \u00bbMadonna\u00ab rief.", "tokens": ["Und", "ihr", "Mund", "sanft", ":", "\u00bb", "Ma\u00b7don\u00b7na", "\u00ab", "rief", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ADJD", "$.", "$(", "FM", "$(", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.245": {"line.1": {"text": "Weil man das Singen ja nicht sieht,", "tokens": ["Weil", "man", "das", "Sin\u00b7gen", "ja", "nicht", "sieht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "ADV", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sang K\u00f6nigin halblaut ein Lied,", "tokens": ["Sang", "K\u00f6\u00b7ni\u00b7gin", "hal\u00b7blaut", "ein", "Lied", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.246": {"line.1": {"text": "Lie\u00df wie ein Taschentuch es fallen", "tokens": ["Lie\u00df", "wie", "ein", "Ta\u00b7schen\u00b7tuch", "es", "fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In Huld als Dank ihren Vasallen.", "tokens": ["In", "Huld", "als", "Dank", "ih\u00b7ren", "Va\u00b7sal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KOKOM", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.247": {"line.1": {"text": "Und Milos Venus lud uns ein,", "tokens": ["Und", "Mi\u00b7los", "Ve\u00b7nus", "lud", "uns", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Marmor hatte Feuerschein,", "tokens": ["Ihr", "Mar\u00b7mor", "hat\u00b7te", "Feu\u00b7er\u00b7schein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.248": {"line.1": {"text": "Ihr Leib war wie ein Sonnenst\u00fcck,", "tokens": ["Ihr", "Leib", "war", "wie", "ein", "Son\u00b7nen\u00b7st\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es war ihr h\u00f6chster Augenblick.", "tokens": ["Es", "war", "ihr", "h\u00f6chs\u00b7ter", "Au\u00b7gen\u00b7blick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.249": {"line.1": {"text": "Denn einst, als man Paris beschossen,", "tokens": ["Denn", "einst", ",", "als", "man", "Pa\u00b7ris", "be\u00b7schos\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "PIS", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat das die Venus schwer verdrossen,", "tokens": ["Hat", "das", "die", "Ve\u00b7nus", "schwer", "ver\u00b7dros\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.250": {"line.1": {"text": "Sie legte sich in eine Kist'.", "tokens": ["Sie", "leg\u00b7te", "sich", "in", "ei\u00b7ne", "Kist'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Versteckt in einer Fuhre Mist,", "tokens": ["Ver\u00b7steckt", "in", "ei\u00b7ner", "Fuh\u00b7re", "Mist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.251": {"line.1": {"text": "Lag sie in einer der Kasernen,", "tokens": ["Lag", "sie", "in", "ei\u00b7ner", "der", "Ka\u00b7ser\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis sich der Deutsche tat entfernen.", "tokens": ["Bis", "sich", "der", "Deut\u00b7sche", "tat", "ent\u00b7fer\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.252": {"line.1": {"text": "Sich rettend so aus den Gefahren", "tokens": ["Sich", "ret\u00b7tend", "so", "aus", "den", "Ge\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wartet sie jetzt auf Balthasaren.", "tokens": ["War\u00b7tet", "sie", "jetzt", "auf", "Balt\u00b7ha\u00b7sa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}}, "stanza.253": {"line.1": {"text": "Blank, und von Mist nicht einen Schimmer,", "tokens": ["Blank", ",", "und", "von", "Mist", "nicht", "ei\u00b7nen", "Schim\u00b7mer", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "APPR", "NN", "PTKNEG", "ART", "NN", "$,"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Steht sie im Louvrehinterzimmer.", "tokens": ["Steht", "sie", "im", "Louv\u00b7re\u00b7hin\u00b7ter\u00b7zim\u00b7mer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.254": {"line.1": {"text": "Und dann, an diesem Nachmittag,", "tokens": ["Und", "dann", ",", "an", "die\u00b7sem", "Nach\u00b7mit\u00b7tag", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sonne ihr am Nabel lag.", "tokens": ["Die", "Son\u00b7ne", "ihr", "am", "Na\u00b7bel", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.255": {"line.1": {"text": "Da kam der Balthasar auch hin", "tokens": ["Da", "kam", "der", "Balt\u00b7ha\u00b7sar", "auch", "hin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihm zur Seit' Frau K\u00f6nigin.", "tokens": ["Und", "ihm", "zur", "Seit'", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.256": {"line.1": {"text": "Ganz harmlos sagt der Balthasar:", "tokens": ["Ganz", "harm\u00b7los", "sagt", "der", "Balt\u00b7ha\u00b7sar", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbdie Venus ist mal sonderbar!", "tokens": ["\u00bb", "die", "Ve\u00b7nus", "ist", "mal", "son\u00b7der\u00b7bar", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.257": {"line.1": {"text": "Ich sage euch, da\u00df ihr es wi\u00dft,", "tokens": ["Ich", "sa\u00b7ge", "euch", ",", "da\u00df", "ihr", "es", "wi\u00dft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie hier nicht die Sch\u00f6nste ist.\u00ab", "tokens": ["Da\u00df", "sie", "hier", "nicht", "die", "Sch\u00f6ns\u00b7te", "ist", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "PTKNEG", "ART", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.258": {"line.1": {"text": "Und er sah nur Frau K\u00f6nigin", "tokens": ["Und", "er", "sah", "nur", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV", "NN", "NN"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und sah nicht mehr zur Venus hin.", "tokens": ["Und", "sah", "nicht", "mehr", "zur", "Ve\u00b7nus", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.259": {"line.1": {"text": "Als echte Venus freut sie sich,", "tokens": ["Als", "ech\u00b7te", "Ve\u00b7nus", "freut", "sie", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "VVFIN", "PPER", "PRF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sonn' sie sich vom Nabel strich", "tokens": ["Die", "Sonn'", "sie", "sich", "vom", "Na\u00b7bel", "strich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPER", "PRF", "APPRART", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.260": {"line.1": {"text": "Und legt sie auf das Goldhaupt hin", "tokens": ["Und", "legt", "sie", "auf", "das", "Gold\u00b7haupt", "hin"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Krone der Frau K\u00f6nigin.", "tokens": ["Als", "Kro\u00b7ne", "der", "Frau", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.261": {"line.1": {"text": "Frau K\u00f6nigin hat nicht verneint,", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "hat", "nicht", "ver\u00b7neint", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Frau Venus hat uns still vereint,", "tokens": ["Frau", "Ve\u00b7nus", "hat", "uns", "still", "ver\u00b7eint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.262": {"line.1": {"text": "Es waren sich die Herzen nah,", "tokens": ["Es", "wa\u00b7ren", "sich", "die", "Her\u00b7zen", "nah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als w\u00e4r' ich Vater, sie Mama,", "tokens": ["Als", "w\u00e4r'", "ich", "Va\u00b7ter", ",", "sie", "Ma\u00b7ma", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "NN", "$,", "PPER", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.263": {"line.1": {"text": "Sie dr\u00fcckte mir die Lippen zu", "tokens": ["Sie", "dr\u00fcck\u00b7te", "mir", "die", "Lip\u00b7pen", "zu"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ward noch sch\u00f6ner und sprach: \u00bbDu.\u00ab", "tokens": ["Und", "ward", "noch", "sch\u00f6\u00b7ner", "und", "sprach", ":", "\u00bb", "Du", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "KON", "VVFIN", "$.", "$(", "PPER", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.264": {"line.1": {"text": "Von den Gen\u00fcssen der Genu\u00df", "tokens": ["Von", "den", "Ge\u00b7n\u00fcs\u00b7sen", "der", "Ge\u00b7nu\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist so ein richtiger erster Ku\u00df,", "tokens": ["Ist", "so", "ein", "rich\u00b7ti\u00b7ger", "ers\u00b7ter", "Ku\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.265": {"line.1": {"text": "Es m\u00fcssen beide t\u00fcchtig wollen,", "tokens": ["Es", "m\u00fcs\u00b7sen", "bei\u00b7de", "t\u00fcch\u00b7tig", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "ADJD", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sch\u00f6pft man heftig aus dem Vollen.", "tokens": ["Dann", "sch\u00f6pft", "man", "hef\u00b7tig", "aus", "dem", "Vol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.266": {"line.1": {"text": "So hatt' ich es mir ausgedacht,", "tokens": ["So", "hatt'", "ich", "es", "mir", "aus\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch anders ist die Welt gemacht.", "tokens": ["Doch", "an\u00b7ders", "ist", "die", "Welt", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.267": {"line.1": {"text": "Auch ich hab' es erfahren m\u00fcssen:", "tokens": ["Auch", "ich", "hab'", "es", "er\u00b7fah\u00b7ren", "m\u00fcs\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PPER", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein keusches Weib kann noch nicht k\u00fcssen,", "tokens": ["Ein", "keu\u00b7sches", "Weib", "kann", "noch", "nicht", "k\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.268": {"line.1": {"text": "Sie kann die Lippen noch nicht stellen,", "tokens": ["Sie", "kann", "die", "Lip\u00b7pen", "noch", "nicht", "stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tut oft den andern Mund verfehlen,", "tokens": ["Tut", "oft", "den", "an\u00b7dern", "Mund", "ver\u00b7feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.269": {"line.1": {"text": "Sie stellt sich ungeschickt noch an,", "tokens": ["Sie", "stellt", "sich", "un\u00b7ge\u00b7schickt", "noch", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADJD", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man k\u00fc\u00dft statt Lippe oft den Zahn.", "tokens": ["Man", "k\u00fc\u00dft", "statt", "Lip\u00b7pe", "oft", "den", "Zahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NE", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.270": {"line.1": {"text": "Doch Liebe \u00fcbt das K\u00fcssen ein,", "tokens": ["Doch", "Lie\u00b7be", "\u00fcbt", "das", "K\u00fcs\u00b7sen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und dunkel soll es dabei sein.", "tokens": ["Und", "dun\u00b7kel", "soll", "es", "da\u00b7bei", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VMFIN", "PPER", "PAV", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.271": {"line.1": {"text": "Wir fuhren weich in einem Wagen", "tokens": ["Wir", "fuh\u00b7ren", "weich", "in", "ei\u00b7nem", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und lie\u00dfen durch Paris uns tragen.", "tokens": ["Und", "lie\u00b7\u00dfen", "durch", "Pa\u00b7ris", "uns", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.272": {"line.1": {"text": "Der Wagen war ein fliegend Haus,", "tokens": ["Der", "Wa\u00b7gen", "war", "ein", "flie\u00b7gend", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Drin \u00fcbten wir das K\u00fcssen aus.", "tokens": ["Drin", "\u00fcb\u00b7ten", "wir", "das", "K\u00fcs\u00b7sen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.273": {"line.1": {"text": "Man k\u00fc\u00dft sich, und man spricht kein Wort,", "tokens": ["Man", "k\u00fc\u00dft", "sich", ",", "und", "man", "spricht", "kein", "Wort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "$,", "KON", "PIS", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und denkt nicht, \u2013 man ist einfach fort.", "tokens": ["Und", "denkt", "nicht", ",", "\u2013", "man", "ist", "ein\u00b7fach", "fort", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "$(", "PIS", "VAFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.274": {"line.1": {"text": "Das Herz hat jahrelang gehastet,", "tokens": ["Das", "Herz", "hat", "jah\u00b7re\u00b7lang", "ge\u00b7has\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis es den Mund fand, wo es rastet;", "tokens": ["Bis", "es", "den", "Mund", "fand", ",", "wo", "es", "ras\u00b7tet", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.275": {"line.1": {"text": "Es tat ja Tag und Nacht stets rennen,", "tokens": ["Es", "tat", "ja", "Tag", "und", "Nacht", "stets", "ren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "KON", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man kann's dem Herzen wirklich g\u00f6nnen.", "tokens": ["Man", "kann's", "dem", "Her\u00b7zen", "wirk\u00b7lich", "g\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.276": {"line.1": {"text": "Oft hab' ich dr\u00fcber nachgedacht,", "tokens": ["Oft", "hab'", "ich", "dr\u00fc\u00b7ber", "nach\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie doch das gute Herz es macht,", "tokens": ["Wie", "doch", "das", "gu\u00b7te", "Herz", "es", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.277": {"line.1": {"text": "Da\u00df immerfort es wachen kann,", "tokens": ["Da\u00df", "im\u00b7mer\u00b7fort", "es", "wa\u00b7chen", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Arbeitend stets von Jugend an.", "tokens": ["Ar\u00b7bei\u00b7tend", "stets", "von", "Ju\u00b7gend", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.278": {"line.1": {"text": "Nachts, wenn der ganze K\u00f6rper ruht,", "tokens": ["Nachts", ",", "wenn", "der", "gan\u00b7ze", "K\u00f6r\u00b7per", "ruht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sortiert es immer noch das Blut,", "tokens": ["Sor\u00b7tiert", "es", "im\u00b7mer", "noch", "das", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.279": {"line.1": {"text": "Der Muskel schafft oft hundert Jahr.", "tokens": ["Der", "Mus\u00b7kel", "schafft", "oft", "hun\u00b7dert", "Jahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "CARD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich find' es gar nicht sonderbar,", "tokens": ["Ich", "find'", "es", "gar", "nicht", "son\u00b7der\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.280": {"line.1": {"text": "Da\u00df er nach Ku\u00df und Liebe dr\u00e4ngt,", "tokens": ["Da\u00df", "er", "nach", "Ku\u00df", "und", "Lie\u00b7be", "dr\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn dieses ihm Erholung schenkt.", "tokens": ["Wenn", "die\u00b7ses", "ihm", "Er\u00b7ho\u00b7lung", "schenkt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.281": {"line.1": {"text": "O, st\u00f6re nie den Mensch, der k\u00fc\u00dft,", "tokens": ["O", ",", "st\u00f6\u00b7re", "nie", "den", "Mensch", ",", "der", "k\u00fc\u00dft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "ADV", "ART", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil das einfach unmenschlich ist!", "tokens": ["Weil", "das", "ein\u00b7fach", "un\u00b7menschlich", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.282": {"line.1": {"text": "Und in Paris ist man gew\u00f6hnt,", "tokens": ["Und", "in", "Pa\u00b7ris", "ist", "man", "ge\u00b7w\u00f6hnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "VAFIN", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man die Liebe jedem g\u00f6nnt.", "tokens": ["Da\u00df", "man", "die", "Lie\u00b7be", "je\u00b7dem", "g\u00f6nnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.283": {"line.1": {"text": "So k\u00fc\u00dften wir und waren fort,", "tokens": ["So", "k\u00fc\u00df\u00b7ten", "wir", "und", "wa\u00b7ren", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KON", "VAFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sogar noch am Platz ", "tokens": ["So\u00b7gar", "noch", "am", "Platz"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPRART", "NN"], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.284": {"line.1": {"text": "Wo einst man k\u00f6pfte Nacht und Tag,", "tokens": ["Wo", "einst", "man", "k\u00f6pf\u00b7te", "Nacht", "und", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIS", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Pflaster mir voll K\u00fcsse lag.", "tokens": ["Das", "Pflas\u00b7ter", "mir", "voll", "K\u00fcs\u00b7se", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.285": {"line.1": {"text": "Laternen tanzten um uns her,", "tokens": ["La\u00b7ter\u00b7nen", "tanz\u00b7ten", "um", "uns", "her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als wenn der Platz die Milchstra\u00df' w\u00e4r'.", "tokens": ["Als", "wenn", "der", "Platz", "die", "Milch\u00b7stra\u00df'", "w\u00e4r'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.286": {"line.1": {"text": "Doch pl\u00f6tzlich blieb mein Kopf nicht heil,", "tokens": ["Doch", "pl\u00f6tz\u00b7lich", "blieb", "mein", "Kopf", "nicht", "heil", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPOSAT", "NN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Wort fiel schwer wie ein Fallbeil.", "tokens": ["Ein", "Wort", "fiel", "schwer", "wie", "ein", "Fall\u00b7beil", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.287": {"line.1": {"text": "Wo einst die Guillotine stand,", "tokens": ["Wo", "einst", "die", "Guil\u00b7lo\u00b7ti\u00b7ne", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Balzer sich ganz kopflos fand,", "tokens": ["Der", "Bal\u00b7zer", "sich", "ganz", "kopf\u00b7los", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.288": {"line.1": {"text": "Denn K\u00f6nigin sprach ahnungsvoll,", "tokens": ["Denn", "K\u00f6\u00b7ni\u00b7gin", "sprach", "ah\u00b7nungs\u00b7voll", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von \u00bbTreue\u00ab, die man halten soll:", "tokens": ["Von", "\u00bb", "Treu\u00b7e", "\u00ab", ",", "die", "man", "hal\u00b7ten", "soll", ":"], "token_info": ["word", "punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$(", "NN", "$(", "$,", "PRELS", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.289": {"line.1": {"text": "\u00bbdu, Balzer, dein will ich gern sein,", "tokens": ["\u00bb", "du", ",", "Bal\u00b7zer", ",", "dein", "will", "ich", "gern", "sein", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "$,", "NN", "$,", "PPOSAT", "VMFIN", "PPER", "ADV", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch fiele es dir jemals ein,", "tokens": ["Doch", "fie\u00b7le", "es", "dir", "je\u00b7mals", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.290": {"line.1": {"text": "Da\u00df du mich zum Betrug gew\u00e4hlt,", "tokens": ["Da\u00df", "du", "mich", "zum", "Be\u00b7trug", "ge\u00b7w\u00e4hlt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann glaub' ich nichts mehr auf der Welt.\u00ab", "tokens": ["Dann", "glaub'", "ich", "nichts", "mehr", "auf", "der", "Welt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "ADV", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.291": {"line.1": {"text": "Ich wei\u00df nicht, welch ein Blitz geschah,", "tokens": ["Ich", "wei\u00df", "nicht", ",", "welch", "ein", "Blitz", "ge\u00b7schah", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PWAT", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich die Zukunft pl\u00f6tzlich sah.", "tokens": ["Da\u00df", "ich", "die", "Zu\u00b7kunft", "pl\u00f6tz\u00b7lich", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.292": {"line.1": {"text": "Mitten in meines Blutes Saus", "tokens": ["Mit\u00b7ten", "in", "mei\u00b7nes", "Blu\u00b7tes", "Saus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wischte der Blitz den Kopf mir aus.", "tokens": ["Wischte", "der", "Blitz", "den", "Kopf", "mir", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.293": {"line.1": {"text": "Wer k\u00f6nnte es mit Ernst beschw\u00f6ren,", "tokens": ["Wer", "k\u00f6nn\u00b7te", "es", "mit", "Ernst", "be\u00b7schw\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "APPR", "NE", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ihn die Zeiten nie bet\u00f6ren?", "tokens": ["Da\u00df", "ihn", "die", "Zei\u00b7ten", "nie", "be\u00b7t\u00f6\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.294": {"line.1": {"text": "Wei\u00df man denn, wer man selber ist,", "tokens": ["Wei\u00df", "man", "denn", ",", "wer", "man", "sel\u00b7ber", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "$,", "PWS", "PIS", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Getaufter Heide, genannt Christ.", "tokens": ["Ge\u00b7tauf\u00b7ter", "Hei\u00b7de", ",", "ge\u00b7nannt", "Christ", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "NE", "$,", "VVPP", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.295": {"line.1": {"text": "Nie kann ich f\u00fcr mich garantieren,", "tokens": ["Nie", "kann", "ich", "f\u00fcr", "mich", "ga\u00b7ran\u00b7tie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben ist ein st\u00fcndlich Irren.", "tokens": ["Das", "Le\u00b7ben", "ist", "ein", "st\u00fcnd\u00b7lich", "Ir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.296": {"line.1": {"text": "Heut leg' als Christ ich mich zu Bett,", "tokens": ["Heut", "leg'", "als", "Christ", "ich", "mich", "zu", "Bett", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KOUS", "NN", "PPER", "PRF", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und fr\u00fch bet' ich zu Mohammed.", "tokens": ["Und", "fr\u00fch", "bet'", "ich", "zu", "Mo\u00b7ham\u00b7med", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.297": {"line.1": {"text": "Denn immer blindhin rollt die Welt,", "tokens": ["Denn", "im\u00b7mer", "blind\u00b7hin", "rollt", "die", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kurz, nur die Seel' im Leib aush\u00e4lt.", "tokens": ["Kurz", ",", "nur", "die", "Seel'", "im", "Leib", "aus\u00b7h\u00e4lt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADV", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.298": {"line.1": {"text": "Ersch\u00fcttert hat mich, was sie sprach,", "tokens": ["Er\u00b7sch\u00fct\u00b7tert", "hat", "mich", ",", "was", "sie", "sprach", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPER", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es war der Liebe erstes \u00bbAch\u00ab.", "tokens": ["Es", "war", "der", "Lie\u00b7be", "ers\u00b7tes", "\u00bb", "Ach", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJA", "$(", "ITJ", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.299": {"line.1": {"text": "Man soll im Gl\u00fcck am Leid nicht r\u00fchren,", "tokens": ["Man", "soll", "im", "Gl\u00fcck", "am", "Leid", "nicht", "r\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPRART", "NN", "APPRART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht stets nach der Mechanik sp\u00fcren,", "tokens": ["Nicht", "stets", "nach", "der", "Me\u00b7cha\u00b7nik", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.300": {"line.1": {"text": "Puppen sind wir im Puppenhaus,", "tokens": ["Pup\u00b7pen", "sind", "wir", "im", "Pup\u00b7pen\u00b7haus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Spielt man zu hart, l\u00e4uft S\u00e4gmehl 'raus.", "tokens": ["Spielt", "man", "zu", "hart", ",", "l\u00e4uft", "S\u00e4g\u00b7mehl", "'raus", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PTKA", "ADJD", "$,", "VVFIN", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}