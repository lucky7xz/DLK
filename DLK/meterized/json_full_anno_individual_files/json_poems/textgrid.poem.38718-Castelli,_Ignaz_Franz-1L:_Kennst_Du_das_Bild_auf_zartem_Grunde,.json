{"textgrid.poem.38718": {"metadata": {"author": {"name": "Castelli, Ignaz Franz", "birth": "N.A.", "death": "N.A."}, "title": "1L: Kennst Du das Bild auf zartem Grunde,", "genre": "verse", "period": "N.A.", "pub_year": 1821, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Kennst Du das Bild auf zartem Grunde,", "tokens": ["Kennst", "Du", "das", "Bild", "auf", "zar\u00b7tem", "Grun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zuweilen schm\u00fcckst der Perlen Glanz,", "tokens": ["Zu\u00b7wei\u00b7len", "schm\u00fcckst", "der", "Per\u00b7len", "Glanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er spaltet sich gleich einer Wunde,", "tokens": ["Er", "spal\u00b7tet", "sich", "gleich", "ei\u00b7ner", "Wun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dennoch ist es frisch und ganz.", "tokens": ["Und", "den\u00b7noch", "ist", "es", "frisch", "und", "ganz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Im engen Raum ist's ausgef\u00fchret,", "tokens": ["Im", "en\u00b7gen", "Raum", "ist's", "aus\u00b7ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ein sanftes Buschwerk fa\u00dft es ein,", "tokens": ["Ein", "sanf\u00b7tes", "Busc\u00b7hwerk", "fa\u00dft", "es", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Doch alle Gr\u00f6\u00dfe, die Dich r\u00fchret,", "tokens": ["Doch", "al\u00b7le", "Gr\u00f6\u00b7\u00dfe", ",", "die", "Dich", "r\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kennst Du durch dieses Bild allein, \u2013", "tokens": ["Kennst", "Du", "durch", "die\u00b7ses", "Bild", "al\u00b7lein", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PDAT", "NN", "ADV", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Ihm gleicht an Werth kein Edelstein.", "tokens": ["Ihm", "gleicht", "an", "Werth", "kein", "E\u00b7del\u00b7stein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Es kann ganz leicht das Wa\u00dfer halten,", "tokens": ["Es", "kann", "ganz", "leicht", "das", "Wa\u00b7\u00dfer", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und einen Boden hat's doch nicht,", "tokens": ["Und", "ei\u00b7nen", "Bo\u00b7den", "hat's", "doch", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sah Franzosen es entfalten", "tokens": ["Man", "sah", "Fran\u00b7zo\u00b7sen", "es", "ent\u00b7fal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "NN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von denen keiner fr\u00e4nkisch spricht.", "tokens": ["Von", "de\u00b7nen", "kei\u00b7ner", "fr\u00e4n\u00b7kisch", "spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Kennst Du das Bild auf zartem Grunde,", "tokens": ["Kennst", "Du", "das", "Bild", "auf", "zar\u00b7tem", "Grun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zuweilen schm\u00fcckst der Perlen Glanz,", "tokens": ["Zu\u00b7wei\u00b7len", "schm\u00fcckst", "der", "Per\u00b7len", "Glanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er spaltet sich gleich einer Wunde,", "tokens": ["Er", "spal\u00b7tet", "sich", "gleich", "ei\u00b7ner", "Wun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dennoch ist es frisch und ganz.", "tokens": ["Und", "den\u00b7noch", "ist", "es", "frisch", "und", "ganz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADJD", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Im engen Raum ist's ausgef\u00fchret,", "tokens": ["Im", "en\u00b7gen", "Raum", "ist's", "aus\u00b7ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ein sanftes Buschwerk fa\u00dft es ein,", "tokens": ["Ein", "sanf\u00b7tes", "Busc\u00b7hwerk", "fa\u00dft", "es", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Doch alle Gr\u00f6\u00dfe, die Dich r\u00fchret,", "tokens": ["Doch", "al\u00b7le", "Gr\u00f6\u00b7\u00dfe", ",", "die", "Dich", "r\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kennst Du durch dieses Bild allein, \u2013", "tokens": ["Kennst", "Du", "durch", "die\u00b7ses", "Bild", "al\u00b7lein", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PDAT", "NN", "ADV", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Ihm gleicht an Werth kein Edelstein.", "tokens": ["Ihm", "gleicht", "an", "Werth", "kein", "E\u00b7del\u00b7stein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Es kann ganz leicht das Wa\u00dfer halten,", "tokens": ["Es", "kann", "ganz", "leicht", "das", "Wa\u00b7\u00dfer", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und einen Boden hat's doch nicht,", "tokens": ["Und", "ei\u00b7nen", "Bo\u00b7den", "hat's", "doch", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man sah Franzosen es entfalten", "tokens": ["Man", "sah", "Fran\u00b7zo\u00b7sen", "es", "ent\u00b7fal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "NN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von denen keiner fr\u00e4nkisch spricht.", "tokens": ["Von", "de\u00b7nen", "kei\u00b7ner", "fr\u00e4n\u00b7kisch", "spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}