{"textgrid.poem.53894": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Bei uns in Europa", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr schickt uns aus dem Lande von Ford", "tokens": ["Ihr", "schickt", "uns", "aus", "dem", "Lan\u00b7de", "von", "Ford"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "einen ziemlich miesen Menschenexport:", "tokens": ["ei\u00b7nen", "ziem\u00b7lich", "mie\u00b7sen", "Men\u00b7schen\u00b7ex\u00b7port", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "\u00fcberschwemmt sind Paris und Griechenland", "tokens": ["\u00fc\u00b7bersc\u00b7hwemmt", "sind", "Pa\u00b7ris", "und", "Grie\u00b7chen\u00b7land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "NE", "KON", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "von euerm m\u00e4\u00dfigen Mittelstand.", "tokens": ["von", "eu\u00b7erm", "m\u00e4\u00b7\u00dfi\u00b7gen", "Mit\u00b7tel\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Diese Reisenden, laut und prahlerisch,", "tokens": ["Die\u00b7se", "Rei\u00b7sen\u00b7den", ",", "laut", "und", "prah\u00b7le\u00b7risch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "legen geistig die F\u00fc\u00dfe auf den Tisch,", "tokens": ["le\u00b7gen", "geis\u00b7tig", "die", "F\u00fc\u00b7\u00dfe", "auf", "den", "Tisch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "fallen l\u00e4stig an allen Orten;", "tokens": ["fal\u00b7len", "l\u00e4s\u00b7tig", "an", "al\u00b7len", "Or\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "PIAT", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "und jeder zweite Satz beginnt mit den Worten:", "tokens": ["und", "je\u00b7der", "zwei\u00b7te", "Satz", "be\u00b7ginnt", "mit", "den", "Wor\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "\u00bbbei uns in Amerika . . . \u00ab", "tokens": ["\u00bb", "bei", "uns", "in", "A\u00b7me\u00b7ri\u00b7ka", ".", ".", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "APPR", "PPER", "APPR", "NE", "$.", "$.", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.2": {"line.1": {"text": "Bei euch in Amerika gibts zweierlei Rechte", "tokens": ["Bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "gibts", "zwei\u00b7er\u00b7lei", "Rech\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "(f\u00fcr Arme und Reiche) \u2013 gibt es Gute und Schlechte;", "tokens": ["(", "f\u00fcr", "Ar\u00b7me", "und", "Rei\u00b7che", ")", "\u2013", "gibt", "es", "Gu\u00b7te", "und", "Schlech\u00b7te", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "KON", "NE", "$(", "$(", "VVFIN", "PPER", "NN", "KON", "NN", "$."], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "gibt es solche und solche: Lewis und Mencken,", "tokens": ["gibt", "es", "sol\u00b7che", "und", "sol\u00b7che", ":", "Le\u00b7wis", "und", "Men\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "KON", "PIAT", "$.", "NE", "KON", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "und Dollardiener, die in Dollars denken.", "tokens": ["und", "Dol\u00b7lar\u00b7die\u00b7ner", ",", "die", "in", "Dol\u00b7lars", "den\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Bei euch in Amerika gibt es Republikaner", "tokens": ["Bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "gibt", "es", "Re\u00b7pub\u00b7li\u00b7ka\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VVFIN", "PPER", "NN"], "meter": "-+-+---+--+-+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "und richtende blutige Puritaner.", "tokens": ["und", "rich\u00b7ten\u00b7de", "blu\u00b7ti\u00b7ge", "Pu\u00b7ri\u00b7ta\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Ihr habt Kraft, Jugend und Silberlinge \u2013", "tokens": ["Ihr", "habt", "Kraft", ",", "Ju\u00b7gend", "und", "Sil\u00b7ber\u00b7lin\u00b7ge", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "NN", "KON", "NN", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.8": {"text": "aber ihr seid nicht das Ma\u00df aller Dinge,", "tokens": ["a\u00b7ber", "ihr", "seid", "nicht", "das", "Ma\u00df", "al\u00b7ler", "Din\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PTKNEG", "ART", "NN", "PIAT", "NN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.9": {"text": "bei euch in Amerika.", "tokens": ["bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.3": {"line.1": {"text": "Bei uns in Europa ist das Weib", "tokens": ["Bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "ist", "das", "Weib"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "keine Haremsfrau ohne Unterleib \u2013", "tokens": ["kei\u00b7ne", "Ha\u00b7rems\u00b7frau", "oh\u00b7ne", "Un\u00b7ter\u00b7leib", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "NN", "$("], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "bei uns in Europa ist die schwarze Haut", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "ist", "die", "schwar\u00b7ze", "Haut"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "kein Aussatz, dem man Extra-Bahnwagen baut;", "tokens": ["kein", "Aus\u00b7satz", ",", "dem", "man", "Ex\u00b7tra\u00b7Bahn\u00b7wa\u00b7gen", "baut", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "PIS", "NN", "VVFIN", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}, "line.5": {"text": "bei uns in Europa kann wer ohne Geld sein", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "kann", "wer", "oh\u00b7ne", "Geld", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VMFIN", "PWS", "APPR", "NN", "PPOSAT"], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}, "line.6": {"text": "und dennoch, dennoch auf der Welt sein \u2013", "tokens": ["und", "den\u00b7noch", ",", "den\u00b7noch", "auf", "der", "Welt", "sein", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "APPR", "ART", "NN", "VAINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "bei uns in Europa kann man bestehn,", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "kann", "man", "be\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "ohne in die Sonntags-Schule zu gehn,", "tokens": ["oh\u00b7ne", "in", "die", "Sonn\u00b7tags\u00b7Schu\u00b7le", "zu", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.9": {"text": "weil fast keiner so am Altare steht:", "tokens": ["weil", "fast", "kei\u00b7ner", "so", "am", "Al\u00b7ta\u00b7re", "steht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIS", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+----+-+", "measure": "unknown.measure.tetra"}, "line.10": {"text": "eine pl\u00e4rrende n\u00fcchterne Realit\u00e4t \u2013", "tokens": ["ei\u00b7ne", "pl\u00e4r\u00b7ren\u00b7de", "n\u00fcch\u00b7ter\u00b7ne", "Re\u00b7a\u00b7li\u00b7t\u00e4t", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.11": {"text": "wie bei euch in Amerika.", "tokens": ["wie", "bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPER", "APPR", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "Das wissen nat\u00fcrlich bei euch die Guten", "tokens": ["Das", "wis\u00b7sen", "na\u00b7t\u00fcr\u00b7lich", "bei", "euch", "die", "Gu\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "ganz genau. Der Rest hat von Blasen und Tuten", "tokens": ["ganz", "ge\u00b7nau", ".", "Der", "Rest", "hat", "von", "Bla\u00b7sen", "und", "Tu\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "$.", "ART", "NN", "VAFIN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "keine Ahnung. H\u00f6rt nur den Schmeichelchor", "tokens": ["kei\u00b7ne", "Ah\u00b7nung", ".", "H\u00f6rt", "nur", "den", "Schmei\u00b7chelc\u00b7hor"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "$.", "VVFIN", "ADV", "ART", "NN"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "seiner news-papers; kommt sich so erstklassig vor . . .", "tokens": ["sei\u00b7ner", "news\u00b7\u00b7pa\u00b7pers", ";", "kommt", "sich", "so", "erst\u00b7klas\u00b7sig", "vor", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "$.", "VVFIN", "PRF", "ADV", "ADJD", "PTKVZ", "$.", "$.", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.5": {"text": "H\u00f6r nicht hin, Arbeitsmann. La\u00df sie ziehn,", "tokens": ["H\u00f6r", "nicht", "hin", ",", "Ar\u00b7beits\u00b7mann", ".", "La\u00df", "sie", "ziehn", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "PTKVZ", "$,", "NE", "$.", "VVIMP", "PPER", "VVINF", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "die Eitelkeiten der Bourgeoisien.", "tokens": ["die", "Ei\u00b7tel\u00b7kei\u00b7ten", "der", "Bour\u00b7ge\u00b7o\u00b7i\u00b7si\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+-+--+-++-+", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "P\u00e4sse, Fahnen und Paraden", "tokens": ["P\u00e4s\u00b7se", ",", "Fah\u00b7nen", "und", "Pa\u00b7ra\u00b7den"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "das sind l\u00e4cherliche Zementfassaden . . .", "tokens": ["das", "sind", "l\u00e4\u00b7cher\u00b7li\u00b7che", "Ze\u00b7ment\u00b7fas\u00b7sa\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PDS", "VAFIN", "ADJA", "NN", "$.", "$.", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Denn die wahre Grenze, zwischen Drohnen und Fronen,", "tokens": ["Denn", "die", "wah\u00b7re", "Gren\u00b7ze", ",", "zwi\u00b7schen", "Droh\u00b7nen", "und", "Fro\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+--+-", "measure": "hexameter"}, "line.10": {"text": "l\u00e4uft quer hindurch durch alle Nationen \u2013", "tokens": ["l\u00e4uft", "quer", "hin\u00b7durch", "durch", "al\u00b7le", "Na\u00b7ti\u00b7o\u00b7nen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PAV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "bei euch in Amerika.", "tokens": ["bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.12": {"text": "Wie bei uns in Europa.", "tokens": ["Wie", "bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPER", "APPR", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Ihr schickt uns aus dem Lande von Ford", "tokens": ["Ihr", "schickt", "uns", "aus", "dem", "Lan\u00b7de", "von", "Ford"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "einen ziemlich miesen Menschenexport:", "tokens": ["ei\u00b7nen", "ziem\u00b7lich", "mie\u00b7sen", "Men\u00b7schen\u00b7ex\u00b7port", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "\u00fcberschwemmt sind Paris und Griechenland", "tokens": ["\u00fc\u00b7bersc\u00b7hwemmt", "sind", "Pa\u00b7ris", "und", "Grie\u00b7chen\u00b7land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "NE", "KON", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "von euerm m\u00e4\u00dfigen Mittelstand.", "tokens": ["von", "eu\u00b7erm", "m\u00e4\u00b7\u00dfi\u00b7gen", "Mit\u00b7tel\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Diese Reisenden, laut und prahlerisch,", "tokens": ["Die\u00b7se", "Rei\u00b7sen\u00b7den", ",", "laut", "und", "prah\u00b7le\u00b7risch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "legen geistig die F\u00fc\u00dfe auf den Tisch,", "tokens": ["le\u00b7gen", "geis\u00b7tig", "die", "F\u00fc\u00b7\u00dfe", "auf", "den", "Tisch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "fallen l\u00e4stig an allen Orten;", "tokens": ["fal\u00b7len", "l\u00e4s\u00b7tig", "an", "al\u00b7len", "Or\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "PIAT", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "und jeder zweite Satz beginnt mit den Worten:", "tokens": ["und", "je\u00b7der", "zwei\u00b7te", "Satz", "be\u00b7ginnt", "mit", "den", "Wor\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "\u00bbbei uns in Amerika . . . \u00ab", "tokens": ["\u00bb", "bei", "uns", "in", "A\u00b7me\u00b7ri\u00b7ka", ".", ".", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "APPR", "PPER", "APPR", "NE", "$.", "$.", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.6": {"line.1": {"text": "Bei euch in Amerika gibts zweierlei Rechte", "tokens": ["Bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "gibts", "zwei\u00b7er\u00b7lei", "Rech\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "(f\u00fcr Arme und Reiche) \u2013 gibt es Gute und Schlechte;", "tokens": ["(", "f\u00fcr", "Ar\u00b7me", "und", "Rei\u00b7che", ")", "\u2013", "gibt", "es", "Gu\u00b7te", "und", "Schlech\u00b7te", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "KON", "NE", "$(", "$(", "VVFIN", "PPER", "NN", "KON", "NN", "$."], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "gibt es solche und solche: Lewis und Mencken,", "tokens": ["gibt", "es", "sol\u00b7che", "und", "sol\u00b7che", ":", "Le\u00b7wis", "und", "Men\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "KON", "PIAT", "$.", "NE", "KON", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "und Dollardiener, die in Dollars denken.", "tokens": ["und", "Dol\u00b7lar\u00b7die\u00b7ner", ",", "die", "in", "Dol\u00b7lars", "den\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Bei euch in Amerika gibt es Republikaner", "tokens": ["Bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "gibt", "es", "Re\u00b7pub\u00b7li\u00b7ka\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VVFIN", "PPER", "NN"], "meter": "-+-+---+--+-+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "und richtende blutige Puritaner.", "tokens": ["und", "rich\u00b7ten\u00b7de", "blu\u00b7ti\u00b7ge", "Pu\u00b7ri\u00b7ta\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Ihr habt Kraft, Jugend und Silberlinge \u2013", "tokens": ["Ihr", "habt", "Kraft", ",", "Ju\u00b7gend", "und", "Sil\u00b7ber\u00b7lin\u00b7ge", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "NN", "KON", "NN", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.8": {"text": "aber ihr seid nicht das Ma\u00df aller Dinge,", "tokens": ["a\u00b7ber", "ihr", "seid", "nicht", "das", "Ma\u00df", "al\u00b7ler", "Din\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PTKNEG", "ART", "NN", "PIAT", "NN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.9": {"text": "bei euch in Amerika.", "tokens": ["bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.7": {"line.1": {"text": "Bei uns in Europa ist das Weib", "tokens": ["Bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "ist", "das", "Weib"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "keine Haremsfrau ohne Unterleib \u2013", "tokens": ["kei\u00b7ne", "Ha\u00b7rems\u00b7frau", "oh\u00b7ne", "Un\u00b7ter\u00b7leib", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "NN", "$("], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "bei uns in Europa ist die schwarze Haut", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "ist", "die", "schwar\u00b7ze", "Haut"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "kein Aussatz, dem man Extra-Bahnwagen baut;", "tokens": ["kein", "Aus\u00b7satz", ",", "dem", "man", "Ex\u00b7tra\u00b7Bahn\u00b7wa\u00b7gen", "baut", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "PIS", "NN", "VVFIN", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}, "line.5": {"text": "bei uns in Europa kann wer ohne Geld sein", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "kann", "wer", "oh\u00b7ne", "Geld", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "APPR", "NE", "VMFIN", "PWS", "APPR", "NN", "PPOSAT"], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}, "line.6": {"text": "und dennoch, dennoch auf der Welt sein \u2013", "tokens": ["und", "den\u00b7noch", ",", "den\u00b7noch", "auf", "der", "Welt", "sein", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "APPR", "ART", "NN", "VAINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "bei uns in Europa kann man bestehn,", "tokens": ["bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "kann", "man", "be\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-++--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "ohne in die Sonntags-Schule zu gehn,", "tokens": ["oh\u00b7ne", "in", "die", "Sonn\u00b7tags\u00b7Schu\u00b7le", "zu", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.9": {"text": "weil fast keiner so am Altare steht:", "tokens": ["weil", "fast", "kei\u00b7ner", "so", "am", "Al\u00b7ta\u00b7re", "steht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIS", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+----+-+", "measure": "unknown.measure.tetra"}, "line.10": {"text": "eine pl\u00e4rrende n\u00fcchterne Realit\u00e4t \u2013", "tokens": ["ei\u00b7ne", "pl\u00e4r\u00b7ren\u00b7de", "n\u00fcch\u00b7ter\u00b7ne", "Re\u00b7a\u00b7li\u00b7t\u00e4t", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.11": {"text": "wie bei euch in Amerika.", "tokens": ["wie", "bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPER", "APPR", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "Das wissen nat\u00fcrlich bei euch die Guten", "tokens": ["Das", "wis\u00b7sen", "na\u00b7t\u00fcr\u00b7lich", "bei", "euch", "die", "Gu\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "ganz genau. Der Rest hat von Blasen und Tuten", "tokens": ["ganz", "ge\u00b7nau", ".", "Der", "Rest", "hat", "von", "Bla\u00b7sen", "und", "Tu\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "$.", "ART", "NN", "VAFIN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "keine Ahnung. H\u00f6rt nur den Schmeichelchor", "tokens": ["kei\u00b7ne", "Ah\u00b7nung", ".", "H\u00f6rt", "nur", "den", "Schmei\u00b7chelc\u00b7hor"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "$.", "VVFIN", "ADV", "ART", "NN"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "seiner news-papers; kommt sich so erstklassig vor . . .", "tokens": ["sei\u00b7ner", "news\u00b7\u00b7pa\u00b7pers", ";", "kommt", "sich", "so", "erst\u00b7klas\u00b7sig", "vor", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "$.", "VVFIN", "PRF", "ADV", "ADJD", "PTKVZ", "$.", "$.", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.5": {"text": "H\u00f6r nicht hin, Arbeitsmann. La\u00df sie ziehn,", "tokens": ["H\u00f6r", "nicht", "hin", ",", "Ar\u00b7beits\u00b7mann", ".", "La\u00df", "sie", "ziehn", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "PTKVZ", "$,", "NE", "$.", "VVIMP", "PPER", "VVINF", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "die Eitelkeiten der Bourgeoisien.", "tokens": ["die", "Ei\u00b7tel\u00b7kei\u00b7ten", "der", "Bour\u00b7ge\u00b7o\u00b7i\u00b7si\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+-+--+-++-+", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "P\u00e4sse, Fahnen und Paraden", "tokens": ["P\u00e4s\u00b7se", ",", "Fah\u00b7nen", "und", "Pa\u00b7ra\u00b7den"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "das sind l\u00e4cherliche Zementfassaden . . .", "tokens": ["das", "sind", "l\u00e4\u00b7cher\u00b7li\u00b7che", "Ze\u00b7ment\u00b7fas\u00b7sa\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PDS", "VAFIN", "ADJA", "NN", "$.", "$.", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Denn die wahre Grenze, zwischen Drohnen und Fronen,", "tokens": ["Denn", "die", "wah\u00b7re", "Gren\u00b7ze", ",", "zwi\u00b7schen", "Droh\u00b7nen", "und", "Fro\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+--+-", "measure": "hexameter"}, "line.10": {"text": "l\u00e4uft quer hindurch durch alle Nationen \u2013", "tokens": ["l\u00e4uft", "quer", "hin\u00b7durch", "durch", "al\u00b7le", "Na\u00b7ti\u00b7o\u00b7nen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PAV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "bei euch in Amerika.", "tokens": ["bei", "euch", "in", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.12": {"text": "Wie bei uns in Europa.", "tokens": ["Wie", "bei", "uns", "in", "Eu\u00b7ro\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPER", "APPR", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}