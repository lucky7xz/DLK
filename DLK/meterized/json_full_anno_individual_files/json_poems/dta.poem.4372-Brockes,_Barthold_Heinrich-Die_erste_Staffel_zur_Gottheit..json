{"dta.poem.4372": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Die erste Staffel zur Gottheit.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Ich, der mein Wesen selbst nicht kenne, befinde mich in", "tokens": ["Ich", ",", "der", "mein", "We\u00b7sen", "selbst", "nicht", "ken\u00b7ne", ",", "be\u00b7fin\u00b7de", "mich", "in"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "PPOSAT", "NN", "ADV", "PTKNEG", "VVFIN", "$,", "VVFIN", "PRF", "APPR"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "einer Welt,", "tokens": ["ei\u00b7ner", "Welt", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "In welcher ungez\u00e4hlte Sch\u00f6nheit, Pracht, Ordnung, Nutz", "tokens": ["In", "wel\u00b7cher", "un\u00b7ge\u00b7z\u00e4hl\u00b7te", "Sch\u00f6n\u00b7heit", ",", "Pracht", ",", "Ord\u00b7nung", ",", "Nutz"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "PWAT", "ADJA", "NN", "$,", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "und Lust vorhanden,", "tokens": ["und", "Lust", "vor\u00b7han\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJD", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Und welche nie zu z\u00e4hlnde Wunder in ihrem weiten Kreis", "tokens": ["Und", "wel\u00b7che", "nie", "zu", "z\u00e4hln\u00b7de", "Wun\u00b7der", "in", "ih\u00b7rem", "wei\u00b7ten", "Kreis"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PRELS", "ADV", "APPR", "ADJA", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "enth\u00e4lt.", "tokens": ["ent\u00b7h\u00e4lt", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "Ich find in meinem Geist die Wahrheit: Da\u00df sie nicht von", "tokens": ["Ich", "find", "in", "mei\u00b7nem", "Geist", "die", "Wahr\u00b7heit", ":", "Da\u00df", "sie", "nicht", "von"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "$.", "KOUS", "PPER", "PTKNEG", "APPR"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.8": {"text": "ihr selbst entstanden.", "tokens": ["ihr", "selbst", "ent\u00b7stan\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Ich bin, durch sie, denn \u00fcberf\u00fchret, und werd\u2019 es Anfangs", "tokens": ["Ich", "bin", ",", "durch", "sie", ",", "denn", "\u00fc\u00b7berf\u00b7\u00fch\u00b7ret", ",", "und", "werd'", "es", "An\u00b7fangs"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "APPR", "PPER", "$,", "KON", "VVFIN", "$,", "KON", "VAFIN", "PPER", "NN"], "meter": "-+---+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "ganz allein,", "tokens": ["ganz", "al\u00b7lein", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Durch diesen ewig wahren Schlu\u00df: Es mu\u00df ein GOtt,", "tokens": ["Durch", "die\u00b7sen", "e\u00b7wig", "wah\u00b7ren", "Schlu\u00df", ":", "Es", "mu\u00df", "ein", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJD", "ADJA", "NN", "$.", "PPER", "VMFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "ein Sch\u00f6pfer seyn.", "tokens": ["ein", "Sch\u00f6p\u00b7fer", "seyn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VAINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Wann nun von diesem Welt-Geb\u00e4ude die Wunder-reiche", "tokens": ["Wann", "nun", "von", "die\u00b7sem", "Welt\u00b7Ge\u00b7b\u00e4u\u00b7de", "die", "Wun\u00b7der\u00b7rei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "APPR", "PDAT", "NN", "ART", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Herrlichkeiten", "tokens": ["Herr\u00b7lich\u00b7kei\u00b7ten"], "token_info": ["word"], "pos": ["NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Die allerersten Stuffen sind, die uns zur wahren Gottheit", "tokens": ["Die", "al\u00b7le\u00b7rers\u00b7ten", "Stuf\u00b7fen", "sind", ",", "die", "uns", "zur", "wah\u00b7ren", "Got\u00b7theit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "$,", "PRELS", "PPER", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.4": {"text": "leiten;", "tokens": ["lei\u00b7ten", ";"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Wie kann man doch, sie aufzusteigen, sich wegern, sie nicht", "tokens": ["Wie", "kann", "man", "doch", ",", "sie", "auf\u00b7zu\u00b7stei\u00b7gen", ",", "sich", "we\u00b7gern", ",", "sie", "nicht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VMFIN", "PIS", "ADV", "$,", "PPER", "VVIZU", "$,", "PRF", "VVFIN", "$,", "PPER", "PTKNEG"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "einst betrachten?", "tokens": ["einst", "be\u00b7trach\u00b7ten", "?"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Mit welchem Fug kann man, in ihnen, Den, welcher sie", "tokens": ["Mit", "wel\u00b7chem", "Fug", "kann", "man", ",", "in", "ih\u00b7nen", ",", "Den", ",", "wel\u00b7cher", "sie"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["APPR", "PWAT", "NN", "VMFIN", "PIS", "$,", "APPR", "PPER", "$,", "NE", "$,", "PRELS", "PPER"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "gemacht, verachten?", "tokens": ["ge\u00b7macht", ",", "ver\u00b7ach\u00b7ten", "?"], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Gott zeigt uns selber diesen Weg, um, auf demselben,", "tokens": ["Gott", "zeigt", "uns", "sel\u00b7ber", "die\u00b7sen", "Weg", ",", "um", ",", "auf", "dem\u00b7sel\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "PDAT", "NN", "$,", "KOUI", "$,", "APPR", "PDAT", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Jhn zu finden,", "tokens": ["Jhn", "zu", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Und lehret den sonst leeren Geist, durch Sinnen, mit Sich", "tokens": ["Und", "leh\u00b7ret", "den", "sonst", "lee\u00b7ren", "Geist", ",", "durch", "Sin\u00b7nen", ",", "mit", "Sich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADV", "ADJA", "NN", "$,", "APPR", "NN", "$,", "APPR", "PRF"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "Selbst verbinden,", "tokens": ["Selbst", "ver\u00b7bin\u00b7den", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Sein Wesen, Seine G\u00fcte schmecken, und, uns zum Nutz,", "tokens": ["Sein", "We\u00b7sen", ",", "Sei\u00b7ne", "G\u00fc\u00b7te", "schme\u00b7cken", ",", "und", ",", "uns", "zum", "Nutz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "VVINF", "$,", "KON", "$,", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "und Jhm zum Preise,", "tokens": ["und", "Jhm", "zum", "Prei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Auf diese, als die w\u00fcrdigste, und, Jhn zu ehren, beste Weise,", "tokens": ["Auf", "die\u00b7se", ",", "als", "die", "w\u00fcr\u00b7digs\u00b7te", ",", "und", ",", "Jhn", "zu", "eh\u00b7ren", ",", "bes\u00b7te", "Wei\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PDAT", "$,", "KOUS", "ART", "ADJA", "$,", "KON", "$,", "PPER", "PTKZU", "VVINF", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}}, "stanza.4": {"line.1": {"text": "Gesch\u00f6pf und Sch\u00f6pfer wohl vereinen, da wir aus Leib", "tokens": ["Ge\u00b7sch\u00f6pf", "und", "Sch\u00f6p\u00b7fer", "wohl", "ver\u00b7ei\u00b7nen", ",", "da", "wir", "aus", "Leib"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "ADV", "VVINF", "$,", "KOUS", "PPER", "APPR", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "und Geist bestehn,", "tokens": ["und", "Geist", "be\u00b7stehn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Und um und an uns Creaturen, die ihren Sch\u00f6pfer zei-", "tokens": ["Und", "um", "und", "an", "uns", "Crea\u00b7tu\u00b7ren", ",", "die", "ih\u00b7ren", "Sch\u00f6p\u00b7fer", "zei"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "APPR", "KON", "APPR", "PPER", "NN", "$,", "PRELS", "PPOSAT", "NN", "TRUNC"], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "gen, sehn.", "tokens": ["gen", ",", "sehn", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "VVINF", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.5": {"line.1": {"text": "Wir aber w\u00e4hlen andre Wege, betreten eine fremde", "tokens": ["Wir", "a\u00b7ber", "w\u00e4h\u00b7len", "and\u00b7re", "We\u00b7ge", ",", "be\u00b7tre\u00b7ten", "ei\u00b7ne", "frem\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "VVINF", "ADJA", "NN", "$,", "VVFIN", "ART", "ADJA"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "Bahn,", "tokens": ["Bahn", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Wir sehen unsere Gedanken von GOtt, die wir uns selber", "tokens": ["Wir", "se\u00b7hen", "un\u00b7se\u00b7re", "Ge\u00b7dan\u00b7ken", "von", "Gott", ",", "die", "wir", "uns", "sel\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "$,", "PRELS", "PPER", "PRF", "ADV"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "weben,", "tokens": ["we\u00b7ben", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Ohn\u2019 auf Sein Seyn in Seinen Werken, wo Er Sich", "tokens": ["Ohn'", "auf", "Sein", "Seyn", "in", "Sei\u00b7nen", "Wer\u00b7ken", ",", "wo", "Er", "Sich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "$,", "PWAV", "PPER", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "zeiget, Acht zu geben,", "tokens": ["zei\u00b7get", ",", "Acht", "zu", "ge\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "CARD", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "(o selbst erzieltes G\u00f6tzen-Bild!) f\u00fcr eine wahre Gottheit", "tokens": ["(", "o", "selbst", "er\u00b7ziel\u00b7tes", "G\u00f6t\u00b7zen\u00b7Bild", "!", ")", "f\u00fcr", "ei\u00b7ne", "wah\u00b7re", "Got\u00b7theit"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["$(", "FM", "ADV", "ADJA", "NN", "$.", "$(", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.8": {"text": "an.", "tokens": ["an", "."], "token_info": ["word", "punct"], "pos": ["PTKVZ", "$."], "meter": "+", "measure": "single.up"}, "line.9": {"text": "Wir eignen ihr verschiedne Kr\u00e4fte, die doch nur in uns", "tokens": ["Wir", "eig\u00b7nen", "ihr", "ver\u00b7schied\u00b7ne", "Kr\u00e4f\u00b7te", ",", "die", "doch", "nur", "in", "uns"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "PPOSAT", "ADJA", "NN", "$,", "PRELS", "ADV", "ADV", "APPR", "PPER"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.10": {"text": "selber haften,", "tokens": ["sel\u00b7ber", "haf\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Und, nur in etwas h\u00f6herm Grad, der Menschheit eigne", "tokens": ["Und", ",", "nur", "in", "et\u00b7was", "h\u00f6\u00b7herm", "Grad", ",", "der", "Menschheit", "eig\u00b7ne"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "ADV", "APPR", "PIAT", "ADJA", "NN", "$,", "ART", "NN", "ADJA"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Leidenschaften,", "tokens": ["Lei\u00b7den\u00b7schaf\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.13": {"text": "Die aus uns selber quillen, zu. Wir schneiden, blo\u00df", "tokens": ["Die", "aus", "uns", "sel\u00b7ber", "quil\u00b7len", ",", "zu", ".", "Wir", "schnei\u00b7den", ",", "blo\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word"], "pos": ["ART", "APPR", "PPER", "ADV", "VVINF", "$,", "PTKVZ", "$.", "PPER", "VVFIN", "$,", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "nach unserm Leiste,", "tokens": ["nach", "un\u00b7serm", "Leis\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.15": {"text": "Uns einen GOtt in unserm Hirn, nach unserm eignen", "tokens": ["Uns", "ei\u00b7nen", "Gott", "in", "un\u00b7serm", "Hirn", ",", "nach", "un\u00b7serm", "eig\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "$,", "APPR", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Leib und Geiste.", "tokens": ["Leib", "und", "Geis\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.17": {"text": "Wir legen nicht nur unsre C\u00f6rper, in eines Greisen Bild,", "tokens": ["Wir", "le\u00b7gen", "nicht", "nur", "uns\u00b7re", "C\u00f6r\u00b7per", ",", "in", "ei\u00b7nes", "Grei\u00b7sen", "Bild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "PPOSAT", "NN", "$,", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.18": {"text": "Jhm bey;", "tokens": ["Jhm", "bey", ";"], "token_info": ["word", "word", "punct"], "pos": ["PPER", "PTKVZ", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.19": {"text": "Wir meynen, wenn Er denkt, wie wir, da\u00df Er sodann", "tokens": ["Wir", "mey\u00b7nen", ",", "wenn", "Er", "denkt", ",", "wie", "wir", ",", "da\u00df", "Er", "so\u00b7dann"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,", "PWAV", "PPER", "$,", "KOUS", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "vern\u00fcnftig sey.", "tokens": ["ver\u00b7n\u00fcnf\u00b7tig", "sey", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VAFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.21": {"text": "Wir unternehmen uns zu sagen, durch richtig- und gef\u00fcgte", "tokens": ["Wir", "un\u00b7ter\u00b7neh\u00b7men", "uns", "zu", "sa\u00b7gen", ",", "durch", "rich\u00b7tig", "und", "ge\u00b7f\u00fcg\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PTKZU", "VVINF", "$,", "APPR", "TRUNC", "KON", "ADJA"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.22": {"text": "Schl\u00fcsse,", "tokens": ["Schl\u00fcs\u00b7se", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.23": {"text": "Da\u00df es, nach ihrem Sinn, die Gottheit so und nicht anders", "tokens": ["Da\u00df", "es", ",", "nach", "ih\u00b7rem", "Sinn", ",", "die", "Got\u00b7theit", "so", "und", "nicht", "an\u00b7ders"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "APPR", "PPOSAT", "NN", "$,", "ART", "NN", "ADV", "KON", "PTKNEG", "ADV"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.24": {"text": "machen m\u00fcsse.", "tokens": ["ma\u00b7chen", "m\u00fcs\u00b7se", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Wirfst du mir hier vielleicht nun ein: \u201cMein Freund!", "tokens": ["Wirfst", "du", "mir", "hier", "viel\u00b7leicht", "nun", "ein", ":", "\u201c", "Mein", "Freund", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "ADV", "PTKVZ", "$.", "$(", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "du \u00fcbereilest dich,", "tokens": ["du", "\u00fc\u00b7be\u00b7rei\u00b7lest", "dich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u201edu thust bey deiner eignen Seele nicht wohl, sie so", "tokens": ["\u201e", "du", "thust", "bey", "dei\u00b7ner", "eig\u00b7nen", "See\u00b7le", "nicht", "wohl", ",", "sie", "so"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["$(", "PPER", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "PTKNEG", "ADV", "$,", "PPER", "ADV"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "vermessentlich", "tokens": ["ver\u00b7mes\u00b7sent\u00b7lich"], "token_info": ["word"], "pos": ["ADJD"], "meter": "-+--", "measure": "dactylic.init"}, "line.5": {"text": "\u201eso klein zu machen, zu verachten. Bist du von ihr nicht", "tokens": ["\u201e", "so", "klein", "zu", "ma\u00b7chen", ",", "zu", "ver\u00b7ach\u00b7ten", ".", "Bist", "du", "von", "ihr", "nicht"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "ADJD", "PTKZU", "VVINF", "$,", "PTKZU", "VVFIN", "$.", "VAFIN", "PPER", "APPR", "PPER", "PTKNEG"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.6": {"text": "\u00fcberf\u00fchrt,", "tokens": ["\u00fc\u00b7berf\u00b7\u00fchrt", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "\u201esie stamme selbst vom Sch\u00f6pfer her, Der auch sogar den", "tokens": ["\u201e", "sie", "stam\u00b7me", "selbst", "vom", "Sch\u00f6p\u00b7fer", "her", ",", "Der", "auch", "so\u00b7gar", "den"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VVFIN", "ADV", "APPRART", "NN", "PTKVZ", "$,", "ART", "ADV", "ADV", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Leib formiert", "tokens": ["Leib", "for\u00b7miert"], "token_info": ["word", "word"], "pos": ["NN", "VVPP"], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "\u201ein Adams Leib, nach Seinem Bilde, und Der den Odem,", "tokens": ["\u201e", "in", "A\u00b7dams", "Leib", ",", "nach", "Sei\u00b7nem", "Bil\u00b7de", ",", "und", "Der", "den", "O\u00b7dem", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NE", "NN", "$,", "APPR", "PPOSAT", "NN", "$,", "KON", "ART", "ART", "NN", "$,"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "voller Leben,", "tokens": ["vol\u00b7ler", "Le\u00b7ben", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "\u201ein Adams Nase Selber blies; so ist die Antwort leicht", "tokens": ["\u201e", "in", "A\u00b7dams", "Na\u00b7se", "Sel\u00b7ber", "blies", ";", "so", "ist", "die", "Ant\u00b7wort", "leicht"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "APPR", "NE", "NE", "NE", "VVFIN", "$.", "ADV", "VAFIN", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.12": {"text": "gegeben:", "tokens": ["ge\u00b7ge\u00b7ben", ":"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.7": {"line.1": {"text": "Ist dieses nach dem Wort-Verstande, und nicht fig\u00fcr-", "tokens": ["Ist", "die\u00b7ses", "nach", "dem", "Wor\u00b7tVer\u00b7stan\u00b7de", ",", "und", "nicht", "fi\u00b7g\u00fcr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PDS", "APPR", "ART", "NN", "$,", "KON", "PTKNEG", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "lich zu verstehn;", "tokens": ["lich", "zu", "ver\u00b7stehn", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "So wei\u00df ich nicht, wie solche Seele sich jemahls kann ver-", "tokens": ["So", "wei\u00df", "ich", "nicht", ",", "wie", "sol\u00b7che", "See\u00b7le", "sich", "je\u00b7mahls", "kann", "ver"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "$,", "PWAV", "PIAT", "NN", "PRF", "ADV", "VMFIN", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "dammet sehn,", "tokens": ["dam\u00b7met", "sehn", ","], "token_info": ["word", "word", "punct"], "pos": ["PAV", "VVINF", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Die mit der Gottheit selbst verwandt, wie du ja schreibest,", "tokens": ["Die", "mit", "der", "Got\u00b7theit", "selbst", "ver\u00b7wandt", ",", "wie", "du", "ja", "schrei\u00b7best", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADV", "VVPP", "$,", "PWAV", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "glaubest, lehrest,", "tokens": ["glau\u00b7best", ",", "leh\u00b7rest", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Und an so vielen tausend Stellen die Schrift, nach diesem", "tokens": ["Und", "an", "so", "vie\u00b7len", "tau\u00b7send", "Stel\u00b7len", "die", "Schrift", ",", "nach", "die\u00b7sem"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPR", "ADV", "PIAT", "CARD", "NN", "ART", "NN", "$,", "APPR", "PDAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Sinn, erkl\u00e4rest.", "tokens": ["Sinn", ",", "er\u00b7kl\u00e4\u00b7rest", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "So wirst du ja, aus diesem Satz, die wirkliche Fig\u00fcrlichkeit,", "tokens": ["So", "wirst", "du", "ja", ",", "aus", "die\u00b7sem", "Satz", ",", "die", "wirk\u00b7li\u00b7che", "Fi\u00b7g\u00fcr\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "$,", "APPR", "PDAT", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.10": {"text": "Jm Ausdruck, von dem Odem sehn, und folglich aus dem-", "tokens": ["Jm", "Aus\u00b7druck", ",", "von", "dem", "O\u00b7dem", "sehn", ",", "und", "folg\u00b7lich", "aus", "dem"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "$,", "APPR", "ART", "NN", "VVINF", "$,", "KON", "ADV", "APPR", "TRUNC"], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.11": {"text": "selben schliessen:", "tokens": ["sel\u00b7ben", "schlies\u00b7sen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.12": {"text": "Es sey der Geist des Menschen nicht von g\u00f6ttlicher Be-", "tokens": ["Es", "sey", "der", "Geist", "des", "Men\u00b7schen", "nicht", "von", "g\u00f6tt\u00b7li\u00b7cher", "Be"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "ART", "NN", "PTKNEG", "APPR", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.13": {"text": "schaffenheit,", "tokens": ["schaf\u00b7fen\u00b7heit", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.14": {"text": "Und, da\u00df er folglich GOtt nicht fassen, noch bilden kann,", "tokens": ["Und", ",", "da\u00df", "er", "folg\u00b7lich", "Gott", "nicht", "fas\u00b7sen", ",", "noch", "bil\u00b7den", "kann", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "ADV", "NN", "PTKNEG", "VVINF", "$,", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "gestehen m\u00fcssen.", "tokens": ["ge\u00b7ste\u00b7hen", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Die\u00df alles aber hindert nicht, da\u00df GOtt nicht eine rege", "tokens": ["Die\u00df", "al\u00b7les", "a\u00b7ber", "hin\u00b7dert", "nicht", ",", "da\u00df", "Gott", "nicht", "ei\u00b7ne", "re\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "ADV", "VVFIN", "PTKNEG", "$,", "KOUS", "NN", "PTKNEG", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.2": {"text": "Kraft", "tokens": ["Kraft"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Der Menschen Seelen anerschaffen, und Er ihr eine Ei-", "tokens": ["Der", "Men\u00b7schen", "See\u00b7len", "a\u00b7ner\u00b7schaf\u00b7fen", ",", "und", "Er", "ihr", "ei\u00b7ne", "Ei"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VVINF", "$,", "KON", "PPER", "PPER", "ART", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "genschaft,", "tokens": ["gen\u00b7schaft", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+", "measure": "iambic.single"}}, "stanza.9": {"line.1": {"text": "(jedoch nach Seiner weisen Ordnung) Jhn immer mehr", "tokens": ["(", "je\u00b7doch", "nach", "Sei\u00b7ner", "wei\u00b7sen", "Ord\u00b7nung", ")", "Jhn", "im\u00b7mer", "mehr"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ADV", "APPR", "PPOSAT", "ADJA", "NN", "$(", "PPER", "ADV", "ADV"], "meter": "---+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "und mehr zu kennen", "tokens": ["und", "mehr", "zu", "ken\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Aus Seinen wunderbaren Werken, nicht h\u00e4tte sollen", "tokens": ["Aus", "Sei\u00b7nen", "wun\u00b7der\u00b7ba\u00b7ren", "Wer\u00b7ken", ",", "nicht", "h\u00e4t\u00b7te", "sol\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,", "PTKNEG", "VAFIN", "PIAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "geben k\u00f6nnen.", "tokens": ["ge\u00b7ben", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Aufs wenigste wird die Vernunft unwidersprechlich die\u00df", "tokens": ["Aufs", "we\u00b7nigs\u00b7te", "wird", "die", "Ver\u00b7nunft", "un\u00b7wi\u00b7der\u00b7sprech\u00b7lich", "die\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "PIS", "VAFIN", "ART", "NN", "ADJD", "PDS"], "meter": "-+--+--+-+-+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "dir zeigen:", "tokens": ["dir", "zei\u00b7gen", ":"], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Die Werke seyn die erste Staffel, auf welche wir zur", "tokens": ["Die", "Wer\u00b7ke", "seyn", "die", "ers\u00b7te", "Staf\u00b7fel", ",", "auf", "wel\u00b7che", "wir", "zur"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "ART", "ADJA", "NN", "$,", "APPR", "PRELS", "PPER", "APPRART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Gottheit steigen.", "tokens": ["Got\u00b7theit", "stei\u00b7gen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}}}}