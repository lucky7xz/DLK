{"textgrid.poem.60730": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein recht verschmitzter Fuchs, doch noch ein ganzes Kind,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein recht verschmitzter Fuchs, doch noch ein ganzes Kind,", "tokens": ["Ein", "recht", "ver\u00b7schmitz\u00b7ter", "Fuchs", ",", "doch", "noch", "ein", "gan\u00b7zes", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NE", "$,", "ADV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Sah einst das erste Pferd in seinem Leben.", "tokens": ["Sah", "einst", "das", "ers\u00b7te", "Pferd", "in", "sei\u00b7nem", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zum n\u00e4chsten Wolfe lief er da geschwind,", "tokens": ["Zum", "n\u00e4chs\u00b7ten", "Wol\u00b7fe", "lief", "er", "da", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dem auch nicht viel Erfahrung noch gegeben:", "tokens": ["Dem", "auch", "nicht", "viel", "Er\u00b7fah\u00b7rung", "noch", "ge\u00b7ge\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PTKNEG", "PIAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "\u00bbfreund, eile, denn ich sah soeben", "tokens": ["\u00bb", "freund", ",", "ei\u00b7le", ",", "denn", "ich", "sah", "soe\u00b7ben"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$,", "VVFIN", "$,", "KON", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Ein Tier auf unsrer nahen Wiese springen,", "tokens": ["Ein", "Tier", "auf", "uns\u00b7rer", "na\u00b7hen", "Wie\u00b7se", "sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "So gro\u00df und sch\u00f6n, da\u00df es mich ganz entz\u00fcckt.\u00ab", "tokens": ["So", "gro\u00df", "und", "sch\u00f6n", ",", "da\u00df", "es", "mich", "ganz", "ent\u00b7z\u00fcckt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$,", "KOUS", "PPER", "PRF", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "\u00bbist es so stark, uns beide zu bezwingen?", "tokens": ["\u00bb", "ist", "es", "so", "stark", ",", "uns", "bei\u00b7de", "zu", "be\u00b7zwin\u00b7gen", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ADV", "ADJD", "$,", "PPER", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Beschreibe mir das Bild, das dich ber\u00fcckt,\u00ab", "tokens": ["Be\u00b7schrei\u00b7be", "mir", "das", "Bild", ",", "das", "dich", "be\u00b7r\u00fcckt", ",", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Sprach da der Wolf voll \u00dcberlegenheit.", "tokens": ["Sprach", "da", "der", "Wolf", "voll", "\u00dc\u00b7berl\u00b7e\u00b7gen\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NE", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "\u00bbw\u00e4r ich ein Maler oder ein Student,", "tokens": ["\u00bb", "w\u00e4r", "ich", "ein", "Ma\u00b7ler", "o\u00b7der", "ein", "Stu\u00b7dent", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.11": {"text": "Der gut die Kunst der sch\u00f6nen Rede kennt,", "tokens": ["Der", "gut", "die", "Kunst", "der", "sch\u00f6\u00b7nen", "Re\u00b7de", "kennt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "So w\u00e4re das mir eine Kleinigkeit,\u00ab", "tokens": ["So", "w\u00e4\u00b7re", "das", "mir", "ei\u00b7ne", "Klei\u00b7nig\u00b7keit", ",", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "ART", "PPER", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Entgegnete der Fuchs, \u00bbund leicht sollt es gelingen,", "tokens": ["Ent\u00b7geg\u00b7ne\u00b7te", "der", "Fuchs", ",", "\u00bb", "und", "leicht", "sollt", "es", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "$,", "$(", "KON", "ADJD", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Dir jetzt schon das Entz\u00fccken beizubringen,", "tokens": ["Dir", "jetzt", "schon", "das", "Ent\u00b7z\u00fc\u00b7cken", "bei\u00b7zu\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Das, siehst du erst das Tier, wie mich dich brennt.", "tokens": ["Das", ",", "siehst", "du", "erst", "das", "Tier", ",", "wie", "mich", "dich", "brennt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "VVFIN", "PPER", "ADV", "ART", "NN", "$,", "PWAV", "PPER", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Doch komm \u2013 wer wei\u00df, vielleicht begl\u00fcckt uns heute", "tokens": ["Doch", "komm", "\u2013", "wer", "wei\u00df", ",", "viel\u00b7leicht", "be\u00b7gl\u00fcckt", "uns", "heu\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "$(", "PWS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.17": {"text": "Fortuna mit ganz auserlesner Beute.\u00ab", "tokens": ["For\u00b7tu\u00b7na", "mit", "ganz", "au\u00b7ser\u00b7les\u00b7ner", "Beu\u00b7te", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "ADV", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.18": {"text": "Sie gehn. Das Pferd, als es die beiden sah,", "tokens": ["Sie", "gehn", ".", "Das", "Pferd", ",", "als", "es", "die", "bei\u00b7den", "sah", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$.", "ART", "NN", "$,", "KOUS", "PPER", "ART", "PIAT", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "War weniger entz\u00fcckt und w\u00e4re gern entflohn,", "tokens": ["War", "we\u00b7ni\u00b7ger", "ent\u00b7z\u00fcckt", "und", "w\u00e4\u00b7re", "gern", "ent\u00b7flohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "KON", "VAFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Doch unser Fuchs begr\u00fc\u00dft es schon.", "tokens": ["Doch", "un\u00b7ser", "Fuchs", "be\u00b7gr\u00fc\u00dft", "es", "schon", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NE", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "\u00bbverehrter Herr,\u00ab sprach er und trat recht nah,", "tokens": ["\u00bb", "ver\u00b7ehr\u00b7ter", "Herr", ",", "\u00ab", "sprach", "er", "und", "trat", "recht", "nah", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "$(", "VVFIN", "PPER", "KON", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.22": {"text": "\u00bbseht her, wir stehn in Demut da,", "tokens": ["\u00bb", "seht", "her", ",", "wir", "stehn", "in", "De\u00b7mut", "da", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Um Euer Gnaden zu befragen,", "tokens": ["Um", "Eu\u00b7er", "Gna\u00b7den", "zu", "be\u00b7fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Welch edlen Namen Sie wohl tragen?\u00ab", "tokens": ["Welch", "ed\u00b7len", "Na\u00b7men", "Sie", "wohl", "tra\u00b7gen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIAT", "ADJA", "NN", "PPER", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Das Pferd indes war auch nicht dumm:", "tokens": ["Das", "Pferd", "in\u00b7des", "war", "auch", "nicht", "dumm", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.26": {"text": "\u00bbso lest ihn selbst, ihr Herrn, kommt her,", "tokens": ["\u00bb", "so", "lest", "ihn", "selbst", ",", "ihr", "Herrn", ",", "kommt", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.27": {"text": "Ihn zu entziffern ist nicht schwer,", "tokens": ["Ihn", "zu", "ent\u00b7zif\u00b7fern", "ist", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.28": {"text": "Mein Schuster schrieb ihn um den Huf herum.\u00ab", "tokens": ["Mein", "Schus\u00b7ter", "schrieb", "ihn", "um", "den", "Huf", "he\u00b7rum", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.29": {"text": "Der Fuchs darauf mit ganz verlegnem Stammeln:", "tokens": ["Der", "Fuchs", "da\u00b7rauf", "mit", "ganz", "ver\u00b7leg\u00b7nem", "Stam\u00b7meln", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "PAV", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.30": {"text": "\u00bbich wu\u00dfte keine Kenntnisse zu sammeln,", "tokens": ["\u00bb", "ich", "wu\u00df\u00b7te", "kei\u00b7ne", "Kennt\u00b7nis\u00b7se", "zu", "sam\u00b7meln", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.31": {"text": "Ich kann nicht lesen, hatte weder Geld noch Zeit;", "tokens": ["Ich", "kann", "nicht", "le\u00b7sen", ",", "hat\u00b7te", "we\u00b7der", "Geld", "noch", "Zeit", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VAFIN", "KON", "NN", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Bei meinen Eltern herrschte Not und Sparsamkeit,", "tokens": ["Bei", "mei\u00b7nen", "El\u00b7tern", "herrschte", "Not", "und", "Spar\u00b7sam\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.33": {"text": "Ein Loch zum Wohnen war die ganze Habe \u2013", "tokens": ["Ein", "Loch", "zum", "Woh\u00b7nen", "war", "die", "gan\u00b7ze", "Ha\u00b7be", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.34": {"text": "Jedoch der reiche Wolf besitzt die Gabe.\u00ab", "tokens": ["Je\u00b7doch", "der", "rei\u00b7che", "Wolf", "be\u00b7sitzt", "die", "Ga\u00b7be", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ART", "ADJA", "NE", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.35": {"text": "Der Wolf in seiner Eitelkeit", "tokens": ["Der", "Wolf", "in", "sei\u00b7ner", "Ei\u00b7tel\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NE", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.36": {"text": "Trat dicht herzu, die Schrift zu lesen.", "tokens": ["Trat", "dicht", "her\u00b7zu", ",", "die", "Schrift", "zu", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADV", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.37": {"text": "W\u00e4r er doch lieber nicht so dreist gewesen!", "tokens": ["W\u00e4r", "er", "doch", "lie\u00b7ber", "nicht", "so", "dreist", "ge\u00b7we\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "ADV", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.38": {"text": "Denn mit vier Z\u00e4hnen mu\u00dfte er's bezahlen", "tokens": ["Denn", "mit", "vier", "Z\u00e4h\u00b7nen", "mu\u00df\u00b7te", "er's", "be\u00b7zah\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "CARD", "NN", "VMFIN", "PIS", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.39": {"text": "Und mit noch andern Wundenmalen.", "tokens": ["Und", "mit", "noch", "an\u00b7dern", "Wun\u00b7den\u00b7ma\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.40": {"text": "\u00bbgevatter,\u00ab sprach der Fuchs, \u00bbdein Fall beweist,", "tokens": ["\u00bb", "ge\u00b7vat\u00b7ter", ",", "\u00ab", "sprach", "der", "Fuchs", ",", "\u00bb", "dein", "Fall", "be\u00b7weist", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "$(", "VVFIN", "ART", "NE", "$,", "$(", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.41": {"text": "Da\u00df es mit Recht im Sprichwort hei\u00dft:", "tokens": ["Da\u00df", "es", "mit", "Recht", "im", "Sprich\u00b7wort", "hei\u00dft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.42": {"text": "\u203ader Kluge meide das, was er nicht kennt\u2039 \u2013", "tokens": ["\u203a", "der", "Klu\u00b7ge", "mei\u00b7de", "das", ",", "was", "er", "nicht", "kennt", "\u2039", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "PDS", "$,", "PWS", "PPER", "PTKNEG", "VVFIN", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.43": {"text": "Befolgtest du's, so w\u00e4rst du jetzt nicht Patient.\u00ab", "tokens": ["Be\u00b7folg\u00b7test", "du's", ",", "so", "w\u00e4rst", "du", "jetzt", "nicht", "Pa\u00b7ti\u00b7ent", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PIS", "$,", "ADV", "VAFIN", "PPER", "ADV", "PTKNEG", "NN", "$.", "$("], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}, "stanza.3": {"line.1": {"text": "Ein recht verschmitzter Fuchs, doch noch ein ganzes Kind,", "tokens": ["Ein", "recht", "ver\u00b7schmitz\u00b7ter", "Fuchs", ",", "doch", "noch", "ein", "gan\u00b7zes", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NE", "$,", "ADV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Sah einst das erste Pferd in seinem Leben.", "tokens": ["Sah", "einst", "das", "ers\u00b7te", "Pferd", "in", "sei\u00b7nem", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zum n\u00e4chsten Wolfe lief er da geschwind,", "tokens": ["Zum", "n\u00e4chs\u00b7ten", "Wol\u00b7fe", "lief", "er", "da", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dem auch nicht viel Erfahrung noch gegeben:", "tokens": ["Dem", "auch", "nicht", "viel", "Er\u00b7fah\u00b7rung", "noch", "ge\u00b7ge\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PTKNEG", "PIAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "\u00bbfreund, eile, denn ich sah soeben", "tokens": ["\u00bb", "freund", ",", "ei\u00b7le", ",", "denn", "ich", "sah", "soe\u00b7ben"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$,", "VVFIN", "$,", "KON", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Ein Tier auf unsrer nahen Wiese springen,", "tokens": ["Ein", "Tier", "auf", "uns\u00b7rer", "na\u00b7hen", "Wie\u00b7se", "sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "So gro\u00df und sch\u00f6n, da\u00df es mich ganz entz\u00fcckt.\u00ab", "tokens": ["So", "gro\u00df", "und", "sch\u00f6n", ",", "da\u00df", "es", "mich", "ganz", "ent\u00b7z\u00fcckt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$,", "KOUS", "PPER", "PRF", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "\u00bbist es so stark, uns beide zu bezwingen?", "tokens": ["\u00bb", "ist", "es", "so", "stark", ",", "uns", "bei\u00b7de", "zu", "be\u00b7zwin\u00b7gen", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ADV", "ADJD", "$,", "PPER", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Beschreibe mir das Bild, das dich ber\u00fcckt,\u00ab", "tokens": ["Be\u00b7schrei\u00b7be", "mir", "das", "Bild", ",", "das", "dich", "be\u00b7r\u00fcckt", ",", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Sprach da der Wolf voll \u00dcberlegenheit.", "tokens": ["Sprach", "da", "der", "Wolf", "voll", "\u00dc\u00b7berl\u00b7e\u00b7gen\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NE", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "\u00bbw\u00e4r ich ein Maler oder ein Student,", "tokens": ["\u00bb", "w\u00e4r", "ich", "ein", "Ma\u00b7ler", "o\u00b7der", "ein", "Stu\u00b7dent", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.11": {"text": "Der gut die Kunst der sch\u00f6nen Rede kennt,", "tokens": ["Der", "gut", "die", "Kunst", "der", "sch\u00f6\u00b7nen", "Re\u00b7de", "kennt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "So w\u00e4re das mir eine Kleinigkeit,\u00ab", "tokens": ["So", "w\u00e4\u00b7re", "das", "mir", "ei\u00b7ne", "Klei\u00b7nig\u00b7keit", ",", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "ART", "PPER", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Entgegnete der Fuchs, \u00bbund leicht sollt es gelingen,", "tokens": ["Ent\u00b7geg\u00b7ne\u00b7te", "der", "Fuchs", ",", "\u00bb", "und", "leicht", "sollt", "es", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "$,", "$(", "KON", "ADJD", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Dir jetzt schon das Entz\u00fccken beizubringen,", "tokens": ["Dir", "jetzt", "schon", "das", "Ent\u00b7z\u00fc\u00b7cken", "bei\u00b7zu\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Das, siehst du erst das Tier, wie mich dich brennt.", "tokens": ["Das", ",", "siehst", "du", "erst", "das", "Tier", ",", "wie", "mich", "dich", "brennt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "VVFIN", "PPER", "ADV", "ART", "NN", "$,", "PWAV", "PPER", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Doch komm \u2013 wer wei\u00df, vielleicht begl\u00fcckt uns heute", "tokens": ["Doch", "komm", "\u2013", "wer", "wei\u00df", ",", "viel\u00b7leicht", "be\u00b7gl\u00fcckt", "uns", "heu\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "$(", "PWS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.17": {"text": "Fortuna mit ganz auserlesner Beute.\u00ab", "tokens": ["For\u00b7tu\u00b7na", "mit", "ganz", "au\u00b7ser\u00b7les\u00b7ner", "Beu\u00b7te", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "ADV", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.18": {"text": "Sie gehn. Das Pferd, als es die beiden sah,", "tokens": ["Sie", "gehn", ".", "Das", "Pferd", ",", "als", "es", "die", "bei\u00b7den", "sah", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$.", "ART", "NN", "$,", "KOUS", "PPER", "ART", "PIAT", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "War weniger entz\u00fcckt und w\u00e4re gern entflohn,", "tokens": ["War", "we\u00b7ni\u00b7ger", "ent\u00b7z\u00fcckt", "und", "w\u00e4\u00b7re", "gern", "ent\u00b7flohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "KON", "VAFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Doch unser Fuchs begr\u00fc\u00dft es schon.", "tokens": ["Doch", "un\u00b7ser", "Fuchs", "be\u00b7gr\u00fc\u00dft", "es", "schon", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NE", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "\u00bbverehrter Herr,\u00ab sprach er und trat recht nah,", "tokens": ["\u00bb", "ver\u00b7ehr\u00b7ter", "Herr", ",", "\u00ab", "sprach", "er", "und", "trat", "recht", "nah", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "$(", "VVFIN", "PPER", "KON", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.22": {"text": "\u00bbseht her, wir stehn in Demut da,", "tokens": ["\u00bb", "seht", "her", ",", "wir", "stehn", "in", "De\u00b7mut", "da", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Um Euer Gnaden zu befragen,", "tokens": ["Um", "Eu\u00b7er", "Gna\u00b7den", "zu", "be\u00b7fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Welch edlen Namen Sie wohl tragen?\u00ab", "tokens": ["Welch", "ed\u00b7len", "Na\u00b7men", "Sie", "wohl", "tra\u00b7gen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIAT", "ADJA", "NN", "PPER", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Das Pferd indes war auch nicht dumm:", "tokens": ["Das", "Pferd", "in\u00b7des", "war", "auch", "nicht", "dumm", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.26": {"text": "\u00bbso lest ihn selbst, ihr Herrn, kommt her,", "tokens": ["\u00bb", "so", "lest", "ihn", "selbst", ",", "ihr", "Herrn", ",", "kommt", "her", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.27": {"text": "Ihn zu entziffern ist nicht schwer,", "tokens": ["Ihn", "zu", "ent\u00b7zif\u00b7fern", "ist", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.28": {"text": "Mein Schuster schrieb ihn um den Huf herum.\u00ab", "tokens": ["Mein", "Schus\u00b7ter", "schrieb", "ihn", "um", "den", "Huf", "he\u00b7rum", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.29": {"text": "Der Fuchs darauf mit ganz verlegnem Stammeln:", "tokens": ["Der", "Fuchs", "da\u00b7rauf", "mit", "ganz", "ver\u00b7leg\u00b7nem", "Stam\u00b7meln", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "PAV", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.30": {"text": "\u00bbich wu\u00dfte keine Kenntnisse zu sammeln,", "tokens": ["\u00bb", "ich", "wu\u00df\u00b7te", "kei\u00b7ne", "Kennt\u00b7nis\u00b7se", "zu", "sam\u00b7meln", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.31": {"text": "Ich kann nicht lesen, hatte weder Geld noch Zeit;", "tokens": ["Ich", "kann", "nicht", "le\u00b7sen", ",", "hat\u00b7te", "we\u00b7der", "Geld", "noch", "Zeit", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "VAFIN", "KON", "NN", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Bei meinen Eltern herrschte Not und Sparsamkeit,", "tokens": ["Bei", "mei\u00b7nen", "El\u00b7tern", "herrschte", "Not", "und", "Spar\u00b7sam\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.33": {"text": "Ein Loch zum Wohnen war die ganze Habe \u2013", "tokens": ["Ein", "Loch", "zum", "Woh\u00b7nen", "war", "die", "gan\u00b7ze", "Ha\u00b7be", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.34": {"text": "Jedoch der reiche Wolf besitzt die Gabe.\u00ab", "tokens": ["Je\u00b7doch", "der", "rei\u00b7che", "Wolf", "be\u00b7sitzt", "die", "Ga\u00b7be", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ART", "ADJA", "NE", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.35": {"text": "Der Wolf in seiner Eitelkeit", "tokens": ["Der", "Wolf", "in", "sei\u00b7ner", "Ei\u00b7tel\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NE", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.36": {"text": "Trat dicht herzu, die Schrift zu lesen.", "tokens": ["Trat", "dicht", "her\u00b7zu", ",", "die", "Schrift", "zu", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADV", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.37": {"text": "W\u00e4r er doch lieber nicht so dreist gewesen!", "tokens": ["W\u00e4r", "er", "doch", "lie\u00b7ber", "nicht", "so", "dreist", "ge\u00b7we\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "ADV", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.38": {"text": "Denn mit vier Z\u00e4hnen mu\u00dfte er's bezahlen", "tokens": ["Denn", "mit", "vier", "Z\u00e4h\u00b7nen", "mu\u00df\u00b7te", "er's", "be\u00b7zah\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "CARD", "NN", "VMFIN", "PIS", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.39": {"text": "Und mit noch andern Wundenmalen.", "tokens": ["Und", "mit", "noch", "an\u00b7dern", "Wun\u00b7den\u00b7ma\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.40": {"text": "\u00bbgevatter,\u00ab sprach der Fuchs, \u00bbdein Fall beweist,", "tokens": ["\u00bb", "ge\u00b7vat\u00b7ter", ",", "\u00ab", "sprach", "der", "Fuchs", ",", "\u00bb", "dein", "Fall", "be\u00b7weist", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "$(", "VVFIN", "ART", "NE", "$,", "$(", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.41": {"text": "Da\u00df es mit Recht im Sprichwort hei\u00dft:", "tokens": ["Da\u00df", "es", "mit", "Recht", "im", "Sprich\u00b7wort", "hei\u00dft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.42": {"text": "\u203ader Kluge meide das, was er nicht kennt\u2039 \u2013", "tokens": ["\u203a", "der", "Klu\u00b7ge", "mei\u00b7de", "das", ",", "was", "er", "nicht", "kennt", "\u2039", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "PDS", "$,", "PWS", "PPER", "PTKNEG", "VVFIN", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.43": {"text": "Befolgtest du's, so w\u00e4rst du jetzt nicht Patient.\u00ab", "tokens": ["Be\u00b7folg\u00b7test", "du's", ",", "so", "w\u00e4rst", "du", "jetzt", "nicht", "Pa\u00b7ti\u00b7ent", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PIS", "$,", "ADV", "VAFIN", "PPER", "ADV", "PTKNEG", "NN", "$.", "$("], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}}}}}