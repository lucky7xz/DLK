{"textgrid.poem.54309": {"metadata": {"author": {"name": "Ziegler, Christiana Mariana von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Maulthier tr\u00e4gt den Pack, und darff sich nicht beschweren;", "genre": "verse", "period": "N.A.", "pub_year": 1727, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Maulthier tr\u00e4gt den Pack, und darff sich nicht beschweren;", "tokens": ["Ein", "Mault\u00b7hier", "tr\u00e4gt", "den", "Pack", ",", "und", "darff", "sich", "nicht", "be\u00b7schwe\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,", "KON", "VMFIN", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Hingegen Lepidus klagt \u00fcber seine Last.", "tokens": ["Hin\u00b7ge\u00b7gen", "Le\u00b7pi\u00b7dus", "klagt", "\u00fc\u00b7ber", "sei\u00b7ne", "Last", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Durch solche Prahlerey will er das Volck beth\u00f6ren,", "tokens": ["Durch", "sol\u00b7che", "Prah\u00b7le\u00b7rey", "will", "er", "das", "Volck", "be\u00b7th\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als h\u00e4tt er Tag und Nacht vor Arbeit keine Rast.", "tokens": ["Als", "h\u00e4tt", "er", "Tag", "und", "Nacht", "vor", "Ar\u00b7beit", "kei\u00b7ne", "Rast", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "NN", "KON", "NN", "APPR", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Er l\u00e4st Collegia an Eck und H\u00e4user schlagen,", "tokens": ["Er", "l\u00e4st", "Col\u00b7le\u00b7gia", "an", "Eck", "und", "H\u00e4u\u00b7ser", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "APPR", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Da doch der gute Mensch kaum den Donat versteht;", "tokens": ["Da", "doch", "der", "gu\u00b7te", "Mensch", "kaum", "den", "Do\u00b7nat", "ver\u00b7steht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Allein kein Sch\u00fcler will nach seinen Wischen fragen.", "tokens": ["Al\u00b7lein", "kein", "Sch\u00fc\u00b7ler", "will", "nach", "sei\u00b7nen", "Wi\u00b7schen", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Dieweil ein jeder wei\u00df, wie weit sein Wissen geht.", "tokens": ["Die\u00b7weil", "ein", "je\u00b7der", "wei\u00df", ",", "wie", "weit", "sein", "Wis\u00b7sen", "geht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "PIS", "VVFIN", "$,", "PWAV", "ADJD", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie kan er andern wohl die Rechts-Gelahrheit lehren?", "tokens": ["Wie", "kan", "er", "an\u00b7dern", "wohl", "die", "Rechts\u00b7Ge\u00b7lahr\u00b7heit", "leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "PIS", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Da Themis ihn doch nur vor ein Bastard h\u00e4lt;", "tokens": ["Da", "The\u00b7mis", "ihn", "doch", "nur", "vor", "ein", "Bas\u00b7tard", "h\u00e4lt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PPER", "ADV", "ADV", "APPR", "ART", "NE", "VVFIN", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.11": {"text": "Und dennoch sucht er sich erb\u00e4rmlich zu beschwehren,", "tokens": ["Und", "den\u00b7noch", "sucht", "er", "sich", "er\u00b7b\u00e4rm\u00b7lich", "zu", "be\u00b7schweh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Als l\u00e4g auf selbigen die gr\u00f6ste Last der Welt.", "tokens": ["Als", "l\u00e4g", "auf", "sel\u00b7bi\u00b7gen", "die", "gr\u00f6s\u00b7te", "Last", "der", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "ADJA", "ART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Ein leerer Kopf kan nicht von vieler Arbeit schwitzen.", "tokens": ["Ein", "lee\u00b7rer", "Kopf", "kan", "nicht", "von", "vie\u00b7ler", "Ar\u00b7beit", "schwit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PTKNEG", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Schweig, Prahler, bitt ich dich, du machst vergebens Wind.", "tokens": ["Schweig", ",", "Prah\u00b7ler", ",", "bitt", "ich", "dich", ",", "du", "machst", "ver\u00b7ge\u00b7bens", "Wind", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "$,", "VVFIN", "PPER", "PRF", "$,", "PPER", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Bey Tobac, Bier, Coffee, sieht man dich st\u00fcndlich sitzen.", "tokens": ["Bey", "To\u00b7bac", ",", "Bier", ",", "Cof\u00b7fee", ",", "sieht", "man", "dich", "st\u00fcnd\u00b7lich", "sit\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "$,", "NN", "$,", "VVFIN", "PIS", "PRF", "ADJD", "VVINF", "$."], "meter": "-+--+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Die kluge Welt wird nicht von blauen D\u00fcnsten blind.", "tokens": ["Die", "klu\u00b7ge", "Welt", "wird", "nicht", "von", "blau\u00b7en", "D\u00fcns\u00b7ten", "blind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PTKNEG", "APPR", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "In Sauffen k\u00f6ntest du wohl als Professor lesen,", "tokens": ["In", "Sauf\u00b7fen", "k\u00f6n\u00b7test", "du", "wohl", "als", "Pro\u00b7fes\u00b7sor", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "VMFIN", "PPER", "ADV", "KOUS", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Dein Corpus Juris heist ein teutsches Karten-Spiel.", "tokens": ["Dein", "Cor\u00b7pus", "Ju\u00b7ris", "heist", "ein", "teut\u00b7sches", "Kar\u00b7ten\u00b7Spiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Di\u00df ist die gr\u00f6ste Kunst, so du gelernt, gewesen.", "tokens": ["Di\u00df", "ist", "die", "gr\u00f6s\u00b7te", "Kunst", ",", "so", "du", "ge\u00b7lernt", ",", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "ADV", "PPER", "VVPP", "$,", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Ach! solche Wei\u00dfheit gilt nicht einen Pappen-Stiehl.", "tokens": ["Ach", "!", "sol\u00b7che", "Wei\u00df\u00b7heit", "gilt", "nicht", "ei\u00b7nen", "Pap\u00b7pen\u00b7S\u00b7tiehl", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "PIAT", "NN", "VVFIN", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Ein Maulthier tr\u00e4gt den Pack, und darff sich nicht beschweren;", "tokens": ["Ein", "Mault\u00b7hier", "tr\u00e4gt", "den", "Pack", ",", "und", "darff", "sich", "nicht", "be\u00b7schwe\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,", "KON", "VMFIN", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Hingegen Lepidus klagt \u00fcber seine Last.", "tokens": ["Hin\u00b7ge\u00b7gen", "Le\u00b7pi\u00b7dus", "klagt", "\u00fc\u00b7ber", "sei\u00b7ne", "Last", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Durch solche Prahlerey will er das Volck beth\u00f6ren,", "tokens": ["Durch", "sol\u00b7che", "Prah\u00b7le\u00b7rey", "will", "er", "das", "Volck", "be\u00b7th\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als h\u00e4tt er Tag und Nacht vor Arbeit keine Rast.", "tokens": ["Als", "h\u00e4tt", "er", "Tag", "und", "Nacht", "vor", "Ar\u00b7beit", "kei\u00b7ne", "Rast", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "NN", "KON", "NN", "APPR", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Er l\u00e4st Collegia an Eck und H\u00e4user schlagen,", "tokens": ["Er", "l\u00e4st", "Col\u00b7le\u00b7gia", "an", "Eck", "und", "H\u00e4u\u00b7ser", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "APPR", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Da doch der gute Mensch kaum den Donat versteht;", "tokens": ["Da", "doch", "der", "gu\u00b7te", "Mensch", "kaum", "den", "Do\u00b7nat", "ver\u00b7steht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Allein kein Sch\u00fcler will nach seinen Wischen fragen.", "tokens": ["Al\u00b7lein", "kein", "Sch\u00fc\u00b7ler", "will", "nach", "sei\u00b7nen", "Wi\u00b7schen", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Dieweil ein jeder wei\u00df, wie weit sein Wissen geht.", "tokens": ["Die\u00b7weil", "ein", "je\u00b7der", "wei\u00df", ",", "wie", "weit", "sein", "Wis\u00b7sen", "geht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "PIS", "VVFIN", "$,", "PWAV", "ADJD", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie kan er andern wohl die Rechts-Gelahrheit lehren?", "tokens": ["Wie", "kan", "er", "an\u00b7dern", "wohl", "die", "Rechts\u00b7Ge\u00b7lahr\u00b7heit", "leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "PIS", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Da Themis ihn doch nur vor ein Bastard h\u00e4lt;", "tokens": ["Da", "The\u00b7mis", "ihn", "doch", "nur", "vor", "ein", "Bas\u00b7tard", "h\u00e4lt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PPER", "ADV", "ADV", "APPR", "ART", "NE", "VVFIN", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.11": {"text": "Und dennoch sucht er sich erb\u00e4rmlich zu beschwehren,", "tokens": ["Und", "den\u00b7noch", "sucht", "er", "sich", "er\u00b7b\u00e4rm\u00b7lich", "zu", "be\u00b7schweh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Als l\u00e4g auf selbigen die gr\u00f6ste Last der Welt.", "tokens": ["Als", "l\u00e4g", "auf", "sel\u00b7bi\u00b7gen", "die", "gr\u00f6s\u00b7te", "Last", "der", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "ADJA", "ART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Ein leerer Kopf kan nicht von vieler Arbeit schwitzen.", "tokens": ["Ein", "lee\u00b7rer", "Kopf", "kan", "nicht", "von", "vie\u00b7ler", "Ar\u00b7beit", "schwit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PTKNEG", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Schweig, Prahler, bitt ich dich, du machst vergebens Wind.", "tokens": ["Schweig", ",", "Prah\u00b7ler", ",", "bitt", "ich", "dich", ",", "du", "machst", "ver\u00b7ge\u00b7bens", "Wind", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "$,", "VVFIN", "PPER", "PRF", "$,", "PPER", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Bey Tobac, Bier, Coffee, sieht man dich st\u00fcndlich sitzen.", "tokens": ["Bey", "To\u00b7bac", ",", "Bier", ",", "Cof\u00b7fee", ",", "sieht", "man", "dich", "st\u00fcnd\u00b7lich", "sit\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "$,", "NN", "$,", "VVFIN", "PIS", "PRF", "ADJD", "VVINF", "$."], "meter": "-+--+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Die kluge Welt wird nicht von blauen D\u00fcnsten blind.", "tokens": ["Die", "klu\u00b7ge", "Welt", "wird", "nicht", "von", "blau\u00b7en", "D\u00fcns\u00b7ten", "blind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PTKNEG", "APPR", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "In Sauffen k\u00f6ntest du wohl als Professor lesen,", "tokens": ["In", "Sauf\u00b7fen", "k\u00f6n\u00b7test", "du", "wohl", "als", "Pro\u00b7fes\u00b7sor", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "VMFIN", "PPER", "ADV", "KOUS", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Dein Corpus Juris heist ein teutsches Karten-Spiel.", "tokens": ["Dein", "Cor\u00b7pus", "Ju\u00b7ris", "heist", "ein", "teut\u00b7sches", "Kar\u00b7ten\u00b7Spiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Di\u00df ist die gr\u00f6ste Kunst, so du gelernt, gewesen.", "tokens": ["Di\u00df", "ist", "die", "gr\u00f6s\u00b7te", "Kunst", ",", "so", "du", "ge\u00b7lernt", ",", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "ADV", "PPER", "VVPP", "$,", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Ach! solche Wei\u00dfheit gilt nicht einen Pappen-Stiehl.", "tokens": ["Ach", "!", "sol\u00b7che", "Wei\u00df\u00b7heit", "gilt", "nicht", "ei\u00b7nen", "Pap\u00b7pen\u00b7S\u00b7tiehl", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "PIAT", "NN", "VVFIN", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}