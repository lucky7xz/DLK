{"textgrid.poem.37221": {"metadata": {"author": {"name": "Boie, Heinrich Christian", "birth": "N.A.", "death": "N.A."}, "title": "Der Himmel", "genre": "verse", "period": "N.A.", "pub_year": 1775, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sechs Fromme von verschiedner Innung", "tokens": ["Sechs", "From\u00b7me", "von", "ver\u00b7schied\u00b7ner", "In\u00b7nung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch gleich unstr\u00e4flicher Gesinnung", "tokens": ["Doch", "gleich", "uns\u00b7tr\u00e4f\u00b7li\u00b7cher", "Ge\u00b7sin\u00b7nung"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begegnen, wo nicht Zeit noch Raum", "tokens": ["Be\u00b7geg\u00b7nen", ",", "wo", "nicht", "Zeit", "noch", "Raum"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "PWAV", "PTKNEG", "NN", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mehr engt, sich \u2013 an des Himmels Saum.", "tokens": ["Mehr", "engt", ",", "sich", "\u2013", "an", "des", "Him\u00b7mels", "Saum", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PRF", "$(", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Schnell blizt der Eingang aufgeschlo\u00dfen,", "tokens": ["Schnell", "blizt", "der", "Ein\u00b7gang", "auf\u00b7ge\u00b7schlo\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und von Verkl\u00e4rungsglanz umflo\u00dfen", "tokens": ["Und", "von", "Ver\u00b7kl\u00e4\u00b7rungs\u00b7glanz", "um\u00b7flo\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Tritt mild ein Genius heran", "tokens": ["Tritt", "mild", "ein", "Ge\u00b7nius", "he\u00b7ran"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "ART", "NN", "PTKVZ"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und fragt: Wer Du? \u2013 \u00bbEin Muselman.\u00ab", "tokens": ["Und", "fragt", ":", "Wer", "Du", "?", "\u2013", "\u00bb", "Ein", "Mu\u00b7sel\u00b7man", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "PWS", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$("], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}}, "stanza.3": {"line.1": {"text": "Ins Paradies dort, wo die Frommen", "tokens": ["Ins", "Pa\u00b7ra\u00b7dies", "dort", ",", "wo", "die", "From\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "ADV", "$,", "PWAV", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Zu mehr als Machmuds Lichte kommen! \u2013", "tokens": ["Zu", "mehr", "als", "Mach\u00b7muds", "Lich\u00b7te", "kom\u00b7men", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIS", "KOKOM", "NE", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und Du? \u2013 \u00bbEin Jud.\u00ab \u2013 Im Tempelchor", "tokens": ["Und", "Du", "?", "\u2013", "\u00bb", "Ein", "Jud", ".", "\u00ab", "\u2013", "Im", "Tem\u00b7pel\u00b7chor"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["KON", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$(", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Singt Assaff dort erw\u00e4hlten vor.", "tokens": ["Singt", "A\u00b7ssaff", "dort", "er\u00b7w\u00e4hl\u00b7ten", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und Du, der wundernd steht, als mahn' er", "tokens": ["Und", "Du", ",", "der", "wun\u00b7dernd", "steht", ",", "als", "mahn'", "er"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "$,", "PRELS", "ADJD", "VVFIN", "$,", "KOUS", "VVFIN", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Des Irrthums mich? \u2013 \u00bbEin Lutheraner!\u00ab \u2013", "tokens": ["Des", "Irr\u00b7thums", "mich", "?", "\u2013", "\u00bb", "Ein", "Lu\u00b7the\u00b7ra\u00b7ner", "!", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Geh aufzukl\u00e4ren Deinen Sinn", "tokens": ["Geh", "auf\u00b7zu\u00b7kl\u00e4\u00b7ren", "Dei\u00b7nen", "Sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum schon belehrten Pastor hin.", "tokens": ["Zum", "schon", "be\u00b7lehr\u00b7ten", "Pas\u00b7tor", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADV", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Du denn? \u2013 \u00bbEin Qu\u00e4ker.\u00ab \u2013 Abgeschieden", "tokens": ["Du", "denn", "?", "\u2013", "\u00bb", "Ein", "Qu\u00e4\u00b7ker", ".", "\u00ab", "\u2013", "Ab\u00b7ge\u00b7schie\u00b7den"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "ADV", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$(", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sind Deine Br\u00fcder dort im Frieden.", "tokens": ["Sind", "Dei\u00b7ne", "Br\u00fc\u00b7der", "dort", "im", "Frie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Behalt den Hut auf wenns gef\u00e4llt,", "tokens": ["Be\u00b7halt", "den", "Hut", "auf", "wenns", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vergn\u00fcgt mit Penn der be\u00dfern Welt.", "tokens": ["Ver\u00b7gn\u00fcgt", "mit", "Penn", "der", "be\u00b7\u00dfern", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und Du dort? \u2013 \u00bbUeberf\u00fchrt allm\u00e4hlich", "tokens": ["Und", "Du", "dort", "?", "\u2013", "\u00bb", "Ue\u00b7ber\u00b7f\u00fchrt", "all\u00b7m\u00e4h\u00b7lich"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["KON", "PPER", "ADV", "$.", "$(", "$(", "VVFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mach' allein mein Glauben selig.", "tokens": ["Nicht", "mach'", "al\u00b7lein", "mein", "Glau\u00b7ben", "se\u00b7lig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "ADV", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch fremd gesteh ich scheinen mir", "tokens": ["Doch", "fremd", "ge\u00b7steh", "ich", "schei\u00b7nen", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Christen T\u00fcrk und Jud allhier.\u00ab", "tokens": ["Bei", "Chris\u00b7ten", "T\u00fcrk", "und", "Jud", "all\u00b7hier", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Wie Schuppen von den Augen fallen", "tokens": ["Wie", "Schup\u00b7pen", "von", "den", "Au\u00b7gen", "fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wird bald der Zweifel Dir und allen.", "tokens": ["Wird", "bald", "der", "Zwei\u00b7fel", "Dir", "und", "al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "PPER", "KON", "PIAT", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Jezt theile Ganganellis Ruh! \u2013", "tokens": ["Jezt", "thei\u00b7le", "Gan\u00b7ga\u00b7nel\u00b7lis", "Ruh", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "NE", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von welcher Kirche bist denn Du?", "tokens": ["Von", "wel\u00b7cher", "Kir\u00b7che", "bist", "denn", "Du", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "VAFIN", "KON", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "\u00bbvon keiner!\u00ab \u2013 Anzunehmen w\u00e4re", "tokens": ["\u00bb", "von", "kei\u00b7ner", "!", "\u00ab", "\u2013", "An\u00b7zu\u00b7neh\u00b7men", "w\u00e4\u00b7re"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["$(", "APPR", "PIS", "$.", "$(", "$(", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "D\u00e4cht ich doch irgend eine Lehre? \u2013", "tokens": ["D\u00e4cht", "ich", "doch", "ir\u00b7gend", "ei\u00b7ne", "Leh\u00b7re", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbda\u00df Einer sei, der alles schafft,", "tokens": ["\u00bb", "da\u00df", "Ei\u00b7ner", "sei", ",", "der", "al\u00b7les", "schafft", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PIS", "VAFIN", "$,", "PRELS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Gutes lohnet, b\u00f6ses straft,", "tokens": ["Der", "Gu\u00b7tes", "loh\u00b7net", ",", "b\u00f6\u00b7ses", "straft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Und da\u00df Unsterblichkeit der Seele,", "tokens": ["Und", "da\u00df", "U\u00b7nsterb\u00b7lich\u00b7keit", "der", "See\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NN", "ART", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Die sterbliches verschm\u00e4ht, nicht fehle.", "tokens": ["Die", "sterb\u00b7li\u00b7ches", "ver\u00b7schm\u00e4ht", ",", "nicht", "feh\u00b7le", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVPP", "$,", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Geglaubt das hab ich und ge\u00fcbt.\u00ab \u2013", "tokens": ["Ge\u00b7glaubt", "das", "hab", "ich", "und", "ge\u00b7\u00fcbt", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "PDS", "VAFIN", "PPER", "KON", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nimm Platz denn, wo es Dir beliebt.", "tokens": ["Nimm", "Platz", "denn", ",", "wo", "es", "Dir", "be\u00b7liebt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ADV", "$,", "PWAV", "PPER", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Sechs Fromme von verschiedner Innung", "tokens": ["Sechs", "From\u00b7me", "von", "ver\u00b7schied\u00b7ner", "In\u00b7nung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch gleich unstr\u00e4flicher Gesinnung", "tokens": ["Doch", "gleich", "uns\u00b7tr\u00e4f\u00b7li\u00b7cher", "Ge\u00b7sin\u00b7nung"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begegnen, wo nicht Zeit noch Raum", "tokens": ["Be\u00b7geg\u00b7nen", ",", "wo", "nicht", "Zeit", "noch", "Raum"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "PWAV", "PTKNEG", "NN", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mehr engt, sich \u2013 an des Himmels Saum.", "tokens": ["Mehr", "engt", ",", "sich", "\u2013", "an", "des", "Him\u00b7mels", "Saum", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PRF", "$(", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Schnell blizt der Eingang aufgeschlo\u00dfen,", "tokens": ["Schnell", "blizt", "der", "Ein\u00b7gang", "auf\u00b7ge\u00b7schlo\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und von Verkl\u00e4rungsglanz umflo\u00dfen", "tokens": ["Und", "von", "Ver\u00b7kl\u00e4\u00b7rungs\u00b7glanz", "um\u00b7flo\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Tritt mild ein Genius heran", "tokens": ["Tritt", "mild", "ein", "Ge\u00b7nius", "he\u00b7ran"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "ART", "NN", "PTKVZ"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und fragt: Wer Du? \u2013 \u00bbEin Muselman.\u00ab", "tokens": ["Und", "fragt", ":", "Wer", "Du", "?", "\u2013", "\u00bb", "Ein", "Mu\u00b7sel\u00b7man", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "PWS", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$("], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}}, "stanza.12": {"line.1": {"text": "Ins Paradies dort, wo die Frommen", "tokens": ["Ins", "Pa\u00b7ra\u00b7dies", "dort", ",", "wo", "die", "From\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "ADV", "$,", "PWAV", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Zu mehr als Machmuds Lichte kommen! \u2013", "tokens": ["Zu", "mehr", "als", "Mach\u00b7muds", "Lich\u00b7te", "kom\u00b7men", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIS", "KOKOM", "NE", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und Du? \u2013 \u00bbEin Jud.\u00ab \u2013 Im Tempelchor", "tokens": ["Und", "Du", "?", "\u2013", "\u00bb", "Ein", "Jud", ".", "\u00ab", "\u2013", "Im", "Tem\u00b7pel\u00b7chor"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["KON", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$(", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Singt Assaff dort erw\u00e4hlten vor.", "tokens": ["Singt", "A\u00b7ssaff", "dort", "er\u00b7w\u00e4hl\u00b7ten", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und Du, der wundernd steht, als mahn' er", "tokens": ["Und", "Du", ",", "der", "wun\u00b7dernd", "steht", ",", "als", "mahn'", "er"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "$,", "PRELS", "ADJD", "VVFIN", "$,", "KOUS", "VVFIN", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Des Irrthums mich? \u2013 \u00bbEin Lutheraner!\u00ab \u2013", "tokens": ["Des", "Irr\u00b7thums", "mich", "?", "\u2013", "\u00bb", "Ein", "Lu\u00b7the\u00b7ra\u00b7ner", "!", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "PPER", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Geh aufzukl\u00e4ren Deinen Sinn", "tokens": ["Geh", "auf\u00b7zu\u00b7kl\u00e4\u00b7ren", "Dei\u00b7nen", "Sinn"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum schon belehrten Pastor hin.", "tokens": ["Zum", "schon", "be\u00b7lehr\u00b7ten", "Pas\u00b7tor", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADV", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Du denn? \u2013 \u00bbEin Qu\u00e4ker.\u00ab \u2013 Abgeschieden", "tokens": ["Du", "denn", "?", "\u2013", "\u00bb", "Ein", "Qu\u00e4\u00b7ker", ".", "\u00ab", "\u2013", "Ab\u00b7ge\u00b7schie\u00b7den"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "word"], "pos": ["PPER", "ADV", "$.", "$(", "$(", "ART", "NN", "$.", "$(", "$(", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sind Deine Br\u00fcder dort im Frieden.", "tokens": ["Sind", "Dei\u00b7ne", "Br\u00fc\u00b7der", "dort", "im", "Frie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Behalt den Hut auf wenns gef\u00e4llt,", "tokens": ["Be\u00b7halt", "den", "Hut", "auf", "wenns", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vergn\u00fcgt mit Penn der be\u00dfern Welt.", "tokens": ["Ver\u00b7gn\u00fcgt", "mit", "Penn", "der", "be\u00b7\u00dfern", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Und Du dort? \u2013 \u00bbUeberf\u00fchrt allm\u00e4hlich", "tokens": ["Und", "Du", "dort", "?", "\u2013", "\u00bb", "Ue\u00b7ber\u00b7f\u00fchrt", "all\u00b7m\u00e4h\u00b7lich"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["KON", "PPER", "ADV", "$.", "$(", "$(", "VVFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mach' allein mein Glauben selig.", "tokens": ["Nicht", "mach'", "al\u00b7lein", "mein", "Glau\u00b7ben", "se\u00b7lig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "ADV", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch fremd gesteh ich scheinen mir", "tokens": ["Doch", "fremd", "ge\u00b7steh", "ich", "schei\u00b7nen", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Christen T\u00fcrk und Jud allhier.\u00ab", "tokens": ["Bei", "Chris\u00b7ten", "T\u00fcrk", "und", "Jud", "all\u00b7hier", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Wie Schuppen von den Augen fallen", "tokens": ["Wie", "Schup\u00b7pen", "von", "den", "Au\u00b7gen", "fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wird bald der Zweifel Dir und allen.", "tokens": ["Wird", "bald", "der", "Zwei\u00b7fel", "Dir", "und", "al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "PPER", "KON", "PIAT", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Jezt theile Ganganellis Ruh! \u2013", "tokens": ["Jezt", "thei\u00b7le", "Gan\u00b7ga\u00b7nel\u00b7lis", "Ruh", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "NE", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von welcher Kirche bist denn Du?", "tokens": ["Von", "wel\u00b7cher", "Kir\u00b7che", "bist", "denn", "Du", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "VAFIN", "KON", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "\u00bbvon keiner!\u00ab \u2013 Anzunehmen w\u00e4re", "tokens": ["\u00bb", "von", "kei\u00b7ner", "!", "\u00ab", "\u2013", "An\u00b7zu\u00b7neh\u00b7men", "w\u00e4\u00b7re"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "word", "word"], "pos": ["$(", "APPR", "PIS", "$.", "$(", "$(", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "D\u00e4cht ich doch irgend eine Lehre? \u2013", "tokens": ["D\u00e4cht", "ich", "doch", "ir\u00b7gend", "ei\u00b7ne", "Leh\u00b7re", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbda\u00df Einer sei, der alles schafft,", "tokens": ["\u00bb", "da\u00df", "Ei\u00b7ner", "sei", ",", "der", "al\u00b7les", "schafft", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PIS", "VAFIN", "$,", "PRELS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Gutes lohnet, b\u00f6ses straft,", "tokens": ["Der", "Gu\u00b7tes", "loh\u00b7net", ",", "b\u00f6\u00b7ses", "straft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Und da\u00df Unsterblichkeit der Seele,", "tokens": ["Und", "da\u00df", "U\u00b7nsterb\u00b7lich\u00b7keit", "der", "See\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NN", "ART", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Die sterbliches verschm\u00e4ht, nicht fehle.", "tokens": ["Die", "sterb\u00b7li\u00b7ches", "ver\u00b7schm\u00e4ht", ",", "nicht", "feh\u00b7le", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVPP", "$,", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Geglaubt das hab ich und ge\u00fcbt.\u00ab \u2013", "tokens": ["Ge\u00b7glaubt", "das", "hab", "ich", "und", "ge\u00b7\u00fcbt", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "PDS", "VAFIN", "PPER", "KON", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nimm Platz denn, wo es Dir beliebt.", "tokens": ["Nimm", "Platz", "denn", ",", "wo", "es", "Dir", "be\u00b7liebt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ADV", "$,", "PWAV", "PPER", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}