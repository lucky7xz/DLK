{"dta.poem.10183": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Das Vergangene,  \n bey dem 1729. Jahres-Wechsel,  \n betrachtet.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1735", "urn": "urn:nbn:de:kobv:b4-20086-0", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Es hat der Erden-Kreis den Lauff nun abermahl,", "tokens": ["Es", "hat", "der", "Er\u00b7den\u00b7Kreis", "den", "Lauff", "nun", "a\u00b7ber\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der uns vom Sonnen-Licht entfernete, geendet:", "tokens": ["Der", "uns", "vom", "Son\u00b7nen\u00b7Licht", "ent\u00b7fer\u00b7ne\u00b7te", ",", "ge\u00b7en\u00b7det", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "VVFIN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Er hat sich allbereit, GOTT sey gedanckt! gewendet,", "tokens": ["Er", "hat", "sich", "all\u00b7be\u00b7reit", ",", "GoTT", "sey", "ge\u00b7danckt", "!", "ge\u00b7wen\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "$,", "NE", "VAFIN", "VVPP", "$.", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Von Nordens Frost und Nacht, zum Licht- und Lebens-", "tokens": ["Von", "Nor\u00b7dens", "Frost", "und", "Nacht", ",", "zum", "Licht", "und", "Le\u00b7bens"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "NN", "KON", "NN", "$,", "APPRART", "TRUNC", "KON", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Je mehr die\u00df Wunder-Werck nun zu bewundern wehrt,", "tokens": ["Je", "mehr", "die\u00df", "Wun\u00b7der\u00b7\u00b7Werck", "nun", "zu", "be\u00b7wun\u00b7dern", "wehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PDS", "NN", "ADV", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Je mehr es den Begriff von GOttes Allmacht mehrt,", "tokens": ["Je", "mehr", "es", "den", "Be\u00b7griff", "von", "Got\u00b7tes", "All\u00b7macht", "mehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ART", "NN", "APPR", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Je gr\u00f6sser Heil dadurch der Menschheit wiederf\u00e4hrt,", "tokens": ["Je", "gr\u00f6s\u00b7ser", "Heil", "da\u00b7durch", "der", "Menschheit", "wie\u00b7der\u00b7f\u00e4hrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "PAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Je mehr denn auch daf\u00fcr dem Sch\u00f6pfer Danck geh\u00f6rt;", "tokens": ["Je", "mehr", "denn", "auch", "da\u00b7f\u00fcr", "dem", "Sch\u00f6p\u00b7fer", "Danck", "ge\u00b7h\u00f6rt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "PAV", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Je minder, leider! wird von uns darauf geachtet;", "tokens": ["Je", "min\u00b7der", ",", "lei\u00b7der", "!", "wird", "von", "uns", "da\u00b7rauf", "ge\u00b7ach\u00b7tet", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "ADV", "$.", "VAFIN", "APPR", "PPER", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Je weniger wird es, zu GOttes Ruhm, betrachtet;", "tokens": ["Je", "we\u00b7ni\u00b7ger", "wird", "es", ",", "zu", "Got\u00b7tes", "Ruhm", ",", "be\u00b7trach\u00b7tet", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "$,", "APPR", "NN", "NN", "$,", "VVPP", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "So gar, da\u00df kein Gesch\u00f6pf, auch kein Vernunft-los Thier.", "tokens": ["So", "gar", ",", "da\u00df", "kein", "Ge\u00b7sch\u00f6pf", ",", "auch", "kein", "Ver\u00b7nunft\u00b7los", "Thier", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PIAT", "NN", "$,", "ADV", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Am schwartzen Undancks-Greul so schuldig ist, als wir.", "tokens": ["Am", "schwart\u00b7zen", "Un\u00b7dancks\u00b7Greul", "so", "schul\u00b7dig", "ist", ",", "als", "wir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ADV", "ADJD", "VAFIN", "$,", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "War\u00fcm? es wei\u00df es nicht. Wir wissen Zeit und Stunde,", "tokens": ["Wa\u00b7r\u00fcm", "?", "es", "wei\u00df", "es", "nicht", ".", "Wir", "wis\u00b7sen", "Zeit", "und", "Stun\u00b7de", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PPER", "VVFIN", "PPER", "PTKNEG", "$.", "PPER", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wir rechnen die Minut\u2019, und kennen die Seeunde,", "tokens": ["Wir", "rech\u00b7nen", "die", "Mi\u00b7nut'", ",", "und", "ken\u00b7nen", "die", "See\u00b7un\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "KON", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+--++-", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Wann die so heilsame Ver\u00e4nderung geschicht.", "tokens": ["Wann", "die", "so", "heil\u00b7sa\u00b7me", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", "ge\u00b7schicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADV", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Doch, diesem ungeacht, bestreben wir uns nicht,", "tokens": ["Doch", ",", "die\u00b7sem", "un\u00b7ge\u00b7acht", ",", "be\u00b7stre\u00b7ben", "wir", "uns", "nicht", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PDAT", "ADJD", "$,", "VVFIN", "PPER", "PRF", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Der allgewaltigen, liebreichen, weisen Krafft,", "tokens": ["Der", "all\u00b7ge\u00b7wal\u00b7ti\u00b7gen", ",", "lieb\u00b7rei\u00b7chen", ",", "wei\u00b7sen", "Krafft", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "VVFIN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Die, im erhalten selbst, noch unaufh\u00f6rlich schafft,", "tokens": ["Die", ",", "im", "er\u00b7hal\u00b7ten", "selbst", ",", "noch", "un\u00b7auf\u00b7h\u00f6r\u00b7lich", "schafft", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "$,", "APPRART", "ADJA", "ADV", "$,", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Nur den geringsten Dienst, nur den geringsten Danck,", "tokens": ["Nur", "den", "ge\u00b7rings\u00b7ten", "Dienst", ",", "nur", "den", "ge\u00b7rings\u00b7ten", "Danck", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "F\u00fcr dieses Wunder-Werck zu leisten und zu bringen.", "tokens": ["F\u00fcr", "die\u00b7ses", "Wun\u00b7der\u00b7\u00b7Werck", "zu", "leis\u00b7ten", "und", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wann h\u00f6rt man, Jhm daf\u00fcr ein Lob- und Danck-Lied", "tokens": ["Wann", "h\u00f6rt", "man", ",", "Jhm", "da\u00b7f\u00fcr", "ein", "Lob", "und", "Dan\u00b7ck\u00b7Lied"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PIS", "$,", "PPER", "PAV", "ART", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Sprich, liebster Leser, nicht: Ja! ja! ich finde wol,", "tokens": ["Sprich", ",", "liebs\u00b7ter", "Le\u00b7ser", ",", "nicht", ":", "Ja", "!", "ja", "!", "ich", "fin\u00b7de", "wol", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "ADJA", "NN", "$,", "PTKNEG", "$.", "PTKANT", "$.", "PTKANT", "$.", "PPER", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df man auch ietzt, wie sonst, dem Sch\u00f6pfer dancken soll", "tokens": ["Da\u00df", "man", "auch", "ietzt", ",", "wie", "sonst", ",", "dem", "Sch\u00f6p\u00b7fer", "dan\u00b7cken", "soll"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "ADV", "$,", "PWAV", "ADV", "$,", "ART", "NN", "VVINF", "VMFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "F\u00fcr alles, was Er thut. Denn, da\u00df das Wasser na\u00df;", "tokens": ["F\u00fcr", "al\u00b7les", ",", "was", "Er", "thut", ".", "Denn", ",", "da\u00df", "das", "Was\u00b7ser", "na\u00df", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "PPER", "VVFIN", "$.", "KON", "$,", "KOUS", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das Feuer reg\u2019 und hei\u00df; da\u00df Bluhmen, Laub und Gras", "tokens": ["Das", "Feu\u00b7er", "reg'", "und", "hei\u00df", ";", "da\u00df", "Bluh\u00b7men", ",", "Laub", "und", "Gras"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "KON", "ADJD", "$.", "KOUS", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Aus schwartzer Erde gr\u00fcnen,", "tokens": ["Aus", "schwart\u00b7zer", "Er\u00b7de", "gr\u00fc\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Sind Dinge, die gewi\u00df Bewunderung verdienen:", "tokens": ["Sind", "Din\u00b7ge", ",", "die", "ge\u00b7wi\u00df", "Be\u00b7wun\u00b7de\u00b7rung", "ver\u00b7die\u00b7nen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "PRELS", "ADV", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So auch die\u00df drehn der Welt. Ach nein: mich deucht,", "tokens": ["So", "auch", "die\u00df", "drehn", "der", "Welt", ".", "Ach", "nein", ":", "mich", "deucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PDS", "CARD", "ART", "NN", "$.", "NN", "PTKANT", "$.", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Der so verwunderlich und schr\u00e4g gestellten Erde", "tokens": ["Der", "so", "ver\u00b7wun\u00b7der\u00b7lich", "und", "schr\u00e4g", "ge\u00b7stell\u00b7ten", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADJD", "KON", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Sey wehrt, da\u00df es noch mehr mit Wolbedacht gesehn,", "tokens": ["Sey", "wehrt", ",", "da\u00df", "es", "noch", "mehr", "mit", "Wol\u00b7be\u00b7dacht", "ge\u00b7sehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "$,", "KOUS", "PPER", "ADV", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Mit mehrerm Fleiss\u2019 und Ernst betrachtet,", "tokens": ["Mit", "meh\u00b7rerm", "Fleiss'", "und", "Ernst", "be\u00b7trach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Mit mehr Aufmercksamkeit von uns beachtet,", "tokens": ["Mit", "mehr", "Auf\u00b7merck\u00b7sam\u00b7keit", "von", "uns", "be\u00b7ach\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Und folglich GOTT darinn noch mehr bewundert werde.", "tokens": ["Und", "folg\u00b7lich", "GoTT", "da\u00b7rinn", "noch", "mehr", "be\u00b7wun\u00b7dert", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "PAV", "ADV", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Denn ob gleich alle Ding\u2019 uns ihren Sch\u00f6pfer weisen;", "tokens": ["Denn", "ob", "gleich", "al\u00b7le", "Ding'", "uns", "ih\u00b7ren", "Sch\u00f6p\u00b7fer", "wei\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "PIAT", "NN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Ob Er, im kleinsten auch zu loben und zu preisen;", "tokens": ["Ob", "Er", ",", "im", "kleins\u00b7ten", "auch", "zu", "lo\u00b7ben", "und", "zu", "prei\u00b7sen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPRART", "ADJA", "ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "So werden wir dennoch, wenn wir es recht ergr\u00fcnden,", "tokens": ["So", "wer\u00b7den", "wir", "den\u00b7noch", ",", "wenn", "wir", "es", "recht", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "$,", "KOUS", "PPER", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Was ausserordentlichs in dieser Lenckung finden.", "tokens": ["Was", "aus\u00b7ser\u00b7or\u00b7dent\u00b7lichs", "in", "die\u00b7ser", "Len\u00b7ckung", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Der grosse Sch\u00f6pfer hat, nach Zahlen, Mass\u2019, Gewicht,", "tokens": ["Der", "gros\u00b7se", "Sch\u00f6p\u00b7fer", "hat", ",", "nach", "Zah\u00b7len", ",", "Mass'", ",", "Ge\u00b7wicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "$,", "APPR", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Den grossen Bau der Welt besonders zugericht:", "tokens": ["Den", "gros\u00b7sen", "Bau", "der", "Welt", "be\u00b7son\u00b7ders", "zu\u00b7ge\u00b7richt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df an der grossen Last die Angeln schr\u00e4ge stehn,", "tokens": ["Da\u00df", "an", "der", "gros\u00b7sen", "Last", "die", "An\u00b7geln", "schr\u00e4\u00b7ge", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wodurch so heilsame Ver\u00e4ndrungen geschehn,", "tokens": ["Wo\u00b7durch", "so", "heil\u00b7sa\u00b7me", "Ver\u00b7\u00e4n\u00b7drun\u00b7gen", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-++--+", "measure": "iambic.hexa.chol"}, "line.5": {"text": "Von W\u00e4rm\u2019 und Frost, von Schatten und von Licht;", "tokens": ["Von", "W\u00e4rm'", "und", "Frost", ",", "von", "Schat\u00b7ten", "und", "von", "Licht", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NN", "$,", "APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Da sonst von Schlossen, Schnee und Eis ein\u2019 ew\u2019ge B\u00fcrde", "tokens": ["Da", "sonst", "von", "Schlos\u00b7sen", ",", "Schnee", "und", "Eis", "ein'", "ew'\u00b7ge", "B\u00fcr\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "NN", "$,", "NN", "KON", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "In unsrer halben Welt die Lufft, die Fluth, das Land,", "tokens": ["In", "uns\u00b7rer", "hal\u00b7ben", "Welt", "die", "Lufft", ",", "die", "Fluth", ",", "das", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und, in der andern Helft\u2019, ein unl\u00f6schbarer Brand", "tokens": ["Und", ",", "in", "der", "an\u00b7dern", "Helft'", ",", "ein", "un\u00b7l\u00f6schba\u00b7rer", "Brand"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "APPR", "ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.9": {"text": "Lufft, Erd\u2019 und Fluth verderben w\u00fcrde.", "tokens": ["Lufft", ",", "Erd'", "und", "Fluth", "ver\u00b7der\u00b7ben", "w\u00fcr\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "VVFIN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Wo dieses ein Beweis von Weisheit und von Macht,", "tokens": ["Wo", "die\u00b7ses", "ein", "Be\u00b7weis", "von", "Weis\u00b7heit", "und", "von", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDAT", "ART", "NN", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von G\u00fct\u2019 und Liebe nicht zu nennen,", "tokens": ["Von", "G\u00fct'", "und", "Lie\u00b7be", "nicht", "zu", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und, wo darin die Gottheit nicht zu kennen,", "tokens": ["Und", ",", "wo", "da\u00b7rin", "die", "Got\u00b7theit", "nicht", "zu", "ken\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWAV", "PAV", "ART", "NN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Nicht anzubeten ist: so weis ich wahrlich nicht,", "tokens": ["Nicht", "an\u00b7zu\u00b7be\u00b7ten", "ist", ":", "so", "weis", "ich", "wahr\u00b7lich", "nicht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVIZU", "VAFIN", "$.", "ADV", "PTKVZ", "PPER", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zu welchem Endzweck doch, in diesem Leben,", "tokens": ["Zu", "wel\u00b7chem", "End\u00b7zweck", "doch", ",", "in", "die\u00b7sem", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "ADV", "$,", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Uns des Verstandes Licht", "tokens": ["Uns", "des", "Ver\u00b7stan\u00b7des", "Licht"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "NN", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Gegeben.", "tokens": ["Ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.6": {"line.1": {"text": "Bey dieser Wechsel-Zeit, in der ich, Dem zum Preise,", "tokens": ["Bey", "die\u00b7ser", "Wech\u00b7sel\u00b7Zeit", ",", "in", "der", "ich", ",", "Dem", "zum", "Prei\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$,", "APPR", "PRELS", "PPER", "$,", "ART", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der alles schuff und schafft, mich inniglich erfreu,", "tokens": ["Der", "al\u00b7les", "schuff", "und", "schafft", ",", "mich", "in\u00b7nig\u00b7lich", "er\u00b7freu", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADJD", "KON", "VVFIN", "$,", "PPER", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Besing\u2019 ich, Ehr-Furcht voll, den ietzt aufs neu,", "tokens": ["Be\u00b7sing'", "ich", ",", "Ehr\u00b7Furcht", "voll", ",", "den", "ietzt", "aufs", "neu", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$,", "NN", "ADJD", "$,", "PRELS", "ADV", "APPRART", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "In einer (wollte GOTT! Jhm angenehmen Weise)", "tokens": ["In", "ei\u00b7ner", "(", "woll\u00b7te", "GoTT", "!", "Jhm", "an\u00b7ge\u00b7neh\u00b7men", "Wei\u00b7se", ")"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$(", "VMFIN", "NE", "$.", "PPER", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Desselben Weisheit, Lieb\u2019 und Macht,", "tokens": ["Des\u00b7sel\u00b7ben", "Weis\u00b7heit", ",", "Lieb'", "und", "Macht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der, so wie alle Ding\u2019, auch mich hervor gebracht;", "tokens": ["Der", ",", "so", "wie", "al\u00b7le", "Ding'", ",", "auch", "mich", "her\u00b7vor", "ge\u00b7bracht", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "ADV", "KOKOM", "PIAT", "NN", "$,", "ADV", "PPER", "PTKVZ", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der, wie er so viel Guts, Leib, Sinnen, Kr\u00e4fft\u2019 und Leben,", "tokens": ["Der", ",", "wie", "er", "so", "viel", "Guts", ",", "Leib", ",", "Sin\u00b7nen", ",", "Kr\u00e4fft'", "und", "Le\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PPER", "ADV", "PIAT", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+--+-++-+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Auch eine Seele mir gegeben,", "tokens": ["Auch", "ei\u00b7ne", "See\u00b7le", "mir", "ge\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die, da\u00df sie alle Krafft auf ihren Sch\u00f6pfer richte,", "tokens": ["Die", ",", "da\u00df", "sie", "al\u00b7le", "Krafft", "auf", "ih\u00b7ren", "Sch\u00f6p\u00b7fer", "rich\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "KOUS", "PPER", "PIAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und, Jhm zum Ruhm, ein Lob- und Danck-Lied tichte,", "tokens": ["Und", ",", "Jhm", "zum", "Ruhm", ",", "ein", "Lob", "und", "Dan\u00b7ck\u00b7Lied", "tich\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PPER", "APPRART", "NN", "$,", "ART", "TRUNC", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.11": {"text": "So schuldig als bereit.", "tokens": ["So", "schul\u00b7dig", "als", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Allein,", "tokens": ["Al\u00b7lein", ","], "token_info": ["word", "punct"], "pos": ["ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Was wird der Vorwurff ietzt von meinen Liedern seyn?", "tokens": ["Was", "wird", "der", "Vor\u00b7wurff", "ietzt", "von", "mei\u00b7nen", "Lie\u00b7dern", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NN", "ADV", "APPR", "PPOSAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich habe von der ", "tokens": ["Ich", "ha\u00b7be", "von", "der"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Ich habe gleichfalls schon, wann keine Zeit nicht mehr,", "tokens": ["Ich", "ha\u00b7be", "gleich\u00b7falls", "schon", ",", "wann", "kei\u00b7ne", "Zeit", "nicht", "mehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$,", "PWAV", "PIAT", "NN", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das ", "tokens": ["Das"], "token_info": ["word"], "pos": ["PDS"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "Und dir, auch mir, zur Lehr,", "tokens": ["Und", "dir", ",", "auch", "mir", ",", "zur", "Lehr", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "ADV", "PPER", "$,", "APPRART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Einst anzusehn versucht. Es ist noch \u00fcberblieben,", "tokens": ["Einst", "an\u00b7zu\u00b7sehn", "ver\u00b7sucht", ".", "Es", "ist", "noch", "\u00fc\u00b7berb\u00b7lie\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIZU", "VVPP", "$.", "PPER", "VAFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Auch in das stille Thal der Dinge, so dahin,", "tokens": ["Auch", "in", "das", "stil\u00b7le", "Thal", "der", "Din\u00b7ge", ",", "so", "da\u00b7hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "ART", "NN", "$,", "ADV", "PAV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Mit einem forschenden und ernsten Sinn,", "tokens": ["Mit", "ei\u00b7nem", "for\u00b7schen\u00b7den", "und", "erns\u00b7ten", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "So tieff, als m\u00f6glich ist, der Seelen Krafft zu sencken,", "tokens": ["So", "tieff", ",", "als", "m\u00f6g\u00b7lich", "ist", ",", "der", "See\u00b7len", "Krafft", "zu", "sen\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "ADJD", "VAFIN", "$,", "ART", "NN", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und des ", "tokens": ["Und", "des"], "token_info": ["word", "word"], "pos": ["KON", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.12": {"text": "Als welches mehr, wenn man es wol erweget,", "tokens": ["Als", "wel\u00b7ches", "mehr", ",", "wenn", "man", "es", "wol", "er\u00b7we\u00b7get", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRELS", "ADV", "$,", "KOUS", "PIS", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Verdient und n\u00f6thig ist, da\u00df man es \u00fcberleget.", "tokens": ["Ver\u00b7di\u00b7ent", "und", "n\u00f6\u00b7thig", "ist", ",", "da\u00df", "man", "es", "\u00fc\u00b7ber\u00b7le\u00b7get", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ADJD", "VAFIN", "$,", "KOUS", "PIS", "PPER", "VVFIN", "$."], "meter": "+---+-+-+-+-+-", "measure": "dactylic.init"}, "line.14": {"text": "Da das Vergangne ja, weit mehr noch als es scheinet,", "tokens": ["Da", "das", "Ver\u00b7gang\u00b7ne", "ja", ",", "weit", "mehr", "noch", "als", "es", "schei\u00b7net", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKANT", "$,", "ADJD", "ADV", "ADV", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Mit unsrer Gegenwart des Lebens selbst vereinet:", "tokens": ["Mit", "uns\u00b7rer", "Ge\u00b7gen\u00b7wart", "des", "Le\u00b7bens", "selbst", "ver\u00b7ei\u00b7net", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Da ieder Augenblick uns, wie ein Vlitz, entflieht,", "tokens": ["Da", "ie\u00b7der", "Au\u00b7gen\u00b7blick", "uns", ",", "wie", "ein", "Vlitz", ",", "ent\u00b7flieht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "PPER", "$,", "PWAV", "ART", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Sich zum vergangenen gesellt, sich uns entzieht,", "tokens": ["Sich", "zum", "ver\u00b7gan\u00b7ge\u00b7nen", "ge\u00b7sellt", ",", "sich", "uns", "ent\u00b7zieht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "APPRART", "ADJA", "VVPP", "$,", "PRF", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Ja gar uns mit sich fort, indem wir stets vergehen,", "tokens": ["Ja", "gar", "uns", "mit", "sich", "fort", ",", "in\u00b7dem", "wir", "stets", "ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "PPER", "APPR", "PRF", "PTKVZ", "$,", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Mit ja so streng-als stillem Zwange reisst;", "tokens": ["Mit", "ja", "so", "streng\u00b7als", "stil\u00b7lem", "Zwan\u00b7ge", "reisst", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADV", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "So da\u00df daher, wenn wir es recht ergr\u00fcnden,", "tokens": ["So", "da\u00df", "da\u00b7her", ",", "wenn", "wir", "es", "recht", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PAV", "$,", "KOUS", "PPER", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.21": {"text": "Selbst von der gegenw\u00e4rtgen Zeit", "tokens": ["Selbst", "von", "der", "ge\u00b7gen\u00b7w\u00e4rt\u00b7gen", "Zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.22": {"text": "Die beste Deutlichkeit", "tokens": ["Die", "bes\u00b7te", "Deut\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.23": {"text": "Bey dem vergangenen zu finden.", "tokens": ["Bey", "dem", "ver\u00b7gan\u00b7ge\u00b7nen", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.8": {"line.1": {"text": "Betrachte denn, mit aufmercksamen Sinn,", "tokens": ["Be\u00b7trach\u00b7te", "denn", ",", "mit", "auf\u00b7merck\u00b7sa\u00b7men", "Sinn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dasjenige, was weg, vergangen, und dahin,", "tokens": ["Das\u00b7je\u00b7ni\u00b7ge", ",", "was", "weg", ",", "ver\u00b7gan\u00b7gen", ",", "und", "da\u00b7hin", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "$,", "PWS", "PTKVZ", "$,", "VVPP", "$,", "KON", "PAV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So viel dir m\u00f6glich ist, mein Geist!", "tokens": ["So", "viel", "dir", "m\u00f6g\u00b7lich", "ist", ",", "mein", "Geist", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADJD", "VAFIN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Weil aber von sich selbst kein Mensch, was wahr, verstehen", "tokens": ["Weil", "a\u00b7ber", "von", "sich", "selbst", "kein", "Mensch", ",", "was", "wahr", ",", "ver\u00b7ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["KOUS", "ADV", "APPR", "PRF", "ADV", "PIAT", "NN", "$,", "PWS", "ADJD", "$,", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und, was man eigentlich soll glauben, fassen kann;", "tokens": ["Und", ",", "was", "man", "ei\u00b7gent\u00b7lich", "soll", "glau\u00b7ben", ",", "fas\u00b7sen", "kann", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$,", "PRELS", "PIS", "ADV", "VMFIN", "VVINF", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "So ruff ich dich alhier, Quell aller Weisheit, an?", "tokens": ["So", "ruff", "ich", "dich", "al\u00b7hier", ",", "Quell", "al\u00b7ler", "Weis\u00b7heit", ",", "an", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADV", "$,", "NN", "PIAT", "NN", "$,", "PTKVZ", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "\u201eh\u00f6re, was hievon die Lippen, Dir anietzt zu Ehren", "tokens": ["\u201e", "h\u00f6\u00b7re", ",", "was", "hie\u00b7von", "die", "Lip\u00b7pen", ",", "Dir", "an\u00b7ietzt", "zu", "Eh\u00b7ren"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "VVFIN", "$,", "PWS", "PAV", "ART", "NN", "$,", "PPER", "ADV", "APPR", "NN"], "meter": "+--+--+-+-+-+-", "measure": "elegiambus"}}, "stanza.10": {"line.1": {"text": "Der nimmer stille Flu\u00df der Dinge, die zerst\u00f6rlich,", "tokens": ["Der", "nim\u00b7mer", "stil\u00b7le", "Flu\u00df", "der", "Din\u00b7ge", ",", "die", "zer\u00b7st\u00f6r\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "ART", "NN", "$,", "PRELS", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Entladet sich von sich, versenckt sich unaufh\u00f6rlich", "tokens": ["Ent\u00b7la\u00b7det", "sich", "von", "sich", ",", "ver\u00b7senckt", "sich", "un\u00b7auf\u00b7h\u00f6r\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PRF", "APPR", "PRF", "$,", "VVFIN", "PRF", "ADJD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In die Verwesungs-See, in das Zertrennungs-Meer,", "tokens": ["In", "die", "Ver\u00b7we\u00b7sungs\u00b7See", ",", "in", "das", "Zer\u00b7tren\u00b7nungs\u00b7Meer", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und bleibt doch stets erf\u00fcllt, und ist doch nimmer leer.", "tokens": ["Und", "bleibt", "doch", "stets", "er\u00b7f\u00fcllt", ",", "und", "ist", "doch", "nim\u00b7mer", "leer", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "VVPP", "$,", "KON", "VAFIN", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Von neuen Tropffen wird, in stetigem Gedr\u00e4nge,", "tokens": ["Von", "neu\u00b7en", "Tropf\u00b7fen", "wird", ",", "in", "ste\u00b7ti\u00b7gem", "Ge\u00b7dr\u00e4n\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der ietzt an diesem Ort verhandnen Tropffen Menge", "tokens": ["Der", "ietzt", "an", "die\u00b7sem", "Ort", "ver\u00b7hand\u00b7nen", "Tropf\u00b7fen", "Men\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "PDAT", "NN", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Jm Meer stets weggedr\u00fcckt: so kommt und so vergehet,", "tokens": ["Jm", "Meer", "stets", "weg\u00b7ge\u00b7dr\u00fcckt", ":", "so", "kommt", "und", "so", "ver\u00b7ge\u00b7het", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "VVPP", "$.", "ADV", "VVFIN", "KON", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Erzeugt sich, l\u00f6s\u2019t sich auf, entwickelt sich, entstehet,", "tokens": ["Er\u00b7zeugt", "sich", ",", "l\u00f6s't", "sich", "auf", ",", "ent\u00b7wi\u00b7ckelt", "sich", ",", "ent\u00b7ste\u00b7het", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PRF", "$,", "VVFIN", "PRF", "PTKVZ", "$,", "VVFIN", "PRF", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Was die Natur hervor bringt und formirt,", "tokens": ["Was", "die", "Na\u00b7tur", "her\u00b7vor", "bringt", "und", "for\u00b7mirt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "PTKVZ", "VVFIN", "KON", "VVPP", "$,"], "meter": "-+-+-++-++", "measure": "zehnsilber"}, "line.10": {"text": "Auch was sich wieder\u00fcm, durch sie zertheilt, verliert.", "tokens": ["Auch", "was", "sich", "wie\u00b7de\u00b7r\u00fcm", ",", "durch", "sie", "zer\u00b7theilt", ",", "ver\u00b7liert", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PRELS", "PRF", "PTKVZ", "$,", "APPR", "PPER", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Die unbest\u00e4ndige Best\u00e4ndigkeit auf Erden", "tokens": ["Die", "un\u00b7be\u00b7st\u00e4n\u00b7di\u00b7ge", "Be\u00b7st\u00e4n\u00b7dig\u00b7keit", "auf", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kann wol mit allem Recht uns vorgestellet werden", "tokens": ["Kann", "wol", "mit", "al\u00b7lem", "Recht", "uns", "vor\u00b7ge\u00b7stel\u00b7let", "wer\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "APPR", "PIS", "NN", "PPER", "VVPP", "VAINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als eine Kette, die durch Ringe", "tokens": ["Als", "ei\u00b7ne", "Ket\u00b7te", ",", "die", "durch", "Rin\u00b7ge"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "$,", "PRELS", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Fest an einander hinge;", "tokens": ["Fest", "an", "ein\u00b7an\u00b7der", "hin\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Doch die man, Wechsels-weis\u2019, in solcher Ordnung sieht,", "tokens": ["Doch", "die", "man", ",", "Wech\u00b7sels\u00b7weis'", ",", "in", "sol\u00b7cher", "Ord\u00b7nung", "sieht", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PIS", "$,", "NN", "$,", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df das Entstehen das Vergehen,", "tokens": ["Da\u00df", "das", "Ent\u00b7ste\u00b7hen", "das", "Ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.7": {"text": "Und das Vergehen das Entstehen", "tokens": ["Und", "das", "Ver\u00b7ge\u00b7hen", "das", "Ent\u00b7ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Best\u00e4ndig vorw\u00e4rts zieht.", "tokens": ["Be\u00b7st\u00e4n\u00b7dig", "vor\u00b7w\u00e4rts", "zieht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Wenn nun ein Mensch, dem GOTT Vernunst ge", "tokens": ["Wenn", "nun", "ein", "Mensch", ",", "dem", "GoTT", "Ver\u00b7nunst", "ge"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "$,", "ART", "NN", "NN", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Wesen, seine Daur,", "tokens": ["Sein", "We\u00b7sen", ",", "sei\u00b7ne", "Daur", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bey folchem Zustand \u00fcberdencket,", "tokens": ["Bey", "fol\u00b7chem", "Zu\u00b7stand", "\u00fc\u00b7ber\u00b7den\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie pl\u00f6tzlich die Verg\u00e4nglichkeit", "tokens": ["Wie", "pl\u00f6tz\u00b7lich", "die", "Ver\u00b7g\u00e4ng\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Jhn selbst mit sich dahin zu raffen, dreut;", "tokens": ["Jhn", "selbst", "mit", "sich", "da\u00b7hin", "zu", "raf\u00b7fen", ",", "dreut", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "ADV", "APPR", "PRF", "PAV", "PTKZU", "VVINF", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Da fast kein Wind so schnell, als er, verwehet,", "tokens": ["Da", "fast", "kein", "Wind", "so", "schnell", ",", "als", "er", ",", "ver\u00b7we\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "PIAT", "NN", "ADV", "ADJD", "$,", "KOUS", "PPER", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Da fast kein Dampff so schnell, als er, vergehet,", "tokens": ["Da", "fast", "kein", "Dampff", "so", "schnell", ",", "als", "er", ",", "ver\u00b7ge\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "PIAT", "NN", "ADV", "ADJD", "$,", "KOUS", "PPER", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Da sich das k\u00fcnftge, fast mit dem vergangnen bindet,", "tokens": ["Da", "sich", "das", "k\u00fcnft\u00b7ge", ",", "fast", "mit", "dem", "ver\u00b7gang\u00b7nen", "bin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "ADJA", "$,", "ADV", "APPR", "ART", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und man die Gegenwart kaum kaum dazwischen findet;", "tokens": ["Und", "man", "die", "Ge\u00b7gen\u00b7wart", "kaum", "kaum", "da\u00b7zwi\u00b7schen", "fin\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ART", "NN", "ADV", "ADV", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Sollt\u2019er denn nicht mit Recht bey st\u00e4tigem Vergange", "tokens": ["Sollt'\u00b7er", "denn", "nicht", "mit", "Recht", "bey", "st\u00e4\u00b7ti\u00b7gem", "Ver\u00b7gan\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "PTKNEG", "APPR", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Der Creatur, von dem Zusammenhange,", "tokens": ["Der", "Crea\u00b7tur", ",", "von", "dem", "Zu\u00b7sam\u00b7men\u00b7han\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.12": {"text": "Den das Vergangene mit dem Vergehnden hat,", "tokens": ["Den", "das", "Ver\u00b7gan\u00b7ge\u00b7ne", "mit", "dem", "Ver\u00b7gehn\u00b7den", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "APPR", "ART", "NN", "VAFIN", "$,"], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.13": {"text": "Bem\u00fcht seyn etwas zu erwegen?", "tokens": ["Be\u00b7m\u00fcht", "seyn", "et\u00b7was", "zu", "er\u00b7we\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAINF", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Und, ob das, was in seinem Sinn", "tokens": ["Und", ",", "ob", "das", ",", "was", "in", "sei\u00b7nem", "Sinn"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "$,", "KOUS", "PDS", "$,", "PRELS", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Vergangen und dahin,", "tokens": ["Ver\u00b7gan\u00b7gen", "und", "da\u00b7hin", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "PAV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.16": {"text": "Auch w\u00fcrcklich sey vergangen, \u00fcberlegen?", "tokens": ["Auch", "w\u00fcrck\u00b7lich", "sey", "ver\u00b7gan\u00b7gen", ",", "\u00fc\u00b7berl\u00b7e\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "VVPP", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Indem ich dieses schreib\u2019, heisst es bereits: ich schrieb;", "tokens": ["In\u00b7dem", "ich", "die\u00b7ses", "schreib'", ",", "heisst", "es", "be\u00b7reits", ":", "ich", "schrieb", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "$.", "PPER", "VVFIN", "$."], "meter": "-+-+-+---+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Indem du dieses lie\u00dft, so hast du schon gelesen;", "tokens": ["In\u00b7dem", "du", "die\u00b7ses", "lie\u00dft", ",", "so", "hast", "du", "schon", "ge\u00b7le\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "VVFIN", "$,", "ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Du bist nicht mehr anietzt das, was du noch gewesen", "tokens": ["Du", "bist", "nicht", "mehr", "an\u00b7ietzt", "das", ",", "was", "du", "noch", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADV", "VVFIN", "PDS", "$,", "PWS", "PPER", "ADV", "VAPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "In vor\u2019gem Augenblick, der ietz\u2019ge Puls-Schlag rieb", "tokens": ["In", "vor'\u00b7gem", "Au\u00b7gen\u00b7blick", ",", "der", "ietz'\u00b7ge", "Puls\u00b7Schlag", "rieb"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Von deinem Wesen was. Da nun nichts feste stehet,", "tokens": ["Von", "dei\u00b7nem", "We\u00b7sen", "was", ".", "Da", "nun", "nichts", "fes\u00b7te", "ste\u00b7het", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PWS", "$.", "ADV", "ADV", "PIS", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und aller Menschen Jetzt all\u2019 Augenblick vergehet,", "tokens": ["Und", "al\u00b7ler", "Men\u00b7schen", "Jetzt", "all'", "Au\u00b7gen\u00b7blick", "ver\u00b7ge\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So sollte man ja wol mit recht ein ernstlichs dencken", "tokens": ["So", "soll\u00b7te", "man", "ja", "wol", "mit", "recht", "ein", "ernst\u00b7lichs", "den\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "ADV", "APPR", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Dem, was, nach kurtzem Jetzt, so lang nicht mehr ist, schencken.", "tokens": ["Dem", ",", "was", ",", "nach", "kurt\u00b7zem", "Jetzt", ",", "so", "lang", "nicht", "mehr", "ist", ",", "schen\u00b7cken", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "$,", "PRELS", "$,", "APPR", "ADJA", "ADV", "$,", "ADV", "ADJD", "PTKNEG", "ADV", "VAFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Was ist es eigentlich, was wir vergangen nennen?", "tokens": ["Was", "ist", "es", "ei\u00b7gent\u00b7lich", ",", "was", "wir", "ver\u00b7gan\u00b7gen", "nen\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "ADV", "$,", "PRELS", "PPER", "VVPP", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein Etwas, so wir nicht mehr h\u00f6ren, sehn,", "tokens": ["Ein", "Et\u00b7was", ",", "so", "wir", "nicht", "mehr", "h\u00f6\u00b7ren", ",", "sehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADV", "$,", "ADV", "PPER", "PTKNEG", "ADV", "VVINF", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Empfinden, riechen, schmecken k\u00f6nnen.", "tokens": ["Emp\u00b7fin\u00b7den", ",", "rie\u00b7chen", ",", "schme\u00b7cken", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$,", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Tage, die dahin, die Zeiten, die vergehn,", "tokens": ["Die", "Ta\u00b7ge", ",", "die", "da\u00b7hin", ",", "die", "Zei\u00b7ten", ",", "die", "ver\u00b7gehn", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PAV", "$,", "ART", "NN", "$,", "PRELS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Sind Sachen ebenfalls, wovon man glaubt zu fassen,", "tokens": ["Sind", "Sa\u00b7chen", "e\u00b7ben\u00b7falls", ",", "wo\u00b7von", "man", "glaubt", "zu", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "$,", "PWAV", "PIS", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df sie nicht mehr, und da\u00df sie uns verlassen.", "tokens": ["Da\u00df", "sie", "nicht", "mehr", ",", "und", "da\u00df", "sie", "uns", "ver\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "$,", "KON", "KOUS", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Von C\u00f6rpern, wann sie sich ver\u00e4ndern, sprechen wir,", "tokens": ["Von", "C\u00f6r\u00b7pern", ",", "wann", "sie", "sich", "ver\u00b7\u00e4n\u00b7dern", ",", "spre\u00b7chen", "wir", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "PWAV", "PPER", "PRF", "VVINF", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df, da sie aufgel\u00f6s\u2019t, ihr Wesen sich verlier.", "tokens": ["Da\u00df", ",", "da", "sie", "auf\u00b7ge\u00b7l\u00f6s't", ",", "ihr", "We\u00b7sen", "sich", "ver\u00b7lier", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "KOUS", "PPER", "VVFIN", "$,", "PPOSAT", "NN", "PRF", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wenn Regiment\u2019 und Reiche, die entstanden,", "tokens": ["Wenn", "Re\u00b7gi\u00b7ment'", "und", "Rei\u00b7che", ",", "die", "ent\u00b7stan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NE", "$,", "PRELS", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Durch Zuf\u00e4ll\u2019 untergehn; sie sind nicht mehr verhanden,", "tokens": ["Durch", "Zu\u00b7f\u00e4ll'", "un\u00b7ter\u00b7gehn", ";", "sie", "sind", "nicht", "mehr", "ver\u00b7han\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$.", "PPER", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Spricht ieder, sie sind fort. Was Menschen ie gethan,", "tokens": ["Spricht", "ie\u00b7der", ",", "sie", "sind", "fort", ".", "Was", "Men\u00b7schen", "ie", "ge\u00b7than", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "PPER", "VAFIN", "PTKVZ", "$.", "PWS", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Erzeuget, ausgewirckt, zerst\u00f6rt, erbaut, erdacht,", "tokens": ["Er\u00b7zeu\u00b7get", ",", "aus\u00b7ge\u00b7wirckt", ",", "zer\u00b7st\u00f6rt", ",", "er\u00b7baut", ",", "er\u00b7dacht", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVPP", "$,", "VVPP", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Gef\u00fcget und getrennt, begonnen und vollbracht,", "tokens": ["Ge\u00b7f\u00fc\u00b7get", "und", "ge\u00b7trennt", ",", "be\u00b7gon\u00b7nen", "und", "voll\u00b7bracht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "$,", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Sieht man, wann es vorbey, nicht anders an,", "tokens": ["Sieht", "man", ",", "wann", "es", "vor\u00b7bey", ",", "nicht", "an\u00b7ders", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "PWAV", "PPER", "PTKVZ", "$,", "PTKNEG", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Als zeigte sich davon, im Schoosse der Natur,", "tokens": ["Als", "zeig\u00b7te", "sich", "da\u00b7von", ",", "im", "Schoos\u00b7se", "der", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PRF", "PAV", "$,", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Nicht die geringste Spur.", "tokens": ["Nicht", "die", "ge\u00b7rings\u00b7te", "Spur", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}