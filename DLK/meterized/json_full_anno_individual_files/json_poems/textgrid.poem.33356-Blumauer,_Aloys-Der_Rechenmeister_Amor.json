{"textgrid.poem.33356": {"metadata": {"author": {"name": "Blumauer, Aloys", "birth": "N.A.", "death": "N.A."}, "title": "Der Rechenmeister Amor", "genre": "verse", "period": "N.A.", "pub_year": 1776, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Tausendk\u00fcnstler Amor lie\u00df", "tokens": ["Der", "Tau\u00b7send\u00b7k\u00fcnst\u00b7ler", "A\u00b7mor", "lie\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich bei der jungen Dorilis", "tokens": ["Sich", "bei", "der", "jun\u00b7gen", "Do\u00b7ri\u00b7lis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "ADJA", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum Rechenmeister dingen,", "tokens": ["Zum", "Re\u00b7chen\u00b7meis\u00b7ter", "din\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und wu\u00dft' in einer Stunde da", "tokens": ["Und", "wu\u00dft'", "in", "ei\u00b7ner", "Stun\u00b7de", "da"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die ganze Arithmetika", "tokens": ["Die", "gan\u00b7ze", "A\u00b7rith\u00b7me\u00b7ti\u00b7ka"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ihr spielend beizubringen,", "tokens": ["Ihr", "spie\u00b7lend", "bei\u00b7zu\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVIZU", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Im Rechnen und im Lieben sind", "tokens": ["Im", "Rech\u00b7nen", "und", "im", "Lie\u00b7ben", "sind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "KON", "APPRART", "ADJA", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcnf Species, mein sch\u00f6nes Kind,", "tokens": ["F\u00fcnf", "Spe\u00b7cies", ",", "mein", "sch\u00f6\u00b7nes", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die will ich dich dociren:", "tokens": ["Die", "will", "ich", "dich", "do\u00b7ci\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ich k\u00fcsse dich \u2013 ein \u2013- zwei \u2013 dreimal,", "tokens": ["Ich", "k\u00fcs\u00b7se", "dich", "\u2013", "ein", "\u2013", "zwei", "\u2013", "drei\u00b7mal", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$(", "ART", "$(", "$(", "CARD", "$(", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Du z\u00e4hlest diese K\u00fc\u00dfchen all,", "tokens": ["Du", "z\u00e4h\u00b7lest", "die\u00b7se", "K\u00fc\u00df\u00b7chen", "all", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "PIAT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und das hei\u00dft Numeriren.", "tokens": ["Und", "das", "hei\u00dft", "Nu\u00b7me\u00b7ri\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Zu meinen K\u00fcssen setzest du", "tokens": ["Zu", "mei\u00b7nen", "K\u00fcs\u00b7sen", "set\u00b7zest", "du"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann auch die deinigen hinzu,", "tokens": ["Dann", "auch", "die", "dei\u00b7ni\u00b7gen", "hin\u00b7zu", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "PPOSS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So lernest du Addiren:", "tokens": ["So", "ler\u00b7nest", "du", "Ad\u00b7di\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Z\u00e4hlst du mir deine K\u00fc\u00dfchen her,", "tokens": ["Z\u00e4hlst", "du", "mir", "dei\u00b7ne", "K\u00fc\u00df\u00b7chen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und findest dann um einen mehr:", "tokens": ["Und", "fin\u00b7dest", "dann", "um", "ei\u00b7nen", "mehr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "ART", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So kannst du Subtrahiren.", "tokens": ["So", "kannst", "du", "Sub\u00b7tra\u00b7hi\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Die vierte Species, mein Kind,", "tokens": ["Die", "vier\u00b7te", "Spe\u00b7cies", ",", "mein", "Kind", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "K\u00f6nnt' ich zwar eben so geschwind", "tokens": ["K\u00f6nnt'", "ich", "zwar", "e\u00b7ben", "so", "ge\u00b7schwind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dir praktisch expliciren;", "tokens": ["Dir", "prak\u00b7tisch", "ex\u00b7pli\u00b7ci\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Allein das Einmaleins ist lang,", "tokens": ["Al\u00b7lein", "das", "Ein\u00b7mal\u00b7eins", "ist", "lang", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und jungen M\u00e4dchen wird oft bang", "tokens": ["Und", "jun\u00b7gen", "M\u00e4d\u00b7chen", "wird", "oft", "bang"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Vor dem Multipliciren;", "tokens": ["Vor", "dem", "Mul\u00b7ti\u00b7pli\u00b7ci\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Doch k\u00e4m' ein Nullchen noch hinzu \u2013", "tokens": ["Doch", "k\u00e4m'", "ein", "Null\u00b7chen", "noch", "hin\u00b7zu", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Auch noch so klein \u2013 so w\u00fcrdest du", "tokens": ["Auch", "noch", "so", "klein", "\u2013", "so", "w\u00fcr\u00b7dest", "du"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "ADV", "ADJD", "$(", "ADV", "VAFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Gar bald das Faktum sp\u00fcren.", "tokens": ["Gar", "bald", "das", "Fak\u00b7tum", "sp\u00fc\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "D'rum la\u00df in dieser Specie", "tokens": ["D'\u00b7rum", "la\u00df", "in", "die\u00b7ser", "Spe\u00b7cie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "APPR", "PDAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Nicht fr\u00fcher dich, als in der Eh',", "tokens": ["Nicht", "fr\u00fc\u00b7her", "dich", ",", "als", "in", "der", "Eh'", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "PPER", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+++-+", "measure": "unknown.measure.penta"}, "line.3": {"text": "Durch Hymen instruiren;", "tokens": ["Durch", "Hy\u00b7men", "inst\u00b7ru\u00b7i\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Denn auf's Multipliciren k\u00f6mmt,", "tokens": ["Denn", "auf's", "Mul\u00b7ti\u00b7pli\u00b7ci\u00b7ren", "k\u00f6mmt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Wie man sich auch dagegen stemmt,", "tokens": ["Wie", "man", "sich", "auch", "da\u00b7ge\u00b7gen", "stemmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "ADV", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Von selbst das Dividiren.", "tokens": ["Von", "selbst", "das", "Di\u00b7vi\u00b7di\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Der Tausendk\u00fcnstler Amor lie\u00df", "tokens": ["Der", "Tau\u00b7send\u00b7k\u00fcnst\u00b7ler", "A\u00b7mor", "lie\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich bei der jungen Dorilis", "tokens": ["Sich", "bei", "der", "jun\u00b7gen", "Do\u00b7ri\u00b7lis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "ADJA", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum Rechenmeister dingen,", "tokens": ["Zum", "Re\u00b7chen\u00b7meis\u00b7ter", "din\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und wu\u00dft' in einer Stunde da", "tokens": ["Und", "wu\u00dft'", "in", "ei\u00b7ner", "Stun\u00b7de", "da"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die ganze Arithmetika", "tokens": ["Die", "gan\u00b7ze", "A\u00b7rith\u00b7me\u00b7ti\u00b7ka"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ihr spielend beizubringen,", "tokens": ["Ihr", "spie\u00b7lend", "bei\u00b7zu\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVIZU", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Im Rechnen und im Lieben sind", "tokens": ["Im", "Rech\u00b7nen", "und", "im", "Lie\u00b7ben", "sind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "KON", "APPRART", "ADJA", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcnf Species, mein sch\u00f6nes Kind,", "tokens": ["F\u00fcnf", "Spe\u00b7cies", ",", "mein", "sch\u00f6\u00b7nes", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die will ich dich dociren:", "tokens": ["Die", "will", "ich", "dich", "do\u00b7ci\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ich k\u00fcsse dich \u2013 ein \u2013- zwei \u2013 dreimal,", "tokens": ["Ich", "k\u00fcs\u00b7se", "dich", "\u2013", "ein", "\u2013", "zwei", "\u2013", "drei\u00b7mal", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$(", "ART", "$(", "$(", "CARD", "$(", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Du z\u00e4hlest diese K\u00fc\u00dfchen all,", "tokens": ["Du", "z\u00e4h\u00b7lest", "die\u00b7se", "K\u00fc\u00df\u00b7chen", "all", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "PIAT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und das hei\u00dft Numeriren.", "tokens": ["Und", "das", "hei\u00dft", "Nu\u00b7me\u00b7ri\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Zu meinen K\u00fcssen setzest du", "tokens": ["Zu", "mei\u00b7nen", "K\u00fcs\u00b7sen", "set\u00b7zest", "du"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann auch die deinigen hinzu,", "tokens": ["Dann", "auch", "die", "dei\u00b7ni\u00b7gen", "hin\u00b7zu", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "PPOSS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So lernest du Addiren:", "tokens": ["So", "ler\u00b7nest", "du", "Ad\u00b7di\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Z\u00e4hlst du mir deine K\u00fc\u00dfchen her,", "tokens": ["Z\u00e4hlst", "du", "mir", "dei\u00b7ne", "K\u00fc\u00df\u00b7chen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und findest dann um einen mehr:", "tokens": ["Und", "fin\u00b7dest", "dann", "um", "ei\u00b7nen", "mehr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "ART", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So kannst du Subtrahiren.", "tokens": ["So", "kannst", "du", "Sub\u00b7tra\u00b7hi\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Die vierte Species, mein Kind,", "tokens": ["Die", "vier\u00b7te", "Spe\u00b7cies", ",", "mein", "Kind", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "K\u00f6nnt' ich zwar eben so geschwind", "tokens": ["K\u00f6nnt'", "ich", "zwar", "e\u00b7ben", "so", "ge\u00b7schwind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dir praktisch expliciren;", "tokens": ["Dir", "prak\u00b7tisch", "ex\u00b7pli\u00b7ci\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Allein das Einmaleins ist lang,", "tokens": ["Al\u00b7lein", "das", "Ein\u00b7mal\u00b7eins", "ist", "lang", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und jungen M\u00e4dchen wird oft bang", "tokens": ["Und", "jun\u00b7gen", "M\u00e4d\u00b7chen", "wird", "oft", "bang"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Vor dem Multipliciren;", "tokens": ["Vor", "dem", "Mul\u00b7ti\u00b7pli\u00b7ci\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Doch k\u00e4m' ein Nullchen noch hinzu \u2013", "tokens": ["Doch", "k\u00e4m'", "ein", "Null\u00b7chen", "noch", "hin\u00b7zu", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Auch noch so klein \u2013 so w\u00fcrdest du", "tokens": ["Auch", "noch", "so", "klein", "\u2013", "so", "w\u00fcr\u00b7dest", "du"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "ADV", "ADJD", "$(", "ADV", "VAFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Gar bald das Faktum sp\u00fcren.", "tokens": ["Gar", "bald", "das", "Fak\u00b7tum", "sp\u00fc\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "D'rum la\u00df in dieser Specie", "tokens": ["D'\u00b7rum", "la\u00df", "in", "die\u00b7ser", "Spe\u00b7cie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "APPR", "PDAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Nicht fr\u00fcher dich, als in der Eh',", "tokens": ["Nicht", "fr\u00fc\u00b7her", "dich", ",", "als", "in", "der", "Eh'", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "PPER", "$,", "KOUS", "APPR", "ART", "NN", "$,"], "meter": "-+-+++-+", "measure": "unknown.measure.penta"}, "line.3": {"text": "Durch Hymen instruiren;", "tokens": ["Durch", "Hy\u00b7men", "inst\u00b7ru\u00b7i\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Denn auf's Multipliciren k\u00f6mmt,", "tokens": ["Denn", "auf's", "Mul\u00b7ti\u00b7pli\u00b7ci\u00b7ren", "k\u00f6mmt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Wie man sich auch dagegen stemmt,", "tokens": ["Wie", "man", "sich", "auch", "da\u00b7ge\u00b7gen", "stemmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "ADV", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Von selbst das Dividiren.", "tokens": ["Von", "selbst", "das", "Di\u00b7vi\u00b7di\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}