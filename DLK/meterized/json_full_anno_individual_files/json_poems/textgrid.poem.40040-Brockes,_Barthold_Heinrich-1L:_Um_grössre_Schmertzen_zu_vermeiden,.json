{"textgrid.poem.40040": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: Um gr\u00f6ssre Schmertzen zu vermeiden,", "genre": "verse", "period": "N.A.", "pub_year": 1713, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Um gr\u00f6ssre Schmertzen zu vermeiden,", "tokens": ["Um", "gr\u00f6ss\u00b7re", "Schmert\u00b7zen", "zu", "ver\u00b7mei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Entschlo\u00df ich mich, da\u00df mir ein Zahn,", "tokens": ["Ent\u00b7schlo\u00df", "ich", "mich", ",", "da\u00df", "mir", "ein", "Zahn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "$,", "KOUS", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der mir bishero weh gethan,", "tokens": ["Der", "mir", "bis\u00b7he\u00b7ro", "weh", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "--+-++-+", "measure": "anapaest.init"}, "line.4": {"text": "W\u00fcrd' ausgebrochen, zu erleiden.", "tokens": ["W\u00fcrd'", "aus\u00b7ge\u00b7bro\u00b7chen", ",", "zu", "er\u00b7lei\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Weil aber die Natur, bey starcken Gliedern,", "tokens": ["Weil", "a\u00b7ber", "die", "Na\u00b7tur", ",", "bey", "star\u00b7cken", "Glie\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "(so ich dem Sch\u00f6pfer nie durch Danck kann gnug erwiedern)", "tokens": ["(", "so", "ich", "dem", "Sch\u00f6p\u00b7fer", "nie", "durch", "Danck", "kann", "gnug", "er\u00b7wie\u00b7dern", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "PPER", "ART", "NN", "ADV", "APPR", "NN", "VMFIN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Auch starcke Z\u00e4hne mir verliehn;", "tokens": ["Auch", "star\u00b7cke", "Z\u00e4h\u00b7ne", "mir", "ver\u00b7liehn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So schien es erst, als ob, ihn auszuziehn,", "tokens": ["So", "schien", "es", "erst", ",", "als", "ob", ",", "ihn", "aus\u00b7zu\u00b7ziehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "KOKOM", "KOUS", "$,", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der kluge Carpser selbst, der an Geschicklichkeit", "tokens": ["Der", "klu\u00b7ge", "Carp\u00b7ser", "selbst", ",", "der", "an", "Ge\u00b7schick\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADV", "$,", "PRELS", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Kaum seines gleichen kennt, sich etwas scheut'; allein,", "tokens": ["Kaum", "sei\u00b7nes", "glei\u00b7chen", "kennt", ",", "sich", "et\u00b7was", "scheut'", ";", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "VVFIN", "$,", "PRF", "ADV", "VVFIN", "$.", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Weil ich darauf bestund, war er dazu bereit.", "tokens": ["Weil", "ich", "da\u00b7rauf", "be\u00b7stund", ",", "war", "er", "da\u00b7zu", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PAV", "ADJD", "$,", "VAFIN", "PPER", "PAV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ich nahm mir vor, die strenge Pein,", "tokens": ["Ich", "nahm", "mir", "vor", ",", "die", "stren\u00b7ge", "Pein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ohn' alles Zucken, sonder Schreyn", "tokens": ["Ohn'", "al\u00b7les", "Zu\u00b7cken", ",", "son\u00b7der", "Schreyn"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIAT", "NN", "$,", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Behertzt und standhaft auszustehen.", "tokens": ["Be\u00b7hertzt", "und", "stand\u00b7haft", "aus\u00b7zu\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er setze drauf den Pelican,", "tokens": ["Er", "set\u00b7ze", "drauf", "den", "Pe\u00b7li\u00b7can", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Den ich vorhero wohl besehen,", "tokens": ["Den", "ich", "vor\u00b7he\u00b7ro", "wohl", "be\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Mit Kraft und Vorsicht an.", "tokens": ["Mit", "Kraft", "und", "Vor\u00b7sicht", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Wir hielten uns im Anfang beyde gut:", "tokens": ["Wir", "hiel\u00b7ten", "uns", "im", "An\u00b7fang", "bey\u00b7de", "gut", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "NN", "PIS", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Er brach; ich hielte fest, noch fester doch der Zahn.", "tokens": ["Er", "brach", ";", "ich", "hiel\u00b7te", "fest", ",", "noch", "fes\u00b7ter", "doch", "der", "Zahn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VVFIN", "PTKVZ", "$,", "ADV", "ADJD", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er knackt', ich wiche nicht. Doch endlich war mein Muth", "tokens": ["Er", "knackt'", ",", "ich", "wi\u00b7che", "nicht", ".", "Doch", "end\u00b7lich", "war", "mein", "Muth"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PTKNEG", "$.", "KON", "ADV", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Noch eher, als der Zahn, gebrochen.", "tokens": ["Noch", "e\u00b7her", ",", "als", "der", "Zahn", ",", "ge\u00b7bro\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "ART", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Es ri\u00df ein gr\u00e4\u00dfliches Gekrach,", "tokens": ["Es", "ri\u00df", "ein", "gr\u00e4\u00df\u00b7li\u00b7ches", "Ge\u00b7krach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Wodurch des gantzen Hauptes Knochen", "tokens": ["Wo\u00b7durch", "des", "gant\u00b7zen", "Haup\u00b7tes", "Kno\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Zu spalten schien, ein kurtz doch kl\u00e4glich Ach", "tokens": ["Zu", "spal\u00b7ten", "schien", ",", "ein", "kurtz", "doch", "kl\u00e4g\u00b7lich", "Ach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "VVFIN", "$,", "ART", "ADJD", "ADV", "ADJD", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Mir aus der Brust. Die feurig-wilde Pein,", "tokens": ["Mir", "aus", "der", "Brust", ".", "Die", "feu\u00b7rig\u00b7wil\u00b7de", "Pein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "$.", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.15": {"text": "Der bittre Schmertz, durchdrang so Fleisch, als Bein.", "tokens": ["Der", "bitt\u00b7re", "Schmertz", ",", "durch\u00b7drang", "so", "Fleisch", ",", "als", "Bein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVFIN", "ADV", "NN", "$,", "KOUS", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Die\u00df splittert', jenes ri\u00df, jedoch, zu meinem Leide,", "tokens": ["Die\u00df", "split\u00b7tert'", ",", "je\u00b7nes", "ri\u00df", ",", "je\u00b7doch", ",", "zu", "mei\u00b7nem", "Lei\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PDS", "VVFIN", "$,", "ADV", "$,", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Kein eintzigs gantz entzwey;", "tokens": ["Kein", "eint\u00b7zigs", "gantz", "ent\u00b7zwey", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.18": {"text": "Der Sehnen Z\u00e4higkeit band sie noch alle beyde.", "tokens": ["Der", "Seh\u00b7nen", "Z\u00e4\u00b7hig\u00b7keit", "band", "sie", "noch", "al\u00b7le", "bey\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "PPER", "ADV", "PIAT", "PIS", "$."], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.4": {"line.1": {"text": "Den meist gel\u00f6sten Zahn ergriff der Artzt aufs neu',", "tokens": ["Den", "meist", "ge\u00b7l\u00f6s\u00b7ten", "Zahn", "er\u00b7griff", "der", "Artzt", "aufs", "neu'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "VVFIN", "ART", "NN", "APPRART", "ADJA", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und ich, vor Unmuth Muth. Er w\u00e4hlt' aus zweyen B\u00f6sen", "tokens": ["Und", "ich", ",", "vor", "Un\u00b7muth", "Muth", ".", "Er", "w\u00e4hlt'", "aus", "zwe\u00b7yen", "B\u00f6\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "$,", "APPR", "NN", "NN", "$.", "PPER", "VVFIN", "APPR", "VVFIN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das kleinest', und fing an, das Zahn-Fleisch abzul\u00f6sen.", "tokens": ["Das", "klei\u00b7nest'", ",", "und", "fing", "an", ",", "das", "Zahn\u00b7Fleisch", "ab\u00b7zu\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KON", "VVFIN", "PTKVZ", "$,", "ART", "NN", "VVIZU", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Ob ich nun gleich die scharfen Schmerzten f\u00fchlte,", "tokens": ["Ob", "ich", "nun", "gleich", "die", "schar\u00b7fen", "Schmerz\u00b7ten", "f\u00fchl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Wie er mir dazumahl in frischer Wunde w\u00fchlte,", "tokens": ["Wie", "er", "mir", "da\u00b7zu\u00b7mahl", "in", "fri\u00b7scher", "Wun\u00b7de", "w\u00fchl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie er das Fleisch zerschnitt; so wirckete jdoch", "tokens": ["Wie", "er", "das", "Fleisch", "zer\u00b7schnitt", ";", "so", "wir\u00b7cke\u00b7te", "jdoch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "PPER", "ART", "NN", "VVFIN", "$.", "ADV", "VVFIN", "ADV"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.7": {"text": "Der noch weit gr\u00f6ssre Schmertz, den, wie es so gekracht,", "tokens": ["Der", "noch", "weit", "gr\u00f6ss\u00b7re", "Schmertz", ",", "den", ",", "wie", "es", "so", "ge\u00b7kracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "ADJA", "NN", "$,", "ART", "$,", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.8": {"text": "Der Bruch mir kurtz vorher gemacht,", "tokens": ["Der", "Bruch", "mir", "kurtz", "vor\u00b7her", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Zusamt der Furcht, es w\u00fcrd' annoch", "tokens": ["Zu\u00b7samt", "der", "Furcht", ",", "es", "w\u00fcrd'", "an\u00b7noch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "ART", "NN", "$,", "PPER", "VAFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Dergleichen gr\u00e4\u00dfliches Geknirsch von neuem kommen,", "tokens": ["Derg\u00b7lei\u00b7chen", "gr\u00e4\u00df\u00b7li\u00b7ches", "Ge\u00b7knirsch", "von", "neu\u00b7em", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJA", "NN", "APPR", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Da\u00df ich die Pein des Schnitts, wie herbe sie auch war,", "tokens": ["Da\u00df", "ich", "die", "Pein", "des", "Schnitts", ",", "wie", "her\u00b7be", "sie", "auch", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "$,", "PWAV", "VVFIN", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Doch nicht so gar", "tokens": ["Doch", "nicht", "so", "gar"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PTKNEG", "ADV", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.13": {"text": "Empfindlich aufgenommen.", "tokens": ["Emp\u00b7find\u00b7lich", "auf\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Allein,", "tokens": ["Al\u00b7lein", ","], "token_info": ["word", "punct"], "pos": ["ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Mit welcher Lust nahm ich, bey aller Pein,", "tokens": ["Mit", "wel\u00b7cher", "Lust", "nahm", "ich", ",", "bey", "al\u00b7ler", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "VVFIN", "PPER", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Den Ursprung meiner Quaal, den nunmehr losen Zahn,", "tokens": ["Den", "Ur\u00b7sprung", "mei\u00b7ner", "Qua\u00b7al", ",", "den", "nun\u00b7mehr", "lo\u00b7sen", "Zahn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$,", "PRELS", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Aus Carpsers blut'gen H\u00e4nden an!", "tokens": ["Aus", "Carp\u00b7sers", "blut'\u00b7gen", "H\u00e4n\u00b7den", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Kaum konnte mir, ihn hin und her zu kehren,", "tokens": ["Kaum", "konn\u00b7te", "mir", ",", "ihn", "hin", "und", "her", "zu", "keh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "$,", "PPER", "PTKVZ", "KON", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Die Zacken anzusehn, ein kalter Schauer wehren,", "tokens": ["Die", "Za\u00b7cken", "an\u00b7zu\u00b7sehn", ",", "ein", "kal\u00b7ter", "Schau\u00b7er", "weh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$,", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dar pl\u00f6tzlich mich befiel. Ich leget' ihn denn nieder.", "tokens": ["Dar", "pl\u00f6tz\u00b7lich", "mich", "be\u00b7fiel", ".", "Ich", "le\u00b7get'", "ihn", "denn", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJD", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Itzt aber nehm' ich ihn aufs neue wieder,", "tokens": ["Itzt", "a\u00b7ber", "nehm'", "ich", "ihn", "aufs", "neu\u00b7e", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "APPRART", "ADJA", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Beschaue seine Cron' und messe", "tokens": ["Be\u00b7schau\u00b7e", "sei\u00b7ne", "Cron'", "und", "mes\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Derselben Breit' und Festigkeit,", "tokens": ["Der\u00b7sel\u00b7ben", "Breit'", "und", "Fes\u00b7tig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beseh' der Wurzeln St\u00e4rck' und Gr\u00f6sse,", "tokens": ["Be\u00b7seh'", "der", "Wur\u00b7zeln", "St\u00e4rck", "und", "Gr\u00f6s\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Betrachte die Beschaffenheit,", "tokens": ["Be\u00b7trach\u00b7te", "die", "Be\u00b7schaf\u00b7fen\u00b7heit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wie er im Fleisch gesteckt,", "tokens": ["Wie", "er", "im", "Fleisch", "ge\u00b7steckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Und werde nun so gar", "tokens": ["Und", "wer\u00b7de", "nun", "so", "gar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "ADV", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Dadurch, weil etwas Fleisch daran geblieben war,", "tokens": ["Da\u00b7durch", ",", "weil", "et\u00b7was", "Fleisch", "da\u00b7ran", "ge\u00b7blie\u00b7ben", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "KOUS", "PIAT", "NN", "PAV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie eine Haut annoch den gantzen Knochen deckt,", "tokens": ["Wie", "ei\u00b7ne", "Haut", "an\u00b7noch", "den", "gant\u00b7zen", "Kno\u00b7chen", "deckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Erstaunt gewahr, woraus gantz klar erscheinet,", "tokens": ["Er\u00b7staunt", "ge\u00b7wahr", ",", "wo\u00b7raus", "gantz", "klar", "er\u00b7schei\u00b7net", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "$,", "PWAV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Auf welche Weise Fleisch und Knochen sich vereinet.", "tokens": ["Auf", "wel\u00b7che", "Wei\u00b7se", "Fleisch", "und", "Kno\u00b7chen", "sich", "ver\u00b7ei\u00b7net", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "NN", "KON", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Es zeiget mir der Rest", "tokens": ["Es", "zei\u00b7get", "mir", "der", "Rest"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Von einer Sehn', auf welche Weise", "tokens": ["Von", "ei\u00b7ner", "Sehn'", ",", "auf", "wel\u00b7che", "Wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An dieser zarten Haut so Fleisch, als Sehne, fest;", "tokens": ["An", "die\u00b7ser", "zar\u00b7ten", "Haut", "so", "Fleisch", ",", "als", "Seh\u00b7ne", ",", "fest", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "ADV", "NN", "$,", "KOUS", "NN", "$,", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Doch geht sie nur so weit, als im Geh\u00e4use", "tokens": ["Doch", "geht", "sie", "nur", "so", "weit", ",", "als", "im", "Ge\u00b7h\u00e4u\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "$,", "KOUS", "APPRART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Der Zahn vorher gesteckt. Die\u00df stellt mir nun von neuen", "tokens": ["Der", "Zahn", "vor\u00b7her", "ge\u00b7steckt", ".", "Die\u00df", "stellt", "mir", "nun", "von", "neu\u00b7en"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVPP", "$.", "PDS", "VVFIN", "PPER", "ADV", "APPR", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein weises Wunder dar; es scheint absonderlich", "tokens": ["Ein", "wei\u00b7ses", "Wun\u00b7der", "dar", ";", "es", "scheint", "ab\u00b7son\u00b7der\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So k\u00fcnstlich zugericht't, damit die Haut nicht sich", "tokens": ["So", "k\u00fcnst\u00b7lich", "zu\u00b7ge\u00b7richt't", ",", "da\u00b7mit", "die", "Haut", "nicht", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "$,", "KOUS", "ART", "NN", "PTKNEG", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Versch\u00f6b' und nicht verletzet w\u00fcrd' im K\u00e4uen.", "tokens": ["Ver\u00b7sch\u00f6b'", "und", "nicht", "ver\u00b7let\u00b7zet", "w\u00fcrd'", "im", "K\u00e4u\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "PTKNEG", "VVFIN", "VAFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Noch mehr, es kann in der Natur", "tokens": ["Noch", "mehr", ",", "es", "kann", "in", "der", "Na\u00b7tur"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "PPER", "VMFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An freyer Luft ein Knochen nicht bestehen:", "tokens": ["An", "frey\u00b7er", "Luft", "ein", "Kno\u00b7chen", "nicht", "be\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Daher wir denn, o Wunder! sehen,", "tokens": ["Da\u00b7her", "wir", "denn", ",", "o", "Wun\u00b7der", "!", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "$,", "FM", "NN", "$.", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie eine k\u00fcnstliche besondere Glasur,", "tokens": ["Wie", "ei\u00b7ne", "k\u00fcnst\u00b7li\u00b7che", "be\u00b7son\u00b7de\u00b7re", "Gla\u00b7sur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die ihn so zieret, als ihm n\u00fctzet,", "tokens": ["Die", "ihn", "so", "zie\u00b7ret", ",", "als", "ihm", "n\u00fct\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den Zahn von aussen deckt und sch\u00fctzet.", "tokens": ["Den", "Zahn", "von", "aus\u00b7sen", "deckt", "und", "sch\u00fct\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Da\u00df aus des Kiefers fester Lade", "tokens": ["Da\u00df", "aus", "des", "Kie\u00b7fers", "fes\u00b7ter", "La\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Man Z\u00e4hne hebet sonder Schade,", "tokens": ["Man", "Z\u00e4h\u00b7ne", "he\u00b7bet", "son\u00b7der", "Scha\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "NN", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und da\u00df die Wunden, ohn' Verwilen", "tokens": ["Und", "da\u00df", "die", "Wun\u00b7den", ",", "ohn'", "Ver\u00b7wi\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "KOUS", "ART", "NN", "$,", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und fern're Schmerzten, Wieder heilen;", "tokens": ["Und", "fern'\u00b7re", "Schmerz\u00b7ten", ",", "Wie\u00b7der", "hei\u00b7len", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$,", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ist auch ein grosses Gl\u00fcck.", "tokens": ["Ist", "auch", "ein", "gros\u00b7ses", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Je mehr ich nun auf unsre Z\u00e4hne mercke,", "tokens": ["Je", "mehr", "ich", "nun", "auf", "uns\u00b7re", "Z\u00e4h\u00b7ne", "mer\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADV", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Je mehr find' ich ihnen Wunder-Wercke.", "tokens": ["Je", "mehr", "find'", "ich", "ih\u00b7nen", "Wun\u00b7der\u00b7\u00b7Wer\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.11": {"line.1": {"text": "Da\u00df unsre vord're Z\u00e4hn' im Munde", "tokens": ["Da\u00df", "uns\u00b7re", "vor\u00b7d'\u00b7re", "Z\u00e4hn'", "im", "Mun\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "APPRART", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die d\u00fcnnsten, scharf und schneidend seyn;", "tokens": ["Die", "d\u00fcnns\u00b7ten", ",", "scharf", "und", "schnei\u00b7dend", "seyn", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "ADJD", "KON", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da hat vermuthlich die\u00df zum Grunde,", "tokens": ["Da", "hat", "ver\u00b7muth\u00b7lich", "die\u00df", "zum", "Grun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PDS", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und gibt es selbst der Augen-Schein:", "tokens": ["Und", "gibt", "es", "selbst", "der", "Au\u00b7gen\u00b7Schein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Damit die Speisen desto besser,", "tokens": ["Da\u00b7mit", "die", "Spei\u00b7sen", "des\u00b7to", "bes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ja gleichsam als mit einem Messer,", "tokens": ["Ja", "gleich\u00b7sam", "als", "mit", "ei\u00b7nem", "Mes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADJD", "KOKOM", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Dadurch geschnitten werden k\u00f6nnen.", "tokens": ["Da\u00b7durch", "ge\u00b7schnit\u00b7ten", "wer\u00b7den", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Bewundernd seh' ich auch die andern Spitzen,", "tokens": ["Be\u00b7wun\u00b7dernd", "seh'", "ich", "auch", "die", "an\u00b7dern", "Spit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die nahe bey den ersten sitzen,", "tokens": ["Die", "na\u00b7he", "bey", "den", "ers\u00b7ten", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und die wir Hunde-Z\u00e4hne nennen.", "tokens": ["Und", "die", "wir", "Hun\u00b7de\u00b7Z\u00e4h\u00b7ne", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PPER", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch diese wird, was z\u00e4h', ereilet,", "tokens": ["Durch", "die\u00b7se", "wird", ",", "was", "z\u00e4h'", ",", "er\u00b7ei\u00b7let", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PDS", "VAFIN", "$,", "PWS", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Zerdr\u00fcckt, zermalmt, zertheilet.", "tokens": ["Zer\u00b7dr\u00fcckt", ",", "zer\u00b7malmt", ",", "zer\u00b7thei\u00b7let", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Ist dieses nicht Weisheit gnug;", "tokens": ["Ist", "die\u00b7ses", "nicht", "Weis\u00b7heit", "gnug", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "NN", "ADV", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So lasst uns auch die Backen-Z\u00e4hn'", "tokens": ["So", "lasst", "uns", "auch", "die", "Ba\u00b7cken\u00b7Z\u00e4hn'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und ihre sond're Form besehn!", "tokens": ["Und", "ih\u00b7re", "son\u00b7d'\u00b7re", "Form", "be\u00b7sehn", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Da\u00df wir bequemlich und mit Fug", "tokens": ["Da\u00df", "wir", "be\u00b7quem\u00b7lich", "und", "mit", "Fug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "KON", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das essen", "tokens": ["Das", "es\u00b7sen"], "token_info": ["word", "word"], "pos": ["PDS", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Zermahlen k\u00f6nnen, reiben, pressen;", "tokens": ["Zer\u00b7mah\u00b7len", "k\u00f6n\u00b7nen", ",", "rei\u00b7ben", ",", "pres\u00b7sen", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "VMFIN", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sind diese nicht nur platt und breit,", "tokens": ["Sind", "die\u00b7se", "nicht", "nur", "platt", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nein zu besond'rer Nutzbarkeit,", "tokens": ["Nein", "zu", "be\u00b7son\u00b7d'\u00b7rer", "Nutz\u00b7bar\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PTKZU", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Mit kleinen Tiefen und mit H\u00f6hn", "tokens": ["Mit", "klei\u00b7nen", "Tie\u00b7fen", "und", "mit", "H\u00f6hn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Recht wunderbar versehn.", "tokens": ["Recht", "wun\u00b7der\u00b7bar", "ver\u00b7sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Wenn nur allein die scharf- und spitzen Z\u00e4hne binden,", "tokens": ["Wenn", "nur", "al\u00b7lein", "die", "scha\u00b7rf", "und", "spit\u00b7zen", "Z\u00e4h\u00b7ne", "bin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "ART", "TRUNC", "KON", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Die breiten forn, im Munde st\u00fcnden;", "tokens": ["Die", "brei\u00b7ten", "forn", ",", "im", "Mun\u00b7de", "st\u00fcn\u00b7den", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie m\u00fchsam w\u00fcrd' alsdann uns allen", "tokens": ["Wie", "m\u00fch\u00b7sam", "w\u00fcrd'", "als\u00b7dann", "uns", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VAFIN", "ADV", "PPER", "PIAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das itzt so leichte K\u00e4uen fallen!", "tokens": ["Das", "itzt", "so", "leich\u00b7te", "K\u00e4u\u00b7en", "fal\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Bewund're doch, o Mensch, die\u00df Wunder! stell' es dir", "tokens": ["Be\u00b7wun\u00b7d'\u00b7re", "doch", ",", "o", "Mensch", ",", "die\u00df", "Wun\u00b7der", "!", "stell'", "es", "dir"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "FM", "NN", "$,", "PDS", "NN", "$.", "VVFIN", "PPER", "PPER"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Dem Sch\u00f6pfer, ders gemacht, zum Ruhm, doch \u00f6fters f\u00fcr!", "tokens": ["Dem", "Sch\u00f6p\u00b7fer", ",", "ders", "ge\u00b7macht", ",", "zum", "Ruhm", ",", "doch", "\u00f6f\u00b7ters", "f\u00fcr", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVPP", "$,", "APPRART", "NN", "$,", "ADV", "ADV", "APPR", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Bey jedem Bissen freu' dich Seiner G\u00fcte,", "tokens": ["Bey", "je\u00b7dem", "Bis\u00b7sen", "freu'", "dich", "Sei\u00b7ner", "G\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und weil er ja f\u00fcr das, was Er beschert,", "tokens": ["Und", "weil", "er", "ja", "f\u00fcr", "das", ",", "was", "Er", "be\u00b7schert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "PDS", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Nichts, als ein fr\u00f6hlichs Hertz, begehrt,", "tokens": ["Nichts", ",", "als", "ein", "fr\u00f6h\u00b7lichs", "Hertz", ",", "be\u00b7gehrt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PIS", "$,", "KOUS", "ART", "ADJA", "NN", "$,", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So opfer' Ihm ein danckbares Gem\u00fcthe!", "tokens": ["So", "op\u00b7fer'", "Ihm", "ein", "dan\u00b7ck\u00b7ba\u00b7res", "Ge\u00b7m\u00fc\u00b7the", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}}, "stanza.17": {"line.1": {"text": "Um gr\u00f6ssre Schmertzen zu vermeiden,", "tokens": ["Um", "gr\u00f6ss\u00b7re", "Schmert\u00b7zen", "zu", "ver\u00b7mei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Entschlo\u00df ich mich, da\u00df mir ein Zahn,", "tokens": ["Ent\u00b7schlo\u00df", "ich", "mich", ",", "da\u00df", "mir", "ein", "Zahn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "$,", "KOUS", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der mir bishero weh gethan,", "tokens": ["Der", "mir", "bis\u00b7he\u00b7ro", "weh", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "--+-++-+", "measure": "anapaest.init"}, "line.4": {"text": "W\u00fcrd' ausgebrochen, zu erleiden.", "tokens": ["W\u00fcrd'", "aus\u00b7ge\u00b7bro\u00b7chen", ",", "zu", "er\u00b7lei\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Weil aber die Natur, bey starcken Gliedern,", "tokens": ["Weil", "a\u00b7ber", "die", "Na\u00b7tur", ",", "bey", "star\u00b7cken", "Glie\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "(so ich dem Sch\u00f6pfer nie durch Danck kann gnug erwiedern)", "tokens": ["(", "so", "ich", "dem", "Sch\u00f6p\u00b7fer", "nie", "durch", "Danck", "kann", "gnug", "er\u00b7wie\u00b7dern", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "PPER", "ART", "NN", "ADV", "APPR", "NN", "VMFIN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Auch starcke Z\u00e4hne mir verliehn;", "tokens": ["Auch", "star\u00b7cke", "Z\u00e4h\u00b7ne", "mir", "ver\u00b7liehn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So schien es erst, als ob, ihn auszuziehn,", "tokens": ["So", "schien", "es", "erst", ",", "als", "ob", ",", "ihn", "aus\u00b7zu\u00b7ziehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "KOKOM", "KOUS", "$,", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der kluge Carpser selbst, der an Geschicklichkeit", "tokens": ["Der", "klu\u00b7ge", "Carp\u00b7ser", "selbst", ",", "der", "an", "Ge\u00b7schick\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADV", "$,", "PRELS", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Kaum seines gleichen kennt, sich etwas scheut'; allein,", "tokens": ["Kaum", "sei\u00b7nes", "glei\u00b7chen", "kennt", ",", "sich", "et\u00b7was", "scheut'", ";", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "VVFIN", "$,", "PRF", "ADV", "VVFIN", "$.", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Weil ich darauf bestund, war er dazu bereit.", "tokens": ["Weil", "ich", "da\u00b7rauf", "be\u00b7stund", ",", "war", "er", "da\u00b7zu", "be\u00b7reit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PAV", "ADJD", "$,", "VAFIN", "PPER", "PAV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Ich nahm mir vor, die strenge Pein,", "tokens": ["Ich", "nahm", "mir", "vor", ",", "die", "stren\u00b7ge", "Pein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ohn' alles Zucken, sonder Schreyn", "tokens": ["Ohn'", "al\u00b7les", "Zu\u00b7cken", ",", "son\u00b7der", "Schreyn"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIAT", "NN", "$,", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Behertzt und standhaft auszustehen.", "tokens": ["Be\u00b7hertzt", "und", "stand\u00b7haft", "aus\u00b7zu\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er setze drauf den Pelican,", "tokens": ["Er", "set\u00b7ze", "drauf", "den", "Pe\u00b7li\u00b7can", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Den ich vorhero wohl besehen,", "tokens": ["Den", "ich", "vor\u00b7he\u00b7ro", "wohl", "be\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Mit Kraft und Vorsicht an.", "tokens": ["Mit", "Kraft", "und", "Vor\u00b7sicht", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Wir hielten uns im Anfang beyde gut:", "tokens": ["Wir", "hiel\u00b7ten", "uns", "im", "An\u00b7fang", "bey\u00b7de", "gut", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "NN", "PIS", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Er brach; ich hielte fest, noch fester doch der Zahn.", "tokens": ["Er", "brach", ";", "ich", "hiel\u00b7te", "fest", ",", "noch", "fes\u00b7ter", "doch", "der", "Zahn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VVFIN", "PTKVZ", "$,", "ADV", "ADJD", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er knackt', ich wiche nicht. Doch endlich war mein Muth", "tokens": ["Er", "knackt'", ",", "ich", "wi\u00b7che", "nicht", ".", "Doch", "end\u00b7lich", "war", "mein", "Muth"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PTKNEG", "$.", "KON", "ADV", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Noch eher, als der Zahn, gebrochen.", "tokens": ["Noch", "e\u00b7her", ",", "als", "der", "Zahn", ",", "ge\u00b7bro\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "ART", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Es ri\u00df ein gr\u00e4\u00dfliches Gekrach,", "tokens": ["Es", "ri\u00df", "ein", "gr\u00e4\u00df\u00b7li\u00b7ches", "Ge\u00b7krach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Wodurch des gantzen Hauptes Knochen", "tokens": ["Wo\u00b7durch", "des", "gant\u00b7zen", "Haup\u00b7tes", "Kno\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Zu spalten schien, ein kurtz doch kl\u00e4glich Ach", "tokens": ["Zu", "spal\u00b7ten", "schien", ",", "ein", "kurtz", "doch", "kl\u00e4g\u00b7lich", "Ach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "VVFIN", "$,", "ART", "ADJD", "ADV", "ADJD", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Mir aus der Brust. Die feurig-wilde Pein,", "tokens": ["Mir", "aus", "der", "Brust", ".", "Die", "feu\u00b7rig\u00b7wil\u00b7de", "Pein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "$.", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.15": {"text": "Der bittre Schmertz, durchdrang so Fleisch, als Bein.", "tokens": ["Der", "bitt\u00b7re", "Schmertz", ",", "durch\u00b7drang", "so", "Fleisch", ",", "als", "Bein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVFIN", "ADV", "NN", "$,", "KOUS", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Die\u00df splittert', jenes ri\u00df, jedoch, zu meinem Leide,", "tokens": ["Die\u00df", "split\u00b7tert'", ",", "je\u00b7nes", "ri\u00df", ",", "je\u00b7doch", ",", "zu", "mei\u00b7nem", "Lei\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PDS", "VVFIN", "$,", "ADV", "$,", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Kein eintzigs gantz entzwey;", "tokens": ["Kein", "eint\u00b7zigs", "gantz", "ent\u00b7zwey", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.18": {"text": "Der Sehnen Z\u00e4higkeit band sie noch alle beyde.", "tokens": ["Der", "Seh\u00b7nen", "Z\u00e4\u00b7hig\u00b7keit", "band", "sie", "noch", "al\u00b7le", "bey\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "PPER", "ADV", "PIAT", "PIS", "$."], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.20": {"line.1": {"text": "Den meist gel\u00f6sten Zahn ergriff der Artzt aufs neu',", "tokens": ["Den", "meist", "ge\u00b7l\u00f6s\u00b7ten", "Zahn", "er\u00b7griff", "der", "Artzt", "aufs", "neu'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "VVFIN", "ART", "NN", "APPRART", "ADJA", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und ich, vor Unmuth Muth. Er w\u00e4hlt' aus zweyen B\u00f6sen", "tokens": ["Und", "ich", ",", "vor", "Un\u00b7muth", "Muth", ".", "Er", "w\u00e4hlt'", "aus", "zwe\u00b7yen", "B\u00f6\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "$,", "APPR", "NN", "NN", "$.", "PPER", "VVFIN", "APPR", "VVFIN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das kleinest', und fing an, das Zahn-Fleisch abzul\u00f6sen.", "tokens": ["Das", "klei\u00b7nest'", ",", "und", "fing", "an", ",", "das", "Zahn\u00b7Fleisch", "ab\u00b7zu\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KON", "VVFIN", "PTKVZ", "$,", "ART", "NN", "VVIZU", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Ob ich nun gleich die scharfen Schmerzten f\u00fchlte,", "tokens": ["Ob", "ich", "nun", "gleich", "die", "schar\u00b7fen", "Schmerz\u00b7ten", "f\u00fchl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Wie er mir dazumahl in frischer Wunde w\u00fchlte,", "tokens": ["Wie", "er", "mir", "da\u00b7zu\u00b7mahl", "in", "fri\u00b7scher", "Wun\u00b7de", "w\u00fchl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie er das Fleisch zerschnitt; so wirckete jdoch", "tokens": ["Wie", "er", "das", "Fleisch", "zer\u00b7schnitt", ";", "so", "wir\u00b7cke\u00b7te", "jdoch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "PPER", "ART", "NN", "VVFIN", "$.", "ADV", "VVFIN", "ADV"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.7": {"text": "Der noch weit gr\u00f6ssre Schmertz, den, wie es so gekracht,", "tokens": ["Der", "noch", "weit", "gr\u00f6ss\u00b7re", "Schmertz", ",", "den", ",", "wie", "es", "so", "ge\u00b7kracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "ADJA", "NN", "$,", "ART", "$,", "PWAV", "PPER", "ADV", "VVPP", "$,"], "meter": "-+++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.8": {"text": "Der Bruch mir kurtz vorher gemacht,", "tokens": ["Der", "Bruch", "mir", "kurtz", "vor\u00b7her", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Zusamt der Furcht, es w\u00fcrd' annoch", "tokens": ["Zu\u00b7samt", "der", "Furcht", ",", "es", "w\u00fcrd'", "an\u00b7noch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "ART", "NN", "$,", "PPER", "VAFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Dergleichen gr\u00e4\u00dfliches Geknirsch von neuem kommen,", "tokens": ["Derg\u00b7lei\u00b7chen", "gr\u00e4\u00df\u00b7li\u00b7ches", "Ge\u00b7knirsch", "von", "neu\u00b7em", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJA", "NN", "APPR", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Da\u00df ich die Pein des Schnitts, wie herbe sie auch war,", "tokens": ["Da\u00df", "ich", "die", "Pein", "des", "Schnitts", ",", "wie", "her\u00b7be", "sie", "auch", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "$,", "PWAV", "VVFIN", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Doch nicht so gar", "tokens": ["Doch", "nicht", "so", "gar"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PTKNEG", "ADV", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.13": {"text": "Empfindlich aufgenommen.", "tokens": ["Emp\u00b7find\u00b7lich", "auf\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Allein,", "tokens": ["Al\u00b7lein", ","], "token_info": ["word", "punct"], "pos": ["ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Mit welcher Lust nahm ich, bey aller Pein,", "tokens": ["Mit", "wel\u00b7cher", "Lust", "nahm", "ich", ",", "bey", "al\u00b7ler", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "VVFIN", "PPER", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Den Ursprung meiner Quaal, den nunmehr losen Zahn,", "tokens": ["Den", "Ur\u00b7sprung", "mei\u00b7ner", "Qua\u00b7al", ",", "den", "nun\u00b7mehr", "lo\u00b7sen", "Zahn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$,", "PRELS", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Aus Carpsers blut'gen H\u00e4nden an!", "tokens": ["Aus", "Carp\u00b7sers", "blut'\u00b7gen", "H\u00e4n\u00b7den", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Kaum konnte mir, ihn hin und her zu kehren,", "tokens": ["Kaum", "konn\u00b7te", "mir", ",", "ihn", "hin", "und", "her", "zu", "keh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "$,", "PPER", "PTKVZ", "KON", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Die Zacken anzusehn, ein kalter Schauer wehren,", "tokens": ["Die", "Za\u00b7cken", "an\u00b7zu\u00b7sehn", ",", "ein", "kal\u00b7ter", "Schau\u00b7er", "weh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$,", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dar pl\u00f6tzlich mich befiel. Ich leget' ihn denn nieder.", "tokens": ["Dar", "pl\u00f6tz\u00b7lich", "mich", "be\u00b7fiel", ".", "Ich", "le\u00b7get'", "ihn", "denn", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJD", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Itzt aber nehm' ich ihn aufs neue wieder,", "tokens": ["Itzt", "a\u00b7ber", "nehm'", "ich", "ihn", "aufs", "neu\u00b7e", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "APPRART", "ADJA", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Beschaue seine Cron' und messe", "tokens": ["Be\u00b7schau\u00b7e", "sei\u00b7ne", "Cron'", "und", "mes\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Derselben Breit' und Festigkeit,", "tokens": ["Der\u00b7sel\u00b7ben", "Breit'", "und", "Fes\u00b7tig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beseh' der Wurzeln St\u00e4rck' und Gr\u00f6sse,", "tokens": ["Be\u00b7seh'", "der", "Wur\u00b7zeln", "St\u00e4rck", "und", "Gr\u00f6s\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Betrachte die Beschaffenheit,", "tokens": ["Be\u00b7trach\u00b7te", "die", "Be\u00b7schaf\u00b7fen\u00b7heit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wie er im Fleisch gesteckt,", "tokens": ["Wie", "er", "im", "Fleisch", "ge\u00b7steckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Und werde nun so gar", "tokens": ["Und", "wer\u00b7de", "nun", "so", "gar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "ADV", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Dadurch, weil etwas Fleisch daran geblieben war,", "tokens": ["Da\u00b7durch", ",", "weil", "et\u00b7was", "Fleisch", "da\u00b7ran", "ge\u00b7blie\u00b7ben", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "KOUS", "PIAT", "NN", "PAV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wie eine Haut annoch den gantzen Knochen deckt,", "tokens": ["Wie", "ei\u00b7ne", "Haut", "an\u00b7noch", "den", "gant\u00b7zen", "Kno\u00b7chen", "deckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Erstaunt gewahr, woraus gantz klar erscheinet,", "tokens": ["Er\u00b7staunt", "ge\u00b7wahr", ",", "wo\u00b7raus", "gantz", "klar", "er\u00b7schei\u00b7net", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "$,", "PWAV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Auf welche Weise Fleisch und Knochen sich vereinet.", "tokens": ["Auf", "wel\u00b7che", "Wei\u00b7se", "Fleisch", "und", "Kno\u00b7chen", "sich", "ver\u00b7ei\u00b7net", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "NN", "KON", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.23": {"line.1": {"text": "Es zeiget mir der Rest", "tokens": ["Es", "zei\u00b7get", "mir", "der", "Rest"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Von einer Sehn', auf welche Weise", "tokens": ["Von", "ei\u00b7ner", "Sehn'", ",", "auf", "wel\u00b7che", "Wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "APPR", "PWAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An dieser zarten Haut so Fleisch, als Sehne, fest;", "tokens": ["An", "die\u00b7ser", "zar\u00b7ten", "Haut", "so", "Fleisch", ",", "als", "Seh\u00b7ne", ",", "fest", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "ADV", "NN", "$,", "KOUS", "NN", "$,", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Doch geht sie nur so weit, als im Geh\u00e4use", "tokens": ["Doch", "geht", "sie", "nur", "so", "weit", ",", "als", "im", "Ge\u00b7h\u00e4u\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "$,", "KOUS", "APPRART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Der Zahn vorher gesteckt. Die\u00df stellt mir nun von neuen", "tokens": ["Der", "Zahn", "vor\u00b7her", "ge\u00b7steckt", ".", "Die\u00df", "stellt", "mir", "nun", "von", "neu\u00b7en"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVPP", "$.", "PDS", "VVFIN", "PPER", "ADV", "APPR", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein weises Wunder dar; es scheint absonderlich", "tokens": ["Ein", "wei\u00b7ses", "Wun\u00b7der", "dar", ";", "es", "scheint", "ab\u00b7son\u00b7der\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So k\u00fcnstlich zugericht't, damit die Haut nicht sich", "tokens": ["So", "k\u00fcnst\u00b7lich", "zu\u00b7ge\u00b7richt't", ",", "da\u00b7mit", "die", "Haut", "nicht", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "$,", "KOUS", "ART", "NN", "PTKNEG", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Versch\u00f6b' und nicht verletzet w\u00fcrd' im K\u00e4uen.", "tokens": ["Ver\u00b7sch\u00f6b'", "und", "nicht", "ver\u00b7let\u00b7zet", "w\u00fcrd'", "im", "K\u00e4u\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "PTKNEG", "VVFIN", "VAFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.24": {"line.1": {"text": "Noch mehr, es kann in der Natur", "tokens": ["Noch", "mehr", ",", "es", "kann", "in", "der", "Na\u00b7tur"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "PPER", "VMFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An freyer Luft ein Knochen nicht bestehen:", "tokens": ["An", "frey\u00b7er", "Luft", "ein", "Kno\u00b7chen", "nicht", "be\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ART", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Daher wir denn, o Wunder! sehen,", "tokens": ["Da\u00b7her", "wir", "denn", ",", "o", "Wun\u00b7der", "!", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "$,", "FM", "NN", "$.", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie eine k\u00fcnstliche besondere Glasur,", "tokens": ["Wie", "ei\u00b7ne", "k\u00fcnst\u00b7li\u00b7che", "be\u00b7son\u00b7de\u00b7re", "Gla\u00b7sur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die ihn so zieret, als ihm n\u00fctzet,", "tokens": ["Die", "ihn", "so", "zie\u00b7ret", ",", "als", "ihm", "n\u00fct\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Den Zahn von aussen deckt und sch\u00fctzet.", "tokens": ["Den", "Zahn", "von", "aus\u00b7sen", "deckt", "und", "sch\u00fct\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Da\u00df aus des Kiefers fester Lade", "tokens": ["Da\u00df", "aus", "des", "Kie\u00b7fers", "fes\u00b7ter", "La\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Man Z\u00e4hne hebet sonder Schade,", "tokens": ["Man", "Z\u00e4h\u00b7ne", "he\u00b7bet", "son\u00b7der", "Scha\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "NN", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und da\u00df die Wunden, ohn' Verwilen", "tokens": ["Und", "da\u00df", "die", "Wun\u00b7den", ",", "ohn'", "Ver\u00b7wi\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "KOUS", "ART", "NN", "$,", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und fern're Schmerzten, Wieder heilen;", "tokens": ["Und", "fern'\u00b7re", "Schmerz\u00b7ten", ",", "Wie\u00b7der", "hei\u00b7len", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$,", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ist auch ein grosses Gl\u00fcck.", "tokens": ["Ist", "auch", "ein", "gros\u00b7ses", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.26": {"line.1": {"text": "Je mehr ich nun auf unsre Z\u00e4hne mercke,", "tokens": ["Je", "mehr", "ich", "nun", "auf", "uns\u00b7re", "Z\u00e4h\u00b7ne", "mer\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADV", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Je mehr find' ich ihnen Wunder-Wercke.", "tokens": ["Je", "mehr", "find'", "ich", "ih\u00b7nen", "Wun\u00b7der\u00b7\u00b7Wer\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.27": {"line.1": {"text": "Da\u00df unsre vord're Z\u00e4hn' im Munde", "tokens": ["Da\u00df", "uns\u00b7re", "vor\u00b7d'\u00b7re", "Z\u00e4hn'", "im", "Mun\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "APPRART", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die d\u00fcnnsten, scharf und schneidend seyn;", "tokens": ["Die", "d\u00fcnns\u00b7ten", ",", "scharf", "und", "schnei\u00b7dend", "seyn", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "ADJD", "KON", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da hat vermuthlich die\u00df zum Grunde,", "tokens": ["Da", "hat", "ver\u00b7muth\u00b7lich", "die\u00df", "zum", "Grun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PDS", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und gibt es selbst der Augen-Schein:", "tokens": ["Und", "gibt", "es", "selbst", "der", "Au\u00b7gen\u00b7Schein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Damit die Speisen desto besser,", "tokens": ["Da\u00b7mit", "die", "Spei\u00b7sen", "des\u00b7to", "bes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Ja gleichsam als mit einem Messer,", "tokens": ["Ja", "gleich\u00b7sam", "als", "mit", "ei\u00b7nem", "Mes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADJD", "KOKOM", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Dadurch geschnitten werden k\u00f6nnen.", "tokens": ["Da\u00b7durch", "ge\u00b7schnit\u00b7ten", "wer\u00b7den", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Bewundernd seh' ich auch die andern Spitzen,", "tokens": ["Be\u00b7wun\u00b7dernd", "seh'", "ich", "auch", "die", "an\u00b7dern", "Spit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die nahe bey den ersten sitzen,", "tokens": ["Die", "na\u00b7he", "bey", "den", "ers\u00b7ten", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und die wir Hunde-Z\u00e4hne nennen.", "tokens": ["Und", "die", "wir", "Hun\u00b7de\u00b7Z\u00e4h\u00b7ne", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PPER", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Durch diese wird, was z\u00e4h', ereilet,", "tokens": ["Durch", "die\u00b7se", "wird", ",", "was", "z\u00e4h'", ",", "er\u00b7ei\u00b7let", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PDS", "VAFIN", "$,", "PWS", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Zerdr\u00fcckt, zermalmt, zertheilet.", "tokens": ["Zer\u00b7dr\u00fcckt", ",", "zer\u00b7malmt", ",", "zer\u00b7thei\u00b7let", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.29": {"line.1": {"text": "Ist dieses nicht Weisheit gnug;", "tokens": ["Ist", "die\u00b7ses", "nicht", "Weis\u00b7heit", "gnug", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "NN", "ADV", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So lasst uns auch die Backen-Z\u00e4hn'", "tokens": ["So", "lasst", "uns", "auch", "die", "Ba\u00b7cken\u00b7Z\u00e4hn'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und ihre sond're Form besehn!", "tokens": ["Und", "ih\u00b7re", "son\u00b7d'\u00b7re", "Form", "be\u00b7sehn", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.30": {"line.1": {"text": "Da\u00df wir bequemlich und mit Fug", "tokens": ["Da\u00df", "wir", "be\u00b7quem\u00b7lich", "und", "mit", "Fug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "KON", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das essen", "tokens": ["Das", "es\u00b7sen"], "token_info": ["word", "word"], "pos": ["PDS", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Zermahlen k\u00f6nnen, reiben, pressen;", "tokens": ["Zer\u00b7mah\u00b7len", "k\u00f6n\u00b7nen", ",", "rei\u00b7ben", ",", "pres\u00b7sen", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "VMFIN", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sind diese nicht nur platt und breit,", "tokens": ["Sind", "die\u00b7se", "nicht", "nur", "platt", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nein zu besond'rer Nutzbarkeit,", "tokens": ["Nein", "zu", "be\u00b7son\u00b7d'\u00b7rer", "Nutz\u00b7bar\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PTKZU", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Mit kleinen Tiefen und mit H\u00f6hn", "tokens": ["Mit", "klei\u00b7nen", "Tie\u00b7fen", "und", "mit", "H\u00f6hn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Recht wunderbar versehn.", "tokens": ["Recht", "wun\u00b7der\u00b7bar", "ver\u00b7sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.31": {"line.1": {"text": "Wenn nur allein die scharf- und spitzen Z\u00e4hne binden,", "tokens": ["Wenn", "nur", "al\u00b7lein", "die", "scha\u00b7rf", "und", "spit\u00b7zen", "Z\u00e4h\u00b7ne", "bin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "ART", "TRUNC", "KON", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Die breiten forn, im Munde st\u00fcnden;", "tokens": ["Die", "brei\u00b7ten", "forn", ",", "im", "Mun\u00b7de", "st\u00fcn\u00b7den", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie m\u00fchsam w\u00fcrd' alsdann uns allen", "tokens": ["Wie", "m\u00fch\u00b7sam", "w\u00fcrd'", "als\u00b7dann", "uns", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VAFIN", "ADV", "PPER", "PIAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das itzt so leichte K\u00e4uen fallen!", "tokens": ["Das", "itzt", "so", "leich\u00b7te", "K\u00e4u\u00b7en", "fal\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Bewund're doch, o Mensch, die\u00df Wunder! stell' es dir", "tokens": ["Be\u00b7wun\u00b7d'\u00b7re", "doch", ",", "o", "Mensch", ",", "die\u00df", "Wun\u00b7der", "!", "stell'", "es", "dir"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "FM", "NN", "$,", "PDS", "NN", "$.", "VVFIN", "PPER", "PPER"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Dem Sch\u00f6pfer, ders gemacht, zum Ruhm, doch \u00f6fters f\u00fcr!", "tokens": ["Dem", "Sch\u00f6p\u00b7fer", ",", "ders", "ge\u00b7macht", ",", "zum", "Ruhm", ",", "doch", "\u00f6f\u00b7ters", "f\u00fcr", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVPP", "$,", "APPRART", "NN", "$,", "ADV", "ADV", "APPR", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Bey jedem Bissen freu' dich Seiner G\u00fcte,", "tokens": ["Bey", "je\u00b7dem", "Bis\u00b7sen", "freu'", "dich", "Sei\u00b7ner", "G\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und weil er ja f\u00fcr das, was Er beschert,", "tokens": ["Und", "weil", "er", "ja", "f\u00fcr", "das", ",", "was", "Er", "be\u00b7schert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "PDS", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Nichts, als ein fr\u00f6hlichs Hertz, begehrt,", "tokens": ["Nichts", ",", "als", "ein", "fr\u00f6h\u00b7lichs", "Hertz", ",", "be\u00b7gehrt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PIS", "$,", "KOUS", "ART", "ADJA", "NN", "$,", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So opfer' Ihm ein danckbares Gem\u00fcthe!", "tokens": ["So", "op\u00b7fer'", "Ihm", "ein", "dan\u00b7ck\u00b7ba\u00b7res", "Ge\u00b7m\u00fc\u00b7the", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}}}}}