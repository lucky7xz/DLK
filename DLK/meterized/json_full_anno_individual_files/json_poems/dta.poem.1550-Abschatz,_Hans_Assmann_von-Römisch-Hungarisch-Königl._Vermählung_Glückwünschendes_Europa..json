{"dta.poem.1550": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "R\u00f6misch-Hungarisch-K\u00f6nigl. Verm\u00e4hlung/  \n Gl\u00fcckw\u00fcnschendes Europa.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "O Himmel kl\u00e4re dich/ zeuch deine Wolcken ein!", "tokens": ["O", "Him\u00b7mel", "kl\u00e4\u00b7re", "dich", "/", "zeuch", "dei\u00b7ne", "Wol\u00b7cken", "ein", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "PPER", "$(", "VVIMP", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "IrENE k\u00f6mmt herbey mit g\u00f6ldnen Friedens-Sch\u00e4tzen", "tokens": ["I\u00b7rE\u00b7NE", "k\u00f6mmt", "her\u00b7bey", "mit", "g\u00f6ld\u00b7nen", "Frie\u00b7dens\u00b7Sch\u00e4t\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Des Gro\u00dfen JOSEPHS Thron und Reiche zu erg\u00f6tzen.", "tokens": ["Des", "Gro\u00b7\u00dfen", "Jo\u00b7SE\u00b7PHS", "Thron", "und", "Rei\u00b7che", "zu", "er\u00b7g\u00f6t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "NN", "KON", "NE", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Ohimmel kl\u00e4re dich mit hellem Sonnenschein!", "tokens": ["O\u00b7him\u00b7mel", "kl\u00e4\u00b7re", "dich", "mit", "hel\u00b7lem", "Son\u00b7nen\u00b7schein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "La\u00df ihr auff sanffte Bahn Narci\u00df- und Rosen schneyn!", "tokens": ["La\u00df", "ihr", "auff", "sanff\u00b7te", "Bahn", "Na\u00b7rci\u00df", "und", "Ro\u00b7sen", "schneyn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPR", "ADJA", "NN", "TRUNC", "KON", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Zur Friedens-Unterschrifft sieht man die Feder netzen/", "tokens": ["Zur", "Frie\u00b7dens\u00b7Un\u00b7ter\u00b7schrifft", "sieht", "man", "die", "Fe\u00b7der", "net\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "ART", "NN", "VVINF", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "So bald sie kan den Fu\u00df auff deutsche Gr\u00e4ntzen setzen:", "tokens": ["So", "bald", "sie", "kan", "den", "Fu\u00df", "auff", "deut\u00b7sche", "Gr\u00e4nt\u00b7zen", "set\u00b7zen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "VMFIN", "ART", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Soll nicht AMALIA die Friedens-G\u00f6ttin seyn?", "tokens": ["Soll", "nicht", "A\u00b7mALIA", "die", "Frie\u00b7dens\u00b7G\u00f6t\u00b7tin", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "NN", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Komm/ holde K\u00f6nigs-Braut! es eilet mit Verlangen", "tokens": ["Komm", "/", "hol\u00b7de", "K\u00f6\u00b7nigs\u00b7Braut", "!", "es", "ei\u00b7let", "mit", "Ver\u00b7lan\u00b7gen"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "ADJA", "NN", "$.", "PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der Held von Oesterreich/ IRENEN zu empfangen:", "tokens": ["Der", "Held", "von", "O\u00b7es\u00b7ter\u00b7reich", "/", "I\u00b7rE\u00b7NEN", "zu", "emp\u00b7fan\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$(", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Der treuen L\u00e4nder Wunsch begleitet ihn zu dir.", "tokens": ["Der", "treu\u00b7en", "L\u00e4n\u00b7der", "Wunsch", "be\u00b7glei\u00b7tet", "ihn", "zu", "dir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Es schallet durch die Lufft/ da\u00df JOSEPH und IRENE", "tokens": ["Es", "schal\u00b7let", "durch", "die", "Lufft", "/", "da\u00df", "Jo\u00b7SE\u00b7PH", "und", "I\u00b7rE\u00b7NE"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$(", "KOUS", "NE", "KON", "NE"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.13": {"text": "Gl\u00fcck/ Segen/ Wonn und Heyl in langem Leben kr\u00f6ne!", "tokens": ["Gl\u00fcck", "/", "Se\u00b7gen", "/", "Wonn", "und", "Heyl", "in", "lan\u00b7gem", "Le\u00b7ben", "kr\u00f6\u00b7ne", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NE", "KON", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "So bricht die g\u00f6ldne Zeit der Welt auffs neu herf\u00fcr!", "tokens": ["So", "bricht", "die", "g\u00f6ld\u00b7ne", "Zeit", "der", "Welt", "auffs", "neu", "her\u00b7f\u00fcr", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "ART", "NN", "APPRART", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Kommt/ Helden-T\u00f6chter/ kommt/ last in die Wette h\u00f6ren/", "tokens": ["Kommt", "/", "Hel\u00b7den\u00b7T\u00f6ch\u00b7ter", "/", "kommt", "/", "last", "in", "die", "Wet\u00b7te", "h\u00f6\u00b7ren", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "NN", "$(", "VVFIN", "$(", "VVFIN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Womit ihr seyd bereit", "tokens": ["Wo\u00b7mit", "ihr", "seyd", "be\u00b7reit"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPER", "VAFIN", "ADJD"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Hoffnung unsrer Zeit", "tokens": ["Die", "Hoff\u00b7nung", "uns\u00b7rer", "Zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Des Gro\u00dfen LEOPOLDS Sohn/ JOSEPH/ zu verehren", "tokens": ["Des", "Gro\u00b7\u00dfen", "LeO\u00b7POLDS", "Sohn", "/", "Jo\u00b7SE\u00b7PH", "/", "zu", "ver\u00b7eh\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ART", "NN", "NE", "NN", "$(", "XY", "$(", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ich/ weil Europens Haubt sich auff mein K\u00fcssen leget/", "tokens": ["Ich", "/", "weil", "Eu\u00b7ro\u00b7pens", "Haubt", "sich", "auff", "mein", "K\u00fcs\u00b7sen", "le\u00b7get", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$(", "KOUS", "NE", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Will erst an Reyhen gehn.", "tokens": ["Will", "erst", "an", "Rey\u00b7hen", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ihm soll zu Dienste stehn", "tokens": ["Ihm", "soll", "zu", "Diens\u00b7te", "stehn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "NN", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Was eine fremde Welt und Goa seltnes heget.", "tokens": ["Was", "ei\u00b7ne", "frem\u00b7de", "Welt", "und", "Goa", "selt\u00b7nes", "he\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "KON", "NE", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Was bringt Iberien zu seinem Doppel-Throne?", "tokens": ["Was", "bringt", "I\u00b7be\u00b7ri\u00b7en", "zu", "sei\u00b7nem", "Dop\u00b7pel\u00b7Thro\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NE", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Pactol und Peru f\u00fchrt", "tokens": ["Pac\u00b7tol", "und", "Pe\u00b7ru", "f\u00fchrt"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "KON", "NE", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was ihm zum Zins geb\u00fchrt;", "tokens": ["Was", "ihm", "zum", "Zins", "ge\u00b7b\u00fchrt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Darzu den besten Stein aus meiner theuren Krone.", "tokens": ["Dar\u00b7zu", "den", "bes\u00b7ten", "Stein", "aus", "mei\u00b7ner", "theu\u00b7ren", "Kro\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Stein/ Silber/ gelbes Marck der Berg\u2019 ist zu geringe:", "tokens": ["Stein", "/", "Sil\u00b7ber", "/", "gel\u00b7bes", "Marck", "der", "Ber\u00b7g'", "ist", "zu", "ge\u00b7rin\u00b7ge", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "ADJA", "NN", "ART", "NN", "VAFIN", "APPR", "ADJA", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Der gro\u00dfe Held verdient/", "tokens": ["Der", "gro\u00b7\u00dfe", "Held", "ver\u00b7dient", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mit dem mein Hoffen gr\u00fcnt/", "tokens": ["Mit", "dem", "mein", "Hof\u00b7fen", "gr\u00fcnt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Da\u00df man ihm zum Geschenck ein ander Opffer bringe.", "tokens": ["Da\u00df", "man", "ihm", "zum", "Ge\u00b7schenck", "ein", "an\u00b7der", "Opf\u00b7fer", "brin\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "APPRART", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Der wohlgeneigte Sinn ist offt zu schwach an Kr\u00e4fften:", "tokens": ["Der", "wohl\u00b7ge\u00b7neig\u00b7te", "Sinn", "ist", "offt", "zu", "schwach", "an", "Kr\u00e4ff\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Was thut denn Albion?", "tokens": ["Was", "thut", "denn", "Al\u00b7bi\u00b7on", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich will an seinen Thron", "tokens": ["Ich", "will", "an", "sei\u00b7nen", "Thron"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Orangen und Oliv\u2019 an Palm und Lorbern hefften.", "tokens": ["O\u00b7ran\u00b7gen", "und", "O\u00b7li\u00b7v'", "an", "Palm", "und", "Lor\u00b7bern", "heff\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "APPR", "NE", "KON", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.7": {"line.1": {"text": "Was hab ich rauher Nord dem Helden zu gefallen", "tokens": ["Was", "hab", "ich", "rau\u00b7her", "Nord", "dem", "Hel\u00b7den", "zu", "ge\u00b7fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "PPER", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In meinem Eigenthum?", "tokens": ["In", "mei\u00b7nem", "Ei\u00b7gen\u00b7thum", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Zu mehren seinen Ruhm", "tokens": ["Zu", "meh\u00b7ren", "sei\u00b7nen", "Ruhm"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Durch neuen Glantz und Schein verehr\u2019 ich Berg-Crystallen.", "tokens": ["Durch", "neu\u00b7en", "Glantz", "und", "Schein", "ver\u00b7ehr'", "ich", "Ber\u00b7gCry\u00b7stal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Was ich dir W\u00fcrdiges f\u00fcr andern k\u00f6nne zeigen", "tokens": ["Was", "ich", "dir", "W\u00fcr\u00b7di\u00b7ges", "f\u00fcr", "an\u00b7dern", "k\u00f6n\u00b7ne", "zei\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "PPER", "NN", "APPR", "PIS", "VMFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist wohl nicht viel bey mir:", "tokens": ["Ist", "wohl", "nicht", "viel", "bey", "mir", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "APPR", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch/ K\u00f6nig/ bleibet dir", "tokens": ["Doch", "/", "K\u00f6\u00b7nig", "/", "blei\u00b7bet", "dir"], "token_info": ["word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "$(", "NN", "$(", "VVFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Der D\u00e4nen offner Sund und Hertz auff ewig eigen.", "tokens": ["Der", "D\u00e4\u00b7nen", "off\u00b7ner", "Sund", "und", "Hertz", "auff", "e\u00b7wig", "ei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "KON", "NN", "APPR", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Mein Bley ist viel zu schwer in Oesterreich zu f\u00fchren/", "tokens": ["Mein", "Bley", "ist", "viel", "zu", "schwer", "in", "O\u00b7es\u00b7ter\u00b7reich", "zu", "f\u00fch\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Mein Saltz und Wachs zu schlecht.", "tokens": ["Mein", "Saltz", "und", "Wachs", "zu", "schlecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "NN", "PTKA", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch/ steht das alte Recht/", "tokens": ["Doch", "/", "steht", "das", "al\u00b7te", "Recht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "So wird noch Haubt noch Glied die alte Gunst verlieren.", "tokens": ["So", "wird", "noch", "Haubt", "noch", "Glied", "die", "al\u00b7te", "Gunst", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "NN", "ADV", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Wolt ich Siberien von Zobeln gleich entleren/", "tokens": ["Wolt", "ich", "Si\u00b7be\u00b7ri\u00b7en", "von", "Zo\u00b7beln", "gleich", "ent\u00b7le\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "NE", "APPR", "NN", "ADV", "VVINF", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Was br\u00e4cht ich W\u00fcrdigs dar?", "tokens": ["Was", "br\u00e4cht", "ich", "W\u00fcr\u00b7digs", "dar", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Genung/ da\u00df ihm ein CZAAR", "tokens": ["Ge\u00b7nung", "/", "da\u00df", "ihm", "ein", "CzAAR"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$(", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "In seiner eignen Burg mu\u00df Lieb und Hold gewehren.", "tokens": ["In", "sei\u00b7ner", "eig\u00b7nen", "Burg", "mu\u00df", "Lieb", "und", "Hold", "ge\u00b7weh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VMFIN", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Die Meinung ist wohl gutt/ die ein- und anders f\u00fchret;", "tokens": ["Die", "Mei\u00b7nung", "ist", "wohl", "gutt", "/", "die", "ein", "und", "an\u00b7ders", "f\u00fch\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$(", "ART", "TRUNC", "KON", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch bildt euch/ Kinder/ ein", "tokens": ["Doch", "bildt", "euch", "/", "Kin\u00b7der", "/", "ein"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KON", "VVFIN", "PPER", "$(", "NN", "$(", "ART"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Di\u00df wird eur Gl\u00fccks-Stern seyn/", "tokens": ["Di\u00df", "wird", "eur", "Gl\u00fccks\u00b7S\u00b7tern", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "VAINF", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Und dencket nach/ da\u00df noch was anders ihm geb\u00fchret.", "tokens": ["Und", "den\u00b7cket", "nach", "/", "da\u00df", "noch", "was", "an\u00b7ders", "ihm", "ge\u00b7b\u00fch\u00b7ret", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "$(", "KOUS", "ADV", "PWS", "ADV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Mein JOSEPH schaue nicht nach weit-geholtem Wesen/", "tokens": ["Mein", "Jo\u00b7SE\u00b7PH", "schau\u00b7e", "nicht", "nach", "weit\u00b7ge\u00b7hol\u00b7tem", "We\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKNEG", "APPR", "ADJA", "NN", "$("], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.2": {"text": "Da\u00df der und jener r\u00fchmt.", "tokens": ["Da\u00df", "der", "und", "je\u00b7ner", "r\u00fchmt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "KON", "PDS", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was seinen Jahren ziemt", "tokens": ["Was", "sei\u00b7nen", "Jah\u00b7ren", "ziemt"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Hab ich ihm aus der Schos der Weser ausgelesen.", "tokens": ["Hab", "ich", "ihm", "aus", "der", "Schos", "der", "We\u00b7ser", "aus\u00b7ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "APPR", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Die K\u00e4yser waren ja gewohnt bey mir zu wohnen:", "tokens": ["Die", "K\u00e4y\u00b7ser", "wa\u00b7ren", "ja", "ge\u00b7wohnt", "bey", "mir", "zu", "woh\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich sehe noch f\u00fcr ihn", "tokens": ["Ich", "se\u00b7he", "noch", "f\u00fcr", "ihn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "In meinen Lande bl\u00fchn", "tokens": ["In", "mei\u00b7nen", "Lan\u00b7de", "bl\u00fchn"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Der K\u00f6nigs-Kertzen Zier/ die Pracht der K\u00e4yser-Kronen.", "tokens": ["Der", "K\u00f6\u00b7nigs\u00b7Kert\u00b7zen", "Zier", "/", "die", "Pracht", "der", "K\u00e4y\u00b7ser\u00b7Kro\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$(", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Kommts auff die Blumen an/ so bleib ich nicht dahinden:", "tokens": ["Kommts", "auff", "die", "Blu\u00b7men", "an", "/", "so", "bleib", "ich", "nicht", "da\u00b7hin\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "PTKVZ", "$(", "ADV", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie manches F\u00fcrsten Hand", "tokens": ["Wie", "man\u00b7ches", "F\u00fcrs\u00b7ten", "Hand"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PIAT", "NN", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Vom Po- und Weser-Strand", "tokens": ["Vom", "Po", "und", "We\u00b7ser\u00b7Strand"], "token_info": ["word", "word", "word", "word"], "pos": ["APPRART", "TRUNC", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Lie\u00df ihm den liebsten Krantz in meinen G\u00e4rten winden!", "tokens": ["Lie\u00df", "ihm", "den", "liebs\u00b7ten", "Krantz", "in", "mei\u00b7nen", "G\u00e4r\u00b7ten", "win\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Eur schlauer Geist err\u00e4th/ wohin mein R\u00e4tzel zielet!", "tokens": ["Eur", "schlau\u00b7er", "Geist", "er\u00b7r\u00e4\u00b7th", "/", "wo\u00b7hin", "mein", "R\u00e4t\u00b7zel", "zie\u00b7let", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "$(", "PWAV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Doch/ weil mein eigen ist/", "tokens": ["Doch", "/", "weil", "mein", "ei\u00b7gen", "ist", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "KOUS", "PPOSAT", "ADJD", "VAFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was JOSEPH hat erkist/", "tokens": ["Was", "Jo\u00b7SE\u00b7PH", "hat", "er\u00b7kist", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "VVPP", "$("], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "So ist der Danck f\u00fcr mich gewonnen/ euch verspielet.", "tokens": ["So", "ist", "der", "Danck", "f\u00fcr", "mich", "ge\u00b7won\u00b7nen", "/", "euch", "ver\u00b7spie\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "PPER", "VVPP", "$(", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Ist nicht die Helden-Blum aus Franckreich her entsprossen?", "tokens": ["Ist", "nicht", "die", "Hel\u00b7den\u00b7Blum", "aus", "Fran\u00b7ck\u00b7reich", "her", "ent\u00b7spros\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "ART", "NE", "APPR", "NE", "APZR", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.17": {"line.1": {"text": "Vom Stocke fremder Art:", "tokens": ["Vom", "Sto\u00b7cke", "frem\u00b7der", "Art", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "In Gallien gepaart.", "tokens": ["In", "Gal\u00b7li\u00b7en", "ge\u00b7paart", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Doch da\u00df von Welschem Blutt an seinen Stamm geflossen.", "tokens": ["Doch", "da\u00df", "von", "Wel\u00b7schem", "Blutt", "an", "sei\u00b7nen", "Stamm", "ge\u00b7flos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "APPR", "NN", "ADJD", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Ich lache/ da\u00df sie sich um fremde Blumen zancken.", "tokens": ["Ich", "la\u00b7che", "/", "da\u00df", "sie", "sich", "um", "frem\u00b7de", "Blu\u00b7men", "zan\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KOUS", "PPER", "PRF", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wer Guelffens Felder kennt/", "tokens": ["Wer", "Guel\u00b7ffens", "Fel\u00b7der", "kennt", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "NN", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Necker-Wisen nennt/", "tokens": ["Die", "Ne\u00b7cke\u00b7rWi\u00b7sen", "nennt", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Der suchet sie gewi\u00df nicht ausser meinen Schrancken.", "tokens": ["Der", "su\u00b7chet", "sie", "ge\u00b7wi\u00df", "nicht", "aus\u00b7ser", "mei\u00b7nen", "Schran\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PTKNEG", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Die sch\u00f6nste Weser-Blum ist dein/ o Deutschland/ eigen.", "tokens": ["Die", "sch\u00f6ns\u00b7te", "We\u00b7ser\u00b7Blum", "ist", "dein", "/", "o", "Deutschland", "/", "ei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NE", "VAFIN", "PPOSAT", "$(", "FM", "NE", "$(", "ADJD", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der Deutschen Donau Strand", "tokens": ["Der", "Deut\u00b7schen", "Do\u00b7nau", "Strand"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wird nun ihr fester Stand:", "tokens": ["Wird", "nun", "ihr", "fes\u00b7ter", "Stand", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Daselbsten wird ihr Glantz zu voller Bl\u00fcte steigen.", "tokens": ["Da\u00b7selbs\u00b7ten", "wird", "ihr", "Glantz", "zu", "vol\u00b7ler", "Bl\u00fc\u00b7te", "stei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Blumen/ welche Braunschweig giebt/", "tokens": ["Blu\u00b7men", "/", "wel\u00b7che", "Braun\u00b7schweig", "giebt", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWAT", "NN", "VVFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Solln auff Pannons Auen bl\u00fchen?", "tokens": ["Solln", "auff", "Pan\u00b7nons", "Au\u00b7en", "bl\u00fc\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Blumen/ welche JOSEPH liebt/", "tokens": ["Blu\u00b7men", "/", "wel\u00b7che", "Jo\u00b7SE\u00b7PH", "liebt", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PRELS", "PPER", "VVFIN", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Sollen neue Blumen ziehen!", "tokens": ["Sol\u00b7len", "neu\u00b7e", "Blu\u00b7men", "zie\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Blumen meiner Nieder-Sachsen", "tokens": ["Blu\u00b7men", "mei\u00b7ner", "Nie\u00b7der\u00b7Sach\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["NN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sollen bi\u00df an Stambol wachsen!", "tokens": ["Sol\u00b7len", "bi\u00df", "an", "Stam\u00b7bol", "wach\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Lasst uns einen Blumen-Krantz", "tokens": ["Lasst", "uns", "ei\u00b7nen", "Blu\u00b7men\u00b7Krantz"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Dem verm\u00e4hlten JOSEPH binden/", "tokens": ["Dem", "ver\u00b7m\u00e4hl\u00b7ten", "Jo\u00b7SE\u00b7PH", "bin\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ob gleich aller Blumen Glantz", "tokens": ["Ob", "gleich", "al\u00b7ler", "Blu\u00b7men", "Glantz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PIAT", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Mu\u00df f\u00fcr seiner Blume schwinden!", "tokens": ["Mu\u00df", "f\u00fcr", "sei\u00b7ner", "Blu\u00b7me", "schwin\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Was die Gl\u00fcckes-Inseln tragen", "tokens": ["Was", "die", "Gl\u00fc\u00b7ckes\u00b7In\u00b7seln", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "ART", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Soll sich ihn zu zieren wagen.", "tokens": ["Soll", "sich", "ihn", "zu", "zie\u00b7ren", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "PPER", "PTKZU", "VVINF", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Seht/ hier seyn schon ausgespreit", "tokens": ["Seht", "/", "hier", "seyn", "schon", "aus\u00b7ge\u00b7spreit"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "ADV", "PPOSAT", "ADV", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Tuberosen und Je\u00dfminen!", "tokens": ["Tu\u00b7be\u00b7ro\u00b7sen", "und", "Je\u00df\u00b7mi\u00b7nen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Meine Liljen seyn bereit", "tokens": ["Mei\u00b7ne", "Lil\u00b7jen", "seyn", "be\u00b7reit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Seine Scheitel zu bedienen.", "tokens": ["Sei\u00b7ne", "Schei\u00b7tel", "zu", "be\u00b7die\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Ro\u00dfmarin und frische Myrten", "tokens": ["Ro\u00df\u00b7ma\u00b7rin", "und", "fri\u00b7sche", "Myr\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schm\u00fcckt den gro\u00dfen V\u00f6lcker-Hirten!", "tokens": ["Schm\u00fcckt", "den", "gro\u00b7\u00dfen", "V\u00f6lcker\u00b7Hir\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.27": {"line.1": {"text": "Nehmt der fetten Trifften Klee/", "tokens": ["Nehmt", "der", "fet\u00b7ten", "Triff\u00b7ten", "Klee", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die Bl\u00fcten g\u00f6ldner Fr\u00fcchte!", "tokens": ["Und", "die", "Bl\u00fc\u00b7ten", "g\u00f6ld\u00b7ner", "Fr\u00fcch\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Was auch unter Ei\u00df und Schnee", "tokens": ["Was", "auch", "un\u00b7ter", "Ei\u00df", "und", "Schnee"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Bl\u00fchet/ bring ich zu Gesichte.", "tokens": ["Bl\u00fc\u00b7het", "/", "bring", "ich", "zu", "Ge\u00b7sich\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "See-Blum um den Belt entsprossen", "tokens": ["See\u00b7Blum", "um", "den", "Belt", "ent\u00b7spros\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "ART", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Soll auch nicht seyn ausgeschlossen.", "tokens": ["Soll", "auch", "nicht", "seyn", "aus\u00b7ge\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PTKNEG", "VAINF", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.30": {"line.1": {"text": "Von der nahen Tartarey", "tokens": ["Von", "der", "na\u00b7hen", "Tar\u00b7ta\u00b7rey"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Will ich meinen Borez senden.", "tokens": ["Will", "ich", "mei\u00b7nen", "Bo\u00b7rez", "sen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "JoSEPHS edle Sch\u00e4ferey", "tokens": ["Jo\u00b7SE\u00b7PHS", "ed\u00b7le", "Sch\u00e4\u00b7fe\u00b7rey"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mehre sich an allen Enden!", "tokens": ["Meh\u00b7re", "sich", "an", "al\u00b7len", "En\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.31": {"line.1": {"text": "Draus die Bienen Honig saugen", "tokens": ["Draus", "die", "Bie\u00b7nen", "Ho\u00b7nig", "sau\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Leg\u2019 ich f\u00fcr des Helden Augen.", "tokens": ["Leg'", "ich", "f\u00fcr", "des", "Hel\u00b7den", "Au\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "LeOPOLD/ den werthen Sohn", "tokens": ["LeO\u00b7POLD", "/", "den", "wert\u00b7hen", "Sohn"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Siehst du izt mit Lust verm\u00e4hlen.", "tokens": ["Siehst", "du", "izt", "mit", "Lust", "ver\u00b7m\u00e4h\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "La\u00df dich GOTT auff deinem Thron", "tokens": ["La\u00df", "dich", "GoTT", "auff", "dei\u00b7nem", "Thron"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "NE", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Noch viel Enckel von ihm z\u00e4hlen!", "tokens": ["Noch", "viel", "En\u00b7ckel", "von", "ihm", "z\u00e4h\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Die getreuen L\u00e4nder ruffen:", "tokens": ["Die", "ge\u00b7treu\u00b7en", "L\u00e4n\u00b7der", "ruf\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "GoTT best\u00e4ttig\u2019 unser Hoffen!", "tokens": ["GoTT", "be\u00b7st\u00e4t\u00b7tig'", "un\u00b7ser", "Hof\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Brachtst du unserm Oesterreich/", "tokens": ["Brachtst", "du", "un\u00b7serm", "O\u00b7es\u00b7ter\u00b7reich", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Gro\u00dfe K\u00e4yserin/ den Segen/", "tokens": ["Gro\u00b7\u00dfe", "K\u00e4y\u00b7se\u00b7rin", "/", "den", "Se\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieser soll sich auch zugleich", "tokens": ["Die\u00b7ser", "soll", "sich", "auch", "zu\u00b7gleich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "PRF", "ADV", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Deinem Sohn zur Seite legen/", "tokens": ["Dei\u00b7nem", "Sohn", "zur", "Sei\u00b7te", "le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPRART", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Und von JOSEPHS sch\u00f6nem Bronnen", "tokens": ["Und", "von", "Jo\u00b7SE\u00b7PHS", "sch\u00f6\u00b7nem", "Bron\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Komme Deutschlands Heyl geronnen!", "tokens": ["Kom\u00b7me", "Deutschlands", "Heyl", "ge\u00b7ron\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "NN", "VVPP", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}}}}