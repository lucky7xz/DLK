{"textgrid.poem.46902": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "[fr\u00fchlingsblumen m\u00fc\u00dften]", "genre": "verse", "period": "N.A.", "pub_year": 1827, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Fr\u00fchlingsblumen m\u00fc\u00dften", "tokens": ["Fr\u00fch\u00b7lings\u00b7blu\u00b7men", "m\u00fc\u00df\u00b7ten"], "token_info": ["word", "word"], "pos": ["NN", "VMFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Welken eh' sie bl\u00fchn,", "tokens": ["Wel\u00b7ken", "eh'", "sie", "bl\u00fchn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KOUS", "PPER", "VVINF", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "D\u00e4chten sie und w\u00fc\u00dften,", "tokens": ["D\u00e4ch\u00b7ten", "sie", "und", "w\u00fc\u00df\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was sie machet gr\u00fcn,", "tokens": ["Was", "sie", "ma\u00b7chet", "gr\u00fcn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "ADJD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Da\u00df es stammt aus Gr\u00fcften", "tokens": ["Da\u00df", "es", "stammt", "aus", "Gr\u00fcf\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und aus Moderduft,", "tokens": ["Und", "aus", "Mo\u00b7der\u00b7duft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Was gekl\u00e4rt zu D\u00fcften", "tokens": ["Was", "ge\u00b7kl\u00e4rt", "zu", "D\u00fcf\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "VVPP", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "W\u00fcrzt die Fr\u00fchlingsluft.", "tokens": ["W\u00fcrzt", "die", "Fr\u00fch\u00b7lings\u00b7luft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Drum, wenn uns gemessen", "tokens": ["Drum", ",", "wenn", "uns", "ge\u00b7mes\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PAV", "$,", "KOUS", "PPER", "VVPP"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "L\u00e4ngres Leben blieb,", "tokens": ["L\u00e4ng\u00b7res", "Le\u00b7ben", "blieb", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Lasset uns vergessen,", "tokens": ["Las\u00b7set", "uns", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was wir hatten lieb!", "tokens": ["Was", "wir", "hat\u00b7ten", "lieb", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "La\u00dft es uns begraben,", "tokens": ["La\u00dft", "es", "uns", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPER", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und vergessen? nein!", "tokens": ["Und", "ver\u00b7ges\u00b7sen", "?", "nein", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KON", "VVPP", "$.", "PTKANT", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "La\u00df uns lieb es haben,", "tokens": ["La\u00df", "uns", "lieb", "es", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADJD", "PPER", "VAFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Doch nicht uns zur Pein.", "tokens": ["Doch", "nicht", "uns", "zur", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Sondern wie die Blume", "tokens": ["Son\u00b7dern", "wie", "die", "Blu\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Aus des Tods Gebiet", "tokens": ["Aus", "des", "Tods", "Ge\u00b7biet"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Zu der Leibe Ruhme", "tokens": ["Zu", "der", "Lei\u00b7be", "Ruh\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Heitre Farben zieht,", "tokens": ["Heit\u00b7re", "Far\u00b7ben", "zieht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Also la\u00df uns saugen", "tokens": ["Al\u00b7so", "la\u00df", "uns", "sau\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "PPER", "VVFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Aus verwehtem Hauch,", "tokens": ["Aus", "ver\u00b7weh\u00b7tem", "Hauch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Aus erloschnen Augen,", "tokens": ["Aus", "er\u00b7lo\u00b7schnen", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Sanfte Wehmuth auch.", "tokens": ["Sanf\u00b7te", "Weh\u00b7muth", "auch", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Fr\u00fchlingsblumen m\u00fc\u00dften", "tokens": ["Fr\u00fch\u00b7lings\u00b7blu\u00b7men", "m\u00fc\u00df\u00b7ten"], "token_info": ["word", "word"], "pos": ["NN", "VMFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Welken eh' sie bl\u00fchn,", "tokens": ["Wel\u00b7ken", "eh'", "sie", "bl\u00fchn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KOUS", "PPER", "VVINF", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "D\u00e4chten sie und w\u00fc\u00dften,", "tokens": ["D\u00e4ch\u00b7ten", "sie", "und", "w\u00fc\u00df\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was sie machet gr\u00fcn,", "tokens": ["Was", "sie", "ma\u00b7chet", "gr\u00fcn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "ADJD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Da\u00df es stammt aus Gr\u00fcften", "tokens": ["Da\u00df", "es", "stammt", "aus", "Gr\u00fcf\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und aus Moderduft,", "tokens": ["Und", "aus", "Mo\u00b7der\u00b7duft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Was gekl\u00e4rt zu D\u00fcften", "tokens": ["Was", "ge\u00b7kl\u00e4rt", "zu", "D\u00fcf\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "VVPP", "APPR", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "W\u00fcrzt die Fr\u00fchlingsluft.", "tokens": ["W\u00fcrzt", "die", "Fr\u00fch\u00b7lings\u00b7luft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.5": {"line.1": {"text": "Drum, wenn uns gemessen", "tokens": ["Drum", ",", "wenn", "uns", "ge\u00b7mes\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PAV", "$,", "KOUS", "PPER", "VVPP"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "L\u00e4ngres Leben blieb,", "tokens": ["L\u00e4ng\u00b7res", "Le\u00b7ben", "blieb", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Lasset uns vergessen,", "tokens": ["Las\u00b7set", "uns", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was wir hatten lieb!", "tokens": ["Was", "wir", "hat\u00b7ten", "lieb", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "La\u00dft es uns begraben,", "tokens": ["La\u00dft", "es", "uns", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPER", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Und vergessen? nein!", "tokens": ["Und", "ver\u00b7ges\u00b7sen", "?", "nein", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KON", "VVPP", "$.", "PTKANT", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "La\u00df uns lieb es haben,", "tokens": ["La\u00df", "uns", "lieb", "es", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADJD", "PPER", "VAFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Doch nicht uns zur Pein.", "tokens": ["Doch", "nicht", "uns", "zur", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "Sondern wie die Blume", "tokens": ["Son\u00b7dern", "wie", "die", "Blu\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Aus des Tods Gebiet", "tokens": ["Aus", "des", "Tods", "Ge\u00b7biet"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Zu der Leibe Ruhme", "tokens": ["Zu", "der", "Lei\u00b7be", "Ruh\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Heitre Farben zieht,", "tokens": ["Heit\u00b7re", "Far\u00b7ben", "zieht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Also la\u00df uns saugen", "tokens": ["Al\u00b7so", "la\u00df", "uns", "sau\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "PPER", "VVFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Aus verwehtem Hauch,", "tokens": ["Aus", "ver\u00b7weh\u00b7tem", "Hauch", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Aus erloschnen Augen,", "tokens": ["Aus", "er\u00b7lo\u00b7schnen", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Sanfte Wehmuth auch.", "tokens": ["Sanf\u00b7te", "Weh\u00b7muth", "auch", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}