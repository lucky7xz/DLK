{"dta.poem.12379": {"metadata": {"author": {"name": "Brentano, Clemens", "birth": "N.A.", "death": "N.A."}, "title": "Vom vornehmen R\u00e4uber .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1806", "urn": "urn:nbn:de:kobv:b4-20090519157", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Was wollen wir aber heben an               ", "tokens": ["Was", "wol\u00b7len", "wir", "a\u00b7ber", "he\u00b7ben", "an"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "VVFIN", "PTKVZ"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Von Fritschen dem jungen Edelmann,", "tokens": ["Von", "Frit\u00b7schen", "dem", "jun\u00b7gen", "E\u00b7del\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Hat manchen stolzen Ritt gethan,", "tokens": ["Hat", "man\u00b7chen", "stol\u00b7zen", "Ritt", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bis es ihm schlecht gelungen.", "tokens": ["Bis", "es", "ihm", "schlecht", "ge\u00b7lun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Fritsche zu seinem Knechte sprach:", "tokens": ["Frit\u00b7sche", "zu", "sei\u00b7nem", "Knech\u00b7te", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "\u201esattle mir beide Pferde,", "tokens": ["\u201e", "satt\u00b7le", "mir", "bei\u00b7de", "Pfer\u00b7de", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "\u201ewir wollen nach G\u00f6rlitz auf die Stra\u00dfen reiten,", "tokens": ["\u201e", "wir", "wol\u00b7len", "nach", "G\u00f6r\u00b7litz", "auf", "die", "Stra\u00b7\u00dfen", "rei\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "APPR", "NE", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "\u201edie Fuhrleute wollen wir schauen.\u201c", "tokens": ["\u201e", "die", "Fuhr\u00b7leu\u00b7te", "wol\u00b7len", "wir", "schau\u00b7en", ".", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VMFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.3": {"line.1": {"text": "Da sie nach G\u00f6rlitz auf die Stra\u00dfen kamen,", "tokens": ["Da", "sie", "nach", "G\u00f6r\u00b7litz", "auf", "die", "Stra\u00b7\u00dfen", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NE", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die Wagen wollten sie aufhauen,", "tokens": ["Die", "Wa\u00b7gen", "woll\u00b7ten", "sie", "auf\u00b7hau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So blie\u00df der W\u00e4chter auf seinem Horn,", "tokens": ["So", "blie\u00df", "der", "W\u00e4ch\u00b7ter", "auf", "sei\u00b7nem", "Horn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auf dem Rathhausthurme.", "tokens": ["Auf", "dem", "Rath\u00b7haus\u00b7thur\u00b7me", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Fritsche zu seinem Knechte sprach:", "tokens": ["Frit\u00b7sche", "zu", "sei\u00b7nem", "Knech\u00b7te", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "\u201eich f\u00fcrchte wir seyn verrathen,", "tokens": ["\u201e", "ich", "f\u00fcrch\u00b7te", "wir", "seyn", "ver\u00b7ra\u00b7then", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "PPOSAT", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u201ewenn wir zu Seidenberg blieben,", "tokens": ["\u201e", "wenn", "wir", "zu", "Sei\u00b7den\u00b7berg", "blie\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "\u201eso \u00e4\u00dfen wir Gesotten und Gebraten.\u201c", "tokens": ["\u201e", "so", "\u00e4\u00b7\u00dfen", "wir", "Ge\u00b7sot\u00b7ten", "und", "Ge\u00b7bra\u00b7ten", ".", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Fritsche zu seinem Knechte sprach:", "tokens": ["Frit\u00b7sche", "zu", "sei\u00b7nem", "Knech\u00b7te", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "\u201eey Knecht sieh dich ein wenig um,\u201c", "tokens": ["\u201e", "ey", "Knecht", "sieh", "dich", "ein", "we\u00b7nig", "um", ",", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "NE", "VVFIN", "PRF", "ART", "PIS", "PTKVZ", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er sah den Hauptmann von G\u00f6rlitz herreiten", "tokens": ["Er", "sah", "den", "Haupt\u00b7mann", "von", "G\u00f6r\u00b7litz", "her\u00b7rei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "NE", "VVFIN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von allen Seiten mit Leuten.", "tokens": ["Von", "al\u00b7len", "Sei\u00b7ten", "mit", "Leu\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Der Hauptmann wider den Fritschen sprach:", "tokens": ["Der", "Haupt\u00b7mann", "wi\u00b7der", "den", "Frit\u00b7schen", "sprach", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u201efritsche gib du dich gefangen,", "tokens": ["\u201e", "frit\u00b7sche", "gib", "du", "dich", "ge\u00b7fan\u00b7gen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "VVIMP", "PPER", "PRF", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u201ezu G\u00f6rlitz steht ein lichter Galgen hoch,", "tokens": ["\u201e", "zu", "G\u00f6r\u00b7litz", "steht", "ein", "lich\u00b7ter", "Gal\u00b7gen", "hoch", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NE", "VVFIN", "ART", "ADJA", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "\u201edaran sollt du Fritsche hangen.\u201c", "tokens": ["\u201e", "da\u00b7ran", "sollt", "du", "Frit\u00b7sche", "han\u00b7gen", ".", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PAV", "VMFIN", "PPER", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "\u201eda\u00df ich zu G\u00f6rlitz hangen soll,", "tokens": ["\u201e", "da\u00df", "ich", "zu", "G\u00f6r\u00b7litz", "han\u00b7gen", "soll", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "APPR", "NE", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u201ede\u00df la\u00df dich Gott erbarmen,", "tokens": ["\u201e", "de\u00df", "la\u00df", "dich", "Gott", "er\u00b7bar\u00b7men", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "VVIMP", "PPER", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "\u201eso reun mich nichts als meine Stiefel", "tokens": ["\u201e", "so", "reun", "mich", "nichts", "als", "mei\u00b7ne", "Stie\u00b7fel"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "VVFIN", "PPER", "PIS", "KOKOM", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "\u201edazu meine gute Gesellen und Sporn.\u201c", "tokens": ["\u201e", "da\u00b7zu", "mei\u00b7ne", "gu\u00b7te", "Ge\u00b7sel\u00b7len", "und", "Sporn", ".", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PAV", "PPOSAT", "ADJA", "NN", "KON", "NN", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "\u201eje reun dich nichts als deine Stiefel und Sporn,", "tokens": ["\u201e", "je", "reun", "dich", "nichts", "als", "dei\u00b7ne", "Stie\u00b7fel", "und", "Sporn", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "PIS", "KOKOM", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "\u201edazu deine guten Gesellen,", "tokens": ["\u201e", "da\u00b7zu", "dei\u00b7ne", "gu\u00b7ten", "Ge\u00b7sel\u00b7len", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PAV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "\u201ereun dich nicht mehr deine kleinen Kinder,", "tokens": ["\u201e", "reun", "dich", "nicht", "mehr", "dei\u00b7ne", "klei\u00b7nen", "Kin\u00b7der", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "PTKNEG", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "\u201edazu deine sch\u00f6ne Jungfrauen?\u201c", "tokens": ["\u201e", "da\u00b7zu", "dei\u00b7ne", "sch\u00f6\u00b7ne", "Jung\u00b7frau\u00b7en", "?", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PAV", "PPOSAT", "ADJA", "NN", "$.", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}}}}