{"textgrid.poem.49751": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Verwandlung", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Nun sieh einmal die sch\u00f6ne Wiese,", "tokens": ["Nun", "sieh", "ein\u00b7mal", "die", "sch\u00f6\u00b7ne", "Wie\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den kunterbunten Blumenflor!", "tokens": ["Den", "kun\u00b7ter\u00b7bun\u00b7ten", "Blu\u00b7men\u00b7flor", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bald jene gelb, bald r\u00f6tlich diese \u2013", "tokens": ["Bald", "je\u00b7ne", "gelb", ",", "bald", "r\u00f6t\u00b7lich", "die\u00b7se", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJD", "$,", "ADV", "ADJD", "PDAT", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es kommt uns alles lieblich vor.", "tokens": ["Es", "kommt", "uns", "al\u00b7les", "lieb\u00b7lich", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIS", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Man wird die Blumen morgen m\u00e4hen;", "tokens": ["Man", "wird", "die", "Blu\u00b7men", "mor\u00b7gen", "m\u00e4\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sind sie Heu \u2013 da\u00df Gott erbarm!", "tokens": ["Dann", "sind", "sie", "Heu", "\u2013", "da\u00df", "Gott", "er\u00b7barm", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "$(", "KOUS", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und k\u00f6nnen blo\u00df das Rindvieh bl\u00e4hen", "tokens": ["Und", "k\u00f6n\u00b7nen", "blo\u00df", "das", "Rind\u00b7vieh", "bl\u00e4\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Auf ihrem Wege durch den Darm.", "tokens": ["Auf", "ih\u00b7rem", "We\u00b7ge", "durch", "den", "Darm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wenn sie \u2013 verzeiht! \u2013 am Schlusse werden", "tokens": ["Wenn", "sie", "\u2013", "ver\u00b7zeiht", "!", "\u2013", "am", "Schlus\u00b7se", "wer\u00b7den"], "token_info": ["word", "word", "punct", "word", "punct", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$(", "VVFIN", "$.", "$(", "APPRART", "NN", "VAINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zum breiig weichen K\u00fchedreck,", "tokens": ["Zum", "brei\u00b7ig", "wei\u00b7chen", "K\u00fc\u00b7he\u00b7dreck", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So frag' ich mich: Ist das auf Erden", "tokens": ["So", "frag'", "ich", "mich", ":", "Ist", "das", "auf", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "$.", "VAFIN", "PDS", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Los des Sch\u00f6nen und sein Zweck?", "tokens": ["Das", "Los", "des", "Sch\u00f6\u00b7nen", "und", "sein", "Zweck", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und sehen wir nicht auch das Gleiche", "tokens": ["Und", "se\u00b7hen", "wir", "nicht", "auch", "das", "Glei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei vielem, was ein Dichter schuf?", "tokens": ["Bei", "vie\u00b7lem", ",", "was", "ein", "Dich\u00b7ter", "schuf", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man rei\u00dft's aus dem Ideenreiche", "tokens": ["Man", "rei\u00dft's", "aus", "dem", "I\u00b7deen\u00b7rei\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und gibt es Leuten von Beruf.", "tokens": ["Und", "gibt", "es", "Leu\u00b7ten", "von", "Be\u00b7ruf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Dann fri\u00dft's der Lit'raturprofesser,", "tokens": ["Dann", "fri\u00dft's", "der", "Lit'\u00b7ra\u00b7tur\u00b7pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gibt's wieder her mit Kommentar,", "tokens": ["Gibt's", "wie\u00b7der", "her", "mit", "Kom\u00b7men\u00b7tar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und glaubt dabei, es sei noch besser", "tokens": ["Und", "glaubt", "da\u00b7bei", ",", "es", "sei", "noch", "bes\u00b7ser"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PAV", "$,", "PPER", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sei noch sch\u00f6ner, wie es war.", "tokens": ["Und", "sei", "noch", "sch\u00f6\u00b7ner", ",", "wie", "es", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "$,", "PWAV", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Nun sieh einmal die sch\u00f6ne Wiese,", "tokens": ["Nun", "sieh", "ein\u00b7mal", "die", "sch\u00f6\u00b7ne", "Wie\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den kunterbunten Blumenflor!", "tokens": ["Den", "kun\u00b7ter\u00b7bun\u00b7ten", "Blu\u00b7men\u00b7flor", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bald jene gelb, bald r\u00f6tlich diese \u2013", "tokens": ["Bald", "je\u00b7ne", "gelb", ",", "bald", "r\u00f6t\u00b7lich", "die\u00b7se", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJD", "$,", "ADV", "ADJD", "PDAT", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es kommt uns alles lieblich vor.", "tokens": ["Es", "kommt", "uns", "al\u00b7les", "lieb\u00b7lich", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIS", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Man wird die Blumen morgen m\u00e4hen;", "tokens": ["Man", "wird", "die", "Blu\u00b7men", "mor\u00b7gen", "m\u00e4\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sind sie Heu \u2013 da\u00df Gott erbarm!", "tokens": ["Dann", "sind", "sie", "Heu", "\u2013", "da\u00df", "Gott", "er\u00b7barm", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "$(", "KOUS", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und k\u00f6nnen blo\u00df das Rindvieh bl\u00e4hen", "tokens": ["Und", "k\u00f6n\u00b7nen", "blo\u00df", "das", "Rind\u00b7vieh", "bl\u00e4\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Auf ihrem Wege durch den Darm.", "tokens": ["Auf", "ih\u00b7rem", "We\u00b7ge", "durch", "den", "Darm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wenn sie \u2013 verzeiht! \u2013 am Schlusse werden", "tokens": ["Wenn", "sie", "\u2013", "ver\u00b7zeiht", "!", "\u2013", "am", "Schlus\u00b7se", "wer\u00b7den"], "token_info": ["word", "word", "punct", "word", "punct", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$(", "VVFIN", "$.", "$(", "APPRART", "NN", "VAINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zum breiig weichen K\u00fchedreck,", "tokens": ["Zum", "brei\u00b7ig", "wei\u00b7chen", "K\u00fc\u00b7he\u00b7dreck", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So frag' ich mich: Ist das auf Erden", "tokens": ["So", "frag'", "ich", "mich", ":", "Ist", "das", "auf", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "$.", "VAFIN", "PDS", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Los des Sch\u00f6nen und sein Zweck?", "tokens": ["Das", "Los", "des", "Sch\u00f6\u00b7nen", "und", "sein", "Zweck", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Und sehen wir nicht auch das Gleiche", "tokens": ["Und", "se\u00b7hen", "wir", "nicht", "auch", "das", "Glei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei vielem, was ein Dichter schuf?", "tokens": ["Bei", "vie\u00b7lem", ",", "was", "ein", "Dich\u00b7ter", "schuf", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man rei\u00dft's aus dem Ideenreiche", "tokens": ["Man", "rei\u00dft's", "aus", "dem", "I\u00b7deen\u00b7rei\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und gibt es Leuten von Beruf.", "tokens": ["Und", "gibt", "es", "Leu\u00b7ten", "von", "Be\u00b7ruf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Dann fri\u00dft's der Lit'raturprofesser,", "tokens": ["Dann", "fri\u00dft's", "der", "Lit'\u00b7ra\u00b7tur\u00b7pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gibt's wieder her mit Kommentar,", "tokens": ["Gibt's", "wie\u00b7der", "her", "mit", "Kom\u00b7men\u00b7tar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und glaubt dabei, es sei noch besser", "tokens": ["Und", "glaubt", "da\u00b7bei", ",", "es", "sei", "noch", "bes\u00b7ser"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PAV", "$,", "PPER", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sei noch sch\u00f6ner, wie es war.", "tokens": ["Und", "sei", "noch", "sch\u00f6\u00b7ner", ",", "wie", "es", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "$,", "PWAV", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}