{"textgrid.poem.53615": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Worte und Taten", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Eine Sage ist keine Tue.", "tokens": ["Ei\u00b7ne", "Sa\u00b7ge", "ist", "kei\u00b7ne", "Tue", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Betrachten wir das in aller Ruhe.", "tokens": ["Be\u00b7trach\u00b7ten", "wir", "das", "in", "al\u00b7ler", "Ru\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "APPR", "PIAT", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Da sind zum Beispiel die kleinen Damen.", "tokens": ["Da", "sind", "zum", "Bei\u00b7spiel", "die", "klei\u00b7nen", "Da\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn wir denen mal n\u00e4her kamen,", "tokens": ["Wenn", "wir", "de\u00b7nen", "mal", "n\u00e4\u00b7her", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "begegnet es uns wohl anfangs zumeist,", "tokens": ["be\u00b7geg\u00b7net", "es", "uns", "wohl", "an\u00b7fangs", "zu\u00b7meist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "da\u00df uns die F\u00fcrstin von dannen weist.", "tokens": ["da\u00df", "uns", "die", "F\u00fcrs\u00b7tin", "von", "dan\u00b7nen", "weist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "ADV", "VVFIN", "$."], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und es spricht err\u00f6tend die liebe Kleine:", "tokens": ["Und", "es", "spricht", "er\u00b7r\u00f6\u00b7tend", "die", "lie\u00b7be", "Klei\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "ART", "ADJA", "ADJA", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "\u00bbwas denken Sie denn? Ich bin nicht so eine!\u00ab", "tokens": ["\u00bb", "was", "den\u00b7ken", "Sie", "denn", "?", "Ich", "bin", "nicht", "so", "ei\u00b7ne", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "ADV", "$.", "PPER", "VAFIN", "PTKNEG", "ADV", "ART", "$.", "$("], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.7": {"text": "Dann aber r\u00fcckt sie n\u00e4her ran", "tokens": ["Dann", "a\u00b7ber", "r\u00fcckt", "sie", "n\u00e4\u00b7her", "ran"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und fl\u00fcstert: \u00bbWas legen der Herr denn an?\u00ab", "tokens": ["und", "fl\u00fcs\u00b7tert", ":", "\u00bb", "Was", "le\u00b7gen", "der", "Herr", "denn", "an", "?", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PWS", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Und nach all dem Gerede und nach ein paar Schritt \u2013", "tokens": ["Und", "nach", "all", "dem", "Ge\u00b7re\u00b7de", "und", "nach", "ein", "paar", "Schritt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "ART", "NN", "KON", "APPR", "ART", "PIAT", "NN", "$("], "meter": "--+----+-+-+", "measure": "anapaest.init"}, "line.10": {"text": "geht sie mit.", "tokens": ["geht", "sie", "mit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Worte und Taten \u2013 das ist so hienieden \u2013", "tokens": ["Wor\u00b7te", "und", "Ta\u00b7ten", "\u2013", "das", "ist", "so", "hien\u00b7ie\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$(", "PDS", "VAFIN", "ADV", "ADV", "$("], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "sind manchmal verschieden.", "tokens": ["sind", "manch\u00b7mal", "ver\u00b7schie\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "Da h\u00e4tten wir Philipp Scheidemann.", "tokens": ["Da", "h\u00e4t\u00b7ten", "wir", "Phi\u00b7lipp", "Schei\u00b7de\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NE", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "H\u00f6rt ihn immer nur flei\u00dfig an!", "tokens": ["H\u00f6rt", "ihn", "im\u00b7mer", "nur", "flei\u00b7\u00dfig", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Spricht gescheit und klar und vern\u00fcnftig \u2013", "tokens": ["Spricht", "ge\u00b7scheit", "und", "klar", "und", "ver\u00b7n\u00fcnf\u00b7tig", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "KON", "ADJD", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "gar nicht parteiisch, gar nicht z\u00fcnftig \u2013", "tokens": ["gar", "nicht", "par\u00b7tei\u00b7isch", ",", "gar", "nicht", "z\u00fcnf\u00b7tig", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ADJD", "$,", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "sieht die Dinge so, wie sie sind \u2013", "tokens": ["sieht", "die", "Din\u00b7ge", "so", ",", "wie", "sie", "sind", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "$,", "PWAV", "PPER", "VAFIN", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.6": {"text": "kurz: ein begabtes kassler Kind.", "tokens": ["kurz", ":", "ein", "be\u00b7gab\u00b7tes", "kass\u00b7ler", "Kind", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$.", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Aber wie kann doch das bi\u00dfchen Handeln", "tokens": ["A\u00b7ber", "wie", "kann", "doch", "das", "bi\u00df\u00b7chen", "Han\u00b7deln"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "VMFIN", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.8": {"text": "einen ganzen Menschen verwandeln!", "tokens": ["ei\u00b7nen", "gan\u00b7zen", "Men\u00b7schen", "ver\u00b7wan\u00b7deln", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "Nun ist er nicht mehr wiederzuerkennen:", "tokens": ["Nun", "ist", "er", "nicht", "mehr", "wie\u00b7der\u00b7zu\u00b7er\u00b7ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Kompromi\u00df \u2013 Schweigen \u2013 Pennen . . .", "tokens": ["Kom\u00b7pro\u00b7mi\u00df", "\u2013", "Schwei\u00b7gen", "\u2013", "Pen\u00b7nen", ".", ".", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["VVIMP", "$(", "NN", "$(", "NN", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.11": {"text": "Reden: gut. Tun: oh kontr\u00e4r . . .", "tokens": ["Re\u00b7den", ":", "gut", ".", "Tun", ":", "oh", "kont\u00b7r\u00e4r", ".", ".", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "$.", "ADJD", "$.", "FM.la", "$.", "FM", "FM", "$.", "$.", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.12": {"text": "Ach, da\u00df es doch einmal umgekehrt w\u00e4r \u2013!", "tokens": ["Ach", ",", "da\u00df", "es", "doch", "ein\u00b7mal", "um\u00b7ge\u00b7kehrt", "w\u00e4r", "\u2013", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ITJ", "$,", "KOUS", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$(", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Worte und Taten . . . Als da ist die Regierung:", "tokens": ["Wor\u00b7te", "und", "Ta\u00b7ten", ".", ".", ".", "Als", "da", "ist", "die", "Re\u00b7gie\u00b7rung", ":"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$.", "$.", "$.", "KOUS", "ADV", "VAFIN", "ART", "NN", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da hat sie im Reichstag zur Redeverzierung", "tokens": ["Da", "hat", "sie", "im", "Reichs\u00b7tag", "zur", "Re\u00b7de\u00b7ver\u00b7zie\u00b7rung"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "NN", "APPRART", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "gewi\u00dfe Floskeln, gewi\u00dfe Phrasen,", "tokens": ["ge\u00b7wi\u00b7\u00dfe", "Flos\u00b7keln", ",", "ge\u00b7wi\u00b7\u00dfe", "Phra\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "tut gro\u00dfm\u00e4chtig Posaune blasen \u2013", "tokens": ["tut", "gro\u00df\u00b7m\u00e4ch\u00b7tig", "Po\u00b7sau\u00b7ne", "bla\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "NN", "VVINF", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "und die Pressetrib\u00fcne h\u00f6rt aufmerksam zu . . .", "tokens": ["und", "die", "Pres\u00b7se\u00b7tri\u00b7b\u00fc\u00b7ne", "h\u00f6rt", "auf\u00b7merk\u00b7sam", "zu", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$.", "$.", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "Und dann geht alles zu s\u00fc\u00dfer Ruh.", "tokens": ["Und", "dann", "geht", "al\u00b7les", "zu", "s\u00fc\u00b7\u00dfer", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "PTKZU", "ADJA", "NN", "$."], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Man werde \u2013 spricht man \u2013 den Kapp-Putsch bestrafen.", "tokens": ["Man", "wer\u00b7de", "\u2013", "spricht", "man", "\u2013", "den", "Kapp\u00b7Putsch", "be\u00b7stra\u00b7fen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$(", "VVFIN", "PIS", "$(", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Man geht aber sachteken, sachteken schlafen.", "tokens": ["Man", "geht", "a\u00b7ber", "sach\u00b7te\u00b7ken", ",", "sach\u00b7te\u00b7ken", "schla\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "VVINF", "$,", "ADJD", "VVINF", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Man werde \u2013 spricht man \u2013 das Heer reformieren.", "tokens": ["Man", "wer\u00b7de", "\u2013", "spricht", "man", "\u2013", "das", "Heer", "re\u00b7for\u00b7mie\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$(", "VVFIN", "PIS", "$(", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Man steht aber stramm vor Stabsoffizieren.", "tokens": ["Man", "steht", "a\u00b7ber", "stramm", "vor", "Stabs\u00b7of\u00b7fi\u00b7zie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "VVFIN", "APPR", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Man erstrebe in der ganzen Verwaltung", "tokens": ["Man", "er\u00b7stre\u00b7be", "in", "der", "gan\u00b7zen", "Ver\u00b7wal\u00b7tung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "eine neue, demokratische Haltung.", "tokens": ["ei\u00b7ne", "neu\u00b7e", ",", "de\u00b7mo\u00b7kra\u00b7ti\u00b7sche", "Hal\u00b7tung", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "+-+---+--+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "Man \u00e4ndre Schule und Universit\u00e4t . . .", "tokens": ["Man", "\u00e4nd\u00b7re", "Schu\u00b7le", "und", "U\u00b7niv\u00b7er\u00b7si\u00b7t\u00e4t", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIS", "ADJA", "NN", "KON", "NN", "$.", "$.", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Aber wie das so geht:", "tokens": ["A\u00b7ber", "wie", "das", "so", "geht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PDS", "ADV", "VVFIN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.9": {"text": "Warum denn gleich tun? Das w\u00e4re sch\u00f6n dumm.", "tokens": ["Wa\u00b7rum", "denn", "gleich", "tun", "?", "Das", "w\u00e4\u00b7re", "sch\u00f6n", "dumm", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "VVINF", "$.", "PDS", "VAFIN", "ADJD", "ADJD", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.10": {"text": "Reden gen\u00fcgt ja dem Publikum!", "tokens": ["Re\u00b7den", "ge\u00b7n\u00fcgt", "ja", "dem", "Pub\u00b7li\u00b7kum", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.7": {"line.1": {"text": "Wenn einer bei uns nur etwas sagt,", "tokens": ["Wenn", "ei\u00b7ner", "bei", "uns", "nur", "et\u00b7was", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PPER", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+----+-+", "measure": "dactylic.init"}, "line.2": {"text": "ists gar nicht mehr n\u00f6tig, da\u00df er was wagt.", "tokens": ["ists", "gar", "nicht", "mehr", "n\u00f6\u00b7tig", ",", "da\u00df", "er", "was", "wagt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "ADJD", "$,", "KOUS", "PPER", "PIS", "VVFIN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Er mu\u00df nur reden, verk\u00fcnden, bullern \u2013", "tokens": ["Er", "mu\u00df", "nur", "re\u00b7den", ",", "ver\u00b7k\u00fcn\u00b7den", ",", "bul\u00b7lern", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VVINF", "$,", "VVPP", "$,", "VVFIN", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "ihr werdet schon alle nach Hause kullern.", "tokens": ["ihr", "wer\u00b7det", "schon", "al\u00b7le", "nach", "Hau\u00b7se", "kul\u00b7lern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "APPR", "NN", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Er mu\u00df nur bombastisch prophezein \u2013", "tokens": ["Er", "mu\u00df", "nur", "bom\u00b7bas\u00b7tisch", "pro\u00b7phe\u00b7zein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "PTKVZ", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "nachzupr\u00fcfen f\u00e4llt niemandem ein.", "tokens": ["nach\u00b7zu\u00b7pr\u00fc\u00b7fen", "f\u00e4llt", "nie\u00b7man\u00b7dem", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIZU", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "+-+-----+", "measure": "unknown.measure.tri"}, "line.7": {"text": "Mit einem Wort: das Grammophon!", "tokens": ["Mit", "ei\u00b7nem", "Wort", ":", "das", "Gram\u00b7mo\u00b7phon", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Das Weitere \u2013 ach! das findet sich schon.", "tokens": ["Das", "Wei\u00b7te\u00b7re", "\u2013", "ach", "!", "das", "fin\u00b7det", "sich", "schon", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$(", "ITJ", "$.", "PDS", "VVFIN", "PRF", "ADV", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Wir: Demokratie!", "tokens": ["Wir", ":", "De\u00b7mo\u00b7kra\u00b7tie", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["PPER", "$.", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Immer mit die Ruhe!", "tokens": ["Im\u00b7mer", "mit", "die", "Ru\u00b7he", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.11": {"text": "Eine Sage ist keine Tue.", "tokens": ["Ei\u00b7ne", "Sa\u00b7ge", "ist", "kei\u00b7ne", "Tue", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.8": {"line.1": {"text": "Eine Sage ist keine Tue.", "tokens": ["Ei\u00b7ne", "Sa\u00b7ge", "ist", "kei\u00b7ne", "Tue", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Betrachten wir das in aller Ruhe.", "tokens": ["Be\u00b7trach\u00b7ten", "wir", "das", "in", "al\u00b7ler", "Ru\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "APPR", "PIAT", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Da sind zum Beispiel die kleinen Damen.", "tokens": ["Da", "sind", "zum", "Bei\u00b7spiel", "die", "klei\u00b7nen", "Da\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn wir denen mal n\u00e4her kamen,", "tokens": ["Wenn", "wir", "de\u00b7nen", "mal", "n\u00e4\u00b7her", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "begegnet es uns wohl anfangs zumeist,", "tokens": ["be\u00b7geg\u00b7net", "es", "uns", "wohl", "an\u00b7fangs", "zu\u00b7meist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "da\u00df uns die F\u00fcrstin von dannen weist.", "tokens": ["da\u00df", "uns", "die", "F\u00fcrs\u00b7tin", "von", "dan\u00b7nen", "weist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "ADV", "VVFIN", "$."], "meter": "++-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und es spricht err\u00f6tend die liebe Kleine:", "tokens": ["Und", "es", "spricht", "er\u00b7r\u00f6\u00b7tend", "die", "lie\u00b7be", "Klei\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "ART", "ADJA", "ADJA", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "\u00bbwas denken Sie denn? Ich bin nicht so eine!\u00ab", "tokens": ["\u00bb", "was", "den\u00b7ken", "Sie", "denn", "?", "Ich", "bin", "nicht", "so", "ei\u00b7ne", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "ADV", "$.", "PPER", "VAFIN", "PTKNEG", "ADV", "ART", "$.", "$("], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.7": {"text": "Dann aber r\u00fcckt sie n\u00e4her ran", "tokens": ["Dann", "a\u00b7ber", "r\u00fcckt", "sie", "n\u00e4\u00b7her", "ran"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "und fl\u00fcstert: \u00bbWas legen der Herr denn an?\u00ab", "tokens": ["und", "fl\u00fcs\u00b7tert", ":", "\u00bb", "Was", "le\u00b7gen", "der", "Herr", "denn", "an", "?", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PWS", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Und nach all dem Gerede und nach ein paar Schritt \u2013", "tokens": ["Und", "nach", "all", "dem", "Ge\u00b7re\u00b7de", "und", "nach", "ein", "paar", "Schritt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "ART", "NN", "KON", "APPR", "ART", "PIAT", "NN", "$("], "meter": "--+----+-+-+", "measure": "anapaest.init"}, "line.10": {"text": "geht sie mit.", "tokens": ["geht", "sie", "mit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.10": {"line.1": {"text": "Worte und Taten \u2013 das ist so hienieden \u2013", "tokens": ["Wor\u00b7te", "und", "Ta\u00b7ten", "\u2013", "das", "ist", "so", "hien\u00b7ie\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$(", "PDS", "VAFIN", "ADV", "ADV", "$("], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "sind manchmal verschieden.", "tokens": ["sind", "manch\u00b7mal", "ver\u00b7schie\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.11": {"line.1": {"text": "Da h\u00e4tten wir Philipp Scheidemann.", "tokens": ["Da", "h\u00e4t\u00b7ten", "wir", "Phi\u00b7lipp", "Schei\u00b7de\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NE", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "H\u00f6rt ihn immer nur flei\u00dfig an!", "tokens": ["H\u00f6rt", "ihn", "im\u00b7mer", "nur", "flei\u00b7\u00dfig", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Spricht gescheit und klar und vern\u00fcnftig \u2013", "tokens": ["Spricht", "ge\u00b7scheit", "und", "klar", "und", "ver\u00b7n\u00fcnf\u00b7tig", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "KON", "ADJD", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "gar nicht parteiisch, gar nicht z\u00fcnftig \u2013", "tokens": ["gar", "nicht", "par\u00b7tei\u00b7isch", ",", "gar", "nicht", "z\u00fcnf\u00b7tig", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ADJD", "$,", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "sieht die Dinge so, wie sie sind \u2013", "tokens": ["sieht", "die", "Din\u00b7ge", "so", ",", "wie", "sie", "sind", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "$,", "PWAV", "PPER", "VAFIN", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.6": {"text": "kurz: ein begabtes kassler Kind.", "tokens": ["kurz", ":", "ein", "be\u00b7gab\u00b7tes", "kass\u00b7ler", "Kind", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$.", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Aber wie kann doch das bi\u00dfchen Handeln", "tokens": ["A\u00b7ber", "wie", "kann", "doch", "das", "bi\u00df\u00b7chen", "Han\u00b7deln"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "VMFIN", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.8": {"text": "einen ganzen Menschen verwandeln!", "tokens": ["ei\u00b7nen", "gan\u00b7zen", "Men\u00b7schen", "ver\u00b7wan\u00b7deln", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "Nun ist er nicht mehr wiederzuerkennen:", "tokens": ["Nun", "ist", "er", "nicht", "mehr", "wie\u00b7der\u00b7zu\u00b7er\u00b7ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Kompromi\u00df \u2013 Schweigen \u2013 Pennen . . .", "tokens": ["Kom\u00b7pro\u00b7mi\u00df", "\u2013", "Schwei\u00b7gen", "\u2013", "Pen\u00b7nen", ".", ".", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["VVIMP", "$(", "NN", "$(", "NN", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.11": {"text": "Reden: gut. Tun: oh kontr\u00e4r . . .", "tokens": ["Re\u00b7den", ":", "gut", ".", "Tun", ":", "oh", "kont\u00b7r\u00e4r", ".", ".", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "$.", "ADJD", "$.", "FM.la", "$.", "FM", "FM", "$.", "$.", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.12": {"text": "Ach, da\u00df es doch einmal umgekehrt w\u00e4r \u2013!", "tokens": ["Ach", ",", "da\u00df", "es", "doch", "ein\u00b7mal", "um\u00b7ge\u00b7kehrt", "w\u00e4r", "\u2013", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ITJ", "$,", "KOUS", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$(", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Worte und Taten . . . Als da ist die Regierung:", "tokens": ["Wor\u00b7te", "und", "Ta\u00b7ten", ".", ".", ".", "Als", "da", "ist", "die", "Re\u00b7gie\u00b7rung", ":"], "token_info": ["word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$.", "$.", "$.", "KOUS", "ADV", "VAFIN", "ART", "NN", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da hat sie im Reichstag zur Redeverzierung", "tokens": ["Da", "hat", "sie", "im", "Reichs\u00b7tag", "zur", "Re\u00b7de\u00b7ver\u00b7zie\u00b7rung"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "NN", "APPRART", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "gewi\u00dfe Floskeln, gewi\u00dfe Phrasen,", "tokens": ["ge\u00b7wi\u00b7\u00dfe", "Flos\u00b7keln", ",", "ge\u00b7wi\u00b7\u00dfe", "Phra\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "tut gro\u00dfm\u00e4chtig Posaune blasen \u2013", "tokens": ["tut", "gro\u00df\u00b7m\u00e4ch\u00b7tig", "Po\u00b7sau\u00b7ne", "bla\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "NN", "VVINF", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "und die Pressetrib\u00fcne h\u00f6rt aufmerksam zu . . .", "tokens": ["und", "die", "Pres\u00b7se\u00b7tri\u00b7b\u00fc\u00b7ne", "h\u00f6rt", "auf\u00b7merk\u00b7sam", "zu", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$.", "$.", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.6": {"text": "Und dann geht alles zu s\u00fc\u00dfer Ruh.", "tokens": ["Und", "dann", "geht", "al\u00b7les", "zu", "s\u00fc\u00b7\u00dfer", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "PTKZU", "ADJA", "NN", "$."], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.13": {"line.1": {"text": "Man werde \u2013 spricht man \u2013 den Kapp-Putsch bestrafen.", "tokens": ["Man", "wer\u00b7de", "\u2013", "spricht", "man", "\u2013", "den", "Kapp\u00b7Putsch", "be\u00b7stra\u00b7fen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$(", "VVFIN", "PIS", "$(", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Man geht aber sachteken, sachteken schlafen.", "tokens": ["Man", "geht", "a\u00b7ber", "sach\u00b7te\u00b7ken", ",", "sach\u00b7te\u00b7ken", "schla\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "VVINF", "$,", "ADJD", "VVINF", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Man werde \u2013 spricht man \u2013 das Heer reformieren.", "tokens": ["Man", "wer\u00b7de", "\u2013", "spricht", "man", "\u2013", "das", "Heer", "re\u00b7for\u00b7mie\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "$(", "VVFIN", "PIS", "$(", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Man steht aber stramm vor Stabsoffizieren.", "tokens": ["Man", "steht", "a\u00b7ber", "stramm", "vor", "Stabs\u00b7of\u00b7fi\u00b7zie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "VVFIN", "APPR", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Man erstrebe in der ganzen Verwaltung", "tokens": ["Man", "er\u00b7stre\u00b7be", "in", "der", "gan\u00b7zen", "Ver\u00b7wal\u00b7tung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "eine neue, demokratische Haltung.", "tokens": ["ei\u00b7ne", "neu\u00b7e", ",", "de\u00b7mo\u00b7kra\u00b7ti\u00b7sche", "Hal\u00b7tung", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "+-+---+--+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "Man \u00e4ndre Schule und Universit\u00e4t . . .", "tokens": ["Man", "\u00e4nd\u00b7re", "Schu\u00b7le", "und", "U\u00b7niv\u00b7er\u00b7si\u00b7t\u00e4t", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIS", "ADJA", "NN", "KON", "NN", "$.", "$.", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Aber wie das so geht:", "tokens": ["A\u00b7ber", "wie", "das", "so", "geht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PDS", "ADV", "VVFIN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.9": {"text": "Warum denn gleich tun? Das w\u00e4re sch\u00f6n dumm.", "tokens": ["Wa\u00b7rum", "denn", "gleich", "tun", "?", "Das", "w\u00e4\u00b7re", "sch\u00f6n", "dumm", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "VVINF", "$.", "PDS", "VAFIN", "ADJD", "ADJD", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.10": {"text": "Reden gen\u00fcgt ja dem Publikum!", "tokens": ["Re\u00b7den", "ge\u00b7n\u00fcgt", "ja", "dem", "Pub\u00b7li\u00b7kum", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.14": {"line.1": {"text": "Wenn einer bei uns nur etwas sagt,", "tokens": ["Wenn", "ei\u00b7ner", "bei", "uns", "nur", "et\u00b7was", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PPER", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+----+-+", "measure": "dactylic.init"}, "line.2": {"text": "ists gar nicht mehr n\u00f6tig, da\u00df er was wagt.", "tokens": ["ists", "gar", "nicht", "mehr", "n\u00f6\u00b7tig", ",", "da\u00df", "er", "was", "wagt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "ADJD", "$,", "KOUS", "PPER", "PIS", "VVFIN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Er mu\u00df nur reden, verk\u00fcnden, bullern \u2013", "tokens": ["Er", "mu\u00df", "nur", "re\u00b7den", ",", "ver\u00b7k\u00fcn\u00b7den", ",", "bul\u00b7lern", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VVINF", "$,", "VVPP", "$,", "VVFIN", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "ihr werdet schon alle nach Hause kullern.", "tokens": ["ihr", "wer\u00b7det", "schon", "al\u00b7le", "nach", "Hau\u00b7se", "kul\u00b7lern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "APPR", "NN", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Er mu\u00df nur bombastisch prophezein \u2013", "tokens": ["Er", "mu\u00df", "nur", "bom\u00b7bas\u00b7tisch", "pro\u00b7phe\u00b7zein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "PTKVZ", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "nachzupr\u00fcfen f\u00e4llt niemandem ein.", "tokens": ["nach\u00b7zu\u00b7pr\u00fc\u00b7fen", "f\u00e4llt", "nie\u00b7man\u00b7dem", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIZU", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "+-+-----+", "measure": "unknown.measure.tri"}, "line.7": {"text": "Mit einem Wort: das Grammophon!", "tokens": ["Mit", "ei\u00b7nem", "Wort", ":", "das", "Gram\u00b7mo\u00b7phon", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$.", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Das Weitere \u2013 ach! das findet sich schon.", "tokens": ["Das", "Wei\u00b7te\u00b7re", "\u2013", "ach", "!", "das", "fin\u00b7det", "sich", "schon", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$(", "ITJ", "$.", "PDS", "VVFIN", "PRF", "ADV", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Wir: Demokratie!", "tokens": ["Wir", ":", "De\u00b7mo\u00b7kra\u00b7tie", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["PPER", "$.", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.10": {"text": "Immer mit die Ruhe!", "tokens": ["Im\u00b7mer", "mit", "die", "Ru\u00b7he", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.11": {"text": "Eine Sage ist keine Tue.", "tokens": ["Ei\u00b7ne", "Sa\u00b7ge", "ist", "kei\u00b7ne", "Tue", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}}}}