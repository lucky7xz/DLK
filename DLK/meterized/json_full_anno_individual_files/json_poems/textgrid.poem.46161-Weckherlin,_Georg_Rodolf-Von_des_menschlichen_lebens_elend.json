{"textgrid.poem.46161": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Von des menschlichen lebens elend", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Du wenig kot, du wenig staub", "tokens": ["Du", "we\u00b7nig", "kot", ",", "du", "we\u00b7nig", "staub"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "PIS", "VVFIN", "$,", "PPER", "PIS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "hochm\u00fctig durch ein wenig leben,", "tokens": ["hoch\u00b7m\u00fc\u00b7tig", "durch", "ein", "we\u00b7nig", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "durch welches leben wie ein laub", "tokens": ["durch", "wel\u00b7ches", "le\u00b7ben", "wie", "ein", "laub"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "VVFIN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "du kanst ein weil alhie umschweben.", "tokens": ["du", "kanst", "ein", "weil", "al\u00b7hie", "um\u00b7schwe\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "KOUS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Du gras, du heu, in einer stund", "tokens": ["Du", "gras", ",", "du", "heu", ",", "in", "ei\u00b7ner", "stund"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "NN", "$,", "PPER", "ADV", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bald gr\u00fcnend-frisch und bald verdorben,", "tokens": ["bald", "gr\u00fc\u00b7nen\u00b7dfrisch", "und", "bald", "ver\u00b7dor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "mensch, der du, eh dein g\u00e4nger mund", "tokens": ["mensch", ",", "der", "du", ",", "eh", "dein", "g\u00e4n\u00b7ger", "mund"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "PRELS", "PPER", "$,", "KOUS", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "dich sterblich nennet, oft gestorben.", "tokens": ["dich", "sterb\u00b7lich", "nen\u00b7net", ",", "oft", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVFIN", "$,", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Der du dich achtest nicht gering,", "tokens": ["Der", "du", "dich", "ach\u00b7test", "nicht", "ge\u00b7ring", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PRF", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mensch, nein ihr menschen all zusamen,", "tokens": ["mensch", ",", "nein", "ihr", "men\u00b7schen", "all", "zu\u00b7sa\u00b7men", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PTKANT", "PPOSAT", "NN", "PIAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "seid ihr wol mehr dan pfifferling", "tokens": ["seid", "ihr", "wol", "mehr", "dan", "pfif\u00b7fer\u00b7ling"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "VVFIN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und was noch einen schlechtern namen?", "tokens": ["und", "was", "noch", "ei\u00b7nen", "schlech\u00b7tern", "na\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Nein, wan schon euers lebens saft,", "tokens": ["Nein", ",", "wan", "schon", "eu\u00b7ers", "le\u00b7bens", "saft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWAV", "ADV", "PPOSAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "was l\u00e4nger dan tag und nacht wehret,", "tokens": ["was", "l\u00e4n\u00b7ger", "dan", "tag", "und", "nacht", "weh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "ADV", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "sagt mir doch, lieber, aus was kraft", "tokens": ["sagt", "mir", "doch", ",", "lie\u00b7ber", ",", "aus", "was", "kraft"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$,", "ADV", "$,", "APPR", "PRELS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die welt ihr und die welt euch ehret?", "tokens": ["die", "welt", "ihr", "und", "die", "welt", "euch", "eh\u00b7ret", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "KON", "ART", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Wan ihr dan nichts, ist die welt mehr,", "tokens": ["Wan", "ihr", "dan", "nichts", ",", "ist", "die", "welt", "mehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PIS", "$,", "VAFIN", "ART", "NN", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "dan ein versamlung alles b\u00f6sen?", "tokens": ["dan", "ein", "ver\u00b7sam\u00b7lung", "al\u00b7les", "b\u00f6\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "was ist ihr lust, ihr ruhm, ihr ehr,", "tokens": ["was", "ist", "ihr", "lust", ",", "ihr", "ruhm", ",", "ihr", "ehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "VVFIN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "dan leid, spot, schand und loses wesen?", "tokens": ["dan", "leid", ",", "spot", ",", "schand", "und", "lo\u00b7ses", "we\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "VVFIN", "$,", "VVFIN", "KON", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Was sihet und was kan man sunst", "tokens": ["Was", "si\u00b7het", "und", "was", "kan", "man", "sunst"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "KON", "PWS", "VMFIN", "PIS", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bei h\u00f6fen, dan gewaltig liegen,", "tokens": ["bei", "h\u00f6\u00b7fen", ",", "dan", "ge\u00b7wal\u00b7tig", "lie\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "$,", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan mit ehrgeiz, schalkheit, misgunst,", "tokens": ["dan", "mit", "ehr\u00b7geiz", ",", "schalk\u00b7heit", ",", "mis\u00b7gunst", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "APPR", "NE", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "fuchsschw\u00e4nzen, stolz und schimpf betriegen?", "tokens": ["fuchs\u00b7schw\u00e4n\u00b7zen", ",", "stolz", "und", "schimpf", "be\u00b7trie\u00b7gen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Kan einsam wol in einem haus", "tokens": ["Kan", "ein\u00b7sam", "wol", "in", "ei\u00b7nem", "haus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADJD", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "ein mensch ohn allen sorgen wohnen?", "tokens": ["ein", "mensch", "ohn", "al\u00b7len", "sor\u00b7gen", "woh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer handlet \u00fcber meer ohn graus?", "tokens": ["wer", "hand\u00b7let", "\u00fc\u00b7ber", "meer", "ohn", "graus", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "kan der feldbau ohn m\u00fch belohnen?", "tokens": ["kan", "der", "feld\u00b7bau", "ohn", "m\u00fch", "be\u00b7loh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "APPR", "ADJD", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Wer reiset durch die welt mit lust,", "tokens": ["Wer", "rei\u00b7set", "durch", "die", "welt", "mit", "lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df er sich niemals zu befahren?", "tokens": ["da\u00df", "er", "sich", "nie\u00b7mals", "zu", "be\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welche reichtum kan die brust", "tokens": ["und", "wel\u00b7che", "reich\u00b7tum", "kan", "die", "brust"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "NN", "VMFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "stets f\u00fcr sorg und verdru\u00df bewahren?", "tokens": ["stets", "f\u00fcr", "sorg", "und", "ver\u00b7dru\u00df", "be\u00b7wah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "KON", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.9": {"line.1": {"text": "Wer ist, so lang er arm, ohn klag?", "tokens": ["Wer", "ist", ",", "so", "lang", "er", "arm", ",", "ohn", "klag", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "$,", "ADV", "ADJD", "PPER", "ADJD", "$,", "KOUI", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wer hat ein weib, und ist sein eigen?", "tokens": ["wer", "hat", "ein", "weib", ",", "und", "ist", "sein", "ei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NN", "$,", "KON", "VAFIN", "PPOSAT", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer ist, weil ledig er, ohn plag", "tokens": ["wer", "ist", ",", "weil", "le\u00b7dig", "er", ",", "ohn", "plag"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "$,", "KOUS", "ADJD", "PPER", "$,", "KOUI", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "wan seine tag ohn freind sich neigen?", "tokens": ["wan", "sei\u00b7ne", "tag", "ohn", "freind", "sich", "nei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "APPR", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Und wer ist kindlos und erblos", "tokens": ["Und", "wer", "ist", "kind\u00b7los", "und", "er\u00b7blos"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "ganz abzusterben, nicht verdrossen?", "tokens": ["ganz", "ab\u00b7zu\u00b7ster\u00b7ben", ",", "nicht", "ver\u00b7dros\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVIZU", "$,", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer hat vil kinder, dessen scho\u00df", "tokens": ["wer", "hat", "vil", "kin\u00b7der", ",", "des\u00b7sen", "scho\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "PIAT", "NN", "$,", "PDS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "mit forcht und angst nicht oft durchschossen.", "tokens": ["mit", "forcht", "und", "angst", "nicht", "oft", "durch\u00b7schos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "VVFIN", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Und welches j\u00fcnglings herz und hitz", "tokens": ["Und", "wel\u00b7ches", "j\u00fcng\u00b7lings", "herz", "und", "hitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verbringet nichts, das ihn zu reuen?", "tokens": ["ver\u00b7brin\u00b7get", "nichts", ",", "das", "ihn", "zu", "reu\u00b7en", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "PRELS", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welches hohen alters witz", "tokens": ["und", "wel\u00b7ches", "ho\u00b7hen", "al\u00b7ters", "witz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "kan sich f\u00fcr k\u00e4lt und schwachheit freien?", "tokens": ["kan", "sich", "f\u00fcr", "k\u00e4lt", "und", "schwach\u00b7heit", "frei\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPR", "VVFIN", "KON", "ADJD", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.12": {"line.1": {"text": "O dan du stolzer mensch betracht,", "tokens": ["O", "dan", "du", "stol\u00b7zer", "mensch", "be\u00b7tracht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PPER", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "was du nur aus dir selbs zu machen!", "tokens": ["was", "du", "nur", "aus", "dir", "selbs", "zu", "ma\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "APPR", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ein kind, kaum in die welt gebracht,", "tokens": ["ein", "kind", ",", "kaum", "in", "die", "welt", "ge\u00b7bracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "will weinen oder kan nicht lachen,", "tokens": ["will", "wei\u00b7nen", "o\u00b7der", "kan", "nicht", "la\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "KON", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und (weiser, dan du) lehret dich,", "tokens": ["Und", "(", "wei\u00b7ser", ",", "dan", "du", ")", "leh\u00b7ret", "dich", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$(", "ADJA", "$,", "ADV", "PPER", "$(", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wie der mensch sein elendes leben", "tokens": ["wie", "der", "mensch", "sein", "e\u00b7len\u00b7des", "le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "PPOSAT", "ADJA", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "soll weinend fangen an, und sich", "tokens": ["soll", "wei\u00b7nend", "fan\u00b7gen", "an", ",", "und", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "VVPP", "VVFIN", "PTKVZ", "$,", "KON", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "erfreuend dem tod gern ergeben.", "tokens": ["er\u00b7freu\u00b7end", "dem", "tod", "gern", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.14": {"line.1": {"text": "Du wenig kot, du wenig staub", "tokens": ["Du", "we\u00b7nig", "kot", ",", "du", "we\u00b7nig", "staub"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "PIS", "VVFIN", "$,", "PPER", "PIS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "hochm\u00fctig durch ein wenig leben,", "tokens": ["hoch\u00b7m\u00fc\u00b7tig", "durch", "ein", "we\u00b7nig", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "durch welches leben wie ein laub", "tokens": ["durch", "wel\u00b7ches", "le\u00b7ben", "wie", "ein", "laub"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "VVFIN", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "du kanst ein weil alhie umschweben.", "tokens": ["du", "kanst", "ein", "weil", "al\u00b7hie", "um\u00b7schwe\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "KOUS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Du gras, du heu, in einer stund", "tokens": ["Du", "gras", ",", "du", "heu", ",", "in", "ei\u00b7ner", "stund"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "NN", "$,", "PPER", "ADV", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bald gr\u00fcnend-frisch und bald verdorben,", "tokens": ["bald", "gr\u00fc\u00b7nen\u00b7dfrisch", "und", "bald", "ver\u00b7dor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "mensch, der du, eh dein g\u00e4nger mund", "tokens": ["mensch", ",", "der", "du", ",", "eh", "dein", "g\u00e4n\u00b7ger", "mund"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "PRELS", "PPER", "$,", "KOUS", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "dich sterblich nennet, oft gestorben.", "tokens": ["dich", "sterb\u00b7lich", "nen\u00b7net", ",", "oft", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVFIN", "$,", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Der du dich achtest nicht gering,", "tokens": ["Der", "du", "dich", "ach\u00b7test", "nicht", "ge\u00b7ring", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PRF", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "mensch, nein ihr menschen all zusamen,", "tokens": ["mensch", ",", "nein", "ihr", "men\u00b7schen", "all", "zu\u00b7sa\u00b7men", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PTKANT", "PPOSAT", "NN", "PIAT", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "seid ihr wol mehr dan pfifferling", "tokens": ["seid", "ihr", "wol", "mehr", "dan", "pfif\u00b7fer\u00b7ling"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "VVFIN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und was noch einen schlechtern namen?", "tokens": ["und", "was", "noch", "ei\u00b7nen", "schlech\u00b7tern", "na\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Nein, wan schon euers lebens saft,", "tokens": ["Nein", ",", "wan", "schon", "eu\u00b7ers", "le\u00b7bens", "saft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWAV", "ADV", "PPOSAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "was l\u00e4nger dan tag und nacht wehret,", "tokens": ["was", "l\u00e4n\u00b7ger", "dan", "tag", "und", "nacht", "weh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "ADV", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "sagt mir doch, lieber, aus was kraft", "tokens": ["sagt", "mir", "doch", ",", "lie\u00b7ber", ",", "aus", "was", "kraft"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$,", "ADV", "$,", "APPR", "PRELS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die welt ihr und die welt euch ehret?", "tokens": ["die", "welt", "ihr", "und", "die", "welt", "euch", "eh\u00b7ret", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "KON", "ART", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Wan ihr dan nichts, ist die welt mehr,", "tokens": ["Wan", "ihr", "dan", "nichts", ",", "ist", "die", "welt", "mehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PIS", "$,", "VAFIN", "ART", "NN", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "dan ein versamlung alles b\u00f6sen?", "tokens": ["dan", "ein", "ver\u00b7sam\u00b7lung", "al\u00b7les", "b\u00f6\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "was ist ihr lust, ihr ruhm, ihr ehr,", "tokens": ["was", "ist", "ihr", "lust", ",", "ihr", "ruhm", ",", "ihr", "ehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "VVFIN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "dan leid, spot, schand und loses wesen?", "tokens": ["dan", "leid", ",", "spot", ",", "schand", "und", "lo\u00b7ses", "we\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "VVFIN", "$,", "VVFIN", "KON", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Was sihet und was kan man sunst", "tokens": ["Was", "si\u00b7het", "und", "was", "kan", "man", "sunst"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "KON", "PWS", "VMFIN", "PIS", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bei h\u00f6fen, dan gewaltig liegen,", "tokens": ["bei", "h\u00f6\u00b7fen", ",", "dan", "ge\u00b7wal\u00b7tig", "lie\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "$,", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan mit ehrgeiz, schalkheit, misgunst,", "tokens": ["dan", "mit", "ehr\u00b7geiz", ",", "schalk\u00b7heit", ",", "mis\u00b7gunst", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "APPR", "NE", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "fuchsschw\u00e4nzen, stolz und schimpf betriegen?", "tokens": ["fuchs\u00b7schw\u00e4n\u00b7zen", ",", "stolz", "und", "schimpf", "be\u00b7trie\u00b7gen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADJD", "KON", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Kan einsam wol in einem haus", "tokens": ["Kan", "ein\u00b7sam", "wol", "in", "ei\u00b7nem", "haus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADJD", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "ein mensch ohn allen sorgen wohnen?", "tokens": ["ein", "mensch", "ohn", "al\u00b7len", "sor\u00b7gen", "woh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer handlet \u00fcber meer ohn graus?", "tokens": ["wer", "hand\u00b7let", "\u00fc\u00b7ber", "meer", "ohn", "graus", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "kan der feldbau ohn m\u00fch belohnen?", "tokens": ["kan", "der", "feld\u00b7bau", "ohn", "m\u00fch", "be\u00b7loh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "APPR", "ADJD", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.21": {"line.1": {"text": "Wer reiset durch die welt mit lust,", "tokens": ["Wer", "rei\u00b7set", "durch", "die", "welt", "mit", "lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df er sich niemals zu befahren?", "tokens": ["da\u00df", "er", "sich", "nie\u00b7mals", "zu", "be\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welche reichtum kan die brust", "tokens": ["und", "wel\u00b7che", "reich\u00b7tum", "kan", "die", "brust"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "NN", "VMFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "stets f\u00fcr sorg und verdru\u00df bewahren?", "tokens": ["stets", "f\u00fcr", "sorg", "und", "ver\u00b7dru\u00df", "be\u00b7wah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "KON", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.22": {"line.1": {"text": "Wer ist, so lang er arm, ohn klag?", "tokens": ["Wer", "ist", ",", "so", "lang", "er", "arm", ",", "ohn", "klag", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "$,", "ADV", "ADJD", "PPER", "ADJD", "$,", "KOUI", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wer hat ein weib, und ist sein eigen?", "tokens": ["wer", "hat", "ein", "weib", ",", "und", "ist", "sein", "ei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NN", "$,", "KON", "VAFIN", "PPOSAT", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer ist, weil ledig er, ohn plag", "tokens": ["wer", "ist", ",", "weil", "le\u00b7dig", "er", ",", "ohn", "plag"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "$,", "KOUS", "ADJD", "PPER", "$,", "KOUI", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "wan seine tag ohn freind sich neigen?", "tokens": ["wan", "sei\u00b7ne", "tag", "ohn", "freind", "sich", "nei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "APPR", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Und wer ist kindlos und erblos", "tokens": ["Und", "wer", "ist", "kind\u00b7los", "und", "er\u00b7blos"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VAFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "ganz abzusterben, nicht verdrossen?", "tokens": ["ganz", "ab\u00b7zu\u00b7ster\u00b7ben", ",", "nicht", "ver\u00b7dros\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVIZU", "$,", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "wer hat vil kinder, dessen scho\u00df", "tokens": ["wer", "hat", "vil", "kin\u00b7der", ",", "des\u00b7sen", "scho\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VAFIN", "PIAT", "NN", "$,", "PDS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "mit forcht und angst nicht oft durchschossen.", "tokens": ["mit", "forcht", "und", "angst", "nicht", "oft", "durch\u00b7schos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "VVFIN", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Und welches j\u00fcnglings herz und hitz", "tokens": ["Und", "wel\u00b7ches", "j\u00fcng\u00b7lings", "herz", "und", "hitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "verbringet nichts, das ihn zu reuen?", "tokens": ["ver\u00b7brin\u00b7get", "nichts", ",", "das", "ihn", "zu", "reu\u00b7en", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "PRELS", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und welches hohen alters witz", "tokens": ["und", "wel\u00b7ches", "ho\u00b7hen", "al\u00b7ters", "witz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "kan sich f\u00fcr k\u00e4lt und schwachheit freien?", "tokens": ["kan", "sich", "f\u00fcr", "k\u00e4lt", "und", "schwach\u00b7heit", "frei\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPR", "VVFIN", "KON", "ADJD", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.25": {"line.1": {"text": "O dan du stolzer mensch betracht,", "tokens": ["O", "dan", "du", "stol\u00b7zer", "mensch", "be\u00b7tracht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PPER", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "was du nur aus dir selbs zu machen!", "tokens": ["was", "du", "nur", "aus", "dir", "selbs", "zu", "ma\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "APPR", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ein kind, kaum in die welt gebracht,", "tokens": ["ein", "kind", ",", "kaum", "in", "die", "welt", "ge\u00b7bracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "will weinen oder kan nicht lachen,", "tokens": ["will", "wei\u00b7nen", "o\u00b7der", "kan", "nicht", "la\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "KON", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Und (weiser, dan du) lehret dich,", "tokens": ["Und", "(", "wei\u00b7ser", ",", "dan", "du", ")", "leh\u00b7ret", "dich", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$(", "ADJA", "$,", "ADV", "PPER", "$(", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wie der mensch sein elendes leben", "tokens": ["wie", "der", "mensch", "sein", "e\u00b7len\u00b7des", "le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "PPOSAT", "ADJA", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "soll weinend fangen an, und sich", "tokens": ["soll", "wei\u00b7nend", "fan\u00b7gen", "an", ",", "und", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "VVPP", "VVFIN", "PTKVZ", "$,", "KON", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "erfreuend dem tod gern ergeben.", "tokens": ["er\u00b7freu\u00b7end", "dem", "tod", "gern", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}}}}