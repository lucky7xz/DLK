{"textgrid.poem.57501": {"metadata": {"author": {"name": "Gottsched, Johann Christoph", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie sehr ich dich bisher, mein ", "genre": "verse", "period": "N.A.", "pub_year": 1733, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie sehr ich dich bisher, mein ", "tokens": ["Wie", "sehr", "ich", "dich", "bis\u00b7her", ",", "mein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PWAV", "ADV", "PPER", "PRF", "ADV", "$,", "PPOSAT"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie sehr mich dein Verstand und redlich Herz ergetzt,", "tokens": ["Wie", "sehr", "mich", "dein", "Ver\u00b7stand", "und", "red\u00b7lich", "Herz", "er\u00b7getzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "PPOSAT", "NN", "KON", "ADJD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und kurz, dein tugendhaft, gelehrt und kluges Wesen,", "tokens": ["Und", "kurz", ",", "dein", "tu\u00b7gend\u00b7haft", ",", "ge\u00b7lehrt", "und", "klu\u00b7ges", "We\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPOSAT", "NN", "$,", "VVPP", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "La\u00df ich zum erstenmal in diesen Zeilen lesen.", "tokens": ["La\u00df", "ich", "zum", "ers\u00b7ten\u00b7mal", "in", "die\u00b7sen", "Zei\u00b7len", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADV", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich h\u00e4tt es l\u00e4ngst gethan, aus Antrieb meiner Pflicht,", "tokens": ["Ich", "h\u00e4tt", "es", "l\u00e4ngst", "ge\u00b7than", ",", "aus", "An\u00b7trieb", "mei\u00b7ner", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Es fehlte mir dazu an Lust und Vorsatz nicht:", "tokens": ["Es", "fehl\u00b7te", "mir", "da\u00b7zu", "an", "Lust", "und", "Vor\u00b7satz", "nicht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PAV", "APPR", "NN", "KON", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Allein, ich w\u00fcnschte stets, mit sehnlichem Verlangen,", "tokens": ["Al\u00b7lein", ",", "ich", "w\u00fcnschte", "stets", ",", "mit", "sehn\u00b7li\u00b7chem", "Ver\u00b7lan\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "ADV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.8": {"text": "Mehr Anla\u00df, als bisher, zur Lobschrift zu empfangen.", "tokens": ["Mehr", "An\u00b7la\u00df", ",", "als", "bis\u00b7her", ",", "zur", "Lob\u00b7schrift", "zu", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOUS", "ADV", "$,", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.9": {"text": "Ich dringe mich nicht gern durch Schm\u00e4ucheleyen ein,", "tokens": ["Ich", "drin\u00b7ge", "mich", "nicht", "gern", "durch", "Schm\u00e4u\u00b7che\u00b7le\u00b7yen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Mein Griffel wollte nur der Wahrheit Herold seyn;", "tokens": ["Mein", "Grif\u00b7fel", "woll\u00b7te", "nur", "der", "Wahr\u00b7heit", "He\u00b7rold", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ADV", "ART", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und wartete mit Flei\u00df auf \u00f6ffentliche Proben,", "tokens": ["Und", "war\u00b7te\u00b7te", "mit", "Flei\u00df", "auf", "\u00f6f\u00b7fent\u00b7li\u00b7che", "Pro\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Um deine Trefflichkeit auch \u00f6ffentlich zu loben.", "tokens": ["Um", "dei\u00b7ne", "Treff\u00b7lich\u00b7keit", "auch", "\u00f6f\u00b7fent\u00b7lich", "zu", "lo\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Daran gebrach es mir. Denn ob dir wohl kein Tag,", "tokens": ["Da\u00b7ran", "ge\u00b7brach", "es", "mir", ".", "Denn", "ob", "dir", "wohl", "kein", "Tag", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "$.", "KON", "KOUS", "PPER", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kein Augenblick vergeht, der nicht bezeugen mag,", "tokens": ["Kein", "Au\u00b7gen\u00b7blick", "ver\u00b7geht", ",", "der", "nicht", "be\u00b7zeu\u00b7gen", "mag", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$,", "PRELS", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie n\u00fctzlich dein Bem\u00fchn der Stadt, dem Vaterlande,", "tokens": ["Wie", "n\u00fctz\u00b7lich", "dein", "Be\u00b7m\u00fchn", "der", "Stadt", ",", "dem", "Va\u00b7ter\u00b7lan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPOSAT", "NN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und Sachsens Nachbarn ist; indem man mit Bestande", "tokens": ["Und", "Sach\u00b7sens", "Nach\u00b7barn", "ist", ";", "in\u00b7dem", "man", "mit", "Be\u00b7stan\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "NN", "VAFIN", "$.", "KOUS", "PIS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gar wohl behaupten kann, da\u00df deinen Witz und Flei\u00df", "tokens": ["Gar", "wohl", "be\u00b7haup\u00b7ten", "kann", ",", "da\u00df", "dei\u00b7nen", "Witz", "und", "Flei\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVINF", "VMFIN", "$,", "KOUS", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Halb Deutschland schon genie\u00dft, und sehr zu r\u00fchmen weis:", "tokens": ["Halb", "Deutschland", "schon", "ge\u00b7nie\u00dft", ",", "und", "sehr", "zu", "r\u00fch\u00b7men", "weis", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "VVFIN", "$,", "KON", "ADV", "PTKZU", "VVINF", "PTKVZ", "$."], "meter": "++--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "So war doch dieses mir, wenn ich dich loben wollte,", "tokens": ["So", "war", "doch", "die\u00b7ses", "mir", ",", "wenn", "ich", "dich", "lo\u00b7ben", "woll\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PDAT", "PPER", "$,", "KOUS", "PPER", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Kein sattsamer Beweis, der andern zeigen sollte,", "tokens": ["Kein", "satt\u00b7sa\u00b7mer", "Be\u00b7weis", ",", "der", "an\u00b7dern", "zei\u00b7gen", "soll\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "PRELS", "PIS", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Warum ich dich ger\u00fchmt. Die Welt verlangt was mehr,", "tokens": ["Wa\u00b7rum", "ich", "dich", "ge\u00b7r\u00fchmt", ".", "Die", "Welt", "ver\u00b7langt", "was", "mehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "VVPP", "$.", "ART", "NN", "VVFIN", "PWS", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und giebt uns eher nicht ein g\u00fctiges Geh\u00f6r,", "tokens": ["Und", "giebt", "uns", "e\u00b7her", "nicht", "ein", "g\u00fc\u00b7ti\u00b7ges", "Ge\u00b7h\u00f6r", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Als bis man \u00f6ffentlich was sonderbares findet,", "tokens": ["Als", "bis", "man", "\u00f6f\u00b7fent\u00b7lich", "was", "son\u00b7der\u00b7ba\u00b7res", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PIS", "ADJD", "PWS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Worauf man denn das Lob ber\u00fchmter H\u00e4upter gr\u00fcndet.", "tokens": ["Wo\u00b7rauf", "man", "denn", "das", "Lob", "be\u00b7r\u00fchm\u00b7ter", "H\u00e4up\u00b7ter", "gr\u00fcn\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "ADV", "ART", "NN", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Daher geschieht es denn, da\u00df oft ein Namensfest,", "tokens": ["Da\u00b7her", "ge\u00b7schieht", "es", "denn", ",", "da\u00df", "oft", "ein", "Na\u00b7mens\u00b7fest", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Oft noch was wenigers, den Anla\u00df finden l\u00e4\u00dft,", "tokens": ["Oft", "noch", "was", "we\u00b7ni\u00b7gers", ",", "den", "An\u00b7la\u00df", "fin\u00b7den", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PWS", "PIS", "$,", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Zu preisen, wen man ehrt. Allein das hei\u00dft gezwungen:", "tokens": ["Zu", "prei\u00b7sen", ",", "wen", "man", "ehrt", ".", "Al\u00b7lein", "das", "hei\u00dft", "ge\u00b7zwun\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PWS", "PIS", "VVFIN", "$.", "ADV", "PDS", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Drum hab ich meinen Reim dir so nicht aufgedrungen.", "tokens": ["Drum", "hab", "ich", "mei\u00b7nen", "Reim", "dir", "so", "nicht", "auf\u00b7ge\u00b7drun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPOSAT", "NN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Das n\u00e4chstverwichne Jahr versprach mir schon sehr viel;", "tokens": ["Das", "n\u00e4chst\u00b7ver\u00b7wich\u00b7ne", "Jahr", "ver\u00b7sprach", "mir", "schon", "sehr", "viel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Schicksal zeigte mir das l\u00e4ngstgesuchte Ziel,", "tokens": ["Das", "Schick\u00b7sal", "zeig\u00b7te", "mir", "das", "l\u00e4ngst\u00b7ge\u00b7such\u00b7te", "Ziel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Indem ich schon vernahm, da\u00df endlich Wunsch und Hoffen,", "tokens": ["In\u00b7dem", "ich", "schon", "ver\u00b7nahm", ",", "da\u00df", "end\u00b7lich", "Wunsch", "und", "Hof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$,", "KOUS", "ADV", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Durch dein vergr\u00f6\u00dfert Gl\u00fcck vollkommen eingetroffen.", "tokens": ["Durch", "dein", "ver\u00b7gr\u00f6\u00b7\u00dfert", "Gl\u00fcck", "voll\u00b7kom\u00b7men", "ein\u00b7ge\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Man sprach, der Sachsen Haupt, (welch ein gerechter Held!)", "tokens": ["Man", "sprach", ",", "der", "Sach\u00b7sen", "Haupt", ",", "(", "welch", "ein", "ge\u00b7rech\u00b7ter", "Held", "!", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$,", "ART", "NN", "NN", "$,", "$(", "PWAT", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Hof und Land regiert. Es war auch kein Gedichte,", "tokens": ["Der", "Hof", "und", "Land", "re\u00b7giert", ".", "Es", "war", "auch", "kein", "Ge\u00b7dich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$.", "PPER", "VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Wahrheit sch\u00fctzte selbst das fl\u00fcchtige Ger\u00fcchte.", "tokens": ["Die", "Wahr\u00b7heit", "sch\u00fctz\u00b7te", "selbst", "das", "fl\u00fcch\u00b7ti\u00b7ge", "Ge\u00b7r\u00fcch\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Du hattest in der That die Ehre l\u00e4ngst verdient,", "tokens": ["Du", "hat\u00b7test", "in", "der", "That", "die", "Eh\u00b7re", "l\u00e4ngst", "ver\u00b7dient", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ob deine Demuth gleich sich niemals recht erk\u00fchnt,", "tokens": ["Ob", "dei\u00b7ne", "De\u00b7muth", "gleich", "sich", "nie\u00b7mals", "recht", "er\u00b7k\u00fchnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PRF", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "So hoch empor zu sehn; und gar das Gl\u00fcck zu zwingen,", "tokens": ["So", "hoch", "em\u00b7por", "zu", "sehn", ";", "und", "gar", "das", "Gl\u00fcck", "zu", "zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$.", "KON", "ADV", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Durch Gunst und Geld, den Lohn f\u00fcr dein Verdienst zu bringen.", "tokens": ["Durch", "Gunst", "und", "Geld", ",", "den", "Lohn", "f\u00fcr", "dein", "Ver\u00b7dienst", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Wie r\u00fchmlich war dirs nun, was dazumal geschah,", "tokens": ["Wie", "r\u00fchm\u00b7lich", "war", "dirs", "nun", ",", "was", "da\u00b7zu\u00b7mal", "ge\u00b7schah", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PIS", "ADV", "$,", "PRELS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Da\u00df Hof und K\u00f6nig selbst auf jeden Vorzug sah,", "tokens": ["Da\u00df", "Hof", "und", "K\u00f6\u00b7nig", "selbst", "auf", "je\u00b7den", "Vor\u00b7zug", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ADV", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Der dich vor andern schm\u00fcckt; und, ohne dich zu fragen,", "tokens": ["Der", "dich", "vor", "an\u00b7dern", "schm\u00fcckt", ";", "und", ",", "oh\u00b7ne", "dich", "zu", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "PIS", "VVFIN", "$.", "KON", "$,", "KOUI", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die neue W\u00fcrde dir aus Gnaden angetragen.", "tokens": ["Die", "neu\u00b7e", "W\u00fcr\u00b7de", "dir", "aus", "Gna\u00b7den", "an\u00b7ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Allein hier zeigte sich auch die Bescheidenheit,", "tokens": ["Al\u00b7lein", "hier", "zeig\u00b7te", "sich", "auch", "die", "Be\u00b7schei\u00b7den\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die deine Tugend ziert. Das Gift der Eitelkeit", "tokens": ["Die", "dei\u00b7ne", "Tu\u00b7gend", "ziert", ".", "Das", "Gift", "der", "Ei\u00b7tel\u00b7keit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "NN", "VVFIN", "$.", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hat dich wohl nie befleckt. Du pflegst durch bunte Schaalen,", "tokens": ["Hat", "dich", "wohl", "nie", "be\u00b7fleckt", ".", "Du", "pflegst", "durch", "bun\u00b7te", "Schaa\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVPP", "$.", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Darinn der Kern gebricht, der Welt nichts vorzupralen.", "tokens": ["Da\u00b7rinn", "der", "Kern", "ge\u00b7bricht", ",", "der", "Welt", "nichts", "vor\u00b7zu\u00b7pra\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "VVFIN", "$,", "ART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und hast du diesen nur, so achtest du es nicht,", "tokens": ["Und", "hast", "du", "die\u00b7sen", "nur", ",", "so", "ach\u00b7test", "du", "es", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PDAT", "ADV", "$,", "ADV", "VVFIN", "PPER", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn gleich von au\u00dfen dir ein gro\u00dfer Schein gebricht.", "tokens": ["Wenn", "gleich", "von", "au\u00b7\u00dfen", "dir", "ein", "gro\u00b7\u00dfer", "Schein", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "ADV", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So gieng es denn auch hier. Du warest schon zufrieden,", "tokens": ["So", "gieng", "es", "denn", "auch", "hier", ".", "Du", "wa\u00b7rest", "schon", "zu\u00b7frie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADV", "$.", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Mit dem, was dir das Gl\u00fcck und dein Verdienst beschieden;", "tokens": ["Mit", "dem", ",", "was", "dir", "das", "Gl\u00fcck", "und", "dein", "Ver\u00b7dienst", "be\u00b7schie\u00b7den", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PWS", "PPER", "ART", "NN", "KON", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und schienest, da sich schon des Alters Vorspiel zeigt,", "tokens": ["Und", "schie\u00b7nest", ",", "da", "sich", "schon", "des", "Al\u00b7ters", "Vor\u00b7spiel", "zeigt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PRF", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Zu keiner Aenderung der Lebensart geneigt:", "tokens": ["Zu", "kei\u00b7ner", "A\u00b7en\u00b7de\u00b7rung", "der", "Le\u00b7ben\u00b7sart", "ge\u00b7neigt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Zumal es besser bleibt, verdienen, und nicht haben;", "tokens": ["Zu\u00b7mal", "es", "bes\u00b7ser", "bleibt", ",", "ver\u00b7die\u00b7nen", ",", "und", "nicht", "ha\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVFIN", "$,", "VVFIN", "$,", "KON", "PTKNEG", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Als leer an W\u00fcrden seyn, und sich an Titeln laben.", "tokens": ["Als", "leer", "an", "W\u00fcr\u00b7den", "seyn", ",", "und", "sich", "an", "Ti\u00b7teln", "la\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "NN", "VAINF", "$,", "KON", "PRF", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "So ward indessen mir der Anla\u00df auch geraubt,", "tokens": ["So", "ward", "in\u00b7des\u00b7sen", "mir", "der", "An\u00b7la\u00df", "auch", "ge\u00b7raubt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PPER", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Den ich, zu deinem Ruhm, zu haben schon geglaubt.", "tokens": ["Den", "ich", ",", "zu", "dei\u00b7nem", "Ruhm", ",", "zu", "ha\u00b7ben", "schon", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "$,", "APPR", "PPOSAT", "NN", "$,", "PTKZU", "VAINF", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich schwieg also bisher, und lernte mit Ergetzen,", "tokens": ["Ich", "schwieg", "al\u00b7so", "bis\u00b7her", ",", "und", "lern\u00b7te", "mit", "Er\u00b7get\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "KON", "VVFIN", "APPR", "NN", "$,"], "meter": "-+--+--+--+--", "measure": "amphibrach.tetra.plus"}, "line.4": {"text": "Dein Wesen, theurer Mann! fast t\u00e4glich h\u00f6her sch\u00e4tzen.", "tokens": ["Dein", "We\u00b7sen", ",", "theu\u00b7rer", "Mann", "!", "fast", "t\u00e4g\u00b7lich", "h\u00f6\u00b7her", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "ADJD", "NN", "$.", "ADV", "ADJD", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dein Bruder, der wie du, des Landes Wohlfahrt st\u00fctzt,", "tokens": ["Dein", "Bru\u00b7der", ",", "der", "wie", "du", ",", "des", "Lan\u00b7des", "Wohl\u00b7fahrt", "st\u00fctzt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "KOKOM", "PPER", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Gerechtigkeit und Recht durch manchen Ausspruch sch\u00fctzt,", "tokens": ["Ge\u00b7rech\u00b7tig\u00b7keit", "und", "Recht", "durch", "man\u00b7chen", "Aus\u00b7spruch", "sch\u00fctzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "War ebenfalls, wie du, ein Gegenstand der Augen,", "tokens": ["War", "e\u00b7ben\u00b7falls", ",", "wie", "du", ",", "ein", "Ge\u00b7gen\u00b7stand", "der", "Au\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$,", "PWAV", "PPER", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und konnte mir, nebst dir, zum Musterbilde taugen;", "tokens": ["Und", "konn\u00b7te", "mir", ",", "nebst", "dir", ",", "zum", "Mus\u00b7ter\u00b7bil\u00b7de", "tau\u00b7gen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "$,", "APPR", "PPER", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Zum Muster, da\u00df sich noch ein tugendhafter Mann,", "tokens": ["Zum", "Mus\u00b7ter", ",", "da\u00df", "sich", "noch", "ein", "tu\u00b7gend\u00b7haf\u00b7ter", "Mann", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "KOUS", "PRF", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Durch F\u00e4higkeit und Flei\u00df zu Ehren bringen kann.", "tokens": ["Durch", "F\u00e4\u00b7hig\u00b7keit", "und", "Flei\u00df", "zu", "Eh\u00b7ren", "brin\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Doch mu\u00dft ich alles das noch allezeit verschweigen,", "tokens": ["Doch", "mu\u00dft", "ich", "al\u00b7les", "das", "noch", "al\u00b7le\u00b7zeit", "ver\u00b7schwei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PIS", "ART", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und dorft es eher nicht, als heute, deutlich zeigen.", "tokens": ["Und", "dorft", "es", "e\u00b7her", "nicht", ",", "als", "heu\u00b7te", ",", "deut\u00b7lich", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "KOUS", "ADV", "$,", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Denn itzt erscheint der Tag, da dein geschickter Sohn", "tokens": ["Denn", "itzt", "er\u00b7scheint", "der", "Tag", ",", "da", "dein", "ge\u00b7schick\u00b7ter", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$,", "KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der sch\u00f6ne Lehrerschmuck im Philosophenorden,", "tokens": ["Der", "sch\u00f6\u00b7ne", "Leh\u00b7rer\u00b7schmuck", "im", "Phi\u00b7lo\u00b7so\u00b7phen\u00b7or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mit allgemeinem Ruhm auch ihm zu Theil geworden.", "tokens": ["Mit", "all\u00b7ge\u00b7mei\u00b7nem", "Ruhm", "auch", "ihm", "zu", "Theil", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "PPER", "APPR", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ganz Leipzig sieht dabey die wohlgerathne Frucht,", "tokens": ["Ganz", "Leip\u00b7zig", "sieht", "da\u00b7bey", "die", "wohl\u00b7ge\u00b7rath\u00b7ne", "Frucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PAV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das sch\u00f6nste Probest\u00fcck von Wagners kluger Zucht;", "tokens": ["Das", "sch\u00f6ns\u00b7te", "Pro\u00b7be\u00b7st\u00fcck", "von", "Wag\u00b7ners", "klu\u00b7ger", "Zucht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Des Vaters Ebenbild in seines Sohnes Jugend,", "tokens": ["Des", "Va\u00b7ters", "E\u00b7ben\u00b7bild", "in", "sei\u00b7nes", "Soh\u00b7nes", "Ju\u00b7gend", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und diesen auf der Spur der v\u00e4terlichen Tugend.", "tokens": ["Und", "die\u00b7sen", "auf", "der", "Spur", "der", "v\u00e4\u00b7ter\u00b7li\u00b7chen", "Tu\u00b7gend", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "APPR", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Man siehts und lobet dich, und r\u00fchmt den Sohn zugleich,", "tokens": ["Man", "siehts", "und", "lo\u00b7bet", "dich", ",", "und", "r\u00fchmt", "den", "Sohn", "zu\u00b7gleich", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "PPER", "$,", "KON", "VVFIN", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und wer nicht neidisch ist, wird an Vergn\u00fcgen reich;", "tokens": ["Und", "wer", "nicht", "nei\u00b7disch", "ist", ",", "wird", "an", "Ver\u00b7gn\u00fc\u00b7gen", "reich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PTKNEG", "ADJD", "VAFIN", "$,", "VAFIN", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und w\u00fcnscht der Vaterstadt viel V\u00e4ter deinesgleichen,", "tokens": ["Und", "w\u00fcnscht", "der", "Va\u00b7ter\u00b7stadt", "viel", "V\u00e4\u00b7ter", "dei\u00b7nes\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Viel S\u00f6hne, wie dein Sohn, dem tausend S\u00f6hne weichen.", "tokens": ["Viel", "S\u00f6h\u00b7ne", ",", "wie", "dein", "Sohn", ",", "dem", "tau\u00b7send", "S\u00f6h\u00b7ne", "wei\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PWAV", "PPOSAT", "NN", "$,", "PRELS", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ich merke, da\u00df man hier vieleicht den Einwurf macht:", "tokens": ["Ich", "mer\u00b7ke", ",", "da\u00df", "man", "hier", "vie\u00b7leicht", "den", "Ein\u00b7wurf", "macht", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PIS", "ADV", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es sey kein Lob f\u00fcr dich, was ich hier ausgedacht.", "tokens": ["Es", "sey", "kein", "Lob", "f\u00fcr", "dich", ",", "was", "ich", "hier", "aus\u00b7ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "APPR", "PPER", "$,", "PWS", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das hie\u00dfe gar nicht viel, den Sohn dahin zu f\u00fchren,", "tokens": ["Das", "hie\u00b7\u00dfe", "gar", "nicht", "viel", ",", "den", "Sohn", "da\u00b7hin", "zu", "f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PTKNEG", "ADV", "$,", "ART", "NN", "PAV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wo soviel andre stehn, und sich mit Lorbern zieren;", "tokens": ["Wo", "so\u00b7viel", "and\u00b7re", "stehn", ",", "und", "sich", "mit", "Lor\u00b7bern", "zie\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "PIS", "VVINF", "$,", "KON", "PRF", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und wo die W\u00fcrdigkeit nicht stets allein gemacht,", "tokens": ["Und", "wo", "die", "W\u00fcr\u00b7dig\u00b7keit", "nicht", "stets", "al\u00b7lein", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "PTKNEG", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df ihnen solcher Schmuck vor andern wird gebracht.", "tokens": ["Da\u00df", "ih\u00b7nen", "sol\u00b7cher", "Schmuck", "vor", "an\u00b7dern", "wird", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "APPR", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Das sey kein gro\u00dfer Ruhm, den Sohn nach zwanzig Jahren", "tokens": ["Das", "sey", "kein", "gro\u00b7\u00dfer", "Ruhm", ",", "den", "Sohn", "nach", "zwan\u00b7zig", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PIAT", "ADJA", "NN", "$,", "ART", "NN", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Magisterm\u00e4\u00dfig sehn; nachdem man oft erfahren,", "tokens": ["Ma\u00b7gis\u00b7ter\u00b7m\u00e4\u00b7\u00dfig", "sehn", ";", "nach\u00b7dem", "man", "oft", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "$.", "KOUS", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Da\u00df hier und sonsten oft ein funfzehnj\u00e4hrig Kind,", "tokens": ["Da\u00df", "hier", "und", "sons\u00b7ten", "oft", "ein", "funf\u00b7zehn\u00b7j\u00e4h\u00b7rig", "Kind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "KON", "VVFIN", "ADV", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Bey dem Verstand und Witz noch gar nicht zeitig sind,", "tokens": ["Bey", "dem", "Ver\u00b7stand", "und", "Witz", "noch", "gar", "nicht", "zei\u00b7tig", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "NN", "ADV", "ADV", "PTKNEG", "ADJD", "VAFIN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.11": {"text": "Der weisen ", "tokens": ["Der", "wei\u00b7sen"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.12": {"text": "Und also h\u00e4tt ich dir ein schlechtes Lob ersonnen.", "tokens": ["Und", "al\u00b7so", "h\u00e4tt", "ich", "dir", "ein", "schlech\u00b7tes", "Lob", "er\u00b7son\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "PPER", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Jedoch, was ich gethan, ist mit Bedacht geschehn.", "tokens": ["Je\u00b7doch", ",", "was", "ich", "ge\u00b7than", ",", "ist", "mit", "Be\u00b7dacht", "ge\u00b7schehn", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWS", "PPER", "VVPP", "$,", "VAFIN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich habe deines Sohns Geschicklichkeit gesehn,", "tokens": ["Ich", "ha\u00b7be", "dei\u00b7nes", "Sohns", "Ge\u00b7schick\u00b7lich\u00b7keit", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und seinen Flei\u00df erkannt, und dich und ihn gepriesen,", "tokens": ["Und", "sei\u00b7nen", "Flei\u00df", "er\u00b7kannt", ",", "und", "dich", "und", "ihn", "ge\u00b7prie\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVPP", "$,", "KON", "PPER", "KON", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn beyder edles Thun sich so gesetzt erwiesen;", "tokens": ["Wenn", "bey\u00b7der", "ed\u00b7les", "Thun", "sich", "so", "ge\u00b7setzt", "er\u00b7wie\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ADJA", "NN", "PRF", "ADV", "VVPP", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht gar zu sehr geeilt, wie so viel andre thun,", "tokens": ["Nicht", "gar", "zu", "sehr", "ge\u00b7eilt", ",", "wie", "so", "viel", "and\u00b7re", "thun", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "PTKA", "ADV", "VVPP", "$,", "PWAV", "ADV", "ADV", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die nachmals m\u00fcde sind, und desto l\u00e4nger ruhn:", "tokens": ["Die", "nach\u00b7mals", "m\u00fc\u00b7de", "sind", ",", "und", "des\u00b7to", "l\u00e4n\u00b7ger", "ruhn", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "$,", "KON", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Nein, sondern Schritt vor Schritt die rechte Bahn gegangen,", "tokens": ["Nein", ",", "son\u00b7dern", "Schritt", "vor", "Schritt", "die", "rech\u00b7te", "Bahn", "ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KON", "NN", "APPR", "NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Um desto sicherer zum Zwecke zu gelangen.", "tokens": ["Um", "des\u00b7to", "si\u00b7che\u00b7rer", "zum", "Zwe\u00b7cke", "zu", "ge\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "ADJD", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Das wird gewi\u00df geschehn; man sieht es schon voraus.", "tokens": ["Das", "wird", "ge\u00b7wi\u00df", "ge\u00b7schehn", ";", "man", "sieht", "es", "schon", "vo\u00b7raus", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "VVPP", "$.", "PIS", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Erfreue dich dabey, du hochgesch\u00e4tztes Haus!", "tokens": ["Er\u00b7freu\u00b7e", "dich", "da\u00b7bey", ",", "du", "hoch\u00b7ge\u00b7sch\u00e4tz\u00b7tes", "Haus", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PAV", "$,", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In kurzem wird die Welt aus tausend Fr\u00fcchten lesen,", "tokens": ["In", "kur\u00b7zem", "wird", "die", "Welt", "aus", "tau\u00b7send", "Fr\u00fcch\u00b7ten", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "VAFIN", "ART", "NN", "APPR", "CARD", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wie edel dieses Reis, wie sch\u00f6n sein Stamm gewesen.", "tokens": ["Wie", "e\u00b7del", "die\u00b7ses", "Reis", ",", "wie", "sch\u00f6n", "sein", "Stamm", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PDAT", "NN", "$,", "PWAV", "ADJD", "PPOSAT", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Wie sehr ich dich bisher, mein ", "tokens": ["Wie", "sehr", "ich", "dich", "bis\u00b7her", ",", "mein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["PWAV", "ADV", "PPER", "PRF", "ADV", "$,", "PPOSAT"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie sehr mich dein Verstand und redlich Herz ergetzt,", "tokens": ["Wie", "sehr", "mich", "dein", "Ver\u00b7stand", "und", "red\u00b7lich", "Herz", "er\u00b7getzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "PPOSAT", "NN", "KON", "ADJD", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und kurz, dein tugendhaft, gelehrt und kluges Wesen,", "tokens": ["Und", "kurz", ",", "dein", "tu\u00b7gend\u00b7haft", ",", "ge\u00b7lehrt", "und", "klu\u00b7ges", "We\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPOSAT", "NN", "$,", "VVPP", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "La\u00df ich zum erstenmal in diesen Zeilen lesen.", "tokens": ["La\u00df", "ich", "zum", "ers\u00b7ten\u00b7mal", "in", "die\u00b7sen", "Zei\u00b7len", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADV", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich h\u00e4tt es l\u00e4ngst gethan, aus Antrieb meiner Pflicht,", "tokens": ["Ich", "h\u00e4tt", "es", "l\u00e4ngst", "ge\u00b7than", ",", "aus", "An\u00b7trieb", "mei\u00b7ner", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,", "APPR", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Es fehlte mir dazu an Lust und Vorsatz nicht:", "tokens": ["Es", "fehl\u00b7te", "mir", "da\u00b7zu", "an", "Lust", "und", "Vor\u00b7satz", "nicht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PAV", "APPR", "NN", "KON", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Allein, ich w\u00fcnschte stets, mit sehnlichem Verlangen,", "tokens": ["Al\u00b7lein", ",", "ich", "w\u00fcnschte", "stets", ",", "mit", "sehn\u00b7li\u00b7chem", "Ver\u00b7lan\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "ADV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.8": {"text": "Mehr Anla\u00df, als bisher, zur Lobschrift zu empfangen.", "tokens": ["Mehr", "An\u00b7la\u00df", ",", "als", "bis\u00b7her", ",", "zur", "Lob\u00b7schrift", "zu", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOUS", "ADV", "$,", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.9": {"text": "Ich dringe mich nicht gern durch Schm\u00e4ucheleyen ein,", "tokens": ["Ich", "drin\u00b7ge", "mich", "nicht", "gern", "durch", "Schm\u00e4u\u00b7che\u00b7le\u00b7yen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Mein Griffel wollte nur der Wahrheit Herold seyn;", "tokens": ["Mein", "Grif\u00b7fel", "woll\u00b7te", "nur", "der", "Wahr\u00b7heit", "He\u00b7rold", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ADV", "ART", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und wartete mit Flei\u00df auf \u00f6ffentliche Proben,", "tokens": ["Und", "war\u00b7te\u00b7te", "mit", "Flei\u00df", "auf", "\u00f6f\u00b7fent\u00b7li\u00b7che", "Pro\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Um deine Trefflichkeit auch \u00f6ffentlich zu loben.", "tokens": ["Um", "dei\u00b7ne", "Treff\u00b7lich\u00b7keit", "auch", "\u00f6f\u00b7fent\u00b7lich", "zu", "lo\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Daran gebrach es mir. Denn ob dir wohl kein Tag,", "tokens": ["Da\u00b7ran", "ge\u00b7brach", "es", "mir", ".", "Denn", "ob", "dir", "wohl", "kein", "Tag", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "$.", "KON", "KOUS", "PPER", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kein Augenblick vergeht, der nicht bezeugen mag,", "tokens": ["Kein", "Au\u00b7gen\u00b7blick", "ver\u00b7geht", ",", "der", "nicht", "be\u00b7zeu\u00b7gen", "mag", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$,", "PRELS", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wie n\u00fctzlich dein Bem\u00fchn der Stadt, dem Vaterlande,", "tokens": ["Wie", "n\u00fctz\u00b7lich", "dein", "Be\u00b7m\u00fchn", "der", "Stadt", ",", "dem", "Va\u00b7ter\u00b7lan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPOSAT", "NN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und Sachsens Nachbarn ist; indem man mit Bestande", "tokens": ["Und", "Sach\u00b7sens", "Nach\u00b7barn", "ist", ";", "in\u00b7dem", "man", "mit", "Be\u00b7stan\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "NN", "VAFIN", "$.", "KOUS", "PIS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gar wohl behaupten kann, da\u00df deinen Witz und Flei\u00df", "tokens": ["Gar", "wohl", "be\u00b7haup\u00b7ten", "kann", ",", "da\u00df", "dei\u00b7nen", "Witz", "und", "Flei\u00df"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVINF", "VMFIN", "$,", "KOUS", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Halb Deutschland schon genie\u00dft, und sehr zu r\u00fchmen weis:", "tokens": ["Halb", "Deutschland", "schon", "ge\u00b7nie\u00dft", ",", "und", "sehr", "zu", "r\u00fch\u00b7men", "weis", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "VVFIN", "$,", "KON", "ADV", "PTKZU", "VVINF", "PTKVZ", "$."], "meter": "++--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.7": {"text": "So war doch dieses mir, wenn ich dich loben wollte,", "tokens": ["So", "war", "doch", "die\u00b7ses", "mir", ",", "wenn", "ich", "dich", "lo\u00b7ben", "woll\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PDAT", "PPER", "$,", "KOUS", "PPER", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Kein sattsamer Beweis, der andern zeigen sollte,", "tokens": ["Kein", "satt\u00b7sa\u00b7mer", "Be\u00b7weis", ",", "der", "an\u00b7dern", "zei\u00b7gen", "soll\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "PRELS", "PIS", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Warum ich dich ger\u00fchmt. Die Welt verlangt was mehr,", "tokens": ["Wa\u00b7rum", "ich", "dich", "ge\u00b7r\u00fchmt", ".", "Die", "Welt", "ver\u00b7langt", "was", "mehr", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "VVPP", "$.", "ART", "NN", "VVFIN", "PWS", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und giebt uns eher nicht ein g\u00fctiges Geh\u00f6r,", "tokens": ["Und", "giebt", "uns", "e\u00b7her", "nicht", "ein", "g\u00fc\u00b7ti\u00b7ges", "Ge\u00b7h\u00f6r", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Als bis man \u00f6ffentlich was sonderbares findet,", "tokens": ["Als", "bis", "man", "\u00f6f\u00b7fent\u00b7lich", "was", "son\u00b7der\u00b7ba\u00b7res", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PIS", "ADJD", "PWS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Worauf man denn das Lob ber\u00fchmter H\u00e4upter gr\u00fcndet.", "tokens": ["Wo\u00b7rauf", "man", "denn", "das", "Lob", "be\u00b7r\u00fchm\u00b7ter", "H\u00e4up\u00b7ter", "gr\u00fcn\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "ADV", "ART", "NN", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Daher geschieht es denn, da\u00df oft ein Namensfest,", "tokens": ["Da\u00b7her", "ge\u00b7schieht", "es", "denn", ",", "da\u00df", "oft", "ein", "Na\u00b7mens\u00b7fest", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Oft noch was wenigers, den Anla\u00df finden l\u00e4\u00dft,", "tokens": ["Oft", "noch", "was", "we\u00b7ni\u00b7gers", ",", "den", "An\u00b7la\u00df", "fin\u00b7den", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PWS", "PIS", "$,", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Zu preisen, wen man ehrt. Allein das hei\u00dft gezwungen:", "tokens": ["Zu", "prei\u00b7sen", ",", "wen", "man", "ehrt", ".", "Al\u00b7lein", "das", "hei\u00dft", "ge\u00b7zwun\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PWS", "PIS", "VVFIN", "$.", "ADV", "PDS", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Drum hab ich meinen Reim dir so nicht aufgedrungen.", "tokens": ["Drum", "hab", "ich", "mei\u00b7nen", "Reim", "dir", "so", "nicht", "auf\u00b7ge\u00b7drun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPOSAT", "NN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Das n\u00e4chstverwichne Jahr versprach mir schon sehr viel;", "tokens": ["Das", "n\u00e4chst\u00b7ver\u00b7wich\u00b7ne", "Jahr", "ver\u00b7sprach", "mir", "schon", "sehr", "viel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Schicksal zeigte mir das l\u00e4ngstgesuchte Ziel,", "tokens": ["Das", "Schick\u00b7sal", "zeig\u00b7te", "mir", "das", "l\u00e4ngst\u00b7ge\u00b7such\u00b7te", "Ziel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Indem ich schon vernahm, da\u00df endlich Wunsch und Hoffen,", "tokens": ["In\u00b7dem", "ich", "schon", "ver\u00b7nahm", ",", "da\u00df", "end\u00b7lich", "Wunsch", "und", "Hof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$,", "KOUS", "ADV", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Durch dein vergr\u00f6\u00dfert Gl\u00fcck vollkommen eingetroffen.", "tokens": ["Durch", "dein", "ver\u00b7gr\u00f6\u00b7\u00dfert", "Gl\u00fcck", "voll\u00b7kom\u00b7men", "ein\u00b7ge\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Man sprach, der Sachsen Haupt, (welch ein gerechter Held!)", "tokens": ["Man", "sprach", ",", "der", "Sach\u00b7sen", "Haupt", ",", "(", "welch", "ein", "ge\u00b7rech\u00b7ter", "Held", "!", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "$,", "ART", "NN", "NN", "$,", "$(", "PWAT", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Hof und Land regiert. Es war auch kein Gedichte,", "tokens": ["Der", "Hof", "und", "Land", "re\u00b7giert", ".", "Es", "war", "auch", "kein", "Ge\u00b7dich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$.", "PPER", "VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Wahrheit sch\u00fctzte selbst das fl\u00fcchtige Ger\u00fcchte.", "tokens": ["Die", "Wahr\u00b7heit", "sch\u00fctz\u00b7te", "selbst", "das", "fl\u00fcch\u00b7ti\u00b7ge", "Ge\u00b7r\u00fcch\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Du hattest in der That die Ehre l\u00e4ngst verdient,", "tokens": ["Du", "hat\u00b7test", "in", "der", "That", "die", "Eh\u00b7re", "l\u00e4ngst", "ver\u00b7dient", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ob deine Demuth gleich sich niemals recht erk\u00fchnt,", "tokens": ["Ob", "dei\u00b7ne", "De\u00b7muth", "gleich", "sich", "nie\u00b7mals", "recht", "er\u00b7k\u00fchnt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PRF", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "So hoch empor zu sehn; und gar das Gl\u00fcck zu zwingen,", "tokens": ["So", "hoch", "em\u00b7por", "zu", "sehn", ";", "und", "gar", "das", "Gl\u00fcck", "zu", "zwin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$.", "KON", "ADV", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Durch Gunst und Geld, den Lohn f\u00fcr dein Verdienst zu bringen.", "tokens": ["Durch", "Gunst", "und", "Geld", ",", "den", "Lohn", "f\u00fcr", "dein", "Ver\u00b7dienst", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,", "ART", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Wie r\u00fchmlich war dirs nun, was dazumal geschah,", "tokens": ["Wie", "r\u00fchm\u00b7lich", "war", "dirs", "nun", ",", "was", "da\u00b7zu\u00b7mal", "ge\u00b7schah", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PIS", "ADV", "$,", "PRELS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Da\u00df Hof und K\u00f6nig selbst auf jeden Vorzug sah,", "tokens": ["Da\u00df", "Hof", "und", "K\u00f6\u00b7nig", "selbst", "auf", "je\u00b7den", "Vor\u00b7zug", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ADV", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Der dich vor andern schm\u00fcckt; und, ohne dich zu fragen,", "tokens": ["Der", "dich", "vor", "an\u00b7dern", "schm\u00fcckt", ";", "und", ",", "oh\u00b7ne", "dich", "zu", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "PIS", "VVFIN", "$.", "KON", "$,", "KOUI", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die neue W\u00fcrde dir aus Gnaden angetragen.", "tokens": ["Die", "neu\u00b7e", "W\u00fcr\u00b7de", "dir", "aus", "Gna\u00b7den", "an\u00b7ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Allein hier zeigte sich auch die Bescheidenheit,", "tokens": ["Al\u00b7lein", "hier", "zeig\u00b7te", "sich", "auch", "die", "Be\u00b7schei\u00b7den\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die deine Tugend ziert. Das Gift der Eitelkeit", "tokens": ["Die", "dei\u00b7ne", "Tu\u00b7gend", "ziert", ".", "Das", "Gift", "der", "Ei\u00b7tel\u00b7keit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "NN", "VVFIN", "$.", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hat dich wohl nie befleckt. Du pflegst durch bunte Schaalen,", "tokens": ["Hat", "dich", "wohl", "nie", "be\u00b7fleckt", ".", "Du", "pflegst", "durch", "bun\u00b7te", "Schaa\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVPP", "$.", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Darinn der Kern gebricht, der Welt nichts vorzupralen.", "tokens": ["Da\u00b7rinn", "der", "Kern", "ge\u00b7bricht", ",", "der", "Welt", "nichts", "vor\u00b7zu\u00b7pra\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "VVFIN", "$,", "ART", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und hast du diesen nur, so achtest du es nicht,", "tokens": ["Und", "hast", "du", "die\u00b7sen", "nur", ",", "so", "ach\u00b7test", "du", "es", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PDAT", "ADV", "$,", "ADV", "VVFIN", "PPER", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn gleich von au\u00dfen dir ein gro\u00dfer Schein gebricht.", "tokens": ["Wenn", "gleich", "von", "au\u00b7\u00dfen", "dir", "ein", "gro\u00b7\u00dfer", "Schein", "ge\u00b7bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "ADV", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So gieng es denn auch hier. Du warest schon zufrieden,", "tokens": ["So", "gieng", "es", "denn", "auch", "hier", ".", "Du", "wa\u00b7rest", "schon", "zu\u00b7frie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADV", "$.", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Mit dem, was dir das Gl\u00fcck und dein Verdienst beschieden;", "tokens": ["Mit", "dem", ",", "was", "dir", "das", "Gl\u00fcck", "und", "dein", "Ver\u00b7dienst", "be\u00b7schie\u00b7den", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PWS", "PPER", "ART", "NN", "KON", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und schienest, da sich schon des Alters Vorspiel zeigt,", "tokens": ["Und", "schie\u00b7nest", ",", "da", "sich", "schon", "des", "Al\u00b7ters", "Vor\u00b7spiel", "zeigt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PRF", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Zu keiner Aenderung der Lebensart geneigt:", "tokens": ["Zu", "kei\u00b7ner", "A\u00b7en\u00b7de\u00b7rung", "der", "Le\u00b7ben\u00b7sart", "ge\u00b7neigt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Zumal es besser bleibt, verdienen, und nicht haben;", "tokens": ["Zu\u00b7mal", "es", "bes\u00b7ser", "bleibt", ",", "ver\u00b7die\u00b7nen", ",", "und", "nicht", "ha\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVFIN", "$,", "VVFIN", "$,", "KON", "PTKNEG", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Als leer an W\u00fcrden seyn, und sich an Titeln laben.", "tokens": ["Als", "leer", "an", "W\u00fcr\u00b7den", "seyn", ",", "und", "sich", "an", "Ti\u00b7teln", "la\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "NN", "VAINF", "$,", "KON", "PRF", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "So ward indessen mir der Anla\u00df auch geraubt,", "tokens": ["So", "ward", "in\u00b7des\u00b7sen", "mir", "der", "An\u00b7la\u00df", "auch", "ge\u00b7raubt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PPER", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Den ich, zu deinem Ruhm, zu haben schon geglaubt.", "tokens": ["Den", "ich", ",", "zu", "dei\u00b7nem", "Ruhm", ",", "zu", "ha\u00b7ben", "schon", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "$,", "APPR", "PPOSAT", "NN", "$,", "PTKZU", "VAINF", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich schwieg also bisher, und lernte mit Ergetzen,", "tokens": ["Ich", "schwieg", "al\u00b7so", "bis\u00b7her", ",", "und", "lern\u00b7te", "mit", "Er\u00b7get\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "KON", "VVFIN", "APPR", "NN", "$,"], "meter": "-+--+--+--+--", "measure": "amphibrach.tetra.plus"}, "line.4": {"text": "Dein Wesen, theurer Mann! fast t\u00e4glich h\u00f6her sch\u00e4tzen.", "tokens": ["Dein", "We\u00b7sen", ",", "theu\u00b7rer", "Mann", "!", "fast", "t\u00e4g\u00b7lich", "h\u00f6\u00b7her", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "ADJD", "NN", "$.", "ADV", "ADJD", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dein Bruder, der wie du, des Landes Wohlfahrt st\u00fctzt,", "tokens": ["Dein", "Bru\u00b7der", ",", "der", "wie", "du", ",", "des", "Lan\u00b7des", "Wohl\u00b7fahrt", "st\u00fctzt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "KOKOM", "PPER", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Gerechtigkeit und Recht durch manchen Ausspruch sch\u00fctzt,", "tokens": ["Ge\u00b7rech\u00b7tig\u00b7keit", "und", "Recht", "durch", "man\u00b7chen", "Aus\u00b7spruch", "sch\u00fctzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "War ebenfalls, wie du, ein Gegenstand der Augen,", "tokens": ["War", "e\u00b7ben\u00b7falls", ",", "wie", "du", ",", "ein", "Ge\u00b7gen\u00b7stand", "der", "Au\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$,", "PWAV", "PPER", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und konnte mir, nebst dir, zum Musterbilde taugen;", "tokens": ["Und", "konn\u00b7te", "mir", ",", "nebst", "dir", ",", "zum", "Mus\u00b7ter\u00b7bil\u00b7de", "tau\u00b7gen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "$,", "APPR", "PPER", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Zum Muster, da\u00df sich noch ein tugendhafter Mann,", "tokens": ["Zum", "Mus\u00b7ter", ",", "da\u00df", "sich", "noch", "ein", "tu\u00b7gend\u00b7haf\u00b7ter", "Mann", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "KOUS", "PRF", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Durch F\u00e4higkeit und Flei\u00df zu Ehren bringen kann.", "tokens": ["Durch", "F\u00e4\u00b7hig\u00b7keit", "und", "Flei\u00df", "zu", "Eh\u00b7ren", "brin\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Doch mu\u00dft ich alles das noch allezeit verschweigen,", "tokens": ["Doch", "mu\u00dft", "ich", "al\u00b7les", "das", "noch", "al\u00b7le\u00b7zeit", "ver\u00b7schwei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PIS", "ART", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und dorft es eher nicht, als heute, deutlich zeigen.", "tokens": ["Und", "dorft", "es", "e\u00b7her", "nicht", ",", "als", "heu\u00b7te", ",", "deut\u00b7lich", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "$,", "KOUS", "ADV", "$,", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Denn itzt erscheint der Tag, da dein geschickter Sohn", "tokens": ["Denn", "itzt", "er\u00b7scheint", "der", "Tag", ",", "da", "dein", "ge\u00b7schick\u00b7ter", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$,", "KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der sch\u00f6ne Lehrerschmuck im Philosophenorden,", "tokens": ["Der", "sch\u00f6\u00b7ne", "Leh\u00b7rer\u00b7schmuck", "im", "Phi\u00b7lo\u00b7so\u00b7phen\u00b7or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mit allgemeinem Ruhm auch ihm zu Theil geworden.", "tokens": ["Mit", "all\u00b7ge\u00b7mei\u00b7nem", "Ruhm", "auch", "ihm", "zu", "Theil", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "PPER", "APPR", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ganz Leipzig sieht dabey die wohlgerathne Frucht,", "tokens": ["Ganz", "Leip\u00b7zig", "sieht", "da\u00b7bey", "die", "wohl\u00b7ge\u00b7rath\u00b7ne", "Frucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PAV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das sch\u00f6nste Probest\u00fcck von Wagners kluger Zucht;", "tokens": ["Das", "sch\u00f6ns\u00b7te", "Pro\u00b7be\u00b7st\u00fcck", "von", "Wag\u00b7ners", "klu\u00b7ger", "Zucht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Des Vaters Ebenbild in seines Sohnes Jugend,", "tokens": ["Des", "Va\u00b7ters", "E\u00b7ben\u00b7bild", "in", "sei\u00b7nes", "Soh\u00b7nes", "Ju\u00b7gend", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und diesen auf der Spur der v\u00e4terlichen Tugend.", "tokens": ["Und", "die\u00b7sen", "auf", "der", "Spur", "der", "v\u00e4\u00b7ter\u00b7li\u00b7chen", "Tu\u00b7gend", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "APPR", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Man siehts und lobet dich, und r\u00fchmt den Sohn zugleich,", "tokens": ["Man", "siehts", "und", "lo\u00b7bet", "dich", ",", "und", "r\u00fchmt", "den", "Sohn", "zu\u00b7gleich", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "PPER", "$,", "KON", "VVFIN", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und wer nicht neidisch ist, wird an Vergn\u00fcgen reich;", "tokens": ["Und", "wer", "nicht", "nei\u00b7disch", "ist", ",", "wird", "an", "Ver\u00b7gn\u00fc\u00b7gen", "reich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PTKNEG", "ADJD", "VAFIN", "$,", "VAFIN", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und w\u00fcnscht der Vaterstadt viel V\u00e4ter deinesgleichen,", "tokens": ["Und", "w\u00fcnscht", "der", "Va\u00b7ter\u00b7stadt", "viel", "V\u00e4\u00b7ter", "dei\u00b7nes\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Viel S\u00f6hne, wie dein Sohn, dem tausend S\u00f6hne weichen.", "tokens": ["Viel", "S\u00f6h\u00b7ne", ",", "wie", "dein", "Sohn", ",", "dem", "tau\u00b7send", "S\u00f6h\u00b7ne", "wei\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PWAV", "PPOSAT", "NN", "$,", "PRELS", "CARD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Ich merke, da\u00df man hier vieleicht den Einwurf macht:", "tokens": ["Ich", "mer\u00b7ke", ",", "da\u00df", "man", "hier", "vie\u00b7leicht", "den", "Ein\u00b7wurf", "macht", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PIS", "ADV", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es sey kein Lob f\u00fcr dich, was ich hier ausgedacht.", "tokens": ["Es", "sey", "kein", "Lob", "f\u00fcr", "dich", ",", "was", "ich", "hier", "aus\u00b7ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "APPR", "PPER", "$,", "PWS", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das hie\u00dfe gar nicht viel, den Sohn dahin zu f\u00fchren,", "tokens": ["Das", "hie\u00b7\u00dfe", "gar", "nicht", "viel", ",", "den", "Sohn", "da\u00b7hin", "zu", "f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PTKNEG", "ADV", "$,", "ART", "NN", "PAV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wo soviel andre stehn, und sich mit Lorbern zieren;", "tokens": ["Wo", "so\u00b7viel", "and\u00b7re", "stehn", ",", "und", "sich", "mit", "Lor\u00b7bern", "zie\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "PIS", "VVINF", "$,", "KON", "PRF", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und wo die W\u00fcrdigkeit nicht stets allein gemacht,", "tokens": ["Und", "wo", "die", "W\u00fcr\u00b7dig\u00b7keit", "nicht", "stets", "al\u00b7lein", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "PTKNEG", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df ihnen solcher Schmuck vor andern wird gebracht.", "tokens": ["Da\u00df", "ih\u00b7nen", "sol\u00b7cher", "Schmuck", "vor", "an\u00b7dern", "wird", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "APPR", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Das sey kein gro\u00dfer Ruhm, den Sohn nach zwanzig Jahren", "tokens": ["Das", "sey", "kein", "gro\u00b7\u00dfer", "Ruhm", ",", "den", "Sohn", "nach", "zwan\u00b7zig", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PIAT", "ADJA", "NN", "$,", "ART", "NN", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Magisterm\u00e4\u00dfig sehn; nachdem man oft erfahren,", "tokens": ["Ma\u00b7gis\u00b7ter\u00b7m\u00e4\u00b7\u00dfig", "sehn", ";", "nach\u00b7dem", "man", "oft", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "$.", "KOUS", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Da\u00df hier und sonsten oft ein funfzehnj\u00e4hrig Kind,", "tokens": ["Da\u00df", "hier", "und", "sons\u00b7ten", "oft", "ein", "funf\u00b7zehn\u00b7j\u00e4h\u00b7rig", "Kind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "KON", "VVFIN", "ADV", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Bey dem Verstand und Witz noch gar nicht zeitig sind,", "tokens": ["Bey", "dem", "Ver\u00b7stand", "und", "Witz", "noch", "gar", "nicht", "zei\u00b7tig", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "NN", "ADV", "ADV", "PTKNEG", "ADJD", "VAFIN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.11": {"text": "Der weisen ", "tokens": ["Der", "wei\u00b7sen"], "token_info": ["word", "word"], "pos": ["ART", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.12": {"text": "Und also h\u00e4tt ich dir ein schlechtes Lob ersonnen.", "tokens": ["Und", "al\u00b7so", "h\u00e4tt", "ich", "dir", "ein", "schlech\u00b7tes", "Lob", "er\u00b7son\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "PPER", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "Jedoch, was ich gethan, ist mit Bedacht geschehn.", "tokens": ["Je\u00b7doch", ",", "was", "ich", "ge\u00b7than", ",", "ist", "mit", "Be\u00b7dacht", "ge\u00b7schehn", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWS", "PPER", "VVPP", "$,", "VAFIN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich habe deines Sohns Geschicklichkeit gesehn,", "tokens": ["Ich", "ha\u00b7be", "dei\u00b7nes", "Sohns", "Ge\u00b7schick\u00b7lich\u00b7keit", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und seinen Flei\u00df erkannt, und dich und ihn gepriesen,", "tokens": ["Und", "sei\u00b7nen", "Flei\u00df", "er\u00b7kannt", ",", "und", "dich", "und", "ihn", "ge\u00b7prie\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVPP", "$,", "KON", "PPER", "KON", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn beyder edles Thun sich so gesetzt erwiesen;", "tokens": ["Wenn", "bey\u00b7der", "ed\u00b7les", "Thun", "sich", "so", "ge\u00b7setzt", "er\u00b7wie\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ADJA", "NN", "PRF", "ADV", "VVPP", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht gar zu sehr geeilt, wie so viel andre thun,", "tokens": ["Nicht", "gar", "zu", "sehr", "ge\u00b7eilt", ",", "wie", "so", "viel", "and\u00b7re", "thun", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "PTKA", "ADV", "VVPP", "$,", "PWAV", "ADV", "ADV", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die nachmals m\u00fcde sind, und desto l\u00e4nger ruhn:", "tokens": ["Die", "nach\u00b7mals", "m\u00fc\u00b7de", "sind", ",", "und", "des\u00b7to", "l\u00e4n\u00b7ger", "ruhn", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "$,", "KON", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Nein, sondern Schritt vor Schritt die rechte Bahn gegangen,", "tokens": ["Nein", ",", "son\u00b7dern", "Schritt", "vor", "Schritt", "die", "rech\u00b7te", "Bahn", "ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KON", "NN", "APPR", "NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Um desto sicherer zum Zwecke zu gelangen.", "tokens": ["Um", "des\u00b7to", "si\u00b7che\u00b7rer", "zum", "Zwe\u00b7cke", "zu", "ge\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "ADJD", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Das wird gewi\u00df geschehn; man sieht es schon voraus.", "tokens": ["Das", "wird", "ge\u00b7wi\u00df", "ge\u00b7schehn", ";", "man", "sieht", "es", "schon", "vo\u00b7raus", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "VVPP", "$.", "PIS", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Erfreue dich dabey, du hochgesch\u00e4tztes Haus!", "tokens": ["Er\u00b7freu\u00b7e", "dich", "da\u00b7bey", ",", "du", "hoch\u00b7ge\u00b7sch\u00e4tz\u00b7tes", "Haus", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PAV", "$,", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In kurzem wird die Welt aus tausend Fr\u00fcchten lesen,", "tokens": ["In", "kur\u00b7zem", "wird", "die", "Welt", "aus", "tau\u00b7send", "Fr\u00fcch\u00b7ten", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "VAFIN", "ART", "NN", "APPR", "CARD", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wie edel dieses Reis, wie sch\u00f6n sein Stamm gewesen.", "tokens": ["Wie", "e\u00b7del", "die\u00b7ses", "Reis", ",", "wie", "sch\u00f6n", "sein", "Stamm", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PDAT", "NN", "$,", "PWAV", "ADJD", "PPOSAT", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}