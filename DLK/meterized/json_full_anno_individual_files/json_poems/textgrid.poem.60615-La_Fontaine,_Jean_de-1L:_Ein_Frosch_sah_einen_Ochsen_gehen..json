{"textgrid.poem.60615": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Frosch sah einen Ochsen gehen.", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Frosch sah einen Ochsen gehen.", "tokens": ["Ein", "Frosch", "sah", "ei\u00b7nen", "Och\u00b7sen", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie stattlich war der anzusehen!", "tokens": ["Wie", "statt\u00b7lich", "war", "der", "an\u00b7zu\u00b7se\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ART", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er, der nicht gr\u00f6\u00dfer als ein Ei, war neidisch drauf,", "tokens": ["Er", ",", "der", "nicht", "gr\u00f6\u00b7\u00dfer", "als", "ein", "Ei", ",", "war", "nei\u00b7disch", "drauf", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$,", "VAFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er spreizt sich, bl\u00e4ht mit Macht sich auf,", "tokens": ["Er", "spreizt", "sich", ",", "bl\u00e4ht", "mit", "Macht", "sich", "auf", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "$,", "VVFIN", "APPR", "NN", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Um gleich zu sein dem gro\u00dfen Tier,", "tokens": ["Um", "gleich", "zu", "sein", "dem", "gro\u00b7\u00dfen", "Tier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "APPR", "PPOSAT", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und rief: \u00bbIhr Br\u00fcder achtet und vergleicht!", "tokens": ["Und", "rief", ":", "\u00bb", "Ihr", "Br\u00fc\u00b7der", "ach\u00b7tet", "und", "ver\u00b7gleicht", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PPOSAT", "NN", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wie, bin ich nun so weit? Ach, sagt es mir!\u00ab \u2013", "tokens": ["Wie", ",", "bin", "ich", "nun", "so", "weit", "?", "Ach", ",", "sagt", "es", "mir", "!", "\u00ab", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWAV", "$,", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$.", "ITJ", "$,", "VVFIN", "PPER", "PPER", "$.", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbnein!\u00ab \u2013 \u00bbAber jetzt?\u00ab \u2013 \u00bbWas denkst du dir!\u00ab \u2013", "tokens": ["\u00bb", "nein", "!", "\u00ab", "\u2013", "\u00bb", "A\u00b7ber", "jetzt", "?", "\u00ab", "\u2013", "\u00bb", "Was", "denkst", "du", "dir", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PTKANT", "$.", "$(", "$(", "$(", "KON", "ADV", "$.", "$(", "$(", "$(", "PWS", "VVFIN", "PPER", "PPER", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "\u00bbund jetzt?\u00ab \u2013 \u00bbNoch lange nicht erreicht!\u00ab \u2013", "tokens": ["\u00bb", "und", "jetzt", "?", "\u00ab", "\u2013", "\u00bb", "Noch", "lan\u00b7ge", "nicht", "er\u00b7reicht", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "KON", "ADV", "$.", "$(", "$(", "$(", "ADV", "ADV", "PTKNEG", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Das Fr\u00f6schlein hat sich furchtbar aufgeblasen,", "tokens": ["Das", "Fr\u00f6schlein", "hat", "sich", "furcht\u00b7bar", "auf\u00b7ge\u00b7bla\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PRF", "ADJD", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Es platzte und verschied im gr\u00fcnen Rasen.", "tokens": ["Es", "platz\u00b7te", "und", "ver\u00b7schied", "im", "gr\u00fc\u00b7nen", "Ra\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Die Welt bev\u00f6lkern viele solcher dummen Leute:", "tokens": ["Die", "Welt", "be\u00b7v\u00f6l\u00b7kern", "vie\u00b7le", "sol\u00b7cher", "dum\u00b7men", "Leu\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Jedweder B\u00fcrger m\u00f6chte baun wie gro\u00dfe Herrn,", "tokens": ["Jed\u00b7we\u00b7der", "B\u00fcr\u00b7ger", "m\u00f6ch\u00b7te", "baun", "wie", "gro\u00b7\u00dfe", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "VVINF", "KOKOM", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der kleine F\u00fcrst \u2013 er h\u00e4lt Gesandte heute,", "tokens": ["Der", "klei\u00b7ne", "F\u00fcrst", "\u2013", "er", "h\u00e4lt", "Ge\u00b7sand\u00b7te", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "PPER", "VVFIN", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das kleinste Gr\u00e4flein prunkt mit Pagen gern.", "tokens": ["Das", "kleins\u00b7te", "Gr\u00e4f\u00b7lein", "prunkt", "mit", "Pa\u00b7gen", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Ein Frosch sah einen Ochsen gehen.", "tokens": ["Ein", "Frosch", "sah", "ei\u00b7nen", "Och\u00b7sen", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie stattlich war der anzusehen!", "tokens": ["Wie", "statt\u00b7lich", "war", "der", "an\u00b7zu\u00b7se\u00b7hen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "ART", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er, der nicht gr\u00f6\u00dfer als ein Ei, war neidisch drauf,", "tokens": ["Er", ",", "der", "nicht", "gr\u00f6\u00b7\u00dfer", "als", "ein", "Ei", ",", "war", "nei\u00b7disch", "drauf", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$,", "VAFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er spreizt sich, bl\u00e4ht mit Macht sich auf,", "tokens": ["Er", "spreizt", "sich", ",", "bl\u00e4ht", "mit", "Macht", "sich", "auf", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "$,", "VVFIN", "APPR", "NN", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Um gleich zu sein dem gro\u00dfen Tier,", "tokens": ["Um", "gleich", "zu", "sein", "dem", "gro\u00b7\u00dfen", "Tier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "APPR", "PPOSAT", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und rief: \u00bbIhr Br\u00fcder achtet und vergleicht!", "tokens": ["Und", "rief", ":", "\u00bb", "Ihr", "Br\u00fc\u00b7der", "ach\u00b7tet", "und", "ver\u00b7gleicht", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PPOSAT", "NN", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wie, bin ich nun so weit? Ach, sagt es mir!\u00ab \u2013", "tokens": ["Wie", ",", "bin", "ich", "nun", "so", "weit", "?", "Ach", ",", "sagt", "es", "mir", "!", "\u00ab", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWAV", "$,", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$.", "ITJ", "$,", "VVFIN", "PPER", "PPER", "$.", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "\u00bbnein!\u00ab \u2013 \u00bbAber jetzt?\u00ab \u2013 \u00bbWas denkst du dir!\u00ab \u2013", "tokens": ["\u00bb", "nein", "!", "\u00ab", "\u2013", "\u00bb", "A\u00b7ber", "jetzt", "?", "\u00ab", "\u2013", "\u00bb", "Was", "denkst", "du", "dir", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PTKANT", "$.", "$(", "$(", "$(", "KON", "ADV", "$.", "$(", "$(", "$(", "PWS", "VVFIN", "PPER", "PPER", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "\u00bbund jetzt?\u00ab \u2013 \u00bbNoch lange nicht erreicht!\u00ab \u2013", "tokens": ["\u00bb", "und", "jetzt", "?", "\u00ab", "\u2013", "\u00bb", "Noch", "lan\u00b7ge", "nicht", "er\u00b7reicht", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "KON", "ADV", "$.", "$(", "$(", "$(", "ADV", "ADV", "PTKNEG", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Das Fr\u00f6schlein hat sich furchtbar aufgeblasen,", "tokens": ["Das", "Fr\u00f6schlein", "hat", "sich", "furcht\u00b7bar", "auf\u00b7ge\u00b7bla\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PRF", "ADJD", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Es platzte und verschied im gr\u00fcnen Rasen.", "tokens": ["Es", "platz\u00b7te", "und", "ver\u00b7schied", "im", "gr\u00fc\u00b7nen", "Ra\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Die Welt bev\u00f6lkern viele solcher dummen Leute:", "tokens": ["Die", "Welt", "be\u00b7v\u00f6l\u00b7kern", "vie\u00b7le", "sol\u00b7cher", "dum\u00b7men", "Leu\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Jedweder B\u00fcrger m\u00f6chte baun wie gro\u00dfe Herrn,", "tokens": ["Jed\u00b7we\u00b7der", "B\u00fcr\u00b7ger", "m\u00f6ch\u00b7te", "baun", "wie", "gro\u00b7\u00dfe", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "VVINF", "KOKOM", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der kleine F\u00fcrst \u2013 er h\u00e4lt Gesandte heute,", "tokens": ["Der", "klei\u00b7ne", "F\u00fcrst", "\u2013", "er", "h\u00e4lt", "Ge\u00b7sand\u00b7te", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "PPER", "VVFIN", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das kleinste Gr\u00e4flein prunkt mit Pagen gern.", "tokens": ["Das", "kleins\u00b7te", "Gr\u00e4f\u00b7lein", "prunkt", "mit", "Pa\u00b7gen", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}