{"textgrid.poem.42795": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Blindschl", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich hatte einmal eine Liebschaft mit", "tokens": ["Ich", "hat\u00b7te", "ein\u00b7mal", "ei\u00b7ne", "Lieb\u00b7schaft", "mit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Einer Blindschleiche angefangen;", "tokens": ["Ei\u00b7ner", "Blind\u00b7schlei\u00b7che", "an\u00b7ge\u00b7fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "+-++-+-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Wir sind ein St\u00fcck Leben zusammen gegangen", "tokens": ["Wir", "sind", "ein", "St\u00fcck", "Le\u00b7ben", "zu\u00b7sam\u00b7men", "ge\u00b7gan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "NN", "ADV", "VVPP"], "meter": "+--++--+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Im ungleichen Schritt und Tritt.", "tokens": ["Im", "un\u00b7glei\u00b7chen", "Schritt", "und", "Tritt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "KON", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.2": {"line.1": {"text": "Die Sache war ziemlich sentimental.", "tokens": ["Die", "Sa\u00b7che", "war", "ziem\u00b7lich", "sen\u00b7ti\u00b7men\u00b7tal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In einem feudalen Th\u00fcringer Tal", "tokens": ["In", "ei\u00b7nem", "feu\u00b7da\u00b7len", "Th\u00fc\u00b7rin\u00b7ger", "Tal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Fand ich \u2013 nein glaubte zu finden \u2013 einmal", "tokens": ["Fand", "ich", "\u2013", "nein", "glaub\u00b7te", "zu", "fin\u00b7den", "\u2013", "ein\u00b7mal"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "$(", "PTKANT", "VVFIN", "PTKZU", "VVINF", "$(", "ADV"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Den ledernen Handgriff einer", "tokens": ["Den", "le\u00b7der\u00b7nen", "Hand\u00b7griff", "ei\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Damenhandtasche. Es war aber keiner.", "tokens": ["Da\u00b7men\u00b7hand\u00b7ta\u00b7sche", ".", "Es", "war", "a\u00b7ber", "kei\u00b7ner", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VAFIN", "ADV", "PIS", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.3": {"line.1": {"text": "Ich nannte sie \u00bbBlindschi\u00ab. Sie nannte mich", "tokens": ["Ich", "nann\u00b7te", "sie", "\u00bb", "Blind\u00b7schi", "\u00ab", ".", "Sie", "nann\u00b7te", "mich"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$(", "NN", "$(", "$.", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Nach wenigen Tagen schon \u00bbEicherich\u00ab", "tokens": ["Nach", "we\u00b7ni\u00b7gen", "Ta\u00b7gen", "schon", "\u00bb", "Ei\u00b7che\u00b7rich", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADV", "$(", "NN", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und dann, denn sie war sehr gelehrig,", "tokens": ["Und", "dann", ",", "denn", "sie", "war", "sehr", "ge\u00b7leh\u00b7rig", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KON", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Verst\u00e4ndlicher abgek\u00fcrzt \u00bbErich\u00ab.", "tokens": ["Ver\u00b7st\u00e4nd\u00b7li\u00b7cher", "ab\u00b7ge\u00b7k\u00fcrzt", "\u00bb", "E\u00b7rich", "\u00ab", "."], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["NN", "VVPP", "$(", "NE", "$(", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Allmittags haben gemeinsam wir", "tokens": ["All\u00b7mit\u00b7tags", "ha\u00b7ben", "ge\u00b7mein\u00b7sam", "wir"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADJD", "PPER"], "meter": "+-++--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Am gleichen Tische gegessen,", "tokens": ["Am", "glei\u00b7chen", "Ti\u00b7sche", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie Regenw\u00fcrmer mit zwei Tropfen Bier,", "tokens": ["Sie", "Re\u00b7gen\u00b7w\u00fcr\u00b7mer", "mit", "zwei", "Trop\u00b7fen", "Bier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPR", "CARD", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ich totere Delikatessen.", "tokens": ["Ich", "to\u00b7te\u00b7re", "De\u00b7li\u00b7ka\u00b7tes\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Sie opferte mir ihren zierlichen Schwanz.", "tokens": ["Sie", "op\u00b7fer\u00b7te", "mir", "ih\u00b7ren", "zier\u00b7li\u00b7chen", "Schwanz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich lehrte sie \u00fcberwinden", "tokens": ["Ich", "lehr\u00b7te", "sie", "\u00fc\u00b7berw\u00b7in\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und Knoten schlagen und Spitzentanz,", "tokens": ["Und", "Kno\u00b7ten", "schla\u00b7gen", "und", "Spit\u00b7zen\u00b7tanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Schluckdegen und Selbstbinder binden.", "tokens": ["Schluck\u00b7de\u00b7gen", "und", "Selbst\u00b7bin\u00b7der", "bin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVINF", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Sie war so appetitlich und nett.", "tokens": ["Sie", "war", "so", "ap\u00b7pe\u00b7tit\u00b7lich", "und", "nett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sie schlief Nacht \u00fcber in meinem Bett", "tokens": ["Sie", "schlief", "Nacht", "\u00fc\u00b7ber", "in", "mei\u00b7nem", "Bett"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "APPR", "APPR", "PPOSAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Als wie ein k\u00fchlender Schmuckreif am Hals,", "tokens": ["Als", "wie", "ein", "k\u00fch\u00b7len\u00b7der", "Schmuck\u00b7reif", "am", "Hals", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Metallisch und doch so sch\u00f6n weichlich.", "tokens": ["Me\u00b7tal\u00b7lisch", "und", "doch", "so", "sch\u00f6n", "weich\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADV", "ADV", "ADJD", "ADJD", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "Und wenn ihr wirklich was schlimmstenfalls", "tokens": ["Und", "wenn", "ihr", "wirk\u00b7lich", "was", "schlimms\u00b7ten\u00b7falls"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADJD", "PWS", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Passierte, so war es nie reichlich.", "tokens": ["Pas\u00b7sier\u00b7te", ",", "so", "war", "es", "nie", "reich\u00b7lich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Kein Sexuelles und keine Dressur.", "tokens": ["Kein", "Se\u00b7xu\u00b7el\u00b7les", "und", "kei\u00b7ne", "Dres\u00b7sur", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "PIAT", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich war ihr ein Freund und ein Lehrer,", "tokens": ["Ich", "war", "ihr", "ein", "Freund", "und", "ein", "Leh\u00b7rer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Was keiner von meinen Bekannten erfuhr;", "tokens": ["Was", "kei\u00b7ner", "von", "mei\u00b7nen", "Be\u00b7kann\u00b7ten", "er\u00b7fuhr", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wer mich besuchte, der sah sie nur", "tokens": ["Wer", "mich", "be\u00b7such\u00b7te", ",", "der", "sah", "sie", "nur"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "VVFIN", "$,", "PRELS", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Auf meinem Schreibtisch steif neben der Uhr", "tokens": ["Auf", "mei\u00b7nem", "Schreib\u00b7tisch", "steif", "ne\u00b7ben", "der", "Uhr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Als bronzenen Briefbeschwerer.", "tokens": ["Als", "bron\u00b7ze\u00b7nen", "Brief\u00b7be\u00b7schwe\u00b7rer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Und Jahre vergingen. Dann schlief ich einmal", "tokens": ["Und", "Jah\u00b7re", "ver\u00b7gin\u00b7gen", ".", "Dann", "schlief", "ich", "ein\u00b7mal"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVINF", "$.", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mit Blindschi und tr\u00e4umte im Betti", "tokens": ["Mit", "Blind\u00b7schi", "und", "tr\u00e4um\u00b7te", "im", "Bet\u00b7ti"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "VVFIN", "APPRART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "(jetzt werde ich wieder sentimental)", "tokens": ["(", "jetzt", "wer\u00b7de", "ich", "wie\u00b7der", "sen\u00b7ti\u00b7men\u00b7tal", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Gerade, ich \u00e4\u00dfe Spaghetti.", "tokens": ["Ge\u00b7ra\u00b7de", ",", "ich", "\u00e4\u00b7\u00dfe", "Spag\u00b7het\u00b7ti", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "NE", "$."], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}}, "stanza.9": {"line.1": {"text": "Da kam es, da\u00df irgendwas aus mir pfiff.", "tokens": ["Da", "kam", "es", ",", "da\u00df", "ir\u00b7gend\u00b7was", "aus", "mir", "pfiff", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PIS", "APPR", "PPER", "VVFIN", "$."], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Mag sein, da\u00df es f\u00fcrchterlich krachte.", "tokens": ["Mag", "sein", ",", "da\u00df", "es", "f\u00fcrch\u00b7ter\u00b7lich", "krach\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VAINF", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Fest steht, da\u00df Blindschi erwachte", "tokens": ["Fest", "steht", ",", "da\u00df", "Blind\u00b7schi", "er\u00b7wach\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "VVFIN", "$,", "KOUS", "NN", "VVFIN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und \u2013 sie, die sonst niemals nachts muckte \u2013", "tokens": ["Und", "\u2013", "sie", ",", "die", "sonst", "nie\u00b7mals", "nachts", "muck\u00b7te", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "PPER", "$,", "PRELS", "ADV", "ADV", "ADV", "VVFIN", "$("], "meter": "--+-+-+--", "measure": "anapaest.init"}, "line.5": {"text": "Wild z\u00fcngelte, da\u00df ich nach ihr griff", "tokens": ["Wild", "z\u00fcn\u00b7gel\u00b7te", ",", "da\u00df", "ich", "nach", "ihr", "griff"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "$,", "KOUS", "PPER", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "Und sie, noch tr\u00e4umend, verschluckte.", "tokens": ["Und", "sie", ",", "noch", "tr\u00e4u\u00b7mend", ",", "ver\u00b7schluck\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "$,", "ADV", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.10": {"line.1": {"text": "Es gleich zu sagen: Sie ging nicht tot.", "tokens": ["Es", "gleich", "zu", "sa\u00b7gen", ":", "Sie", "ging", "nicht", "tot", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PTKZU", "VVINF", "$.", "PPER", "VVFIN", "PTKNEG", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sie ist mir wieder entwichen,", "tokens": ["Sie", "ist", "mir", "wie\u00b7der", "ent\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ist in die W\u00e4lder geschlichen", "tokens": ["Ist", "in", "die", "W\u00e4l\u00b7der", "ge\u00b7schli\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVPP"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Und sucht dort einsam ihr t\u00e4gliches Brot.", "tokens": ["Und", "sucht", "dort", "ein\u00b7sam", "ihr", "t\u00e4g\u00b7li\u00b7ches", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Vorbei! Es w\u00e4re \u2013 ich bin doch nicht blind \u2013", "tokens": ["Vor\u00b7bei", "!", "Es", "w\u00e4\u00b7re", "\u2013", "ich", "bin", "doch", "nicht", "blind", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VAFIN", "$(", "PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Vergebens, ihr nachzuschleichen.", "tokens": ["Ver\u00b7ge\u00b7bens", ",", "ihr", "nach\u00b7zu\u00b7schlei\u00b7chen", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Weil ihre Wege zu dunkel sind.", "tokens": ["Weil", "ih\u00b7re", "We\u00b7ge", "zu", "dun\u00b7kel", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKA", "ADJD", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Weil wir einander nicht gleichen.", "tokens": ["Weil", "wir", "ein\u00b7an\u00b7der", "nicht", "glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.12": {"line.1": {"text": "Ich hatte einmal eine Liebschaft mit", "tokens": ["Ich", "hat\u00b7te", "ein\u00b7mal", "ei\u00b7ne", "Lieb\u00b7schaft", "mit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Einer Blindschleiche angefangen;", "tokens": ["Ei\u00b7ner", "Blind\u00b7schlei\u00b7che", "an\u00b7ge\u00b7fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "+-++-+-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Wir sind ein St\u00fcck Leben zusammen gegangen", "tokens": ["Wir", "sind", "ein", "St\u00fcck", "Le\u00b7ben", "zu\u00b7sam\u00b7men", "ge\u00b7gan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "NN", "ADV", "VVPP"], "meter": "+--++--+--+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Im ungleichen Schritt und Tritt.", "tokens": ["Im", "un\u00b7glei\u00b7chen", "Schritt", "und", "Tritt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "KON", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.13": {"line.1": {"text": "Die Sache war ziemlich sentimental.", "tokens": ["Die", "Sa\u00b7che", "war", "ziem\u00b7lich", "sen\u00b7ti\u00b7men\u00b7tal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In einem feudalen Th\u00fcringer Tal", "tokens": ["In", "ei\u00b7nem", "feu\u00b7da\u00b7len", "Th\u00fc\u00b7rin\u00b7ger", "Tal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Fand ich \u2013 nein glaubte zu finden \u2013 einmal", "tokens": ["Fand", "ich", "\u2013", "nein", "glaub\u00b7te", "zu", "fin\u00b7den", "\u2013", "ein\u00b7mal"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "$(", "PTKANT", "VVFIN", "PTKZU", "VVINF", "$(", "ADV"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Den ledernen Handgriff einer", "tokens": ["Den", "le\u00b7der\u00b7nen", "Hand\u00b7griff", "ei\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Damenhandtasche. Es war aber keiner.", "tokens": ["Da\u00b7men\u00b7hand\u00b7ta\u00b7sche", ".", "Es", "war", "a\u00b7ber", "kei\u00b7ner", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VAFIN", "ADV", "PIS", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.14": {"line.1": {"text": "Ich nannte sie \u00bbBlindschi\u00ab. Sie nannte mich", "tokens": ["Ich", "nann\u00b7te", "sie", "\u00bb", "Blind\u00b7schi", "\u00ab", ".", "Sie", "nann\u00b7te", "mich"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$(", "NN", "$(", "$.", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Nach wenigen Tagen schon \u00bbEicherich\u00ab", "tokens": ["Nach", "we\u00b7ni\u00b7gen", "Ta\u00b7gen", "schon", "\u00bb", "Ei\u00b7che\u00b7rich", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADV", "$(", "NN", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und dann, denn sie war sehr gelehrig,", "tokens": ["Und", "dann", ",", "denn", "sie", "war", "sehr", "ge\u00b7leh\u00b7rig", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KON", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Verst\u00e4ndlicher abgek\u00fcrzt \u00bbErich\u00ab.", "tokens": ["Ver\u00b7st\u00e4nd\u00b7li\u00b7cher", "ab\u00b7ge\u00b7k\u00fcrzt", "\u00bb", "E\u00b7rich", "\u00ab", "."], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["NN", "VVPP", "$(", "NE", "$(", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.15": {"line.1": {"text": "Allmittags haben gemeinsam wir", "tokens": ["All\u00b7mit\u00b7tags", "ha\u00b7ben", "ge\u00b7mein\u00b7sam", "wir"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADJD", "PPER"], "meter": "+-++--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Am gleichen Tische gegessen,", "tokens": ["Am", "glei\u00b7chen", "Ti\u00b7sche", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie Regenw\u00fcrmer mit zwei Tropfen Bier,", "tokens": ["Sie", "Re\u00b7gen\u00b7w\u00fcr\u00b7mer", "mit", "zwei", "Trop\u00b7fen", "Bier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPR", "CARD", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ich totere Delikatessen.", "tokens": ["Ich", "to\u00b7te\u00b7re", "De\u00b7li\u00b7ka\u00b7tes\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.16": {"line.1": {"text": "Sie opferte mir ihren zierlichen Schwanz.", "tokens": ["Sie", "op\u00b7fer\u00b7te", "mir", "ih\u00b7ren", "zier\u00b7li\u00b7chen", "Schwanz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich lehrte sie \u00fcberwinden", "tokens": ["Ich", "lehr\u00b7te", "sie", "\u00fc\u00b7berw\u00b7in\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und Knoten schlagen und Spitzentanz,", "tokens": ["Und", "Kno\u00b7ten", "schla\u00b7gen", "und", "Spit\u00b7zen\u00b7tanz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Schluckdegen und Selbstbinder binden.", "tokens": ["Schluck\u00b7de\u00b7gen", "und", "Selbst\u00b7bin\u00b7der", "bin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVINF", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}}, "stanza.17": {"line.1": {"text": "Sie war so appetitlich und nett.", "tokens": ["Sie", "war", "so", "ap\u00b7pe\u00b7tit\u00b7lich", "und", "nett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sie schlief Nacht \u00fcber in meinem Bett", "tokens": ["Sie", "schlief", "Nacht", "\u00fc\u00b7ber", "in", "mei\u00b7nem", "Bett"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "APPR", "APPR", "PPOSAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Als wie ein k\u00fchlender Schmuckreif am Hals,", "tokens": ["Als", "wie", "ein", "k\u00fch\u00b7len\u00b7der", "Schmuck\u00b7reif", "am", "Hals", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Metallisch und doch so sch\u00f6n weichlich.", "tokens": ["Me\u00b7tal\u00b7lisch", "und", "doch", "so", "sch\u00f6n", "weich\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADV", "ADV", "ADJD", "ADJD", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "Und wenn ihr wirklich was schlimmstenfalls", "tokens": ["Und", "wenn", "ihr", "wirk\u00b7lich", "was", "schlimms\u00b7ten\u00b7falls"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADJD", "PWS", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Passierte, so war es nie reichlich.", "tokens": ["Pas\u00b7sier\u00b7te", ",", "so", "war", "es", "nie", "reich\u00b7lich", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.18": {"line.1": {"text": "Kein Sexuelles und keine Dressur.", "tokens": ["Kein", "Se\u00b7xu\u00b7el\u00b7les", "und", "kei\u00b7ne", "Dres\u00b7sur", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "PIAT", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich war ihr ein Freund und ein Lehrer,", "tokens": ["Ich", "war", "ihr", "ein", "Freund", "und", "ein", "Leh\u00b7rer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Was keiner von meinen Bekannten erfuhr;", "tokens": ["Was", "kei\u00b7ner", "von", "mei\u00b7nen", "Be\u00b7kann\u00b7ten", "er\u00b7fuhr", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Wer mich besuchte, der sah sie nur", "tokens": ["Wer", "mich", "be\u00b7such\u00b7te", ",", "der", "sah", "sie", "nur"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "VVFIN", "$,", "PRELS", "VVFIN", "PPER", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Auf meinem Schreibtisch steif neben der Uhr", "tokens": ["Auf", "mei\u00b7nem", "Schreib\u00b7tisch", "steif", "ne\u00b7ben", "der", "Uhr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Als bronzenen Briefbeschwerer.", "tokens": ["Als", "bron\u00b7ze\u00b7nen", "Brief\u00b7be\u00b7schwe\u00b7rer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.19": {"line.1": {"text": "Und Jahre vergingen. Dann schlief ich einmal", "tokens": ["Und", "Jah\u00b7re", "ver\u00b7gin\u00b7gen", ".", "Dann", "schlief", "ich", "ein\u00b7mal"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVINF", "$.", "ADV", "VVFIN", "PPER", "ADV"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mit Blindschi und tr\u00e4umte im Betti", "tokens": ["Mit", "Blind\u00b7schi", "und", "tr\u00e4um\u00b7te", "im", "Bet\u00b7ti"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "VVFIN", "APPRART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "(jetzt werde ich wieder sentimental)", "tokens": ["(", "jetzt", "wer\u00b7de", "ich", "wie\u00b7der", "sen\u00b7ti\u00b7men\u00b7tal", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Gerade, ich \u00e4\u00dfe Spaghetti.", "tokens": ["Ge\u00b7ra\u00b7de", ",", "ich", "\u00e4\u00b7\u00dfe", "Spag\u00b7het\u00b7ti", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "NE", "$."], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}}, "stanza.20": {"line.1": {"text": "Da kam es, da\u00df irgendwas aus mir pfiff.", "tokens": ["Da", "kam", "es", ",", "da\u00df", "ir\u00b7gend\u00b7was", "aus", "mir", "pfiff", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PIS", "APPR", "PPER", "VVFIN", "$."], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Mag sein, da\u00df es f\u00fcrchterlich krachte.", "tokens": ["Mag", "sein", ",", "da\u00df", "es", "f\u00fcrch\u00b7ter\u00b7lich", "krach\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VAINF", "$,", "KOUS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Fest steht, da\u00df Blindschi erwachte", "tokens": ["Fest", "steht", ",", "da\u00df", "Blind\u00b7schi", "er\u00b7wach\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "VVFIN", "$,", "KOUS", "NN", "VVFIN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und \u2013 sie, die sonst niemals nachts muckte \u2013", "tokens": ["Und", "\u2013", "sie", ",", "die", "sonst", "nie\u00b7mals", "nachts", "muck\u00b7te", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "PPER", "$,", "PRELS", "ADV", "ADV", "ADV", "VVFIN", "$("], "meter": "--+-+-+--", "measure": "anapaest.init"}, "line.5": {"text": "Wild z\u00fcngelte, da\u00df ich nach ihr griff", "tokens": ["Wild", "z\u00fcn\u00b7gel\u00b7te", ",", "da\u00df", "ich", "nach", "ihr", "griff"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "$,", "KOUS", "PPER", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "Und sie, noch tr\u00e4umend, verschluckte.", "tokens": ["Und", "sie", ",", "noch", "tr\u00e4u\u00b7mend", ",", "ver\u00b7schluck\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "$,", "ADV", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.21": {"line.1": {"text": "Es gleich zu sagen: Sie ging nicht tot.", "tokens": ["Es", "gleich", "zu", "sa\u00b7gen", ":", "Sie", "ging", "nicht", "tot", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PTKZU", "VVINF", "$.", "PPER", "VVFIN", "PTKNEG", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sie ist mir wieder entwichen,", "tokens": ["Sie", "ist", "mir", "wie\u00b7der", "ent\u00b7wi\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ist in die W\u00e4lder geschlichen", "tokens": ["Ist", "in", "die", "W\u00e4l\u00b7der", "ge\u00b7schli\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVPP"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Und sucht dort einsam ihr t\u00e4gliches Brot.", "tokens": ["Und", "sucht", "dort", "ein\u00b7sam", "ihr", "t\u00e4g\u00b7li\u00b7ches", "Brot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Vorbei! Es w\u00e4re \u2013 ich bin doch nicht blind \u2013", "tokens": ["Vor\u00b7bei", "!", "Es", "w\u00e4\u00b7re", "\u2013", "ich", "bin", "doch", "nicht", "blind", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VAFIN", "$(", "PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Vergebens, ihr nachzuschleichen.", "tokens": ["Ver\u00b7ge\u00b7bens", ",", "ihr", "nach\u00b7zu\u00b7schlei\u00b7chen", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Weil ihre Wege zu dunkel sind.", "tokens": ["Weil", "ih\u00b7re", "We\u00b7ge", "zu", "dun\u00b7kel", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKA", "ADJD", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Weil wir einander nicht gleichen.", "tokens": ["Weil", "wir", "ein\u00b7an\u00b7der", "nicht", "glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}