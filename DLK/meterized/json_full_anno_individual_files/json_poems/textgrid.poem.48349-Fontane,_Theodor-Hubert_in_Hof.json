{"textgrid.poem.48349": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Hubert in Hof", "genre": "verse", "period": "N.A.", "pub_year": 1858, "urn": "N.A.", "language": ["de:0.85", "sv:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hubert der Maler \u2013 am Isarstrand", "tokens": ["Hu\u00b7bert", "der", "Ma\u00b7ler", "\u2013", "am", "Is\u00b7ar\u00b7strand"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NE", "ART", "NN", "$(", "APPRART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sitzt er in Bajuvarenland.", "tokens": ["Sitzt", "er", "in", "Ba\u00b7ju\u00b7va\u00b7ren\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "Er sitzt und sinnt: Wohl bin ich froh", "tokens": ["Er", "sitzt", "und", "sinnt", ":", "Wohl", "bin", "ich", "froh"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "$.", "ADV", "VAFIN", "PPER", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In der M\u00f6nchestadt, in Monaco,", "tokens": ["In", "der", "M\u00f6n\u00b7ches\u00b7tadt", ",", "in", "Mo\u00b7na\u00b7co", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "APPR", "NE", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wohl trink' ich hier Weihen-Stephan am Quell,", "tokens": ["Wohl", "trink'", "ich", "hier", "Wei\u00b7hen\u00b7Ste\u00b7phan", "am", "Quell", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "NE", "APPRART", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und doch mein Aug', es wird tr\u00fcb und hell,", "tokens": ["Und", "doch", "mein", "Aug'", ",", "es", "wird", "tr\u00fcb", "und", "hell", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "$,", "PPER", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Mein Aug', es sieht, als w\u00e4r' es im Traum,", "tokens": ["Mein", "Aug'", ",", "es", "sieht", ",", "als", "w\u00e4r'", "es", "im", "Traum", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "VVFIN", "$,", "KOKOM", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "Am L\u00fctzowplatz einen Weihnachtsbaum.", "tokens": ["Am", "L\u00fct\u00b7zow\u00b7platz", "ei\u00b7nen", "Weih\u00b7nachts\u00b7baum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.7": {"text": "Es geht nicht l\u00e4nger, ich will nach Haus,", "tokens": ["Es", "geht", "nicht", "l\u00e4n\u00b7ger", ",", "ich", "will", "nach", "Haus", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADJD", "$,", "PPER", "VMFIN", "APPR", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Mir geht hier Laun' und Stimmung aus,", "tokens": ["Mir", "geht", "hier", "Laun'", "und", "Stim\u00b7mung", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Ich reis' auch gleich, ohne lange zu schreiben,", "tokens": ["Ich", "reis'", "auch", "gleich", ",", "oh\u00b7ne", "lan\u00b7ge", "zu", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "KOUI", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Und wenn f\u00fcnf Minuten in Hof wir bleiben,", "tokens": ["Und", "wenn", "f\u00fcnf", "Mi\u00b7nu\u00b7ten", "in", "Hof", "wir", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "CARD", "NN", "APPR", "NN", "PPER", "VVINF", "$,"], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "So telegraphier' ich nach Berlin-West:", "tokens": ["So", "te\u00b7le\u00b7gra\u00b7phier'", "ich", "nach", "Ber\u00b7lin\u00b7West", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "\u00bbkomme noch heute, komme zum Fest.", "tokens": ["\u00bb", "kom\u00b7me", "noch", "heu\u00b7te", ",", "kom\u00b7me", "zum", "Fest", "."], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "ADV", "ADV", "$,", "VVFIN", "APPRART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.13": {"text": "Hubert in Hof.\u00ab", "tokens": ["Hu\u00b7bert", "in", "Hof", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "NE", "$.", "$("], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.3": {"line.1": {"text": "Gesagt, getan. Er nimmt ein Billett.", "tokens": ["Ge\u00b7sagt", ",", "ge\u00b7tan", ".", "Er", "nimmt", "ein", "Bil\u00b7lett", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$.", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ei, das Reisen, es ist doch nett,", "tokens": ["Ei", ",", "das", "Rei\u00b7sen", ",", "es", "ist", "doch", "nett", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Der Wagen ist warm, die Sitze sind breit,", "tokens": ["Der", "Wa\u00b7gen", "ist", "warm", ",", "die", "Sit\u00b7ze", "sind", "breit", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und drau\u00dfen so still. Und wie h\u00fcbsch es schneit.", "tokens": ["Und", "drau\u00b7\u00dfen", "so", "still", ".", "Und", "wie", "h\u00fcbsch", "es", "schneit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "$.", "KON", "PWAV", "ADJD", "PPER", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "\u00bbich mache mir nichts aus Sturm und Regen,", "tokens": ["\u00bb", "ich", "ma\u00b7che", "mir", "nichts", "aus", "Sturm", "und", "Re\u00b7gen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "PIS", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Aber Schnee, ", "tokens": ["A\u00b7ber", "Schnee", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Den sch\u00fcttelt man ab, der macht nicht na\u00df,", "tokens": ["Den", "sch\u00fct\u00b7telt", "man", "ab", ",", "der", "macht", "nicht", "na\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIS", "PTKVZ", "$,", "PRELS", "VVFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Schneewetter, vor allem lieb' ich ", "tokens": ["Schnee\u00b7wet\u00b7ter", ",", "vor", "al\u00b7lem", "lieb'", "ich"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "APPR", "PIS", "VVFIN", "PPER"], "meter": "++--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "Schnee d\u00e4mpft selbst des Eilzugs Gest\u00f6hn und Gedr\u00f6hn,", "tokens": ["Schnee", "d\u00e4mpft", "selbst", "des", "Eil\u00b7zugs", "Ge\u00b7st\u00f6hn", "und", "Ge\u00b7dr\u00f6hn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Schnee ist blo\u00df h\u00fcbsch, Schnee ist blo\u00df sch\u00f6n!\u00ab", "tokens": ["Schnee", "ist", "blo\u00df", "h\u00fcbsch", ",", "Schnee", "ist", "blo\u00df", "sch\u00f6n", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VAFIN", "ADV", "ADJD", "$,", "NN", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "So Hubert, als er in erster Stund'", "tokens": ["So", "Hu\u00b7bert", ",", "als", "er", "in", "ers\u00b7ter", "Stund'"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "$,", "KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In N\u00e4he von Freysing sich befund.", "tokens": ["In", "N\u00e4\u00b7he", "von", "Frey\u00b7sing", "sich", "be\u00b7fund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "PRF", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Auch in Ingolstadt noch. Aber schon bei F\u00fcrth", "tokens": ["Auch", "in", "In\u00b7gol\u00b7stadt", "noch", ".", "A\u00b7ber", "schon", "bei", "F\u00fcrth"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "ADV", "$.", "KON", "ADV", "APPR", "NN"], "meter": "---+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Sache ziemlich bedenklich wird,", "tokens": ["Die", "Sa\u00b7che", "ziem\u00b7lich", "be\u00b7denk\u00b7lich", "wird", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Es schneit und schneit, es f\u00e4llt und f\u00e4llt,", "tokens": ["Es", "schneit", "und", "schneit", ",", "es", "f\u00e4llt", "und", "f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "ADJD", "$,", "PPER", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Schneehaufe wird die ganze Welt,", "tokens": ["Ein", "Schnee\u00b7hau\u00b7fe", "wird", "die", "gan\u00b7ze", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-++-+-+-+", "measure": "unknown.measure.penta"}, "line.7": {"text": "B\u00e4ume, D\u00e4cher, Kirchturmspitzen,", "tokens": ["B\u00e4u\u00b7me", ",", "D\u00e4\u00b7cher", ",", "Kirch\u00b7turm\u00b7spit\u00b7zen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Alle schon tief in der Kappe sitzen,", "tokens": ["Al\u00b7le", "schon", "tief", "in", "der", "Kap\u00b7pe", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.9": {"text": "Und als die Maschine, die l\u00e4ngst nicht mehr fleucht,", "tokens": ["Und", "als", "die", "Ma\u00b7schi\u00b7ne", ",", "die", "l\u00e4ngst", "nicht", "mehr", "fleucht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "$,", "PRELS", "ADV", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Sich bis nach Hof hin durchgekeucht,", "tokens": ["Sich", "bis", "nach", "Hof", "hin", "durch\u00b7ge\u00b7keucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "APPR", "NE", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Da sitzen sie fest, der Zug steht still,", "tokens": ["Da", "sit\u00b7zen", "sie", "fest", ",", "der", "Zug", "steht", "still", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Die Wand nicht weiter sich \u00f6ffnen will,", "tokens": ["Die", "Wand", "nicht", "wei\u00b7ter", "sich", "\u00f6ff\u00b7nen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKNEG", "ADV", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Und die Schaffner rufen: \u00bbAussteigen; zu Nacht", "tokens": ["Und", "die", "Schaff\u00b7ner", "ru\u00b7fen", ":", "\u00bb", "Aus\u00b7stei\u00b7gen", ";", "zu", "Nacht"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ART", "NN", "VVINF", "$.", "$(", "NN", "$.", "APPR", "NN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Wird ", "tokens": ["Wird"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Entsetzen, Lachen, Fluchen, Gewimmer,", "tokens": ["Ent\u00b7set\u00b7zen", ",", "La\u00b7chen", ",", "Flu\u00b7chen", ",", "Ge\u00b7wim\u00b7mer", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.16": {"text": "Alles st\u00fcrzt in das Wartezimmer,", "tokens": ["Al\u00b7les", "st\u00fcrzt", "in", "das", "War\u00b7te\u00b7zim\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.17": {"text": "Nur einer kennt eine h\u00f6here Pflicht,", "tokens": ["Nur", "ei\u00b7ner", "kennt", "ei\u00b7ne", "h\u00f6\u00b7he\u00b7re", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Er telegraphiert: \u00bbErwartet mich nicht.", "tokens": ["Er", "te\u00b7le\u00b7gra\u00b7phiert", ":", "\u00bb", "Er\u00b7war\u00b7tet", "mich", "nicht", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.19": {"text": "Eingeschneit. Macht Euch keine Sorgen.", "tokens": ["Ein\u00b7ge\u00b7schneit", ".", "Macht", "Euch", "kei\u00b7ne", "Sor\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "PPER", "PIAT", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.20": {"text": "Ich sitze hier fest, komm' also morgen.", "tokens": ["Ich", "sit\u00b7ze", "hier", "fest", ",", "komm'", "al\u00b7so", "mor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$,", "VVFIN", "ADV", "ADV", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.21": {"text": "Hubert in Hof.\u00ab", "tokens": ["Hu\u00b7bert", "in", "Hof", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "NE", "$.", "$("], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.5": {"line.1": {"text": "Das klang noch zun\u00e4chst vergn\u00fcglich fast,", "tokens": ["Das", "klang", "noch", "zu\u00b7n\u00e4chst", "ver\u00b7gn\u00fcg\u00b7lich", "fast", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "ADJD", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aber die L\u00e4nge, sie hat die Last,", "tokens": ["A\u00b7ber", "die", "L\u00e4n\u00b7ge", ",", "sie", "hat", "die", "Last", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Ihr alle kennt den Ausspruch ja:", "tokens": ["Ihr", "al\u00b7le", "kennt", "den", "Aus\u00b7spruch", "ja", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "VVFIN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbfr\u00fch um acht in Potsdam, was soll ich da?\u00ab", "tokens": ["\u00bb", "fr\u00fch", "um", "acht", "in", "Pots\u00b7dam", ",", "was", "soll", "ich", "da", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "APPR", "CARD", "APPR", "NE", "$,", "PWS", "VMFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und Potsdam ist immer doch Potsdam noch,", "tokens": ["Und", "Pots\u00b7dam", "ist", "im\u00b7mer", "doch", "Pots\u00b7dam", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ADV", "ADV", "NE", "ADV", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Aber \u00bb", "tokens": ["A\u00b7ber", "\u00bb"], "token_info": ["word", "punct"], "pos": ["KON", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Wen kann es tr\u00f6sten, wer kann dran genesen,", "tokens": ["Wen", "kann", "es", "tr\u00f6s\u00b7ten", ",", "wer", "kann", "dran", "ge\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "VVINF", "$,", "PWS", "VMFIN", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Da\u00df Jean Paul in Hof auf der Schule gewesen?", "tokens": ["Da\u00df", "Jean", "Paul", "in", "Hof", "auf", "der", "Schu\u00b7le", "ge\u00b7we\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "APPR", "NN", "APPR", "ART", "NN", "VAPP", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}}, "stanza.6": {"line.1": {"text": "Und der Wartesaal! Himmel, welche Ger\u00fcche,", "tokens": ["Und", "der", "War\u00b7te\u00b7saal", "!", "Him\u00b7mel", ",", "wel\u00b7che", "Ge\u00b7r\u00fc\u00b7che", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$.", "NN", "$,", "PWAT", "NN", "$,"], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Dunst und Wrasen aus Keller und K\u00fcche,", "tokens": ["Dunst", "und", "Wra\u00b7sen", "aus", "Kel\u00b7ler", "und", "K\u00fc\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Von Stiefelsohlen die Schneekrustenschmelze,", "tokens": ["Von", "Stie\u00b7fel\u00b7soh\u00b7len", "die", "Schnee\u00b7krus\u00b7ten\u00b7schmel\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Zigarren aus \u00d6streich, Judenpelze,", "tokens": ["Zi\u00b7gar\u00b7ren", "aus", "\u00d6s\u00b7treich", ",", "Ju\u00b7den\u00b7pel\u00b7ze", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "APPR", "NE", "$,", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "K\u00f6rbe mit Eiern, mit Hering, mit K\u00e4se,", "tokens": ["K\u00f6r\u00b7be", "mit", "Ei\u00b7ern", ",", "mit", "He\u00b7ring", ",", "mit", "K\u00e4\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "APPR", "NN", "$,", "APPR", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Kanonen\u00f6fen mit Glutgebl\u00e4se,", "tokens": ["Ka\u00b7no\u00b7ne\u00b7n\u00f6\u00b7fen", "mit", "Glut\u00b7ge\u00b7bl\u00e4\u00b7se", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Zwiebelbeefsteak, bayrische W\u00fcrste,", "tokens": ["Zwie\u00b7bel\u00b7beefs\u00b7teak", ",", "bay\u00b7ri\u00b7sche", "W\u00fcrs\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Gepfeffert, gesalzen von wegen der D\u00fcrste.", "tokens": ["Ge\u00b7pfef\u00b7fert", ",", "ge\u00b7sal\u00b7zen", "von", "we\u00b7gen", "der", "D\u00fcrs\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "APPR", "APPR", "ART", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.9": {"text": "Ja D\u00fcrste! Riesig w\u00e4chst der Wunsch", "tokens": ["Ja", "D\u00fcrs\u00b7te", "!", "Rie\u00b7sig", "w\u00e4chst", "der", "Wunsch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "NN", "$.", "NE", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Nach Gl\u00fchwein, Knickebein, Grog und Punsch,", "tokens": ["Nach", "Gl\u00fch\u00b7wein", ",", "Kni\u00b7cke\u00b7bein", ",", "Grog", "und", "Pun\u00b7sch", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Salate von Fisch, Mayonnaise von Hummer.", "tokens": ["Sa\u00b7la\u00b7te", "von", "Fisch", ",", "Ma\u00b7yon\u00b7nai\u00b7se", "von", "Hum\u00b7mer", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$,", "NN", "APPR", "NE", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.12": {"text": "Manch vermostrichte Zeitungsnummer,", "tokens": ["Manch", "ver\u00b7most\u00b7rich\u00b7te", "Zei\u00b7tungs\u00b7num\u00b7mer", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.13": {"text": "Vier Wochen alte Kladderadatsche,", "tokens": ["Vier", "Wo\u00b7chen", "al\u00b7te", "Klad\u00b7de\u00b7ra\u00b7dat\u00b7sche", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Witze, politisches Getratsche,", "tokens": ["Wit\u00b7ze", ",", "po\u00b7li\u00b7ti\u00b7sches", "Ge\u00b7trat\u00b7sche", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.15": {"text": "Harfenistinnen, Geige, Klaviergeklimper,", "tokens": ["Har\u00b7fe\u00b7nis\u00b7tin\u00b7nen", ",", "Gei\u00b7ge", ",", "Kla\u00b7vier\u00b7ge\u00b7klim\u00b7per", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.16": {"text": "Courmacher, derb und mit Gezimper,", "tokens": ["Cour\u00b7ma\u00b7cher", ",", "derb", "und", "mit", "Ge\u00b7zim\u00b7per", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Und allviertelst\u00fcndlich ein neuer Rapport:", "tokens": ["Und", "all\u00b7vier\u00b7tel\u00b7st\u00fcnd\u00b7lich", "ein", "neu\u00b7er", "Rap\u00b7port", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$."], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "\u00bbes schneit und schneit noch immer fort.\u00ab", "tokens": ["\u00bb", "es", "schneit", "und", "schneit", "noch", "im\u00b7mer", "fort", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "ADJD", "KON", "ADJD", "ADV", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "So sitzen sie fest und spielen Skat,", "tokens": ["So", "sit\u00b7zen", "sie", "fest", "und", "spie\u00b7len", "Skat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "KON", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.20": {"text": "Und nach Haus hin sehnt sich, fr\u00fch und spat,", "tokens": ["Und", "nach", "Haus", "hin", "sehnt", "sich", ",", "fr\u00fch", "und", "spat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "ADV", "VVFIN", "PRF", "$,", "ADJD", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.21": {"text": "Hubert in Hof.", "tokens": ["Hu\u00b7bert", "in", "Hof", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.7": {"line.1": {"text": "Doch Gott sei Dank, 's steht irgendwo", "tokens": ["Doch", "Gott", "sei", "Dank", ",", "'s", "steht", "ir\u00b7gend\u00b7wo"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NN", "VAFIN", "NN", "$,", "PPER", "VVFIN", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "(konfuz oder K\u00f6nig Salomo),", "tokens": ["(", "kon\u00b7fuz", "o\u00b7der", "K\u00f6\u00b7nig", "Sa\u00b7lo\u00b7mo", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "KON", "NN", "NE", "$(", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "\u00bbein jedes Ding hat seine Zeit\u00ab,", "tokens": ["\u00bb", "ein", "je\u00b7des", "Ding", "hat", "sei\u00b7ne", "Zeit", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "PIAT", "NN", "VAFIN", "PPOSAT", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und so hat's denn auch endlich ausgeschneit.", "tokens": ["Und", "so", "hat's", "denn", "auch", "end\u00b7lich", "aus\u00b7ge\u00b7schneit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "\u00bbeinsteigen!\u00ab erklingt das s\u00fc\u00dfe Wort,", "tokens": ["\u00bb", "ein\u00b7stei\u00b7gen", "!", "\u00ab", "er\u00b7klingt", "das", "s\u00fc\u00b7\u00dfe", "Wort", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVPP", "$.", "$(", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Und wieder norderw\u00e4rts geht es fort,", "tokens": ["Und", "wie\u00b7der", "nor\u00b7der\u00b7w\u00e4rts", "geht", "es", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Lokomotive, tapfrer Held,", "tokens": ["Lo\u00b7ko\u00b7mo\u00b7ti\u00b7ve", ",", "tapf\u00b7rer", "Held", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "Schl\u00e4gt sich durch bis Bitterfeld.", "tokens": ["Schl\u00e4gt", "sich", "durch", "bis", "Bit\u00b7ter\u00b7feld", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "In Wittenberg, wie Sirenengesang,", "tokens": ["In", "Wit\u00b7ten\u00b7berg", ",", "wie", "Si\u00b7re\u00b7nen\u00b7ge\u00b7sang", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "PWAV", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbapfelkuchen!\u00ab klingt es den Bahnsteig entlang,", "tokens": ["\u00bb", "ap\u00b7fel\u00b7ku\u00b7chen", "!", "\u00ab", "klingt", "es", "den", "Bahn\u00b7steig", "ent\u00b7lang", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVINF", "$.", "$(", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.11": {"text": "Aber Wachs ins Ohr, nur nicht kosten woll'n,", "tokens": ["A\u00b7ber", "Wachs", "ins", "Ohr", ",", "nur", "nicht", "kos\u00b7ten", "woll'n", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPRART", "NN", "$,", "ADV", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Es ruft ja der be\u00dfre Weihnachtsstoll'n \u2013", "tokens": ["Es", "ruft", "ja", "der", "be\u00df\u00b7re", "Weih\u00b7nachtss\u00b7toll'n", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Er ruft ... Und treppauf mit einem Satz", "tokens": ["Er", "ruft", "...", "Und", "trep\u00b7pauf", "mit", "ei\u00b7nem", "Satz"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "KON", "PAV", "APPR", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Ist Hubert jetzt heim am L\u00fctzowplatz,", "tokens": ["Ist", "Hu\u00b7bert", "jetzt", "heim", "am", "L\u00fct\u00b7zow\u00b7platz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "PTKVZ", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Hubert in Hof.", "tokens": ["Hu\u00b7bert", "in", "Hof", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.8": {"line.1": {"text": "Hubert der Maler \u2013 am Isarstrand", "tokens": ["Hu\u00b7bert", "der", "Ma\u00b7ler", "\u2013", "am", "Is\u00b7ar\u00b7strand"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NE", "ART", "NN", "$(", "APPRART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sitzt er in Bajuvarenland.", "tokens": ["Sitzt", "er", "in", "Ba\u00b7ju\u00b7va\u00b7ren\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.9": {"line.1": {"text": "Er sitzt und sinnt: Wohl bin ich froh", "tokens": ["Er", "sitzt", "und", "sinnt", ":", "Wohl", "bin", "ich", "froh"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "$.", "ADV", "VAFIN", "PPER", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In der M\u00f6nchestadt, in Monaco,", "tokens": ["In", "der", "M\u00f6n\u00b7ches\u00b7tadt", ",", "in", "Mo\u00b7na\u00b7co", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "APPR", "NE", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wohl trink' ich hier Weihen-Stephan am Quell,", "tokens": ["Wohl", "trink'", "ich", "hier", "Wei\u00b7hen\u00b7Ste\u00b7phan", "am", "Quell", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "NE", "APPRART", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und doch mein Aug', es wird tr\u00fcb und hell,", "tokens": ["Und", "doch", "mein", "Aug'", ",", "es", "wird", "tr\u00fcb", "und", "hell", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "NN", "$,", "PPER", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Mein Aug', es sieht, als w\u00e4r' es im Traum,", "tokens": ["Mein", "Aug'", ",", "es", "sieht", ",", "als", "w\u00e4r'", "es", "im", "Traum", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "VVFIN", "$,", "KOKOM", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "Am L\u00fctzowplatz einen Weihnachtsbaum.", "tokens": ["Am", "L\u00fct\u00b7zow\u00b7platz", "ei\u00b7nen", "Weih\u00b7nachts\u00b7baum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.7": {"text": "Es geht nicht l\u00e4nger, ich will nach Haus,", "tokens": ["Es", "geht", "nicht", "l\u00e4n\u00b7ger", ",", "ich", "will", "nach", "Haus", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADJD", "$,", "PPER", "VMFIN", "APPR", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Mir geht hier Laun' und Stimmung aus,", "tokens": ["Mir", "geht", "hier", "Laun'", "und", "Stim\u00b7mung", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Ich reis' auch gleich, ohne lange zu schreiben,", "tokens": ["Ich", "reis'", "auch", "gleich", ",", "oh\u00b7ne", "lan\u00b7ge", "zu", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$,", "KOUI", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Und wenn f\u00fcnf Minuten in Hof wir bleiben,", "tokens": ["Und", "wenn", "f\u00fcnf", "Mi\u00b7nu\u00b7ten", "in", "Hof", "wir", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "CARD", "NN", "APPR", "NN", "PPER", "VVINF", "$,"], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "So telegraphier' ich nach Berlin-West:", "tokens": ["So", "te\u00b7le\u00b7gra\u00b7phier'", "ich", "nach", "Ber\u00b7lin\u00b7West", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "\u00bbkomme noch heute, komme zum Fest.", "tokens": ["\u00bb", "kom\u00b7me", "noch", "heu\u00b7te", ",", "kom\u00b7me", "zum", "Fest", "."], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "ADV", "ADV", "$,", "VVFIN", "APPRART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.13": {"text": "Hubert in Hof.\u00ab", "tokens": ["Hu\u00b7bert", "in", "Hof", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "NE", "$.", "$("], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.10": {"line.1": {"text": "Gesagt, getan. Er nimmt ein Billett.", "tokens": ["Ge\u00b7sagt", ",", "ge\u00b7tan", ".", "Er", "nimmt", "ein", "Bil\u00b7lett", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$.", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ei, das Reisen, es ist doch nett,", "tokens": ["Ei", ",", "das", "Rei\u00b7sen", ",", "es", "ist", "doch", "nett", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Der Wagen ist warm, die Sitze sind breit,", "tokens": ["Der", "Wa\u00b7gen", "ist", "warm", ",", "die", "Sit\u00b7ze", "sind", "breit", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und drau\u00dfen so still. Und wie h\u00fcbsch es schneit.", "tokens": ["Und", "drau\u00b7\u00dfen", "so", "still", ".", "Und", "wie", "h\u00fcbsch", "es", "schneit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "$.", "KON", "PWAV", "ADJD", "PPER", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "\u00bbich mache mir nichts aus Sturm und Regen,", "tokens": ["\u00bb", "ich", "ma\u00b7che", "mir", "nichts", "aus", "Sturm", "und", "Re\u00b7gen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "PIS", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Aber Schnee, ", "tokens": ["A\u00b7ber", "Schnee", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Den sch\u00fcttelt man ab, der macht nicht na\u00df,", "tokens": ["Den", "sch\u00fct\u00b7telt", "man", "ab", ",", "der", "macht", "nicht", "na\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIS", "PTKVZ", "$,", "PRELS", "VVFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Schneewetter, vor allem lieb' ich ", "tokens": ["Schnee\u00b7wet\u00b7ter", ",", "vor", "al\u00b7lem", "lieb'", "ich"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "APPR", "PIS", "VVFIN", "PPER"], "meter": "++--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "Schnee d\u00e4mpft selbst des Eilzugs Gest\u00f6hn und Gedr\u00f6hn,", "tokens": ["Schnee", "d\u00e4mpft", "selbst", "des", "Eil\u00b7zugs", "Ge\u00b7st\u00f6hn", "und", "Ge\u00b7dr\u00f6hn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Schnee ist blo\u00df h\u00fcbsch, Schnee ist blo\u00df sch\u00f6n!\u00ab", "tokens": ["Schnee", "ist", "blo\u00df", "h\u00fcbsch", ",", "Schnee", "ist", "blo\u00df", "sch\u00f6n", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VAFIN", "ADV", "ADJD", "$,", "NN", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "So Hubert, als er in erster Stund'", "tokens": ["So", "Hu\u00b7bert", ",", "als", "er", "in", "ers\u00b7ter", "Stund'"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "$,", "KOUS", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In N\u00e4he von Freysing sich befund.", "tokens": ["In", "N\u00e4\u00b7he", "von", "Frey\u00b7sing", "sich", "be\u00b7fund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "PRF", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Auch in Ingolstadt noch. Aber schon bei F\u00fcrth", "tokens": ["Auch", "in", "In\u00b7gol\u00b7stadt", "noch", ".", "A\u00b7ber", "schon", "bei", "F\u00fcrth"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "ADV", "$.", "KON", "ADV", "APPR", "NN"], "meter": "---+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Sache ziemlich bedenklich wird,", "tokens": ["Die", "Sa\u00b7che", "ziem\u00b7lich", "be\u00b7denk\u00b7lich", "wird", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Es schneit und schneit, es f\u00e4llt und f\u00e4llt,", "tokens": ["Es", "schneit", "und", "schneit", ",", "es", "f\u00e4llt", "und", "f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "ADJD", "$,", "PPER", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Schneehaufe wird die ganze Welt,", "tokens": ["Ein", "Schnee\u00b7hau\u00b7fe", "wird", "die", "gan\u00b7ze", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-++-+-+-+", "measure": "unknown.measure.penta"}, "line.7": {"text": "B\u00e4ume, D\u00e4cher, Kirchturmspitzen,", "tokens": ["B\u00e4u\u00b7me", ",", "D\u00e4\u00b7cher", ",", "Kirch\u00b7turm\u00b7spit\u00b7zen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Alle schon tief in der Kappe sitzen,", "tokens": ["Al\u00b7le", "schon", "tief", "in", "der", "Kap\u00b7pe", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.9": {"text": "Und als die Maschine, die l\u00e4ngst nicht mehr fleucht,", "tokens": ["Und", "als", "die", "Ma\u00b7schi\u00b7ne", ",", "die", "l\u00e4ngst", "nicht", "mehr", "fleucht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "$,", "PRELS", "ADV", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Sich bis nach Hof hin durchgekeucht,", "tokens": ["Sich", "bis", "nach", "Hof", "hin", "durch\u00b7ge\u00b7keucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "APPR", "NE", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Da sitzen sie fest, der Zug steht still,", "tokens": ["Da", "sit\u00b7zen", "sie", "fest", ",", "der", "Zug", "steht", "still", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Die Wand nicht weiter sich \u00f6ffnen will,", "tokens": ["Die", "Wand", "nicht", "wei\u00b7ter", "sich", "\u00f6ff\u00b7nen", "will", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKNEG", "ADV", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Und die Schaffner rufen: \u00bbAussteigen; zu Nacht", "tokens": ["Und", "die", "Schaff\u00b7ner", "ru\u00b7fen", ":", "\u00bb", "Aus\u00b7stei\u00b7gen", ";", "zu", "Nacht"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ART", "NN", "VVINF", "$.", "$(", "NN", "$.", "APPR", "NN"], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Wird ", "tokens": ["Wird"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Entsetzen, Lachen, Fluchen, Gewimmer,", "tokens": ["Ent\u00b7set\u00b7zen", ",", "La\u00b7chen", ",", "Flu\u00b7chen", ",", "Ge\u00b7wim\u00b7mer", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.16": {"text": "Alles st\u00fcrzt in das Wartezimmer,", "tokens": ["Al\u00b7les", "st\u00fcrzt", "in", "das", "War\u00b7te\u00b7zim\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.17": {"text": "Nur einer kennt eine h\u00f6here Pflicht,", "tokens": ["Nur", "ei\u00b7ner", "kennt", "ei\u00b7ne", "h\u00f6\u00b7he\u00b7re", "Pflicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Er telegraphiert: \u00bbErwartet mich nicht.", "tokens": ["Er", "te\u00b7le\u00b7gra\u00b7phiert", ":", "\u00bb", "Er\u00b7war\u00b7tet", "mich", "nicht", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.19": {"text": "Eingeschneit. Macht Euch keine Sorgen.", "tokens": ["Ein\u00b7ge\u00b7schneit", ".", "Macht", "Euch", "kei\u00b7ne", "Sor\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "PPER", "PIAT", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.20": {"text": "Ich sitze hier fest, komm' also morgen.", "tokens": ["Ich", "sit\u00b7ze", "hier", "fest", ",", "komm'", "al\u00b7so", "mor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$,", "VVFIN", "ADV", "ADV", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.21": {"text": "Hubert in Hof.\u00ab", "tokens": ["Hu\u00b7bert", "in", "Hof", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "APPR", "NE", "$.", "$("], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.12": {"line.1": {"text": "Das klang noch zun\u00e4chst vergn\u00fcglich fast,", "tokens": ["Das", "klang", "noch", "zu\u00b7n\u00e4chst", "ver\u00b7gn\u00fcg\u00b7lich", "fast", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "ADJD", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aber die L\u00e4nge, sie hat die Last,", "tokens": ["A\u00b7ber", "die", "L\u00e4n\u00b7ge", ",", "sie", "hat", "die", "Last", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Ihr alle kennt den Ausspruch ja:", "tokens": ["Ihr", "al\u00b7le", "kennt", "den", "Aus\u00b7spruch", "ja", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "VVFIN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbfr\u00fch um acht in Potsdam, was soll ich da?\u00ab", "tokens": ["\u00bb", "fr\u00fch", "um", "acht", "in", "Pots\u00b7dam", ",", "was", "soll", "ich", "da", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "APPR", "CARD", "APPR", "NE", "$,", "PWS", "VMFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Und Potsdam ist immer doch Potsdam noch,", "tokens": ["Und", "Pots\u00b7dam", "ist", "im\u00b7mer", "doch", "Pots\u00b7dam", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ADV", "ADV", "NE", "ADV", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Aber \u00bb", "tokens": ["A\u00b7ber", "\u00bb"], "token_info": ["word", "punct"], "pos": ["KON", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Wen kann es tr\u00f6sten, wer kann dran genesen,", "tokens": ["Wen", "kann", "es", "tr\u00f6s\u00b7ten", ",", "wer", "kann", "dran", "ge\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "VVINF", "$,", "PWS", "VMFIN", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Da\u00df Jean Paul in Hof auf der Schule gewesen?", "tokens": ["Da\u00df", "Jean", "Paul", "in", "Hof", "auf", "der", "Schu\u00b7le", "ge\u00b7we\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "APPR", "NN", "APPR", "ART", "NN", "VAPP", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}}, "stanza.13": {"line.1": {"text": "Und der Wartesaal! Himmel, welche Ger\u00fcche,", "tokens": ["Und", "der", "War\u00b7te\u00b7saal", "!", "Him\u00b7mel", ",", "wel\u00b7che", "Ge\u00b7r\u00fc\u00b7che", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$.", "NN", "$,", "PWAT", "NN", "$,"], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Dunst und Wrasen aus Keller und K\u00fcche,", "tokens": ["Dunst", "und", "Wra\u00b7sen", "aus", "Kel\u00b7ler", "und", "K\u00fc\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Von Stiefelsohlen die Schneekrustenschmelze,", "tokens": ["Von", "Stie\u00b7fel\u00b7soh\u00b7len", "die", "Schnee\u00b7krus\u00b7ten\u00b7schmel\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Zigarren aus \u00d6streich, Judenpelze,", "tokens": ["Zi\u00b7gar\u00b7ren", "aus", "\u00d6s\u00b7treich", ",", "Ju\u00b7den\u00b7pel\u00b7ze", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "APPR", "NE", "$,", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "K\u00f6rbe mit Eiern, mit Hering, mit K\u00e4se,", "tokens": ["K\u00f6r\u00b7be", "mit", "Ei\u00b7ern", ",", "mit", "He\u00b7ring", ",", "mit", "K\u00e4\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "APPR", "NN", "$,", "APPR", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Kanonen\u00f6fen mit Glutgebl\u00e4se,", "tokens": ["Ka\u00b7no\u00b7ne\u00b7n\u00f6\u00b7fen", "mit", "Glut\u00b7ge\u00b7bl\u00e4\u00b7se", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Zwiebelbeefsteak, bayrische W\u00fcrste,", "tokens": ["Zwie\u00b7bel\u00b7beefs\u00b7teak", ",", "bay\u00b7ri\u00b7sche", "W\u00fcrs\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Gepfeffert, gesalzen von wegen der D\u00fcrste.", "tokens": ["Ge\u00b7pfef\u00b7fert", ",", "ge\u00b7sal\u00b7zen", "von", "we\u00b7gen", "der", "D\u00fcrs\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "APPR", "APPR", "ART", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.9": {"text": "Ja D\u00fcrste! Riesig w\u00e4chst der Wunsch", "tokens": ["Ja", "D\u00fcrs\u00b7te", "!", "Rie\u00b7sig", "w\u00e4chst", "der", "Wunsch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "NN", "$.", "NE", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Nach Gl\u00fchwein, Knickebein, Grog und Punsch,", "tokens": ["Nach", "Gl\u00fch\u00b7wein", ",", "Kni\u00b7cke\u00b7bein", ",", "Grog", "und", "Pun\u00b7sch", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Salate von Fisch, Mayonnaise von Hummer.", "tokens": ["Sa\u00b7la\u00b7te", "von", "Fisch", ",", "Ma\u00b7yon\u00b7nai\u00b7se", "von", "Hum\u00b7mer", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$,", "NN", "APPR", "NE", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.12": {"text": "Manch vermostrichte Zeitungsnummer,", "tokens": ["Manch", "ver\u00b7most\u00b7rich\u00b7te", "Zei\u00b7tungs\u00b7num\u00b7mer", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.13": {"text": "Vier Wochen alte Kladderadatsche,", "tokens": ["Vier", "Wo\u00b7chen", "al\u00b7te", "Klad\u00b7de\u00b7ra\u00b7dat\u00b7sche", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Witze, politisches Getratsche,", "tokens": ["Wit\u00b7ze", ",", "po\u00b7li\u00b7ti\u00b7sches", "Ge\u00b7trat\u00b7sche", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.15": {"text": "Harfenistinnen, Geige, Klaviergeklimper,", "tokens": ["Har\u00b7fe\u00b7nis\u00b7tin\u00b7nen", ",", "Gei\u00b7ge", ",", "Kla\u00b7vier\u00b7ge\u00b7klim\u00b7per", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.16": {"text": "Courmacher, derb und mit Gezimper,", "tokens": ["Cour\u00b7ma\u00b7cher", ",", "derb", "und", "mit", "Ge\u00b7zim\u00b7per", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Und allviertelst\u00fcndlich ein neuer Rapport:", "tokens": ["Und", "all\u00b7vier\u00b7tel\u00b7st\u00fcnd\u00b7lich", "ein", "neu\u00b7er", "Rap\u00b7port", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$."], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "\u00bbes schneit und schneit noch immer fort.\u00ab", "tokens": ["\u00bb", "es", "schneit", "und", "schneit", "noch", "im\u00b7mer", "fort", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "ADJD", "KON", "ADJD", "ADV", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "So sitzen sie fest und spielen Skat,", "tokens": ["So", "sit\u00b7zen", "sie", "fest", "und", "spie\u00b7len", "Skat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "KON", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.20": {"text": "Und nach Haus hin sehnt sich, fr\u00fch und spat,", "tokens": ["Und", "nach", "Haus", "hin", "sehnt", "sich", ",", "fr\u00fch", "und", "spat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "ADV", "VVFIN", "PRF", "$,", "ADJD", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.21": {"text": "Hubert in Hof.", "tokens": ["Hu\u00b7bert", "in", "Hof", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.14": {"line.1": {"text": "Doch Gott sei Dank, 's steht irgendwo", "tokens": ["Doch", "Gott", "sei", "Dank", ",", "'s", "steht", "ir\u00b7gend\u00b7wo"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NN", "VAFIN", "NN", "$,", "PPER", "VVFIN", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "(konfuz oder K\u00f6nig Salomo),", "tokens": ["(", "kon\u00b7fuz", "o\u00b7der", "K\u00f6\u00b7nig", "Sa\u00b7lo\u00b7mo", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "KON", "NN", "NE", "$(", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "\u00bbein jedes Ding hat seine Zeit\u00ab,", "tokens": ["\u00bb", "ein", "je\u00b7des", "Ding", "hat", "sei\u00b7ne", "Zeit", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "PIAT", "NN", "VAFIN", "PPOSAT", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und so hat's denn auch endlich ausgeschneit.", "tokens": ["Und", "so", "hat's", "denn", "auch", "end\u00b7lich", "aus\u00b7ge\u00b7schneit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "\u00bbeinsteigen!\u00ab erklingt das s\u00fc\u00dfe Wort,", "tokens": ["\u00bb", "ein\u00b7stei\u00b7gen", "!", "\u00ab", "er\u00b7klingt", "das", "s\u00fc\u00b7\u00dfe", "Wort", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVPP", "$.", "$(", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Und wieder norderw\u00e4rts geht es fort,", "tokens": ["Und", "wie\u00b7der", "nor\u00b7der\u00b7w\u00e4rts", "geht", "es", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Lokomotive, tapfrer Held,", "tokens": ["Lo\u00b7ko\u00b7mo\u00b7ti\u00b7ve", ",", "tapf\u00b7rer", "Held", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "Schl\u00e4gt sich durch bis Bitterfeld.", "tokens": ["Schl\u00e4gt", "sich", "durch", "bis", "Bit\u00b7ter\u00b7feld", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "In Wittenberg, wie Sirenengesang,", "tokens": ["In", "Wit\u00b7ten\u00b7berg", ",", "wie", "Si\u00b7re\u00b7nen\u00b7ge\u00b7sang", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "PWAV", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbapfelkuchen!\u00ab klingt es den Bahnsteig entlang,", "tokens": ["\u00bb", "ap\u00b7fel\u00b7ku\u00b7chen", "!", "\u00ab", "klingt", "es", "den", "Bahn\u00b7steig", "ent\u00b7lang", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVINF", "$.", "$(", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.11": {"text": "Aber Wachs ins Ohr, nur nicht kosten woll'n,", "tokens": ["A\u00b7ber", "Wachs", "ins", "Ohr", ",", "nur", "nicht", "kos\u00b7ten", "woll'n", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPRART", "NN", "$,", "ADV", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Es ruft ja der be\u00dfre Weihnachtsstoll'n \u2013", "tokens": ["Es", "ruft", "ja", "der", "be\u00df\u00b7re", "Weih\u00b7nachtss\u00b7toll'n", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Er ruft ... Und treppauf mit einem Satz", "tokens": ["Er", "ruft", "...", "Und", "trep\u00b7pauf", "mit", "ei\u00b7nem", "Satz"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "KON", "PAV", "APPR", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Ist Hubert jetzt heim am L\u00fctzowplatz,", "tokens": ["Ist", "Hu\u00b7bert", "jetzt", "heim", "am", "L\u00fct\u00b7zow\u00b7platz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "PTKVZ", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Hubert in Hof.", "tokens": ["Hu\u00b7bert", "in", "Hof", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}}}}