{"dta.poem.12576": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n F\u00fcnfter Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "AlS diese gro\u00dfe Schlacht/ wie dann in vielen Jah-\nren", "tokens": ["AlS", "die\u00b7se", "gro\u00b7\u00dfe", "Schlacht", "/", "wie", "dann", "in", "vie\u00b7len", "Jah", "ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "PDAT", "ADJA", "NN", "$(", "KOKOM", "ADV", "APPR", "PIAT", "TRUNC", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kein Mensch dergleichen hat im Deutschen Land\u2019 er-", "tokens": ["Kein", "Mensch", "derg\u00b7lei\u00b7chen", "hat", "im", "Deut\u00b7schen", "Land'", "er"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "PIS", "VAFIN", "APPRART", "ADJA", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Da solche M\u00e4nge blieb/ zum Ende war gebracht/", "tokens": ["Da", "sol\u00b7che", "M\u00e4n\u00b7ge", "blieb", "/", "zum", "En\u00b7de", "war", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVFIN", "$(", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Gieng alles Schweden-Heer mit der Cur-Sachsen Macht", "tokens": ["Gieng", "al\u00b7les", "Schwe\u00b7den\u00b7Heer", "mit", "der", "Cur\u00b7Sach\u00b7sen", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gerad auf Leipzig hin/ woselbst drey tausend Krieger", "tokens": ["Ge\u00b7rad", "auf", "Leip\u00b7zig", "hin", "/", "wo\u00b7selbst", "drey", "tau\u00b7send", "Krie\u00b7ger"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "PTKVZ", "$(", "ADV", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Sich hatten eingesetzt/ die der begl\u00fcckte Sieger/", "tokens": ["Sich", "hat\u00b7ten", "ein\u00b7ge\u00b7setzt", "/", "die", "der", "be\u00b7gl\u00fcck\u00b7te", "Sie\u00b7ger", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "VAFIN", "VVPP", "$(", "PRELS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Gustavus/ alsobald zur fr\u00fcen ", "tokens": ["Gus\u00b7ta\u00b7vus", "/", "al\u00b7so\u00b7bald", "zur", "fr\u00fcen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$(", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Mit Ernst ermahnen lie\u00df. Sie schlugen es nicht ab/", "tokens": ["Mit", "Ernst", "er\u00b7mah\u00b7nen", "lie\u00df", ".", "Sie", "schlu\u00b7gen", "es", "nicht", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "PTKNEG", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "und baten doch \u00fcm Zeit/ sich dessen zu berahten.", "tokens": ["und", "ba\u00b7ten", "doch", "\u00fcm", "Zeit", "/", "sich", "des\u00b7sen", "zu", "be\u00b7rah\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPRART", "NN", "$(", "PRF", "PDS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der K\u00f6nig gieng es ein. Da\u00df gleichwol ohne Thaten", "tokens": ["Der", "K\u00f6\u00b7nig", "gieng", "es", "ein", ".", "Da\u00df", "gleich\u00b7wol", "oh\u00b7ne", "Tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKVZ", "$.", "KOUS", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Die Zeit nicht \u00fcbergieng/ lie\u00df er Cur-Sach sens Schaar", "tokens": ["Die", "Zeit", "nicht", "\u00fc\u00b7berg\u00b7ieng", "/", "lie\u00df", "er", "Cur\u00b7Sach", "sens", "Schaar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKNEG", "VVFIN", "$(", "VVFIN", "PPER", "NE", "ADV", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Vor Leipzig haltend stehn/ bi\u00df da\u00df es \u00fcber war/", "tokens": ["Vor", "Leip\u00b7zig", "hal\u00b7tend", "stehn", "/", "bi\u00df", "da\u00df", "es", "\u00fc\u00b7ber", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJD", "VVINF", "$(", "APPR", "KOUS", "PPER", "APPR", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "und gieng mit seiner Macht dem Feinde nachzusetzen/", "tokens": ["und", "gieng", "mit", "sei\u00b7ner", "Macht", "dem", "Fein\u00b7de", "nach\u00b7zu\u00b7set\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Der rund \u00fcm M\u00f6rseburg/ die Scharten au\u00dfzuwetzen/", "tokens": ["Der", "rund", "\u00fcm", "M\u00f6r\u00b7se\u00b7burg", "/", "die", "Schar\u00b7ten", "au\u00df\u00b7zu\u00b7wet\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPRART", "NN", "$(", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Den Mars zu Hilffe bat. Hier gieng es wieder an", "tokens": ["Den", "Mars", "zu", "Hilf\u00b7fe", "bat", ".", "Hier", "gieng", "es", "wie\u00b7der", "an"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$.", "ADV", "VVFIN", "PPER", "ADV", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Da\u00df von des K\u00e4ysers Volck bey dreymal tausend Mann", "tokens": ["Da\u00df", "von", "des", "K\u00e4y\u00b7sers", "Volck", "bey", "drey\u00b7mal", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "NN", "APPR", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Zu Pferd in Band und Schwerdt der tapfren Schweden", "tokens": ["Zu", "Pferd", "in", "Band", "und", "Schwerdt", "der", "tapf\u00b7ren", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "NN", "KON", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.18": {"text": "fielen/", "tokens": ["fie\u00b7len", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.19": {"text": "Hier sah man abermahl nach Gut im Blute w\u00fchlen/", "tokens": ["Hier", "sah", "man", "a\u00b7ber\u00b7mahl", "nach", "Gut", "im", "Blu\u00b7te", "w\u00fch\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPR", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Wovon manch armes Blut ein reichliches geno\u00df.", "tokens": ["Wo\u00b7von", "manch", "ar\u00b7mes", "Blut", "ein", "reich\u00b7li\u00b7ches", "ge\u00b7no\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "ADJA", "NN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Nach diesem galt es Hall und dessen festes Schlo\u00df/", "tokens": ["Nach", "die\u00b7sem", "galt", "es", "Hall", "und", "des\u00b7sen", "fes\u00b7tes", "Schlo\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "NE", "KON", "PRELAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Die Stadt ergab sich gern/ das Schlo\u00df hielt hart dargegen/", "tokens": ["Die", "Stadt", "er\u00b7gab", "sich", "gern", "/", "das", "Schlo\u00df", "hielt", "hart", "dar\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "$(", "ART", "NN", "VVFIN", "ADJD", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Must\u2019 aber endlich auch die Waffen niederlegen", "tokens": ["Must'", "a\u00b7ber", "end\u00b7lich", "auch", "die", "Waf\u00b7fen", "nie\u00b7der\u00b7le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "ADV", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "und gut Gustavisch seyn/ weil kein Entsatz nicht war.", "tokens": ["und", "gut", "Gus\u00b7ta\u00b7visch", "seyn", "/", "weil", "kein", "Ent\u00b7satz", "nicht", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJD", "VAINF", "$(", "KOUS", "PIAT", "NN", "PTKNEG", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Hierzwischen wurd es auch mit Leipzig alles klar/", "tokens": ["Hier\u00b7zwi\u00b7schen", "wurd", "es", "auch", "mit", "Leip\u00b7zig", "al\u00b7les", "klar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "APPR", "NE", "PIS", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "und muste sich der Feind allda mit Gut und Leben/", "tokens": ["und", "mus\u00b7te", "sich", "der", "Feind", "all\u00b7da", "mit", "Gut", "und", "Le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PRF", "ART", "NN", "ADV", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "So starck und reich er war/ den S\u00e4chsischen ergeben.", "tokens": ["So", "starck", "und", "reich", "er", "war", "/", "den", "S\u00e4ch\u00b7si\u00b7schen", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "PPER", "VAFIN", "$(", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Drey tausend war er starck/ drey Tonnen Goldes reich.", "tokens": ["Drey", "tau\u00b7send", "war", "er", "starck", "/", "drey", "Ton\u00b7nen", "Gol\u00b7des", "reich", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "VAFIN", "PPER", "ADJD", "$(", "CARD", "NN", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Di\u00df war nun abermal dem Feind ein gro\u00dfer Streich/", "tokens": ["Di\u00df", "war", "nun", "a\u00b7ber\u00b7mal", "dem", "Feind", "ein", "gro\u00b7\u00dfer", "Streich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "ART", "NN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Wor\u00fcber Stadt und Land in gro\u00dfen Freuden stunden/", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "Stadt", "und", "Land", "in", "gro\u00b7\u00dfen", "Freu\u00b7den", "stun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "NN", "KON", "NN", "APPR", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Dann beyde waren nun von einem Joch entbunden/", "tokens": ["Dann", "bey\u00b7de", "wa\u00b7ren", "nun", "von", "ei\u00b7nem", "Joch", "ent\u00b7bun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VAFIN", "ADV", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Das unertr\u00e4glich war. Weil GOtt ein Danck gef\u00e4llt/", "tokens": ["Das", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", "war", ".", "Weil", "Gott", "ein", "Danck", "ge\u00b7f\u00e4llt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "VAFIN", "$.", "KOUS", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "So wurd\u2019 auch \u00fcberall ein Danckfest angest\u00e4llt/", "tokens": ["So", "wurd'", "auch", "\u00fc\u00b7be\u00b7rall", "ein", "Dan\u00b7ck\u00b7fest", "an\u00b7ge\u00b7st\u00e4llt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.34": {"text": "F\u00fcr solchen gro\u00dfen Sieg dem HERren Dauck zu sagen/", "tokens": ["F\u00fcr", "sol\u00b7chen", "gro\u00b7\u00dfen", "Sieg", "dem", "HeR\u00b7ren", "Dauck", "zu", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "ART", "NN", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Der durch ein kleines Volck kan gro\u00dfe Hauffen schlagen.", "tokens": ["Der", "durch", "ein", "klei\u00b7nes", "Volck", "kan", "gro\u00b7\u00dfe", "Hauf\u00b7fen", "schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "VMFIN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Indessen hatte sich auch Erfurt in die Hand", "tokens": ["In\u00b7des\u00b7sen", "hat\u00b7te", "sich", "auch", "Er\u00b7furt", "in", "die", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PRF", "ADV", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.37": {"text": "Der Schweden eingesetzt/ worauf das Franckenland", "tokens": ["Der", "Schwe\u00b7den", "ein\u00b7ge\u00b7setzt", "/", "wo\u00b7rauf", "das", "Fran\u00b7cken\u00b7land"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NE", "VVPP", "$(", "PWAV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "und K\u00f6nigshofen erst/ ein Platz von gro\u00dfer St\u00e4rcke/", "tokens": ["und", "K\u00f6\u00b7nigs\u00b7ho\u00b7fen", "erst", "/", "ein", "Platz", "von", "gro\u00b7\u00dfer", "St\u00e4r\u00b7cke", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "$(", "ART", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Sehr schnell bemeistert wurd\u2019. Es halffen krine Wercke", "tokens": ["Sehr", "schnell", "be\u00b7meis\u00b7tert", "wurd'", ".", "Es", "half\u00b7fen", "kri\u00b7ne", "Wer\u00b7cke"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "VAFIN", "$.", "PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Die auf dem blatten Land und hohen Bergen sind/", "tokens": ["Die", "auf", "dem", "blat\u00b7ten", "Land", "und", "ho\u00b7hen", "Ber\u00b7gen", "sind", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "KON", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Der Schweden gro\u00dfe Flut/ die durch gew\u00fcndschtem Wind", "tokens": ["Der", "Schwe\u00b7den", "gro\u00b7\u00dfe", "Flut", "/", "die", "durch", "ge\u00b7w\u00fcnd\u00b7schtem", "Wind"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NE", "ADJA", "NN", "$(", "ART", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "Sehr schnell getrieben kam/ zu d\u00e4mmen und zu halten/", "tokens": ["Sehr", "schnell", "ge\u00b7trie\u00b7ben", "kam", "/", "zu", "d\u00e4m\u00b7men", "und", "zu", "hal\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "VVFIN", "$(", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Es war kein Hertz so hei\u00df/ es muste da erkalten", "tokens": ["Es", "war", "kein", "Hertz", "so", "hei\u00df", "/", "es", "mus\u00b7te", "da", "er\u00b7kal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "ADJD", "$(", "PPER", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Wann es die Schweden sah. Was hatte Fugger nicht/", "tokens": ["Wann", "es", "die", "Schwe\u00b7den", "sah", ".", "Was", "hat\u00b7te", "Fug\u00b7ger", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NE", "VVFIN", "$.", "PWS", "VAFIN", "NN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Was Altring/ andre mehr/ f\u00fcr Hauffen aufgericht/", "tokens": ["Was", "A\u00b7ltring", "/", "and\u00b7re", "mehr", "/", "f\u00fcr", "Hauf\u00b7fen", "auf\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$(", "PIS", "ADV", "$(", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Das Reich vor \u00e4rgern Sto\u00df der Schweden zu best\u00fctzen/", "tokens": ["Das", "Reich", "vor", "\u00e4r\u00b7gern", "Sto\u00df", "der", "Schwe\u00b7den", "zu", "be\u00b7st\u00fct\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.47": {"text": "Umbsonst/ sie drungen durch mit donndern und mit blitzen.", "tokens": ["Um\u00b7bsonst", "/", "sie", "drun\u00b7gen", "durch", "mit", "donn\u00b7dern", "und", "mit", "blit\u00b7zen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PPER", "PAV", "APPR", "APPR", "NN", "KON", "APPR", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.48": {"text": "Da\u00df alles forchtsam wurd\u2019 und mancher schon entlieff", "tokens": ["Da\u00df", "al\u00b7les", "forcht\u00b7sam", "wurd'", "und", "man\u00b7cher", "schon", "ent\u00b7lieff"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADJD", "VAFIN", "KON", "PIS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Eh man jhm recht mit Ernst nach seiner Hauben griff.", "tokens": ["Eh", "man", "jhm", "recht", "mit", "Ernst", "nach", "sei\u00b7ner", "Hau\u00b7ben", "griff", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "APPR", "NE", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "Damit kam Schweinfurt auch zu des Gustavus H\u00e4nden/", "tokens": ["Da\u00b7mit", "kam", "Schwein\u00b7furt", "auch", "zu", "des", "Gus\u00b7ta\u00b7vus", "H\u00e4n\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "ADV", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "Der es als einen Pa\u00df am M\u00e4yn an allen Enden", "tokens": ["Der", "es", "als", "ei\u00b7nen", "Pa\u00df", "am", "M\u00e4yn", "an", "al\u00b7len", "En\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "KOUS", "ART", "NN", "APPRART", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.52": {"text": "Sehr starck verschantzen lie\u00df/ und dann nach W\u00fcrtzburg", "tokens": ["Sehr", "starck", "ver\u00b7schant\u00b7zen", "lie\u00df", "/", "und", "dann", "nach", "W\u00fcrtz\u00b7burg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVINF", "VVFIN", "$(", "KON", "ADV", "APPR", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.53": {"text": "gieng/", "tokens": ["gieng", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "-", "measure": "single.down"}, "line.54": {"text": "Woselbst der Raht aus Forcht in Demuth Jhn empfieng", "tokens": ["Wo\u00b7selbst", "der", "Raht", "aus", "Forcht", "in", "De\u00b7muth", "Jhn", "emp\u00b7fi\u00b7eng"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR", "NN", "APPR", "NN", "PPER", "VVFIN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.55": {"text": "und Jhm die Schl\u00fcssel gab. Was aber in dem Schlosse", "tokens": ["und", "Jhm", "die", "Schl\u00fcs\u00b7sel", "gab", ".", "Was", "a\u00b7ber", "in", "dem", "Schlos\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ART", "NN", "VVFIN", "$.", "PWS", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Zu der Besatzung war/ das wehrte sich und schosse", "tokens": ["Zu", "der", "Be\u00b7sat\u00b7zung", "war", "/", "das", "wehr\u00b7te", "sich", "und", "schos\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VAFIN", "$(", "PDS", "VVFIN", "PRF", "KON", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Sehr grausam nach der Stadt. Di\u00df trieb den K\u00f6nig an", "tokens": ["Sehr", "grau\u00b7sam", "nach", "der", "Stadt", ".", "Di\u00df", "trieb", "den", "K\u00f6\u00b7nig", "an"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "$.", "PDS", "VVFIN", "ART", "NN", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Das b\u00f6se Nachbar-Hau\u00df mit etlich tausend Mann", "tokens": ["Das", "b\u00f6\u00b7se", "Nach\u00b7ba\u00b7rHau\u00df", "mit", "et\u00b7lich", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "und vielem Feuerwerck auf guten Sinn zu bringen/", "tokens": ["und", "vie\u00b7lem", "Feu\u00b7er\u00b7werck", "auf", "gu\u00b7ten", "Sinn", "zu", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "NN", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Er lie\u00df auch alsobald das gantze Schlo\u00df beringen", "tokens": ["Er", "lie\u00df", "auch", "al\u00b7so\u00b7bald", "das", "gant\u00b7ze", "Schlo\u00df", "be\u00b7rin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "und gro\u00dfe St\u00fcrme thun/ die Pforten sprungen auf/", "tokens": ["und", "gro\u00b7\u00dfe", "St\u00fcr\u00b7me", "thun", "/", "die", "Pfor\u00b7ten", "sprun\u00b7gen", "auf", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVINF", "$(", "ART", "NN", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.62": {"text": "Da mit so gieng es an/ da fiel der helle Hauf", "tokens": ["Da", "mit", "so", "gieng", "es", "an", "/", "da", "fiel", "der", "hel\u00b7le", "Hauf"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADV", "VVFIN", "PPER", "PTKVZ", "$(", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "Mit w\u00fcrgen auf das Schlo\u00df/ da musten Hirt- und Her-", "tokens": ["Mit", "w\u00fcr\u00b7gen", "auf", "das", "Schlo\u00df", "/", "da", "mus\u00b7ten", "Hir\u00b7t", "und", "Her"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "VVFIN", "APPR", "ART", "NN", "$(", "ADV", "VMFIN", "TRUNC", "KON", "TRUNC"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.64": {"text": "Weil keine Gnade war/ zu todten K\u00f6rpern werden/", "tokens": ["Weil", "kei\u00b7ne", "Gna\u00b7de", "war", "/", "zu", "tod\u00b7ten", "K\u00f6r\u00b7pern", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "VAFIN", "$(", "APPR", "ADJA", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Worauf das gantze Schlo\u00df zn pl\u00fcndern wurd\u2019 erlaubt/", "tokens": ["Wo\u00b7rauf", "das", "gant\u00b7ze", "Schlo\u00df", "zn", "pl\u00fcn\u00b7dern", "wurd'", "er\u00b7laubt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "NE", "VVINF", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.66": {"text": "Was es f\u00fcr Beuthen gab wird leichtlich nicht geglaubt.", "tokens": ["Was", "es", "f\u00fcr", "Beu\u00b7then", "gab", "wird", "leicht\u00b7lich", "nicht", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "NN", "VVFIN", "VAFIN", "ADJD", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.67": {"text": "Da haben sie das Geld mit H\u00fcten zu gemessen/", "tokens": ["Da", "ha\u00b7ben", "sie", "das", "Geld", "mit", "H\u00fc\u00b7ten", "zu", "ge\u00b7mes\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.68": {"text": "Damit wurd alle M\u00fch und Arbeit gantz vergessen.", "tokens": ["Da\u00b7mit", "wurd", "al\u00b7le", "M\u00fch", "und", "Ar\u00b7beit", "gantz", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PIAT", "NN", "KON", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "Di\u00df gl\u00fccklich au\u00dfgericht/ besatzte man den Ort/", "tokens": ["Di\u00df", "gl\u00fcck\u00b7lich", "au\u00df\u00b7ge\u00b7richt", "/", "be\u00b7satz\u00b7te", "man", "den", "Ort", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "VVPP", "$(", "VVFIN", "PIS", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "und fuhr/ noch andern Sieg zu haben/ weiter sort.", "tokens": ["und", "fuhr", "/", "noch", "an\u00b7dern", "Sieg", "zu", "ha\u00b7ben", "/", "wei\u00b7ter", "sort", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "ADV", "ADJA", "NN", "PTKZU", "VAINF", "$(", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "Ein Theil verf\u00fcgte sich nach Wertheim an dem Meyne/", "tokens": ["Ein", "Theil", "ver\u00b7f\u00fcg\u00b7te", "sich", "nach", "Wert\u00b7heim", "an", "dem", "Mey\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Ein Theil nach Mildenberg/ von seinem milden Weine", "tokens": ["Ein", "Theil", "nach", "Mil\u00b7den\u00b7berg", "/", "von", "sei\u00b7nem", "mil\u00b7den", "Wei\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE", "$(", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Vielleichte mild genannt/ und schlugen alles todt", "tokens": ["Viel\u00b7leich\u00b7te", "mild", "ge\u00b7nannt", "/", "und", "schlu\u00b7gen", "al\u00b7les", "todt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "VVPP", "$(", "KON", "VVFIN", "PIS", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.74": {"text": "Was in den Waffen war. So gar hatt\u2019 alle Noth", "tokens": ["Was", "in", "den", "Waf\u00b7fen", "war", ".", "So", "gar", "hatt'", "al\u00b7le", "Noth"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "APPR", "ART", "NN", "VAFIN", "$.", "ADV", "ADV", "VAFIN", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "Aus sonderbarer Rach die K\u00e4ysrischen \u00fcmgeben/", "tokens": ["Aus", "son\u00b7der\u00b7ba\u00b7rer", "Rach", "die", "K\u00e4y\u00b7sri\u00b7schen", "\u00fcm\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.76": {"text": "Da\u00df ihr ergrimmter Feind/ der Schwed/ ihr Blut und Le-", "tokens": ["Da\u00df", "ihr", "er\u00b7grimm\u00b7ter", "Feind", "/", "der", "Schwed", "/", "ihr", "Blut", "und", "Le"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "$(", "ART", "NN", "$(", "PPOSAT", "NN", "KON", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Wie keines Menschen hielt. Des K\u00e4ysers Volck vernahm", "tokens": ["Wie", "kei\u00b7nes", "Men\u00b7schen", "hielt", ".", "Des", "K\u00e4y\u00b7sers", "Volck", "ver\u00b7nahm"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PIAT", "NN", "VVFIN", "$.", "ART", "NN", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.78": {"text": "Da\u00df Wertheim schlecht besetzt von Schweden w\u00e4r\u2019/ und", "tokens": ["Da\u00df", "Wert\u00b7heim", "schlecht", "be\u00b7setzt", "von", "Schwe\u00b7den", "w\u00e4r'", "/", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["KOUS", "NN", "ADJD", "VVPP", "APPR", "NE", "VAFIN", "$(", "KON"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.79": {"text": "kam", "tokens": ["kam"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "-", "measure": "single.down"}, "line.80": {"text": "Fast in drey tausend starck de\u00dfwegen angezogen/", "tokens": ["Fast", "in", "drey", "tau\u00b7send", "starck", "de\u00df\u00b7we\u00b7gen", "an\u00b7ge\u00b7zo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "CARD", "CARD", "NN", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Auf sie Versuch zuthun/ wurd\u2019 aber sehr betrogen.", "tokens": ["Auf", "sie", "Ver\u00b7such", "zu\u00b7thun", "/", "wurd'", "a\u00b7ber", "sehr", "be\u00b7tro\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "NN", "VVINF", "$(", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.82": {"text": "Der Anschlag wurd entdeckt/ dem Wertheim Hilf gethan/", "tokens": ["Der", "An\u00b7schlag", "wurd", "ent\u00b7deckt", "/", "dem", "Wert\u00b7heim", "Hilf", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$(", "ART", "NN", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "und in den B\u00fcschen rumb ein etlich tausend Mann", "tokens": ["und", "in", "den", "B\u00fc\u00b7schen", "rumb", "ein", "et\u00b7lich", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ART", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "Sehr heimlich eingesetzt. Ein Ort zu \u00fcberraschen", "tokens": ["Sehr", "heim\u00b7lich", "ein\u00b7ge\u00b7setzt", ".", "Ein", "Ort", "zu", "\u00fc\u00b7berr\u00b7a\u00b7schen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "$.", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Darf Leuthe/ welche nicht aus ihren Schulen waschen/", "tokens": ["Darf", "Leu\u00b7the", "/", "wel\u00b7che", "nicht", "aus", "ih\u00b7ren", "Schu\u00b7len", "wa\u00b7schen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "$(", "PRELS", "PTKNEG", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "Sonst ists verlohren Ding. Wie nun des K\u00e4ysers Schar", "tokens": ["Sonst", "ists", "ver\u00b7loh\u00b7ren", "Ding", ".", "Wie", "nun", "des", "K\u00e4y\u00b7sers", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "VVPP", "NN", "$.", "PWAV", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.87": {"text": "Auf Wertheim unbesorgt in vollem Zuge war/", "tokens": ["Auf", "Wert\u00b7heim", "un\u00b7be\u00b7sorgt", "in", "vol\u00b7lem", "Zu\u00b7ge", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "APPR", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "Da kam von dort und da ein Schwarm auf sie geflogen/", "tokens": ["Da", "kam", "von", "dort", "und", "da", "ein", "Schwarm", "auf", "sie", "ge\u00b7flo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "ADV", "KON", "ADV", "ART", "NN", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Da\u00df sie gantz blutig hat den k\u00fcrtzeren gezogen.", "tokens": ["Da\u00df", "sie", "gantz", "blu\u00b7tig", "hat", "den", "k\u00fcrt\u00b7ze\u00b7ren", "ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "ART", "ADJA", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Zwey hundert blieben todt/ das meiste kam zur Flucht/", "tokens": ["Zwey", "hun\u00b7dert", "blie\u00b7ben", "todt", "/", "das", "meis\u00b7te", "kam", "zur", "Flucht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "VVFIN", "ADJD", "$(", "ART", "ADJA", "VVFIN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "Wurd\u2019 aber hin und her vom Sieger auf gesucht/", "tokens": ["Wurd'", "a\u00b7ber", "hin", "und", "her", "vom", "Sie\u00b7ger", "auf", "ge\u00b7sucht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKVZ", "KON", "ADV", "APPRART", "NN", "APPR", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "So/ da\u00df nicht viel entkam. Es sind auch zweymal sieben", "tokens": ["So", "/", "da\u00df", "nicht", "viel", "ent\u00b7kam", ".", "Es", "sind", "auch", "zwey\u00b7mal", "sie\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$(", "KOUS", "PTKNEG", "ADV", "VVFIN", "$.", "PPER", "VAFIN", "ADV", "ADV", "CARD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.93": {"text": "Von Fahnen und dabey ein gro\u00dfes Gut geblieben/", "tokens": ["Von", "Fah\u00b7nen", "und", "da\u00b7bey", "ein", "gro\u00b7\u00dfes", "Gut", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "PAV", "ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.94": {"text": "Das gute Beuthen gab. Es nahm auch diese Zeit", "tokens": ["Das", "gu\u00b7te", "Beu\u00b7then", "gab", ".", "Es", "nahm", "auch", "die\u00b7se", "Zeit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Herr Houbald Obrister/ mit List und Tapferkeit", "tokens": ["Herr", "Hou\u00b7bald", "O\u00b7bris\u00b7ter", "/", "mit", "List", "und", "Tap\u00b7fer\u00b7keit"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "NE", "NN", "$(", "APPR", "NN", "KON", "NN"], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.96": {"text": "Die Vestung Hanau ein/ ein Ort sehr schwer zu kriegen/", "tokens": ["Die", "Ves\u00b7tung", "Ha\u00b7nau", "ein", "/", "ein", "Ort", "sehr", "schwer", "zu", "krie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ART", "$(", "ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "Er aber kunte sie so wunder-schnell besiegen.", "tokens": ["Er", "a\u00b7ber", "kun\u00b7te", "sie", "so", "wun\u00b7der\u00b7schnell", "be\u00b7sie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VMFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Eh man von jhm vernahm/ da war er schon davor/", "tokens": ["Eh", "man", "von", "jhm", "ver\u00b7nahm", "/", "da", "war", "er", "schon", "da\u00b7vor", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PPER", "VVFIN", "$(", "ADV", "VAFIN", "PPER", "ADV", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.99": {"text": "und eh ein Schu\u00df geschah/ da war er schon im Thor.", "tokens": ["und", "eh", "ein", "Schu\u00df", "ge\u00b7schah", "/", "da", "war", "er", "schon", "im", "Thor", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "VVFIN", "$(", "ADV", "VAFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Aus welcher Vestung er die angelegnen Oerter", "tokens": ["Aus", "wel\u00b7cher", "Ves\u00b7tung", "er", "die", "an\u00b7ge\u00b7leg\u00b7nen", "O\u00b7er\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--", "measure": "unknown.measure.hexa"}, "line.101": {"text": "Cur-M\u00e4yntz/ die Wetterau und mehr durch scharffe W\u00f6r-", "tokens": ["Cur\u00b7M\u00e4yntz", "/", "die", "Wet\u00b7te\u00b7rau", "und", "mehr", "durch", "scharf\u00b7fe", "W\u00f6r"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "ART", "NN", "KON", "ADV", "APPR", "ADJA", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.102": {"text": "Zur hohen Schatzung trieb. Es kam auch dieser Zeit", "tokens": ["Zur", "ho\u00b7hen", "Schat\u00b7zung", "trieb", ".", "Es", "kam", "auch", "die\u00b7ser", "Zeit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "Nicht weit von Rotenburg zu einem scharffen Streit/", "tokens": ["Nicht", "weit", "von", "Ro\u00b7ten\u00b7burg", "zu", "ei\u00b7nem", "scharf\u00b7fen", "Streit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "APPR", "NE", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "In dem die K\u00e4ysrischen ein tausend Mann verlohren.", "tokens": ["In", "dem", "die", "K\u00e4y\u00b7sri\u00b7schen", "ein", "tau\u00b7send", "Mann", "ver\u00b7loh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "ART", "CARD", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "Di\u00df kam zu Mergentheim den andern kaum zu Ohren/", "tokens": ["Di\u00df", "kam", "zu", "Mer\u00b7gen\u00b7theim", "den", "an\u00b7dern", "kaum", "zu", "Oh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NE", "ART", "ADJA", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Verlie\u00dfen sie die Stadt eh da\u00df sie jemand trieb.", "tokens": ["Ver\u00b7lie\u00b7\u00dfen", "sie", "die", "Stadt", "eh", "da\u00df", "sie", "je\u00b7mand", "trieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "KOUS", "KOUS", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Di\u00df war den Schwedischen gewi\u00df nicht wenig lieh/", "tokens": ["Di\u00df", "war", "den", "Schwe\u00b7di\u00b7schen", "ge\u00b7wi\u00df", "nicht", "we\u00b7nig", "lieh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "ADV", "PTKNEG", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "Da\u00df sich so mancher Platz/ ohn\u2019 alles widerstehen/", "tokens": ["Da\u00df", "sich", "so", "man\u00b7cher", "Platz", "/", "ohn'", "al\u00b7les", "wi\u00b7der\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ADV", "PIAT", "NN", "$(", "APPR", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Dann also war es auch mit Ascheburg geschehen/", "tokens": ["Dann", "al\u00b7so", "war", "es", "auch", "mit", "A\u00b7sche\u00b7burg", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ADV", "APPR", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "In jhre H\u00e4nde gab. Bey dieser feinen Stadt/", "tokens": ["In", "jhre", "H\u00e4n\u00b7de", "gab", ".", "Bey", "die\u00b7ser", "fei\u00b7nen", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "APPR", "PDAT", "ADJA", "NN", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.111": {"text": "Die bald das sch\u00f6nste Schlo\u00df im gantzen Deutschland hat/", "tokens": ["Die", "bald", "das", "sch\u00f6ns\u00b7te", "Schlo\u00df", "im", "gant\u00b7zen", "Deutschland", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "ADJA", "NN", "APPRART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.112": {"text": "Zog der begl\u00fcckte Schwed mit zwantzig tausend Streitern/", "tokens": ["Zog", "der", "be\u00b7gl\u00fcck\u00b7te", "Schwed", "mit", "zwant\u00b7zig", "tau\u00b7send", "Strei\u00b7tern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "Dem Kern von seinem Heer/ theils Fu\u00dfvolck und theils Rei-", "tokens": ["Dem", "Kern", "von", "sei\u00b7nem", "Heer", "/", "theils", "Fu\u00df\u00b7volck", "und", "theils", "Rei"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$(", "ADV", "NN", "KON", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Der h\u00f6ltzern Br\u00fccken zu/ und gab sich \u00fcbern M\u00e4yn/", "tokens": ["Der", "h\u00f6lt\u00b7zern", "Br\u00fc\u00b7cken", "zu", "/", "und", "gab", "sich", "\u00fc\u00b7bern", "M\u00e4yn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "$(", "KON", "VVFIN", "PRF", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "Kam auch nicht lang darauf zu Sachsenhausen ein/", "tokens": ["Kam", "auch", "nicht", "lang", "da\u00b7rauf", "zu", "Sach\u00b7sen\u00b7hau\u00b7sen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PTKNEG", "ADJD", "PAV", "APPR", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Das Franckfurth \u00f6ffnen lie\u00df. Was wolt\u2019 es widerstehen/", "tokens": ["Das", "Fran\u00b7ck\u00b7furth", "\u00f6ff\u00b7nen", "lie\u00df", ".", "Was", "wolt'", "es", "wi\u00b7der\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VVFIN", "$.", "PWS", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.117": {"text": "Da war vor solche Macht kein Widerstand zu sehen", "tokens": ["Da", "war", "vor", "sol\u00b7che", "Macht", "kein", "Wi\u00b7der\u00b7stand", "zu", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPR", "PIAT", "NN", "PIAT", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "Der jhr gewachsen war. Es gieng auch selber mit", "tokens": ["Der", "jhr", "ge\u00b7wach\u00b7sen", "war", ".", "Es", "gieng", "auch", "sel\u00b7ber", "mit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "VVPP", "VAFIN", "$.", "PPER", "VVFIN", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.119": {"text": "Dem K\u00f6nig an die Hand/ lie\u00df freyen Ritt und Schritt", "tokens": ["Dem", "K\u00f6\u00b7nig", "an", "die", "Hand", "/", "lie\u00df", "frey\u00b7en", "Ritt", "und", "Schritt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$(", "VVFIN", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Durch seine Pforten hin/ und nahm 600. Schweden", "tokens": ["Durch", "sei\u00b7ne", "Pfor\u00b7ten", "hin", "/", "und", "nahm", "600.", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "ordinal", "word"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$(", "KON", "VVFIN", "ADJA", "NN"], "meter": "++-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.121": {"text": "In Sachsenhausen ein. Von allem viel zu reden", "tokens": ["In", "Sach\u00b7sen\u00b7hau\u00b7sen", "ein", ".", "Von", "al\u00b7lem", "viel", "zu", "re\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKVZ", "$.", "APPR", "PIS", "PIS", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Bed\u00f6rffte lange Zeit. Als Franckfurt Schwedisch war", "tokens": ["Be\u00b7d\u00f6rff\u00b7te", "lan\u00b7ge", "Zeit", ".", "Als", "Fran\u00b7ck\u00b7furt", "Schwe\u00b7disch", "war"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "ADJA", "NN", "$.", "KOUS", "NE", "NN", "VAFIN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.123": {"text": "Stundt H\u00f6chst/ die nechste Stadt an Franckfurt/ in Ge-", "tokens": ["Stundt", "H\u00f6chst", "/", "die", "nechs\u00b7te", "Stadt", "an", "Fran\u00b7ck\u00b7furt", "/", "in", "Ge"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["FM.la", "FM.la", "$(", "ART", "ADJA", "NN", "APPR", "NE", "$(", "APPR", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.124": {"text": "Der K\u00f6nig lie\u00df es auch den Abend noch beschiessen/", "tokens": ["Der", "K\u00f6\u00b7nig", "lie\u00df", "es", "auch", "den", "A\u00b7bend", "noch", "be\u00b7schies\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "ART", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "und haben sich daselbst dreyhundert geben m\u00fcssen/", "tokens": ["und", "ha\u00b7ben", "sich", "da\u00b7selbst", "drey\u00b7hun\u00b7dert", "ge\u00b7ben", "m\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "PAV", "CARD", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.126": {"text": "Den Schweden Dienst zu thun. Damit so war der M\u00e4yn", "tokens": ["Den", "Schwe\u00b7den", "Dienst", "zu", "thun", ".", "Da\u00b7mit", "so", "war", "der", "M\u00e4yn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$.", "PAV", "ADV", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "In Schwedischer Gewalt. Nun gieng es an den Reyhn.", "tokens": ["In", "Schwe\u00b7di\u00b7scher", "Ge\u00b7walt", ".", "Nun", "gieng", "es", "an", "den", "Reyhn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "So bald es mit dem H\u00f6chst am M\u00e4yne war geschehen", "tokens": ["So", "bald", "es", "mit", "dem", "H\u00f6chst", "am", "M\u00e4y\u00b7ne", "war", "ge\u00b7sche\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "PPER", "APPR", "ART", "NN", "APPRART", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Lie\u00df man das m\u00fcde Volck ein wenig ruhig gehen/", "tokens": ["Lie\u00df", "man", "das", "m\u00fc\u00b7de", "Volck", "ein", "we\u00b7nig", "ru\u00b7hig", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "ART", "PIS", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Das langst dem M\u00e4yn und Reyhn sich fr\u00f6lich nieder gab/", "tokens": ["Das", "langst", "dem", "M\u00e4yn", "und", "Reyhn", "sich", "fr\u00f6\u00b7lich", "nie\u00b7der", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ART", "NN", "KON", "NN", "PRF", "ADJD", "PTKVZ", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "Wo Wein und Brod und Vieh/ Gut/ Geld und ander", "tokens": ["Wo", "Wein", "und", "Brod", "und", "Vieh", "/", "Gut", "/", "Geld", "und", "an\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "NN", "KON", "NN", "KON", "NN", "$(", "ADJD", "$(", "NN", "KON", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.132": {"text": "Haab/", "tokens": ["Ha\u00b7ab", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "-+", "measure": "iambic.single"}, "line.133": {"text": "Was solchem Volcke dient/ nicht weit zu holen waren/", "tokens": ["Was", "sol\u00b7chem", "Vol\u00b7cke", "dient", "/", "nicht", "weit", "zu", "ho\u00b7len", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVFIN", "$(", "PTKNEG", "ADJD", "PTKZU", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.134": {"text": "und hatten sich darzu sehr wenig zu befahren.", "tokens": ["und", "hat\u00b7ten", "sich", "dar\u00b7zu", "sehr", "we\u00b7nig", "zu", "be\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "PAV", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Der Feind war aus dem Feld/ in St\u00e4dten war wol was/", "tokens": ["Der", "Feind", "war", "aus", "dem", "Feld", "/", "in", "St\u00e4d\u00b7ten", "war", "wol", "was", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "ART", "NN", "$(", "APPR", "NN", "VAFIN", "ADV", "PWS", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.136": {"text": "War aber voller Forcht. So kam auch \u00fcber das", "tokens": ["War", "a\u00b7ber", "vol\u00b7ler", "Forcht", ".", "So", "kam", "auch", "\u00fc\u00b7ber", "das"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$.", "ADV", "VVFIN", "ADV", "APPR", "ART"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "Mit zw\u00f6lffmal\u2019 tausend Mann der tapfre F\u00fcrst von Cassel", "tokens": ["Mit", "zw\u00f6lff\u00b7mal'", "tau\u00b7send", "Mann", "der", "tapf\u00b7re", "F\u00fcrst", "von", "Cas\u00b7sel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "CARD", "NN", "ART", "ADJA", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.138": {"text": "Dem gro\u00dfen K\u00f6nig zu/ der mit dem St\u00fcck-Geprassel", "tokens": ["Dem", "gro\u00b7\u00dfen", "K\u00f6\u00b7nig", "zu", "/", "der", "mit", "dem", "St\u00fc\u00b7ck\u00b7Ge\u00b7pras\u00b7sel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "$(", "ART", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.139": {"text": "und andren Ehren mehr jhn gro\u00df willkommen hie\u00df/", "tokens": ["und", "an\u00b7dren", "Eh\u00b7ren", "mehr", "jhn", "gro\u00df", "will\u00b7kom\u00b7men", "hie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "PPER", "ADJD", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "Und sich mit jhm sehr froh in Franckfurt sehen lie\u00df.", "tokens": ["Und", "sich", "mit", "jhm", "sehr", "froh", "in", "Fran\u00b7ck\u00b7furt", "se\u00b7hen", "lie\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "PPER", "ADV", "ADJD", "APPR", "NE", "VVINF", "VVFIN", "$."], "meter": "--+--+-+-+--+", "measure": "anapaest.di.plus"}, "line.141": {"text": "Indessen gieng ein Theil auf tausend Spanniarten/", "tokens": ["In\u00b7des\u00b7sen", "gieng", "ein", "Theil", "auf", "tau\u00b7send", "Span\u00b7ni\u00b7ar\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "Die Wallof/ einen Pa\u00df/ dem Ringgau zu/ bewahrten.", "tokens": ["Die", "Wal\u00b7lof", "/", "ei\u00b7nen", "Pa\u00df", "/", "dem", "Ring\u00b7gau", "zu", "/", "be\u00b7wahr\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "NN", "$(", "ART", "NN", "PTKZU", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Es kam zu einem Stret/ in dem der Schweden Hieb", "tokens": ["Es", "kam", "zu", "ei\u00b7nem", "Stret", "/", "in", "dem", "der", "Schwe\u00b7den", "Hieb"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$(", "APPR", "PRELS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "Den Feind so trefflich traff/ da\u00df fast die Helffte blieb.", "tokens": ["Den", "Feind", "so", "treff\u00b7lich", "traff", "/", "da\u00df", "fast", "die", "Helff\u00b7te", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "VVFIN", "$(", "KOUS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "Wie nun die Schweden sich allhier so lustig hielten/", "tokens": ["Wie", "nun", "die", "Schwe\u00b7den", "sich", "all\u00b7hier", "so", "lus\u00b7tig", "hiel\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NE", "PRF", "ADV", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.146": {"text": "und \u00fcber jhren Feind fast t\u00e4glich Meister spielten/", "tokens": ["und", "\u00fc\u00b7ber", "jhren", "Feind", "fast", "t\u00e4g\u00b7lich", "Meis\u00b7ter", "spiel\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "ADV", "ADJD", "NN", "VVFIN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.147": {"text": "Gieng Tylli gro\u00df von Macht und Grimm auf N\u00fcrnberg", "tokens": ["Gieng", "Tyl\u00b7li", "gro\u00df", "von", "Macht", "und", "Grimm", "auf", "N\u00fcrn\u00b7berg"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "ADJD", "APPR", "NN", "KON", "NE", "APPR", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.148": {"text": "hin/", "tokens": ["hin", "/"], "token_info": ["word", "punct"], "pos": ["ADV", "$("], "meter": "+", "measure": "single.up"}, "line.149": {"text": "und schlo\u00df die sch\u00f6ne Stadt/ da\u00df keines was darin", "tokens": ["und", "schlo\u00df", "die", "sch\u00f6\u00b7ne", "Stadt", "/", "da\u00df", "kei\u00b7nes", "was", "da\u00b7rin"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$(", "KOUS", "PIS", "PWS", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Heraus/ und was heraus hinein nicht kunte kommen.", "tokens": ["He\u00b7raus", "/", "und", "was", "he\u00b7raus", "hin\u00b7ein", "nicht", "kun\u00b7te", "kom\u00b7men", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KON", "PWS", "ADV", "ADV", "PTKNEG", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.151": {"text": "Der K\u00f6nig hatte das durch Boten kaum vernommen", "tokens": ["Der", "K\u00f6\u00b7nig", "hat\u00b7te", "das", "durch", "Bo\u00b7ten", "kaum", "ver\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ART", "APPR", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "Entschlo\u00df er Hilf zu thun/ und lie\u00df das Hessen-Heer", "tokens": ["Ent\u00b7schlo\u00df", "er", "Hilf", "zu", "thun", "/", "und", "lie\u00df", "das", "Hes\u00b7sen\u00b7Heer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "NE", "PTKZU", "VVINF", "$(", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Auf das vor N\u00fcrnberg lo\u00df/ er eylte selbst auch sehr", "tokens": ["Auf", "das", "vor", "N\u00fcrn\u00b7berg", "lo\u00df", "/", "er", "eyl\u00b7te", "selbst", "auch", "sehr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NE", "PTKVZ", "$(", "PPER", "VVFIN", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Mit seiner Macht hinnach/ den Feind hinweg zu treiben/", "tokens": ["Mit", "sei\u00b7ner", "Macht", "hin\u00b7nach", "/", "den", "Feind", "hin\u00b7weg", "zu", "trei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "$(", "ART", "NN", "APZR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Graff Tylli aber wolt\u2019 ihm also lang nicht bleiben/", "tokens": ["Graff", "Tyl\u00b7li", "a\u00b7ber", "wolt'", "ihm", "al\u00b7so", "lang", "nicht", "blei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "VMFIN", "PPER", "ADV", "ADJD", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.156": {"text": "Gieng durch/ verlie\u00df die Stadt. Sie hielt jhn auch nicht", "tokens": ["Gieng", "durch", "/", "ver\u00b7lie\u00df", "die", "Stadt", ".", "Sie", "hielt", "jhn", "auch", "nicht"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "$(", "VVFIN", "ART", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.157": {"text": "gro\u00df/", "tokens": ["gro\u00df", "/"], "token_info": ["word", "punct"], "pos": ["ADJD", "$("], "meter": "-", "measure": "single.down"}, "line.158": {"text": "Dann solcher b\u00f6sen G\u00e4st ist jeder gerne lo\u00df/", "tokens": ["Dann", "sol\u00b7cher", "b\u00f6\u00b7sen", "G\u00e4st", "ist", "je\u00b7der", "ger\u00b7ne", "lo\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "ADJA", "NN", "VAFIN", "PIS", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "Man kan sie nimmermehr erf\u00fcllen oder stillen.", "tokens": ["Man", "kan", "sie", "nim\u00b7mer\u00b7mehr", "er\u00b7f\u00fcl\u00b7len", "o\u00b7der", "stil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "ADV", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Hierauf ver\u00e4nderte der K\u00f6nig seinen Willen/", "tokens": ["Hier\u00b7auf", "ver\u00b7\u00e4n\u00b7der\u00b7te", "der", "K\u00f6\u00b7nig", "sei\u00b7nen", "Wil\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.161": {"text": "Und machte sich vor M\u00e4yntz/ nahm aber/ eh der Reyhn", "tokens": ["Und", "mach\u00b7te", "sich", "vor", "M\u00e4yntz", "/", "nahm", "a\u00b7ber", "/", "eh", "der", "Reyhn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "$(", "VVFIN", "ADV", "$(", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Von jhm wurd\u2019 angetast/ die gantze Bergstra\u00df ein/", "tokens": ["Von", "jhm", "wurd'", "an\u00b7ge\u00b7tast", "/", "die", "gant\u00b7ze", "Berg\u00b7stra\u00df", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "VVPP", "$(", "ART", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "Wo sonder Widerstand jhm alles offen stundte/", "tokens": ["Wo", "son\u00b7der", "Wi\u00b7der\u00b7stand", "jhm", "al\u00b7les", "of\u00b7fen", "stund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "PPER", "PIS", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "Wo er auch alles voll von Wein und Fr\u00fcchten fundte.", "tokens": ["Wo", "er", "auch", "al\u00b7les", "voll", "von", "Wein", "und", "Fr\u00fcch\u00b7ten", "fund\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PIS", "ADJD", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.165": {"text": "Die Bergstra\u00df ist ein Land da Milch und H\u00f6nig fl\u00fc\u00dft/", "tokens": ["Die", "Berg\u00b7stra\u00df", "ist", "ein", "Land", "da", "Milch", "und", "H\u00f6\u00b7nig", "fl\u00fc\u00dft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "ADV", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "Das voll von Wein und Frucht und sch\u00f6nen Pl\u00e4tzen ist.", "tokens": ["Das", "voll", "von", "Wein", "und", "Frucht", "und", "sch\u00f6\u00b7nen", "Pl\u00e4t\u00b7zen", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "APPR", "NN", "KON", "NN", "KON", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "In dem der tapfre Schwed sich an dem Reyhnstrohm machte/", "tokens": ["In", "dem", "der", "tapf\u00b7re", "Schwed", "sich", "an", "dem", "Reyhns\u00b7trohm", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "ADJA", "NN", "PRF", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.168": {"text": "Geschah es/ da\u00df der He\u00df fich in den Ringgau brachte/", "tokens": ["Ge\u00b7schah", "es", "/", "da\u00df", "der", "He\u00df", "fich", "in", "den", "Ring\u00b7gau", "brach\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$(", "KOUS", "ART", "NN", "ADJD", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.169": {"text": "Nahm auch dit Wetterau/ de\u00dfgleichen Friedberg ein/", "tokens": ["Nahm", "auch", "dit", "Wet\u00b7te\u00b7rau", "/", "de\u00df\u00b7glei\u00b7chen", "Fried\u00b7berg", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "$(", "ADV", "NE", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Woselbst viel Spannische verw\u00e4llt gewesen seyn.", "tokens": ["Wo\u00b7selbst", "viel", "Span\u00b7ni\u00b7sche", "ver\u00b7w\u00e4llt", "ge\u00b7we\u00b7sen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVPP", "VAPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Nach dem di\u00df alles war ins K\u00f6nigs Hand gekommen", "tokens": ["Nach", "dem", "di\u00df", "al\u00b7les", "war", "ins", "K\u00f6\u00b7nigs", "Hand", "ge\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "PDS", "PIS", "VAFIN", "APPRART", "NN", "NN", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Wurd\u2019 unverweilt ein Zug recht \u00fcber Reyhn genommen.", "tokens": ["Wurd'", "un\u00b7ver\u00b7weilt", "ein", "Zug", "recht", "\u00fc\u00b7ber", "Reyhn", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "ART", "NN", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.173": {"text": "So bald der Vortrab sich bey Stockstadt \u00fcbergab", "tokens": ["So", "bald", "der", "Vor\u00b7trab", "sich", "bey", "Stock\u00b7stadt", "\u00fc\u00b7berg\u00b7ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NN", "PRF", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.174": {"text": "und jhn der Feind ersah/ da hielt er solchen ab", "tokens": ["und", "jhn", "der", "Feind", "er\u00b7sah", "/", "da", "hielt", "er", "sol\u00b7chen", "ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ART", "NN", "VVFIN", "$(", "ADV", "VVFIN", "PPER", "PIAT", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "So viel jhm m\u00f6glich war/ es kam zu gro\u00dfem Streite/", "tokens": ["So", "viel", "jhm", "m\u00f6g\u00b7lich", "war", "/", "es", "kam", "zu", "gro\u00b7\u00dfem", "Strei\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADJD", "VAFIN", "$(", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.176": {"text": "Da\u00df also dieser Held von Schweden seine Leuthe", "tokens": ["Da\u00df", "al\u00b7so", "die\u00b7ser", "Held", "von", "Schwe\u00b7den", "sei\u00b7ne", "Leu\u00b7the"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PDAT", "NN", "APPR", "NE", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "In gro\u00dfen N\u00f6then sah/ dieweil des Feindes Schar", "tokens": ["In", "gro\u00b7\u00dfen", "N\u00f6\u00b7then", "sah", "/", "die\u00b7weil", "des", "Fein\u00b7des", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$(", "KOUS", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.178": {"text": "Fast dreymal m\u00e4chtiger/ darzu in Vortheln war/", "tokens": ["Fast", "drey\u00b7mal", "m\u00e4ch\u00b7ti\u00b7ger", "/", "dar\u00b7zu", "in", "Vor\u00b7theln", "war", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJA", "$(", "PAV", "APPR", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Worob des K\u00f6nigs Hertz fast schmertzlich sich betr\u00fcbte/", "tokens": ["Wo\u00b7rob", "des", "K\u00f6\u00b7nigs", "Hertz", "fast", "schmertz\u00b7lich", "sich", "be\u00b7tr\u00fcb\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "NN", "ADV", "ADJD", "PRF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "Dann es die Seinige wie seine Glieder liebte.", "tokens": ["Dann", "es", "die", "Sei\u00b7ni\u00b7ge", "wie", "sei\u00b7ne", "Glie\u00b7der", "lieb\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "PPOSS", "KOKOM", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "Noch gleichwol wehrte sich der Vortrab wie ein L\u00f6u/", "tokens": ["Noch", "gleich\u00b7wol", "wehr\u00b7te", "sich", "der", "Vor\u00b7trab", "wie", "ein", "L\u00f6u", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "ART", "NN", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Bi\u00df da\u00df sein Nachdruck kam/ der jhn von N\u00f6hten frey/", "tokens": ["Bi\u00df", "da\u00df", "sein", "Nach\u00b7druck", "kam", "/", "der", "jhn", "von", "N\u00f6h\u00b7ten", "frey", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PPOSAT", "NN", "VVFIN", "$(", "PRELS", "PPER", "APPR", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.183": {"text": "Hergegen seinen Feind voll Forcht und Schrecken machte/", "tokens": ["Her\u00b7ge\u00b7gen", "sei\u00b7nen", "Feind", "voll", "Forcht", "und", "Schre\u00b7cken", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "ADJD", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Darzu auch also bald zum schn\u00f6den fliehen brachte.", "tokens": ["Dar\u00b7zu", "auch", "al\u00b7so", "bald", "zum", "schn\u00f6\u00b7den", "flie\u00b7hen", "brach\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "ADV", "ADV", "APPRART", "ADJA", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.185": {"text": "Di\u00df gl\u00fccklich au\u00dfgericht/ wurd\u2019 in dem gantzen Heer", "tokens": ["Di\u00df", "gl\u00fcck\u00b7lich", "au\u00df\u00b7ge\u00b7richt", "/", "wurd'", "in", "dem", "gant\u00b7zen", "Heer"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "ADJD", "VVPP", "$(", "VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "Ein Beten angeh\u00f6rt/ da\u00df GOtt zu seiner Ehr\u2019", "tokens": ["Ein", "Be\u00b7ten", "an\u00b7ge\u00b7h\u00f6rt", "/", "da\u00df", "Gott", "zu", "sei\u00b7ner", "Ehr'"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "$(", "KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "und seines Kirchleins Trost ein mehrers wolle geben/", "tokens": ["und", "sei\u00b7nes", "Kirch\u00b7leins", "Trost", "ein", "meh\u00b7rers", "wol\u00b7le", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "ART", "PIS", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Worauf man alles sah sich \u00fcbern Reyhn erheben.", "tokens": ["Wo\u00b7rauf", "man", "al\u00b7les", "sah", "sich", "\u00fc\u00b7bern", "Reyhn", "er\u00b7he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "PIS", "VVFIN", "PRF", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "So bald die gantze Macht den Reyhn hin\u00fcber war/", "tokens": ["So", "bald", "die", "gant\u00b7ze", "Macht", "den", "Reyhn", "hin\u00b7\u00fc\u00b7ber", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "ART", "NN", "ADV", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "Stundt\u2019 Oppenheim/ die Stadt/ in eu\u00dferster Gefahr/", "tokens": ["Stundt'", "Op\u00b7pen\u00b7heim", "/", "die", "Stadt", "/", "in", "eu\u00b7\u00dfers\u00b7ter", "Ge\u00b7fahr", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$(", "ART", "NN", "$(", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.191": {"text": "Kam auch mit Sturm an sie/ das Schlo\u00df hierbey inglei-", "tokens": ["Kam", "auch", "mit", "Sturm", "an", "sie", "/", "das", "Schlo\u00df", "hier\u00b7bey", "in\u00b7glei"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "ADV", "APPR", "NN", "APPR", "PPER", "$(", "ART", "NN", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Di\u00df zwung die Lottringer aus Worms hinweg zu weichen/", "tokens": ["Di\u00df", "zwung", "die", "Lott\u00b7rin\u00b7ger", "aus", "Worms", "hin\u00b7weg", "zu", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "NN", "APZR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "und sich nach Franckenthal zu geben. Di\u00df gethan/", "tokens": ["und", "sich", "nach", "Fran\u00b7cken\u00b7thal", "zu", "ge\u00b7ben", ".", "Di\u00df", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "NE", "PTKZU", "VVINF", "$.", "PDS", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Kam alles rund \u00fcm M\u00e4yntz/ die Stadt zu schl\u00fc\u00dfen/ an/", "tokens": ["Kam", "al\u00b7les", "rund", "\u00fcm", "M\u00e4yntz", "/", "die", "Stadt", "zu", "schl\u00fc\u00b7\u00dfen", "/", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "PIS", "ADJD", "APPRART", "NN", "$(", "ART", "NN", "PTKZU", "VVINF", "$(", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Worauf auch alsobald die groben St\u00fccke br\u00fcllten/", "tokens": ["Wo\u00b7rauf", "auch", "al\u00b7so\u00b7bald", "die", "gro\u00b7ben", "St\u00fc\u00b7cke", "br\u00fcll\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "und alles in der Stadt mit Forcht und Schrecken f\u00fcllten.", "tokens": ["und", "al\u00b7les", "in", "der", "Stadt", "mit", "Forcht", "und", "Schre\u00b7cken", "f\u00fcll\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "ART", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Was solte solche thun/ da war kein andrer Raht/", "tokens": ["Was", "sol\u00b7te", "sol\u00b7che", "thun", "/", "da", "war", "kein", "an\u00b7drer", "Raht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PIS", "VVINF", "$(", "ADV", "VAFIN", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.198": {"text": "Weil kein Entsatz erschien/ als Schwedische Genad.", "tokens": ["Weil", "kein", "Ent\u00b7satz", "er\u00b7schien", "/", "als", "Schwe\u00b7di\u00b7sche", "Ge\u00b7nad", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "VVFIN", "$(", "KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Es kam auch anders nicht/ sie muste sich ergeben/", "tokens": ["Es", "kam", "auch", "an\u00b7ders", "nicht", "/", "sie", "mus\u00b7te", "sich", "er\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "$(", "PPER", "VMFIN", "PRF", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Weil bey so gro\u00dfem Ernst und Sturm ihr Gut und Leben", "tokens": ["Weil", "bey", "so", "gro\u00b7\u00dfem", "Ernst", "und", "Sturm", "ihr", "Gut", "und", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ADV", "ADJA", "NN", "KON", "NN", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.201": {"text": "In gro\u00dfen N\u00f6then war. Der Spanjer must heraus/", "tokens": ["In", "gro\u00b7\u00dfen", "N\u00f6\u00b7then", "war", ".", "Der", "Span\u00b7jer", "must", "he\u00b7raus", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.202": {"text": "und der begl\u00fcckte Schwed hergegen in das Hau\u00df.", "tokens": ["und", "der", "be\u00b7gl\u00fcck\u00b7te", "Schwed", "her\u00b7ge\u00b7gen", "in", "das", "Hau\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Als M\u00e4yntz erobert war/ da war es auch mit Bingen", "tokens": ["Als", "M\u00e4yntz", "er\u00b7o\u00b7bert", "war", "/", "da", "war", "es", "auch", "mit", "Bin\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "VVPP", "VAFIN", "$(", "ADV", "VAFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "und K\u00f6nigstein gethan/ ein Ort nicht bald zu zwingen.", "tokens": ["und", "K\u00f6\u00b7nig\u00b7stein", "ge\u00b7than", "/", "ein", "Ort", "nicht", "bald", "zu", "zwin\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "$(", "ART", "NN", "PTKNEG", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "Wir sehen aber wol/ wann uns das Hertze wund/", "tokens": ["Wir", "se\u00b7hen", "a\u00b7ber", "wol", "/", "wann", "uns", "das", "Hert\u00b7ze", "wund", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$(", "PWAV", "PPER", "PDS", "VVFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "So ist der gantze Leib geschw\u00e4cht und ungesund.", "tokens": ["So", "ist", "der", "gant\u00b7ze", "Leib", "ge\u00b7schw\u00e4cht", "und", "un\u00b7ge\u00b7sund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "VVPP", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Hier folgten andre mehr/ als Baccherach am Reyhne/", "tokens": ["Hier", "folg\u00b7ten", "and\u00b7re", "mehr", "/", "als", "Bac\u00b7che\u00b7rach", "am", "Reyh\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "$(", "KOUS", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "Hier wachsen/ d\u00fcncket mich/ die allerbesten Weine.", "tokens": ["Hier", "wach\u00b7sen", "/", "d\u00fcn\u00b7cket", "mich", "/", "die", "al\u00b7ler\u00b7bes\u00b7ten", "Wei\u00b7ne", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "$(", "VVFIN", "PPER", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Es hat auch Baccherach den Nahmen vom Altar", "tokens": ["Es", "hat", "auch", "Bac\u00b7che\u00b7rach", "den", "Nah\u00b7men", "vom", "Al\u00b7tar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "NE", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "Des Bachus/ wie man sagt. Ich wei\u00df nicht/ ob es wahr/", "tokens": ["Des", "Ba\u00b7chus", "/", "wie", "man", "sagt", ".", "Ich", "wei\u00df", "nicht", "/", "ob", "es", "wahr", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$(", "PWAV", "PIS", "VVFIN", "$.", "PPER", "VVFIN", "PTKNEG", "$(", "KOUS", "PPER", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.211": {"text": "Das wei\u00df ich aber wol/ was hier f\u00fcr Wein zu finden.", "tokens": ["Das", "wei\u00df", "ich", "a\u00b7ber", "wol", "/", "was", "hier", "f\u00fcr", "Wein", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "$(", "PWS", "ADV", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "O edler Trauben-Safft voll Krafft uns zu entz\u00fcnden/", "tokens": ["O", "ed\u00b7ler", "Trau\u00b7ben\u00b7\u00b7S\u00b7afft", "voll", "Krafft", "uns", "zu", "ent\u00b7z\u00fcn\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "ADJD", "NN", "PPER", "PTKZU", "VVINF", "$("], "meter": "---+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.213": {"text": "Da\u00df wir ein gr\u00f6\u00dfers thun als unsre Krafft vermag!", "tokens": ["Da\u00df", "wir", "ein", "gr\u00f6\u00b7\u00dfers", "thun", "als", "uns\u00b7re", "Krafft", "ver\u00b7mag", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "VVINF", "KOKOM", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Hier folgte Boppert nach/ und was der Schweden Schlag", "tokens": ["Hier", "folg\u00b7te", "Bop\u00b7pert", "nach", "/", "und", "was", "der", "Schwe\u00b7den", "Schlag"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "APPR", "$(", "KON", "PWS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "Nicht gern erwarten wolt\u2019. Es ist f\u00fcrwar zu reden/", "tokens": ["Nicht", "gern", "er\u00b7war\u00b7ten", "wolt'", ".", "Es", "ist", "f\u00fcr\u00b7war", "zu", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "VVINF", "VMFIN", "$.", "PPER", "VAFIN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Da\u00df mancher solchen Ha\u00df und Grollen auf die Schweden", "tokens": ["Da\u00df", "man\u00b7cher", "sol\u00b7chen", "Ha\u00df", "und", "Grol\u00b7len", "auf", "die", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "PIAT", "NN", "KON", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Getragen/ da\u00df er nicht mocht harren fie zu sehn/", "tokens": ["Ge\u00b7tra\u00b7gen", "/", "da\u00df", "er", "nicht", "mocht", "har\u00b7ren", "fie", "zu", "sehn", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KOUS", "PPER", "PTKNEG", "ADJD", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "Wie in der ", "tokens": ["Wie", "in", "der"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "APPR", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.219": {"text": "Da\u00df man ein Ort verlief/ eh sich ein Schwed erzeigte/", "tokens": ["Da\u00df", "man", "ein", "Ort", "ver\u00b7lief", "/", "eh", "sich", "ein", "Schwed", "er\u00b7zeig\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "VVFIN", "$(", "KOUS", "PRF", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.220": {"text": "We\u00dfwegen mancher Platz sich zu den Schweden neigte.", "tokens": ["We\u00df\u00b7we\u00b7gen", "man\u00b7cher", "Platz", "sich", "zu", "den", "Schwe\u00b7den", "neig\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "PRF", "APPR", "ART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.221": {"text": "Es war anjetzt das Fest der Weyhnacht f\u00fcr der Th\u00fcr/", "tokens": ["Es", "war", "an\u00b7jetzt", "das", "Fest", "der", "Weyh\u00b7nacht", "f\u00fcr", "der", "Th\u00fcr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "So fiel auch sonsten viel von den Gesandten f\u00fcr/", "tokens": ["So", "fiel", "auch", "sons\u00b7ten", "viel", "von", "den", "Ge\u00b7sand\u00b7ten", "f\u00fcr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ADV", "APPR", "ART", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "Die Franckreichs/ Engelands/ Cur-Pfaltz/ der Herren", "tokens": ["Die", "Fran\u00b7ck\u00b7reichs", "/", "En\u00b7ge\u00b7lands", "/", "Cur\u00b7Pfaltz", "/", "der", "Her\u00b7ren"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$(", "NE", "$(", "NE", "$(", "ART", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.224": {"text": "Staten/", "tokens": ["Sta\u00b7ten", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.225": {"text": "Cur-C\u00f6llens/ N\u00fcrenbergs und andrer Werbung thaten/", "tokens": ["Cur\u00b7C\u00f6l\u00b7lens", "/", "N\u00fc\u00b7ren\u00b7bergs", "und", "an\u00b7drer", "Wer\u00b7bung", "tha\u00b7ten", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NE", "KON", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "Darum Gustavus sich zu M\u00e4yntz die hohe Zeit", "tokens": ["Da\u00b7rum", "Gus\u00b7ta\u00b7vus", "sich", "zu", "M\u00e4yntz", "die", "ho\u00b7he", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "NE", "PRF", "APPR", "NN", "ART", "ADJA", "NN"], "meter": "--+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.227": {"text": "Zu feyern/ und hernach den Fremden gut Bescheid", "tokens": ["Zu", "fey\u00b7ern", "/", "und", "her\u00b7nach", "den", "Frem\u00b7den", "gut", "Be\u00b7scheid"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$(", "KON", "ADV", "ART", "NN", "ADJD", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.228": {"text": "Zu geben/ was enthielt. Wir lassen jhn hierinnen", "tokens": ["Zu", "ge\u00b7ben", "/", "was", "ent\u00b7hielt", ".", "Wir", "las\u00b7sen", "jhn", "hie\u00b7rin\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "und fehen/ was sein Heer im Felde mag beginnen.", "tokens": ["und", "fe\u00b7hen", "/", "was", "sein", "Heer", "im", "Fel\u00b7de", "mag", "be\u00b7gin\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "PWS", "PPOSAT", "NN", "APPRART", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.230": {"text": "Der Reyhn-Graf machte sich zum festen Franckenthal/", "tokens": ["Der", "Rey\u00b7hn\u00b7Graf", "mach\u00b7te", "sich", "zum", "fes\u00b7ten", "Fran\u00b7cken\u00b7thal", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPRART", "ADJA", "NN", "$("], "meter": "-+-++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.231": {"text": "Er kam so bald nicht an/ siel eine gro\u00dfe Zahl", "tokens": ["Er", "kam", "so", "bald", "nicht", "an", "/", "siel", "ei\u00b7ne", "gro\u00b7\u00dfe", "Zahl"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "PTKVZ", "$(", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "Von solcher Vestung aus/ in Meynung jhn zu schlagen/", "tokens": ["Von", "sol\u00b7cher", "Ves\u00b7tung", "aus", "/", "in", "Mey\u00b7nung", "jhn", "zu", "schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PTKVZ", "$(", "APPR", "NN", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Wo nicht/ aufs wenigst jhn von solcher abzujagen.", "tokens": ["Wo", "nicht", "/", "aufs", "we\u00b7nigst", "jhn", "von", "sol\u00b7cher", "ab\u00b7zu\u00b7ja\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$(", "APPRART", "VVFIN", "PPER", "APPR", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "So gut jhr Anschlag war/ so \u00fcbel war jhr Gl\u00fcck.", "tokens": ["So", "gut", "jhr", "An\u00b7schlag", "war", "/", "so", "\u00fc\u00b7bel", "war", "jhr", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "NN", "VAFIN", "$(", "ADV", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Es traff sie alles selbst/ da\u00df sie sich schnell zu r\u00fcck", "tokens": ["Es", "traff", "sie", "al\u00b7les", "selbst", "/", "da\u00df", "sie", "sich", "schnell", "zu", "r\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PIS", "ADV", "$(", "KOUS", "PPER", "PRF", "ADJD", "PTKZU", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "und m\u00e4rcklich mit Verlust der jhren musten geben/", "tokens": ["und", "m\u00e4r\u00b7ck\u00b7lich", "mit", "Ver\u00b7lust", "der", "jhren", "mus\u00b7ten", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "NN", "ART", "VVINF", "VMFIN", "VVINF", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.237": {"text": "Er bracht\u2019 auch eine Hilf aus Spanien \u00fcms Leben.", "tokens": ["Er", "bracht'", "auch", "ei\u00b7ne", "Hilf", "aus", "Spa\u00b7ni\u00b7en", "\u00fcms", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "APPR", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "Di\u00df war noch unverschmertzt/ kam ein halb tausend Mann", "tokens": ["Di\u00df", "war", "noch", "un\u00b7ver\u00b7schmertzt", "/", "kam", "ein", "halb", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "$(", "VVFIN", "ART", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "Von Hertzog Bernhards Volck sehr still bey Mannheim an", "tokens": ["Von", "Hert\u00b7zog", "Bern\u00b7hards", "Volck", "sehr", "still", "bey", "Mann\u00b7heim", "an"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "NE", "NN", "ADV", "ADJD", "APPR", "NE", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "und t\u00f6dtete die Wacht/ die unbehutsam wachte/", "tokens": ["und", "t\u00f6d\u00b7te\u00b7te", "die", "Wacht", "/", "die", "un\u00b7be\u00b7hut\u00b7sam", "wach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$(", "ART", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Worauf es auch die Stadt in seine Klauen brachte/", "tokens": ["Wo\u00b7rauf", "es", "auch", "die", "Stadt", "in", "sei\u00b7ne", "Klau\u00b7en", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "und alles nieder hieb was in Besatzung war.", "tokens": ["und", "al\u00b7les", "nie\u00b7der", "hieb", "was", "in", "Be\u00b7sat\u00b7zung", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PTKVZ", "VVFIN", "PIS", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Di\u00df war kein schlechter Ort. Ein andre Schweden-Schar/", "tokens": ["Di\u00df", "war", "kein", "schlech\u00b7ter", "Ort", ".", "Ein", "and\u00b7re", "Schwe\u00b7den\u00b7Schar", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PIAT", "ADJA", "NN", "$.", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.244": {"text": "Die der Gustavus Horn/ Feld-Marschall/ kl\u00fcglich f\u00fchrte/", "tokens": ["Die", "der", "Gus\u00b7ta\u00b7vus", "Horn", "/", "Feld\u00b7Mar\u00b7schall", "/", "kl\u00fcg\u00b7lich", "f\u00fchr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ART", "NE", "NE", "$(", "NN", "$(", "ADJD", "VVFIN", "$("], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.245": {"text": "Bemeisterte Heilbron/ der kluge Krieger sp\u00fcrte", "tokens": ["Be\u00b7meis\u00b7ter\u00b7te", "Heil\u00b7bron", "/", "der", "klu\u00b7ge", "Krie\u00b7ger", "sp\u00fcr\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "NE", "$(", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.246": {"text": "Was diese sch\u00f6ne Stadt f\u00fcr ein bequ\u00e4mer Pa\u00df/", "tokens": ["Was", "die\u00b7se", "sch\u00f6\u00b7ne", "Stadt", "f\u00fcr", "ein", "be\u00b7qu\u00e4\u00b7mer", "Pa\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "We\u00dfwegen auch der Feind sehr starck zu Pferde sa\u00df/", "tokens": ["We\u00df\u00b7we\u00b7gen", "auch", "der", "Feind", "sehr", "starck", "zu", "Pfer\u00b7de", "sa\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "ADV", "ADJD", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "Dem Ort Entsatz zu thun/ kunt\u2019 aber nicht geschehen/", "tokens": ["Dem", "Ort", "Ent\u00b7satz", "zu", "thun", "/", "kunt'", "a\u00b7ber", "nicht", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "VMFIN", "ADV", "PTKNEG", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Der Ort must an den Horn/ der Feind zu r\u00fccke gehen.", "tokens": ["Der", "Ort", "must", "an", "den", "Horn", "/", "der", "Feind", "zu", "r\u00fc\u00b7cke", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "APPR", "ART", "NN", "$(", "ART", "NN", "PTKZU", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Nun wurd\u2019 auch Franckenthal und Heydelberg beschr\u00e4nckt/", "tokens": ["Nun", "wurd'", "auch", "Fran\u00b7cken\u00b7thal", "und", "Hey\u00b7del\u00b7berg", "be\u00b7schr\u00e4nckt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "NN", "KON", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.251": {"text": "Auchma ncher K\u00e4yfrischer durchs Bauer-Volck gekr\u00e4nckt", "tokens": ["Auch\u00b7ma", "ncher", "K\u00e4yfr\u00b7i\u00b7scher", "durchs", "Bau\u00b7e\u00b7rVolck", "ge\u00b7kr\u00e4nckt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJA", "NN", "APPRART", "NN", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "und \u00fcm den Hal\u00df gebracht/ weil solches solche Plagen", "tokens": ["und", "\u00fcm", "den", "Hal\u00df", "ge\u00b7bracht", "/", "weil", "sol\u00b7ches", "sol\u00b7che", "Pla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "ART", "NN", "VVPP", "$(", "KOUS", "PIS", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "ung grotze Tyranney nicht l\u00e4nger mocht\u2019 ertragen.", "tokens": ["ung", "grot\u00b7ze", "Ty\u00b7ran\u00b7ney", "nicht", "l\u00e4n\u00b7ger", "mocht'", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "PTKNEG", "ADJD", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Jetzt war der Schweden H\u00e4upt/ de\u00dfgleichen sein Gemahl", "tokens": ["Jetzt", "war", "der", "Schwe\u00b7den", "H\u00e4upt", "/", "de\u00df\u00b7glei\u00b7chen", "sein", "Ge\u00b7mahl"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "$(", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.255": {"text": "und K\u00f6nig Friederich mit einer gro\u00dfen Zahl", "tokens": ["und", "K\u00f6\u00b7nig", "Frie\u00b7de\u00b7rich", "mit", "ei\u00b7ner", "gro\u00b7\u00dfen", "Zahl"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "NE", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Von jhren Dienenden nach Franckfurt wiederkommen/", "tokens": ["Von", "jhren", "Die\u00b7nen\u00b7den", "nach", "Fran\u00b7ck\u00b7furt", "wie\u00b7der\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "NE", "VVINF", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.257": {"text": "und von der gantzen Stadt sehr herrlich aufgenommen.", "tokens": ["und", "von", "der", "gant\u00b7zen", "Stadt", "sehr", "herr\u00b7lich", "auf\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Verblieben auch daselbst zusammen eine Zeit/", "tokens": ["Ver\u00b7blie\u00b7ben", "auch", "da\u00b7selbst", "zu\u00b7sam\u00b7men", "ei\u00b7ne", "Zeit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PAV", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.259": {"text": "Bi\u00df da\u00df der Str\u00f6hme Gr\u00f6\u00df und wilde Strengigkeit", "tokens": ["Bi\u00df", "da\u00df", "der", "Str\u00f6h\u00b7me", "Gr\u00f6\u00df", "und", "wil\u00b7de", "Stren\u00b7gig\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "ART", "NN", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "Sich milder sehen lie\u00df/ beschlossen mittler weile/", "tokens": ["Sich", "mil\u00b7der", "se\u00b7hen", "lie\u00df", "/", "be\u00b7schlos\u00b7sen", "mitt\u00b7ler", "wei\u00b7le", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "VVINF", "VVFIN", "$(", "VVPP", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.261": {"text": "Was ma\u00dfen man den Krieg dem lang verjagten Theile", "tokens": ["Was", "ma\u00b7\u00dfen", "man", "den", "Krieg", "dem", "lang", "ver\u00b7jag\u00b7ten", "Thei\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PIS", "ART", "NN", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Zum b\u00e4sten f\u00fchren konnt\u2019. Es kam die frohe Zeit", "tokens": ["Zum", "b\u00e4s\u00b7ten", "f\u00fch\u00b7ren", "konnt'", ".", "Es", "kam", "die", "fro\u00b7he", "Zeit"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "VVINF", "VMFIN", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "Des Fr\u00fchlings an/ und da\u00df der St\u00f6hme Strengigkeit", "tokens": ["Des", "Fr\u00fch\u00b7lings", "an", "/", "und", "da\u00df", "der", "St\u00f6h\u00b7me", "Stren\u00b7gig\u00b7keit"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKVZ", "$(", "KON", "KOUS", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Durch heitre Lufft vergieng/ da brach der L\u00f6u von Schweden/", "tokens": ["Durch", "heit\u00b7re", "Lufft", "ver\u00b7gieng", "/", "da", "brach", "der", "L\u00f6u", "von", "Schwe\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$(", "ADV", "VVFIN", "ART", "NN", "APPR", "NE", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.265": {"text": "Der gro\u00dfe Held Gustav nach vielen unterreden", "tokens": ["Der", "gro\u00b7\u00dfe", "Held", "Gus\u00b7tav", "nach", "vie\u00b7len", "un\u00b7ter\u00b7re\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NE", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.266": {"text": "Von Fronckfurt wieder auf/ und zog nach Creutzennach/", "tokens": ["Von", "Fron\u00b7ck\u00b7furt", "wie\u00b7der", "auf", "/", "und", "zog", "nach", "Creut\u00b7zen\u00b7nach", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "APPR", "$(", "KON", "VVFIN", "APPR", "NE", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.267": {"text": "Wie er dann auch mit Sturm in seine Pforten brach/", "tokens": ["Wie", "er", "dann", "auch", "mit", "Sturm", "in", "sei\u00b7ne", "Pfor\u00b7ten", "brach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "ADV", "APPR", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "und alles w\u00fcrgen lie\u00df/ was man in Waffen fundte.", "tokens": ["und", "al\u00b7les", "w\u00fcr\u00b7gen", "lie\u00df", "/", "was", "man", "in", "Waf\u00b7fen", "fund\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVINF", "VVFIN", "$(", "PWS", "PIS", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.269": {"text": "Als aber dessen Schlo\u00df jhm tapfer widerstundte/", "tokens": ["Als", "a\u00b7ber", "des\u00b7sen", "Schlo\u00df", "jhm", "tap\u00b7fer", "wi\u00b7der\u00b7stund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PRELAT", "NN", "PPER", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Durch", "tokens": ["Durch"], "token_info": ["word"], "pos": ["APPR"], "meter": "+", "measure": "single.up"}, "line.271": {"text": "Worauf der Feind darin \u00fcm Gnade hat gerufft/", "tokens": ["Wo\u00b7rauf", "der", "Feind", "da\u00b7rin", "\u00fcm", "Gna\u00b7de", "hat", "ge\u00b7rufft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PAV", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Die jhm auch wiederfuhr. War also dieser Enden", "tokens": ["Die", "jhm", "auch", "wie\u00b7der\u00b7fuhr", ".", "War", "al\u00b7so", "die\u00b7ser", "En\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PPER", "ADV", "VVFIN", "$.", "VAFIN", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Fast alles/ St\u00e4dt\u2019 und Land/ den Schweden in den H\u00e4n-", "tokens": ["Fast", "al\u00b7les", "/", "St\u00e4dt'", "und", "Land", "/", "den", "Schwe\u00b7den", "in", "den", "H\u00e4n"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "PIS", "$(", "NN", "KON", "NN", "$(", "ART", "NE", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "Bi\u00df auf das Heydelberg/ die Vestung Franckenthal", "tokens": ["Bi\u00df", "auf", "das", "Hey\u00b7del\u00b7berg", "/", "die", "Ves\u00b7tung", "Fran\u00b7cken\u00b7thal"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "NN", "$(", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "und bi\u00df auf Philipsburg/ das ander allzumahl", "tokens": ["und", "bi\u00df", "auf", "Phi\u00b7lips\u00b7burg", "/", "das", "an\u00b7der", "all\u00b7zu\u00b7mahl"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "NE", "$(", "ART", "ADJD", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "War in der Schweden Macht. Wir lassen nun den K\u00f6nig", "tokens": ["War", "in", "der", "Schwe\u00b7den", "Macht", ".", "Wir", "las\u00b7sen", "nun", "den", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Zu Franckfurt in der Ruh/ woselbsten jhm nicht wenig", "tokens": ["Zu", "Fran\u00b7ck\u00b7furt", "in", "der", "Ruh", "/", "wo\u00b7selbs\u00b7ten", "jhm", "nicht", "we\u00b7nig"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "ART", "NN", "$(", "PWAV", "PPER", "PTKNEG", "ADV"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.278": {"text": "Zu Ehren wird gethan/ und gehn ins Sachsenland/", "tokens": ["Zu", "Eh\u00b7ren", "wird", "ge\u00b7than", "/", "und", "gehn", "ins", "Sach\u00b7sen\u00b7land", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "VVPP", "$(", "KON", "VVFIN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "Zu sehen/ was alda anjetzo f\u00fcr ein Stand/", "tokens": ["Zu", "se\u00b7hen", "/", "was", "al\u00b7da", "an\u00b7je\u00b7tzo", "f\u00fcr", "ein", "Stand", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "ADV", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "Ob es den K\u00e4ysrischen daselbst was b\u00e4sser gehet/", "tokens": ["Ob", "es", "den", "K\u00e4y\u00b7sri\u00b7schen", "da\u00b7selbst", "was", "b\u00e4s\u00b7ser", "ge\u00b7het", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PAV", "PWS", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Als es am Reyhn und M\u00e4yn anjetzt mit jhnen stehet?", "tokens": ["Als", "es", "am", "Reyhn", "und", "M\u00e4yn", "an\u00b7jetzt", "mit", "jh\u00b7nen", "ste\u00b7het", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "KON", "NN", "ADV", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Es sahe dieser Kr\u00e4y\u00df was die gethane Schlacht", "tokens": ["Es", "sa\u00b7he", "die\u00b7ser", "Kr\u00e4y\u00df", "was", "die", "ge\u00b7tha\u00b7ne", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "PWS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "F\u00fcr Forcht und Schrecken hatt\u2019 ins gantze Reich gebracht/", "tokens": ["F\u00fcr", "Forcht", "und", "Schre\u00b7cken", "hatt'", "ins", "gant\u00b7ze", "Reich", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VAFIN", "APPRART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Ja/ da\u00df sich niemand nun von solchem dorffte trauen", "tokens": ["Ja", "/", "da\u00df", "sich", "nie\u00b7mand", "nun", "von", "sol\u00b7chem", "dorff\u00b7te", "trau\u00b7en"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$(", "KOUS", "PRF", "PIS", "ADV", "APPR", "PIAT", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "In einer rechten Schlacht mit Schweden rum zu hauen.", "tokens": ["In", "ei\u00b7ner", "rech\u00b7ten", "Schlacht", "mit", "Schwe\u00b7den", "rum", "zu", "hau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "NE", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.286": {"text": "De\u00dfwegen er ein Heer von sechsmal tausend Mann", "tokens": ["De\u00df\u00b7we\u00b7gen", "er", "ein", "Heer", "von", "sechs\u00b7mal", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "ART", "NN", "APPR", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.287": {"text": "Zu Fu\u00df\u2019 und Pferde warb/ hielt auch beym K\u00f6nig an", "tokens": ["Zu", "Fu\u00df'", "und", "Pfer\u00b7de", "warb", "/", "hielt", "auch", "beym", "K\u00f6\u00b7nig", "an"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "VVFIN", "$(", "VVFIN", "ADV", "APPRART", "NN", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.288": {"text": "um einen guten Held demselben vorzusetzen/", "tokens": ["um", "ei\u00b7nen", "gu\u00b7ten", "Held", "dem\u00b7sel\u00b7ben", "vor\u00b7zu\u00b7set\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.289": {"text": "Der jhm den tapfren Todt/ des Feindes raub und sch\u00e4-", "tokens": ["Der", "jhm", "den", "tapf\u00b7ren", "Todt", "/", "des", "Fein\u00b7des", "raub", "und", "sch\u00e4"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "ART", "ADJA", "NN", "$(", "ART", "NN", "PTKVZ", "KON", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Zu t\u00f6dten/ \u00fcberlie\u00df. Er kam auch in das Feld/", "tokens": ["Zu", "t\u00f6d\u00b7ten", "/", "\u00fc\u00b7ber\u00b7lie\u00df", ".", "Er", "kam", "auch", "in", "das", "Feld", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Zu welchem sich der F\u00fcrst und hochber\u00fchmte Held", "tokens": ["Zu", "wel\u00b7chem", "sich", "der", "F\u00fcrst", "und", "hoch\u00b7be\u00b7r\u00fchm\u00b7te", "Held"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PRF", "ART", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Georg von L\u00fcneburg mit seinem Hauffen machte/", "tokens": ["Ge\u00b7org", "von", "L\u00fc\u00b7ne\u00b7burg", "mit", "sei\u00b7nem", "Hauf\u00b7fen", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Der ein f\u00fcnf tausend Mann allein zu Felde brachte.", "tokens": ["Der", "ein", "f\u00fcnf", "tau\u00b7send", "Mann", "al\u00b7lein", "zu", "Fel\u00b7de", "brach\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "CARD", "CARD", "NN", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Ein gro\u00dfes war es da\u00df der Brehmer Bischoff that/", "tokens": ["Ein", "gro\u00b7\u00dfes", "war", "es", "da\u00df", "der", "Breh\u00b7mer", "Bi\u00b7schoff", "that", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "PPER", "KOUS", "ART", "NN", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Der etlich tausend Mann f\u00fcr sich geworben hat/", "tokens": ["Der", "et\u00b7lich", "tau\u00b7send", "Mann", "f\u00fcr", "sich", "ge\u00b7wor\u00b7ben", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "CARD", "NN", "APPR", "PRF", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "Weil der von Gron\u00dffeld jhn von allen Seiten plagte/", "tokens": ["Weil", "der", "von", "Gron\u00df\u00b7feld", "jhn", "von", "al\u00b7len", "Sei\u00b7ten", "plag\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "APPR", "NN", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "Auch seine treue Leuth\u2019 aus Verden weg verjagte/", "tokens": ["Auch", "sei\u00b7ne", "treu\u00b7e", "Leuth'", "aus", "Ver\u00b7den", "weg", "ver\u00b7jag\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "APPR", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "So gab er sich zur Wehr. ", "tokens": ["So", "gab", "er", "sich", "zur", "Wehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.299": {"text": "Zog General Bannier mit gro\u00dfen Hauffen auff", "tokens": ["Zog", "Ge\u00b7ne\u00b7ral", "Ban\u00b7nier", "mit", "gro\u00b7\u00dfen", "Hauf\u00b7fen", "auff"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "NE", "NE", "APPR", "ADJA", "NN", "APPR"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.300": {"text": "Von eylfmal tausend Mann zu Fu\u00df und vielen Pferden/", "tokens": ["Von", "eylf\u00b7mal", "tau\u00b7send", "Mann", "zu", "Fu\u00df", "und", "vie\u00b7len", "Pfer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "APPR", "NN", "KON", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Des \u00f6den Magdeburgs ein andrer Herr zu werden.", "tokens": ["Des", "\u00f6\u00b7den", "Mag\u00b7de\u00b7burgs", "ein", "an\u00b7drer", "Herr", "zu", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.302": {"text": "Jetzt gab sich Wi\u00dfmar auch/ worauf dasselbe Heer/", "tokens": ["Jetzt", "gab", "sich", "Wi\u00df\u00b7mar", "auch", "/", "wo\u00b7rauf", "das\u00b7sel\u00b7be", "Heer", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "NE", "ADV", "$(", "PWAV", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.303": {"text": "Das es bel\u00e4gert hielt/ und keinen Feind nicht mehr", "tokens": ["Das", "es", "be\u00b7l\u00e4\u00b7gert", "hielt", "/", "und", "kei\u00b7nen", "Feind", "nicht", "mehr"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "PPER", "VVPP", "VVFIN", "$(", "KON", "PIAT", "NN", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "In Mecklenburg vernahm/ sich nach dem Brehmer machte/", "tokens": ["In", "Meck\u00b7len\u00b7burg", "ver\u00b7nahm", "/", "sich", "nach", "dem", "Breh\u00b7mer", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$(", "PRF", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.305": {"text": "Da\u00df also dieser Herr ein Heer zusammen brachte", "tokens": ["Da\u00df", "al\u00b7so", "die\u00b7ser", "Herr", "ein", "Heer", "zu\u00b7sam\u00b7men", "brach\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PDAT", "NN", "ART", "NN", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "Das sich acht tausend schrieb/ und Verden wieder nahm/", "tokens": ["Das", "sich", "acht", "tau\u00b7send", "schrieb", "/", "und", "Ver\u00b7den", "wie\u00b7der", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PRF", "CARD", "CARD", "VVFIN", "$(", "KON", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Bald aber auch darauf zu einem Streiche kam/", "tokens": ["Bald", "a\u00b7ber", "auch", "da\u00b7rauf", "zu", "ei\u00b7nem", "Strei\u00b7che", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "PAV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Den jhm des K\u00e4ysers Volck am Weserstrohm versetzte/", "tokens": ["Den", "jhm", "des", "K\u00e4y\u00b7sers", "Volck", "am", "We\u00b7ser\u00b7strohm", "ver\u00b7setz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Da\u00df man des Brehmer-Volcks drey hundert Geist-lo\u00df", "tokens": ["Da\u00df", "man", "des", "Breh\u00b7mer\u00b7Volcks", "drey", "hun\u00b7dert", "Geist\u00b7lo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "NN", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.310": {"text": "sch\u00e4tzte/", "tokens": ["sch\u00e4tz\u00b7te", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.311": {"text": "Jetzt gab sich D\u00f6mitz auch/ woraus 400. Mann", "tokens": ["Jetzt", "gab", "sich", "D\u00f6\u00b7mitz", "auch", "/", "wo\u00b7raus", "400.", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "ordinal", "word"], "pos": ["ADV", "VVFIN", "PRF", "NE", "ADV", "$(", "PWAV", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.312": {"text": "(zweyhundert machten sich zum Pappenheim hinan)", "tokens": ["(", "zwey\u00b7hun\u00b7dert", "mach\u00b7ten", "sich", "zum", "Pap\u00b7pen\u00b7heim", "hi\u00b7nan", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "CARD", "VVFIN", "PRF", "APPRART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "Mit jhren Waffen sind nach Mecklenburg gekommen/", "tokens": ["Mit", "jhren", "Waf\u00b7fen", "sind", "nach", "Meck\u00b7len\u00b7burg", "ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "APPR", "NE", "VVPP", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.314": {"text": "und haben auch daselbst des F\u00fcrsten Dienst genommen.", "tokens": ["und", "ha\u00b7ben", "auch", "da\u00b7selbst", "des", "F\u00fcrs\u00b7ten", "Dienst", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PAV", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Es kam auch Magdeburg nunmehr an den Bannier/", "tokens": ["Es", "kam", "auch", "Mag\u00b7de\u00b7burg", "nun\u00b7mehr", "an", "den", "Ban\u00b7nier", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NE", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.316": {"text": "Nach dem des K\u00e4ysers Volck mit einer List von hier", "tokens": ["Nach", "dem", "des", "K\u00e4y\u00b7sers", "Volck", "mit", "ei\u00b7ner", "List", "von", "hier"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "NN", "NN", "APPR", "ART", "NN", "APPR", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.317": {"text": "Sich hatte weg gemacht. Jetzt wurd\u2019 es gut befunden/", "tokens": ["Sich", "hat\u00b7te", "weg", "ge\u00b7macht", ".", "Jetzt", "wurd'", "es", "gut", "be\u00b7fun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "VAFIN", "ADV", "VVPP", "$.", "ADV", "VAFIN", "PPER", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.318": {"text": "Da\u00df man den schlauen Held/ den Mann von hundert Wun-", "tokens": ["Da\u00df", "man", "den", "schlau\u00b7en", "Held", "/", "den", "Mann", "von", "hun\u00b7dert", "Wun"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "ADJA", "NN", "$(", "ART", "NN", "APPR", "CARD", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "Vom Hause Pappenheim aus diesen Landen trieb/", "tokens": ["Vom", "Hau\u00b7se", "Pap\u00b7pen\u00b7heim", "aus", "die\u00b7sen", "Lan\u00b7den", "trieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "APPR", "PDAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.320": {"text": "Worauf der Held Bannier nach seiner Lincken hieb/", "tokens": ["Wo\u00b7rauf", "der", "Held", "Ban\u00b7nier", "nach", "sei\u00b7ner", "Lin\u00b7cken", "hieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "NE", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.321": {"text": "Der F\u00fcrst vou L\u00fcneburg zur Rechten/ beyde thaten/", "tokens": ["Der", "F\u00fcrst", "vou", "L\u00fc\u00b7ne\u00b7burg", "zur", "Rech\u00b7ten", "/", "bey\u00b7de", "tha\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "NE", "APPRART", "NN", "$(", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.322": {"text": "Was Helden zugeh\u00f6rt/ doch ohne wolgerahten", "tokens": ["Was", "Hel\u00b7den", "zu\u00b7ge\u00b7h\u00f6rt", "/", "doch", "oh\u00b7ne", "wol\u00b7ge\u00b7rah\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "NN", "VVPP", "$(", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.323": {"text": "Jhm eines anzuthun/ er gieng sehr schlau dahin/", "tokens": ["Jhm", "ei\u00b7nes", "an\u00b7zu\u00b7thun", "/", "er", "gieng", "sehr", "schlau", "da\u00b7hin", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "VVIZU", "$(", "PPER", "VVFIN", "ADV", "ADJD", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.324": {"text": "und brachte dieser Zug dem Lande schlecht Gewinn.", "tokens": ["und", "brach\u00b7te", "die\u00b7ser", "Zug", "dem", "Lan\u00b7de", "schlecht", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "NN", "ART", "NN", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.325": {"text": "Indessen machte sich ein ander Hauff an Verden/", "tokens": ["In\u00b7des\u00b7sen", "mach\u00b7te", "sich", "ein", "an\u00b7der", "Hauff", "an", "Ver\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.326": {"text": "und muste solcher Ort nun wieder K\u00e4ysrisch werden.", "tokens": ["und", "mus\u00b7te", "sol\u00b7cher", "Ort", "nun", "wie\u00b7der", "K\u00e4y\u00b7srisch", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIAT", "NN", "ADV", "ADV", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.327": {"text": "Sie tobten auch anjetzt im gantzen Stifft her\u00fcm/", "tokens": ["Sie", "tob\u00b7ten", "auch", "an\u00b7jetzt", "im", "gant\u00b7zen", "Stifft", "he\u00b7r\u00fcm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPRART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.328": {"text": "und war durchs gantze Land ein gro\u00dfes ", "tokens": ["und", "war", "durchs", "gant\u00b7ze", "Land", "ein", "gro\u00b7\u00dfes"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "APPRART", "ADJA", "NN", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.329": {"text": "Vom Pappenheim erregt/ der L\u00fcneburg bedr\u00f6ute/", "tokens": ["Vom", "Pap\u00b7pen\u00b7heim", "er\u00b7regt", "/", "der", "L\u00fc\u00b7ne\u00b7burg", "be\u00b7dr\u00f6u\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$(", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "und sich fast \u00fcberall am Weserstrohm au\u00dfstreute.", "tokens": ["und", "sich", "fast", "\u00fc\u00b7be\u00b7rall", "am", "We\u00b7ser\u00b7strohm", "au\u00df\u00b7streu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADV", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Bannier that was er kunt uud hielt jhn in dem Zaum/", "tokens": ["Ban\u00b7nier", "that", "was", "er", "kunt", "u\u00b7ud", "hielt", "jhn", "in", "dem", "Zaum", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PWS", "PPER", "PTKVZ", "KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.332": {"text": "Wie dann der Hessen Held jhm auch sehr wenig Raum", "tokens": ["Wie", "dann", "der", "Hes\u00b7sen", "Held", "jhm", "auch", "sehr", "we\u00b7nig", "Raum"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "NN", "PPER", "ADV", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.333": {"text": "In seinem R\u00fccken lie\u00df/ darzu noch einen Hauffen/", "tokens": ["In", "sei\u00b7nem", "R\u00fc\u00b7cken", "lie\u00df", "/", "dar\u00b7zu", "noch", "ei\u00b7nen", "Hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "PAV", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Der nach dem Pappenheim aus C\u00f6llen kam gelauffen/", "tokens": ["Der", "nach", "dem", "Pap\u00b7pen\u00b7heim", "aus", "C\u00f6l\u00b7len", "kam", "ge\u00b7lauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "APPR", "NE", "VVFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Jhm Hilf zu thun/ vertrieb. Er nahm auch Paderborn/", "tokens": ["Jhm", "Hilf", "zu", "thun", "/", "ver\u00b7trieb", ".", "Er", "nahm", "auch", "Pa\u00b7der\u00b7born", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "PTKZU", "VVINF", "$(", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.336": {"text": "Jtzt gieng auch G\u00f6ttingen/ die feste Stadt/ verlohrn/", "tokens": ["Jtzt", "gieng", "auch", "G\u00f6t\u00b7tin\u00b7gen", "/", "die", "fes\u00b7te", "Stadt", "/", "ver\u00b7lohrn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NN", "$(", "ART", "ADJA", "NN", "$(", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "und Go\u00dflar noch darzu/ die beyde Gut und Leben", "tokens": ["und", "Go\u00df\u00b7lar", "noch", "dar\u00b7zu", "/", "die", "bey\u00b7de", "Gut", "und", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "ADV", "PAV", "$(", "ART", "PIAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.338": {"text": "Zu retten/ sich an den von Weymar musten geben/", "tokens": ["Zu", "ret\u00b7ten", "/", "sich", "an", "den", "von", "Wey\u00b7mar", "mus\u00b7ten", "ge\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PRF", "APPR", "ART", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.339": {"text": "Der Hertzog Wilhelm hie\u00df/ und jetzt ein Feldherr war.", "tokens": ["Der", "Hert\u00b7zog", "Wil\u00b7helm", "hie\u00df", "/", "und", "jetzt", "ein", "Feld\u00b7herr", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "VVFIN", "$(", "KON", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "Mit welchem des Banniers begl\u00fcckte Krieges-Schar", "tokens": ["Mit", "wel\u00b7chem", "des", "Ban\u00b7niers", "be\u00b7gl\u00fcck\u00b7te", "Krie\u00b7ge\u00b7sSchar"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.341": {"text": "Zugleich zu Felde gieng. Genug von Nieder-Sachsen.", "tokens": ["Zu\u00b7gleich", "zu", "Fel\u00b7de", "gieng", ".", "Ge\u00b7nug", "von", "Nie\u00b7der\u00b7Sach\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVFIN", "$.", "ADV", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.342": {"text": "Was seh ich dort \u00fcm Prag f\u00fcr einen Streit erwachsen/", "tokens": ["Was", "seh", "ich", "dort", "\u00fcm", "Prag", "f\u00fcr", "ei\u00b7nen", "Streit", "er\u00b7wach\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPRART", "NN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.343": {"text": "Wir wollen auch hinzu. Als Leipzig \u00fcber war", "tokens": ["Wir", "wol\u00b7len", "auch", "hin\u00b7zu", ".", "Als", "Leip\u00b7zig", "\u00fc\u00b7ber", "war"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "PTKVZ", "$.", "KOUS", "NE", "APPR", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.344": {"text": "und Meissen Feindes lo\u00df/ gieng der Cur-Sachsen Schar", "tokens": ["und", "Meis\u00b7sen", "Fein\u00b7des", "lo\u00df", "/", "gieng", "der", "Cur\u00b7Sach\u00b7sen", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "PTKVZ", "$(", "VVFIN", "ART", "NN", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.345": {"text": "Den nechsten Weg auf Prag/ als auf des Krieges Qu\u00e4l-", "tokens": ["Den", "nechs\u00b7ten", "Weg", "auf", "Prag", "/", "als", "auf", "des", "Krie\u00b7ges", "Qu\u00e4l"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "$(", "KOKOM", "APPR", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.346": {"text": "Besah/ bezwung es auch ", "tokens": ["Be\u00b7sah", "/", "be\u00b7zwung", "es", "auch"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$(", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.347": {"text": "Worauf das meiste Land sich an Cur-Sachsen hieng/", "tokens": ["Wo\u00b7rauf", "das", "meis\u00b7te", "Land", "sich", "an", "Cur\u00b7Sach\u00b7sen", "hieng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "PRF", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.348": {"text": "Dieweil es unter jhr des Glaubens Schutz empfieng/", "tokens": ["Die\u00b7weil", "es", "un\u00b7ter", "jhr", "des", "Glau\u00b7bens", "Schutz", "emp\u00b7fi\u00b7eng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPER", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.349": {"text": "Des Glaubens/ welchem es gezwungen muste fluchen/", "tokens": ["Des", "Glau\u00b7bens", "/", "wel\u00b7chem", "es", "ge\u00b7zwun\u00b7gen", "mus\u00b7te", "flu\u00b7chen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PPER", "VVINF", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.350": {"text": "und seinen ", "tokens": ["und", "sei\u00b7nen"], "token_info": ["word", "word"], "pos": ["KON", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.351": {"text": "Der Cur F\u00fcrst kam auch selbst von Dre\u00dfden ab nach Prag/", "tokens": ["Der", "Cur", "F\u00fcrst", "kam", "auch", "selbst", "von", "Dre\u00df\u00b7den", "ab", "nach", "Prag", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "ADV", "ADV", "APPR", "NE", "PTKVZ", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.352": {"text": "und hielte da was Hof/ auch einen Danck-Fest-Tag/", "tokens": ["und", "hiel\u00b7te", "da", "was", "Hof", "/", "auch", "ei\u00b7nen", "Dan\u00b7ck\u00b7Fest\u00b7Tag", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PWS", "NN", "$(", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.353": {"text": "An welchem \u00fcberall in Prag des Luthers Lehre", "tokens": ["An", "wel\u00b7chem", "\u00fc\u00b7be\u00b7rall", "in", "Prag", "des", "Lu\u00b7thers", "Leh\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADV", "APPR", "NE", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.354": {"text": "Sehr froh geprediget und unserm GOtt zu Ehre", "tokens": ["Sehr", "froh", "ge\u00b7pre\u00b7di\u00b7get", "und", "un\u00b7serm", "Gott", "zu", "Eh\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVPP", "KON", "PPOSAT", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.355": {"text": "Manch Lied geh\u00f6ret wurd. Anjetzund nahm man ab", "tokens": ["Manch", "Lied", "ge\u00b7h\u00f6\u00b7ret", "wurd", ".", "An\u00b7jet\u00b7zund", "nahm", "man", "ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVPP", "VAFIN", "$.", "NN", "VVFIN", "PIS", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.356": {"text": "Die H\u00e4upter von dem Thurm und that sie in ein Grab/", "tokens": ["Die", "H\u00e4up\u00b7ter", "von", "dem", "Thurm", "und", "that", "sie", "in", "ein", "Grab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.357": {"text": "Die H\u00e4upter/ die das H\u00e4upt von B\u00f6h\u00e4imb ab lie\u00df schlagen/", "tokens": ["Die", "H\u00e4up\u00b7ter", "/", "die", "das", "H\u00e4upt", "von", "B\u00f6\u00b7h\u00e4i\u00b7mb", "ab", "lie\u00df", "schla\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "ART", "NN", "APPR", "NE", "PTKVZ", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.358": {"text": "Die lie\u00df der Sachsen Held geehrt zu Grabe tragen.", "tokens": ["Die", "lie\u00df", "der", "Sach\u00b7sen", "Held", "ge\u00b7ehrt", "zu", "Gra\u00b7be", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "NN", "VVPP", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.359": {"text": "Nach diesem machte sich ein Hauff nach Eger hin/", "tokens": ["Nach", "die\u00b7sem", "mach\u00b7te", "sich", "ein", "Hauff", "nach", "E\u00b7ger", "hin", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PRF", "ART", "NN", "APPR", "NE", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.360": {"text": "Eh jhn der Raht ersah/ da war er schon darin/", "tokens": ["Eh", "jhn", "der", "Raht", "er\u00b7sah", "/", "da", "war", "er", "schon", "da\u00b7rin", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$(", "ADV", "VAFIN", "PPER", "ADV", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.361": {"text": "Dann jhn die B\u00fcrgerschafft sehr gern hatt\u2019 eingelassen.", "tokens": ["Dann", "jhn", "die", "B\u00fcr\u00b7ger\u00b7schafft", "sehr", "gern", "hatt'", "ein\u00b7ge\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "ADV", "ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.362": {"text": "Ob wol der Raht befahl die Waffen zu erfassen/", "tokens": ["Ob", "wol", "der", "Raht", "be\u00b7fahl", "die", "Waf\u00b7fen", "zu", "er\u00b7fas\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVFIN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.363": {"text": "So war doch keiner da/ der jhm Geh\u00f6re gab/", "tokens": ["So", "war", "doch", "kei\u00b7ner", "da", "/", "der", "jhm", "Ge\u00b7h\u00f6\u00b7re", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIS", "ADV", "$(", "PRELS", "PPER", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.364": {"text": "und also nahm man auch den Elenbogen ab.", "tokens": ["und", "al\u00b7so", "nahm", "man", "auch", "den", "E\u00b7len\u00b7bo\u00b7gen", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.365": {"text": "Es wurd auch Budewei\u00df von dem von Thurn berungen/", "tokens": ["Es", "wurd", "auch", "Bu\u00b7de\u00b7wei\u00df", "von", "dem", "von", "Thurn", "be\u00b7run\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "APPR", "ART", "APPR", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.366": {"text": "Man hat es aber nie bey dieser Zeit bezwungen.", "tokens": ["Man", "hat", "es", "a\u00b7ber", "nie", "bey", "die\u00b7ser", "Zeit", "be\u00b7zwun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "ADV", "ADV", "APPR", "PDAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.367": {"text": "Nach dem der ", "tokens": ["Nach", "dem", "der"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.368": {"text": "Was sich den Norden L\u00f6u im Deutschland unterfieng/", "tokens": ["Was", "sich", "den", "Nor\u00b7den", "L\u00f6u", "im", "Deutschland", "un\u00b7ter\u00b7fieng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "ART", "NN", "NE", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.369": {"text": "und was die Raute that/ da wolt er auch nicht feyern/", "tokens": ["und", "was", "die", "Rau\u00b7te", "that", "/", "da", "wolt", "er", "auch", "nicht", "fey\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VVFIN", "$(", "ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.370": {"text": "und \u00fcbern K\u00e4yser her. Denselben zu besteuern/", "tokens": ["und", "\u00fc\u00b7bern", "K\u00e4y\u00b7ser", "her", ".", "Den\u00b7sel\u00b7ben", "zu", "be\u00b7steu\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "PTKVZ", "$.", "PDS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.371": {"text": "Must\u2019 eine gro\u00dfe Macht nach solchem Lande zu/", "tokens": ["Must'", "ei\u00b7ne", "gro\u00b7\u00dfe", "Macht", "nach", "sol\u00b7chem", "Lan\u00b7de", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.372": {"text": "Damit begab er sich auch wieder in die Ruh.", "tokens": ["Da\u00b7mit", "be\u00b7gab", "er", "sich", "auch", "wie\u00b7der", "in", "die", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.373": {"text": "In dessen hatte man die Herren Jesuiten", "tokens": ["In", "des\u00b7sen", "hat\u00b7te", "man", "die", "Her\u00b7ren", "Je\u00b7su\u00b7i\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDS", "VAFIN", "PIS", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.374": {"text": "Aus Prag hinweg gejagt/ weil derer Sinnen w\u00fcten", "tokens": ["Aus", "Prag", "hin\u00b7weg", "ge\u00b7jagt", "/", "weil", "de\u00b7rer", "Sin\u00b7nen", "w\u00fc\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APZR", "VVPP", "$(", "KOUS", "PDS", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.375": {"text": "Der Rauten gifftig schien/ dargegen kam die Schar/", "tokens": ["Der", "Rau\u00b7ten", "giff\u00b7tig", "schien", "/", "dar\u00b7ge\u00b7gen", "kam", "die", "Schar", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVFIN", "$(", "PAV", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.376": {"text": "Die Glaubens wegen lang aus B\u00f6h\u00e4im fl\u00fcchtig war/", "tokens": ["Die", "Glau\u00b7bens", "we\u00b7gen", "lang", "aus", "B\u00f6\u00b7h\u00e4im", "fl\u00fcch\u00b7tig", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJD", "APPR", "NE", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.377": {"text": "Mit Freuden wieder ein/ kam also hier zu Lande", "tokens": ["Mit", "Freu\u00b7den", "wie\u00b7der", "ein", "/", "kam", "al\u00b7so", "hier", "zu", "Lan\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "ADV", "ART", "$(", "VVFIN", "ADV", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.378": {"text": "Die alte Glaubens Art fast zu dem alten Stande.", "tokens": ["Die", "al\u00b7te", "Glau\u00b7bens", "Art", "fast", "zu", "dem", "al\u00b7ten", "Stan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.379": {"text": "Bey dieser Zeiten Lauf hielt alles Pabstum Raht/", "tokens": ["Bey", "die\u00b7ser", "Zei\u00b7ten", "Lauf", "hielt", "al\u00b7les", "Pab\u00b7stum", "Raht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "NN", "VVFIN", "PIAT", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.380": {"text": "Was Art dem Lutherthum/ das nun viel Wunder that/", "tokens": ["Was", "Art", "dem", "Lu\u00b7ther\u00b7thum", "/", "das", "nun", "viel", "Wun\u00b7der", "that", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "ART", "NN", "$(", "PDS", "ADV", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.381": {"text": "Gewehret k\u00f6nnte seyn/ da wurd\u2019 aus allen Seiten", "tokens": ["Ge\u00b7weh\u00b7ret", "k\u00f6nn\u00b7te", "seyn", "/", "da", "wurd'", "aus", "al\u00b7len", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "VMFIN", "VAINF", "$(", "ADV", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.382": {"text": "Ein gro\u00dfes Geld gereicht/ das bey so gro\u00dfem streiten", "tokens": ["Ein", "gro\u00b7\u00dfes", "Geld", "ge\u00b7reicht", "/", "das", "bey", "so", "gro\u00b7\u00dfem", "strei\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVPP", "$(", "PDS", "APPR", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.383": {"text": "(dann Geld erh\u00e4lt den Krieg) nicht mu\u00df gesparet seyn.", "tokens": ["(", "dann", "Geld", "er\u00b7h\u00e4lt", "den", "Krieg", ")", "nicht", "mu\u00df", "ge\u00b7spa\u00b7ret", "seyn", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "NN", "VVFIN", "ART", "NN", "$(", "PTKNEG", "VMFIN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.384": {"text": "Di\u00df alles reichte man dem Herrn von Wallenstein/", "tokens": ["Di\u00df", "al\u00b7les", "reich\u00b7te", "man", "dem", "Herrn", "von", "Wal\u00b7len\u00b7stein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "PIS", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.385": {"text": "Ein neues Heer damit ins freye Feld zu bringen.", "tokens": ["Ein", "neu\u00b7es", "Heer", "da\u00b7mit", "ins", "frey\u00b7e", "Feld", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PAV", "APPRART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.386": {"text": "Er aber/ der nicht l\u00e4ngst durch viel in vielen Dingen", "tokens": ["Er", "a\u00b7ber", "/", "der", "nicht", "l\u00e4ngst", "durch", "viel", "in", "vie\u00b7len", "Din\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "$(", "ART", "PTKNEG", "ADV", "APPR", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.387": {"text": "Sehr gro\u00df beleidigt war/ gab hier ein scheel Geschicht/", "tokens": ["Sehr", "gro\u00df", "be\u00b7lei\u00b7digt", "war", "/", "gab", "hier", "ein", "scheel", "Ge\u00b7schicht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "VAFIN", "$(", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.388": {"text": "Di\u00df alles ungeacht war gantz kein sparen nicht/", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "war", "gantz", "kein", "spa\u00b7ren", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "ADJD", "VAFIN", "ADV", "PIAT", "VVFIN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.389": {"text": "Dem Scheelen s\u00fc\u00dfe Wort und anders mehr zu geben/", "tokens": ["Dem", "Schee\u00b7len", "s\u00fc\u00b7\u00dfe", "Wort", "und", "an\u00b7ders", "mehr", "zu", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "KON", "ADV", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.390": {"text": "Bi\u00df er sich lencken lie\u00df/ und all sein Widerstreben", "tokens": ["Bi\u00df", "er", "sich", "len\u00b7cken", "lie\u00df", "/", "und", "all", "sein", "Wi\u00b7der\u00b7stre\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "VVINF", "VVFIN", "$(", "KON", "PIAT", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.391": {"text": "Gantz ab und tod erschien. Er nahm es endlich an/", "tokens": ["Gantz", "ab", "und", "tod", "er\u00b7schien", ".", "Er", "nahm", "es", "end\u00b7lich", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "KON", "NN", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.392": {"text": "und warb in kurtzer Zeit ein drey\u00dfig tausend Mann/", "tokens": ["und", "warb", "in", "kurt\u00b7zer", "Zeit", "ein", "drey\u00b7\u00dfig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "ART", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.393": {"text": "Bey welchem B\u00f6h\u00e4im sich in gro\u00dfer Angst befundte/", "tokens": ["Bey", "wel\u00b7chem", "B\u00f6\u00b7h\u00e4im", "sich", "in", "gro\u00b7\u00dfer", "Angst", "be\u00b7fund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NE", "PRF", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.394": {"text": "Auch der Cur-Sachsen Macht in stetem L\u00e4rmen stundte.", "tokens": ["Auch", "der", "Cur\u00b7Sach\u00b7sen", "Macht", "in", "ste\u00b7tem", "L\u00e4r\u00b7men", "stund\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.395": {"text": "Bald hatte dieser Sieg/ bald wieder Niederlag/", "tokens": ["Bald", "hat\u00b7te", "die\u00b7ser", "Sieg", "/", "bald", "wie\u00b7der", "Nie\u00b7der\u00b7lag", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PDAT", "NN", "$(", "ADV", "ADV", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.396": {"text": "Bald jener eben so. Ein jeder sah auf Prag.", "tokens": ["Bald", "je\u00b7ner", "e\u00b7ben", "so", ".", "Ein", "je\u00b7der", "sah", "auf", "Prag", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "ADV", "ADV", "$.", "ART", "PIS", "VVFIN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.397": {"text": "Und darum gab es auch \u00fcm Prag viel harte St\u00f6\u00dfe/", "tokens": ["Und", "da\u00b7rum", "gab", "es", "auch", "\u00fcm", "Prag", "viel", "har\u00b7te", "St\u00f6\u00b7\u00dfe", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.398": {"text": "Die gleichwol/ wie man wei\u00df/ von allzu grober Gr\u00f6\u00dfe", "tokens": ["Die", "gleich\u00b7wol", "/", "wie", "man", "wei\u00df", "/", "von", "all\u00b7zu", "gro\u00b7ber", "Gr\u00f6\u00b7\u00dfe"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADV", "$(", "PWAV", "PIS", "VVFIN", "$(", "APPR", "PTKA", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.399": {"text": "Niemals gewesen sind. In dem der Waffen-Schmied", "tokens": ["Nie\u00b7mals", "ge\u00b7we\u00b7sen", "sind", ".", "In", "dem", "der", "Waf\u00b7fen\u00b7Schmied"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAPP", "VAFIN", "$.", "APPR", "ART", "ART", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.400": {"text": "In voller Arbeit war/ erscholl das W\u00f6rtlein Fried/", "tokens": ["In", "vol\u00b7ler", "Ar\u00b7beit", "war", "/", "er\u00b7scholl", "das", "W\u00f6rt\u00b7lein", "Fried", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$(", "ADJD", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.401": {"text": "Und wolte man anjetzt zu Torgau Friede machen.", "tokens": ["Und", "wol\u00b7te", "man", "an\u00b7jetzt", "zu", "Tor\u00b7gau", "Frie\u00b7de", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ADV", "APPR", "NE", "NN", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.402": {"text": "Weil aber jhrer viel hier \u00fcm das jhre sprachen/", "tokens": ["Weil", "a\u00b7ber", "jhrer", "viel", "hier", "\u00fcm", "das", "jhre", "spra\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "ADV", "ADV", "ADV", "ART", "PPOSAT", "VVFIN", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.403": {"text": "So wurde nichts daraus/ daher der Drummelschlag", "tokens": ["So", "wur\u00b7de", "nichts", "da\u00b7raus", "/", "da\u00b7her", "der", "Drum\u00b7mel\u00b7schlag"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIS", "PAV", "$(", "PAV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.404": {"text": "An allen Orten war. Wir wenden uns von Prag", "tokens": ["An", "al\u00b7len", "Or\u00b7ten", "war", ".", "Wir", "wen\u00b7den", "uns", "von", "Prag"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "$.", "PPER", "VVFIN", "PPER", "APPR", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.405": {"text": "und gehen in das Reich/ zu sehen/ was die Waffen", "tokens": ["und", "ge\u00b7hen", "in", "das", "Reich", "/", "zu", "se\u00b7hen", "/", "was", "die", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$(", "PTKZU", "VVINF", "$(", "PWS", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.406": {"text": "Des m\u00e4chtigen Gustavs allda f\u00fcr Wercke schaffen/", "tokens": ["Des", "m\u00e4ch\u00b7ti\u00b7gen", "Gus\u00b7tavs", "all\u00b7da", "f\u00fcr", "Wer\u00b7cke", "schaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.407": {"text": "Der nunmehr N\u00fcrenberg/ die wolgebaute Stadt", "tokens": ["Der", "nun\u00b7mehr", "N\u00fc\u00b7ren\u00b7berg", "/", "die", "wol\u00b7ge\u00b7bau\u00b7te", "Stadt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADV", "NE", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.408": {"text": "und Aug des Deutschen Lands/ zu seiner Seiten hat/", "tokens": ["und", "Aug", "des", "Deut\u00b7schen", "Lands", "/", "zu", "sei\u00b7ner", "Sei\u00b7ten", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "ADJA", "NN", "$(", "APPR", "PPOSAT", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.409": {"text": "Die unl\u00e4ngst auf die Schar des Tylli/ der sich spitzte", "tokens": ["Die", "un\u00b7l\u00e4ngst", "auf", "die", "Schar", "des", "Tyl\u00b7li", "/", "der", "sich", "spitz\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJD", "APPR", "ART", "NN", "ART", "NE", "$(", "PRELS", "PRF", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.410": {"text": "Derselben Herr zu seyn/ aus jhren Bechern blitzte/", "tokens": ["Der\u00b7sel\u00b7ben", "Herr", "zu", "seyn", "/", "aus", "jhren", "Be\u00b7chern", "blitz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PTKZU", "VAINF", "$(", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.411": {"text": "Darzu ein gro\u00dfes Volck f\u00fcr Schweden werben lie\u00df/", "tokens": ["Dar\u00b7zu", "ein", "gro\u00b7\u00dfes", "Volck", "f\u00fcr", "Schwe\u00b7den", "wer\u00b7ben", "lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "APPR", "NE", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.412": {"text": "Das unter dem von Solms zu dem von Weymar stie\u00df.", "tokens": ["Das", "un\u00b7ter", "dem", "von", "Solms", "zu", "dem", "von", "Wey\u00b7mar", "stie\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "APPR", "NE", "APPR", "ART", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.413": {"text": "Es war vom Helden Horn das Bamberg eingenommen/", "tokens": ["Es", "war", "vom", "Hel\u00b7den", "Horn", "das", "Bam\u00b7berg", "ein\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "NN", "NE", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.414": {"text": "Aus dem er t\u00e4glich lie\u00df sein Volck f\u00fcr Forchheim kommen/", "tokens": ["Aus", "dem", "er", "t\u00e4g\u00b7lich", "lie\u00df", "sein", "Volck", "f\u00fcr", "Forch\u00b7heim", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADJD", "VVFIN", "PPOSAT", "NN", "APPR", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.415": {"text": "Das ein sehr fester Ort und annoch Jungfer ist.", "tokens": ["Das", "ein", "sehr", "fes\u00b7ter", "Ort", "und", "an\u00b7noch", "Jung\u00b7fer", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "ADV", "ADJA", "NN", "KON", "ADV", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.416": {"text": "Hierwider machte sich Graf Tylli sehr ger\u00fcst", "tokens": ["Hier\u00b7wi\u00b7der", "mach\u00b7te", "sich", "Graf", "Tyl\u00b7li", "sehr", "ge\u00b7r\u00fcst"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "NE", "NE", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.417": {"text": "Dem Ort Entsatz zu thun/ und Bamberg frey zu machen/", "tokens": ["Dem", "Ort", "Ent\u00b7satz", "zu", "thun", "/", "und", "Bam\u00b7berg", "frey", "zu", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "KON", "NN", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.418": {"text": "Dann Bamberg und sein Land hatt\u2019 alle b\u00e4ste Sachen", "tokens": ["Dann", "Bam\u00b7berg", "und", "sein", "Land", "hatt'", "al\u00b7le", "b\u00e4s\u00b7te", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "KON", "PPOSAT", "NN", "VAFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.419": {"text": "In Forchheim eingesetzt. Der Tylli kam heran/", "tokens": ["In", "Forch\u00b7heim", "ein\u00b7ge\u00b7setzt", ".", "Der", "Tyl\u00b7li", "kam", "he\u00b7ran", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$.", "ART", "NE", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.420": {"text": "und t\u00f6dtete dem Horn ein etlich hundert Mann/", "tokens": ["und", "t\u00f6d\u00b7te\u00b7te", "dem", "Horn", "ein", "et\u00b7lich", "hun\u00b7dert", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "ADJD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.421": {"text": "Da\u00df er gezwungen sich von Bamberg must\u2019 erheben/", "tokens": ["Da\u00df", "er", "ge\u00b7zwun\u00b7gen", "sich", "von", "Bam\u00b7berg", "must'", "er\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "PRF", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.422": {"text": "und solches seinem Feind\u2019 als St\u00e4rckern wieder geben.", "tokens": ["und", "sol\u00b7ches", "sei\u00b7nem", "Feind'", "als", "St\u00e4r\u00b7ckern", "wie\u00b7der", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "PPOSAT", "NN", "KOUS", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.423": {"text": "Dann Bamberg ist ein Ort der gro\u00df und unbewallt/", "tokens": ["Dann", "Bam\u00b7berg", "ist", "ein", "Ort", "der", "gro\u00df", "und", "un\u00b7be\u00b7wallt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VAFIN", "ART", "NN", "ART", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.424": {"text": "Ja ohne Graben ist/ ein gro\u00dfer Aufenthalt", "tokens": ["Ja", "oh\u00b7ne", "Gra\u00b7ben", "ist", "/", "ein", "gro\u00b7\u00dfer", "Auf\u00b7ent\u00b7halt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKANT", "APPR", "NN", "VAFIN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.425": {"text": "Der Pf\u00e4ff- und M\u00f6ncherey/ die gr\u00f6ste Stadt in Francken/", "tokens": ["Der", "Pf\u00e4ff", "und", "M\u00f6n\u00b7che\u00b7rey", "/", "die", "gr\u00f6s\u00b7te", "Stadt", "in", "Fran\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "$(", "ART", "ADJA", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.426": {"text": "Sehr zierlich auf gebaut und rund herum von Rancken", "tokens": ["Sehr", "zier\u00b7lich", "auf", "ge\u00b7baut", "und", "rund", "he\u00b7rum", "von", "Ran\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPR", "VVPP", "KON", "ADJD", "PTKVZ", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.427": {"text": "auch s\u00fc\u00dfem Holtze voll/ benetzt sich mit dem M\u00e4yn.", "tokens": ["auch", "s\u00fc\u00b7\u00dfem", "Holt\u00b7ze", "voll", "/", "be\u00b7netzt", "sich", "mit", "dem", "M\u00e4yn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "ADJD", "$(", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.428": {"text": "Und weil sehr gro\u00dfe Berg auf einer Seiten seyn/", "tokens": ["Und", "weil", "sehr", "gro\u00b7\u00dfe", "Berg", "auf", "ei\u00b7ner", "Sei\u00b7ten", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ADJA", "NN", "APPR", "ART", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.429": {"text": "Als ist es unbequ\u00e4m dieselbe zu besch\u00fctzen.", "tokens": ["Als", "ist", "es", "un\u00b7be\u00b7qu\u00e4m", "die\u00b7sel\u00b7be", "zu", "be\u00b7sch\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "ADJD", "PDAT", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.430": {"text": "Kan anders nichts als nur zu guten Tagen n\u00fctzen.", "tokens": ["Kan", "an\u00b7ders", "nichts", "als", "nur", "zu", "gu\u00b7ten", "Ta\u00b7gen", "n\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PIS", "KOKOM", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.431": {"text": "Hergegen aber ist das Forchheim eine Stadt/", "tokens": ["Her\u00b7ge\u00b7gen", "a\u00b7ber", "ist", "das", "Forch\u00b7heim", "ei\u00b7ne", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.432": {"text": "Die klein und eben liegt/ auch starcke Wercke hat/", "tokens": ["Die", "klein", "und", "e\u00b7ben", "liegt", "/", "auch", "star\u00b7cke", "Wer\u00b7cke", "hat", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADV", "VVFIN", "$(", "ADV", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.433": {"text": "Ist auch/ wie vor gesagt/ stets unbesiegt geblieben.", "tokens": ["Ist", "auch", "/", "wie", "vor", "ge\u00b7sagt", "/", "stets", "un\u00b7be\u00b7siegt", "ge\u00b7blie\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$(", "KOKOM", "APPR", "VVPP", "$(", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.434": {"text": "Wie nun Gustavus Horn aus Bamberg war vertrieben", "tokens": ["Wie", "nun", "Gus\u00b7ta\u00b7vus", "Horn", "aus", "Bam\u00b7berg", "war", "ver\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "NE", "NE", "APPR", "NE", "VAFIN", "VVPP"], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.435": {"text": "Zog er nach Schweinfurt hin/ wo seines Volckes war/", "tokens": ["Zog", "er", "nach", "Schwein\u00b7furt", "hin", "/", "wo", "sei\u00b7nes", "Vol\u00b7ckes", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "PTKVZ", "$(", "PWAV", "PPOSAT", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.436": {"text": "Umb welche Gegend er sein halb-zerstreute Schar", "tokens": ["Umb", "wel\u00b7che", "Ge\u00b7gend", "er", "sein", "halb\u00b7zer\u00b7streu\u00b7te", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.437": {"text": "Sich wieder samlen lie\u00df/ und/ weil jhn niemand triebe/", "tokens": ["Sich", "wie\u00b7der", "sam\u00b7len", "lie\u00df", "/", "und", "/", "weil", "jhn", "nie\u00b7mand", "trie\u00b7be", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVINF", "VVFIN", "$(", "KON", "$(", "KOUS", "PPER", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.438": {"text": "Daselbst bi\u00df zu des H\u00e4upts von Schweden Ankunft bliebe.", "tokens": ["Da\u00b7selbst", "bi\u00df", "zu", "des", "H\u00e4upts", "von", "Schwe\u00b7den", "An\u00b7kunft", "blie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.439": {"text": "Indessen hielte sich der Tylli auch in Ruh/", "tokens": ["In\u00b7des\u00b7sen", "hiel\u00b7te", "sich", "der", "Tyl\u00b7li", "auch", "in", "Ruh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NE", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.440": {"text": "und lag von Bamberg an bi\u00df bald an Amberg zu.", "tokens": ["und", "lag", "von", "Bam\u00b7berg", "an", "bi\u00df", "bald", "an", "Am\u00b7berg", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "APPR", "KON", "ADV", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.441": {"text": "Bey welchem Lager sich die N\u00fcrenberger Weyden", "tokens": ["Bey", "wel\u00b7chem", "La\u00b7ger", "sich", "die", "N\u00fc\u00b7ren\u00b7ber\u00b7ger", "Wey\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "PRF", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.442": {"text": "und derer St\u00e4dtichen ein gro\u00dfes musten leyden/", "tokens": ["und", "de\u00b7rer", "St\u00e4d\u00b7ti\u00b7chen", "ein", "gro\u00b7\u00dfes", "mus\u00b7ten", "ley\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "NN", "ART", "ADJA", "VMFIN", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.443": {"text": "Weil man sie feindlich hielt/ und weil auch diese Stadt/", "tokens": ["Weil", "man", "sie", "feind\u00b7lich", "hielt", "/", "und", "weil", "auch", "die\u00b7se", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADJD", "VVFIN", "$(", "KON", "KOUS", "ADV", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.444": {"text": "Die Schwedisch Volck einnahm und warb/ nicht Tyllisch", "tokens": ["Die", "Schwe\u00b7disch", "Volck", "ein\u00b7nahm", "und", "warb", "/", "nicht", "Tyl\u00b7lisch"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "KON", "VVFIN", "$(", "PTKNEG", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.445": {"text": "that.", "tokens": ["that", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+", "measure": "single.up"}, "line.446": {"text": "Als aber der August-Gustavus wolt\u2019 ich sagen/", "tokens": ["Als", "a\u00b7ber", "der", "Au\u00b7gust\u00b7Gu\u00b7sta\u00b7vus", "wolt'", "ich", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.447": {"text": "Dem Horn zu Hilffe kam/ des Tylli nechstes schlagen", "tokens": ["Dem", "Horn", "zu", "Hilf\u00b7fe", "kam", "/", "des", "Tyl\u00b7li", "nechs\u00b7tes", "schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$(", "ART", "NE", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.448": {"text": "Zu r\u00e4chen/ machte sich Graf Tylli aus dem Staub/", "tokens": ["Zu", "r\u00e4\u00b7chen", "/", "mach\u00b7te", "sich", "Graf", "Tyl\u00b7li", "aus", "dem", "Staub", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "PRF", "NE", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.449": {"text": "und gab den Schwedischen die gantze Pfaltz zum Raub.", "tokens": ["und", "gab", "den", "Schwe\u00b7di\u00b7schen", "die", "gant\u00b7ze", "Pfaltz", "zum", "Raub", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.450": {"text": "Jetzt ", "tokens": ["Jetzt"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.451": {"text": "sehen/", "tokens": ["se\u00b7hen", "/"], "token_info": ["word", "punct"], "pos": ["VVINF", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.452": {"text": "Wo jhm nicht wenig ist von Ehr und Hilff geschehen.", "tokens": ["Wo", "jhm", "nicht", "we\u00b7nig", "ist", "von", "Ehr", "und", "Hilff", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PTKNEG", "ADV", "VAFIN", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.453": {"text": "Er hatte noch nicht voll zween Tage da verbracht", "tokens": ["Er", "hat\u00b7te", "noch", "nicht", "voll", "zween", "Ta\u00b7ge", "da", "ver\u00b7bracht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "VVFIN", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.454": {"text": "und seinen Leib erfrischt/ gieng er mit aller Macht", "tokens": ["und", "sei\u00b7nen", "Leib", "er\u00b7frischt", "/", "gieng", "er", "mit", "al\u00b7ler", "Macht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$(", "VVFIN", "PPER", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.455": {"text": "Des Tylli Hauffen nach/ noch eins mit jhm zu wagen/", "tokens": ["Des", "Tyl\u00b7li", "Hauf\u00b7fen", "nach", "/", "noch", "eins", "mit", "jhm", "zu", "wa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NN", "APPR", "$(", "ADV", "PIS", "APPR", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.456": {"text": "Er aber lie\u00df sich fort bi\u00df an die Donau jagen.", "tokens": ["Er", "a\u00b7ber", "lie\u00df", "sich", "fort", "bi\u00df", "an", "die", "Do\u00b7nau", "ja\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PRF", "PTKVZ", "KON", "APPR", "ART", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.457": {"text": "Worauf der K\u00f6nig sich nach Donawerth begab/", "tokens": ["Wo\u00b7rauf", "der", "K\u00f6\u00b7nig", "sich", "nach", "Do\u00b7na\u00b7werth", "be\u00b7gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PRF", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.458": {"text": "und nahm die feine Stadt fast sonder Ansto\u00df ab.", "tokens": ["und", "nahm", "die", "fei\u00b7ne", "Stadt", "fast", "son\u00b7der", "An\u00b7sto\u00df", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "ADV", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.459": {"text": "Damit so galt es R\u00e4yn/ den festen Pa\u00df nach Beyern/", "tokens": ["Da\u00b7mit", "so", "galt", "es", "R\u00e4yn", "/", "den", "fes\u00b7ten", "Pa\u00df", "nach", "Be\u00b7yern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "NN", "$(", "ART", "ADJA", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.460": {"text": "Der an dem Leche ligt/ da muste Tylli steuern", "tokens": ["Der", "an", "dem", "Le\u00b7che", "ligt", "/", "da", "mus\u00b7te", "Tyl\u00b7li", "steu\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ART", "NN", "VVFIN", "$(", "ADV", "VMFIN", "NE", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.461": {"text": "So viel jhm m\u00fcglich war/ er thats als ein Soldat.", "tokens": ["So", "viel", "jhm", "m\u00fcg\u00b7lich", "war", "/", "er", "thats", "als", "ein", "Sol\u00b7dat", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADJD", "VAFIN", "$(", "PPER", "VVFIN", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.462": {"text": "Gustavus aber wust hierzu noch b\u00e4ssern Raht", "tokens": ["Gus\u00b7ta\u00b7vus", "a\u00b7ber", "wust", "hier\u00b7zu", "noch", "b\u00e4s\u00b7sern", "Raht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "VVFIN", "PAV", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.463": {"text": "Des Ortes Herr zu seyn/ als Tylli den zu sch\u00fctzen.", "tokens": ["Des", "Or\u00b7tes", "Herr", "zu", "seyn", "/", "als", "Tyl\u00b7li", "den", "zu", "sch\u00fct\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VAINF", "$(", "KOUS", "NE", "ART", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.464": {"text": "Hilf GOtt! wie sah man hier die Donnder-St\u00fccke blitzen.", "tokens": ["Hilf", "Gott", "!", "wie", "sah", "man", "hier", "die", "Donn\u00b7der\u00b7St\u00fc\u00b7cke", "blit\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$.", "PWAV", "VVFIN", "PIS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.465": {"text": "Wie donnderde der Goth bi\u00df er hin\u00fcber kam/", "tokens": ["Wie", "donn\u00b7der\u00b7de", "der", "Goth", "bi\u00df", "er", "hin\u00b7\u00fc\u00b7ber", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ART", "NN", "APPR", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.466": {"text": "Bis da\u00df des Tylli Macht die Flucht vom Leche nahm/", "tokens": ["Bis", "da\u00df", "des", "Tyl\u00b7li", "Macht", "die", "Flucht", "vom", "Le\u00b7che", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "ART", "NE", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.467": {"text": "Die sich in einem Wald\u2019 am Lechischen Gestatte", "tokens": ["Die", "sich", "in", "ei\u00b7nem", "Wald'", "am", "Le\u00b7chi\u00b7schen", "Ge\u00b7stat\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PRF", "APPR", "ART", "NN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.468": {"text": "Zur gro\u00dfen Gegenwehr jhm widersetzet hatte.", "tokens": ["Zur", "gro\u00b7\u00dfen", "Ge\u00b7gen\u00b7wehr", "jhm", "wi\u00b7der\u00b7set\u00b7zet", "hat\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "PPER", "VVFIN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.469": {"text": "Wie pra\u00dflete der Wald/ wann der Cartaunen Knall", "tokens": ["Wie", "pra\u00df\u00b7le\u00b7te", "der", "Wald", "/", "wann", "der", "Car\u00b7tau\u00b7nen", "Knall"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$(", "PWAV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.470": {"text": "Der strengen Kugel Macht/ der dicken B\u00e4ume Fall/", "tokens": ["Der", "stren\u00b7gen", "Ku\u00b7gel", "Macht", "/", "der", "di\u00b7cken", "B\u00e4u\u00b7me", "Fall", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$(", "ART", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.471": {"text": "Das Pferd- und Mensch-Geschrey/ der Mu\u00dfquetirer", "tokens": ["Das", "Pferd", "und", "Men\u00b7schGe\u00b7schrey", "/", "der", "Mu\u00df\u00b7que\u00b7ti\u00b7rer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "TRUNC", "KON", "NN", "$(", "ART", "NN"], "meter": "-+-+-+--+--", "measure": "iambic.tetra.relaxed"}, "line.472": {"text": "paffen/", "tokens": ["paf\u00b7fen", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.473": {"text": "Der Sterbenden ihr Weh/ das schlagen auf die Waffen", "tokens": ["Der", "Ster\u00b7ben\u00b7den", "ihr", "Weh", "/", "das", "schla\u00b7gen", "auf", "die", "Waf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "NN", "$(", "PDS", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.474": {"text": "Sich gr\u00e4ulich h\u00f6ren lie\u00df. Der Wald erschlug so viel", "tokens": ["Sich", "gr\u00e4u\u00b7lich", "h\u00f6\u00b7ren", "lie\u00df", ".", "Der", "Wald", "er\u00b7schlug", "so", "viel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "VVINF", "VVFIN", "$.", "ART", "NN", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.475": {"text": "Als das Gesch\u00fctze that. Bey diesem Jammer-Spiel ", "tokens": ["Als", "das", "Ge\u00b7sch\u00fct\u00b7ze", "that", ".", "Bey", "die\u00b7sem", "Jam\u00b7mer\u00b7Spiel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$.", "APPR", "PDAT", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.476": {"text": "(dann also nennet man der groben St\u00fccke morden)", "tokens": ["(", "dann", "al\u00b7so", "nen\u00b7net", "man", "der", "gro\u00b7ben", "St\u00fc\u00b7cke", "mor\u00b7den", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADV", "VVFIN", "PIS", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.477": {"text": "Sind \u00fcber tausend Mann dem Feind\u2019 erw\u00fcrget worden.", "tokens": ["Sind", "\u00fc\u00b7ber", "tau\u00b7send", "Mann", "dem", "Feind'", "er\u00b7w\u00fcr\u00b7get", "wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "CARD", "NN", "ART", "NN", "VVPP", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.478": {"text": "Es traff den Tylli selbst/ wie jhm den bald hernach", "tokens": ["Es", "traff", "den", "Tyl\u00b7li", "selbst", "/", "wie", "jhm", "den", "bald", "her\u00b7nach"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NE", "ADV", "$(", "PWAV", "PPER", "ART", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.479": {"text": "Aus Schmertzen eines Schu\u00df\u2019s das gro\u00dfe Hertz zerbrach/", "tokens": ["Aus", "Schmert\u00b7zen", "ei\u00b7nes", "Schu\u00df's", "das", "gro\u00b7\u00dfe", "Hertz", "zer\u00b7brach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.480": {"text": "und starb in Ingolstadt. Die Warheit sol es melden/", "tokens": ["und", "starb", "in", "In\u00b7gol\u00b7stadt", ".", "Die", "War\u00b7heit", "sol", "es", "mel\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "$.", "ART", "NN", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.481": {"text": "Da\u00df dieser tapfre Graf ein Kern von vielen Helden", "tokens": ["Da\u00df", "die\u00b7ser", "tapf\u00b7re", "Graf", "ein", "Kern", "von", "vie\u00b7len", "Hel\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "ART", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.482": {"text": "Alhier gewesen sey/ der gro\u00dfe Schlachten that/", "tokens": ["Al\u00b7hier", "ge\u00b7we\u00b7sen", "sey", "/", "der", "gro\u00b7\u00dfe", "Schlach\u00b7ten", "that", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAPP", "VAFIN", "$(", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.483": {"text": "und gro\u00dfe Sieg\u2019 erhielt. So bald als Reyn/ die Stadt/", "tokens": ["und", "gro\u00b7\u00dfe", "Sieg'", "er\u00b7hielt", ".", "So", "bald", "als", "Reyn", "/", "die", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVFIN", "$.", "ADV", "ADV", "KOUS", "NN", "$(", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.484": {"text": "Der Pa\u00df nach B\u00e4yern zu/ so gantz verlassen stundte/", "tokens": ["Der", "Pa\u00df", "nach", "B\u00e4\u00b7yern", "zu", "/", "so", "gantz", "ver\u00b7las\u00b7sen", "stund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "PTKZU", "$(", "ADV", "ADV", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.485": {"text": "Der Schwed auch \u00fcber das sich jhr am Halse fundte/", "tokens": ["Der", "Schwed", "auch", "\u00fc\u00b7ber", "das", "sich", "jhr", "am", "Hal\u00b7se", "fund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PRELS", "PRF", "PPER", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.486": {"text": "Begab sie sich des Streits/ und lie\u00df den K\u00f6nig ein/", "tokens": ["Be\u00b7gab", "sie", "sich", "des", "Streits", "/", "und", "lie\u00df", "den", "K\u00f6\u00b7nig", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ART", "NN", "$(", "KON", "VVFIN", "ART", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.487": {"text": "Worauf viel andre mehr an Jhn gekommen seyn.", "tokens": ["Wo\u00b7rauf", "viel", "and\u00b7re", "mehr", "an", "Jhn", "ge\u00b7kom\u00b7men", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIAT", "PIS", "ADV", "APPR", "PPER", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.488": {"text": "Dann jetzo war das Thor in B\u00e4yern aufgeschlossen/", "tokens": ["Dann", "jet\u00b7zo", "war", "das", "Thor", "in", "B\u00e4\u00b7yern", "auf\u00b7ge\u00b7schlos\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ART", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.489": {"text": "Auch Augspurg eingekriegt eh da\u00df es eins beschossen", "tokens": ["Auch", "Augs\u00b7purg", "ein\u00b7ge\u00b7kriegt", "eh", "da\u00df", "es", "eins", "be\u00b7schos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "VVFIN", "KOUS", "KOUS", "PPER", "PIS", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.490": {"text": "und recht bedr\u00f6uet wurd\u2019. Es sah die Niederlag/", "tokens": ["und", "recht", "be\u00b7dr\u00f6u\u00b7et", "wurd'", ".", "Es", "sah", "die", "Nie\u00b7der\u00b7lag", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "VAFIN", "$.", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.491": {"text": "Des Tylli Flucht und Tod/ und da\u00df der Schweden Schlag", "tokens": ["Des", "Tyl\u00b7li", "Flucht", "und", "Tod", "/", "und", "da\u00df", "der", "Schwe\u00b7den", "Schlag"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NE", "NN", "KON", "NN", "$(", "KON", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.492": {"text": "Nicht wolt\u2019 erwartet seyn/ Es sah an allen Enden/", "tokens": ["Nicht", "wolt'", "er\u00b7war\u00b7tet", "seyn", "/", "Es", "sah", "an", "al\u00b7len", "En\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "VVPP", "VAINF", "$(", "PPER", "VVFIN", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.493": {"text": "Wo schon nicht stracks/ doch nechst sich in der Schweden", "tokens": ["Wo", "schon", "nicht", "stracks", "/", "doch", "nechst", "sich", "in", "der", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "PTKNEG", "ADV", "$(", "ADV", "VVFIN", "PRF", "APPR", "ART", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.494": {"text": "H\u00e4nden.", "tokens": ["H\u00e4n\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.495": {"text": "So gieng das Deutsche Rom ", "tokens": ["So", "gieng", "das", "Deut\u00b7sche", "Rom"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.496": {"text": "Der von der B\u00fcrgerschafft mit hoher Ehr und Pracht", "tokens": ["Der", "von", "der", "B\u00fcr\u00b7ger\u00b7schafft", "mit", "ho\u00b7her", "Ehr", "und", "Pracht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.497": {"text": "Wurd\u2019 in die Stadt geholt. Die P\u00e4bstler musten weichen", "tokens": ["Wurd'", "in", "die", "Stadt", "ge\u00b7holt", ".", "Die", "P\u00e4bst\u00b7ler", "mus\u00b7ten", "wei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVPP", "$.", "ART", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.498": {"text": "und den Versto\u00dfenen die Schl\u00fcssel wieder reichen/", "tokens": ["und", "den", "Ver\u00b7sto\u00b7\u00dfe\u00b7nen", "die", "Schl\u00fcs\u00b7sel", "wie\u00b7der", "rei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ART", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.499": {"text": "Worob die B\u00fcrgerschaft in gro\u00dfen Freuden war.", "tokens": ["Wo\u00b7rob", "die", "B\u00fcr\u00b7ger\u00b7schaft", "in", "gro\u00b7\u00dfen", "Freu\u00b7den", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.500": {"text": "Indessen machte sich des B\u00e4yer F\u00fcrsten Schar", "tokens": ["In\u00b7des\u00b7sen", "mach\u00b7te", "sich", "des", "B\u00e4\u00b7yer", "F\u00fcrs\u00b7ten", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.501": {"text": "Zur Vestung Ingolstadt/ daselbst den Fu\u00df zu setzen/", "tokens": ["Zur", "Ves\u00b7tung", "In\u00b7gol\u00b7stadt", "/", "da\u00b7selbst", "den", "Fu\u00df", "zu", "set\u00b7zen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NE", "$(", "PAV", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.502": {"text": "und allen Flei\u00df zu thun die Scharten au\u00dfzuwetzen.", "tokens": ["und", "al\u00b7len", "Flei\u00df", "zu", "thun", "die", "Schar\u00b7ten", "au\u00df\u00b7zu\u00b7wet\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "PTKZU", "VVINF", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.503": {"text": "Der K\u00f6nig folgte nach/ der Beyer nahm die Flucht/", "tokens": ["Der", "K\u00f6\u00b7nig", "folg\u00b7te", "nach", "/", "der", "Be\u00b7yer", "nahm", "die", "Flucht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "$(", "ART", "NN", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.504": {"text": "Worauf man allen Weg zur Vestung hat gesucht/", "tokens": ["Wo\u00b7rauf", "man", "al\u00b7len", "Weg", "zur", "Ves\u00b7tung", "hat", "ge\u00b7sucht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "PIAT", "NN", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.505": {"text": "Derselben Herr zu seyn. Man lie\u00df die St\u00fccke pflantzen/", "tokens": ["Der\u00b7sel\u00b7ben", "Herr", "zu", "seyn", ".", "Man", "lie\u00df", "die", "St\u00fc\u00b7cke", "pflant\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PTKZU", "VAINF", "$.", "PIS", "VVFIN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.506": {"text": "Bescho\u00df es/ nahm jhr auch zwey wolgelegte Schantzen/", "tokens": ["Be\u00b7scho\u00df", "es", "/", "nahm", "jhr", "auch", "zwey", "wol\u00b7ge\u00b7leg\u00b7te", "Schant\u00b7zen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$(", "VVFIN", "PPER", "ADV", "CARD", "ADJA", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.507": {"text": "Half aber alles nichts. Man wei\u00df/ da\u00df Ingolstadt", "tokens": ["Half", "a\u00b7ber", "al\u00b7les", "nichts", ".", "Man", "wei\u00df", "/", "da\u00df", "In\u00b7gol\u00b7stadt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["NE", "ADV", "PIS", "PIS", "$.", "PIS", "VVFIN", "$(", "KOUS", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.508": {"text": "An Vestungen nicht viel von jhres gleichen hat.", "tokens": ["An", "Ves\u00b7tun\u00b7gen", "nicht", "viel", "von", "jhres", "glei\u00b7chen", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKNEG", "ADV", "APPR", "PPOSAT", "ADJA", "VAFIN", "$."], "meter": "--+-++--+-+", "measure": "iambic.penta.relaxed"}, "line.509": {"text": "Kam jhr ein Donnder zu/ sie donnderte dargegen/", "tokens": ["Kam", "jhr", "ein", "Donn\u00b7der", "zu", "/", "sie", "donn\u00b7der\u00b7te", "dar\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKZU", "$(", "PPER", "VVFIN", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.510": {"text": "Wovon das b\u00e4ste Pferd sich muste niederlegen/", "tokens": ["Wo\u00b7von", "das", "b\u00e4s\u00b7te", "Pferd", "sich", "mus\u00b7te", "nie\u00b7der\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "PRF", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.511": {"text": "Das den Gustavus trug/ mit welches weisser Haut/", "tokens": ["Das", "den", "Gus\u00b7ta\u00b7vus", "trug", "/", "mit", "wel\u00b7ches", "weis\u00b7ser", "Haut", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "NN", "VVFIN", "$(", "APPR", "PWAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.512": {"text": "Die man daselbst gef\u00fcllt im Waffen-Hause schaut/", "tokens": ["Die", "man", "da\u00b7selbst", "ge\u00b7f\u00fcllt", "im", "Waf\u00b7fen\u00b7Hau\u00b7se", "schaut", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PAV", "VVPP", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.513": {"text": "Sehr gro\u00df geprahlet wird. Gustavus war in N\u00f6then.", "tokens": ["Sehr", "gro\u00df", "ge\u00b7prah\u00b7let", "wird", ".", "Gus\u00b7ta\u00b7vus", "war", "in", "N\u00f6\u00b7then", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "VAFIN", "$.", "NE", "VAFIN", "APPR", "NN", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.514": {"text": "Man sah auch jhr Gesch\u00fctz den Held von Durlach t\u00f6dten/", "tokens": ["Man", "sah", "auch", "jhr", "Ge\u00b7sch\u00fctz", "den", "Held", "von", "Dur\u00b7lach", "t\u00f6d\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "PPOSAT", "NN", "ART", "NN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.515": {"text": "Der sehr betauert wurd\u2019. In dem die\u00df hier verlieff", "tokens": ["Der", "sehr", "be\u00b7tau\u00b7ert", "wurd'", ".", "In", "dem", "die\u00df", "hier", "ver\u00b7lieff"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVPP", "VAFIN", "$.", "APPR", "ART", "PDS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.516": {"text": "That der ergrimmte L\u00f6u von B\u00e4yern einen Griff", "tokens": ["That", "der", "er\u00b7grimm\u00b7te", "L\u00f6u", "von", "B\u00e4\u00b7yern", "ei\u00b7nen", "Griff"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN", "APPR", "NN", "ART", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.517": {"text": "Nach Regenspurg/ und brachts ", "tokens": ["Nach", "Re\u00b7gen\u00b7spurg", "/", "und", "brachts"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPR", "NE", "$(", "KON", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.518": {"text": "Worauf Gustavus Horn/ der solchem vorzubauen", "tokens": ["Wo\u00b7rauf", "Gus\u00b7ta\u00b7vus", "Horn", "/", "der", "sol\u00b7chem", "vor\u00b7zu\u00b7bau\u00b7en"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PAV", "NE", "NE", "$(", "ART", "PIAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.519": {"text": "Befehlcht/ und flei\u00dfig war/ sich wieder r\u00fcckwerts gab/", "tokens": ["Be\u00b7fehlcht", "/", "und", "flei\u00b7\u00dfig", "war", "/", "sich", "wie\u00b7der", "r\u00fcck\u00b7werts", "gab", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "ADJD", "VAFIN", "$(", "PRF", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.520": {"text": "und lie\u00df der K\u00f6nig nun von Ingolstadt gantz ab/", "tokens": ["und", "lie\u00df", "der", "K\u00f6\u00b7nig", "nun", "von", "In\u00b7gol\u00b7stadt", "gantz", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "APPR", "NE", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.521": {"text": "Weil er den Ort zu starck und wol bemannet fundte/", "tokens": ["Weil", "er", "den", "Ort", "zu", "starck", "und", "wol", "be\u00b7man\u00b7net", "fund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "NN", "KON", "ADV", "VVFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.522": {"text": "Dargegen jhm der Weg nach M\u00f6nchen offen stundte/", "tokens": ["Dar\u00b7ge\u00b7gen", "jhm", "der", "Weg", "nach", "M\u00f6n\u00b7chen", "of\u00b7fen", "stund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ART", "NN", "APPR", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.523": {"text": "Wie er dann alsobald sich auch dahin erhob/", "tokens": ["Wie", "er", "dann", "al\u00b7so\u00b7bald", "sich", "auch", "da\u00b7hin", "er\u00b7hob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "KOUS", "PRF", "ADV", "PAV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.524": {"text": "und lag der sch\u00f6nen Stadt ", "tokens": ["und", "lag", "der", "sch\u00f6\u00b7nen", "Stadt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.525": {"text": "Aus der er gro\u00dfes Geld und hundert St\u00fccke brachte/", "tokens": ["Aus", "der", "er", "gro\u00b7\u00dfes", "Geld", "und", "hun\u00b7dert", "St\u00fc\u00b7cke", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADJA", "NN", "KON", "CARD", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.526": {"text": "Die jhm ein Bauersmann aus Einfalt kuntig machte.", "tokens": ["Die", "jhm", "ein", "Bau\u00b7ers\u00b7mann", "aus", "Ein\u00b7falt", "kun\u00b7tig", "mach\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "APPR", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.527": {"text": "Auf welches dieser Held sein Kriegs-Heer mustern lie\u00df/", "tokens": ["Auf", "wel\u00b7ches", "die\u00b7ser", "Held", "sein", "Kriegs\u00b7Heer", "mus\u00b7tern", "lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PDAT", "NN", "PPOSAT", "NN", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.528": {"text": "Dabey er sich auch selbst sehr freundlich brauchen lie\u00df/", "tokens": ["Da\u00b7bey", "er", "sich", "auch", "selbst", "sehr", "freund\u00b7lich", "brau\u00b7chen", "lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "ADV", "ADV", "ADJD", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.529": {"text": "und war sein stetes Wort: Jhr Freund\u2019 und Cammeraden/", "tokens": ["und", "war", "sein", "ste\u00b7tes", "Wort", ":", "Ihr", "Freund'", "und", "Cam\u00b7me\u00b7ra\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPOSAT", "ADJA", "NN", "$.", "PPOSAT", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.530": {"text": "Jhr lieben Br\u00fcder jhr/ jhr redlichen Soldaten.", "tokens": ["Ihr", "lie\u00b7ben", "Br\u00fc\u00b7der", "jhr", "/", "jhr", "red\u00b7li\u00b7chen", "Sol\u00b7da\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PPOSAT", "$(", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.531": {"text": "Wit welchen Worten er des Volckes Hertz bezwung/", "tokens": ["Wit", "wel\u00b7chen", "Wor\u00b7ten", "er", "des", "Vol\u00b7ckes", "Hertz", "be\u00b7zwung", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "PPER", "ART", "NN", "NN", "NN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.532": {"text": "Da\u00df fast ein jeder sich f\u00fcr jhn zum Tode drung.", "tokens": ["Da\u00df", "fast", "ein", "je\u00b7der", "sich", "f\u00fcr", "jhn", "zum", "To\u00b7de", "drung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "PIAT", "PRF", "APPR", "PPER", "APPRART", "NN", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.533": {"text": "Nach dieser Musterung gieng er mit allen Helden/", "tokens": ["Nach", "die\u00b7ser", "Mus\u00b7te\u00b7rung", "gieng", "er", "mit", "al\u00b7len", "Hel\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "APPR", "PIAT", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.534": {"text": "Die wir zwar eben nicht mit Nahmen k\u00f6nnen melden/", "tokens": ["Die", "wir", "zwar", "e\u00b7ben", "nicht", "mit", "Nah\u00b7men", "k\u00f6n\u00b7nen", "mel\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADV", "PTKNEG", "APPR", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.535": {"text": "In einen Tempel ein und danckte seinem HERrn/", "tokens": ["In", "ei\u00b7nen", "Tem\u00b7pel", "ein", "und", "danck\u00b7te", "sei\u00b7nem", "HeRrn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PTKVZ", "KON", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.536": {"text": "Da\u00df er sein Heer mit jhm so gl\u00fccklich und so ferrn", "tokens": ["Da\u00df", "er", "sein", "Heer", "mit", "jhm", "so", "gl\u00fcck\u00b7lich", "und", "so", "ferrn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "APPR", "PPER", "ADV", "ADJD", "KON", "ADV", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.537": {"text": "Herum gef\u00fchret hatt\u2019 und manchen Sieg gegeben.", "tokens": ["He\u00b7rum", "ge\u00b7f\u00fch\u00b7ret", "hatt'", "und", "man\u00b7chen", "Sieg", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAFIN", "KON", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.538": {"text": "Er lie\u00df auch predigen/ da\u00df bey der Menschen Leben", "tokens": ["Er", "lie\u00df", "auch", "pre\u00b7di\u00b7gen", "/", "da\u00df", "bey", "der", "Men\u00b7schen", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "VVFIN", "$(", "KOUS", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.539": {"text": "In solcher Lehre nicht allhier geschehen war.", "tokens": ["In", "sol\u00b7cher", "Leh\u00b7re", "nicht", "all\u00b7hier", "ge\u00b7sche\u00b7hen", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PTKNEG", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.540": {"text": "Daher der P\u00f6fel sich/ die arm-verf\u00fchrte Schar/", "tokens": ["Da\u00b7her", "der", "P\u00f6\u00b7fel", "sich", "/", "die", "ar\u00b7mver\u00b7f\u00fchr\u00b7te", "Schar", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PRF", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.541": {"text": "Sehr gro\u00df verwunderte/ das wir/ als Lutheraner", "tokens": ["Sehr", "gro\u00df", "ver\u00b7wun\u00b7der\u00b7te", "/", "das", "wir", "/", "als", "Lu\u00b7the\u00b7ra\u00b7ner"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "$(", "PRELS", "PPER", "$(", "KOUS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.542": {"text": "An Christum gl\u00e4ubeten/ weil jhre Kriegsvermahner", "tokens": ["An", "Chris\u00b7tum", "gl\u00e4u\u00b7be\u00b7ten", "/", "weil", "jhre", "Kriegs\u00b7ver\u00b7mah\u00b7ner"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NE", "VVFIN", "$(", "KOUS", "PPOSAT", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.543": {"text": "und Priester jhnen uns so sch\u00e4ndlich bilden ein/", "tokens": ["und", "Pries\u00b7ter", "jh\u00b7nen", "uns", "so", "sch\u00e4nd\u00b7lich", "bil\u00b7den", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPER", "PPER", "ADV", "ADJD", "VVFIN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.544": {"text": "Als solten wir auf sie pur-lautre T\u00fcrcken seyn/", "tokens": ["Als", "sol\u00b7ten", "wir", "auf", "sie", "pur\u00b7lau\u00b7tre", "T\u00fcr\u00b7cken", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VMFIN", "PPER", "APPR", "PPER", "ADJA", "NN", "VAINF", "$("], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.545": {"text": "Die weder GOttes Wort noch eine Tugend liebten/", "tokens": ["Die", "we\u00b7der", "Got\u00b7tes", "Wort", "noch", "ei\u00b7ne", "Tu\u00b7gend", "lieb\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "KON", "NN", "NN", "ADV", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.546": {"text": "Die sich alleine nur in Lust und Lastern \u00fcbten.", "tokens": ["Die", "sich", "al\u00b7lei\u00b7ne", "nur", "in", "Lust", "und", "Las\u00b7tern", "\u00fcb\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ADV", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.547": {"text": "So scheu\u00dflich werden wir im Pabsthum abgemahlt/", "tokens": ["So", "scheu\u00df\u00b7lich", "wer\u00b7den", "wir", "im", "Pab\u00b7sthum", "ab\u00b7ge\u00b7mahlt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.548": {"text": "Daher gesengt/ gebrennt/ gesotten und gepfahlt/", "tokens": ["Da\u00b7her", "ge\u00b7sengt", "/", "ge\u00b7brennt", "/", "ge\u00b7sot\u00b7ten", "und", "ge\u00b7pfahlt", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "$(", "VVPP", "$(", "VVPP", "KON", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.549": {"text": "Gek\u00f6pfet und erhenckt/ und ob schon dieses Morden", "tokens": ["Ge\u00b7k\u00f6p\u00b7fet", "und", "er\u00b7henckt", "/", "und", "ob", "schon", "die\u00b7ses", "Mor\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVPP", "KON", "VVFIN", "$(", "KON", "KOUS", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.550": {"text": "Nunmehr gestillet ist/ ists doch begangen worden.", "tokens": ["Nun\u00b7mehr", "ge\u00b7stil\u00b7let", "ist", "/", "ists", "doch", "be\u00b7gan\u00b7gen", "wor\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAFIN", "$(", "VAFIN", "ADV", "VVPP", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.551": {"text": "Wohin? Verlauf dich nicht. Die Predig war gethan/", "tokens": ["Wo\u00b7hin", "?", "Ver\u00b7lauf", "dich", "nicht", ".", "Die", "Pre\u00b7dig", "war", "ge\u00b7than", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "NN", "PPER", "PTKNEG", "$.", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.552": {"text": "Worauf das Norden-H\u00e4upt auf seiner freyen Bahn", "tokens": ["Wo\u00b7rauf", "das", "Nor\u00b7den\u00b7H\u00e4upt", "auf", "sei\u00b7ner", "frey\u00b7en", "Bahn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.553": {"text": "Dem P\u00f6fel etlich Gold und Silber lie\u00df verstreuen/", "tokens": ["Dem", "P\u00f6\u00b7fel", "et\u00b7lich", "Gold", "und", "Sil\u00b7ber", "lie\u00df", "ver\u00b7streu\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "NN", "KON", "NN", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.554": {"text": "Der solches mit Gl\u00fcck zu dem gro\u00dfen K\u00f6nig schreyen", "tokens": ["Der", "sol\u00b7ches", "mit", "Gl\u00fcck", "zu", "dem", "gro\u00b7\u00dfen", "K\u00f6\u00b7nig", "schre\u00b7yen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "APPR", "NN", "APPR", "ART", "ADJA", "NN", "VVINF"], "meter": "-+--+--+-+--+", "measure": "amphibrach.tri.plus"}, "line.555": {"text": "Zum Denckmahl samlete. Dann es ist einmal Ja/", "tokens": ["Zum", "Denck\u00b7mahl", "sam\u00b7le\u00b7te", ".", "Dann", "es", "ist", "ein\u00b7mal", "Ja", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$.", "ADV", "PPER", "VAFIN", "ADV", "PTKANT", "$("], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.556": {"text": "Es leugnets auch kein Feind/ der diesen K\u00f6nig sah", "tokens": ["Es", "leug\u00b7nets", "auch", "kein", "Feind", "/", "der", "die\u00b7sen", "K\u00f6\u00b7nig", "sah"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN", "$(", "ART", "PDAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.557": {"text": "Kunt\u2019 jhm nicht feindlich seyn/ sein Antlitz war zu pr\u00e4chtig/", "tokens": ["Kunt'", "jhm", "nicht", "feind\u00b7lich", "seyn", "/", "sein", "Ant\u00b7litz", "war", "zu", "pr\u00e4ch\u00b7tig", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "ADJD", "VAINF", "$(", "PPOSAT", "NN", "VAFIN", "PTKA", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.558": {"text": "und GOtt/ der Helden Held/ in diesem Helden m\u00e4chtig.", "tokens": ["und", "Gott", "/", "der", "Hel\u00b7den", "Held", "/", "in", "die\u00b7sem", "Hel\u00b7den", "m\u00e4ch\u00b7tig", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$(", "ART", "NN", "NN", "$(", "APPR", "PDAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.559": {"text": "Gleich wie ", "tokens": ["Gleich", "wie"], "token_info": ["word", "word"], "pos": ["ADV", "KOKOM"], "meter": "-+", "measure": "iambic.single"}, "line.560": {"text": "Vor der Carthager H\u00e4upt der Dido sch\u00f6nen Thron/", "tokens": ["Vor", "der", "Car\u00b7tha\u00b7ger", "H\u00e4upt", "der", "Di\u00b7do", "sch\u00f6\u00b7nen", "Thron", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ART", "NE", "ADJA", "NN", "$("], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.561": {"text": "Aus Mormor/ Silber/ Gold und Helffenbein erbauet/", "tokens": ["Aus", "Mor\u00b7mor", "/", "Sil\u00b7ber", "/", "Gold", "und", "Helf\u00b7fen\u00b7bein", "er\u00b7bau\u00b7et", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "NN", "$(", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.562": {"text": "Mit vielen Helden wurd\u2019 erfreulich angeschauet:", "tokens": ["Mit", "vie\u00b7len", "Hel\u00b7den", "wurd'", "er\u00b7freu\u00b7lich", "an\u00b7ge\u00b7schau\u00b7et", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.563": {"text": "Nicht anders/ eben so/ wurd\u2019 auch Gustavus hier/", "tokens": ["Nicht", "an\u00b7ders", "/", "e\u00b7ben", "so", "/", "wurd'", "auch", "Gus\u00b7ta\u00b7vus", "hier", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "$(", "ADV", "ADV", "$(", "VAFIN", "ADV", "NE", "ADV", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.564": {"text": "In des von B\u00e4yern Stadt/ die eine rechte Zier", "tokens": ["In", "des", "von", "B\u00e4\u00b7yern", "Stadt", "/", "die", "ei\u00b7ne", "rech\u00b7te", "Zier"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NN", "NN", "$(", "ART", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.565": {"text": "Von Deutschen Sitzen ist/ mit vielen tapfren F\u00fcrsten/", "tokens": ["Von", "Deut\u00b7schen", "Sit\u00b7zen", "ist", "/", "mit", "vie\u00b7len", "tapf\u00b7ren", "F\u00fcrs\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$(", "APPR", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.566": {"text": "Die alle nach der Lust der Ablast pflag zu d\u00fcrsten/", "tokens": ["Die", "al\u00b7le", "nach", "der", "Lust", "der", "Ab\u00b7last", "pflag", "zu", "d\u00fcrs\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "APPR", "ART", "NN", "ART", "NN", "VVFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.567": {"text": "Erfreulich angeschaut. Da waren Friederich/", "tokens": ["Er\u00b7freu\u00b7lich", "an\u00b7ge\u00b7schaut", ".", "Da", "wa\u00b7ren", "Frie\u00b7de\u00b7rich", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "$.", "ADV", "VAFIN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.568": {"text": "Der Pfaltzgraf an dem Reyhn/ Augustns/ welcher sich", "tokens": ["Der", "Pfaltz\u00b7graf", "an", "dem", "Reyhn", "/", "Au\u00b7gustns", "/", "wel\u00b7cher", "sich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$(", "NE", "$(", "PRELS", "PRF"], "meter": "-+-+-++-+-+", "measure": "unknown.measure.hexa"}, "line.569": {"text": "Von Sultzbach F\u00fcrsten schrieb/ Bernhardus von den", "tokens": ["Von", "Sultz\u00b7bach", "F\u00fcrs\u00b7ten", "schrieb", "/", "Bern\u00b7har\u00b7dus", "von", "den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NE", "NN", "VVFIN", "$(", "NE", "APPR", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.570": {"text": "Sachsen/", "tokens": ["Sach\u00b7sen", "/"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.571": {"text": "Ein Held/ der ewig sol mit seinem Lobe wachsen/", "tokens": ["Ein", "Held", "/", "der", "e\u00b7wig", "sol", "mit", "sei\u00b7nem", "Lo\u00b7be", "wach\u00b7sen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "ADJD", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.572": {"text": "und andre gro\u00dfe mehr. Wir lassen diese Freund\u2019", "tokens": ["und", "and\u00b7re", "gro\u00b7\u00dfe", "mehr", ".", "Wir", "las\u00b7sen", "die\u00b7se", "Freund'"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "ADJA", "ADV", "$.", "PPER", "VVFIN", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.573": {"text": "und gehen in das Feld/ zu sehen/ was der Feind", "tokens": ["und", "ge\u00b7hen", "in", "das", "Feld", "/", "zu", "se\u00b7hen", "/", "was", "der", "Feind"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$(", "PTKZU", "VVINF", "$(", "PWS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.574": {"text": "F\u00fcr einen Sieg erlangt/ der sich mit tausend Pferden", "tokens": ["F\u00fcr", "ei\u00b7nen", "Sieg", "er\u00b7langt", "/", "der", "sich", "mit", "tau\u00b7send", "Pfer\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVPP", "$(", "PRELS", "PRF", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.575": {"text": "Aus Regenspurg begiebt/ der Schweden Tod zu werden/", "tokens": ["Aus", "Re\u00b7gen\u00b7spurg", "be\u00b7giebt", "/", "der", "Schwe\u00b7den", "Tod", "zu", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$(", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.576": {"text": "Die \u00fcm das Freisingen sich haben eingesetzt/", "tokens": ["Die", "\u00fcm", "das", "Frei\u00b7sin\u00b7gen", "sich", "ha\u00b7ben", "ein\u00b7ge\u00b7setzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "PRF", "VAFIN", "VVPP", "$("], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.577": {"text": "Er wolte Schaden thun/ wurd\u2019 aber selbst verletzt.", "tokens": ["Er", "wol\u00b7te", "Scha\u00b7den", "thun", "/", "wurd'", "a\u00b7ber", "selbst", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "VVINF", "$(", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.578": {"text": "Dann jhn die Schwedischen mit einem starcken sch\u00fc\u00dfen/", "tokens": ["Dann", "jhn", "die", "Schwe\u00b7di\u00b7schen", "mit", "ei\u00b7nem", "star\u00b7cken", "sch\u00fc\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "APPR", "ART", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.579": {"text": "Als den Verrahtenen/ sehr b\u00f6\u00df willkommen hie\u00dfen.", "tokens": ["Als", "den", "Ver\u00b7rah\u00b7te\u00b7nen", "/", "sehr", "b\u00f6\u00df", "will\u00b7kom\u00b7men", "hie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$(", "ADV", "ADJD", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.580": {"text": "Di\u00df trieb jhn wiederum in seinen alten Platz.", "tokens": ["Di\u00df", "trieb", "jhn", "wie\u00b7de\u00b7rum", "in", "sei\u00b7nen", "al\u00b7ten", "Platz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.581": {"text": "Den Schaden gut zu thun/ verf\u00fcgte sich Graf Kratz", "tokens": ["Den", "Scha\u00b7den", "gut", "zu", "thun", "/", "ver\u00b7f\u00fcg\u00b7te", "sich", "Graf", "Kratz"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "PTKZU", "VVINF", "$(", "VVFIN", "PRF", "NE", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.582": {"text": "Mit zehen tausend Mann vor Weissenburg/ und kratzte", "tokens": ["Mit", "ze\u00b7hen", "tau\u00b7send", "Mann", "vor", "Weis\u00b7sen\u00b7burg", "/", "und", "kratz\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "CARD", "CARD", "NN", "APPR", "NE", "$(", "KON", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.583": {"text": "Die Schweden da so starck/ weil man sie spat entsatzte/", "tokens": ["Die", "Schwe\u00b7den", "da", "so", "starck", "/", "weil", "man", "sie", "spat", "ent\u00b7satz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "ADV", "ADJD", "$(", "KOUS", "PIS", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.584": {"text": "Da\u00df er die Stadt gewann. Weil aber kurtz zuvor", "tokens": ["Da\u00df", "er", "die", "Stadt", "ge\u00b7wann", ".", "Weil", "a\u00b7ber", "kurtz", "zu\u00b7vor"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$.", "KOUS", "ADV", "ADJD", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.585": {"text": "Der Oberst Schlammersdorf des Landsbergs seinem Thor/", "tokens": ["Der", "O\u00b7berst", "Schlam\u00b7mers\u00b7dorf", "des", "Lands\u00b7bergs", "sei\u00b7nem", "Thor", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.586": {"text": "und noch ein ander Heer/ dem gantzen Oberlande/", "tokens": ["und", "noch", "ein", "an\u00b7der", "Heer", "/", "dem", "gant\u00b7zen", "O\u00b7berl\u00b7an\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.587": {"text": "Dem F\u00fc\u00dfen/ Ravensperg und mehrern nach dem Strande", "tokens": ["Dem", "F\u00fc\u00b7\u00dfen", "/", "Ra\u00b7ven\u00b7sperg", "und", "meh\u00b7rern", "nach", "dem", "Stran\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "NE", "KON", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.588": {"text": "Des schmalen Boden Sees wol hatten obgesiegt/", "tokens": ["Des", "schma\u00b7len", "Bo\u00b7den", "Sees", "wol", "hat\u00b7ten", "ob\u00b7ge\u00b7siegt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "ADV", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.589": {"text": "Als war an Weissenburg ein wenigs eingekriegt.", "tokens": ["Als", "war", "an", "Weis\u00b7sen\u00b7burg", "ein", "we\u00b7nigs", "ein\u00b7ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "APPR", "NN", "ART", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.590": {"text": "Es hatten sich anjetzt bey zehen tausend Bauern/", "tokens": ["Es", "hat\u00b7ten", "sich", "an\u00b7jetzt", "bey", "ze\u00b7hen", "tau\u00b7send", "Bau\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+--+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.591": {"text": "Den Schweden widersetzt/ die gleichsam wie die Mauern", "tokens": ["Den", "Schwe\u00b7den", "wi\u00b7der\u00b7setzt", "/", "die", "gleich\u00b7sam", "wie", "die", "Mau\u00b7ern"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NE", "VVPP", "$(", "ART", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.592": {"text": "F\u00fcr das Tyroler Land und ihres wolten seyn/", "tokens": ["F\u00fcr", "das", "Ty\u00b7ro\u00b7ler", "Land", "und", "ih\u00b7res", "wol\u00b7ten", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "KON", "PPOSAT", "VMFIN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.593": {"text": "Auf diese haueten drey tausend Schweden eiu/", "tokens": ["Auf", "die\u00b7se", "hau\u00b7e\u00b7ten", "drey", "tau\u00b7send", "Schwe\u00b7den", "ei\u00b7u", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "CARD", "CARD", "NE", "NE", "$("], "meter": "+--+---+-+--+", "measure": "dactylic.di.plus"}, "line.594": {"text": "und zwungen sie mit Macht den Waffen abzustehen.", "tokens": ["und", "zwun\u00b7gen", "sie", "mit", "Macht", "den", "Waf\u00b7fen", "ab\u00b7zu\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NN", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.595": {"text": "Di\u00df mochte nicht f\u00fcr voll ein Monat seyn geschehen/", "tokens": ["Di\u00df", "moch\u00b7te", "nicht", "f\u00fcr", "voll", "ein", "Mo\u00b7nat", "seyn", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKNEG", "APPR", "ADJD", "ART", "NN", "PPOSAT", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.596": {"text": "Verboste sich di\u00df Volck und fiel mit gro\u00dfer St\u00e4rck", "tokens": ["Ver\u00b7bos\u00b7te", "sich", "di\u00df", "Volck", "und", "fiel", "mit", "gro\u00b7\u00dfer", "St\u00e4rck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "PDS", "NN", "KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.597": {"text": "In etlich tausend starck an die in Ravenspergk/", "tokens": ["In", "et\u00b7lich", "tau\u00b7send", "starck", "an", "die", "in", "Ra\u00b7ven\u00b7spergk", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "CARD", "NN", "APPR", "PRELS", "APPR", "NE", "$("], "meter": "-+-+-----+-+", "measure": "unknown.measure.tetra"}, "line.598": {"text": "Schlug alles grausam todt/ was Schwedens Nahmen", "tokens": ["Schlug", "al\u00b7les", "grau\u00b7sam", "todt", "/", "was", "Schwe\u00b7dens", "Nah\u00b7men"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PIS", "ADJD", "ADJD", "$(", "PWS", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.599": {"text": "f\u00fchrte.", "tokens": ["f\u00fchr\u00b7te", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.600": {"text": "Daher der Oberste/ der nun zu ", "tokens": ["Da\u00b7her", "der", "O\u00b7bers\u00b7te", "/", "der", "nun", "zu"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "$(", "ART", "ADV", "PTKZU"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.601": {"text": "Das auch gut Schwedisch war/ ein etlich tausend Mann", "tokens": ["Das", "auch", "gut", "Schwe\u00b7disch", "war", "/", "ein", "et\u00b7lich", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "ADV", "ADJD", "NN", "VAFIN", "$(", "ART", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.602": {"text": "Zu Hauffen samlen lie\u00df/ dann es kam Zeitung an/", "tokens": ["Zu", "Hauf\u00b7fen", "sam\u00b7len", "lie\u00df", "/", "dann", "es", "kam", "Zei\u00b7tung", "an", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "VVFIN", "$(", "ADV", "PPER", "VVFIN", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.603": {"text": "Da\u00df sich ein K\u00e4ysrischer aus Elsas m\u00e4chtig machte/", "tokens": ["Da\u00df", "sich", "ein", "K\u00e4y\u00b7sri\u00b7scher", "aus", "El\u00b7sas", "m\u00e4ch\u00b7tig", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "APPR", "NE", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.604": {"text": "Den Bauern Hilf zu thun/ di\u00df alles aber brachte", "tokens": ["Den", "Bau\u00b7ern", "Hilf", "zu", "thun", "/", "di\u00df", "al\u00b7les", "a\u00b7ber", "brach\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "PTKZU", "VVINF", "$(", "PDS", "PIS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.605": {"text": "Den Bauern anders nichts als ", "tokens": ["Den", "Bau\u00b7ern", "an\u00b7ders", "nichts", "als"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "PIS", "KOKOM"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.606": {"text": "und kam fast all jhr Haab in einen gro\u00dfen Brand/", "tokens": ["und", "kam", "fast", "all", "jhr", "Ha\u00b7ab", "in", "ei\u00b7nen", "gro\u00b7\u00dfen", "Brand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "PPOSAT", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.607": {"text": "Dann sie der Schweden Faust bey hunderten entleibte/", "tokens": ["Dann", "sie", "der", "Schwe\u00b7den", "Faust", "bey", "hun\u00b7der\u00b7ten", "ent\u00b7leib\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "ADJA", "NN", "APPR", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.608": {"text": "und ihre D\u00f6rffer noch durch Brand zur Lufft versteubte.", "tokens": ["und", "ih\u00b7re", "D\u00f6rf\u00b7fer", "noch", "durch", "Brand", "zur", "Lufft", "ver\u00b7steub\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ADV", "APPR", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.609": {"text": "Wer hat es je geh\u00f6rt/ da\u00df sich ein Bauern-Krieg", "tokens": ["Wer", "hat", "es", "je", "ge\u00b7h\u00f6rt", "/", "da\u00df", "sich", "ein", "Bau\u00b7ern\u00b7Krieg"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "PPER", "ADV", "VVFIN", "$(", "KOUS", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.610": {"text": "Sehr wol geendet hat? Bekomt er einmal Sieg", "tokens": ["Sehr", "wol", "ge\u00b7en\u00b7det", "hat", "?", "Be\u00b7komt", "er", "ein\u00b7mal", "Sieg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVPP", "VAFIN", "$.", "VVFIN", "PPER", "ADV", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.611": {"text": "So folgen neunmal Schl\u00e4g. Es wollen bey dem Schla-", "tokens": ["So", "fol\u00b7gen", "neun\u00b7mal", "Schl\u00e4g", ".", "Es", "wol\u00b7len", "bey", "dem", "Schla"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "NN", "$.", "PPER", "VMFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.612": {"text": "Vernunft und Gl\u00fccke seyn/ das grobe Flegel-tragen", "tokens": ["Ver\u00b7nunft", "und", "Gl\u00fc\u00b7cke", "seyn", "/", "das", "gro\u00b7be", "Fle\u00b7gel\u00b7tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "KON", "NN", "VAINF", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.613": {"text": "Ist viel zu schlecht darzu. In dem di\u00df hier verlieff", "tokens": ["Ist", "viel", "zu", "schlecht", "dar\u00b7zu", ".", "In", "dem", "di\u00df", "hier", "ver\u00b7lieff"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PTKA", "ADJD", "PAV", "$.", "APPR", "ART", "PDS", "ADV", "VVFIN"], "meter": "-+-+-+---+-+", "measure": "unknown.measure.penta"}, "line.614": {"text": "That eine Feinds Parthey auf Speyer einen Griff/", "tokens": ["That", "ei\u00b7ne", "Feinds", "Par\u00b7they", "auf", "Spey\u00b7er", "ei\u00b7nen", "Griff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NN", "APPR", "NN", "ART", "NN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.615": {"text": "und zwung die Schwedische den Recht-Platz zu verlassen/", "tokens": ["und", "zwung", "die", "Schwe\u00b7di\u00b7sche", "den", "Recht\u00b7Platz", "zu", "ver\u00b7las\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.616": {"text": "Worauf sie also bald sich allem anzumassen", "tokens": ["Wo\u00b7rauf", "sie", "al\u00b7so", "bald", "sich", "al\u00b7lem", "an\u00b7zu\u00b7mas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "ADV", "ADV", "PRF", "PIS", "VVIZU"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.617": {"text": "Kein gro\u00df bedencken trug/ bepl\u00fcnderte die Stadt/", "tokens": ["Kein", "gro\u00df", "be\u00b7den\u00b7cken", "trug", "/", "be\u00b7pl\u00fcn\u00b7der\u00b7te", "die", "Stadt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "VVINF", "VVFIN", "$(", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.618": {"text": "und nahm nicht einen aus/ wie viel er Titul hatt.", "tokens": ["und", "nahm", "nicht", "ei\u00b7nen", "aus", "/", "wie", "viel", "er", "Ti\u00b7tul", "hatt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ART", "APPR", "$(", "PWAV", "PIS", "PPER", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.619": {"text": "Hergegen wars mit M\u00e4yntz und Mannheim wohl best\u00e4llet/", "tokens": ["Her\u00b7ge\u00b7gen", "wars", "mit", "M\u00e4yntz", "und", "Mann\u00b7heim", "wohl", "be\u00b7st\u00e4l\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "NN", "KON", "NE", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.620": {"text": "und wurden nebenst Worms von Schweden starck bew\u00e4llet.", "tokens": ["und", "wur\u00b7den", "ne\u00b7benst", "Worms", "von", "Schwe\u00b7den", "starck", "be\u00b7w\u00e4l\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "NN", "APPR", "NE", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.621": {"text": "So schien auch der Verlust an Speyer wol ersetzt/", "tokens": ["So", "schien", "auch", "der", "Ver\u00b7lust", "an", "Spey\u00b7er", "wol", "er\u00b7setzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "APPR", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.622": {"text": "In dem das Bennfeld wurd\u2019/ ein Ort sehr starck gesch\u00e4tzt/", "tokens": ["In", "dem", "das", "Benn\u00b7feld", "wurd'", "/", "ein", "Ort", "sehr", "starck", "ge\u00b7sch\u00e4tzt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "VAFIN", "$(", "ART", "NN", "ADV", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.623": {"text": "Von dem Gustavus Horn mit gro\u00dfem Sturm er\u00f6bert.", "tokens": ["Von", "dem", "Gus\u00b7ta\u00b7vus", "Horn", "mit", "gro\u00b7\u00dfem", "Sturm", "er\u00b7\u00f6\u00b7bert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NE", "NE", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.624": {"text": "Es wurden auch anjetzt viel Spanier gest\u00f6bert/", "tokens": ["Es", "wur\u00b7den", "auch", "an\u00b7jetzt", "viel", "Spa\u00b7nier", "ge\u00b7st\u00f6\u00b7bert", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "VVPP", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.625": {"text": "und in den Sand gebracht/ die man aus Niederland/", "tokens": ["und", "in", "den", "Sand", "ge\u00b7bracht", "/", "die", "man", "aus", "Nie\u00b7der\u00b7land", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVPP", "$(", "PRELS", "PIS", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.626": {"text": "um eine Hilf zu thun/ hatt\u2019 an den Reyhn gesand/", "tokens": ["um", "ei\u00b7ne", "Hilf", "zu", "thun", "/", "hatt'", "an", "den", "Reyhn", "ge\u00b7sand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "NN", "PTKZU", "VVINF", "$(", "VAFIN", "APPR", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.627": {"text": "Die Axel Oxenstirn/ der Cantzler von den Schweden/", "tokens": ["Die", "A\u00b7xel", "O\u00b7xens\u00b7tirn", "/", "der", "Cantz\u00b7ler", "von", "den", "Schwe\u00b7den", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "$(", "ART", "NN", "APPR", "ART", "NE", "$("], "meter": "-++-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.628": {"text": "Von dessen Raht und That Europa wei\u00df zu reden/", "tokens": ["Von", "des\u00b7sen", "Raht", "und", "That", "Eu\u00b7ro\u00b7pa", "wei\u00df", "zu", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "KON", "NN", "NE", "VVFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.629": {"text": "Sehr schlecht willkommen hie\u00df. Er war nicht nur ein Raht", "tokens": ["Sehr", "schlecht", "will\u00b7kom\u00b7men", "hie\u00df", ".", "Er", "war", "nicht", "nur", "ein", "Raht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "ADJD", "VVFIN", "$.", "PPER", "VAFIN", "PTKNEG", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.630": {"text": "und Cantzler/ sondern auch ein tapferer Soldat/", "tokens": ["und", "Cantz\u00b7ler", "/", "son\u00b7dern", "auch", "ein", "tap\u00b7fe\u00b7rer", "Sol\u00b7dat", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$(", "KON", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.631": {"text": "Wie diese Spannier das Zeugni\u00df werden geben/", "tokens": ["Wie", "die\u00b7se", "Span\u00b7nier", "das", "Zeug\u00b7ni\u00df", "wer\u00b7den", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDAT", "NN", "ART", "NN", "VAFIN", "VVINF", "$("], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.632": {"text": "Die \u00fcber Hal\u00df und Kopf sich musten r\u00fcckwerts heben.", "tokens": ["Die", "\u00fc\u00b7ber", "Hal\u00df", "und", "Kopf", "sich", "mus\u00b7ten", "r\u00fcck\u00b7werts", "he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "KON", "NN", "PRF", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.633": {"text": "Jetzt fiel auch Brandenburg mit einer Schweden Schar", "tokens": ["Jetzt", "fiel", "auch", "Bran\u00b7den\u00b7burg", "mit", "ei\u00b7ner", "Schwe\u00b7den", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "NE", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.634": {"text": "In Schlesien/ und zwung was jhr entgegen war/", "tokens": ["In", "Schle\u00b7si\u00b7en", "/", "und", "zwung", "was", "jhr", "ent\u00b7ge\u00b7gen", "war", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KON", "NN", "PWS", "PPER", "PTKVZ", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.635": {"text": "Gro\u00dfglogau/ Sprotau/ Steyn und Sagen/ Freystadt/", "tokens": ["Gro\u00df\u00b7glo\u00b7gau", "/", "Spro\u00b7tau", "/", "Steyn", "und", "Sa\u00b7gen", "/", "Freys\u00b7tadt", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NN", "KON", "NN", "$(", "NN", "$("], "meter": "-----+-+--+", "measure": "iambic.tri.chol"}, "line.636": {"text": "Jauer/", "tokens": ["Jau\u00b7er", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.637": {"text": "und wurd\u2019 jhr alles das zu zwingen wenig sauer/", "tokens": ["und", "wurd'", "jhr", "al\u00b7les", "das", "zu", "zwin\u00b7gen", "we\u00b7nig", "sau\u00b7er", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PIS", "ART", "PTKZU", "VVINF", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.638": {"text": "Weil alles zaghaft war. Dagegen fiel der Held", "tokens": ["Weil", "al\u00b7les", "zag\u00b7haft", "war", ".", "Da\u00b7ge\u00b7gen", "fiel", "der", "Held"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADJD", "VAFIN", "$.", "PAV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.639": {"text": "Von Pappenheim mit Macht in das Casseler Feld/", "tokens": ["Von", "Pap\u00b7pen\u00b7heim", "mit", "Macht", "in", "das", "Cas\u00b7se\u00b7ler", "Feld", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.640": {"text": "Must\u2019 aber bald darauf dasselbe wieder meyden", "tokens": ["Must'", "a\u00b7ber", "bald", "da\u00b7rauf", "das\u00b7sel\u00b7be", "wie\u00b7der", "mey\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "ADV", "PAV", "PDAT", "ADV", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.641": {"text": "und in das Niederland/ dem Mastrich/ das im Leyden", "tokens": ["und", "in", "das", "Nie\u00b7der\u00b7land", "/", "dem", "Mast\u00b7rich", "/", "das", "im", "Ley\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "$(", "ART", "NN", "$(", "ART", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.642": {"text": "und letzten Z\u00fcgen war/ ein Trost und Hilf zu seyn/", "tokens": ["und", "letz\u00b7ten", "Z\u00fc\u00b7gen", "war", "/", "ein", "Trost", "und", "Hilf", "zu", "seyn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "$(", "ART", "NN", "KON", "NE", "PTKZU", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.643": {"text": "Di\u00df alles ungeacht nahm Holland solches ein/", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "nahm", "Hol\u00b7land", "sol\u00b7ches", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "ADJD", "VVFIN", "NE", "PIAT", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.644": {"text": "und lie\u00df an jhn das Feld. Ja/ was noch mehr zu sagen/", "tokens": ["und", "lie\u00df", "an", "jhn", "das", "Feld", ".", "Ja", "/", "was", "noch", "mehr", "zu", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPER", "ART", "NN", "$.", "PTKANT", "$(", "PWS", "ADV", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.645": {"text": "Es wurd auch nun der Sachs aus Prag hinweg geschlagen", "tokens": ["Es", "wurd", "auch", "nun", "der", "Sachs", "aus", "Prag", "hin\u00b7weg", "ge\u00b7schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ART", "NN", "APPR", "NE", "APZR", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.646": {"text": "Durch den von Wallenstein/ der eine gro\u00dfe Macht", "tokens": ["Durch", "den", "von", "Wal\u00b7len\u00b7stein", "/", "der", "ei\u00b7ne", "gro\u00b7\u00dfe", "Macht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NN", "$(", "ART", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.647": {"text": "Von 60000. Mann nun frisch hatt\u2019 aufgebracht.", "tokens": ["Von", "60000.", "Mann", "nun", "frisch", "hatt'", "auf\u00b7ge\u00b7bracht", "."], "token_info": ["word", "ordinal", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.648": {"text": "Das Eger folgte Prag/ und dem der Ellenbogen/", "tokens": ["Das", "E\u00b7ger", "folg\u00b7te", "Prag", "/", "und", "dem", "der", "El\u00b7len\u00b7bo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NE", "$(", "KON", "ART", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.649": {"text": "Worauf der Wallenstein nach N\u00fcrenberg gezogen/", "tokens": ["Wo\u00b7rauf", "der", "Wal\u00b7len\u00b7stein", "nach", "N\u00fc\u00b7ren\u00b7berg", "ge\u00b7zo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.650": {"text": "Dem sich des Beyers Heer sehr bald zur Seiten gab/", "tokens": ["Dem", "sich", "des", "Be\u00b7yers", "Heer", "sehr", "bald", "zur", "Sei\u00b7ten", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PRF", "ART", "NN", "NN", "ADV", "ADV", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.651": {"text": "Den K\u00f6nig/ seinen Feind/ aus seinem B\u00e4yern ab", "tokens": ["Den", "K\u00f6\u00b7nig", "/", "sei\u00b7nen", "Feind", "/", "aus", "sei\u00b7nem", "B\u00e4\u00b7yern", "ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "PPOSAT", "NN", "$(", "APPR", "PPOSAT", "NN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.652": {"text": "und vor das N\u00fcrenberg/ als seine Stadt/ zu bringen/", "tokens": ["und", "vor", "das", "N\u00fc\u00b7ren\u00b7berg", "/", "als", "sei\u00b7ne", "Stadt", "/", "zu", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$(", "KOUS", "PPOSAT", "NN", "$(", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.653": {"text": "Wolwissend/ da\u00df er jhr f\u00fcr allen andern Dingen", "tokens": ["Wol\u00b7wis\u00b7send", "/", "da\u00df", "er", "jhr", "f\u00fcr", "al\u00b7len", "an\u00b7dern", "Din\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "KOUS", "PPER", "PPER", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.654": {"text": "W\u00fcrd Hilff und Beystand thun. Es gieng auch alles an/", "tokens": ["W\u00fcrd", "Hilff", "und", "Beys\u00b7tand", "thun", ".", "Es", "gieng", "auch", "al\u00b7les", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "VVINF", "$.", "PPER", "VVFIN", "ADV", "PIS", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.655": {"text": "Der K\u00f6nig s\u00e4umte nicht mit eben so viel Mann", "tokens": ["Der", "K\u00f6\u00b7nig", "s\u00e4um\u00b7te", "nicht", "mit", "e\u00b7ben", "so", "viel", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "APPR", "ADV", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.656": {"text": "Als Wallenstein vermocht auf N\u00fcrenberg zu gehen/", "tokens": ["Als", "Wal\u00b7len\u00b7stein", "ver\u00b7mocht", "auf", "N\u00fc\u00b7ren\u00b7berg", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVPP", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.657": {"text": "Der guten Stadt f\u00fcr jhm auch kr\u00e4ftig bey zu stehen.", "tokens": ["Der", "gu\u00b7ten", "Stadt", "f\u00fcr", "jhm", "auch", "kr\u00e4f\u00b7tig", "bey", "zu", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PPER", "ADV", "ADJD", "APPR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.658": {"text": "So stundten nun allhier zwey Heer von gro\u00dfer Macht/", "tokens": ["So", "stund\u00b7ten", "nun", "all\u00b7hier", "zwey", "Heer", "von", "gro\u00b7\u00dfer", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "CARD", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.659": {"text": "Auf hundert tausend Mann und noch vielmehr geacht/", "tokens": ["Auf", "hun\u00b7dert", "tau\u00b7send", "Mann", "und", "noch", "viel\u00b7mehr", "ge\u00b7acht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "KON", "ADV", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.660": {"text": "Die s\u00e4mptlich wolgeschickt zum ernsten Fechten waren/", "tokens": ["Die", "s\u00e4mpt\u00b7lich", "wol\u00b7ge\u00b7schickt", "zum", "erns\u00b7ten", "Fech\u00b7ten", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "APPRART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.661": {"text": "Die Proben wiesens aus. Als aber beyden Scharen/", "tokens": ["Die", "Pro\u00b7ben", "wie\u00b7sens", "aus", ".", "Als", "a\u00b7ber", "bey\u00b7den", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKVZ", "$.", "KOUS", "ADV", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.662": {"text": "So wol der\u2019 guten Stadt/ die Seuch und Hungers Noth", "tokens": ["So", "wol", "der'", "gu\u00b7ten", "Stadt", "/", "die", "Seuch", "und", "Hun\u00b7gers", "Noth"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "$(", "ART", "NN", "KON", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.663": {"text": "(ich hatte selber da mehr Gold als liebes Brod)", "tokens": ["(", "ich", "hat\u00b7te", "sel\u00b7ber", "da", "mehr", "Gold", "als", "lie\u00b7bes", "Brod", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ADV", "ADV", "PIAT", "NN", "KOUS", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.664": {"text": "Hart auf dem Halse lag/ da\u00df manche tausend starben/", "tokens": ["Hart", "auf", "dem", "Hal\u00b7se", "lag", "/", "da\u00df", "man\u00b7che", "tau\u00b7send", "star\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "VVFIN", "$(", "KOUS", "PIS", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.665": {"text": "Als brach der K\u00f6nig auf/ eh jhm noch mehr verdarben/", "tokens": ["Als", "brach", "der", "K\u00f6\u00b7nig", "auf", "/", "eh", "jhm", "noch", "mehr", "ver\u00b7dar\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "ART", "NN", "APPR", "$(", "KOUS", "PPER", "ADV", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.666": {"text": "und griff ", "tokens": ["und", "griff"], "token_info": ["word", "word"], "pos": ["KON", "VVFIN"], "meter": "-+", "measure": "iambic.single"}, "line.667": {"text": "Worbey Bernhardus hat gleich wie ein L\u00f6u gethan/", "tokens": ["Wor\u00b7bey", "Bern\u00b7har\u00b7dus", "hat", "gleich", "wie", "ein", "L\u00f6u", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "ADV", "KOKOM", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.668": {"text": "Der auf dem Berg hinauf/ wo sich der Feind enthielte/", "tokens": ["Der", "auf", "dem", "Berg", "hin\u00b7auf", "/", "wo", "sich", "der", "Feind", "ent\u00b7hiel\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "PTKVZ", "$(", "PWAV", "PRF", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.669": {"text": "und grausam in den Thal mit groben St\u00fccken spielte/", "tokens": ["und", "grau\u00b7sam", "in", "den", "Thal", "mit", "gro\u00b7ben", "St\u00fc\u00b7cken", "spiel\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.670": {"text": "Mit seinen Scharen drung/ und meynte/ da\u00df der Dachs", "tokens": ["Mit", "sei\u00b7nen", "Scha\u00b7ren", "drung", "/", "und", "meyn\u00b7te", "/", "da\u00df", "der", "Dachs"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NE", "$(", "KON", "VVFIN", "$(", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.671": {"text": "Solt\u2019 aus dem Loche gehn/ der tapfre Weymar-Sachs", "tokens": ["Solt'", "aus", "dem", "Lo\u00b7che", "gehn", "/", "der", "tapf\u00b7re", "Wey\u00b7ma\u00b7rSachs"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "APPR", "ART", "NN", "VVINF", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.672": {"text": "Wolt\u2019 in dem Felde sich mit seinem Feinde schlagen/", "tokens": ["Wolt'", "in", "dem", "Fel\u00b7de", "sich", "mit", "sei\u00b7nem", "Fein\u00b7de", "schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "++-+-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.673": {"text": "Er aber Wallenstein begehrt es nicht zu wagen/", "tokens": ["Er", "a\u00b7ber", "Wal\u00b7len\u00b7stein", "be\u00b7gehrt", "es", "nicht", "zu", "wa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "NN", "VVFIN", "PPER", "PTKNEG", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.674": {"text": "Enthielt sich auch im Loch und brauchte f\u00fcr die Spitz", "tokens": ["Ent\u00b7hielt", "sich", "auch", "im", "Loch", "und", "brauch\u00b7te", "f\u00fcr", "die", "Spitz"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "APPRART", "NN", "KON", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.675": {"text": "Auf seinen Gegentheil das donndernde Gesch\u00fctz.", "tokens": ["Auf", "sei\u00b7nen", "Ge\u00b7gen\u00b7theil", "das", "donn\u00b7dern\u00b7de", "Ge\u00b7sch\u00fctz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.676": {"text": "Als nun der K\u00f6nig sah da\u00df sich der Feind nicht regte/", "tokens": ["Als", "nun", "der", "K\u00f6\u00b7nig", "sah", "da\u00df", "sich", "der", "Feind", "nicht", "reg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVFIN", "KOUS", "PRF", "ART", "NN", "PTKNEG", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.677": {"text": "und nur mit St\u00fccken sich jhm grausam widerlegte/", "tokens": ["und", "nur", "mit", "St\u00fc\u00b7cken", "sich", "jhm", "grau\u00b7sam", "wi\u00b7der\u00b7leg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "PRF", "PPER", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.678": {"text": "Da\u00df gro\u00dfe M\u00e4nge blieb/ Bannier/ der gro\u00dfe Held/", "tokens": ["Da\u00df", "gro\u00b7\u00dfe", "M\u00e4n\u00b7ge", "blieb", "/", "Ban\u00b7nier", "/", "der", "gro\u00b7\u00dfe", "Held", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "VVFIN", "$(", "NE", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.679": {"text": "Verwundt/ und Torsten Sohn/ der sich hernach der Welt", "tokens": ["Ver\u00b7wundt", "/", "und", "Tors\u00b7ten", "Sohn", "/", "der", "sich", "her\u00b7nach", "der", "Welt"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$(", "KON", "NN", "NN", "$(", "PRELS", "PRF", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.680": {"text": "Sehr tapfer hat bezeugt/ vom Wallenstein gefangen/", "tokens": ["Sehr", "tap\u00b7fer", "hat", "be\u00b7zeugt", "/", "vom", "Wal\u00b7len\u00b7stein", "ge\u00b7fan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "VVPP", "$(", "APPRART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.681": {"text": "und da\u00df sehr wenig w\u00e4r auf di\u00dfmal zu erlangen/", "tokens": ["und", "da\u00df", "sehr", "we\u00b7nig", "w\u00e4r", "auf", "di\u00df\u00b7mal", "zu", "er\u00b7lan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "PIS", "VAFIN", "APPR", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.682": {"text": "Bewog es jhn den Feind zu lassen/ und zu sehn/", "tokens": ["Be\u00b7wog", "es", "jhn", "den", "Feind", "zu", "las\u00b7sen", "/", "und", "zu", "sehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "ART", "NN", "PTKZU", "VVINF", "$(", "KON", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.683": {"text": "Wohin er/ wann er weg/ sich r\u00fcstete zu gehen.", "tokens": ["Wo\u00b7hin", "er", "/", "wann", "er", "weg", "/", "sich", "r\u00fcs\u00b7te\u00b7te", "zu", "ge\u00b7hen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "$(", "PWAV", "PPER", "PTKVZ", "$(", "PRF", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.684": {"text": "Hierauf zertheilte sich der Feind/ und gieng der B\u00e4yer", "tokens": ["Hier\u00b7auf", "zer\u00b7theil\u00b7te", "sich", "der", "Feind", "/", "und", "gieng", "der", "B\u00e4\u00b7yer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PRF", "ART", "NN", "$(", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.685": {"text": "Nach seinem B\u00e4yern zu/ das \u00fcberige Feuer/", "tokens": ["Nach", "sei\u00b7nem", "B\u00e4\u00b7yern", "zu", "/", "das", "\u00fc\u00b7be\u00b7ri\u00b7ge", "Feu\u00b7er", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.686": {"text": "Von Schweden eingelegt/ zu d\u00e4mpfen/ nahm auch R\u00e4yhn", "tokens": ["Von", "Schwe\u00b7den", "ein\u00b7ge\u00b7legt", "/", "zu", "d\u00e4mp\u00b7fen", "/", "nahm", "auch", "R\u00e4yhn"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NE", "VVPP", "$(", "PTKZU", "VVINF", "$(", "VVFIN", "ADV", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.687": {"text": "Fast ohne gro\u00dfen Sturm in wenig Tagen ein.", "tokens": ["Fast", "oh\u00b7ne", "gro\u00b7\u00dfen", "Sturm", "in", "we\u00b7nig", "Ta\u00b7gen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJA", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.688": {"text": "Sa bald der K\u00f6nig sah wohin sich dieser kehrte/", "tokens": ["Sa", "bald", "der", "K\u00f6\u00b7nig", "sah", "wo\u00b7hin", "sich", "die\u00b7ser", "kehr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "VVFIN", "PWAV", "PRF", "PDS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.689": {"text": "Erhob er sich geschwind/ damit er jhm verwehrte/", "tokens": ["Er\u00b7hob", "er", "sich", "ge\u00b7schwind", "/", "da\u00b7mit", "er", "jhm", "ver\u00b7wehr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "$(", "KOUS", "PPER", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.690": {"text": "Was m\u00e4chtigers zu thun. In dem als di\u00df verlieff/", "tokens": ["Was", "m\u00e4ch\u00b7ti\u00b7gers", "zu", "thun", ".", "In", "dem", "als", "di\u00df", "ver\u00b7lieff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "PTKZU", "VVINF", "$.", "APPR", "ART", "KOKOM", "PDS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.691": {"text": "Da\u00df der ergrimmte L\u00f6u hier nach dem B\u00e4yer griff/", "tokens": ["Da\u00df", "der", "er\u00b7grimm\u00b7te", "L\u00f6u", "hier", "nach", "dem", "B\u00e4\u00b7yer", "griff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.692": {"text": "Fiel der von Wallenstein durch Francken in das Mei\u00dfen/", "tokens": ["Fiel", "der", "von", "Wal\u00b7len\u00b7stein", "durch", "Fran\u00b7cken", "in", "das", "Mei\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "NN", "APPR", "NN", "APPR", "ART", "NN", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.693": {"text": "Dem aber Bernhards Schwerd heroisch nach zu schmei\u00dfen/", "tokens": ["Dem", "a\u00b7ber", "Bern\u00b7hards", "Schwerd", "he\u00b7ro\u00b7isch", "nach", "zu", "schmei\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "NE", "NE", "ADJD", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.694": {"text": "Sich nichts bed\u00e4mpfen lie\u00df/ wie solches der Croat", "tokens": ["Sich", "nichts", "be\u00b7d\u00e4mp\u00b7fen", "lie\u00df", "/", "wie", "sol\u00b7ches", "der", "Croat"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "PIS", "VVINF", "VVFIN", "$(", "KOKOM", "PIAT", "ART", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.695": {"text": "um Coburg/ Schwabach/ Roth und sonst gef\u00fchlet hat.", "tokens": ["um", "Co\u00b7burg", "/", "Schwa\u00b7bach", "/", "Roth", "und", "sonst", "ge\u00b7f\u00fch\u00b7let", "hat", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "NN", "$(", "NN", "KON", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.696": {"text": "Viel hundert kamen hier um ihr verteufelt Leben/", "tokens": ["Viel", "hun\u00b7dert", "ka\u00b7men", "hier", "um", "ihr", "ver\u00b7teu\u00b7felt", "Le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "VVFIN", "ADV", "APPR", "PPER", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.697": {"text": "Viel hundert musten sich hier auch gefangen geben/", "tokens": ["Viel", "hun\u00b7dert", "mus\u00b7ten", "sich", "hier", "auch", "ge\u00b7fan\u00b7gen", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "VMFIN", "PRF", "ADV", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.698": {"text": "und nach den Bergen zu/ die in dem Schweden sind/", "tokens": ["und", "nach", "den", "Ber\u00b7gen", "zu", "/", "die", "in", "dem", "Schwe\u00b7den", "sind", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "PTKZU", "$(", "ART", "APPR", "ART", "NE", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.699": {"text": "In denen man viel Stahl und gutes Kupfer find/", "tokens": ["In", "de\u00b7nen", "man", "viel", "Stahl", "und", "gu\u00b7tes", "Kup\u00b7fer", "find", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIS", "PIAT", "NN", "KON", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.700": {"text": "F\u00fcr ihren Raub daselbst in Banden stets zu graben/", "tokens": ["F\u00fcr", "ih\u00b7ren", "Raub", "da\u00b7selbst", "in", "Ban\u00b7den", "stets", "zu", "gra\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PAV", "APPR", "NN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.701": {"text": "Die Marck und Bein bey uns vorher durchw\u00fchlet haben.", "tokens": ["Die", "Marck", "und", "Bein", "bey", "uns", "vor\u00b7her", "durch\u00b7w\u00fch\u00b7let", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "APPR", "PPER", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.702": {"text": "Wie nun der Schweden Held vernahm/ was Wallenstein", "tokens": ["Wie", "nun", "der", "Schwe\u00b7den", "Held", "ver\u00b7nahm", "/", "was", "Wal\u00b7len\u00b7stein"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$(", "PWS", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.703": {"text": "Vor einen Anschlag hatt\u2019/ und da\u00df es muste seyn/", "tokens": ["Vor", "ei\u00b7nen", "An\u00b7schlag", "hatt'", "/", "und", "da\u00df", "es", "mus\u00b7te", "seyn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "$(", "KON", "KOUS", "PPER", "VMFIN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.704": {"text": "Dem Sachsen Hilf zu thun/ weil man auf allen Seiten", "tokens": ["Dem", "Sach\u00b7sen", "Hilf", "zu", "thun", "/", "weil", "man", "auf", "al\u00b7len", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "PTKZU", "VVINF", "$(", "KOUS", "PIS", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.705": {"text": "Sehr schnell zusameu kam/ denselben zu bestreiten/", "tokens": ["Sehr", "schnell", "zu\u00b7sa\u00b7meu", "kam", "/", "den\u00b7sel\u00b7ben", "zu", "be\u00b7strei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "VVFIN", "$(", "PDS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.706": {"text": "Dann auch der Pappenheim jhm in sein Land ein kam/", "tokens": ["Dann", "auch", "der", "Pap\u00b7pen\u00b7heim", "jhm", "in", "sein", "Land", "ein", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.707": {"text": "Ein audrer aber gar sein Leipzig wieder nahm/", "tokens": ["Ein", "aud\u00b7rer", "a\u00b7ber", "gar", "sein", "Leip\u00b7zig", "wie\u00b7der", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADV", "ADV", "PPOSAT", "NE", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.708": {"text": "Verlie\u00df er diesen Theil und eilete nach Mei\u00dfen/", "tokens": ["Ver\u00b7lie\u00df", "er", "die\u00b7sen", "Theil", "und", "ei\u00b7le\u00b7te", "nach", "Mei\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PDAT", "NN", "KON", "VVFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.709": {"text": "Nechst seinem Bernhard sich mit jhnen rum zu schmei\u00dfen.", "tokens": ["Nechst", "sei\u00b7nem", "Bern\u00b7hard", "sich", "mit", "jh\u00b7nen", "rum", "zu", "schmei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NE", "PRF", "APPR", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.710": {"text": "Es wurde dieser Zug so schnell und streng gesch\u00e4tzt/", "tokens": ["Es", "wur\u00b7de", "die\u00b7ser", "Zug", "so", "schnell", "und", "streng", "ge\u00b7sch\u00e4tzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PDAT", "NN", "ADV", "ADJD", "KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.711": {"text": "Als wann ein grimmer L\u00f6u nach einem Raube setzt.", "tokens": ["Als", "wann", "ein", "grim\u00b7mer", "L\u00f6u", "nach", "ei\u00b7nem", "Rau\u00b7be", "setzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "ART", "ADJA", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.712": {"text": "Wann du von Wei\u00dfenfels nach Leipzig wilst verreisen/", "tokens": ["Wann", "du", "von", "Wei\u00b7\u00dfen\u00b7fels", "nach", "Leip\u00b7zig", "wilst", "ver\u00b7rei\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "NN", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.713": {"text": "Wird unterwegens sich ein kleines St\u00e4dtlein weisen/", "tokens": ["Wird", "un\u00b7ter\u00b7we\u00b7gens", "sich", "ein", "klei\u00b7nes", "St\u00e4dt\u00b7lein", "wei\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PRF", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.714": {"text": "Das sch\u00f6n zu Felde ligt und in die Ferne gleist/", "tokens": ["Das", "sch\u00f6n", "zu", "Fel\u00b7de", "ligt", "und", "in", "die", "Fer\u00b7ne", "gleist", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "APPR", "NN", "VVFIN", "KON", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.715": {"text": "Hat auch ein kleines Schlo\u00df das beydes L\u00fctzen heist/", "tokens": ["Hat", "auch", "ein", "klei\u00b7nes", "Schlo\u00df", "das", "bey\u00b7des", "L\u00fct\u00b7zen", "heist", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.716": {"text": "Ist uns vor dieser Zeit nicht viel bekannt gewesen/", "tokens": ["Ist", "uns", "vor", "die\u00b7ser", "Zeit", "nicht", "viel", "be\u00b7kannt", "ge\u00b7we\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PDAT", "NN", "PTKNEG", "ADV", "ADJD", "VAPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.717": {"text": "Nun aber kan man es bey allen V\u00f6lckern lesen.", "tokens": ["Nun", "a\u00b7ber", "kan", "man", "es", "bey", "al\u00b7len", "V\u00f6l\u00b7ckern", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PIS", "PPER", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.718": {"text": "Dann L\u00fctzen/ L\u00fctzen ist der unbegl\u00fcckte Platz", "tokens": ["Dann", "L\u00fct\u00b7zen", "/", "L\u00fct\u00b7zen", "ist", "der", "un\u00b7be\u00b7gl\u00fcck\u00b7te", "Platz"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "$(", "NN", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.719": {"text": "Wo alles Schweden-Reichs geliebster Trost und Schatz", "tokens": ["Wo", "al\u00b7les", "Schwe\u00b7den\u00b7Reichs", "ge\u00b7liebs\u00b7ter", "Trost", "und", "Schatz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIAT", "NN", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.720": {"text": "und wo des Jsraels sein Josua geblieben.", "tokens": ["und", "wo", "des", "Js\u00b7raels", "sein", "Jo\u00b7sua", "ge\u00b7blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "PPOSAT", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.721": {"text": "O der verfluchten Faust/ die diese That betrieben", "tokens": ["O", "der", "ver\u00b7fluch\u00b7ten", "Faust", "/", "die", "die\u00b7se", "That", "be\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "ART", "ADJA", "NN", "$(", "ART", "PDAT", "NN", "VVPP"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.722": {"text": "und Jhn erschossen hat! Dann wie des Feindes Heer", "tokens": ["und", "Jhn", "er\u00b7schos\u00b7sen", "hat", "!", "Dann", "wie", "des", "Fein\u00b7des", "Heer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVPP", "VAFIN", "$.", "ADV", "KOKOM", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.723": {"text": "Zur Schlacht gewillt erschien/ ermahnten Sach und Ehr\u2019/", "tokens": ["Zur", "Schlacht", "ge\u00b7willt", "er\u00b7schien", "/", "er\u00b7mahn\u00b7ten", "Sach", "und", "Ehr'", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "VVFIN", "$(", "ADJA", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.724": {"text": "Ob man schon dieses Theils am Volcke zu dem schlagen", "tokens": ["Ob", "man", "schon", "die\u00b7ses", "Theils", "am", "Vol\u00b7cke", "zu", "dem", "schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "PDAT", "NN", "APPRART", "NN", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.725": {"text": "Ein gro\u00dfes schw\u00e4cher war/ mit solchem es zu wagen.", "tokens": ["Ein", "gro\u00b7\u00dfes", "schw\u00e4\u00b7cher", "war", "/", "mit", "sol\u00b7chem", "es", "zu", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJD", "VAFIN", "$(", "APPR", "PIAT", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.726": {"text": "Daher der K\u00f6nig sich nach seiner alten Art", "tokens": ["Da\u00b7her", "der", "K\u00f6\u00b7nig", "sich", "nach", "sei\u00b7ner", "al\u00b7ten", "Art"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ART", "NN", "PRF", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.727": {"text": "Sich dort und da erwie\u00df mit seiner Gegenwart/", "tokens": ["Sich", "dort", "und", "da", "er\u00b7wie\u00df", "mit", "sei\u00b7ner", "Ge\u00b7gen\u00b7wart", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "KON", "ADV", "VVFIN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.728": {"text": "Dem Volcke Muth und Sieg in seine Faust zu bringen/", "tokens": ["Dem", "Vol\u00b7cke", "Muth", "und", "Sieg", "in", "sei\u00b7ne", "Faust", "zu", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.729": {"text": "Weil jeder seines Volcks mit dreyen solte ringen/", "tokens": ["Weil", "je\u00b7der", "sei\u00b7nes", "Volcks", "mit", "drey\u00b7en", "sol\u00b7te", "rin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "APPR", "CARD", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.730": {"text": "So m\u00e4chtig war der Feind. In dem er Hilff und Raht", "tokens": ["So", "m\u00e4ch\u00b7tig", "war", "der", "Feind", ".", "In", "dem", "er", "Hilff", "und", "Raht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "$.", "APPR", "PRELS", "PPER", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.731": {"text": "Zu geben emsig war/ und dieses alles that", "tokens": ["Zu", "ge\u00b7ben", "em\u00b7sig", "war", "/", "und", "die\u00b7ses", "al\u00b7les", "that"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "ADJD", "VAFIN", "$(", "KON", "PDS", "PIS", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.732": {"text": "Was Helden zugeh\u00f6rt/ gerieth er ohne wissen", "tokens": ["Was", "Hel\u00b7den", "zu\u00b7ge\u00b7h\u00f6rt", "/", "ge\u00b7rieth", "er", "oh\u00b7ne", "wis\u00b7sen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "NN", "VVPP", "$(", "VVFIN", "PPER", "APPR", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.733": {"text": "In eine Feinds Parthey/ die Jhn mit vielen sch\u00fcssen", "tokens": ["In", "ei\u00b7ne", "Feinds", "Par\u00b7they", "/", "die", "Jhn", "mit", "vie\u00b7len", "sch\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN", "$(", "PRELS", "PPER", "APPR", "PIAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.734": {"text": "Aus seinen Sattel warf/ da\u00df jhn sein Geist verlie\u00df. ", "tokens": ["Aus", "sei\u00b7nen", "Sat\u00b7tel", "warf", "/", "da\u00df", "jhn", "sein", "Geist", "ver\u00b7lie\u00df", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "KOUS", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.735": {"text": "Wie aber Samson dort die S\u00e4ulen nieder rie\u00df/", "tokens": ["Wie", "a\u00b7ber", "Sam\u00b7son", "dort", "die", "S\u00e4u\u00b7len", "nie\u00b7der", "rie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "NE", "ADV", "ART", "NN", "PTKVZ", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.736": {"text": "Da\u00df sich sein Feind mit jhm dem Tode must\u2019 ergeben/", "tokens": ["Da\u00df", "sich", "sein", "Feind", "mit", "jhm", "dem", "To\u00b7de", "must'", "er\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPOSAT", "NN", "APPR", "PPER", "ART", "NN", "VMFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.737": {"text": "Also geschah auch hier. So bald des Helden Leben", "tokens": ["Al\u00b7so", "ge\u00b7schah", "auch", "hier", ".", "So", "bald", "des", "Hel\u00b7den", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "$.", "ADV", "ADV", "ART", "NN", "NN"], "meter": "---+-+-+-+-+-", "measure": "unknown.measure.penta"}, "line.738": {"text": "Sein Ende hatt\u2019 erreicht/ that der ber\u00fchmte F\u00fcrst", "tokens": ["Sein", "En\u00b7de", "hatt'", "er\u00b7reicht", "/", "that", "der", "be\u00b7r\u00fchm\u00b7te", "F\u00fcrst"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "VVPP", "$(", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.739": {"text": "Bernhardus wie ein L\u00f6u/ den nach der Rache d\u00fcrst/", "tokens": ["Bern\u00b7har\u00b7dus", "wie", "ein", "L\u00f6u", "/", "den", "nach", "der", "Ra\u00b7che", "d\u00fcrst", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOKOM", "ART", "NN", "$(", "ART", "APPR", "ART", "NN", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.740": {"text": "und schry: Das H\u00e4upt ist todt/ wir wollen auch nicht leben/", "tokens": ["und", "schry", ":", "Das", "H\u00e4upt", "ist", "todt", "/", "wir", "wol\u00b7len", "auch", "nicht", "le\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "$.", "ART", "NN", "VAFIN", "ADJD", "$(", "PPER", "VMFIN", "ADV", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.741": {"text": "Jhr Br\u00fcder lasset uns Jhm das Geleite geben.", "tokens": ["Ihr", "Br\u00fc\u00b7der", "las\u00b7set", "uns", "Jhm", "das", "Ge\u00b7lei\u00b7te", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.742": {"text": "Greifft seinen W\u00fcrger an/ wie meine Rechte thut/", "tokens": ["Greifft", "sei\u00b7nen", "W\u00fcr\u00b7ger", "an", "/", "wie", "mei\u00b7ne", "Rech\u00b7te", "thut", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$(", "KOKOM", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.743": {"text": "und nehmt gerechte Rach \u00fcm euers K\u00f6nigs Blut.", "tokens": ["und", "nehmt", "ge\u00b7rech\u00b7te", "Rach", "\u00fcm", "eu\u00b7ers", "K\u00f6\u00b7nigs", "Blut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJA", "NN", "APPRART", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.744": {"text": "Damit so sahe man aus Menschen L\u00f6uen werden/", "tokens": ["Da\u00b7mit", "so", "sa\u00b7he", "man", "aus", "Men\u00b7schen", "L\u00f6u\u00b7en", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "PIS", "APPR", "NN", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.745": {"text": "Die jhres K\u00f6nigs Blut mit schrecklichen Geberden", "tokens": ["Die", "jhres", "K\u00f6\u00b7nigs", "Blut", "mit", "schreck\u00b7li\u00b7chen", "Ge\u00b7ber\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "NN", "NN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.746": {"text": "Zu r\u00e4chen dr\u00f6ueten/ und br\u00fcllten/ da\u00df die Lufft", "tokens": ["Zu", "r\u00e4\u00b7chen", "dr\u00f6u\u00b7e\u00b7ten", "/", "und", "br\u00fcll\u00b7ten", "/", "da\u00df", "die", "Lufft"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "VVFIN", "$(", "KON", "VVFIN", "$(", "KOUS", "ART", "NN"], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.747": {"text": "Voll Rauch und Nebel wurd\u2019/ es wurde Rach gerufft", "tokens": ["Voll", "Rauch", "und", "Ne\u00b7bel", "wurd'", "/", "es", "wur\u00b7de", "Rach", "ge\u00b7rufft"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "NN", "KON", "NN", "VAFIN", "$(", "PPER", "VAFIN", "NN", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.748": {"text": "So lang man Feinde sah/ da\u00df also diese L\u00f6uen", "tokens": ["So", "lang", "man", "Fein\u00b7de", "sah", "/", "da\u00df", "al\u00b7so", "die\u00b7se", "L\u00f6u\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "PIS", "NN", "VVFIN", "$(", "KOUS", "ADV", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.749": {"text": "Den Feind zerstreueten/ und jhm das gro\u00dfe Freueu", "tokens": ["Den", "Feind", "zer\u00b7streu\u00b7e\u00b7ten", "/", "und", "jhm", "das", "gro\u00b7\u00dfe", "Freu\u00b7eu"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$(", "KON", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.750": {"text": "Ob jhres K\u00f6nigs Fall verg\u00e4llten. Diese Schlacht", "tokens": ["Ob", "jhres", "K\u00f6\u00b7nigs", "Fall", "ver\u00b7g\u00e4ll\u00b7ten", ".", "Die\u00b7se", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "NN", "VVFIN", "$.", "PDAT", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.751": {"text": "Hat auch den Pappenheim/ den Helden/ umgebracht/", "tokens": ["Hat", "auch", "den", "Pap\u00b7pen\u00b7heim", "/", "den", "Hel\u00b7den", "/", "um\u00b7ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$(", "ART", "NN", "$(", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.752": {"text": "Den man so ungern hat im Pabsthum eingeb\u00fc\u00dfet/", "tokens": ["Den", "man", "so", "un\u00b7gern", "hat", "im", "Pab\u00b7sthum", "ein\u00b7ge\u00b7b\u00fc\u00b7\u00dfet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADV", "ADV", "VAFIN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.753": {"text": "So ungern als man hier den K\u00f6nig hat gemisset.", "tokens": ["So", "un\u00b7gern", "als", "man", "hier", "den", "K\u00f6\u00b7nig", "hat", "ge\u00b7mis\u00b7set", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KOUS", "PIS", "ADV", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.754": {"text": "Wurd\u2019 also dieser Sieg zu L\u00fctzen hoch erkaufft.", "tokens": ["Wurd'", "al\u00b7so", "die\u00b7ser", "Sieg", "zu", "L\u00fct\u00b7zen", "hoch", "er\u00b7kaufft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PDAT", "NN", "APPR", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.755": {"text": "Es lag das weite Feld mit Feinden voll gehaufft/", "tokens": ["Es", "lag", "das", "wei\u00b7te", "Feld", "mit", "Fein\u00b7den", "voll", "ge\u00b7haufft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "APPR", "NN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.756": {"text": "Und wann die d\u00fcstre Nacht nicht h\u00e4tte Schutz genommen", "tokens": ["Und", "wann", "die", "d\u00fcst\u00b7re", "Nacht", "nicht", "h\u00e4t\u00b7te", "Schutz", "ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "ADJA", "NN", "PTKNEG", "VAFIN", "NN", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.757": {"text": "So w\u00e4re Wallenstein sehr kahl nach Hause kommen.", "tokens": ["So", "w\u00e4\u00b7re", "Wal\u00b7len\u00b7stein", "sehr", "kahl", "nach", "Hau\u00b7se", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "ADV", "ADJD", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.758": {"text": "Es blieb ihm ohne das ein trefflich gro\u00dfer Hauff", "tokens": ["Es", "blieb", "ihm", "oh\u00b7ne", "das", "ein", "treff\u00b7lich", "gro\u00b7\u00dfer", "Hauff"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PRELS", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.759": {"text": "Von vielen tausenden mit dem Gesch\u00fctz im Lauff.", "tokens": ["Von", "vie\u00b7len", "tau\u00b7sen\u00b7den", "mit", "dem", "Ge\u00b7sch\u00fctz", "im", "Lauff", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "APPR", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.760": {"text": "Hat also Bernhard dich/ O K\u00f6nig! wol gerochen/", "tokens": ["Hat", "al\u00b7so", "Bern\u00b7hard", "dich", "/", "O", "K\u00f6\u00b7nig", "!", "wol", "ge\u00b7ro\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NE", "PPER", "$(", "NE", "NN", "$.", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.761": {"text": "Da\u00df man bi\u00df diesen Tag von deines Feindes Knochen", "tokens": ["Da\u00df", "man", "bi\u00df", "die\u00b7sen", "Tag", "von", "dei\u00b7nes", "Fein\u00b7des", "Kno\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "PDAT", "NN", "APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.762": {"text": "Das Feld bestreuet siht. Was jauchtzet man zu Gath/", "tokens": ["Das", "Feld", "be\u00b7streu\u00b7et", "siht", ".", "Was", "jaucht\u00b7zet", "man", "zu", "Gath", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVFIN", "$.", "PWS", "VVFIN", "PIS", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.763": {"text": "Was h\u00fcpfet Ascalon/ Philister eure Stadt", "tokens": ["Was", "h\u00fcp\u00b7fet", "A\u00b7sca\u00b7lon", "/", "Phi\u00b7lis\u00b7ter", "eu\u00b7re", "Stadt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "NE", "$(", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.764": {"text": "Da\u00df dieser Josua das Leben hat verlohren/", "tokens": ["Da\u00df", "die\u00b7ser", "Jo\u00b7sua", "das", "Le\u00b7ben", "hat", "ver\u00b7loh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "NE", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.765": {"text": "Der unserm Jsrael zum Helffer war erkohren!", "tokens": ["Der", "un\u00b7serm", "Js\u00b7rael", "zum", "Helf\u00b7fer", "war", "er\u00b7koh\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "APPRART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.766": {"text": "Bez\u00e4umet euren Mund und jauchtzet nicht zu sehr/", "tokens": ["Be\u00b7z\u00e4u\u00b7met", "eu\u00b7ren", "Mund", "und", "jaucht\u00b7zet", "nicht", "zu", "sehr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "KON", "VVFIN", "PTKNEG", "PTKA", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.767": {"text": "Dann Pappenheim ligt auch/ so ist auch euer Heer", "tokens": ["Dann", "Pap\u00b7pen\u00b7heim", "ligt", "auch", "/", "so", "ist", "auch", "eu\u00b7er", "Heer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "VVFIN", "ADV", "$(", "ADV", "VAFIN", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.768": {"text": "Zur schn\u00f6den Flucht gebracht/ ist schon Gustav gestorben/", "tokens": ["Zur", "schn\u00f6\u00b7den", "Flucht", "ge\u00b7bracht", "/", "ist", "schon", "Gus\u00b7tav", "ge\u00b7stor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVPP", "$(", "VAFIN", "ADV", "NE", "VVPP", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.769": {"text": "So ist doch Bernhard noch zum Streiten unverdorben/", "tokens": ["So", "ist", "doch", "Bern\u00b7hard", "noch", "zum", "Strei\u00b7ten", "un\u00b7ver\u00b7dor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "NE", "ADV", "APPRART", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.770": {"text": "und andre Helden mehr. Was euch der Himmel droht/", "tokens": ["und", "and\u00b7re", "Hel\u00b7den", "mehr", ".", "Was", "euch", "der", "Him\u00b7mel", "droht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "$.", "PWS", "PPER", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.771": {"text": "Das f\u00fchrt er auch wolaus/ ist schon Gustavus todt.", "tokens": ["Das", "f\u00fchrt", "er", "auch", "wo\u00b7laus", "/", "ist", "schon", "Gus\u00b7ta\u00b7vus", "todt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "$(", "VAFIN", "ADV", "NE", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.772": {"text": "Man lese/ wo man dich von vielen Kugel-Wunden/", "tokens": ["Man", "le\u00b7se", "/", "wo", "man", "dich", "von", "vie\u00b7len", "Ku\u00b7gel\u00b7Wun\u00b7den", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$(", "PWAV", "PIS", "PRF", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.773": {"text": "Gustavus/ theurer Held/ entseelet hat gefunden:", "tokens": ["Gus\u00b7ta\u00b7vus", "/", "theu\u00b7rer", "Held", "/", "ent\u00b7see\u00b7let", "hat", "ge\u00b7fun\u00b7den", ":"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$(", "ADJD", "NN", "$(", "VVPP", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.774": {"text": "Hier hat der Schweden H\u00e4upt f\u00fcr Deutschland seinen Geist", "tokens": ["Hier", "hat", "der", "Schwe\u00b7den", "H\u00e4upt", "f\u00fcr", "Deutschland", "sei\u00b7nen", "Geist"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "APPR", "NE", "PPOSAT", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.775": {"text": "Geopfert/ den GOtt liebt/ den alle Nach-Welt preist.", "tokens": ["Ge\u00b7o\u00b7pfert", "/", "den", "Gott", "liebt", "/", "den", "al\u00b7le", "Nach\u00b7Welt", "preist", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "ART", "NN", "VVFIN", "$(", "ART", "PIAT", "NN", "VVFIN", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}}}}}