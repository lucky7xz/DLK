{"textgrid.poem.63370": {"metadata": {"author": {"name": "Klabund", "birth": "N.A.", "death": "N.A."}, "title": "Ewige Ostern", "genre": "verse", "period": "N.A.", "pub_year": 1909, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Als sie warfen Gott in Banden,", "tokens": ["Als", "sie", "war\u00b7fen", "Gott", "in", "Ban\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als sie ihn ans Kreuz geschlagen,", "tokens": ["Als", "sie", "ihn", "ans", "Kreuz", "ge\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist der Herr nach dreien Tagen", "tokens": ["Ist", "der", "Herr", "nach", "drei\u00b7en", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "APPR", "CARD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Auferstanden.", "tokens": ["Auf\u00b7er\u00b7stan\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Felder dorren. Nebel feuchten.", "tokens": ["Fel\u00b7der", "dor\u00b7ren", ".", "Ne\u00b7bel", "feuch\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVINF", "$.", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie auch hart der Winter w\u00fcte:", "tokens": ["Wie", "auch", "hart", "der", "Win\u00b7ter", "w\u00fc\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einst wird wieder Bl\u00fct' bei Bl\u00fcte", "tokens": ["Einst", "wird", "wie\u00b7der", "Bl\u00fct'", "bei", "Bl\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV", "NN", "APPR", "NN"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.4": {"text": "Leuchten.", "tokens": ["Leuch\u00b7ten", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.3": {"line.1": {"text": "Ganz Europa brach in Tr\u00fcmmer,", "tokens": ["Ganz", "Eu\u00b7ro\u00b7pa", "brach", "in", "Tr\u00fcm\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und an Deutschland fri\u00dft der Geier, \u2013", "tokens": ["Und", "an", "Deutschland", "fri\u00dft", "der", "Gei\u00b7er", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "NE", "VVFIN", "ART", "NN", "$,", "$("], "meter": "-+---+-", "measure": "dactylic.init"}, "line.3": {"text": "Doch der Frigga heiliger Schleier", "tokens": ["Doch", "der", "Frig\u00b7ga", "hei\u00b7li\u00b7ger", "Schlei\u00b7er"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NE", "ADJA", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Weht noch immer.", "tokens": ["Weht", "noch", "im\u00b7mer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "$."], "meter": "+-++", "measure": "unknown.measure.tri"}}, "stanza.4": {"line.1": {"text": "Leben, Liebe, Lenz und Lieder:", "tokens": ["Le\u00b7ben", ",", "Lie\u00b7be", ",", "Lenz", "und", "Lie\u00b7der", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit der Erde mag's vergehen.", "tokens": ["Mit", "der", "Er\u00b7de", "mag's", "ver\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf dem n\u00e4chsten Sterne sehen", "tokens": ["Auf", "dem", "n\u00e4chs\u00b7ten", "Ster\u00b7ne", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wir uns wieder.", "tokens": ["Wir", "uns", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PPER", "ADV", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Als sie warfen Gott in Banden,", "tokens": ["Als", "sie", "war\u00b7fen", "Gott", "in", "Ban\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als sie ihn ans Kreuz geschlagen,", "tokens": ["Als", "sie", "ihn", "ans", "Kreuz", "ge\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist der Herr nach dreien Tagen", "tokens": ["Ist", "der", "Herr", "nach", "drei\u00b7en", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "APPR", "CARD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Auferstanden.", "tokens": ["Auf\u00b7er\u00b7stan\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Felder dorren. Nebel feuchten.", "tokens": ["Fel\u00b7der", "dor\u00b7ren", ".", "Ne\u00b7bel", "feuch\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVINF", "$.", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie auch hart der Winter w\u00fcte:", "tokens": ["Wie", "auch", "hart", "der", "Win\u00b7ter", "w\u00fc\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einst wird wieder Bl\u00fct' bei Bl\u00fcte", "tokens": ["Einst", "wird", "wie\u00b7der", "Bl\u00fct'", "bei", "Bl\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV", "NN", "APPR", "NN"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.4": {"text": "Leuchten.", "tokens": ["Leuch\u00b7ten", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.7": {"line.1": {"text": "Ganz Europa brach in Tr\u00fcmmer,", "tokens": ["Ganz", "Eu\u00b7ro\u00b7pa", "brach", "in", "Tr\u00fcm\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und an Deutschland fri\u00dft der Geier, \u2013", "tokens": ["Und", "an", "Deutschland", "fri\u00dft", "der", "Gei\u00b7er", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "NE", "VVFIN", "ART", "NN", "$,", "$("], "meter": "-+---+-", "measure": "dactylic.init"}, "line.3": {"text": "Doch der Frigga heiliger Schleier", "tokens": ["Doch", "der", "Frig\u00b7ga", "hei\u00b7li\u00b7ger", "Schlei\u00b7er"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NE", "ADJA", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Weht noch immer.", "tokens": ["Weht", "noch", "im\u00b7mer", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "$."], "meter": "+-++", "measure": "unknown.measure.tri"}}, "stanza.8": {"line.1": {"text": "Leben, Liebe, Lenz und Lieder:", "tokens": ["Le\u00b7ben", ",", "Lie\u00b7be", ",", "Lenz", "und", "Lie\u00b7der", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit der Erde mag's vergehen.", "tokens": ["Mit", "der", "Er\u00b7de", "mag's", "ver\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf dem n\u00e4chsten Sterne sehen", "tokens": ["Auf", "dem", "n\u00e4chs\u00b7ten", "Ster\u00b7ne", "se\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wir uns wieder.", "tokens": ["Wir", "uns", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PPER", "ADV", "$."], "meter": "+-+-", "measure": "trochaic.di"}}}}}