{"textgrid.poem.36985": {"metadata": {"author": {"name": "Gla\u00dfbrenner, Adolf", "birth": "N.A.", "death": "N.A."}, "title": "Der Brieftr\u00e4ger", "genre": "verse", "period": "N.A.", "pub_year": 1843, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Meine Lotte, die Gr\u00e4fin, war eben dabei", "tokens": ["Mei\u00b7ne", "Lot\u00b7te", ",", "die", "Gr\u00e4\u00b7fin", ",", "war", "e\u00b7ben", "da\u00b7bei"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NE", "$,", "ART", "NN", "$,", "VAFIN", "ADV", "PAV"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Zu schl\u00fcrfen vom Mokka drei Tassen,", "tokens": ["Zu", "schl\u00fcr\u00b7fen", "vom", "Mok\u00b7ka", "drei", "Tas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPRART", "NE", "CARD", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Um mir zum Fr\u00fchst\u00fcck den w\u00fcrzigen Rauch", "tokens": ["Um", "mir", "zum", "Fr\u00fch\u00b7st\u00fcck", "den", "w\u00fcr\u00b7zi\u00b7gen", "Rauch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PPER", "APPRART", "NN", "ART", "ADJA", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "In die Nase ziehen zu lassen:", "tokens": ["In", "die", "Na\u00b7se", "zie\u00b7hen", "zu", "las\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "PTKZU", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Da rief es von drau\u00dfen schon wieder: \u00bbHerein!\u00ab", "tokens": ["Da", "rief", "es", "von", "drau\u00b7\u00dfen", "schon", "wie\u00b7der", ":", "\u00bb", "Her\u00b7ein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADV", "ADV", "ADV", "$.", "$(", "PTKVZ", "$.", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Und es trat, als von Lotten geklopft war,", "tokens": ["Und", "es", "trat", ",", "als", "von", "Lot\u00b7ten", "ge\u00b7klopft", "war", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "KOUS", "APPR", "NN", "VVPP", "VAFIN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Der Brieftr\u00e4ger ein, ein knieschlotternder Greis,", "tokens": ["Der", "Brief\u00b7tr\u00e4\u00b7ger", "ein", ",", "ein", "knie\u00b7schlot\u00b7tern\u00b7der", "Greis", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Der gepudert, toupirt und bezopft war.", "tokens": ["Der", "ge\u00b7pu\u00b7dert", ",", "tou\u00b7pirt", "und", "be\u00b7zopft", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVPP", "$,", "VVPP", "KON", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Er setzte sich nieder und suchte sodann", "tokens": ["Er", "setz\u00b7te", "sich", "nie\u00b7der", "und", "such\u00b7te", "so\u00b7dann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "PTKVZ", "KON", "VVFIN", "ADV"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Unter Aechzen und Athemgeschnappe,", "tokens": ["Un\u00b7ter", "A\u00b7ech\u00b7zen", "und", "At\u00b7hem\u00b7ge\u00b7schnap\u00b7pe", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.3": {"text": "Einen Brief, dessen Siegel zerbrochen schon war,", "tokens": ["Ei\u00b7nen", "Brief", ",", "des\u00b7sen", "Sie\u00b7gel", "zer\u00b7bro\u00b7chen", "schon", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELAT", "NN", "VVPP", "ADV", "VAFIN", "$,"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Langsam hervor aus der Mappe.", "tokens": ["Lang\u00b7sam", "her\u00b7vor", "aus", "der", "Map\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "APPR", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.4": {"line.1": {"text": "\u00bbdrei kupferne Pim ... Pampels Porto ... u ... und,\u00ab", "tokens": ["\u00bb", "drei", "kup\u00b7fer\u00b7ne", "Pim", "...", "Pam\u00b7pels", "Por\u00b7to", "...", "u", "...", "und", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "CARD", "ADJA", "NN", "$(", "NE", "NE", "$(", "NE", "$(", "KON", "$,", "$("], "meter": "+-+-++-++--", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Sprach er stotternd, ihn mir \u00fcberreichend,", "tokens": ["Sprach", "er", "stot\u00b7ternd", ",", "ihn", "mir", "\u00fc\u00b7berr\u00b7ei\u00b7chend", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "$,", "PPER", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "\u00bbund zehn f\u00fcr die Durchsicht der Regier ... gier ... gierung!\u00ab", "tokens": ["\u00bb", "und", "zehn", "f\u00fcr", "die", "Durch\u00b7sicht", "der", "Re\u00b7gier", "...", "gier", "...", "gie\u00b7rung", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "KON", "CARD", "APPR", "ART", "NN", "ART", "NN", "$(", "VVIMP", "$(", "VVIMP", "$.", "$("], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.4": {"text": "Versetzte er hustend und keuchend.", "tokens": ["Ver\u00b7setz\u00b7te", "er", "hus\u00b7tend", "und", "keu\u00b7chend", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Ich zahlte, wogegen so freundlich er war,", "tokens": ["Ich", "zahl\u00b7te", ",", "wo\u00b7ge\u00b7gen", "so", "freund\u00b7lich", "er", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "ADV", "ADJD", "PPER", "VAFIN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mein Erstaunen in etwas zu mildern", "tokens": ["Mein", "Er\u00b7stau\u00b7nen", "in", "et\u00b7was", "zu", "mil\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "APPR", "PIS", "PTKZU", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Und das, was despotische Willk\u00fchr mir schien,", "tokens": ["Und", "das", ",", "was", "des\u00b7po\u00b7ti\u00b7sche", "Will\u00b7k\u00fchr", "mir", "schien", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "$,", "PWS", "ADJA", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Als sittliche Ordnung zu schildern.", "tokens": ["Als", "sitt\u00b7li\u00b7che", "Ord\u00b7nung", "zu", "schil\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.6": {"line.1": {"text": "\u00bbdie Regier ... ung erachtet die Oeff.. entlichkeit,", "tokens": ["\u00bb", "die", "Re\u00b7gier", "...", "ung", "e\u00b7rach\u00b7tet", "die", "Oeff", "..", "ent\u00b7lich\u00b7keit", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ART", "NN", "$(", "NN", "VVFIN", "ART", "NN", "$.", "PTKVZ", "$,"], "meter": "-+---+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "So weit sie ihr nutzbar, f f f\u00fcr heilig,", "tokens": ["So", "weit", "sie", "ihr", "nutz\u00b7bar", ",", "f", "f", "f\u00fcr", "hei\u00b7lig", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPER", "ADJD", "$,", "NE", "NE", "APPR", "ADJD", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Den geheimen Verkehr zwischen B\u00fcrgern a ... a ...", "tokens": ["Den", "ge\u00b7hei\u00b7men", "Ver\u00b7kehr", "zwi\u00b7schen", "B\u00fcr\u00b7gern", "a", "...", "a", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "NE", "$(", "NE", "$("], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "A ... aber f\u00fcr sehr na ... nachtheilig.", "tokens": ["A", "...", "a\u00b7ber", "f\u00fcr", "sehr", "na", "...", "nacht\u00b7hei\u00b7lig", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "ADV", "APPR", "ADV", "ITJ", "$(", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Sie mu ... mu ... mu\u00df immer in Kenntni\u00df von dem,", "tokens": ["Sie", "mu", "...", "mu", "...", "mu\u00df", "im\u00b7mer", "in", "Kennt\u00b7ni\u00df", "von", "dem", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$(", "XY", "$(", "VMFIN", "ADV", "APPR", "NN", "APPR", "ART", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Was No ... Noth thu ... thu ... thu ... thut thut ... bleiben,", "tokens": ["Was", "No", "...", "Noth", "thu", "...", "thu", "...", "thu", "...", "thut", "thut", "...", "blei\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "FM", "$(", "NN", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "VVFIN", "$(", "VVINF", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Und darum A ... A ... Alles wi ... wissen, wa ... was", "tokens": ["Und", "da\u00b7rum", "A", "...", "A", "...", "Al\u00b7les", "wi", "...", "wis\u00b7sen", ",", "wa", "...", "was"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["KON", "PAV", "NE", "$(", "NE", "$(", "PIS", "KOKOM", "$(", "VVINF", "$,", "XY", "$(", "PWS"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Die Burr ... B\u00fcrger dede ... denken und treiben.", "tokens": ["Die", "Burr", "...", "B\u00fcr\u00b7ger", "de\u00b7de", "...", "den\u00b7ken", "und", "trei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "NN", "VVFIN", "$(", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Das Po ... Po ... Po ... Porto ist billig, der Pppreis", "tokens": ["Das", "Po", "...", "Po", "...", "Po", "...", "Por\u00b7to", "ist", "bil\u00b7lig", ",", "der", "Pppreis"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "VAFIN", "ADJD", "$,", "ART", "NN"], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "F\u00fcr's D.. Durchlesen allerdings theuer,", "tokens": ["F\u00fcr's", "D.", ".", "Durch\u00b7le\u00b7sen", "al\u00b7ler\u00b7dings", "theu\u00b7er", ","], "token_info": ["word", "abbreviation", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$.", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Doch ist dies eine weh ... wenig dr\u00fcckende und", "tokens": ["Doch", "ist", "dies", "ei\u00b7ne", "weh", "...", "we\u00b7nig", "dr\u00fc\u00b7cken\u00b7de", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VAFIN", "PDS", "ART", "ADV", "$(", "PIAT", "ADJA", "KON"], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Mu ... Moralisch wirr ... wirkende Steuer.", "tokens": ["Mu", "...", "Mo\u00b7ra\u00b7lisch", "wirr", "...", "wir\u00b7ken\u00b7de", "Steu\u00b7er", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$(", "NN", "ADJD", "$(", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "Sie h ... emmt den geheimen und geist'gen Verkehr,", "tokens": ["Sie", "h", "...", "emmt", "den", "ge\u00b7hei\u00b7men", "und", "geist'\u00b7gen", "Ver\u00b7kehr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "VVFIN", "ART", "ADJA", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "So Bei ... Beide im Dienst sind beim Satan;", "tokens": ["So", "Bei", "...", "Bei\u00b7de", "im", "Dienst", "sind", "beim", "Sa\u00b7tan", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "$(", "PIS", "APPRART", "NN", "VAFIN", "APPRART", "NN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Sie h\u00e4lt die Burr ... B\u00fcrger vom Schrei ... Schreiben ab,", "tokens": ["Sie", "h\u00e4lt", "die", "Burr", "...", "B\u00fcr\u00b7ger", "vom", "Schrei", "...", "Schrei\u00b7ben", "ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$(", "NN", "APPRART", "NN", "$(", "NN", "PTKVZ", "$,"], "meter": "-+--+-+++-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und spie ... spornt sie zur Arbeit und That an.\u00ab \u2013", "tokens": ["Und", "spie", "...", "spornt", "sie", "zur", "Ar\u00b7beit", "und", "That", "an", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "$(", "VVFIN", "PPER", "APPRART", "NN", "KON", "NN", "PTKVZ", "$.", "$(", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "\u00bbdie h\u00f6here Dummheit, die sich offenbart", "tokens": ["\u00bb", "die", "h\u00f6\u00b7he\u00b7re", "Dumm\u00b7heit", ",", "die", "sich", "of\u00b7fen\u00b7bart"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PRF", "ADJD"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "In all solchem Schu- und Verriegeln,", "tokens": ["In", "all", "sol\u00b7chem", "Schu", "und", "Ver\u00b7rie\u00b7geln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "TRUNC", "KON", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Anerkenne ich gern, doch warum trotzdem", "tokens": ["An\u00b7er\u00b7ken\u00b7ne", "ich", "gern", ",", "doch", "wa\u00b7rum", "trotz\u00b7dem"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "PPER", "ADV", "$,", "KON", "PWAV", "PAV"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ihre Briefe die Leute noch ", "tokens": ["Ih\u00b7re", "Brie\u00b7fe", "die", "Leu\u00b7te", "noch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ART", "NN", "ADV"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.11": {"line.1": {"text": "\u00bbdas m\u00fcssen,\u00ab unterbrach mich der Alte, \u00bbsie thun", "tokens": ["\u00bb", "das", "m\u00fcs\u00b7sen", ",", "\u00ab", "un\u00b7ter\u00b7brach", "mich", "der", "Al\u00b7te", ",", "\u00bb", "sie", "thun"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["$(", "PDS", "VMFIN", "$,", "$(", "VVFIN", "PPER", "ART", "NN", "$,", "$(", "PPER", "VVINF"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Weil's zum fi ... na ... na ... nanziellen Wohl ist", "tokens": ["Weil's", "zum", "fi", "...", "na", "...", "na", "...", "nan\u00b7zi\u00b7el\u00b7len", "Wohl", "ist"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "APPRART", "NE", "$(", "ITJ", "$(", "ITJ", "$(", "VVFIN", "ADV", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Des Sult ... ha ... ha ... han's, weil der Siegel la ... la ...", "tokens": ["Des", "Sult", "...", "ha", "...", "ha", "...", "han's", ",", "weil", "der", "Sie\u00b7gel", "la", "...", "la", "..."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "$,", "KOUS", "ART", "NN", "NE", "$(", "FM", "$("], "meter": "-+--+--++-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "La ... Lack sein Monopo ... pol ist.\u00ab", "tokens": ["La", "...", "Lack", "sein", "Mo\u00b7no\u00b7po", "...", "pol", "ist", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$(", "NN", "PPOSAT", "NN", "$(", "NE", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Nach diesem Gestottere stotterte er", "tokens": ["Nach", "die\u00b7sem", "Ge\u00b7stot\u00b7te\u00b7re", "stot\u00b7ter\u00b7te", "er"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ein Adieu noch unwirsch und verdrie\u00dflich,", "tokens": ["Ein", "A\u00b7die\u00b7u", "noch", "un\u00b7wirsch", "und", "ver\u00b7drie\u00df\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und schlotterte Kniee und wackelte Zopf", "tokens": ["Und", "schlot\u00b7ter\u00b7te", "Kni\u00b7ee", "und", "wa\u00b7ckel\u00b7te", "Zopf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "KON", "ADJA", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und hustete stark und verlie\u00df mich.", "tokens": ["Und", "hus\u00b7te\u00b7te", "stark", "und", "ver\u00b7lie\u00df", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "KON", "VVFIN", "PPER", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.13": {"line.1": {"text": "Ich aber ich, konnte nunmehr das Billet,", "tokens": ["Ich", "a\u00b7ber", "ich", ",", "konn\u00b7te", "nun\u00b7mehr", "das", "Bil\u00b7let", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PPER", "$,", "VMFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Das staatsrevidirte, entfalten,", "tokens": ["Das", "staats\u00b7re\u00b7vi\u00b7dir\u00b7te", ",", "ent\u00b7fal\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und will meinen irdischen Lesern nicht ", "tokens": ["Und", "will", "mei\u00b7nen", "ir\u00b7di\u00b7schen", "Le\u00b7sern", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "PPOSAT", "ADJA", "NN", "PTKNEG"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "L\u00e4nger das, was dasselbe-", "tokens": ["L\u00e4n\u00b7ger", "das", ",", "was", "das\u00b7sel\u00b7be"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "PDS", "$,", "PWS", "TRUNC"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.14": {"line.1": {"text": "Meine Lotte, die Gr\u00e4fin, war eben dabei", "tokens": ["Mei\u00b7ne", "Lot\u00b7te", ",", "die", "Gr\u00e4\u00b7fin", ",", "war", "e\u00b7ben", "da\u00b7bei"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NE", "$,", "ART", "NN", "$,", "VAFIN", "ADV", "PAV"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Zu schl\u00fcrfen vom Mokka drei Tassen,", "tokens": ["Zu", "schl\u00fcr\u00b7fen", "vom", "Mok\u00b7ka", "drei", "Tas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPRART", "NE", "CARD", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Um mir zum Fr\u00fchst\u00fcck den w\u00fcrzigen Rauch", "tokens": ["Um", "mir", "zum", "Fr\u00fch\u00b7st\u00fcck", "den", "w\u00fcr\u00b7zi\u00b7gen", "Rauch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PPER", "APPRART", "NN", "ART", "ADJA", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "In die Nase ziehen zu lassen:", "tokens": ["In", "die", "Na\u00b7se", "zie\u00b7hen", "zu", "las\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "PTKZU", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.15": {"line.1": {"text": "Da rief es von drau\u00dfen schon wieder: \u00bbHerein!\u00ab", "tokens": ["Da", "rief", "es", "von", "drau\u00b7\u00dfen", "schon", "wie\u00b7der", ":", "\u00bb", "Her\u00b7ein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADV", "ADV", "ADV", "$.", "$(", "PTKVZ", "$.", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Und es trat, als von Lotten geklopft war,", "tokens": ["Und", "es", "trat", ",", "als", "von", "Lot\u00b7ten", "ge\u00b7klopft", "war", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "KOUS", "APPR", "NN", "VVPP", "VAFIN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Der Brieftr\u00e4ger ein, ein knieschlotternder Greis,", "tokens": ["Der", "Brief\u00b7tr\u00e4\u00b7ger", "ein", ",", "ein", "knie\u00b7schlot\u00b7tern\u00b7der", "Greis", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Der gepudert, toupirt und bezopft war.", "tokens": ["Der", "ge\u00b7pu\u00b7dert", ",", "tou\u00b7pirt", "und", "be\u00b7zopft", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVPP", "$,", "VVPP", "KON", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.16": {"line.1": {"text": "Er setzte sich nieder und suchte sodann", "tokens": ["Er", "setz\u00b7te", "sich", "nie\u00b7der", "und", "such\u00b7te", "so\u00b7dann"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "PTKVZ", "KON", "VVFIN", "ADV"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Unter Aechzen und Athemgeschnappe,", "tokens": ["Un\u00b7ter", "A\u00b7ech\u00b7zen", "und", "At\u00b7hem\u00b7ge\u00b7schnap\u00b7pe", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.3": {"text": "Einen Brief, dessen Siegel zerbrochen schon war,", "tokens": ["Ei\u00b7nen", "Brief", ",", "des\u00b7sen", "Sie\u00b7gel", "zer\u00b7bro\u00b7chen", "schon", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELAT", "NN", "VVPP", "ADV", "VAFIN", "$,"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Langsam hervor aus der Mappe.", "tokens": ["Lang\u00b7sam", "her\u00b7vor", "aus", "der", "Map\u00b7pe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "APPR", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.17": {"line.1": {"text": "\u00bbdrei kupferne Pim ... Pampels Porto ... u ... und,\u00ab", "tokens": ["\u00bb", "drei", "kup\u00b7fer\u00b7ne", "Pim", "...", "Pam\u00b7pels", "Por\u00b7to", "...", "u", "...", "und", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "CARD", "ADJA", "NN", "$(", "NE", "NE", "$(", "NE", "$(", "KON", "$,", "$("], "meter": "+-+-++-++--", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Sprach er stotternd, ihn mir \u00fcberreichend,", "tokens": ["Sprach", "er", "stot\u00b7ternd", ",", "ihn", "mir", "\u00fc\u00b7berr\u00b7ei\u00b7chend", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "$,", "PPER", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "\u00bbund zehn f\u00fcr die Durchsicht der Regier ... gier ... gierung!\u00ab", "tokens": ["\u00bb", "und", "zehn", "f\u00fcr", "die", "Durch\u00b7sicht", "der", "Re\u00b7gier", "...", "gier", "...", "gie\u00b7rung", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "KON", "CARD", "APPR", "ART", "NN", "ART", "NN", "$(", "VVIMP", "$(", "VVIMP", "$.", "$("], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.4": {"text": "Versetzte er hustend und keuchend.", "tokens": ["Ver\u00b7setz\u00b7te", "er", "hus\u00b7tend", "und", "keu\u00b7chend", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.18": {"line.1": {"text": "Ich zahlte, wogegen so freundlich er war,", "tokens": ["Ich", "zahl\u00b7te", ",", "wo\u00b7ge\u00b7gen", "so", "freund\u00b7lich", "er", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "ADV", "ADJD", "PPER", "VAFIN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mein Erstaunen in etwas zu mildern", "tokens": ["Mein", "Er\u00b7stau\u00b7nen", "in", "et\u00b7was", "zu", "mil\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "APPR", "PIS", "PTKZU", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Und das, was despotische Willk\u00fchr mir schien,", "tokens": ["Und", "das", ",", "was", "des\u00b7po\u00b7ti\u00b7sche", "Will\u00b7k\u00fchr", "mir", "schien", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "$,", "PWS", "ADJA", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Als sittliche Ordnung zu schildern.", "tokens": ["Als", "sitt\u00b7li\u00b7che", "Ord\u00b7nung", "zu", "schil\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.19": {"line.1": {"text": "\u00bbdie Regier ... ung erachtet die Oeff.. entlichkeit,", "tokens": ["\u00bb", "die", "Re\u00b7gier", "...", "ung", "e\u00b7rach\u00b7tet", "die", "Oeff", "..", "ent\u00b7lich\u00b7keit", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ART", "NN", "$(", "NN", "VVFIN", "ART", "NN", "$.", "PTKVZ", "$,"], "meter": "-+---+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "So weit sie ihr nutzbar, f f f\u00fcr heilig,", "tokens": ["So", "weit", "sie", "ihr", "nutz\u00b7bar", ",", "f", "f", "f\u00fcr", "hei\u00b7lig", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "PPER", "ADJD", "$,", "NE", "NE", "APPR", "ADJD", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Den geheimen Verkehr zwischen B\u00fcrgern a ... a ...", "tokens": ["Den", "ge\u00b7hei\u00b7men", "Ver\u00b7kehr", "zwi\u00b7schen", "B\u00fcr\u00b7gern", "a", "...", "a", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "NE", "$(", "NE", "$("], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "A ... aber f\u00fcr sehr na ... nachtheilig.", "tokens": ["A", "...", "a\u00b7ber", "f\u00fcr", "sehr", "na", "...", "nacht\u00b7hei\u00b7lig", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "ADV", "APPR", "ADV", "ITJ", "$(", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Sie mu ... mu ... mu\u00df immer in Kenntni\u00df von dem,", "tokens": ["Sie", "mu", "...", "mu", "...", "mu\u00df", "im\u00b7mer", "in", "Kennt\u00b7ni\u00df", "von", "dem", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$(", "XY", "$(", "VMFIN", "ADV", "APPR", "NN", "APPR", "ART", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Was No ... Noth thu ... thu ... thu ... thut thut ... bleiben,", "tokens": ["Was", "No", "...", "Noth", "thu", "...", "thu", "...", "thu", "...", "thut", "thut", "...", "blei\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "FM", "$(", "NN", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "VVFIN", "$(", "VVINF", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Und darum A ... A ... Alles wi ... wissen, wa ... was", "tokens": ["Und", "da\u00b7rum", "A", "...", "A", "...", "Al\u00b7les", "wi", "...", "wis\u00b7sen", ",", "wa", "...", "was"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["KON", "PAV", "NE", "$(", "NE", "$(", "PIS", "KOKOM", "$(", "VVINF", "$,", "XY", "$(", "PWS"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Die Burr ... B\u00fcrger dede ... denken und treiben.", "tokens": ["Die", "Burr", "...", "B\u00fcr\u00b7ger", "de\u00b7de", "...", "den\u00b7ken", "und", "trei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "NN", "VVFIN", "$(", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.21": {"line.1": {"text": "Das Po ... Po ... Po ... Porto ist billig, der Pppreis", "tokens": ["Das", "Po", "...", "Po", "...", "Po", "...", "Por\u00b7to", "ist", "bil\u00b7lig", ",", "der", "Pppreis"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "VAFIN", "ADJD", "$,", "ART", "NN"], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "F\u00fcr's D.. Durchlesen allerdings theuer,", "tokens": ["F\u00fcr's", "D.", ".", "Durch\u00b7le\u00b7sen", "al\u00b7ler\u00b7dings", "theu\u00b7er", ","], "token_info": ["word", "abbreviation", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$.", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Doch ist dies eine weh ... wenig dr\u00fcckende und", "tokens": ["Doch", "ist", "dies", "ei\u00b7ne", "weh", "...", "we\u00b7nig", "dr\u00fc\u00b7cken\u00b7de", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VAFIN", "PDS", "ART", "ADV", "$(", "PIAT", "ADJA", "KON"], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Mu ... Moralisch wirr ... wirkende Steuer.", "tokens": ["Mu", "...", "Mo\u00b7ra\u00b7lisch", "wirr", "...", "wir\u00b7ken\u00b7de", "Steu\u00b7er", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$(", "NN", "ADJD", "$(", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.22": {"line.1": {"text": "Sie h ... emmt den geheimen und geist'gen Verkehr,", "tokens": ["Sie", "h", "...", "emmt", "den", "ge\u00b7hei\u00b7men", "und", "geist'\u00b7gen", "Ver\u00b7kehr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "VVFIN", "ART", "ADJA", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "So Bei ... Beide im Dienst sind beim Satan;", "tokens": ["So", "Bei", "...", "Bei\u00b7de", "im", "Dienst", "sind", "beim", "Sa\u00b7tan", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "$(", "PIS", "APPRART", "NN", "VAFIN", "APPRART", "NN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Sie h\u00e4lt die Burr ... B\u00fcrger vom Schrei ... Schreiben ab,", "tokens": ["Sie", "h\u00e4lt", "die", "Burr", "...", "B\u00fcr\u00b7ger", "vom", "Schrei", "...", "Schrei\u00b7ben", "ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$(", "NN", "APPRART", "NN", "$(", "NN", "PTKVZ", "$,"], "meter": "-+--+-+++-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und spie ... spornt sie zur Arbeit und That an.\u00ab \u2013", "tokens": ["Und", "spie", "...", "spornt", "sie", "zur", "Ar\u00b7beit", "und", "That", "an", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "$(", "VVFIN", "PPER", "APPRART", "NN", "KON", "NN", "PTKVZ", "$.", "$(", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.23": {"line.1": {"text": "\u00bbdie h\u00f6here Dummheit, die sich offenbart", "tokens": ["\u00bb", "die", "h\u00f6\u00b7he\u00b7re", "Dumm\u00b7heit", ",", "die", "sich", "of\u00b7fen\u00b7bart"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "$,", "PRELS", "PRF", "ADJD"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "In all solchem Schu- und Verriegeln,", "tokens": ["In", "all", "sol\u00b7chem", "Schu", "und", "Ver\u00b7rie\u00b7geln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "TRUNC", "KON", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Anerkenne ich gern, doch warum trotzdem", "tokens": ["An\u00b7er\u00b7ken\u00b7ne", "ich", "gern", ",", "doch", "wa\u00b7rum", "trotz\u00b7dem"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "PPER", "ADV", "$,", "KON", "PWAV", "PAV"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ihre Briefe die Leute noch ", "tokens": ["Ih\u00b7re", "Brie\u00b7fe", "die", "Leu\u00b7te", "noch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ART", "NN", "ADV"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.24": {"line.1": {"text": "\u00bbdas m\u00fcssen,\u00ab unterbrach mich der Alte, \u00bbsie thun", "tokens": ["\u00bb", "das", "m\u00fcs\u00b7sen", ",", "\u00ab", "un\u00b7ter\u00b7brach", "mich", "der", "Al\u00b7te", ",", "\u00bb", "sie", "thun"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["$(", "PDS", "VMFIN", "$,", "$(", "VVFIN", "PPER", "ART", "NN", "$,", "$(", "PPER", "VVINF"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Weil's zum fi ... na ... na ... nanziellen Wohl ist", "tokens": ["Weil's", "zum", "fi", "...", "na", "...", "na", "...", "nan\u00b7zi\u00b7el\u00b7len", "Wohl", "ist"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "APPRART", "NE", "$(", "ITJ", "$(", "ITJ", "$(", "VVFIN", "ADV", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Des Sult ... ha ... ha ... han's, weil der Siegel la ... la ...", "tokens": ["Des", "Sult", "...", "ha", "...", "ha", "...", "han's", ",", "weil", "der", "Sie\u00b7gel", "la", "...", "la", "..."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "$,", "KOUS", "ART", "NN", "NE", "$(", "FM", "$("], "meter": "-+--+--++-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "La ... Lack sein Monopo ... pol ist.\u00ab", "tokens": ["La", "...", "Lack", "sein", "Mo\u00b7no\u00b7po", "...", "pol", "ist", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$(", "NN", "PPOSAT", "NN", "$(", "NE", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Nach diesem Gestottere stotterte er", "tokens": ["Nach", "die\u00b7sem", "Ge\u00b7stot\u00b7te\u00b7re", "stot\u00b7ter\u00b7te", "er"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ein Adieu noch unwirsch und verdrie\u00dflich,", "tokens": ["Ein", "A\u00b7die\u00b7u", "noch", "un\u00b7wirsch", "und", "ver\u00b7drie\u00df\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und schlotterte Kniee und wackelte Zopf", "tokens": ["Und", "schlot\u00b7ter\u00b7te", "Kni\u00b7ee", "und", "wa\u00b7ckel\u00b7te", "Zopf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "KON", "ADJA", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und hustete stark und verlie\u00df mich.", "tokens": ["Und", "hus\u00b7te\u00b7te", "stark", "und", "ver\u00b7lie\u00df", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "KON", "VVFIN", "PPER", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.26": {"line.1": {"text": "Ich aber ich, konnte nunmehr das Billet,", "tokens": ["Ich", "a\u00b7ber", "ich", ",", "konn\u00b7te", "nun\u00b7mehr", "das", "Bil\u00b7let", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PPER", "$,", "VMFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Das staatsrevidirte, entfalten,", "tokens": ["Das", "staats\u00b7re\u00b7vi\u00b7dir\u00b7te", ",", "ent\u00b7fal\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und will meinen irdischen Lesern nicht ", "tokens": ["Und", "will", "mei\u00b7nen", "ir\u00b7di\u00b7schen", "Le\u00b7sern", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "PPOSAT", "ADJA", "NN", "PTKNEG"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "L\u00e4nger das, was dasselbe-", "tokens": ["L\u00e4n\u00b7ger", "das", ",", "was", "das\u00b7sel\u00b7be"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "PDS", "$,", "PWS", "TRUNC"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}}}}