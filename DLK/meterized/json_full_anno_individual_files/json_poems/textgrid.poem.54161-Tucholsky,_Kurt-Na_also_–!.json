{"textgrid.poem.54161": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Na also \u2013!", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der alte Kahl, ordensbesternt,", "tokens": ["Der", "al\u00b7te", "Kahl", ",", "or\u00b7dens\u00b7bes\u00b7ternt", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geheimrat und so, hat umgelernt.", "tokens": ["Ge\u00b7heim\u00b7rat", "und", "so", ",", "hat", "um\u00b7ge\u00b7lernt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "KON", "ADV", "$,", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Er hat einen ganzen Hinrichtungsakt", "tokens": ["Er", "hat", "ei\u00b7nen", "gan\u00b7zen", "Hin\u00b7rich\u00b7tungs\u00b7akt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "gesehn \u2013 der Kopf wurde abgehackt.", "tokens": ["ge\u00b7sehn", "\u2013", "der", "Kopf", "wur\u00b7de", "ab\u00b7ge\u00b7hackt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und Geheimrat Kahl schrieb juristisch und k\u00fchl:", "tokens": ["Und", "Ge\u00b7heim\u00b7rat", "Kahl", "schrieb", "ju\u00b7ris\u00b7tisch", "und", "k\u00fchl", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.6": {"text": "\u00bbdas ist gut f\u00fcr das Gerechtigkeitsgef\u00fchl.", "tokens": ["\u00bb", "das", "ist", "gut", "f\u00fcr", "das", "Ge\u00b7rech\u00b7tig\u00b7keits\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Allemal.\u00ab", "tokens": ["Al\u00b7le\u00b7mal", ".", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["ADV", "$.", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "(gez.) Kahl", "tokens": ["(", "ge\u00b7z.", ")", "Kahl"], "token_info": ["punct", "abbreviation", "punct", "word"], "pos": ["$(", "NE", "$(", "NN"], "meter": "+", "measure": "single.up"}}, "stanza.2": {"line.1": {"text": "Dann hat der Mann an Einsicht gewonnen,", "tokens": ["Dann", "hat", "der", "Mann", "an", "Ein\u00b7sicht", "ge\u00b7won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "hat nachgedacht und sich besonnen.", "tokens": ["hat", "nach\u00b7ge\u00b7dacht", "und", "sich", "be\u00b7son\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "KON", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und er sprach und schrieb, wo es auch sei:", "tokens": ["Und", "er", "sprach", "und", "schrieb", ",", "wo", "es", "auch", "sei", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "KON", "VVFIN", "$,", "PWAV", "PPER", "ADV", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "eine Hinrichtung ist eine Barbarei.", "tokens": ["ei\u00b7ne", "Hin\u00b7rich\u00b7tung", "ist", "ei\u00b7ne", "Bar\u00b7ba\u00b7rei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Ein zweiter Mord. Zu gar nichts n\u00fctze.", "tokens": ["Ein", "zwei\u00b7ter", "Mord", ".", "Zu", "gar", "nichts", "n\u00fct\u00b7ze", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "APPR", "ADV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Justiz gedeiht nicht in blutiger Pf\u00fctze.", "tokens": ["Jus\u00b7tiz", "ge\u00b7deiht", "nicht", "in", "blu\u00b7ti\u00b7ger", "Pf\u00fct\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKNEG", "APPR", "ADJA", "NN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.7": {"text": "Ein braver Mann sprach im Reichstagssaal.", "tokens": ["Ein", "bra\u00b7ver", "Mann", "sprach", "im", "Reichs\u00b7tags\u00b7saal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Kahl.", "tokens": ["Kahl", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+", "measure": "single.up"}}, "stanza.3": {"line.1": {"text": "Darauf haben die Nazis ihn angegriffen.", "tokens": ["Da\u00b7rauf", "ha\u00b7ben", "die", "Na\u00b7zis", "ihn", "an\u00b7ge\u00b7grif\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Darauf haben die Stammtische auf ihn gepfiffen.", "tokens": ["Da\u00b7rauf", "ha\u00b7ben", "die", "Stamm\u00b7ti\u00b7sche", "auf", "ihn", "ge\u00b7pfif\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+---+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und jetzt auf einmal, ein neuer Ton", "tokens": ["Und", "jetzt", "auf", "ein\u00b7mal", ",", "ein", "neu\u00b7er", "Ton"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADV", "$,", "ART", "ADJA", "NN"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "ert\u00f6nt in der Reichstagskommission:", "tokens": ["er\u00b7t\u00f6nt", "in", "der", "Reichs\u00b7tags\u00b7kom\u00b7mis\u00b7si\u00b7on", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "\u00bbwir brauchen die Todesstrafe, zur Zeit!", "tokens": ["\u00bb", "wir", "brau\u00b7chen", "die", "To\u00b7des\u00b7stra\u00b7fe", ",", "zur", "Zeit", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "NN", "$,", "APPRART", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Insonderheit im politischen Streit!", "tokens": ["In\u00b7son\u00b7der\u00b7heit", "im", "po\u00b7li\u00b7ti\u00b7schen", "Streit", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Humanit\u00e4t in allen Ehren \u2013", "tokens": ["Hu\u00b7ma\u00b7ni\u00b7t\u00e4t", "in", "al\u00b7len", "Eh\u00b7ren", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "wir k\u00f6nnen den Hackklotz nicht entbehren.\u00ab", "tokens": ["wir", "k\u00f6n\u00b7nen", "den", "Hack\u00b7klotz", "nicht", "ent\u00b7beh\u00b7ren", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "(wir verurteilen bekanntlich nach dieser Methode", "tokens": ["(", "wir", "ver\u00b7ur\u00b7tei\u00b7len", "be\u00b7kannt\u00b7lich", "nach", "die\u00b7ser", "Me\u00b7tho\u00b7de"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VVFIN", "ADV", "APPR", "PDAT", "NN"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}, "line.10": {"text": "alle Nazi-M\u00f6rder zum Tode.)", "tokens": ["al\u00b7le", "Na\u00b7zi\u00b7M\u00f6r\u00b7der", "zum", "To\u00b7de", ".", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PIAT", "NN", "APPRART", "NN", "$.", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.11": {"text": "\u00bbheraus mit dem Beil!. Die Waage bleibt drin.", "tokens": ["\u00bb", "he\u00b7raus", "mit", "dem", "Beil", "!", ".", "Die", "Waa\u00b7ge", "bleibt", "drin", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "$.", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "----+-+--+", "measure": "iambic.tri.chol"}, "line.12": {"text": "Richtet sie nicht! Richtet sie hin!\u00ab", "tokens": ["Rich\u00b7tet", "sie", "nicht", "!", "Rich\u00b7tet", "sie", "hin", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.13": {"text": "Na also \u2013! Da hat in bewegten Stunden", "tokens": ["Na", "al\u00b7so", "\u2013", "!", "Da", "hat", "in", "be\u00b7weg\u00b7ten", "Stun\u00b7den"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "ADV", "$(", "$.", "ADV", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "ein deutscher Professor heimgefunden.", "tokens": ["ein", "deut\u00b7scher", "Pro\u00b7fes\u00b7sor", "heim\u00b7ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Christus s\u00e4te. Es wuchs nicht viel.", "tokens": ["Chris\u00b7tus", "s\u00e4\u00b7te", ".", "Es", "wuchs", "nicht", "viel", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$.", "PPER", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.16": {"text": "Rode aus die Pfl\u00e4nzchen mit Stumpf und Stiel!", "tokens": ["Ro\u00b7de", "aus", "die", "Pfl\u00e4nz\u00b7chen", "mit", "Stumpf", "und", "Stiel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.17": {"text": "Das christliche Feld bleibt allemal", "tokens": ["Das", "christ\u00b7li\u00b7che", "Feld", "bleibt", "al\u00b7le\u00b7mal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "kahl.", "tokens": ["kahl", "."], "token_info": ["word", "punct"], "pos": ["ADJD", "$."], "meter": "+", "measure": "single.up"}}, "stanza.4": {"line.1": {"text": "Der alte Kahl, ordensbesternt,", "tokens": ["Der", "al\u00b7te", "Kahl", ",", "or\u00b7dens\u00b7bes\u00b7ternt", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geheimrat und so, hat umgelernt.", "tokens": ["Ge\u00b7heim\u00b7rat", "und", "so", ",", "hat", "um\u00b7ge\u00b7lernt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "KON", "ADV", "$,", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Er hat einen ganzen Hinrichtungsakt", "tokens": ["Er", "hat", "ei\u00b7nen", "gan\u00b7zen", "Hin\u00b7rich\u00b7tungs\u00b7akt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "gesehn \u2013 der Kopf wurde abgehackt.", "tokens": ["ge\u00b7sehn", "\u2013", "der", "Kopf", "wur\u00b7de", "ab\u00b7ge\u00b7hackt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und Geheimrat Kahl schrieb juristisch und k\u00fchl:", "tokens": ["Und", "Ge\u00b7heim\u00b7rat", "Kahl", "schrieb", "ju\u00b7ris\u00b7tisch", "und", "k\u00fchl", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.6": {"text": "\u00bbdas ist gut f\u00fcr das Gerechtigkeitsgef\u00fchl.", "tokens": ["\u00bb", "das", "ist", "gut", "f\u00fcr", "das", "Ge\u00b7rech\u00b7tig\u00b7keits\u00b7ge\u00b7f\u00fchl", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VAFIN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Allemal.\u00ab", "tokens": ["Al\u00b7le\u00b7mal", ".", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["ADV", "$.", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "(gez.) Kahl", "tokens": ["(", "ge\u00b7z.", ")", "Kahl"], "token_info": ["punct", "abbreviation", "punct", "word"], "pos": ["$(", "NE", "$(", "NN"], "meter": "+", "measure": "single.up"}}, "stanza.5": {"line.1": {"text": "Dann hat der Mann an Einsicht gewonnen,", "tokens": ["Dann", "hat", "der", "Mann", "an", "Ein\u00b7sicht", "ge\u00b7won\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "hat nachgedacht und sich besonnen.", "tokens": ["hat", "nach\u00b7ge\u00b7dacht", "und", "sich", "be\u00b7son\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "KON", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und er sprach und schrieb, wo es auch sei:", "tokens": ["Und", "er", "sprach", "und", "schrieb", ",", "wo", "es", "auch", "sei", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "KON", "VVFIN", "$,", "PWAV", "PPER", "ADV", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "eine Hinrichtung ist eine Barbarei.", "tokens": ["ei\u00b7ne", "Hin\u00b7rich\u00b7tung", "ist", "ei\u00b7ne", "Bar\u00b7ba\u00b7rei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$."], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Ein zweiter Mord. Zu gar nichts n\u00fctze.", "tokens": ["Ein", "zwei\u00b7ter", "Mord", ".", "Zu", "gar", "nichts", "n\u00fct\u00b7ze", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "APPR", "ADV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Justiz gedeiht nicht in blutiger Pf\u00fctze.", "tokens": ["Jus\u00b7tiz", "ge\u00b7deiht", "nicht", "in", "blu\u00b7ti\u00b7ger", "Pf\u00fct\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKNEG", "APPR", "ADJA", "NN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.7": {"text": "Ein braver Mann sprach im Reichstagssaal.", "tokens": ["Ein", "bra\u00b7ver", "Mann", "sprach", "im", "Reichs\u00b7tags\u00b7saal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Kahl.", "tokens": ["Kahl", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+", "measure": "single.up"}}, "stanza.6": {"line.1": {"text": "Darauf haben die Nazis ihn angegriffen.", "tokens": ["Da\u00b7rauf", "ha\u00b7ben", "die", "Na\u00b7zis", "ihn", "an\u00b7ge\u00b7grif\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "PPER", "VVPP", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Darauf haben die Stammtische auf ihn gepfiffen.", "tokens": ["Da\u00b7rauf", "ha\u00b7ben", "die", "Stamm\u00b7ti\u00b7sche", "auf", "ihn", "ge\u00b7pfif\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+---+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und jetzt auf einmal, ein neuer Ton", "tokens": ["Und", "jetzt", "auf", "ein\u00b7mal", ",", "ein", "neu\u00b7er", "Ton"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADV", "$,", "ART", "ADJA", "NN"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "ert\u00f6nt in der Reichstagskommission:", "tokens": ["er\u00b7t\u00f6nt", "in", "der", "Reichs\u00b7tags\u00b7kom\u00b7mis\u00b7si\u00b7on", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "\u00bbwir brauchen die Todesstrafe, zur Zeit!", "tokens": ["\u00bb", "wir", "brau\u00b7chen", "die", "To\u00b7des\u00b7stra\u00b7fe", ",", "zur", "Zeit", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "NN", "$,", "APPRART", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Insonderheit im politischen Streit!", "tokens": ["In\u00b7son\u00b7der\u00b7heit", "im", "po\u00b7li\u00b7ti\u00b7schen", "Streit", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Humanit\u00e4t in allen Ehren \u2013", "tokens": ["Hu\u00b7ma\u00b7ni\u00b7t\u00e4t", "in", "al\u00b7len", "Eh\u00b7ren", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "wir k\u00f6nnen den Hackklotz nicht entbehren.\u00ab", "tokens": ["wir", "k\u00f6n\u00b7nen", "den", "Hack\u00b7klotz", "nicht", "ent\u00b7beh\u00b7ren", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "(wir verurteilen bekanntlich nach dieser Methode", "tokens": ["(", "wir", "ver\u00b7ur\u00b7tei\u00b7len", "be\u00b7kannt\u00b7lich", "nach", "die\u00b7ser", "Me\u00b7tho\u00b7de"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VVFIN", "ADV", "APPR", "PDAT", "NN"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}, "line.10": {"text": "alle Nazi-M\u00f6rder zum Tode.)", "tokens": ["al\u00b7le", "Na\u00b7zi\u00b7M\u00f6r\u00b7der", "zum", "To\u00b7de", ".", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PIAT", "NN", "APPRART", "NN", "$.", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.11": {"text": "\u00bbheraus mit dem Beil!. Die Waage bleibt drin.", "tokens": ["\u00bb", "he\u00b7raus", "mit", "dem", "Beil", "!", ".", "Die", "Waa\u00b7ge", "bleibt", "drin", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "$.", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "----+-+--+", "measure": "iambic.tri.chol"}, "line.12": {"text": "Richtet sie nicht! Richtet sie hin!\u00ab", "tokens": ["Rich\u00b7tet", "sie", "nicht", "!", "Rich\u00b7tet", "sie", "hin", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$.", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.13": {"text": "Na also \u2013! Da hat in bewegten Stunden", "tokens": ["Na", "al\u00b7so", "\u2013", "!", "Da", "hat", "in", "be\u00b7weg\u00b7ten", "Stun\u00b7den"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "ADV", "$(", "$.", "ADV", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "ein deutscher Professor heimgefunden.", "tokens": ["ein", "deut\u00b7scher", "Pro\u00b7fes\u00b7sor", "heim\u00b7ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "Christus s\u00e4te. Es wuchs nicht viel.", "tokens": ["Chris\u00b7tus", "s\u00e4\u00b7te", ".", "Es", "wuchs", "nicht", "viel", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$.", "PPER", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.16": {"text": "Rode aus die Pfl\u00e4nzchen mit Stumpf und Stiel!", "tokens": ["Ro\u00b7de", "aus", "die", "Pfl\u00e4nz\u00b7chen", "mit", "Stumpf", "und", "Stiel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.17": {"text": "Das christliche Feld bleibt allemal", "tokens": ["Das", "christ\u00b7li\u00b7che", "Feld", "bleibt", "al\u00b7le\u00b7mal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "kahl.", "tokens": ["kahl", "."], "token_info": ["word", "punct"], "pos": ["ADJD", "$."], "meter": "+", "measure": "single.up"}}}}}