{"textgrid.poem.60055": {"metadata": {"author": {"name": "Jacobi, Johann Georg", "birth": "N.A.", "death": "N.A."}, "title": "1L: Leiser nannt' ich deinen Nahmen", "genre": "verse", "period": "N.A.", "pub_year": 1777, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Leiser nannt' ich deinen Nahmen", "tokens": ["Lei\u00b7ser", "nannt'", "ich", "dei\u00b7nen", "Nah\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und mein Auge warb um dich:", "tokens": ["Und", "mein", "Au\u00b7ge", "warb", "um", "dich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Liebe Chloe! n\u00e4her kamen", "tokens": ["Lie\u00b7be", "Chloe", "!", "n\u00e4\u00b7her", "ka\u00b7men"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "NE", "$.", "ADJD", "VVFIN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Unser beyder Herzen sich.", "tokens": ["Un\u00b7ser", "bey\u00b7der", "Her\u00b7zen", "sich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PIAT", "NN", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Und du nanntest ", "tokens": ["Und", "du", "nann\u00b7test"], "token_info": ["word", "word", "word"], "pos": ["KON", "PPER", "VVFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.2": {"text": "Hoffen lie\u00df dein Auge mich:", "tokens": ["Hof\u00b7fen", "lie\u00df", "dein", "Au\u00b7ge", "mich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Liebe Chloe! n\u00e4her kamen", "tokens": ["Lie\u00b7be", "Chloe", "!", "n\u00e4\u00b7her", "ka\u00b7men"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "NE", "$.", "ADJD", "VVFIN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Unser beyder Lippen sich.", "tokens": ["Un\u00b7ser", "bey\u00b7der", "Lip\u00b7pen", "sich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PIAT", "NN", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "O, es war ein s\u00fc\u00dfes Neigen;", "tokens": ["O", ",", "es", "war", "ein", "s\u00fc\u00b7\u00dfes", "Nei\u00b7gen", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bis wir endlich, Mund an Mund,", "tokens": ["Bis", "wir", "end\u00b7lich", ",", "Mund", "an", "Mund", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Fest uns hielten, ohne Zeugen:", "tokens": ["Fest", "uns", "hiel\u00b7ten", ",", "oh\u00b7ne", "Zeu\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "$,", "KOUI", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und geschlossen war der Bund.", "tokens": ["Und", "ge\u00b7schlos\u00b7sen", "war", "der", "Bund", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Leiser nannt' ich deinen Nahmen", "tokens": ["Lei\u00b7ser", "nannt'", "ich", "dei\u00b7nen", "Nah\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und mein Auge warb um dich:", "tokens": ["Und", "mein", "Au\u00b7ge", "warb", "um", "dich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Liebe Chloe! n\u00e4her kamen", "tokens": ["Lie\u00b7be", "Chloe", "!", "n\u00e4\u00b7her", "ka\u00b7men"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "NE", "$.", "ADJD", "VVFIN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Unser beyder Herzen sich.", "tokens": ["Un\u00b7ser", "bey\u00b7der", "Her\u00b7zen", "sich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PIAT", "NN", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Und du nanntest ", "tokens": ["Und", "du", "nann\u00b7test"], "token_info": ["word", "word", "word"], "pos": ["KON", "PPER", "VVFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.2": {"text": "Hoffen lie\u00df dein Auge mich:", "tokens": ["Hof\u00b7fen", "lie\u00df", "dein", "Au\u00b7ge", "mich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Liebe Chloe! n\u00e4her kamen", "tokens": ["Lie\u00b7be", "Chloe", "!", "n\u00e4\u00b7her", "ka\u00b7men"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "NE", "$.", "ADJD", "VVFIN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Unser beyder Lippen sich.", "tokens": ["Un\u00b7ser", "bey\u00b7der", "Lip\u00b7pen", "sich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PIAT", "NN", "PRF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "O, es war ein s\u00fc\u00dfes Neigen;", "tokens": ["O", ",", "es", "war", "ein", "s\u00fc\u00b7\u00dfes", "Nei\u00b7gen", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bis wir endlich, Mund an Mund,", "tokens": ["Bis", "wir", "end\u00b7lich", ",", "Mund", "an", "Mund", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Fest uns hielten, ohne Zeugen:", "tokens": ["Fest", "uns", "hiel\u00b7ten", ",", "oh\u00b7ne", "Zeu\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "$,", "KOUI", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und geschlossen war der Bund.", "tokens": ["Und", "ge\u00b7schlos\u00b7sen", "war", "der", "Bund", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}