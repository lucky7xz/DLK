{"textgrid.poem.49607": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Lilly", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sie stammte wohl aus Hamburgs Mauern,", "tokens": ["Sie", "stamm\u00b7te", "wohl", "aus", "Ham\u00b7burgs", "Mau\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NE", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das dorten an der Elbe liegt,", "tokens": ["Das", "dor\u00b7ten", "an", "der", "El\u00b7be", "liegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPR", "ART", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und hat zu mancher Leut' Bedauern", "tokens": ["Und", "hat", "zu", "man\u00b7cher", "Leut'", "Be\u00b7dau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "APPR", "PIAT", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In M\u00fcnchen hier ein Kind gekriegt.", "tokens": ["In", "M\u00fcn\u00b7chen", "hier", "ein", "Kind", "ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Die Mutter als gebor'ne Holle", "tokens": ["Die", "Mut\u00b7ter", "als", "ge\u00b7bor'\u00b7ne", "Hol\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verm\u00e4hlte sich mit Menk &amp; Sohn;", "tokens": ["Ver\u00b7m\u00e4hl\u00b7te", "sich", "mit", "Menk", "&amp;", "Sohn", ";"], "token_info": ["word", "word", "word", "word", "XML_entity", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NN", "NE", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Er handelte en gros in Wolle,", "tokens": ["Er", "han\u00b7del\u00b7te", "en", "gros", "in", "Wol\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "ADJD", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Lilly war das Kind davon.", "tokens": ["Und", "Lil\u00b7ly", "war", "das", "Kind", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Bemerkt sei, da\u00df der Elternvater", "tokens": ["Be\u00b7merkt", "sei", ",", "da\u00df", "der", "El\u00b7tern\u00b7va\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "\u2013 Und zwar derjen'ge mutterseits \u2013", "tokens": ["\u2013", "Und", "zwar", "der\u00b7jen'\u00b7ge", "mut\u00b7ter\u00b7seits", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "PDS", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch mitregierte als Senater", "tokens": ["Auch", "mit\u00b7re\u00b7gier\u00b7te", "als", "Se\u00b7na\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vor siebzig Jahren schon bereits.", "tokens": ["Vor", "sieb\u00b7zig", "Jah\u00b7ren", "schon", "be\u00b7reits", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "In einer solchen Geldfamil'che", "tokens": ["In", "ei\u00b7ner", "sol\u00b7chen", "Geld\u00b7fa\u00b7mil'\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kann nur der Anstand heimisch sein;", "tokens": ["Kann", "nur", "der", "An\u00b7stand", "hei\u00b7misch", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man zieht ihn mit der Muttermilche", "tokens": ["Man", "zieht", "ihn", "mit", "der", "Mut\u00b7ter\u00b7mil\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als selbstverst\u00e4ndlich mit hinein.", "tokens": ["Als", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich", "mit", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Es war nun Lilly auch in Liebe", "tokens": ["Es", "war", "nun", "Lil\u00b7ly", "auch", "in", "Lie\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "NE", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zur sch\u00f6nheitsreichen Kunst entbrannt,", "tokens": ["Zur", "sch\u00f6n\u00b7heits\u00b7rei\u00b7chen", "Kunst", "ent\u00b7brannt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sie entwickelte die Triebe", "tokens": ["Und", "sie", "ent\u00b7wi\u00b7ckel\u00b7te", "die", "Trie\u00b7be"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Teils \u00f6lgemalt, teils angewandt.", "tokens": ["Teils", "\u00f6l\u00b7ge\u00b7malt", ",", "teils", "an\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "In solchen F\u00e4llen des Talentes", "tokens": ["In", "sol\u00b7chen", "F\u00e4l\u00b7len", "des", "Ta\u00b7len\u00b7tes"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zieht alle Welt nach M\u00fcnchen her,", "tokens": ["Zieht", "al\u00b7le", "Welt", "nach", "M\u00fcn\u00b7chen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum Studium des Ornamentes,", "tokens": ["Zum", "Stu\u00b7di\u00b7um", "des", "Or\u00b7na\u00b7men\u00b7tes", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zur Kunst im Handwerk in die Lehr'.", "tokens": ["Zur", "Kunst", "im", "Hand\u00b7werk", "in", "die", "Lehr'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Auch Lilly Menk war angekommen", "tokens": ["Auch", "Lil\u00b7ly", "Menk", "war", "an\u00b7ge\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Voll Eifer und Bemalungssucht.", "tokens": ["Voll", "Ei\u00b7fer", "und", "Be\u00b7ma\u00b7lungs\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie hat ein Ende es genommen", "tokens": ["Wie", "hat", "ein", "En\u00b7de", "es", "ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "ART", "NN", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit illegaler Leibesfrucht?", "tokens": ["Mit", "il\u00b7le\u00b7ga\u00b7ler", "Lei\u00b7bes\u00b7frucht", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wenn man 'nem Kind das Beste bietet,", "tokens": ["Wenn", "man", "'nem", "Kind", "das", "Bes\u00b7te", "bie\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann glaubt man, es wird keusch und klug;", "tokens": ["Dann", "glaubt", "man", ",", "es", "wird", "keusch", "und", "klug", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "PPER", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Doch Lilly hat sich eingemietet", "tokens": ["Doch", "Lil\u00b7ly", "hat", "sich", "ein\u00b7ge\u00b7mie\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "PRF", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In Schwabing, und das sagt genug.", "tokens": ["In", "Schwa\u00b7bing", ",", "und", "das", "sagt", "ge\u00b7nug", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "KON", "PDS", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Hier ging sie zu dem Malprofesser,", "tokens": ["Hier", "ging", "sie", "zu", "dem", "Mal\u00b7pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo sie den Geist der Kunst erfuhr,", "tokens": ["Wo", "sie", "den", "Geist", "der", "Kunst", "er\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das Stilgef\u00fchl als Sch\u00f6nheitsmesser,", "tokens": ["Das", "Stil\u00b7ge\u00b7f\u00fchl", "als", "Sch\u00f6n\u00b7heits\u00b7mes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Ohrenschneckenhaarfrisur.", "tokens": ["Die", "Oh\u00b7ren\u00b7schne\u00b7cken\u00b7haar\u00b7fri\u00b7sur", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Auch sonst begann sie sich zu \u00e4ndern,", "tokens": ["Auch", "sonst", "be\u00b7gann", "sie", "sich", "zu", "\u00e4n\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als an der Freiheit sie genippt,", "tokens": ["Als", "an", "der", "Frei\u00b7heit", "sie", "ge\u00b7nippt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie ging jetzt in Reformgew\u00e4ndern,", "tokens": ["Sie", "ging", "jetzt", "in", "Re\u00b7form\u00b7ge\u00b7w\u00e4n\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+++-+-", "measure": "unknown.measure.penta"}, "line.4": {"text": "In denen leicht der Busen schwippt.", "tokens": ["In", "de\u00b7nen", "leicht", "der", "Bu\u00b7sen", "schwippt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Und mit den andern Kunstbefliss'nen", "tokens": ["Und", "mit", "den", "an\u00b7dern", "Kunst\u00b7be\u00b7fliss'\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Versank sie tiefer in den Sumpf,", "tokens": ["Ver\u00b7sank", "sie", "tie\u00b7fer", "in", "den", "Sumpf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ging \u00f6fter aus mit 'nem zerriss'nen", "tokens": ["Ging", "\u00f6f\u00b7ter", "aus", "mit", "'nem", "zer\u00b7riss'\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und durchgebrochnen Seidenstrumpf.", "tokens": ["Und", "durch\u00b7ge\u00b7broch\u00b7nen", "Sei\u00b7dens\u00b7trumpf", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Sie trug mit gr\u00f6\u00dfter Seelenruhe,", "tokens": ["Sie", "trug", "mit", "gr\u00f6\u00df\u00b7ter", "See\u00b7len\u00b7ru\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Noch eh' ein Vierteljahr verging,", "tokens": ["Noch", "eh'", "ein", "Vier\u00b7tel\u00b7jahr", "ver\u00b7ging", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die abgelatschten Kn\u00f6pfelschuhe", "tokens": ["Die", "ab\u00b7ge\u00b7latschten", "Kn\u00f6p\u00b7fel\u00b7schu\u00b7he"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und achtete es ganz gering.", "tokens": ["Und", "ach\u00b7te\u00b7te", "es", "ganz", "ge\u00b7ring", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ein Weib verliert den Grundcharakter,", "tokens": ["Ein", "Weib", "ver\u00b7liert", "den", "Grund\u00b7cha\u00b7rak\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn es den Ordnungssinn verliert;", "tokens": ["Wenn", "es", "den", "Ord\u00b7nungs\u00b7sinn", "ver\u00b7liert", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Tugend scheint ihm abgeschmackter,", "tokens": ["Die", "Tu\u00b7gend", "scheint", "ihm", "ab\u00b7ge\u00b7schmack\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein fester Halt wird demoliert.", "tokens": ["Sein", "fes\u00b7ter", "Halt", "wird", "de\u00b7mo\u00b7liert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}}, "stanza.14": {"line.1": {"text": "Man sieht es bald ins Laster h\u00fcpfen", "tokens": ["Man", "sieht", "es", "bald", "ins", "Las\u00b7ter", "h\u00fcp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "APPRART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit einem k\u00fchnen Sprunggelenk.", "tokens": ["Mit", "ei\u00b7nem", "k\u00fch\u00b7nen", "Sprung\u00b7ge\u00b7lenk", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nun lasset mich den Schleier l\u00fcpfen", "tokens": ["Nun", "las\u00b7set", "mich", "den", "Schlei\u00b7er", "l\u00fcp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von unsrer armen Lilly Menk!", "tokens": ["Von", "uns\u00b7rer", "ar\u00b7men", "Lil\u00b7ly", "Menk", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NE", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Sie nahm sich Atelier und Zimmer", "tokens": ["Sie", "nahm", "sich", "A\u00b7te\u00b7lier", "und", "Zim\u00b7mer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Im vierten Stock mit eigner T\u00fcr,", "tokens": ["Im", "vier\u00b7ten", "Stock", "mit", "eig\u00b7ner", "T\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da gibt Gelegenheit sich immer", "tokens": ["Da", "gibt", "Ge\u00b7le\u00b7gen\u00b7heit", "sich", "im\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NN", "PRF", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zu der und jener Ungeb\u00fchr.", "tokens": ["Zu", "der", "und", "je\u00b7ner", "Un\u00b7ge\u00b7b\u00fchr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "KON", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Erst wu\u00dfte sie wohl selbstverst\u00e4ndlich,", "tokens": ["Erst", "wu\u00df\u00b7te", "sie", "wohl", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sie aus Hamburg war, es nicht:", "tokens": ["Da", "sie", "aus", "Ham\u00b7burg", "war", ",", "es", "nicht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NE", "VAFIN", "$,", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "In dieser Stadt ist unabwendlich", "tokens": ["In", "die\u00b7ser", "Stadt", "ist", "un\u00b7ab\u00b7wend\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Keuschheit eine Lebenspflicht.", "tokens": ["Die", "Keuschheit", "ei\u00b7ne", "Le\u00b7bens\u00b7pflicht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.17": {"line.1": {"text": "In M\u00fcnchen ist es nicht dasselbe,", "tokens": ["In", "M\u00fcn\u00b7chen", "ist", "es", "nicht", "das\u00b7sel\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAFIN", "PPER", "PTKNEG", "PDAT", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hier kann man vieles eher tun", "tokens": ["Hier", "kann", "man", "vie\u00b7les", "e\u00b7her", "tun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "PIS", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Als wie in Hamburg an der Elbe", "tokens": ["Als", "wie", "in", "Ham\u00b7burg", "an", "der", "El\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "APPR", "NE", "APPR", "ART", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als unerfahr'nes dummes Huhn.", "tokens": ["Als", "un\u00b7er\u00b7fahr'\u00b7nes", "dum\u00b7mes", "Huhn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Es war gerad' in jenen Tagen,", "tokens": ["Es", "war", "ge\u00b7rad'", "in", "je\u00b7nen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sich der Karneval erhob,", "tokens": ["Da", "sich", "der", "Kar\u00b7ne\u00b7val", "er\u00b7hob", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wo das Vergn\u00fcgen sozusagen", "tokens": ["Wo", "das", "Ver\u00b7gn\u00fc\u00b7gen", "so\u00b7zu\u00b7sa\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sich in die M\u00e4dchenherzen schob.", "tokens": ["Sich", "in", "die", "M\u00e4d\u00b7chen\u00b7her\u00b7zen", "schob", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Redouten, B\u00e4lle, K\u00fcnstlerfeste,", "tokens": ["Re\u00b7dou\u00b7ten", ",", "B\u00e4l\u00b7le", ",", "K\u00fcnst\u00b7ler\u00b7fes\u00b7te", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Bal par\u00e9 noch obendrein,", "tokens": ["Der", "Bal", "pa\u00b7r\u00e9", "noch", "o\u00b7ben\u00b7drein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie kann dagegen selbst die Beste", "tokens": ["Wie", "kann", "da\u00b7ge\u00b7gen", "selbst", "die", "Bes\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VMFIN", "PAV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Keuscheste gepanzert sein?", "tokens": ["Und", "Keu\u00b7sches\u00b7te", "ge\u00b7pan\u00b7zert", "sein", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Nicht weit von ihr wohnt' ein Schlawiner,", "tokens": ["Nicht", "weit", "von", "ihr", "wohnt'", "ein", "Schla\u00b7wi\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "APPR", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ganz ohne Geld und Broterwerb,", "tokens": ["Ganz", "oh\u00b7ne", "Geld", "und", "Bro\u00b7ter\u00b7werb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein Vater wirkte als Rabbiner,", "tokens": ["Sein", "Va\u00b7ter", "wirk\u00b7te", "als", "Rab\u00b7bi\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er selbst war nichts als blo\u00df ein Serb'.", "tokens": ["Er", "selbst", "war", "nichts", "als", "blo\u00df", "ein", "Serb'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "PIS", "KOKOM", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Doch trug er lange schwarze Haare", "tokens": ["Doch", "trug", "er", "lan\u00b7ge", "schwar\u00b7ze", "Haa\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und eingeschmiert mit Nierenfett.", "tokens": ["Und", "ein\u00b7ge\u00b7schmiert", "mit", "Nie\u00b7ren\u00b7fett", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein M\u00e4dchen sieht darin das Wahre", "tokens": ["Ein", "M\u00e4d\u00b7chen", "sieht", "da\u00b7rin", "das", "Wah\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PAV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und findet es auch wundernett.", "tokens": ["Und", "fin\u00b7det", "es", "auch", "wun\u00b7der\u00b7nett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Sein Angesicht war nicht gewaschen,", "tokens": ["Sein", "An\u00b7ge\u00b7sicht", "war", "nicht", "ge\u00b7wa\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch lag darin ein stiller Schmerz;", "tokens": ["Doch", "lag", "da\u00b7rin", "ein", "stil\u00b7ler", "Schmerz", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der kam von leeren Hosentaschen", "tokens": ["Der", "kam", "von", "lee\u00b7ren", "Ho\u00b7sen\u00b7ta\u00b7schen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und r\u00fchrte jedes Frauenherz.", "tokens": ["Und", "r\u00fchr\u00b7te", "je\u00b7des", "Frau\u00b7en\u00b7herz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Man mu\u00df dazu aus Hamburg stammen", "tokens": ["Man", "mu\u00df", "da\u00b7zu", "aus", "Ham\u00b7burg", "stam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PAV", "APPR", "NE", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und unverstand'nes M\u00e4dchen sein,", "tokens": ["Und", "un\u00b7ver\u00b7stan\u00b7d'\u00b7nes", "M\u00e4d\u00b7chen", "sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAINF", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Dann steht man gleich in hellen Flammen", "tokens": ["Dann", "steht", "man", "gleich", "in", "hel\u00b7len", "Flam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr ein Schlawinermoschusschwein.", "tokens": ["F\u00fcr", "ein", "Schla\u00b7wi\u00b7ner\u00b7mo\u00b7schus\u00b7schwein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.24": {"line.1": {"text": "Wenn nur die Reinlichkeiten fehlen,", "tokens": ["Wenn", "nur", "die", "Rein\u00b7lich\u00b7kei\u00b7ten", "feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was liegt der Malerin daran?", "tokens": ["Was", "liegt", "der", "Ma\u00b7le\u00b7rin", "da\u00b7ran", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr hochgestimmte K\u00fcnstlerseelen", "tokens": ["F\u00fcr", "hoch\u00b7ges\u00b7timm\u00b7te", "K\u00fcnst\u00b7ler\u00b7see\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Seife blo\u00df ein leerer Wahn.", "tokens": ["Ist", "Sei\u00b7fe", "blo\u00df", "ein", "lee\u00b7rer", "Wahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Nach diesem hier Vorausgeschickten", "tokens": ["Nach", "die\u00b7sem", "hier", "Vor\u00b7aus\u00b7ge\u00b7schick\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADV", "NN"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Will ich bemerken, da\u00df sie sich", "tokens": ["Will", "ich", "be\u00b7mer\u00b7ken", ",", "da\u00df", "sie", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "PPER", "VVINF", "$,", "KOUS", "PPER", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum erstenmal ins Auge blickten", "tokens": ["Zum", "ers\u00b7ten\u00b7mal", "ins", "Au\u00b7ge", "blick\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADV", "APPRART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Klarinett und Geigenstrich.", "tokens": ["Bei", "Kla\u00b7ri\u00b7nett", "und", "Gei\u00b7gen\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Bei einem K\u00fcnstlerlumpenballe", "tokens": ["Bei", "ei\u00b7nem", "K\u00fcnst\u00b7ler\u00b7lum\u00b7pen\u00b7bal\u00b7le"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ergab sich dieses Resultat,", "tokens": ["Er\u00b7gab", "sich", "die\u00b7ses", "Re\u00b7sul\u00b7tat", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df Lilly Menk in ihrem Falle", "tokens": ["Da\u00df", "Lil\u00b7ly", "Menk", "in", "ih\u00b7rem", "Fal\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vom Unschuldspfad danebentrat.", "tokens": ["Vom", "Un\u00b7schuld\u00b7spfad", "da\u00b7ne\u00b7bent\u00b7rat", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.27": {"line.1": {"text": "Ach Gott! Man kann im gro\u00dfen ganzen", "tokens": ["Ach", "Gott", "!", "Man", "kann", "im", "gro\u00b7\u00dfen", "gan\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "NN", "$.", "PIS", "VMFIN", "APPRART", "ADJA", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die armen M\u00e4dchen schon verstehn,", "tokens": ["Die", "ar\u00b7men", "M\u00e4d\u00b7chen", "schon", "ver\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn die Prinzipien beim Tanzen", "tokens": ["Wenn", "die", "Prin\u00b7zi\u00b7pi\u00b7en", "beim", "Tan\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "APPRART", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Bei ihnen aus dem Leime gehn.", "tokens": ["Bei", "ih\u00b7nen", "aus", "dem", "Lei\u00b7me", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Das junge Blut mu\u00df sich erhitzen,", "tokens": ["Das", "jun\u00b7ge", "Blut", "mu\u00df", "sich", "er\u00b7hit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Herz ist sowieso entbl\u00f6\u00dft,", "tokens": ["Das", "Herz", "ist", "so\u00b7wi\u00b7e\u00b7so", "ent\u00b7bl\u00f6\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und bei dem fortgesetzten Schwitzen", "tokens": ["Und", "bei", "dem", "fort\u00b7ge\u00b7setz\u00b7ten", "Schwit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wird schlie\u00dflich alles aufgel\u00f6st.", "tokens": ["Wird", "schlie\u00df\u00b7lich", "al\u00b7les", "auf\u00b7ge\u00b7l\u00f6st", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Und die verfluchten Walzertakte!", "tokens": ["Und", "die", "ver\u00b7fluch\u00b7ten", "Wal\u00b7zer\u00b7tak\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die sind die rechte Melodie", "tokens": ["Die", "sind", "die", "rech\u00b7te", "Me\u00b7lo\u00b7die"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum illegalen Trauungsakte", "tokens": ["Zum", "il\u00b7le\u00b7ga\u00b7len", "Trau\u00b7ungs\u00b7ak\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und zur verbot'nen Lustpartie!", "tokens": ["Und", "zur", "ver\u00b7bot'\u00b7nen", "Lust\u00b7par\u00b7tie", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Wer dieses einmal recht begriffen,", "tokens": ["Wer", "die\u00b7ses", "ein\u00b7mal", "recht", "be\u00b7grif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Tralala im Wiegeschritt,", "tokens": ["Das", "Tra\u00b7la\u00b7la", "im", "Wie\u00b7ge\u00b7schritt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat auf die Tugend bald gepfiffen", "tokens": ["Hat", "auf", "die", "Tu\u00b7gend", "bald", "ge\u00b7pfif\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und gibt sie preis, i gitt, i gitt!", "tokens": ["Und", "gibt", "sie", "preis", ",", "i", "gitt", ",", "i", "gitt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "NE", "VVFIN", "$,", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Als Lilly sich an Mirko dr\u00fcckte,", "tokens": ["Als", "Lil\u00b7ly", "sich", "an", "Mir\u00b7ko", "dr\u00fcck\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PRF", "APPR", "NE", "VVFIN", "$,"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Verga\u00df sie alles ganz und gar,", "tokens": ["Ver\u00b7ga\u00df", "sie", "al\u00b7les", "ganz", "und", "gar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was sich f\u00fcr sie und Hamburg schickte,", "tokens": ["Was", "sich", "f\u00fcr", "sie", "und", "Ham\u00b7burg", "schick\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "PPER", "KON", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und was ihr fr\u00fcher heilig war.", "tokens": ["Und", "was", "ihr", "fr\u00fc\u00b7her", "hei\u00b7lig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Sie spitzte ihre Rosenlippen,", "tokens": ["Sie", "spitz\u00b7te", "ih\u00b7re", "Ro\u00b7sen\u00b7lip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er spitzte auch sein fettes Maul,", "tokens": ["Er", "spitz\u00b7te", "auch", "sein", "fet\u00b7tes", "Maul", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Unschuld mu\u00dfte \u00fcberkippen,", "tokens": ["Die", "Un\u00b7schuld", "mu\u00df\u00b7te", "\u00fc\u00b7ber\u00b7kip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Keuschheit war im Kerne faul.", "tokens": ["Die", "Keuschheit", "war", "im", "Ker\u00b7ne", "faul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPRART", "NN", "ADJD", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.33": {"line.1": {"text": "Und Walzer, Schottisch und Fran\u00e7aise,", "tokens": ["Und", "Wal\u00b7zer", ",", "Schot\u00b7tisch", "und", "Fran\u00e7ai\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ein Knutschen hier, ein Knutschen dort,", "tokens": ["Ein", "Knut\u00b7schen", "hier", ",", "ein", "Knut\u00b7schen", "dort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie sich das alles sachgem\u00e4\u00dfe", "tokens": ["Wie", "sich", "das", "al\u00b7les", "sach\u00b7ge\u00b7m\u00e4\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "ART", "PIS", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Entwickelte so fort und fort!", "tokens": ["Ent\u00b7wi\u00b7ckel\u00b7te", "so", "fort", "und", "fort", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Sie sa\u00dfen in der gro\u00dfen Pause", "tokens": ["Sie", "sa\u00b7\u00dfen", "in", "der", "gro\u00b7\u00dfen", "Pau\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schon hinter einem Tannenbaum.", "tokens": ["Schon", "hin\u00b7ter", "ei\u00b7nem", "Tan\u00b7nen\u00b7baum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zuletzt ging er mit ihr nach Hause,", "tokens": ["Zu\u00b7letzt", "ging", "er", "mit", "ihr", "nach", "Hau\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "APPR", "NN", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Und da begann ihr Liebestraum.", "tokens": ["Und", "da", "be\u00b7gann", "ihr", "Lie\u00b7best\u00b7raum", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Vorbei war's mit den Stilgef\u00fchlen,", "tokens": ["Vor\u00b7bei", "wa\u00b7r's", "mit", "den", "Stil\u00b7ge\u00b7f\u00fch\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sie mu\u00dften schweigen. Vorderhand", "tokens": ["Sie", "mu\u00df\u00b7ten", "schwei\u00b7gen", ".", "Vor\u00b7der\u00b7hand"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PPER", "VMFIN", "VVINF", "$.", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat sie die Kunst nicht mehr an St\u00fchlen", "tokens": ["Hat", "sie", "die", "Kunst", "nicht", "mehr", "an", "St\u00fch\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ART", "NN", "PTKNEG", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und an Kommoden angewandt.", "tokens": ["Und", "an", "Kom\u00b7mo\u00b7den", "an\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "F\u00fcr Teppich- und Tapetenmuster", "tokens": ["F\u00fcr", "Tep\u00b7pich", "und", "Ta\u00b7pe\u00b7ten\u00b7mus\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "TRUNC", "KON", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Erlosch ihr Malerinnensinn,", "tokens": ["Er\u00b7losch", "ihr", "Ma\u00b7le\u00b7rin\u00b7nen\u00b7sinn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie liebte t\u00e4glich unbewu\u00dfter", "tokens": ["Sie", "lieb\u00b7te", "t\u00e4g\u00b7lich", "un\u00b7be\u00b7wu\u00df\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sah das wahre Gl\u00fcck darin.", "tokens": ["Und", "sah", "das", "wah\u00b7re", "Gl\u00fcck", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Sie sprach nicht mehr von Farbenflecken,", "tokens": ["Sie", "sprach", "nicht", "mehr", "von", "Far\u00b7ben\u00b7fle\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mehr von \u00bbecht\u00ab, nicht mehr von \u00bbKitsch\u00ab;", "tokens": ["Nicht", "mehr", "von", "\u00bb", "echt", "\u00ab", ",", "nicht", "mehr", "von", "\u00bb", "Kitsch", "\u00ab", ";"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "$(", "ADJD", "$(", "$,", "PTKNEG", "ADV", "APPR", "$(", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie wollte nur zusammenstecken", "tokens": ["Sie", "woll\u00b7te", "nur", "zu\u00b7sam\u00b7men\u00b7ste\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit Mirko Stanko Dobrowitsch!", "tokens": ["Mit", "Mir\u00b7ko", "Stan\u00b7ko", "Do\u00b7bro\u00b7witsch", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.38": {"line.1": {"text": "Den Schlu\u00df kann man sich selber denken;", "tokens": ["Den", "Schlu\u00df", "kann", "man", "sich", "sel\u00b7ber", "den\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von sowas kommt ein Kind davon,", "tokens": ["Von", "so\u00b7was", "kommt", "ein", "Kind", "da\u00b7von", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch schwerer ist's, sich zu versenken", "tokens": ["Doch", "schwe\u00b7rer", "ist's", ",", "sich", "zu", "ver\u00b7sen\u00b7ken"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "VAFIN", "$,", "PRF", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In das Gef\u00fchl von Menk &amp; Sohn.", "tokens": ["In", "das", "Ge\u00b7f\u00fchl", "von", "Menk", "&amp;", "Sohn", "."], "token_info": ["word", "word", "word", "word", "word", "XML_entity", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "NE", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.39": {"line.1": {"text": "Die Mutter als gebor'ne Holle", "tokens": ["Die", "Mut\u00b7ter", "als", "ge\u00b7bor'\u00b7ne", "Hol\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War trostlos oder desperat", "tokens": ["War", "trost\u00b7los", "o\u00b7der", "des\u00b7pe\u00b7rat"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wu\u00dft' nicht, was sie sagen solle,", "tokens": ["Und", "wu\u00dft'", "nicht", ",", "was", "sie", "sa\u00b7gen", "sol\u00b7le", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "PRELS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}}, "stanza.40": {"line.1": {"text": "Als Enkelin von 'nem Senater", "tokens": ["Als", "En\u00b7ke\u00b7lin", "von", "'nem", "Se\u00b7na\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt eine Serbengro\u00dfmama!", "tokens": ["Jetzt", "ei\u00b7ne", "Ser\u00b7ben\u00b7gro\u00df\u00b7ma\u00b7ma", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und \u00e4hnlich dachte auch der Vater,", "tokens": ["Und", "\u00e4hn\u00b7lich", "dach\u00b7te", "auch", "der", "Va\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sobald er die Bescherung sah.", "tokens": ["So\u00b7bald", "er", "die", "Be\u00b7sche\u00b7rung", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Indes, man mu\u00df es mal goutieren,", "tokens": ["In\u00b7des", ",", "man", "mu\u00df", "es", "mal", "gou\u00b7tie\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PIS", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wenn es noch so h\u00e4\u00dflich r\u00f6ch'!", "tokens": ["Und", "wenn", "es", "noch", "so", "h\u00e4\u00df\u00b7lich", "r\u00f6ch'", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und mu\u00df die Sache korrigieren.", "tokens": ["Und", "mu\u00df", "die", "Sa\u00b7che", "kor\u00b7ri\u00b7gie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vielleicht durch eine Hochzeit? N\u00f6ch?", "tokens": ["Viel\u00b7leicht", "durch", "ei\u00b7ne", "Hoch\u00b7zeit", "?", "N\u00f6ch", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Nun wurde Lilly eine Serbin,", "tokens": ["Nun", "wur\u00b7de", "Lil\u00b7ly", "ei\u00b7ne", "Ser\u00b7bin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Mirko dachte sich als Mann,", "tokens": ["Denn", "Mir\u00b7ko", "dach\u00b7te", "sich", "als", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PRF", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df man mit Geld und einer Erbin", "tokens": ["Da\u00df", "man", "mit", "Geld", "und", "ei\u00b7ner", "Er\u00b7bin"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "NN", "KON", "ART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Am Ende sch\u00f6ner leben kann.", "tokens": ["Am", "En\u00b7de", "sch\u00f6\u00b7ner", "le\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Wie lange sie am Honig schl\u00fcrfen?!", "tokens": ["Wie", "lan\u00b7ge", "sie", "am", "Ho\u00b7nig", "schl\u00fcr\u00b7fen", "?!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und was es f\u00fcr ein Ende nimmt??!", "tokens": ["Und", "was", "es", "f\u00fcr", "ein", "En\u00b7de", "nimmt", "??!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch, da\u00df sie ", "tokens": ["Doch", ",", "da\u00df", "sie"], "token_info": ["word", "punct", "word", "word"], "pos": ["KON", "$,", "KOUS", "PPER"], "meter": "+--", "measure": "dactylic.init"}, "line.4": {"text": "Das wei\u00df ich heute schon bestimmt.", "tokens": ["Das", "wei\u00df", "ich", "heu\u00b7te", "schon", "be\u00b7stimmt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Sie stammte wohl aus Hamburgs Mauern,", "tokens": ["Sie", "stamm\u00b7te", "wohl", "aus", "Ham\u00b7burgs", "Mau\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NE", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das dorten an der Elbe liegt,", "tokens": ["Das", "dor\u00b7ten", "an", "der", "El\u00b7be", "liegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPR", "ART", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und hat zu mancher Leut' Bedauern", "tokens": ["Und", "hat", "zu", "man\u00b7cher", "Leut'", "Be\u00b7dau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "APPR", "PIAT", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In M\u00fcnchen hier ein Kind gekriegt.", "tokens": ["In", "M\u00fcn\u00b7chen", "hier", "ein", "Kind", "ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Die Mutter als gebor'ne Holle", "tokens": ["Die", "Mut\u00b7ter", "als", "ge\u00b7bor'\u00b7ne", "Hol\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verm\u00e4hlte sich mit Menk &amp; Sohn;", "tokens": ["Ver\u00b7m\u00e4hl\u00b7te", "sich", "mit", "Menk", "&amp;", "Sohn", ";"], "token_info": ["word", "word", "word", "word", "XML_entity", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "NN", "NE", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Er handelte en gros in Wolle,", "tokens": ["Er", "han\u00b7del\u00b7te", "en", "gros", "in", "Wol\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "ADJD", "APPR", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Lilly war das Kind davon.", "tokens": ["Und", "Lil\u00b7ly", "war", "das", "Kind", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Bemerkt sei, da\u00df der Elternvater", "tokens": ["Be\u00b7merkt", "sei", ",", "da\u00df", "der", "El\u00b7tern\u00b7va\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "\u2013 Und zwar derjen'ge mutterseits \u2013", "tokens": ["\u2013", "Und", "zwar", "der\u00b7jen'\u00b7ge", "mut\u00b7ter\u00b7seits", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "PDS", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch mitregierte als Senater", "tokens": ["Auch", "mit\u00b7re\u00b7gier\u00b7te", "als", "Se\u00b7na\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vor siebzig Jahren schon bereits.", "tokens": ["Vor", "sieb\u00b7zig", "Jah\u00b7ren", "schon", "be\u00b7reits", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "In einer solchen Geldfamil'che", "tokens": ["In", "ei\u00b7ner", "sol\u00b7chen", "Geld\u00b7fa\u00b7mil'\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kann nur der Anstand heimisch sein;", "tokens": ["Kann", "nur", "der", "An\u00b7stand", "hei\u00b7misch", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Man zieht ihn mit der Muttermilche", "tokens": ["Man", "zieht", "ihn", "mit", "der", "Mut\u00b7ter\u00b7mil\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als selbstverst\u00e4ndlich mit hinein.", "tokens": ["Als", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich", "mit", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPR", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Es war nun Lilly auch in Liebe", "tokens": ["Es", "war", "nun", "Lil\u00b7ly", "auch", "in", "Lie\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "NE", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zur sch\u00f6nheitsreichen Kunst entbrannt,", "tokens": ["Zur", "sch\u00f6n\u00b7heits\u00b7rei\u00b7chen", "Kunst", "ent\u00b7brannt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sie entwickelte die Triebe", "tokens": ["Und", "sie", "ent\u00b7wi\u00b7ckel\u00b7te", "die", "Trie\u00b7be"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Teils \u00f6lgemalt, teils angewandt.", "tokens": ["Teils", "\u00f6l\u00b7ge\u00b7malt", ",", "teils", "an\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "In solchen F\u00e4llen des Talentes", "tokens": ["In", "sol\u00b7chen", "F\u00e4l\u00b7len", "des", "Ta\u00b7len\u00b7tes"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zieht alle Welt nach M\u00fcnchen her,", "tokens": ["Zieht", "al\u00b7le", "Welt", "nach", "M\u00fcn\u00b7chen", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum Studium des Ornamentes,", "tokens": ["Zum", "Stu\u00b7di\u00b7um", "des", "Or\u00b7na\u00b7men\u00b7tes", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zur Kunst im Handwerk in die Lehr'.", "tokens": ["Zur", "Kunst", "im", "Hand\u00b7werk", "in", "die", "Lehr'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Auch Lilly Menk war angekommen", "tokens": ["Auch", "Lil\u00b7ly", "Menk", "war", "an\u00b7ge\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Voll Eifer und Bemalungssucht.", "tokens": ["Voll", "Ei\u00b7fer", "und", "Be\u00b7ma\u00b7lungs\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie hat ein Ende es genommen", "tokens": ["Wie", "hat", "ein", "En\u00b7de", "es", "ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "ART", "NN", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit illegaler Leibesfrucht?", "tokens": ["Mit", "il\u00b7le\u00b7ga\u00b7ler", "Lei\u00b7bes\u00b7frucht", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Wenn man 'nem Kind das Beste bietet,", "tokens": ["Wenn", "man", "'nem", "Kind", "das", "Bes\u00b7te", "bie\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann glaubt man, es wird keusch und klug;", "tokens": ["Dann", "glaubt", "man", ",", "es", "wird", "keusch", "und", "klug", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "PPER", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.3": {"text": "Doch Lilly hat sich eingemietet", "tokens": ["Doch", "Lil\u00b7ly", "hat", "sich", "ein\u00b7ge\u00b7mie\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NE", "VAFIN", "PRF", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In Schwabing, und das sagt genug.", "tokens": ["In", "Schwa\u00b7bing", ",", "und", "das", "sagt", "ge\u00b7nug", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "KON", "PDS", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Hier ging sie zu dem Malprofesser,", "tokens": ["Hier", "ging", "sie", "zu", "dem", "Mal\u00b7pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo sie den Geist der Kunst erfuhr,", "tokens": ["Wo", "sie", "den", "Geist", "der", "Kunst", "er\u00b7fuhr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das Stilgef\u00fchl als Sch\u00f6nheitsmesser,", "tokens": ["Das", "Stil\u00b7ge\u00b7f\u00fchl", "als", "Sch\u00f6n\u00b7heits\u00b7mes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Ohrenschneckenhaarfrisur.", "tokens": ["Die", "Oh\u00b7ren\u00b7schne\u00b7cken\u00b7haar\u00b7fri\u00b7sur", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Auch sonst begann sie sich zu \u00e4ndern,", "tokens": ["Auch", "sonst", "be\u00b7gann", "sie", "sich", "zu", "\u00e4n\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als an der Freiheit sie genippt,", "tokens": ["Als", "an", "der", "Frei\u00b7heit", "sie", "ge\u00b7nippt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie ging jetzt in Reformgew\u00e4ndern,", "tokens": ["Sie", "ging", "jetzt", "in", "Re\u00b7form\u00b7ge\u00b7w\u00e4n\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+++-+-", "measure": "unknown.measure.penta"}, "line.4": {"text": "In denen leicht der Busen schwippt.", "tokens": ["In", "de\u00b7nen", "leicht", "der", "Bu\u00b7sen", "schwippt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Und mit den andern Kunstbefliss'nen", "tokens": ["Und", "mit", "den", "an\u00b7dern", "Kunst\u00b7be\u00b7fliss'\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Versank sie tiefer in den Sumpf,", "tokens": ["Ver\u00b7sank", "sie", "tie\u00b7fer", "in", "den", "Sumpf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ging \u00f6fter aus mit 'nem zerriss'nen", "tokens": ["Ging", "\u00f6f\u00b7ter", "aus", "mit", "'nem", "zer\u00b7riss'\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und durchgebrochnen Seidenstrumpf.", "tokens": ["Und", "durch\u00b7ge\u00b7broch\u00b7nen", "Sei\u00b7dens\u00b7trumpf", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Sie trug mit gr\u00f6\u00dfter Seelenruhe,", "tokens": ["Sie", "trug", "mit", "gr\u00f6\u00df\u00b7ter", "See\u00b7len\u00b7ru\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Noch eh' ein Vierteljahr verging,", "tokens": ["Noch", "eh'", "ein", "Vier\u00b7tel\u00b7jahr", "ver\u00b7ging", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die abgelatschten Kn\u00f6pfelschuhe", "tokens": ["Die", "ab\u00b7ge\u00b7latschten", "Kn\u00f6p\u00b7fel\u00b7schu\u00b7he"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und achtete es ganz gering.", "tokens": ["Und", "ach\u00b7te\u00b7te", "es", "ganz", "ge\u00b7ring", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Ein Weib verliert den Grundcharakter,", "tokens": ["Ein", "Weib", "ver\u00b7liert", "den", "Grund\u00b7cha\u00b7rak\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn es den Ordnungssinn verliert;", "tokens": ["Wenn", "es", "den", "Ord\u00b7nungs\u00b7sinn", "ver\u00b7liert", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Tugend scheint ihm abgeschmackter,", "tokens": ["Die", "Tu\u00b7gend", "scheint", "ihm", "ab\u00b7ge\u00b7schmack\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein fester Halt wird demoliert.", "tokens": ["Sein", "fes\u00b7ter", "Halt", "wird", "de\u00b7mo\u00b7liert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}}, "stanza.57": {"line.1": {"text": "Man sieht es bald ins Laster h\u00fcpfen", "tokens": ["Man", "sieht", "es", "bald", "ins", "Las\u00b7ter", "h\u00fcp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "APPRART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit einem k\u00fchnen Sprunggelenk.", "tokens": ["Mit", "ei\u00b7nem", "k\u00fch\u00b7nen", "Sprung\u00b7ge\u00b7lenk", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nun lasset mich den Schleier l\u00fcpfen", "tokens": ["Nun", "las\u00b7set", "mich", "den", "Schlei\u00b7er", "l\u00fcp\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von unsrer armen Lilly Menk!", "tokens": ["Von", "uns\u00b7rer", "ar\u00b7men", "Lil\u00b7ly", "Menk", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NE", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Sie nahm sich Atelier und Zimmer", "tokens": ["Sie", "nahm", "sich", "A\u00b7te\u00b7lier", "und", "Zim\u00b7mer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Im vierten Stock mit eigner T\u00fcr,", "tokens": ["Im", "vier\u00b7ten", "Stock", "mit", "eig\u00b7ner", "T\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da gibt Gelegenheit sich immer", "tokens": ["Da", "gibt", "Ge\u00b7le\u00b7gen\u00b7heit", "sich", "im\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NN", "PRF", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zu der und jener Ungeb\u00fchr.", "tokens": ["Zu", "der", "und", "je\u00b7ner", "Un\u00b7ge\u00b7b\u00fchr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "KON", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Erst wu\u00dfte sie wohl selbstverst\u00e4ndlich,", "tokens": ["Erst", "wu\u00df\u00b7te", "sie", "wohl", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sie aus Hamburg war, es nicht:", "tokens": ["Da", "sie", "aus", "Ham\u00b7burg", "war", ",", "es", "nicht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NE", "VAFIN", "$,", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "In dieser Stadt ist unabwendlich", "tokens": ["In", "die\u00b7ser", "Stadt", "ist", "un\u00b7ab\u00b7wend\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Keuschheit eine Lebenspflicht.", "tokens": ["Die", "Keuschheit", "ei\u00b7ne", "Le\u00b7bens\u00b7pflicht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.60": {"line.1": {"text": "In M\u00fcnchen ist es nicht dasselbe,", "tokens": ["In", "M\u00fcn\u00b7chen", "ist", "es", "nicht", "das\u00b7sel\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAFIN", "PPER", "PTKNEG", "PDAT", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hier kann man vieles eher tun", "tokens": ["Hier", "kann", "man", "vie\u00b7les", "e\u00b7her", "tun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "PIS", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Als wie in Hamburg an der Elbe", "tokens": ["Als", "wie", "in", "Ham\u00b7burg", "an", "der", "El\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "APPR", "NE", "APPR", "ART", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als unerfahr'nes dummes Huhn.", "tokens": ["Als", "un\u00b7er\u00b7fahr'\u00b7nes", "dum\u00b7mes", "Huhn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Es war gerad' in jenen Tagen,", "tokens": ["Es", "war", "ge\u00b7rad'", "in", "je\u00b7nen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sich der Karneval erhob,", "tokens": ["Da", "sich", "der", "Kar\u00b7ne\u00b7val", "er\u00b7hob", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wo das Vergn\u00fcgen sozusagen", "tokens": ["Wo", "das", "Ver\u00b7gn\u00fc\u00b7gen", "so\u00b7zu\u00b7sa\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sich in die M\u00e4dchenherzen schob.", "tokens": ["Sich", "in", "die", "M\u00e4d\u00b7chen\u00b7her\u00b7zen", "schob", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.62": {"line.1": {"text": "Redouten, B\u00e4lle, K\u00fcnstlerfeste,", "tokens": ["Re\u00b7dou\u00b7ten", ",", "B\u00e4l\u00b7le", ",", "K\u00fcnst\u00b7ler\u00b7fes\u00b7te", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Bal par\u00e9 noch obendrein,", "tokens": ["Der", "Bal", "pa\u00b7r\u00e9", "noch", "o\u00b7ben\u00b7drein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie kann dagegen selbst die Beste", "tokens": ["Wie", "kann", "da\u00b7ge\u00b7gen", "selbst", "die", "Bes\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VMFIN", "PAV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Keuscheste gepanzert sein?", "tokens": ["Und", "Keu\u00b7sches\u00b7te", "ge\u00b7pan\u00b7zert", "sein", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Nicht weit von ihr wohnt' ein Schlawiner,", "tokens": ["Nicht", "weit", "von", "ihr", "wohnt'", "ein", "Schla\u00b7wi\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "APPR", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ganz ohne Geld und Broterwerb,", "tokens": ["Ganz", "oh\u00b7ne", "Geld", "und", "Bro\u00b7ter\u00b7werb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein Vater wirkte als Rabbiner,", "tokens": ["Sein", "Va\u00b7ter", "wirk\u00b7te", "als", "Rab\u00b7bi\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er selbst war nichts als blo\u00df ein Serb'.", "tokens": ["Er", "selbst", "war", "nichts", "als", "blo\u00df", "ein", "Serb'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "PIS", "KOKOM", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Doch trug er lange schwarze Haare", "tokens": ["Doch", "trug", "er", "lan\u00b7ge", "schwar\u00b7ze", "Haa\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und eingeschmiert mit Nierenfett.", "tokens": ["Und", "ein\u00b7ge\u00b7schmiert", "mit", "Nie\u00b7ren\u00b7fett", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein M\u00e4dchen sieht darin das Wahre", "tokens": ["Ein", "M\u00e4d\u00b7chen", "sieht", "da\u00b7rin", "das", "Wah\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PAV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und findet es auch wundernett.", "tokens": ["Und", "fin\u00b7det", "es", "auch", "wun\u00b7der\u00b7nett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Sein Angesicht war nicht gewaschen,", "tokens": ["Sein", "An\u00b7ge\u00b7sicht", "war", "nicht", "ge\u00b7wa\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch lag darin ein stiller Schmerz;", "tokens": ["Doch", "lag", "da\u00b7rin", "ein", "stil\u00b7ler", "Schmerz", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der kam von leeren Hosentaschen", "tokens": ["Der", "kam", "von", "lee\u00b7ren", "Ho\u00b7sen\u00b7ta\u00b7schen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und r\u00fchrte jedes Frauenherz.", "tokens": ["Und", "r\u00fchr\u00b7te", "je\u00b7des", "Frau\u00b7en\u00b7herz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Man mu\u00df dazu aus Hamburg stammen", "tokens": ["Man", "mu\u00df", "da\u00b7zu", "aus", "Ham\u00b7burg", "stam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PAV", "APPR", "NE", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und unverstand'nes M\u00e4dchen sein,", "tokens": ["Und", "un\u00b7ver\u00b7stan\u00b7d'\u00b7nes", "M\u00e4d\u00b7chen", "sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAINF", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Dann steht man gleich in hellen Flammen", "tokens": ["Dann", "steht", "man", "gleich", "in", "hel\u00b7len", "Flam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr ein Schlawinermoschusschwein.", "tokens": ["F\u00fcr", "ein", "Schla\u00b7wi\u00b7ner\u00b7mo\u00b7schus\u00b7schwein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.67": {"line.1": {"text": "Wenn nur die Reinlichkeiten fehlen,", "tokens": ["Wenn", "nur", "die", "Rein\u00b7lich\u00b7kei\u00b7ten", "feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was liegt der Malerin daran?", "tokens": ["Was", "liegt", "der", "Ma\u00b7le\u00b7rin", "da\u00b7ran", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr hochgestimmte K\u00fcnstlerseelen", "tokens": ["F\u00fcr", "hoch\u00b7ges\u00b7timm\u00b7te", "K\u00fcnst\u00b7ler\u00b7see\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Seife blo\u00df ein leerer Wahn.", "tokens": ["Ist", "Sei\u00b7fe", "blo\u00df", "ein", "lee\u00b7rer", "Wahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Nach diesem hier Vorausgeschickten", "tokens": ["Nach", "die\u00b7sem", "hier", "Vor\u00b7aus\u00b7ge\u00b7schick\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADV", "NN"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Will ich bemerken, da\u00df sie sich", "tokens": ["Will", "ich", "be\u00b7mer\u00b7ken", ",", "da\u00df", "sie", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VMFIN", "PPER", "VVINF", "$,", "KOUS", "PPER", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum erstenmal ins Auge blickten", "tokens": ["Zum", "ers\u00b7ten\u00b7mal", "ins", "Au\u00b7ge", "blick\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADV", "APPRART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bei Klarinett und Geigenstrich.", "tokens": ["Bei", "Kla\u00b7ri\u00b7nett", "und", "Gei\u00b7gen\u00b7strich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "Bei einem K\u00fcnstlerlumpenballe", "tokens": ["Bei", "ei\u00b7nem", "K\u00fcnst\u00b7ler\u00b7lum\u00b7pen\u00b7bal\u00b7le"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ergab sich dieses Resultat,", "tokens": ["Er\u00b7gab", "sich", "die\u00b7ses", "Re\u00b7sul\u00b7tat", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df Lilly Menk in ihrem Falle", "tokens": ["Da\u00df", "Lil\u00b7ly", "Menk", "in", "ih\u00b7rem", "Fal\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vom Unschuldspfad danebentrat.", "tokens": ["Vom", "Un\u00b7schuld\u00b7spfad", "da\u00b7ne\u00b7bent\u00b7rat", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.70": {"line.1": {"text": "Ach Gott! Man kann im gro\u00dfen ganzen", "tokens": ["Ach", "Gott", "!", "Man", "kann", "im", "gro\u00b7\u00dfen", "gan\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "NN", "$.", "PIS", "VMFIN", "APPRART", "ADJA", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die armen M\u00e4dchen schon verstehn,", "tokens": ["Die", "ar\u00b7men", "M\u00e4d\u00b7chen", "schon", "ver\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn die Prinzipien beim Tanzen", "tokens": ["Wenn", "die", "Prin\u00b7zi\u00b7pi\u00b7en", "beim", "Tan\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "APPRART", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Bei ihnen aus dem Leime gehn.", "tokens": ["Bei", "ih\u00b7nen", "aus", "dem", "Lei\u00b7me", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.71": {"line.1": {"text": "Das junge Blut mu\u00df sich erhitzen,", "tokens": ["Das", "jun\u00b7ge", "Blut", "mu\u00df", "sich", "er\u00b7hit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Herz ist sowieso entbl\u00f6\u00dft,", "tokens": ["Das", "Herz", "ist", "so\u00b7wi\u00b7e\u00b7so", "ent\u00b7bl\u00f6\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und bei dem fortgesetzten Schwitzen", "tokens": ["Und", "bei", "dem", "fort\u00b7ge\u00b7setz\u00b7ten", "Schwit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wird schlie\u00dflich alles aufgel\u00f6st.", "tokens": ["Wird", "schlie\u00df\u00b7lich", "al\u00b7les", "auf\u00b7ge\u00b7l\u00f6st", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Und die verfluchten Walzertakte!", "tokens": ["Und", "die", "ver\u00b7fluch\u00b7ten", "Wal\u00b7zer\u00b7tak\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die sind die rechte Melodie", "tokens": ["Die", "sind", "die", "rech\u00b7te", "Me\u00b7lo\u00b7die"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum illegalen Trauungsakte", "tokens": ["Zum", "il\u00b7le\u00b7ga\u00b7len", "Trau\u00b7ungs\u00b7ak\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und zur verbot'nen Lustpartie!", "tokens": ["Und", "zur", "ver\u00b7bot'\u00b7nen", "Lust\u00b7par\u00b7tie", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "Wer dieses einmal recht begriffen,", "tokens": ["Wer", "die\u00b7ses", "ein\u00b7mal", "recht", "be\u00b7grif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Tralala im Wiegeschritt,", "tokens": ["Das", "Tra\u00b7la\u00b7la", "im", "Wie\u00b7ge\u00b7schritt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat auf die Tugend bald gepfiffen", "tokens": ["Hat", "auf", "die", "Tu\u00b7gend", "bald", "ge\u00b7pfif\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und gibt sie preis, i gitt, i gitt!", "tokens": ["Und", "gibt", "sie", "preis", ",", "i", "gitt", ",", "i", "gitt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$,", "NE", "VVFIN", "$,", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Als Lilly sich an Mirko dr\u00fcckte,", "tokens": ["Als", "Lil\u00b7ly", "sich", "an", "Mir\u00b7ko", "dr\u00fcck\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PRF", "APPR", "NE", "VVFIN", "$,"], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Verga\u00df sie alles ganz und gar,", "tokens": ["Ver\u00b7ga\u00df", "sie", "al\u00b7les", "ganz", "und", "gar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was sich f\u00fcr sie und Hamburg schickte,", "tokens": ["Was", "sich", "f\u00fcr", "sie", "und", "Ham\u00b7burg", "schick\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "PPER", "KON", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und was ihr fr\u00fcher heilig war.", "tokens": ["Und", "was", "ihr", "fr\u00fc\u00b7her", "hei\u00b7lig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Sie spitzte ihre Rosenlippen,", "tokens": ["Sie", "spitz\u00b7te", "ih\u00b7re", "Ro\u00b7sen\u00b7lip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er spitzte auch sein fettes Maul,", "tokens": ["Er", "spitz\u00b7te", "auch", "sein", "fet\u00b7tes", "Maul", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Unschuld mu\u00dfte \u00fcberkippen,", "tokens": ["Die", "Un\u00b7schuld", "mu\u00df\u00b7te", "\u00fc\u00b7ber\u00b7kip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Keuschheit war im Kerne faul.", "tokens": ["Die", "Keuschheit", "war", "im", "Ker\u00b7ne", "faul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPRART", "NN", "ADJD", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.76": {"line.1": {"text": "Und Walzer, Schottisch und Fran\u00e7aise,", "tokens": ["Und", "Wal\u00b7zer", ",", "Schot\u00b7tisch", "und", "Fran\u00e7ai\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ein Knutschen hier, ein Knutschen dort,", "tokens": ["Ein", "Knut\u00b7schen", "hier", ",", "ein", "Knut\u00b7schen", "dort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie sich das alles sachgem\u00e4\u00dfe", "tokens": ["Wie", "sich", "das", "al\u00b7les", "sach\u00b7ge\u00b7m\u00e4\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "ART", "PIS", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Entwickelte so fort und fort!", "tokens": ["Ent\u00b7wi\u00b7ckel\u00b7te", "so", "fort", "und", "fort", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.77": {"line.1": {"text": "Sie sa\u00dfen in der gro\u00dfen Pause", "tokens": ["Sie", "sa\u00b7\u00dfen", "in", "der", "gro\u00b7\u00dfen", "Pau\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schon hinter einem Tannenbaum.", "tokens": ["Schon", "hin\u00b7ter", "ei\u00b7nem", "Tan\u00b7nen\u00b7baum", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zuletzt ging er mit ihr nach Hause,", "tokens": ["Zu\u00b7letzt", "ging", "er", "mit", "ihr", "nach", "Hau\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPOSAT", "APPR", "NN", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Und da begann ihr Liebestraum.", "tokens": ["Und", "da", "be\u00b7gann", "ihr", "Lie\u00b7best\u00b7raum", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.78": {"line.1": {"text": "Vorbei war's mit den Stilgef\u00fchlen,", "tokens": ["Vor\u00b7bei", "wa\u00b7r's", "mit", "den", "Stil\u00b7ge\u00b7f\u00fch\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Sie mu\u00dften schweigen. Vorderhand", "tokens": ["Sie", "mu\u00df\u00b7ten", "schwei\u00b7gen", ".", "Vor\u00b7der\u00b7hand"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PPER", "VMFIN", "VVINF", "$.", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat sie die Kunst nicht mehr an St\u00fchlen", "tokens": ["Hat", "sie", "die", "Kunst", "nicht", "mehr", "an", "St\u00fch\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ART", "NN", "PTKNEG", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und an Kommoden angewandt.", "tokens": ["Und", "an", "Kom\u00b7mo\u00b7den", "an\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "F\u00fcr Teppich- und Tapetenmuster", "tokens": ["F\u00fcr", "Tep\u00b7pich", "und", "Ta\u00b7pe\u00b7ten\u00b7mus\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "TRUNC", "KON", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Erlosch ihr Malerinnensinn,", "tokens": ["Er\u00b7losch", "ihr", "Ma\u00b7le\u00b7rin\u00b7nen\u00b7sinn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie liebte t\u00e4glich unbewu\u00dfter", "tokens": ["Sie", "lieb\u00b7te", "t\u00e4g\u00b7lich", "un\u00b7be\u00b7wu\u00df\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und sah das wahre Gl\u00fcck darin.", "tokens": ["Und", "sah", "das", "wah\u00b7re", "Gl\u00fcck", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Sie sprach nicht mehr von Farbenflecken,", "tokens": ["Sie", "sprach", "nicht", "mehr", "von", "Far\u00b7ben\u00b7fle\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mehr von \u00bbecht\u00ab, nicht mehr von \u00bbKitsch\u00ab;", "tokens": ["Nicht", "mehr", "von", "\u00bb", "echt", "\u00ab", ",", "nicht", "mehr", "von", "\u00bb", "Kitsch", "\u00ab", ";"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "$(", "ADJD", "$(", "$,", "PTKNEG", "ADV", "APPR", "$(", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie wollte nur zusammenstecken", "tokens": ["Sie", "woll\u00b7te", "nur", "zu\u00b7sam\u00b7men\u00b7ste\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit Mirko Stanko Dobrowitsch!", "tokens": ["Mit", "Mir\u00b7ko", "Stan\u00b7ko", "Do\u00b7bro\u00b7witsch", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.81": {"line.1": {"text": "Den Schlu\u00df kann man sich selber denken;", "tokens": ["Den", "Schlu\u00df", "kann", "man", "sich", "sel\u00b7ber", "den\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Von sowas kommt ein Kind davon,", "tokens": ["Von", "so\u00b7was", "kommt", "ein", "Kind", "da\u00b7von", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch schwerer ist's, sich zu versenken", "tokens": ["Doch", "schwe\u00b7rer", "ist's", ",", "sich", "zu", "ver\u00b7sen\u00b7ken"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "VAFIN", "$,", "PRF", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In das Gef\u00fchl von Menk &amp; Sohn.", "tokens": ["In", "das", "Ge\u00b7f\u00fchl", "von", "Menk", "&amp;", "Sohn", "."], "token_info": ["word", "word", "word", "word", "word", "XML_entity", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "NE", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.82": {"line.1": {"text": "Die Mutter als gebor'ne Holle", "tokens": ["Die", "Mut\u00b7ter", "als", "ge\u00b7bor'\u00b7ne", "Hol\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War trostlos oder desperat", "tokens": ["War", "trost\u00b7los", "o\u00b7der", "des\u00b7pe\u00b7rat"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wu\u00dft' nicht, was sie sagen solle,", "tokens": ["Und", "wu\u00dft'", "nicht", ",", "was", "sie", "sa\u00b7gen", "sol\u00b7le", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "PRELS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}}, "stanza.83": {"line.1": {"text": "Als Enkelin von 'nem Senater", "tokens": ["Als", "En\u00b7ke\u00b7lin", "von", "'nem", "Se\u00b7na\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt eine Serbengro\u00dfmama!", "tokens": ["Jetzt", "ei\u00b7ne", "Ser\u00b7ben\u00b7gro\u00df\u00b7ma\u00b7ma", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und \u00e4hnlich dachte auch der Vater,", "tokens": ["Und", "\u00e4hn\u00b7lich", "dach\u00b7te", "auch", "der", "Va\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sobald er die Bescherung sah.", "tokens": ["So\u00b7bald", "er", "die", "Be\u00b7sche\u00b7rung", "sah", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Indes, man mu\u00df es mal goutieren,", "tokens": ["In\u00b7des", ",", "man", "mu\u00df", "es", "mal", "gou\u00b7tie\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PIS", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und wenn es noch so h\u00e4\u00dflich r\u00f6ch'!", "tokens": ["Und", "wenn", "es", "noch", "so", "h\u00e4\u00df\u00b7lich", "r\u00f6ch'", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und mu\u00df die Sache korrigieren.", "tokens": ["Und", "mu\u00df", "die", "Sa\u00b7che", "kor\u00b7ri\u00b7gie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vielleicht durch eine Hochzeit? N\u00f6ch?", "tokens": ["Viel\u00b7leicht", "durch", "ei\u00b7ne", "Hoch\u00b7zeit", "?", "N\u00f6ch", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$.", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.85": {"line.1": {"text": "Nun wurde Lilly eine Serbin,", "tokens": ["Nun", "wur\u00b7de", "Lil\u00b7ly", "ei\u00b7ne", "Ser\u00b7bin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Mirko dachte sich als Mann,", "tokens": ["Denn", "Mir\u00b7ko", "dach\u00b7te", "sich", "als", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PRF", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df man mit Geld und einer Erbin", "tokens": ["Da\u00df", "man", "mit", "Geld", "und", "ei\u00b7ner", "Er\u00b7bin"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "NN", "KON", "ART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Am Ende sch\u00f6ner leben kann.", "tokens": ["Am", "En\u00b7de", "sch\u00f6\u00b7ner", "le\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "Wie lange sie am Honig schl\u00fcrfen?!", "tokens": ["Wie", "lan\u00b7ge", "sie", "am", "Ho\u00b7nig", "schl\u00fcr\u00b7fen", "?!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und was es f\u00fcr ein Ende nimmt??!", "tokens": ["Und", "was", "es", "f\u00fcr", "ein", "En\u00b7de", "nimmt", "??!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch, da\u00df sie ", "tokens": ["Doch", ",", "da\u00df", "sie"], "token_info": ["word", "punct", "word", "word"], "pos": ["KON", "$,", "KOUS", "PPER"], "meter": "+--", "measure": "dactylic.init"}, "line.4": {"text": "Das wei\u00df ich heute schon bestimmt.", "tokens": ["Das", "wei\u00df", "ich", "heu\u00b7te", "schon", "be\u00b7stimmt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}