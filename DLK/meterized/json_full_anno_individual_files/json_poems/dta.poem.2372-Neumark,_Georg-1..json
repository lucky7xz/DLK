{"dta.poem.2372": {"metadata": {"author": {"name": "Neumark, Georg", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1652", "urn": "urn:nbn:de:kobv:b4-20428-0", "language": ["de:0.99"], "booktitle": "Neumark, Georg: Poetisch- und Musikalisches Lustw\u00e4ldchen. Hamburg, 1652."}, "poem": {"stanza.1": {"line.1": {"text": "Unter den sch\u00f6nen rohten Apfelwangen/", "tokens": ["Un\u00b7ter", "den", "sch\u00f6\u00b7nen", "roh\u00b7ten", "Ap\u00b7fel\u00b7wan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Offte nicht hoffend wird ein Wurm gefangen;", "tokens": ["Off\u00b7te", "nicht", "hof\u00b7fend", "wird", "ein", "Wurm", "ge\u00b7fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "VVPP", "VAFIN", "ART", "NN", "PTKVZ", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Also die Menschen/ die man meint vol treuen/", "tokens": ["Al\u00b7so", "die", "Men\u00b7schen", "/", "die", "man", "meint", "vol", "treu\u00b7en", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$(", "PRELS", "PIS", "VVFIN", "ADJD", "VVINF", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Soll man offt scheuen.", "tokens": ["Soll", "man", "offt", "scheu\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.2": {"line.1": {"text": "Denn wenn vielleicht der Mund an einem Ohrte/", "tokens": ["Denn", "wenn", "viel\u00b7leicht", "der", "Mund", "an", "ei\u00b7nem", "Ohr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "St\u00f6sset heraus die zukkers\u00fcssen Worte/", "tokens": ["St\u00f6s\u00b7set", "he\u00b7raus", "die", "zuk\u00b7ker\u00b7s\u00fcs\u00b7sen", "Wor\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APZR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "L\u00e4sset das falsche Hertz ihm doch gefallen", "tokens": ["L\u00e4s\u00b7set", "das", "fal\u00b7sche", "Hertz", "ihm", "doch", "ge\u00b7fal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "NN", "PPER", "ADV", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Bittere Gallen.", "tokens": ["Bit\u00b7te\u00b7re", "Gal\u00b7len", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.3": {"line.1": {"text": "Eben auch so (zwar sch\u00f6nste Sch\u00e4ferinne/", "tokens": ["E\u00b7ben", "auch", "so", "(", "zwar", "sch\u00f6ns\u00b7te", "Sch\u00e4\u00b7fe\u00b7rin\u00b7ne", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "$(", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Falscheste Doris andre Perelinne)", "tokens": ["Fal\u00b7sches\u00b7te", "Do\u00b7ris", "and\u00b7re", "Pe\u00b7re\u00b7lin\u00b7ne", ")"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NE", "ADJA", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Biflu gesinnet. Denn auf dich zu bauen/", "tokens": ["Bi\u00b7flu", "ge\u00b7sin\u00b7net", ".", "Denn", "auf", "dich", "zu", "bau\u00b7en", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$.", "KON", "APPR", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Darf man nicht trauen.", "tokens": ["Darf", "man", "nicht", "trau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "PTKNEG", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "Gleich wie die Nu\u00df/ die man mit grossem Krachen/", "tokens": ["Gleich", "wie", "die", "Nu\u00df", "/", "die", "man", "mit", "gros\u00b7sem", "Kra\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "NN", "$(", "PRELS", "PIS", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Zwischen den Z\u00e4hnen pfleget aufzumachen/", "tokens": ["Zwi\u00b7schen", "den", "Z\u00e4h\u00b7nen", "pfle\u00b7get", "auf\u00b7zu\u00b7ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "VVIZU", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Krellet/ und endlich doch ist nichts zu naschen/", "tokens": ["Krel\u00b7let", "/", "und", "end\u00b7lich", "doch", "ist", "nichts", "zu", "na\u00b7schen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "KON", "ADV", "ADV", "VAFIN", "PIS", "PTKZU", "VVINF", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Als W\u00fcrm\u2019 und Aschen.", "tokens": ["Als", "W\u00fcrm'", "und", "A\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Also bem\u00fchet bin ich auch gewesen/", "tokens": ["Al\u00b7so", "be\u00b7m\u00fc\u00b7het", "bin", "ich", "auch", "ge\u00b7we\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "VAFIN", "PPER", "ADV", "VAPP", "$("], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Eh ich treulose deiner Gunft genesen;", "tokens": ["Eh", "ich", "treu\u00b7lo\u00b7se", "dei\u00b7ner", "Gunft", "ge\u00b7ne\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Aber nun find\u2019 ich vor die Liebesfreuden", "tokens": ["A\u00b7ber", "nun", "find'", "ich", "vor", "die", "Lie\u00b7bes\u00b7freu\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Liederlichs Reiden.", "tokens": ["Lie\u00b7der\u00b7lichs", "Rei\u00b7den", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.6": {"line.1": {"text": "Tausendmal hett\u2019 ich wol darauf geschworen/", "tokens": ["Tau\u00b7send\u00b7mal", "hett'", "ich", "wol", "da\u00b7rauf", "ge\u00b7schwo\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PAV", "VVPP", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da\u00df du zum Liebsten mich allein erkohren/", "tokens": ["Da\u00df", "du", "zum", "Liebs\u00b7ten", "mich", "al\u00b7lein", "er\u00b7koh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Aber nun seh\u2019 ich da\u00df ich jenem Bleichen", "tokens": ["A\u00b7ber", "nun", "seh'", "ich", "da\u00df", "ich", "je\u00b7nem", "Blei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "KOUS", "PPER", "PDAT", "NN"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Schimpflich mu\u00df weichen.", "tokens": ["Schimpf\u00b7lich", "mu\u00df", "wei\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.7": {"line.1": {"text": "Warlich/ Er ist doch ja von schlechten Gaben/", "tokens": ["War\u00b7lich", "/", "Er", "ist", "doch", "ja", "von", "schlech\u00b7ten", "Ga\u00b7ben", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PPER", "VAFIN", "ADV", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wunder/ da\u00df du den Thoren lieb kanst haben/", "tokens": ["Wun\u00b7der", "/", "da\u00df", "du", "den", "Tho\u00b7ren", "lieb", "kanst", "ha\u00b7ben", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KOUS", "PPER", "ART", "NN", "ADJD", "VMFIN", "VAINF", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Doch! Er ist Reich/ hat grosse Klumpen Gelder", "tokens": ["Doch", "!", "Er", "ist", "Reich", "/", "hat", "gros\u00b7se", "Klum\u00b7pen", "Gel\u00b7der"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "$.", "PPER", "VAFIN", "NN", "$(", "VAFIN", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wiesen und Felder.", "tokens": ["Wie\u00b7sen", "und", "Fel\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.8": {"line.1": {"text": "H\u00f6re/ wie offt hast du gesagt: Jhr Sterne/", "tokens": ["H\u00f6\u00b7re", "/", "wie", "offt", "hast", "du", "ge\u00b7sagt", ":", "Ihr", "Ster\u00b7ne", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$(", "KOKOM", "ADV", "VAFIN", "PPER", "VVPP", "$.", "PPOSAT", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Zeuget da\u00df mein Gem\u00fcht\u2019 an Falschheit ferne/", "tokens": ["Zeu\u00b7get", "da\u00df", "mein", "Ge\u00b7m\u00fcht'", "an", "Falschheit", "fer\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOUS", "PPOSAT", "NN", "APPR", "NN", "ADV", "$("], "meter": "+--+-+-++-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Auch da\u00df mein Lieben sey ohn alles Triegen.", "tokens": ["Auch", "da\u00df", "mein", "Lie\u00b7ben", "sey", "ohn", "al\u00b7les", "Trie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPOSAT", "ADJA", "VAFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Pfuy der L\u00fcgen!", "tokens": ["Pfuy", "der", "L\u00fc\u00b7gen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.9": {"line.1": {"text": "Weistu nicht da\u00df der Himmel die gerochen/", "tokens": ["Weis\u00b7tu", "nicht", "da\u00df", "der", "Him\u00b7mel", "die", "ge\u00b7ro\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "KOUS", "ART", "NN", "ART", "ADJA", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Welche so sch\u00e4ndlich ihren Eyd gebrochen/", "tokens": ["Wel\u00b7che", "so", "sch\u00e4nd\u00b7lich", "ih\u00b7ren", "Eyd", "ge\u00b7bro\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJD", "PPOSAT", "NN", "VVPP", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Fahre nur hin du wirst es bald erfahren/", "tokens": ["Fah\u00b7re", "nur", "hin", "du", "wirst", "es", "bald", "er\u00b7fah\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "PPER", "VAFIN", "PPER", "ADV", "VVINF", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "In turtzen Jahren.", "tokens": ["In", "turt\u00b7zen", "Jah\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.10": {"line.1": {"text": "Aber nicht/ da\u00df mich etwan deine Schmertzen", "tokens": ["A\u00b7ber", "nicht", "/", "da\u00df", "mich", "et\u00b7wan", "dei\u00b7ne", "Schmert\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "PTKNEG", "$(", "KOUS", "PPER", "ADV", "PPOSAT", "NN"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "K\u00fctzelen solten. Nein/ ich trag\u2019 im Hertzen/", "tokens": ["K\u00fct\u00b7ze\u00b7len", "sol\u00b7ten", ".", "Nein", "/", "ich", "trag'", "im", "Hert\u00b7zen", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "$.", "PTKANT", "$(", "PPER", "VVFIN", "APPRART", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Als dein getreuer Sch\u00e4fer/ mit dir Armen/", "tokens": ["Als", "dein", "ge\u00b7treu\u00b7er", "Sch\u00e4\u00b7fer", "/", "mit", "dir", "Ar\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "$(", "APPR", "PPER", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Grosses Erbarmen.", "tokens": ["Gros\u00b7ses", "Er\u00b7bar\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.11": {"line.1": {"text": "Dieses nun bin ich von dir inne worden/", "tokens": ["Die\u00b7ses", "nun", "bin", "ich", "von", "dir", "in\u00b7ne", "wor\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "PPER", "APPR", "PPER", "PTKVZ", "VAPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sch\u00e4ndliche Doris aus dem falschen Orden.", "tokens": ["Sch\u00e4nd\u00b7li\u00b7che", "Do\u00b7ris", "aus", "dem", "fal\u00b7schen", "Or\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Doch acht\u2019 ichs nicht/ bistu doch nicht alleine/", "tokens": ["Doch", "acht'", "ichs", "nicht", "/", "bis\u00b7tu", "doch", "nicht", "al\u00b7lei\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PTKNEG", "$(", "ADV", "ADV", "PTKNEG", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die ich wolmeine.", "tokens": ["Die", "ich", "wol\u00b7mei\u00b7ne", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "VMFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Wilstu nicht lieben/ kanstu es wol lassen/", "tokens": ["Wils\u00b7tu", "nicht", "lie\u00b7ben", "/", "kans\u00b7tu", "es", "wol", "las\u00b7sen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "VVINF", "$(", "VMFIN", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "So wol als Hertzen kan ich dich auch hassen/", "tokens": ["So", "wol", "als", "Hert\u00b7zen", "kan", "ich", "dich", "auch", "has\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KOUS", "NN", "VMFIN", "PPER", "PRF", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Doris hinweg ein ander treues K\u00fcssen", "tokens": ["Do\u00b7ris", "hin\u00b7weg", "ein", "an\u00b7der", "treu\u00b7es", "K\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "APZR", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wil ich bald wissen.", "tokens": ["Wil", "ich", "bald", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "---+-", "measure": "unknown.measure.single"}}}}}