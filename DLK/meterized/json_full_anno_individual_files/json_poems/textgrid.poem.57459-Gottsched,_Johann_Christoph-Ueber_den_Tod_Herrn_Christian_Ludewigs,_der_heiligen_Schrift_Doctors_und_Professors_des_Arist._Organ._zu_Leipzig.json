{"textgrid.poem.57459": {"metadata": {"author": {"name": "Gottsched, Johann Christoph", "birth": "N.A.", "death": "N.A."}, "title": "Ueber den Tod Herrn Christian Ludewigs, der heiligen Schrift Doctors und Professors des Arist. Organ. zu Leipzig", "genre": "verse", "period": "N.A.", "pub_year": 1733, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Seit dem der Weise von Stagyr", "tokens": ["Seit", "dem", "der", "Wei\u00b7se", "von", "Sta\u00b7gyr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ART", "NN", "APPR", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Dem Denken Regeln vorgeschrieben,", "tokens": ["Dem", "Den\u00b7ken", "Re\u00b7geln", "vor\u00b7ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und unsre forschende Begier", "tokens": ["Und", "uns\u00b7re", "for\u00b7schen\u00b7de", "Be\u00b7gier"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.4": {"text": "Bis auf den h\u00f6chsten Punct getrieben;", "tokens": ["Bis", "auf", "den", "h\u00f6chs\u00b7ten", "Punct", "ge\u00b7trie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Seit dem der neuen Lehrer Zunft", "tokens": ["Seit", "dem", "der", "neu\u00b7en", "Leh\u00b7rer", "Zunft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Kunst noch mehr gepr\u00fcft, gebessert und erl\u00e4utert:", "tokens": ["Die", "Kunst", "noch", "mehr", "ge\u00b7pr\u00fcft", ",", "ge\u00b7bes\u00b7sert", "und", "er\u00b7l\u00e4u\u00b7tert", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "VVPP", "$,", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Sind auch die Kr\u00e4fte der Vernunft,", "tokens": ["Sind", "auch", "die", "Kr\u00e4f\u00b7te", "der", "Ver\u00b7nunft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Durch ungemeinen Flei\u00df, unendlich sehr erweitert.", "tokens": ["Durch", "un\u00b7ge\u00b7mei\u00b7nen", "Flei\u00df", ",", "un\u00b7end\u00b7lich", "sehr", "er\u00b7wei\u00b7tert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJD", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Des Erdballs Umkreis ist erkannt,", "tokens": ["Des", "Erd\u00b7balls", "Um\u00b7kreis", "ist", "er\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Inhalt durch und durch gemessen;", "tokens": ["Sein", "In\u00b7halt", "durch", "und", "durch", "ge\u00b7mes\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "KON", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die lange Ruh ist ganz verbannt,", "tokens": ["Die", "lan\u00b7ge", "Ruh", "ist", "ganz", "ver\u00b7bannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Darinn er vor der Zeit gesessen.", "tokens": ["Da\u00b7rinn", "er", "vor", "der", "Zeit", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er mu\u00df, nach der Planeten Art,", "tokens": ["Er", "mu\u00df", ",", "nach", "der", "Pla\u00b7ne\u00b7ten", "Art", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$,", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "Um seinen Mittelpunkt, den Sonnenk\u00f6rper rollen:", "tokens": ["Um", "sei\u00b7nen", "Mit\u00b7tel\u00b7punkt", ",", "den", "Son\u00b7nen\u00b7k\u00f6r\u00b7per", "rol\u00b7len", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "$,", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Da dieser seinen Lauf erspart,", "tokens": ["Da", "die\u00b7ser", "sei\u00b7nen", "Lauf", "er\u00b7spart", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und alle Sterne sonst geruhig stehen sollen.", "tokens": ["Und", "al\u00b7le", "Ster\u00b7ne", "sonst", "ge\u00b7ru\u00b7hig", "ste\u00b7hen", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Man schreibt dem Laufe der Natur", "tokens": ["Man", "schreibt", "dem", "Lau\u00b7fe", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die ordentlichsten Grundgesetze;", "tokens": ["Die", "or\u00b7dent\u00b7lichs\u00b7ten", "Grund\u00b7ge\u00b7set\u00b7ze", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man k\u00f6mmt auf ihrer Kr\u00e4fte Spur,", "tokens": ["Man", "k\u00f6mmt", "auf", "ih\u00b7rer", "Kr\u00e4f\u00b7te", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und findet der Bewegung Sch\u00e4tze.", "tokens": ["Und", "fin\u00b7det", "der", "Be\u00b7we\u00b7gung", "Sch\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man weis, was in den L\u00fcften kracht,", "tokens": ["Man", "weis", ",", "was", "in", "den", "L\u00fcf\u00b7ten", "kracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und was den Ocean zur Fluth und Ebbe zwinget?", "tokens": ["Und", "was", "den", "O\u00b7cean", "zur", "Fluth", "und", "Eb\u00b7be", "zwin\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "APPRART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Was Schlossen, Sturm und Regen macht?", "tokens": ["Was", "Schlos\u00b7sen", ",", "Sturm", "und", "Re\u00b7gen", "macht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$,", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Warum die Erde bebt, warum ihr Abgrund springet?", "tokens": ["Wa\u00b7rum", "die", "Er\u00b7de", "bebt", ",", "wa\u00b7rum", "ihr", "Ab\u00b7grund", "sprin\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "$,", "PWAV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Man hat den Menschen selbst erforscht,", "tokens": ["Man", "hat", "den", "Men\u00b7schen", "selbst", "er\u00b7forscht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und seiner Glieder Bau zerleget;", "tokens": ["Und", "sei\u00b7ner", "Glie\u00b7der", "Bau", "zer\u00b7le\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man weis, was unsern Leib zermorscht,", "tokens": ["Man", "weis", ",", "was", "un\u00b7sern", "Leib", "zer\u00b7morscht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und wie das Herz im Busen schl\u00e4get.", "tokens": ["Und", "wie", "das", "Herz", "im", "Bu\u00b7sen", "schl\u00e4\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man hat den Gliedern nachgesp\u00fcrt,", "tokens": ["Man", "hat", "den", "Glie\u00b7dern", "nach\u00b7ge\u00b7sp\u00fcrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die manchen Nervengang in das Gehirne schicken,", "tokens": ["Die", "man\u00b7chen", "Ner\u00b7ven\u00b7gang", "in", "das", "Ge\u00b7hir\u00b7ne", "schi\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Von dem, was sie von au\u00dfen r\u00fchrt,", "tokens": ["Von", "dem", ",", "was", "sie", "von", "au\u00b7\u00dfen", "r\u00fchrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PPER", "APPR", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Dem Geiste, der da wohnt, die Bilder einzudr\u00fccken.", "tokens": ["Dem", "Geis\u00b7te", ",", "der", "da", "wohnt", ",", "die", "Bil\u00b7der", "ein\u00b7zu\u00b7dr\u00fc\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "VVFIN", "$,", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Man hat so gar des Geistes Kraft,", "tokens": ["Man", "hat", "so", "gar", "des", "Geis\u00b7tes", "Kraft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der uns zu Menschen macht, ergr\u00fcndet;", "tokens": ["Der", "uns", "zu", "Men\u00b7schen", "macht", ",", "er\u00b7gr\u00fcn\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und kennt mit guter Wissenschaft,", "tokens": ["Und", "kennt", "mit", "gu\u00b7ter", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was in uns denket und empfindet.", "tokens": ["Was", "in", "uns", "den\u00b7ket", "und", "emp\u00b7fin\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PPER", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man thut sein einfach Wesen dar,", "tokens": ["Man", "thut", "sein", "ein\u00b7fach", "We\u00b7sen", "dar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "ADJD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das keine F\u00e4ulni\u00df trennt, kein Moder kann verderben;", "tokens": ["Das", "kei\u00b7ne", "F\u00e4ul\u00b7ni\u00df", "trennt", ",", "kein", "Mo\u00b7der", "kann", "ver\u00b7der\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "$,", "PIAT", "NN", "VMFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und macht es durch Beweise klar,", "tokens": ["Und", "macht", "es", "durch", "Be\u00b7wei\u00b7se", "klar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df unsre Seelen nicht, wie diese K\u00f6rper, sterben.", "tokens": ["Da\u00df", "uns\u00b7re", "See\u00b7len", "nicht", ",", "wie", "die\u00b7se", "K\u00f6r\u00b7per", ",", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKNEG", "$,", "PWAV", "PDAT", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Was giebt nicht ferner der Verstand", "tokens": ["Was", "giebt", "nicht", "fer\u00b7ner", "der", "Ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PTKNEG", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr auserlesne Sittenlehren?", "tokens": ["F\u00fcr", "au\u00b7ser\u00b7les\u00b7ne", "Sit\u00b7ten\u00b7leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er zeugt das Recht, der V\u00f6lker Band,", "tokens": ["Er", "zeugt", "das", "Recht", ",", "der", "V\u00f6l\u00b7ker", "Band", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und hilft der Staaten Wohlfahrt mehren.", "tokens": ["Und", "hilft", "der", "Staa\u00b7ten", "Wohl\u00b7fahrt", "meh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er schafft den B\u00fcrgern Sicherheit;", "tokens": ["Er", "schafft", "den", "B\u00fcr\u00b7gern", "Si\u00b7cher\u00b7heit", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ja wollte man sich stets nach seinen Regeln richten:", "tokens": ["Ja", "woll\u00b7te", "man", "sich", "stets", "nach", "sei\u00b7nen", "Re\u00b7geln", "rich\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VMFIN", "PIS", "PRF", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So br\u00e4cht er gar die g\u00fcldne Zeit,", "tokens": ["So", "br\u00e4cht", "er", "gar", "die", "g\u00fcld\u00b7ne", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Davon die Alten sonst die sch\u00f6nsten Fabeln dichten.", "tokens": ["Da\u00b7von", "die", "Al\u00b7ten", "sonst", "die", "sch\u00f6ns\u00b7ten", "Fa\u00b7beln", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "ART", "ADJA", "NN", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "O himmlisch wirkende Vernunft!", "tokens": ["O", "himm\u00b7lisch", "wir\u00b7ken\u00b7de", "Ver\u00b7nunft", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "O unbeschreiblich edles Wesen!", "tokens": ["O", "un\u00b7be\u00b7schreib\u00b7lich", "ed\u00b7les", "We\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was Dank verdient der Weisen Zunft,", "tokens": ["Was", "Dank", "ver\u00b7dient", "der", "Wei\u00b7sen", "Zunft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die dich zu ihrem Zweck erlesen!", "tokens": ["Die", "dich", "zu", "ih\u00b7rem", "Zweck", "er\u00b7le\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Du gleichfalls, hochverdienter Greis!", "tokens": ["Du", "gleich\u00b7falls", ",", "hoch\u00b7ver\u00b7dien\u00b7ter", "Greis", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Verdienst das ganze Lob, womit wir sie gepriesen;", "tokens": ["Ver\u00b7dienst", "das", "gan\u00b7ze", "Lob", ",", "wo\u00b7mit", "wir", "sie", "ge\u00b7prie\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "PWAV", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Indem du, wie ganz Leipzig weis,", "tokens": ["In\u00b7dem", "du", ",", "wie", "ganz", "Leip\u00b7zig", "weis", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PWAV", "ADV", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Die Regeln der Vernunft so manches Jahr gewiesen.", "tokens": ["Die", "Re\u00b7geln", "der", "Ver\u00b7nunft", "so", "man\u00b7ches", "Jahr", "ge\u00b7wie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "ADV", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Gewiesen? Ja! doch auch zugleich", "tokens": ["Ge\u00b7wie\u00b7sen", "?", "Ja", "!", "doch", "auch", "zu\u00b7gleich"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "$.", "PTKANT", "$.", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im Thun und Lassen angewendet;", "tokens": ["Im", "Thun", "und", "Las\u00b7sen", "an\u00b7ge\u00b7wen\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Im Ungl\u00fcck warst du niemals weich,", "tokens": ["Im", "Un\u00b7gl\u00fcck", "warst", "du", "nie\u00b7mals", "weich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kein gro\u00dfes Gl\u00fcck hat dich verblendet.", "tokens": ["Kein", "gro\u00b7\u00dfes", "Gl\u00fcck", "hat", "dich", "ver\u00b7blen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Du dientest Gott, der Welt, dem Staat,", "tokens": ["Du", "dien\u00b7test", "Gott", ",", "der", "Welt", ",", "dem", "Staat", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und wolltest jedem gern mit ganzen Kr\u00e4ften dienen:", "tokens": ["Und", "woll\u00b7test", "je\u00b7dem", "gern", "mit", "gan\u00b7zen", "Kr\u00e4f\u00b7ten", "die\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So da\u00df an dir, aus jeder That,", "tokens": ["So", "da\u00df", "an", "dir", ",", "aus", "je\u00b7der", "That", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "APPR", "PPER", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ein wahrer Philosoph und rechter Christ erschienen.", "tokens": ["Ein", "wah\u00b7rer", "Phi\u00b7lo\u00b7soph", "und", "rech\u00b7ter", "Christ", "er\u00b7schie\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Es lebt dein Ruhm in mancher Schrift,", "tokens": ["Es", "lebt", "dein", "Ruhm", "in", "man\u00b7cher", "Schrift", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Was darf ihn dieses Blatt beschreiben?", "tokens": ["Was", "darf", "ihn", "die\u00b7ses", "Blatt", "be\u00b7schrei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Das leichtlich Wurm und Motte trifft,", "tokens": ["Das", "leicht\u00b7lich", "Wurm", "und", "Mot\u00b7te", "trifft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Da jene wohl unsterblich bleiben.", "tokens": ["Da", "je\u00b7ne", "wohl", "uns\u00b7terb\u00b7lich", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Ruh sanft in deines Grabes Nacht,", "tokens": ["Ruh", "sanft", "in", "dei\u00b7nes", "Gra\u00b7bes", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Du werther Ueberrest! bis dich die Macht belebet,", "tokens": ["Du", "wert\u00b7her", "Ue\u00b7ber\u00b7rest", "!", "bis", "dich", "die", "Macht", "be\u00b7le\u00b7bet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$.", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die einst der Welt ein Ende macht,", "tokens": ["Die", "einst", "der", "Welt", "ein", "En\u00b7de", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Und dich, wie deinen Geist, zur Herrlichkeit erhebet.", "tokens": ["Und", "dich", ",", "wie", "dei\u00b7nen", "Geist", ",", "zur", "Herr\u00b7lich\u00b7keit", "er\u00b7he\u00b7bet", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "PWAV", "PPOSAT", "NN", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Seit dem der Weise von Stagyr", "tokens": ["Seit", "dem", "der", "Wei\u00b7se", "von", "Sta\u00b7gyr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ART", "NN", "APPR", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Dem Denken Regeln vorgeschrieben,", "tokens": ["Dem", "Den\u00b7ken", "Re\u00b7geln", "vor\u00b7ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und unsre forschende Begier", "tokens": ["Und", "uns\u00b7re", "for\u00b7schen\u00b7de", "Be\u00b7gier"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.4": {"text": "Bis auf den h\u00f6chsten Punct getrieben;", "tokens": ["Bis", "auf", "den", "h\u00f6chs\u00b7ten", "Punct", "ge\u00b7trie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Seit dem der neuen Lehrer Zunft", "tokens": ["Seit", "dem", "der", "neu\u00b7en", "Leh\u00b7rer", "Zunft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Kunst noch mehr gepr\u00fcft, gebessert und erl\u00e4utert:", "tokens": ["Die", "Kunst", "noch", "mehr", "ge\u00b7pr\u00fcft", ",", "ge\u00b7bes\u00b7sert", "und", "er\u00b7l\u00e4u\u00b7tert", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "VVPP", "$,", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Sind auch die Kr\u00e4fte der Vernunft,", "tokens": ["Sind", "auch", "die", "Kr\u00e4f\u00b7te", "der", "Ver\u00b7nunft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Durch ungemeinen Flei\u00df, unendlich sehr erweitert.", "tokens": ["Durch", "un\u00b7ge\u00b7mei\u00b7nen", "Flei\u00df", ",", "un\u00b7end\u00b7lich", "sehr", "er\u00b7wei\u00b7tert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJD", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Des Erdballs Umkreis ist erkannt,", "tokens": ["Des", "Erd\u00b7balls", "Um\u00b7kreis", "ist", "er\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Inhalt durch und durch gemessen;", "tokens": ["Sein", "In\u00b7halt", "durch", "und", "durch", "ge\u00b7mes\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "KON", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die lange Ruh ist ganz verbannt,", "tokens": ["Die", "lan\u00b7ge", "Ruh", "ist", "ganz", "ver\u00b7bannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Darinn er vor der Zeit gesessen.", "tokens": ["Da\u00b7rinn", "er", "vor", "der", "Zeit", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er mu\u00df, nach der Planeten Art,", "tokens": ["Er", "mu\u00df", ",", "nach", "der", "Pla\u00b7ne\u00b7ten", "Art", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$,", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "Um seinen Mittelpunkt, den Sonnenk\u00f6rper rollen:", "tokens": ["Um", "sei\u00b7nen", "Mit\u00b7tel\u00b7punkt", ",", "den", "Son\u00b7nen\u00b7k\u00f6r\u00b7per", "rol\u00b7len", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "$,", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Da dieser seinen Lauf erspart,", "tokens": ["Da", "die\u00b7ser", "sei\u00b7nen", "Lauf", "er\u00b7spart", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und alle Sterne sonst geruhig stehen sollen.", "tokens": ["Und", "al\u00b7le", "Ster\u00b7ne", "sonst", "ge\u00b7ru\u00b7hig", "ste\u00b7hen", "sol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Man schreibt dem Laufe der Natur", "tokens": ["Man", "schreibt", "dem", "Lau\u00b7fe", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die ordentlichsten Grundgesetze;", "tokens": ["Die", "or\u00b7dent\u00b7lichs\u00b7ten", "Grund\u00b7ge\u00b7set\u00b7ze", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man k\u00f6mmt auf ihrer Kr\u00e4fte Spur,", "tokens": ["Man", "k\u00f6mmt", "auf", "ih\u00b7rer", "Kr\u00e4f\u00b7te", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und findet der Bewegung Sch\u00e4tze.", "tokens": ["Und", "fin\u00b7det", "der", "Be\u00b7we\u00b7gung", "Sch\u00e4t\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man weis, was in den L\u00fcften kracht,", "tokens": ["Man", "weis", ",", "was", "in", "den", "L\u00fcf\u00b7ten", "kracht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und was den Ocean zur Fluth und Ebbe zwinget?", "tokens": ["Und", "was", "den", "O\u00b7cean", "zur", "Fluth", "und", "Eb\u00b7be", "zwin\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "APPRART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Was Schlossen, Sturm und Regen macht?", "tokens": ["Was", "Schlos\u00b7sen", ",", "Sturm", "und", "Re\u00b7gen", "macht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$,", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Warum die Erde bebt, warum ihr Abgrund springet?", "tokens": ["Wa\u00b7rum", "die", "Er\u00b7de", "bebt", ",", "wa\u00b7rum", "ihr", "Ab\u00b7grund", "sprin\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "$,", "PWAV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Man hat den Menschen selbst erforscht,", "tokens": ["Man", "hat", "den", "Men\u00b7schen", "selbst", "er\u00b7forscht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und seiner Glieder Bau zerleget;", "tokens": ["Und", "sei\u00b7ner", "Glie\u00b7der", "Bau", "zer\u00b7le\u00b7get", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man weis, was unsern Leib zermorscht,", "tokens": ["Man", "weis", ",", "was", "un\u00b7sern", "Leib", "zer\u00b7morscht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und wie das Herz im Busen schl\u00e4get.", "tokens": ["Und", "wie", "das", "Herz", "im", "Bu\u00b7sen", "schl\u00e4\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man hat den Gliedern nachgesp\u00fcrt,", "tokens": ["Man", "hat", "den", "Glie\u00b7dern", "nach\u00b7ge\u00b7sp\u00fcrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die manchen Nervengang in das Gehirne schicken,", "tokens": ["Die", "man\u00b7chen", "Ner\u00b7ven\u00b7gang", "in", "das", "Ge\u00b7hir\u00b7ne", "schi\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Von dem, was sie von au\u00dfen r\u00fchrt,", "tokens": ["Von", "dem", ",", "was", "sie", "von", "au\u00b7\u00dfen", "r\u00fchrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PPER", "APPR", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Dem Geiste, der da wohnt, die Bilder einzudr\u00fccken.", "tokens": ["Dem", "Geis\u00b7te", ",", "der", "da", "wohnt", ",", "die", "Bil\u00b7der", "ein\u00b7zu\u00b7dr\u00fc\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "VVFIN", "$,", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Man hat so gar des Geistes Kraft,", "tokens": ["Man", "hat", "so", "gar", "des", "Geis\u00b7tes", "Kraft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der uns zu Menschen macht, ergr\u00fcndet;", "tokens": ["Der", "uns", "zu", "Men\u00b7schen", "macht", ",", "er\u00b7gr\u00fcn\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und kennt mit guter Wissenschaft,", "tokens": ["Und", "kennt", "mit", "gu\u00b7ter", "Wis\u00b7sen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was in uns denket und empfindet.", "tokens": ["Was", "in", "uns", "den\u00b7ket", "und", "emp\u00b7fin\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PPER", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Man thut sein einfach Wesen dar,", "tokens": ["Man", "thut", "sein", "ein\u00b7fach", "We\u00b7sen", "dar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "ADJD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das keine F\u00e4ulni\u00df trennt, kein Moder kann verderben;", "tokens": ["Das", "kei\u00b7ne", "F\u00e4ul\u00b7ni\u00df", "trennt", ",", "kein", "Mo\u00b7der", "kann", "ver\u00b7der\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "$,", "PIAT", "NN", "VMFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und macht es durch Beweise klar,", "tokens": ["Und", "macht", "es", "durch", "Be\u00b7wei\u00b7se", "klar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df unsre Seelen nicht, wie diese K\u00f6rper, sterben.", "tokens": ["Da\u00df", "uns\u00b7re", "See\u00b7len", "nicht", ",", "wie", "die\u00b7se", "K\u00f6r\u00b7per", ",", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKNEG", "$,", "PWAV", "PDAT", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Was giebt nicht ferner der Verstand", "tokens": ["Was", "giebt", "nicht", "fer\u00b7ner", "der", "Ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PTKNEG", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr auserlesne Sittenlehren?", "tokens": ["F\u00fcr", "au\u00b7ser\u00b7les\u00b7ne", "Sit\u00b7ten\u00b7leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er zeugt das Recht, der V\u00f6lker Band,", "tokens": ["Er", "zeugt", "das", "Recht", ",", "der", "V\u00f6l\u00b7ker", "Band", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und hilft der Staaten Wohlfahrt mehren.", "tokens": ["Und", "hilft", "der", "Staa\u00b7ten", "Wohl\u00b7fahrt", "meh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er schafft den B\u00fcrgern Sicherheit;", "tokens": ["Er", "schafft", "den", "B\u00fcr\u00b7gern", "Si\u00b7cher\u00b7heit", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ja wollte man sich stets nach seinen Regeln richten:", "tokens": ["Ja", "woll\u00b7te", "man", "sich", "stets", "nach", "sei\u00b7nen", "Re\u00b7geln", "rich\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VMFIN", "PIS", "PRF", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So br\u00e4cht er gar die g\u00fcldne Zeit,", "tokens": ["So", "br\u00e4cht", "er", "gar", "die", "g\u00fcld\u00b7ne", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Davon die Alten sonst die sch\u00f6nsten Fabeln dichten.", "tokens": ["Da\u00b7von", "die", "Al\u00b7ten", "sonst", "die", "sch\u00f6ns\u00b7ten", "Fa\u00b7beln", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "ART", "ADJA", "NN", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "O himmlisch wirkende Vernunft!", "tokens": ["O", "himm\u00b7lisch", "wir\u00b7ken\u00b7de", "Ver\u00b7nunft", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "O unbeschreiblich edles Wesen!", "tokens": ["O", "un\u00b7be\u00b7schreib\u00b7lich", "ed\u00b7les", "We\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was Dank verdient der Weisen Zunft,", "tokens": ["Was", "Dank", "ver\u00b7dient", "der", "Wei\u00b7sen", "Zunft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die dich zu ihrem Zweck erlesen!", "tokens": ["Die", "dich", "zu", "ih\u00b7rem", "Zweck", "er\u00b7le\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Du gleichfalls, hochverdienter Greis!", "tokens": ["Du", "gleich\u00b7falls", ",", "hoch\u00b7ver\u00b7dien\u00b7ter", "Greis", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Verdienst das ganze Lob, womit wir sie gepriesen;", "tokens": ["Ver\u00b7dienst", "das", "gan\u00b7ze", "Lob", ",", "wo\u00b7mit", "wir", "sie", "ge\u00b7prie\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "PWAV", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Indem du, wie ganz Leipzig weis,", "tokens": ["In\u00b7dem", "du", ",", "wie", "ganz", "Leip\u00b7zig", "weis", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PWAV", "ADV", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Die Regeln der Vernunft so manches Jahr gewiesen.", "tokens": ["Die", "Re\u00b7geln", "der", "Ver\u00b7nunft", "so", "man\u00b7ches", "Jahr", "ge\u00b7wie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "ADV", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Gewiesen? Ja! doch auch zugleich", "tokens": ["Ge\u00b7wie\u00b7sen", "?", "Ja", "!", "doch", "auch", "zu\u00b7gleich"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "$.", "PTKANT", "$.", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im Thun und Lassen angewendet;", "tokens": ["Im", "Thun", "und", "Las\u00b7sen", "an\u00b7ge\u00b7wen\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Im Ungl\u00fcck warst du niemals weich,", "tokens": ["Im", "Un\u00b7gl\u00fcck", "warst", "du", "nie\u00b7mals", "weich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kein gro\u00dfes Gl\u00fcck hat dich verblendet.", "tokens": ["Kein", "gro\u00b7\u00dfes", "Gl\u00fcck", "hat", "dich", "ver\u00b7blen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Du dientest Gott, der Welt, dem Staat,", "tokens": ["Du", "dien\u00b7test", "Gott", ",", "der", "Welt", ",", "dem", "Staat", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und wolltest jedem gern mit ganzen Kr\u00e4ften dienen:", "tokens": ["Und", "woll\u00b7test", "je\u00b7dem", "gern", "mit", "gan\u00b7zen", "Kr\u00e4f\u00b7ten", "die\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So da\u00df an dir, aus jeder That,", "tokens": ["So", "da\u00df", "an", "dir", ",", "aus", "je\u00b7der", "That", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "APPR", "PPER", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ein wahrer Philosoph und rechter Christ erschienen.", "tokens": ["Ein", "wah\u00b7rer", "Phi\u00b7lo\u00b7soph", "und", "rech\u00b7ter", "Christ", "er\u00b7schie\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Es lebt dein Ruhm in mancher Schrift,", "tokens": ["Es", "lebt", "dein", "Ruhm", "in", "man\u00b7cher", "Schrift", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Was darf ihn dieses Blatt beschreiben?", "tokens": ["Was", "darf", "ihn", "die\u00b7ses", "Blatt", "be\u00b7schrei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Das leichtlich Wurm und Motte trifft,", "tokens": ["Das", "leicht\u00b7lich", "Wurm", "und", "Mot\u00b7te", "trifft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Da jene wohl unsterblich bleiben.", "tokens": ["Da", "je\u00b7ne", "wohl", "uns\u00b7terb\u00b7lich", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Ruh sanft in deines Grabes Nacht,", "tokens": ["Ruh", "sanft", "in", "dei\u00b7nes", "Gra\u00b7bes", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Du werther Ueberrest! bis dich die Macht belebet,", "tokens": ["Du", "wert\u00b7her", "Ue\u00b7ber\u00b7rest", "!", "bis", "dich", "die", "Macht", "be\u00b7le\u00b7bet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$.", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die einst der Welt ein Ende macht,", "tokens": ["Die", "einst", "der", "Welt", "ein", "En\u00b7de", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Und dich, wie deinen Geist, zur Herrlichkeit erhebet.", "tokens": ["Und", "dich", ",", "wie", "dei\u00b7nen", "Geist", ",", "zur", "Herr\u00b7lich\u00b7keit", "er\u00b7he\u00b7bet", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "PWAV", "PPOSAT", "NN", "$,", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}