{"dta.poem.11328": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "Auf die verm\u00e4hlung des durchl. Hessen-  \n Casselischen erb-printzen, mit der Chur-  \n Brandenburgischen printze\u00dfin  \n Louisa, Dorothea, Sophia.  \n  An.  1700.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1709", "urn": "urn:nbn:de:kobv:b4-20283-5", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Berlin warff unl\u00e4ngst von der Spree", "tokens": ["Ber\u00b7lin", "warff", "un\u00b7l\u00e4ngst", "von", "der", "Spree"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die augen \u00fcber thal und h\u00fcgel,", "tokens": ["Die", "au\u00b7gen", "\u00fc\u00b7ber", "thal", "und", "h\u00fc\u00b7gel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und sahe, gleichsam als im spiegel,", "tokens": ["Und", "sa\u00b7he", ",", "gleich\u00b7sam", "als", "im", "spie\u00b7gel", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ADJD", "KOKOM", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein gl\u00fcck und andrer st\u00e4dte weh.", "tokens": ["Sein", "gl\u00fcck", "und", "an\u00b7drer", "st\u00e4d\u00b7te", "weh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "ADJA", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "O! rief es, du verachter sand!", "tokens": ["O", "!", "rief", "es", ",", "du", "ver\u00b7ach\u00b7ter", "sand", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "PPER", "$,", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wirst mir ja wohl zu lauter eronen:", "tokens": ["Wirst", "mir", "ja", "wohl", "zu", "lau\u00b7ter", "e\u00b7ro\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "PTKA", "ADJD", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Denn wo ist heute doch ein land,", "tokens": ["Denn", "wo", "ist", "heu\u00b7te", "doch", "ein", "land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "VAFIN", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da mehr erfreute seelen wohnen?", "tokens": ["Da", "mehr", "er\u00b7freu\u00b7te", "see\u00b7len", "woh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wenn mancher, den beym \u00fcberflu\u00df", "tokens": ["Wenn", "man\u00b7cher", ",", "den", "beym", "\u00fc\u00b7berf\u00b7lu\u00df"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PIS", "$,", "PRELS", "APPRART", "NN"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Gewinst und mord-begierde plaget,", "tokens": ["Ge\u00b7winst", "und", "mord\u00b7be\u00b7gier\u00b7de", "pla\u00b7get", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An seiner b\u00fcrger knochen naget,", "tokens": ["An", "sei\u00b7ner", "b\u00fcr\u00b7ger", "kno\u00b7chen", "na\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und jeder f\u00fcr ihn sorgen mu\u00df;", "tokens": ["Und", "je\u00b7der", "f\u00fcr", "ihn", "sor\u00b7gen", "mu\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So sorgt mein kluger held f\u00fcr mich,", "tokens": ["So", "sorgt", "mein", "klu\u00b7ger", "held", "f\u00fcr", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "ADJA", "VVFIN", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Auch mitten unter sturm und kriegen:", "tokens": ["Auch", "mit\u00b7ten", "un\u00b7ter", "sturm", "und", "krie\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ADJD", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Alsdenn gedenckt er erst an sich,", "tokens": ["Als\u00b7denn", "ge\u00b7denckt", "er", "erst", "an", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PRF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und an sein eigenes vergn\u00fcgen.", "tokens": ["Und", "an", "sein", "ei\u00b7ge\u00b7nes", "ver\u00b7gn\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "ADJA", "VVINF", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.3": {"line.1": {"text": "Pari\u00df und Londen ist sehr gro\u00df;", "tokens": ["Pa\u00b7ri\u00df", "und", "Lon\u00b7den", "ist", "sehr", "gro\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NE", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Allein es hat die weiten schrancken", "tokens": ["Al\u00b7lein", "es", "hat", "die", "wei\u00b7ten", "schran\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VAFIN", "ART", "ADJA", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mehr der natur als kunst zu dancken:", "tokens": ["Mehr", "der", "na\u00b7tur", "als", "kunst", "zu", "dan\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ART", "NN", "KOKOM", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich war hingegen \u00f6d und blos;", "tokens": ["Ich", "war", "hin\u00b7ge\u00b7gen", "\u00f6d", "und", "blos", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KON", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich hatte kaum ein rechtes dach,", "tokens": ["Ich", "hat\u00b7te", "kaum", "ein", "rech\u00b7tes", "dach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und konte von natur nichts hoffen;", "tokens": ["Und", "kon\u00b7te", "von", "na\u00b7tur", "nichts", "hof\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Doch hab ich alles tausendfach,", "tokens": ["Doch", "hab", "ich", "al\u00b7les", "tau\u00b7send\u00b7fach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "In zweyen f\u00fcrsten angetroffen.", "tokens": ["In", "zwe\u00b7yen", "f\u00fcrs\u00b7ten", "an\u00b7ge\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der eine baute hau\u00df auf hau\u00df:", "tokens": ["Der", "ei\u00b7ne", "bau\u00b7te", "hau\u00df", "auf", "hau\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der andre will sie g\u00fclden schaffen;", "tokens": ["Der", "and\u00b7re", "will", "sie", "g\u00fcl\u00b7den", "schaf\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VMFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er f\u00fchrt es aber nicht mit waffen,", "tokens": ["Er", "f\u00fchrt", "es", "a\u00b7ber", "nicht", "mit", "waf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und durch bekannte mittel aus.", "tokens": ["Und", "durch", "be\u00b7kann\u00b7te", "mit\u00b7tel", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Es ist was altes, da\u00df ein staat", "tokens": ["Es", "ist", "was", "al\u00b7tes", ",", "da\u00df", "ein", "staat"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIS", "ADJA", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Durch raub und wucher zugenommen:", "tokens": ["Durch", "raub", "und", "wu\u00b7cher", "zu\u00b7ge\u00b7nom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Mein reichthum mu\u00df, auf Friedrichs rath,", "tokens": ["Mein", "reicht\u00b7hum", "mu\u00df", ",", "auf", "Fried\u00b7richs", "rath", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "$,", "APPR", "NE", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Durch wohlthun an verjagten kommen.", "tokens": ["Durch", "wohl\u00b7thun", "an", "ver\u00b7jag\u00b7ten", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "APPR", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ach! da\u00df doch helden menschen seyn,", "tokens": ["Ach", "!", "da\u00df", "doch", "hel\u00b7den", "men\u00b7schen", "seyn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "KOUS", "ADV", "ADJA", "NN", "VAINF", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und solche f\u00fcrsten sterben sollen!", "tokens": ["Und", "sol\u00b7che", "f\u00fcrs\u00b7ten", "ster\u00b7ben", "sol\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Viel sind zwar, die es auch seyn wollen;", "tokens": ["Viel", "sind", "zwar", ",", "die", "es", "auch", "seyn", "wol\u00b7len", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "$,", "PRELS", "PPER", "ADV", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie sind es aber sich allein.", "tokens": ["Sie", "sind", "es", "a\u00b7ber", "sich", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PRF", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie siegen; doch ein jeder streich", "tokens": ["Sie", "sie\u00b7gen", ";", "doch", "ein", "je\u00b7der", "streich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "ADV", "ART", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kost auch zugleich zwey unterthanen:", "tokens": ["Kost", "auch", "zu\u00b7gleich", "zwey", "un\u00b7ter\u00b7tha\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "CARD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Mein Churf\u00fcrst mehret land und reich,", "tokens": ["Mein", "Chur\u00b7f\u00fcrst", "meh\u00b7ret", "land", "und", "reich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und brauchet nichts als friedens-fahnen.", "tokens": ["Und", "brau\u00b7chet", "nichts", "als", "frie\u00b7dens\u00b7fah\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Das blut, so dich, Tiber! ergetzt", "tokens": ["Das", "blut", ",", "so", "dich", ",", "Ti\u00b7ber", "!", "er\u00b7getzt"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "ADV", "PPER", "$,", "NE", "$.", "VVPP"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Bey t\u00f6dung deiner n\u00e4chsten freunde,", "tokens": ["Bey", "t\u00f6\u00b7dung", "dei\u00b7ner", "n\u00e4chs\u00b7ten", "freun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Schont dieser auch an seinem feinde,", "tokens": ["Schont", "die\u00b7ser", "auch", "an", "sei\u00b7nem", "fein\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So bald sich die gefahr gesetzt.", "tokens": ["So", "bald", "sich", "die", "ge\u00b7fahr", "ge\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PRF", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sein dr\u00e4uen ist zwar eitel that,", "tokens": ["Sein", "dr\u00e4u\u00b7en", "ist", "zwar", "ei\u00b7tel", "that", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VAFIN", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und wenn er schl\u00e4gt, so will er siegen;", "tokens": ["Und", "wenn", "er", "schl\u00e4gt", ",", "so", "will", "er", "sie\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVFIN", "$,", "ADV", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Doch wenn er \u00fcberwunden hat,", "tokens": ["Doch", "wenn", "er", "\u00fc\u00b7berw\u00b7un\u00b7den", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So l\u00e4st er vater-blicke fliegen.", "tokens": ["So", "l\u00e4st", "er", "va\u00b7ter\u00b7bli\u00b7cke", "flie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Erl\u00f6stes Bonn und Kayserswerth!", "tokens": ["Er\u00b7l\u00f6s\u00b7tes", "Bonn", "und", "Kay\u00b7sers\u00b7werth", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jhr k\u00f6nnt am besten hievon zeugen.", "tokens": ["Ihr", "k\u00f6nnt", "am", "bes\u00b7ten", "hie\u00b7von", "zeu\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKA", "ADJD", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Eur gut und alles war sein eigen:", "tokens": ["Eur", "gut", "und", "al\u00b7les", "war", "sein", "ei\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "KON", "PIS", "VAFIN", "PPOSAT", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die w\u00e4lle lagen umgekehrt;", "tokens": ["Die", "w\u00e4l\u00b7le", "la\u00b7gen", "um\u00b7ge\u00b7kehrt", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Warff aber gleich sein zorn mit euch", "tokens": ["Warff", "a\u00b7ber", "gleich", "sein", "zorn", "mit", "euch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "ADV", "ADV", "PPOSAT", "NN", "APPR", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Viel tausend stoltze Frantzen nieder;", "tokens": ["Viel", "tau\u00b7send", "stolt\u00b7ze", "Frant\u00b7zen", "nie\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So schenckte dennoch sein vergleich", "tokens": ["So", "schenck\u00b7te", "den\u00b7noch", "sein", "ver\u00b7gleich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "PPOSAT", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Weit mehrern noch das leben wieder.", "tokens": ["Weit", "meh\u00b7rern", "noch", "das", "le\u00b7ben", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "ADV", "PDS", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Drum sieht man auch um meinen held", "tokens": ["Drum", "sieht", "man", "auch", "um", "mei\u00b7nen", "held"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PIS", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nichts, als vergn\u00fcgung, heil und segen.", "tokens": ["Nichts", ",", "als", "ver\u00b7gn\u00fc\u00b7gung", ",", "heil", "und", "se\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "KOUS", "NN", "$,", "ADJD", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sein schwerd hilfft grosse kriege legen:", "tokens": ["Sein", "schwerd", "hilfft", "gros\u00b7se", "krie\u00b7ge", "le\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sein hof erschallt durch alle welt;", "tokens": ["Sein", "hof", "er\u00b7schallt", "durch", "al\u00b7le", "welt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Theils weil er kluge diener macht,", "tokens": ["Theils", "weil", "er", "klu\u00b7ge", "die\u00b7ner", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOUS", "PPER", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die alles, wie sie sollen, f\u00fchren;", "tokens": ["Die", "al\u00b7les", ",", "wie", "sie", "sol\u00b7len", ",", "f\u00fch\u00b7ren", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PIS", "$,", "PWAV", "PPER", "VMFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Theils, weil ihn, nebst Charlottens pracht,", "tokens": ["Theils", ",", "weil", "ihn", ",", "nebst", "Char\u00b7lot\u00b7tens", "pracht", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUS", "PPER", "$,", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Auch zwey der gr\u00f6sten kinder zieren.", "tokens": ["Auch", "zwey", "der", "gr\u00f6s\u00b7ten", "kin\u00b7der", "zie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Ach aber! wie geschiehet mir?", "tokens": ["Ach", "a\u00b7ber", "!", "wie", "ge\u00b7schie\u00b7het", "mir", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "$.", "PWAV", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo denck ich hin? Was will ich nennen?", "tokens": ["Wo", "denck", "ich", "hin", "?", "Was", "will", "ich", "nen\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVIMP", "PPER", "PTKVZ", "$.", "PWS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der himmel will die letzten trennen:", "tokens": ["Der", "him\u00b7mel", "will", "die", "letz\u00b7ten", "tren\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Louise ist am l\u00e4ngsten hier.", "tokens": ["Lou\u00b7i\u00b7se", "ist", "am", "l\u00e4ngs\u00b7ten", "hier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "APPRART", "ADJA", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie scheidet, und (o hartes wort!)", "tokens": ["Sie", "schei\u00b7det", ",", "und", "(", "o", "har\u00b7tes", "wort", "!", ")"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "$(", "FM", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sie scheidet, auf ihr meistes leben,", "tokens": ["Sie", "schei\u00b7det", ",", "auf", "ihr", "meis\u00b7tes", "le\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "APPR", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Und nimmt in einem tage fort,", "tokens": ["Und", "nimmt", "in", "ei\u00b7nem", "ta\u00b7ge", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Was hundert jahr kaum wieder geben.", "tokens": ["Was", "hun\u00b7dert", "jahr", "kaum", "wie\u00b7der", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "CARD", "NN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Zwar sie trifft ein gew\u00fcnschtes lo\u00df;", "tokens": ["Zwar", "sie", "trifft", "ein", "ge\u00b7w\u00fcnschtes", "lo\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ART", "ADJA", "PTKVZ", "$."], "meter": "-+-+---", "measure": "unknown.measure.di"}, "line.2": {"text": "Sie kehrt in Hessens stamm zur\u00fccke,", "tokens": ["Sie", "kehrt", "in", "Hes\u00b7sens", "stamm", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und macht mit sich und ihrem gl\u00fccke", "tokens": ["Und", "macht", "mit", "sich", "und", "ih\u00b7rem", "gl\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PRF", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Auch zweyer v\u00f6lcker hoffnung gro\u00df.", "tokens": ["Auch", "zwey\u00b7er", "v\u00f6l\u00b7cker", "hoff\u00b7nung", "gro\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie wird durch ein gemahl erfreut,", "tokens": ["Sie", "wird", "durch", "ein", "ge\u00b7mahl", "er\u00b7freut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der mehr verdienst als jahre zehlet,", "tokens": ["Der", "mehr", "ver\u00b7dienst", "als", "jah\u00b7re", "zeh\u00b7let", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "KOKOM", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Und dem nichts zur vollkommenheit,", "tokens": ["Und", "dem", "nichts", "zur", "voll\u00b7kom\u00b7men\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PIS", "APPRART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "Als eine solche f\u00fcrstin, fehlet.", "tokens": ["Als", "ei\u00b7ne", "sol\u00b7che", "f\u00fcrs\u00b7tin", ",", "feh\u00b7let", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "ART", "PIAT", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Doch dieses, was ihr ruh gebiehrt,", "tokens": ["Doch", "die\u00b7ses", ",", "was", "ihr", "ruh", "ge\u00b7biehrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hei\u00dft mich zum theil in unruh stehen.", "tokens": ["Hei\u00dft", "mich", "zum", "theil", "in", "un\u00b7ruh", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich seh\u2019, und kan doch auch nicht sehen,", "tokens": ["Ich", "seh'", ",", "und", "kan", "doch", "auch", "nicht", "se\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VMFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wieviel mein hof dabey verliehrt.", "tokens": ["Wie\u00b7viel", "mein", "hof", "da\u00b7bey", "ver\u00b7liehrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PAV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Es ist auf einmal tag und nacht:", "tokens": ["Es", "ist", "auf", "ein\u00b7mal", "tag", "und", "nacht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Man singt und jauchtzt, man seufftzt und zaget:", "tokens": ["Man", "singt", "und", "jauchtzt", ",", "man", "seufftzt", "und", "za\u00b7get", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "$,", "PIS", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Kein hau\u00df ist, das nicht heute lacht;", "tokens": ["Kein", "hau\u00df", "ist", ",", "das", "nicht", "heu\u00b7te", "lacht", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "$,", "PRELS", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Allein auch keines, das nicht klaget.", "tokens": ["Al\u00b7lein", "auch", "kei\u00b7nes", ",", "das", "nicht", "kla\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PIS", "$,", "PRELS", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Du selbst, mein theurer Friderich!", "tokens": ["Du", "selbst", ",", "mein", "theu\u00b7rer", "Fri\u00b7de\u00b7rich", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gehst gleichsam bey der lust im leide:", "tokens": ["Gehst", "gleich\u00b7sam", "bey", "der", "lust", "im", "lei\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "APPRART", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Louisens bindni\u00df macht dir freude;", "tokens": ["Lou\u00b7i\u00b7sens", "bind\u00b7ni\u00df", "macht", "dir", "freu\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Jhr abzug aber jammert dich.", "tokens": ["Ihr", "ab\u00b7zug", "a\u00b7ber", "jam\u00b7mert", "dich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Du zeigst in beyden muth und hertz;", "tokens": ["Du", "zeigst", "in", "bey\u00b7den", "muth", "und", "hertz", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIAT", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dort aber frey, und hier gezwungen:", "tokens": ["Dort", "a\u00b7ber", "frey", ",", "und", "hier", "ge\u00b7zwun\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$,", "KON", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Was ist denn wunder, da\u00df der schmertz", "tokens": ["Was", "ist", "denn", "wun\u00b7der", ",", "da\u00df", "der", "schmertz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "ADJA", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Mir auch die thr\u00e4nen abgedrungen?", "tokens": ["Mir", "auch", "die", "thr\u00e4\u00b7nen", "ab\u00b7ge\u00b7drun\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "So weit vertieffte sich Berlin:", "tokens": ["So", "weit", "ver\u00b7tieff\u00b7te", "sich", "Ber\u00b7lin", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PRF", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Gleich aber ward des himmels bogen", "tokens": ["Gleich", "a\u00b7ber", "ward", "des", "him\u00b7mels", "bo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mit licht und klarheit \u00fcberzogen:", "tokens": ["Mit", "licht", "und", "klar\u00b7heit", "\u00fc\u00b7berz\u00b7o\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die wolcken fiengen an zu fliehn:", "tokens": ["Die", "wol\u00b7cken", "fi\u00b7en\u00b7gen", "an", "zu", "fliehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und endlich lie\u00df, ich wei\u00df nicht, wie?", "tokens": ["Und", "end\u00b7lich", "lie\u00df", ",", "ich", "wei\u00df", "nicht", ",", "wie", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "$,", "PPER", "VVFIN", "PTKNEG", "$,", "PWAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich in der Spree beschilfften r\u00f6hren,", "tokens": ["Sich", "in", "der", "Spree", "be\u00b7schilff\u00b7ten", "r\u00f6h\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NE", "VVFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Zu tilgung aller angst und m\u00fch,", "tokens": ["Zu", "til\u00b7gung", "al\u00b7ler", "angst", "und", "m\u00fch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PIAT", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Gantz deutlich diese stimme h\u00f6ren:", "tokens": ["Gantz", "deut\u00b7lich", "die\u00b7se", "stim\u00b7me", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PDAT", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Halt ein, Berlin! du klagst zu sehr.", "tokens": ["Halt", "ein", ",", "Ber\u00b7lin", "!", "du", "klagst", "zu", "sehr", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "$,", "NE", "$.", "PPER", "VVFIN", "APPR", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Dein hof hat in Charlottens gaben", "tokens": ["Dein", "hof", "hat", "in", "Char\u00b7lot\u00b7tens", "ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "NE", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mehr, als viel k\u00f6nigreiche haben.", "tokens": ["Mehr", ",", "als", "viel", "k\u00f6\u00b7nig\u00b7rei\u00b7che", "ha\u00b7ben", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "KOUS", "PIAT", "ADJA", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bleibt diese hier, was wilst du mehr?", "tokens": ["Bleibt", "die\u00b7se", "hier", ",", "was", "wilst", "du", "mehr", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "ADV", "$,", "PWS", "VMFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der himmel hat gantz recht gethan.", "tokens": ["Der", "him\u00b7mel", "hat", "gantz", "recht", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Gieb Cassel, was es dir geliehen.", "tokens": ["Gieb", "Cas\u00b7sel", ",", "was", "es", "dir", "ge\u00b7lie\u00b7hen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "$,", "PWS", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Stund dir von ihm die mutter an:", "tokens": ["Stund", "dir", "von", "ihm", "die", "mut\u00b7ter", "an", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So la\u00df auch nun die tochter ziehen.", "tokens": ["So", "la\u00df", "auch", "nun", "die", "toch\u00b7ter", "zie\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "O! sprach die frohe stadt hierauf:", "tokens": ["O", "!", "sprach", "die", "fro\u00b7he", "stadt", "hier\u00b7auf", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "ART", "ADJA", "NN", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wer will den schlu\u00df des H\u00f6chsten trennen?", "tokens": ["Wer", "will", "den", "schlu\u00df", "des", "H\u00f6chs\u00b7ten", "tren\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zeuch hin, du tugend-stern der Brennen!", "tokens": ["Zeuch", "hin", ",", "du", "tu\u00b7gen\u00b7ds\u00b7tern", "der", "Bren\u00b7nen", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,", "PPER", "NN", "ART", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und geh nunmehr in Hessen auf!", "tokens": ["Und", "geh", "nun\u00b7mehr", "in", "Hes\u00b7sen", "auf", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Du nimmst des vaters hertz mit dir:", "tokens": ["Du", "nimmst", "des", "va\u00b7ters", "hertz", "mit", "dir", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ach! schaffe, da\u00df sein wunsch gedeye,", "tokens": ["Ach", "!", "schaf\u00b7fe", ",", "da\u00df", "sein", "wunsch", "ge\u00b7de\u00b7ye", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "$,", "KOUS", "PPOSAT", "ADJD", "ADJA", "$,"], "meter": "-+---+--+", "measure": "iambic.tri.chol"}, "line.7": {"text": "Und bring bald einen held herf\u00fcr,", "tokens": ["Und", "bring", "bald", "ei\u00b7nen", "held", "her\u00b7f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIS", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der Teutschland, ihn, und dich erfreue!", "tokens": ["Der", "Teutschland", ",", "ihn", ",", "und", "dich", "er\u00b7freu\u00b7e", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "$,", "KON", "PPER", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}}}}