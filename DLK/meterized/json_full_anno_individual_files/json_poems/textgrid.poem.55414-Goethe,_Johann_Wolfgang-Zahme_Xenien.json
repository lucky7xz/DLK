{"textgrid.poem.55414": {"metadata": {"author": {"name": "Goethe, Johann Wolfgang", "birth": "N.A.", "death": "N.A."}, "title": "Zahme Xenien", "genre": "verse", "period": "N.A.", "pub_year": 1817, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich rufe dich, verrufnes Wort,", "tokens": ["Ich", "ru\u00b7fe", "dich", ",", "ver\u00b7ruf\u00b7nes", "Wort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zur Ordnung auf des Tags:", "tokens": ["Zur", "Ord\u00b7nung", "auf", "des", "Tags", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Denn Wichte, Schelme solchen Schlags,", "tokens": ["Denn", "Wich\u00b7te", ",", "Schel\u00b7me", "sol\u00b7chen", "Schlags", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die wirken immer fort.", "tokens": ["Die", "wir\u00b7ken", "im\u00b7mer", "fort", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "\u00bbwarum willst du dich von uns allen", "tokens": ["\u00bb", "wa\u00b7rum", "willst", "du", "dich", "von", "uns", "al\u00b7len"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "PRF", "APPR", "PPER", "PIAT"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und unsrer Meinung entfernen?\u00ab", "tokens": ["Und", "uns\u00b7rer", "Mei\u00b7nung", "ent\u00b7fer\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "++-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ich schreibe nicht, euch zu gefallen,", "tokens": ["Ich", "schrei\u00b7be", "nicht", ",", "euch", "zu", "ge\u00b7fal\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ihr sollt was lernen!", "tokens": ["Ihr", "sollt", "was", "ler\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "\u00bbist denn das klug und wohlgetan?", "tokens": ["\u00bb", "ist", "denn", "das", "klug", "und", "wohl\u00b7ge\u00b7tan", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "ADV", "ART", "ADJD", "KON", "PWAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was willst du Freund' und Feinde kr\u00e4nken!\u00ab", "tokens": ["Was", "willst", "du", "Freund'", "und", "Fein\u00b7de", "kr\u00e4n\u00b7ken", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Erwachsne gehn mich nichts mehr an,", "tokens": ["Er\u00b7wachs\u00b7ne", "gehn", "mich", "nichts", "mehr", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PIS", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich mu\u00df nun an die Enkel denken.", "tokens": ["Ich", "mu\u00df", "nun", "an", "die", "En\u00b7kel", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und sollst auch du und du und du", "tokens": ["Und", "sollst", "auch", "du", "und", "du", "und", "du"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "PPER", "KON", "PPER", "KON", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht gleich mit mir zerfallen;", "tokens": ["Nicht", "gleich", "mit", "mir", "zer\u00b7fal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was ich dem Enkel zuliebe tu,", "tokens": ["Was", "ich", "dem", "En\u00b7kel", "zu\u00b7lie\u00b7be", "tu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "VVFIN", "NE", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Tu ich euch allen.", "tokens": ["Tu", "ich", "euch", "al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PPER", "PIAT", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Verzeiht einmal dem raschen Wort,", "tokens": ["Ver\u00b7zeiht", "ein\u00b7mal", "dem", "ra\u00b7schen", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und so verzeiht dem Plaudern;", "tokens": ["Und", "so", "ver\u00b7zeiht", "dem", "Plau\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Denn jetzo w\u00e4r's nicht ganz am Ort,", "tokens": ["Denn", "jet\u00b7zo", "w\u00e4r's", "nicht", "ganz", "am", "Ort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PTKNEG", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie bis hieher zu zaudern.", "tokens": ["Wie", "bis", "hie\u00b7her", "zu", "zau\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PAV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Wer in der Weltgeschichte lebt,", "tokens": ["Wer", "in", "der", "Welt\u00b7ge\u00b7schich\u00b7te", "lebt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Augenblick sollt er sich richten?", "tokens": ["Dem", "Au\u00b7gen\u00b7blick", "sollt", "er", "sich", "rich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Wer in die Zeiten schaut und strebt,", "tokens": ["Wer", "in", "die", "Zei\u00b7ten", "schaut", "und", "strebt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nur der ist wert, zu sprechen und zu dichten.", "tokens": ["Nur", "der", "ist", "wert", ",", "zu", "spre\u00b7chen", "und", "zu", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VAFIN", "ADJD", "$,", "PTKZU", "VVINF", "KON", "APPR", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "\u00bbsag mir, worauf die B\u00f6sen sinnen?\u00ab", "tokens": ["\u00bb", "sag", "mir", ",", "wo\u00b7rauf", "die", "B\u00f6\u00b7sen", "sin\u00b7nen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "$,", "PWAV", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Andern den Tag zu verderben,", "tokens": ["An\u00b7dern", "den", "Tag", "zu", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "ART", "NN", "PTKZU", "VVFIN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Sich den Tag zu gewinnen:", "tokens": ["Sich", "den", "Tag", "zu", "ge\u00b7win\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Das, meinen sie, hei\u00dfe erwerben.", "tokens": ["Das", ",", "mei\u00b7nen", "sie", ",", "hei\u00b7\u00dfe", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "$,", "VVFIN", "PPER", "$,", "VVFIN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.8": {"line.1": {"text": "\u00bbwas ist denn deine Absicht gewesen,", "tokens": ["\u00bb", "was", "ist", "denn", "dei\u00b7ne", "Ab\u00b7sicht", "ge\u00b7we\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ADV", "PPOSAT", "NN", "VAPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Jetzt neue Feuer anzubrennen?\u00ab", "tokens": ["Jetzt", "neu\u00b7e", "Feu\u00b7er", "an\u00b7zu\u00b7bren\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJA", "NN", "VVIZU", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Diejenigen sollen's lesen,", "tokens": ["Die\u00b7je\u00b7ni\u00b7gen", "sol\u00b7len's", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die mich nicht mehr h\u00f6ren k\u00f6nnen.", "tokens": ["Die", "mich", "nicht", "mehr", "h\u00f6\u00b7ren", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKNEG", "ADV", "VVINF", "VMINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Einen langen Tag \u00fcber lebt ich sch\u00f6n,", "tokens": ["Ei\u00b7nen", "lan\u00b7gen", "Tag", "\u00fc\u00b7ber", "lebt", "ich", "sch\u00f6n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Eine kurze Nacht.", "tokens": ["Ei\u00b7ne", "kur\u00b7ze", "Nacht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Die Sonne war eben im Aufgehn,", "tokens": ["Die", "Son\u00b7ne", "war", "e\u00b7ben", "im", "Auf\u00b7gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Als ich zu neuem", "tokens": ["Als", "ich", "zu", "neu\u00b7em"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Tag erwacht.", "tokens": ["Tag", "er\u00b7wacht", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.10": {"line.1": {"text": "\u00bbdeine Z\u00f6glinge m\u00f6chten dich fragen:", "tokens": ["\u00bb", "dei\u00b7ne", "Z\u00f6g\u00b7lin\u00b7ge", "m\u00f6ch\u00b7ten", "dich", "fra\u00b7gen", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "VMFIN", "PRF", "VVINF", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Lange lebten wir gern auf Erden,", "tokens": ["Lan\u00b7ge", "leb\u00b7ten", "wir", "gern", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Was willst du uns f\u00fcr Lehre sagen?\u00ab", "tokens": ["Was", "willst", "du", "uns", "f\u00fcr", "Leh\u00b7re", "sa\u00b7gen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PRF", "APPR", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Keine Kunst ist's, alt zu werden,", "tokens": ["Kei\u00b7ne", "Kunst", "ist's", ",", "alt", "zu", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "$,", "ADJD", "PTKZU", "VAINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Es ist Kunst, es zu ertragen.", "tokens": ["Es", "ist", "Kunst", ",", "es", "zu", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Nachdem einer ringt,", "tokens": ["Nach\u00b7dem", "ei\u00b7ner", "ringt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Also ihm gelingt,", "tokens": ["Al\u00b7so", "ihm", "ge\u00b7lingt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wenn Manneskraft und Hab", "tokens": ["Wenn", "Man\u00b7nes\u00b7kraft", "und", "Hab"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Ihm Gott zum Willen gab.", "tokens": ["Ihm", "Gott", "zum", "Wil\u00b7len", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Den hochbestandnen F\u00f6hrenwald", "tokens": ["Den", "hoch\u00b7be\u00b7stand\u00b7nen", "F\u00f6h\u00b7ren\u00b7wald"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Pflanzt ich in jungen Tagen,", "tokens": ["Pflanzt", "ich", "in", "jun\u00b7gen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Er freut mich so! \u2013! \u2013! \u2013 Man wird ihn bald", "tokens": ["Er", "freut", "mich", "so", "!", "\u2013", "!", "\u2013", "!", "\u2013", "Man", "wird", "ihn", "bald"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$.", "$(", "$.", "$(", "$.", "$(", "PIS", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Brennholz niederschlagen.", "tokens": ["Als", "Brenn\u00b7holz", "nie\u00b7der\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Die Axt erklingt, da blinkt schon jedes Beil,", "tokens": ["Die", "Axt", "er\u00b7klingt", ",", "da", "blinkt", "schon", "je\u00b7des", "Beil", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ADV", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die Eiche f\u00e4llt, und jeder holzt sein Teil.", "tokens": ["Die", "Ei\u00b7che", "f\u00e4llt", ",", "und", "je\u00b7der", "holzt", "sein", "Teil", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PIS", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Ein alter Mann ist stets ein K\u00f6nig Lear! \u2013", "tokens": ["Ein", "al\u00b7ter", "Mann", "ist", "stets", "ein", "K\u00f6\u00b7nig", "Le\u00b7ar", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ART", "NN", "NE", "$.", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Was Hand in Hand mitwirkte, stritt,", "tokens": ["Was", "Hand", "in", "Hand", "mit\u00b7wirk\u00b7te", ",", "stritt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "NN", "APPR", "NN", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist l\u00e4ngst vorbeigegangen,", "tokens": ["Ist", "l\u00e4ngst", "vor\u00b7bei\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Was mit und an dir liebte, litt,", "tokens": ["Was", "mit", "und", "an", "dir", "lieb\u00b7te", ",", "litt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "APPR", "KON", "APPR", "PPER", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat sich woanders angehangen;", "tokens": ["Hat", "sich", "woan\u00b7ders", "an\u00b7ge\u00b7han\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Die Jugend ist um ihretwillen hier,", "tokens": ["Die", "Ju\u00b7gend", "ist", "um", "ih\u00b7ret\u00b7wil\u00b7len", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "PPOSAT", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Es w\u00e4re t\u00f6rig zu verlangen:", "tokens": ["Es", "w\u00e4\u00b7re", "t\u00f6\u00b7rig", "zu", "ver\u00b7lan\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Komm, \u00e4ltele du mit mir.", "tokens": ["Komm", ",", "\u00e4l\u00b7te\u00b7le", "du", "mit", "mir", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Gutes zu empfangen, zu erweisen,", "tokens": ["Gu\u00b7tes", "zu", "emp\u00b7fan\u00b7gen", ",", "zu", "er\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Alter! geh auf Reisen. \u2013", "tokens": ["Al\u00b7ter", "!", "geh", "auf", "Rei\u00b7sen", ".", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "VVFIN", "APPR", "NN", "$.", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Meine Freunde Sind aus einer Mittelzeit,", "tokens": ["Mei\u00b7ne", "Freun\u00b7de", "Sind", "aus", "ei\u00b7ner", "Mit\u00b7tel\u00b7zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Eine sch\u00f6ne Gemeinde,", "tokens": ["Ei\u00b7ne", "sch\u00f6\u00b7ne", "Ge\u00b7mein\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.5": {"text": "Weit und breit,", "tokens": ["Weit", "und", "breit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Auch entfernt,", "tokens": ["Auch", "ent\u00b7fernt", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Haben sie von mir gelernt,", "tokens": ["Ha\u00b7ben", "sie", "von", "mir", "ge\u00b7lernt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "In Gesinnung treu;", "tokens": ["In", "Ge\u00b7sin\u00b7nung", "treu", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.9": {"text": "Haben nicht an mir gelitten,", "tokens": ["Ha\u00b7ben", "nicht", "an", "mir", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "APPR", "PPER", "VVPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.10": {"text": "Ich hab ihnen nichts abzubitten;", "tokens": ["Ich", "hab", "ih\u00b7nen", "nichts", "ab\u00b7zu\u00b7bit\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Als Person komm ich neu.", "tokens": ["Als", "Per\u00b7son", "komm", "ich", "neu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "Wir haben kein Konto miteinander,", "tokens": ["Wir", "ha\u00b7ben", "kein", "Kon\u00b7to", "mi\u00b7tein\u00b7an\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Sind wie im Paradies selbander.", "tokens": ["Sind", "wie", "im", "Pa\u00b7ra\u00b7dies", "sel\u00b7ban\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "APPRART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Mit dieser Welt ist's keiner Wege richtig;", "tokens": ["Mit", "die\u00b7ser", "Welt", "ist's", "kei\u00b7ner", "We\u00b7ge", "rich\u00b7tig", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Vergebens bist du brav, vergebens t\u00fcchtig,", "tokens": ["Ver\u00b7ge\u00b7bens", "bist", "du", "brav", ",", "ver\u00b7ge\u00b7bens", "t\u00fcch\u00b7tig", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie will uns zahm, sie will sogar uns nichtig!", "tokens": ["Sie", "will", "uns", "zahm", ",", "sie", "will", "so\u00b7gar", "uns", "nich\u00b7tig", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "$,", "PPER", "VMFIN", "ADV", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Von heiligen M\u00e4nnern und von weisen", "tokens": ["Von", "hei\u00b7li\u00b7gen", "M\u00e4n\u00b7nern", "und", "von", "wei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "KON", "APPR", "ADJA"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Lie\u00df' ich mich recht gern unterweisen,", "tokens": ["Lie\u00df'", "ich", "mich", "recht", "gern", "un\u00b7ter\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PRF", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Aber es m\u00fc\u00dfte kurz geschehn,", "tokens": ["A\u00b7ber", "es", "m\u00fc\u00df\u00b7te", "kurz", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Langes Reden will mir nicht anstehn:", "tokens": ["Lan\u00b7ges", "Re\u00b7den", "will", "mir", "nicht", "an\u00b7stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Wornach soll man am Ende trachten?", "tokens": ["Wor\u00b7nach", "soll", "man", "am", "En\u00b7de", "trach\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PIS", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die Welt zu kennen und sie nicht verachten.", "tokens": ["Die", "Welt", "zu", "ken\u00b7nen", "und", "sie", "nicht", "ver\u00b7ach\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "KON", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.18": {"line.1": {"text": "Hast du es so lange wie ich getrieben,", "tokens": ["Hast", "du", "es", "so", "lan\u00b7ge", "wie", "ich", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "KOKOM", "PPER", "VVPP", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Versuche wie ich das Leben zu lieben.", "tokens": ["Ver\u00b7su\u00b7che", "wie", "ich", "das", "Le\u00b7ben", "zu", "lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.19": {"line.1": {"text": "Ruhig soll ich hier verpassen", "tokens": ["Ru\u00b7hig", "soll", "ich", "hier", "ver\u00b7pas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VMFIN", "PPER", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Meine M\u00fch und Flei\u00df;", "tokens": ["Mei\u00b7ne", "M\u00fch", "und", "Flei\u00df", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Alles soll ich gelten lassen,", "tokens": ["Al\u00b7les", "soll", "ich", "gel\u00b7ten", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Was ich besser wei\u00df.", "tokens": ["Was", "ich", "bes\u00b7ser", "wei\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.20": {"line.1": {"text": "H\u00f6r auf doch, mit Weisheit zu prahlen, zu prangen,", "tokens": ["H\u00f6r", "auf", "doch", ",", "mit", "Weis\u00b7heit", "zu", "prah\u00b7len", ",", "zu", "pran\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "APPR", "ADV", "$,", "APPR", "NN", "PTKZU", "VVINF", "$,", "PTKZU", "VVFIN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Bescheidenheit w\u00fcrde dir l\u00f6blicher stehn:", "tokens": ["Be\u00b7schei\u00b7den\u00b7heit", "w\u00fcr\u00b7de", "dir", "l\u00f6b\u00b7li\u00b7cher", "stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Kaum hast du die Fehler der Jugend begangen,", "tokens": ["Kaum", "hast", "du", "die", "Feh\u00b7ler", "der", "Ju\u00b7gend", "be\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.4": {"text": "So mu\u00dft du die Fehler des Alters begehn.", "tokens": ["So", "mu\u00dft", "du", "die", "Feh\u00b7ler", "des", "Al\u00b7ters", "be\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.21": {"line.1": {"text": "Liebe leidet nicht Gesellen,", "tokens": ["Lie\u00b7be", "lei\u00b7det", "nicht", "Ge\u00b7sel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKNEG", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aber Leiden sucht und hegt sie;", "tokens": ["A\u00b7ber", "Lei\u00b7den", "sucht", "und", "hegt", "sie", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "KON", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Lebenswoge, Well auf Wellen,", "tokens": ["Le\u00b7bens\u00b7wo\u00b7ge", ",", "Well", "auf", "Wel\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Einen wie den andern tr\u00e4gt sie.", "tokens": ["Ei\u00b7nen", "wie", "den", "an\u00b7dern", "tr\u00e4gt", "sie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "KOKOM", "ART", "ADJA", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Einsam oder auch selbander,", "tokens": ["Ein\u00b7sam", "o\u00b7der", "auch", "sel\u00b7ban\u00b7der", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADV", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unter Lieben, unter Leiden,", "tokens": ["Un\u00b7ter", "Lie\u00b7ben", ",", "un\u00b7ter", "Lei\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "$,", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Werden vor- und nacheinander", "tokens": ["Wer\u00b7den", "vor", "und", "na\u00b7ch\u00b7ein\u00b7an\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "TRUNC", "KON", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Einer mit dem andern scheiden.", "tokens": ["Ei\u00b7ner", "mit", "dem", "an\u00b7dern", "schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Wie es dir nicht im Leben ziemt,", "tokens": ["Wie", "es", "dir", "nicht", "im", "Le\u00b7ben", "ziemt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "PTKNEG", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft du nach Ruhm auch nicht am Ende jagen:", "tokens": ["Mu\u00dft", "du", "nach", "Ruhm", "auch", "nicht", "am", "En\u00b7de", "ja\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "NN", "ADV", "PTKNEG", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Denn bist du nur erst hundert Jahr ber\u00fchmt,", "tokens": ["Denn", "bist", "du", "nur", "erst", "hun\u00b7dert", "Jahr", "be\u00b7r\u00fchmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "CARD", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So wei\u00df kein Mensch mehr was von dir zu sagen.", "tokens": ["So", "wei\u00df", "kein", "Mensch", "mehr", "was", "von", "dir", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "ADV", "PWS", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.24": {"line.1": {"text": "Ins holde Leben wenn dich G\u00f6tter senden,", "tokens": ["Ins", "hol\u00b7de", "Le\u00b7ben", "wenn", "dich", "G\u00f6t\u00b7ter", "sen\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "KOUS", "PPER", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Genie\u00dfe wohlgemut und froh!", "tokens": ["Ge\u00b7nie\u00b7\u00dfe", "wohl\u00b7ge\u00b7mut", "und", "froh", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Scheint es bedenklich, dich hinaus zu wenden,", "tokens": ["Scheint", "es", "be\u00b7denk\u00b7lich", ",", "dich", "hin\u00b7aus", "zu", "wen\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "$,", "PRF", "APZR", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Nimm dir's nicht \u00fcbel: allen scheint es so.", "tokens": ["Nimm", "dir's", "nicht", "\u00fc\u00b7bel", ":", "al\u00b7len", "scheint", "es", "so", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PTKNEG", "ADJD", "$.", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.25": {"line.1": {"text": "Nichts vom Verg\u00e4nglichen,", "tokens": ["Nichts", "vom", "Ver\u00b7g\u00e4ng\u00b7li\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "APPRART", "NN", "$,"], "meter": "+--+--", "measure": "dactylic.di.plus"}, "line.2": {"text": "Wie's auch geschah!", "tokens": ["Wie's", "auch", "ge\u00b7schah", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Uns zu verewigen,", "tokens": ["Uns", "zu", "ve\u00b7re\u00b7wi\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Sind wir ja da.", "tokens": ["Sind", "wir", "ja", "da", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.26": {"line.1": {"text": "Hab ich gerechterweise verschuldet", "tokens": ["Hab", "ich", "ge\u00b7rech\u00b7ter\u00b7wei\u00b7se", "ver\u00b7schul\u00b7det"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PPER", "ADV", "VVFIN"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Diese Strafe in alten Tagen?", "tokens": ["Die\u00b7se", "Stra\u00b7fe", "in", "al\u00b7ten", "Ta\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Erst hab ich's an den V\u00e4tern erduldet,", "tokens": ["Erst", "hab", "ich's", "an", "den", "V\u00e4\u00b7tern", "er\u00b7dul\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Jetzt mu\u00df ich's an den Enkeln ertragen.", "tokens": ["Jetzt", "mu\u00df", "ich's", "an", "den", "En\u00b7keln", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.27": {"line.1": {"text": "\u00bbwer will der Menge widerstehn?\u00ab", "tokens": ["\u00bb", "wer", "will", "der", "Men\u00b7ge", "wi\u00b7der\u00b7stehn", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich widerstreb ihr nicht, ich la\u00df sie gehn:", "tokens": ["Ich", "wi\u00b7der\u00b7streb", "ihr", "nicht", ",", "ich", "la\u00df", "sie", "gehn", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,", "PPER", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Sie schwebt und webt und schwankt und schwirrt,", "tokens": ["Sie", "schwebt", "und", "webt", "und", "schwankt", "und", "schwirrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bis sie endlich wieder Einheit wird.", "tokens": ["Bis", "sie", "end\u00b7lich", "wie\u00b7der", "Ein\u00b7heit", "wird", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.28": {"line.1": {"text": "\u00bbwarum erkl\u00e4rst du's nicht und l\u00e4\u00dft sie gehn?\u00ab", "tokens": ["\u00bb", "wa\u00b7rum", "er\u00b7kl\u00e4rst", "du's", "nicht", "und", "l\u00e4\u00dft", "sie", "gehn", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PIS", "PTKNEG", "KON", "VVFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Geht's mich denn an, wenn sie mich nicht verstehn?", "tokens": ["Geht's", "mich", "denn", "an", ",", "wenn", "sie", "mich", "nicht", "ver\u00b7stehn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "PTKVZ", "$,", "KOUS", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.29": {"line.1": {"text": "\u00bbsag nur, wie tr\u00e4gst du so beh\u00e4glich", "tokens": ["\u00bb", "sag", "nur", ",", "wie", "tr\u00e4gst", "du", "so", "be\u00b7h\u00e4g\u00b7lich"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "ADV", "$,", "PWAV", "VVFIN", "PPER", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der tollen Jugend anma\u00dfliches Wesen?\u00ab", "tokens": ["Der", "tol\u00b7len", "Ju\u00b7gend", "an\u00b7ma\u00df\u00b7li\u00b7ches", "We\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$.", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "F\u00fcrwahr, sie w\u00e4ren unertr\u00e4glich,", "tokens": ["F\u00fcr\u00b7wahr", ",", "sie", "w\u00e4\u00b7ren", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "W\u00e4r ich nicht auch unertr\u00e4glich gewesen.", "tokens": ["W\u00e4r", "ich", "nicht", "auch", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "VAPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.30": {"line.1": {"text": "Ich h\u00f6r es gern, wenn auch die Jugend plappert;", "tokens": ["Ich", "h\u00f6r", "es", "gern", ",", "wenn", "auch", "die", "Ju\u00b7gend", "plap\u00b7pert", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+---", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Das Neue klingt, das Alte klappert.", "tokens": ["Das", "Neu\u00b7e", "klingt", ",", "das", "Al\u00b7te", "klap\u00b7pert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "\u00bbwarum willst du nicht mit Gewalt", "tokens": ["\u00bb", "wa\u00b7rum", "willst", "du", "nicht", "mit", "Ge\u00b7walt"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "PTKNEG", "APPR", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Unter die Toren, die Neulinge schlagen!\u00ab", "tokens": ["Un\u00b7ter", "die", "To\u00b7ren", ",", "die", "Neu\u00b7lin\u00b7ge", "schla\u00b7gen", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ART", "NN", "VVINF", "$.", "$("], "meter": "+--+--++-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "W\u00e4r ich nicht mit Ehren alt,", "tokens": ["W\u00e4r", "ich", "nicht", "mit", "Eh\u00b7ren", "alt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "APPR", "NN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie wollt ich die Jugend ertragen!", "tokens": ["Wie", "wollt", "ich", "die", "Ju\u00b7gend", "er\u00b7tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.32": {"line.1": {"text": "\u00bbwas wir denn sollen?", "tokens": ["\u00bb", "was", "wir", "denn", "sol\u00b7len", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "PPER", "ADV", "VMFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Sag uns, in diesen Tagen.\u00ab", "tokens": ["Sag", "uns", ",", "in", "die\u00b7sen", "Ta\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "$,", "APPR", "PDAT", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie machen, was sie wollen,", "tokens": ["Sie", "ma\u00b7chen", ",", "was", "sie", "wol\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PRELS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nur sollen sie mich nicht fragen.", "tokens": ["Nur", "sol\u00b7len", "sie", "mich", "nicht", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.33": {"line.1": {"text": "\u00bbwie doch, betriegerischer Wicht,", "tokens": ["\u00bb", "wie", "doch", ",", "be\u00b7trie\u00b7ge\u00b7ri\u00b7scher", "Wicht", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "KOKOM", "ADV", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vertr\u00e4gst du dich mit allen?\u00ab", "tokens": ["Ver\u00b7tr\u00e4gst", "du", "dich", "mit", "al\u00b7len", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "PIAT", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich leugne die Talente nicht,", "tokens": ["Ich", "leug\u00b7ne", "die", "Ta\u00b7len\u00b7te", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Wenn sie mir auch mi\u00dffallen.", "tokens": ["Wenn", "sie", "mir", "auch", "mi\u00df\u00b7fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.34": {"line.1": {"text": "Wenn einer auch sich \u00fcbersch\u00e4tzt,", "tokens": ["Wenn", "ei\u00b7ner", "auch", "sich", "\u00fc\u00b7bersc\u00b7h\u00e4tzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sterne kann er nicht erreichen,", "tokens": ["Die", "Ster\u00b7ne", "kann", "er", "nicht", "er\u00b7rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zu tief wird er herabgesetzt,", "tokens": ["Zu", "tief", "wird", "er", "her\u00b7ab\u00b7ge\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da ist denn alles bald im gleichen.", "tokens": ["Da", "ist", "denn", "al\u00b7les", "bald", "im", "glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIS", "ADV", "APPRART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Fahrt nur fort, nach eurer Weise", "tokens": ["Fahrt", "nur", "fort", ",", "nach", "eu\u00b7rer", "Wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Welt zu \u00fcberspinnen!", "tokens": ["Die", "Welt", "zu", "\u00fc\u00b7bers\u00b7pin\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich in meinem lebendigen Kreise", "tokens": ["Ich", "in", "mei\u00b7nem", "le\u00b7ben\u00b7di\u00b7gen", "Krei\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Wei\u00df das Leben zu gewinnen.", "tokens": ["Wei\u00df", "das", "Le\u00b7ben", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "Mir will das kranke Zeug nicht munden,", "tokens": ["Mir", "will", "das", "kran\u00b7ke", "Zeug", "nicht", "mun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Autoren sollten erst gesunden.", "tokens": ["Au\u00b7to\u00b7ren", "soll\u00b7ten", "erst", "ge\u00b7sun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Zeig ich die Fehler des Geschlechts,", "tokens": ["Zeig", "ich", "die", "Feh\u00b7ler", "des", "Ge\u00b7schlechts", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So hei\u00dft es: Tue selbst was Rechts.", "tokens": ["So", "hei\u00dft", "es", ":", "Tue", "selbst", "was", "Rechts", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "NN", "ADV", "PWS", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.38": {"line.1": {"text": "\u00bbdu Kr\u00e4ftiger, sei nicht so still,", "tokens": ["\u00bb", "du", "Kr\u00e4f\u00b7ti\u00b7ger", ",", "sei", "nicht", "so", "still", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "NN", "$,", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn auch sich andere scheuen.\u00ab", "tokens": ["Wenn", "auch", "sich", "an\u00b7de\u00b7re", "scheu\u00b7en", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "PRF", "PIS", "VVINF", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wer den Teufel erschrecken will,", "tokens": ["Wer", "den", "Teu\u00b7fel", "er\u00b7schre\u00b7cken", "will", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der mu\u00df laut schreien.", "tokens": ["Der", "mu\u00df", "laut", "schrei\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADJD", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.39": {"line.1": {"text": "\u00bbdu hast an sch\u00f6nen Tagen", "tokens": ["\u00bb", "du", "hast", "an", "sch\u00f6\u00b7nen", "Ta\u00b7gen"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Dich manchmal abgequ\u00e4lt!\u00ab", "tokens": ["Dich", "manch\u00b7mal", "ab\u00b7ge\u00b7qu\u00e4lt", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich habe mich nie verrechnet,", "tokens": ["Ich", "ha\u00b7be", "mich", "nie", "ver\u00b7rech\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Aber oft verz\u00e4hlt.", "tokens": ["A\u00b7ber", "oft", "ver\u00b7z\u00e4hlt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.40": {"line.1": {"text": "\u00dcber Berg und Tal,", "tokens": ["\u00dc\u00b7ber", "Berg", "und", "Tal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Irrtum \u00fcber Irrtum allzumal,", "tokens": ["Irr\u00b7tum", "\u00fc\u00b7ber", "Irr\u00b7tum", "all\u00b7zu\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Kommen wir wieder ins Freie;", "tokens": ["Kom\u00b7men", "wir", "wie\u00b7der", "ins", "Frei\u00b7e", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Doch da ist's gar zu weit und breit,", "tokens": ["Doch", "da", "ist's", "gar", "zu", "weit", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "PTKA", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nun suchen wir in kurzer Zeit", "tokens": ["Nun", "su\u00b7chen", "wir", "in", "kur\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Irrgang und Berg aufs neue.", "tokens": ["Irr\u00b7gang", "und", "Berg", "aufs", "neu\u00b7e", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPRART", "ADJA", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.41": {"line.1": {"text": "Gibt's ein Gespr\u00e4ch, wenn wir uns nicht betr\u00fcgen,", "tokens": ["Gibt's", "ein", "Ge\u00b7spr\u00e4ch", ",", "wenn", "wir", "uns", "nicht", "be\u00b7tr\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$,", "KOUS", "PPER", "PRF", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mehr oder weniger versteckt?", "tokens": ["Mehr", "o\u00b7der", "we\u00b7ni\u00b7ger", "ver\u00b7steckt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "KON", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So ein Ragout von Wahrheit und von L\u00fcgen,", "tokens": ["So", "ein", "Ra\u00b7gout", "von", "Wahr\u00b7heit", "und", "von", "L\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das ist die K\u00f6cherei, die mir am besten schmeckt.", "tokens": ["Das", "ist", "die", "K\u00f6\u00b7che\u00b7rei", ",", "die", "mir", "am", "bes\u00b7ten", "schmeckt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "PRELS", "PPER", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.42": {"line.1": {"text": "Kennst du das Spiel, wo man im lust'gen Kreis", "tokens": ["Kennst", "du", "das", "Spiel", ",", "wo", "man", "im", "lust'\u00b7gen", "Kreis"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "PWAV", "PIS", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Das Pfeifchen sucht und niemals findet,", "tokens": ["Das", "Pfei\u00b7fchen", "sucht", "und", "nie\u00b7mals", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KON", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Weil man's dem Sucher, ohn da\u00df er's wei\u00df,", "tokens": ["Weil", "man's", "dem", "Su\u00b7cher", ",", "ohn", "da\u00df", "er's", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "$,", "KOUI", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "In seines Rockes hintre Falten bindet,", "tokens": ["In", "sei\u00b7nes", "Ro\u00b7ckes", "hin\u00b7tre", "Fal\u00b7ten", "bin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Das hei\u00dft: an seinen Stei\u00df?", "tokens": ["Das", "hei\u00dft", ":", "an", "sei\u00b7nen", "Stei\u00df", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$.", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.43": {"line.1": {"text": "Mit Narren leben wird dir gar nicht schwer,", "tokens": ["Mit", "Nar\u00b7ren", "le\u00b7ben", "wird", "dir", "gar", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Versammle nur ein Tollhaus um dich her.", "tokens": ["Ver\u00b7samm\u00b7le", "nur", "ein", "Toll\u00b7haus", "um", "dich", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bedenke dann, das macht dich gleich gelind,", "tokens": ["Be\u00b7den\u00b7ke", "dann", ",", "das", "macht", "dich", "gleich", "ge\u00b7lind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "PDS", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df Narrenw\u00e4rter selbst auch Narren sind.", "tokens": ["Da\u00df", "Nar\u00b7ren\u00b7w\u00e4r\u00b7ter", "selbst", "auch", "Nar\u00b7ren", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "ADV", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.44": {"line.1": {"text": "Wo recht viel Widerspr\u00fcche schwirren,", "tokens": ["Wo", "recht", "viel", "Wi\u00b7der\u00b7spr\u00fc\u00b7che", "schwir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mag ich am liebsten wandern;", "tokens": ["Mag", "ich", "am", "liebs\u00b7ten", "wan\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Niemand g\u00f6nnt dem andern \u2013", "tokens": ["Nie\u00b7mand", "g\u00f6nnt", "dem", "an\u00b7dern", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Wie lustig! \u2013 das Recht zu irren.", "tokens": ["Wie", "lus\u00b7tig", "!", "\u2013", "das", "Recht", "zu", "ir\u00b7ren", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "$.", "$(", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.45": {"line.1": {"text": "St\u00e4mme wollen gegen St\u00e4mme pochen,", "tokens": ["St\u00e4m\u00b7me", "wol\u00b7len", "ge\u00b7gen", "St\u00e4m\u00b7me", "po\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Kann doch einer, was der andere kann!", "tokens": ["Kann", "doch", "ei\u00b7ner", ",", "was", "der", "an\u00b7de\u00b7re", "kann", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PIS", "$,", "PRELS", "ART", "PIS", "VMFIN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Steckt doch Mark in jedem Knochen,", "tokens": ["Steckt", "doch", "Mark", "in", "je\u00b7dem", "Kno\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und in jedem Hemde steckt ein Mann.", "tokens": ["Und", "in", "je\u00b7dem", "Hem\u00b7de", "steckt", "ein", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.46": {"line.1": {"text": "Hat Welscher-Hahn an seinem Kropf,", "tokens": ["Hat", "Wel\u00b7scher\u00b7Hahn", "an", "sei\u00b7nem", "Kropf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Storch an dem Langhals Freude;", "tokens": ["Storch", "an", "dem", "Lang\u00b7hals", "Freu\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Der Kessel schilt den Ofentopf,", "tokens": ["Der", "Kes\u00b7sel", "schilt", "den", "O\u00b7fen\u00b7topf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Schwarz sind sie alle beide.", "tokens": ["Schwarz", "sind", "sie", "al\u00b7le", "bei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "PIAT", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.47": {"line.1": {"text": "\u00bbwie gerne s\u00e4h ich jeden stolzieren,", "tokens": ["\u00bb", "wie", "ger\u00b7ne", "s\u00e4h", "ich", "je\u00b7den", "stol\u00b7zie\u00b7ren", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOKOM", "ADV", "VVFIN", "PPER", "PIAT", "ADJA", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "K\u00f6nnt er das Pfauenrad vollf\u00fchren.", "tokens": ["K\u00f6nnt", "er", "das", "Pfau\u00b7en\u00b7rad", "voll\u00b7f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Warum nur die h\u00fcbschen Leute", "tokens": ["Wa\u00b7rum", "nur", "die", "h\u00fcb\u00b7schen", "Leu\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Mir nicht gefallen sollen?\u00ab", "tokens": ["Mir", "nicht", "ge\u00b7fal\u00b7len", "sol\u00b7len", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "PTKNEG", "VVINF", "VMFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Manchen h\u00e4lt man f\u00fcr fett,", "tokens": ["Man\u00b7chen", "h\u00e4lt", "man", "f\u00fcr", "fett", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "APPR", "ADJD", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Er ist nur geschwollen.", "tokens": ["Er", "ist", "nur", "ge\u00b7schwol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.49": {"line.1": {"text": "Da reiten sie hin! wer hemmt den Lauf!", "tokens": ["Da", "rei\u00b7ten", "sie", "hin", "!", "wer", "hemmt", "den", "Lauf", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$.", "PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wer reitet denn? Stolz und Unwissenheit.", "tokens": ["Wer", "rei\u00b7tet", "denn", "?", "Stolz", "und", "Un\u00b7wis\u00b7sen\u00b7heit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "$.", "NN", "KON", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "La\u00df sie reiten! da ist gute Zeit,", "tokens": ["La\u00df", "sie", "rei\u00b7ten", "!", "da", "ist", "gu\u00b7te", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "$.", "ADV", "VAFIN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Schimpf und Schade sitzen hinten auf.", "tokens": ["Schimpf", "und", "Scha\u00b7de", "sit\u00b7zen", "hin\u00b7ten", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.50": {"line.1": {"text": "\u00bbwie ist dir's doch so balde", "tokens": ["\u00bb", "wie", "ist", "dir's", "doch", "so", "bal\u00b7de"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KOKOM", "VAFIN", "PIS", "ADV", "ADV", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Zur Ehr und Schmach gediehn?\u00ab", "tokens": ["Zur", "Ehr", "und", "Schmach", "ge\u00b7diehn", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Blieb' der Wolf im Walde,", "tokens": ["Blieb'", "der", "Wolf", "im", "Wal\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "APPRART", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "So w\u00fcrd er nicht beschrien.", "tokens": ["So", "w\u00fcrd", "er", "nicht", "be\u00b7schri\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.51": {"line.1": {"text": "Ich rufe dich, verrufnes Wort,", "tokens": ["Ich", "ru\u00b7fe", "dich", ",", "ver\u00b7ruf\u00b7nes", "Wort", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zur Ordnung auf des Tags:", "tokens": ["Zur", "Ord\u00b7nung", "auf", "des", "Tags", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Denn Wichte, Schelme solchen Schlags,", "tokens": ["Denn", "Wich\u00b7te", ",", "Schel\u00b7me", "sol\u00b7chen", "Schlags", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die wirken immer fort.", "tokens": ["Die", "wir\u00b7ken", "im\u00b7mer", "fort", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.52": {"line.1": {"text": "\u00bbwarum willst du dich von uns allen", "tokens": ["\u00bb", "wa\u00b7rum", "willst", "du", "dich", "von", "uns", "al\u00b7len"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "PRF", "APPR", "PPER", "PIAT"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und unsrer Meinung entfernen?\u00ab", "tokens": ["Und", "uns\u00b7rer", "Mei\u00b7nung", "ent\u00b7fer\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "++-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ich schreibe nicht, euch zu gefallen,", "tokens": ["Ich", "schrei\u00b7be", "nicht", ",", "euch", "zu", "ge\u00b7fal\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ihr sollt was lernen!", "tokens": ["Ihr", "sollt", "was", "ler\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.53": {"line.1": {"text": "\u00bbist denn das klug und wohlgetan?", "tokens": ["\u00bb", "ist", "denn", "das", "klug", "und", "wohl\u00b7ge\u00b7tan", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "ADV", "ART", "ADJD", "KON", "PWAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was willst du Freund' und Feinde kr\u00e4nken!\u00ab", "tokens": ["Was", "willst", "du", "Freund'", "und", "Fein\u00b7de", "kr\u00e4n\u00b7ken", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Erwachsne gehn mich nichts mehr an,", "tokens": ["Er\u00b7wachs\u00b7ne", "gehn", "mich", "nichts", "mehr", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PIS", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich mu\u00df nun an die Enkel denken.", "tokens": ["Ich", "mu\u00df", "nun", "an", "die", "En\u00b7kel", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Und sollst auch du und du und du", "tokens": ["Und", "sollst", "auch", "du", "und", "du", "und", "du"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "PPER", "KON", "PPER", "KON", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht gleich mit mir zerfallen;", "tokens": ["Nicht", "gleich", "mit", "mir", "zer\u00b7fal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was ich dem Enkel zuliebe tu,", "tokens": ["Was", "ich", "dem", "En\u00b7kel", "zu\u00b7lie\u00b7be", "tu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "VVFIN", "NE", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Tu ich euch allen.", "tokens": ["Tu", "ich", "euch", "al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PPER", "PIAT", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.55": {"line.1": {"text": "Verzeiht einmal dem raschen Wort,", "tokens": ["Ver\u00b7zeiht", "ein\u00b7mal", "dem", "ra\u00b7schen", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und so verzeiht dem Plaudern;", "tokens": ["Und", "so", "ver\u00b7zeiht", "dem", "Plau\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Denn jetzo w\u00e4r's nicht ganz am Ort,", "tokens": ["Denn", "jet\u00b7zo", "w\u00e4r's", "nicht", "ganz", "am", "Ort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PTKNEG", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie bis hieher zu zaudern.", "tokens": ["Wie", "bis", "hie\u00b7her", "zu", "zau\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PAV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.56": {"line.1": {"text": "Wer in der Weltgeschichte lebt,", "tokens": ["Wer", "in", "der", "Welt\u00b7ge\u00b7schich\u00b7te", "lebt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Augenblick sollt er sich richten?", "tokens": ["Dem", "Au\u00b7gen\u00b7blick", "sollt", "er", "sich", "rich\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Wer in die Zeiten schaut und strebt,", "tokens": ["Wer", "in", "die", "Zei\u00b7ten", "schaut", "und", "strebt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nur der ist wert, zu sprechen und zu dichten.", "tokens": ["Nur", "der", "ist", "wert", ",", "zu", "spre\u00b7chen", "und", "zu", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VAFIN", "ADJD", "$,", "PTKZU", "VVINF", "KON", "APPR", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.57": {"line.1": {"text": "\u00bbsag mir, worauf die B\u00f6sen sinnen?\u00ab", "tokens": ["\u00bb", "sag", "mir", ",", "wo\u00b7rauf", "die", "B\u00f6\u00b7sen", "sin\u00b7nen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "$,", "PWAV", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Andern den Tag zu verderben,", "tokens": ["An\u00b7dern", "den", "Tag", "zu", "ver\u00b7der\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "ART", "NN", "PTKZU", "VVFIN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Sich den Tag zu gewinnen:", "tokens": ["Sich", "den", "Tag", "zu", "ge\u00b7win\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Das, meinen sie, hei\u00dfe erwerben.", "tokens": ["Das", ",", "mei\u00b7nen", "sie", ",", "hei\u00b7\u00dfe", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "$,", "VVFIN", "PPER", "$,", "VVFIN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.58": {"line.1": {"text": "\u00bbwas ist denn deine Absicht gewesen,", "tokens": ["\u00bb", "was", "ist", "denn", "dei\u00b7ne", "Ab\u00b7sicht", "ge\u00b7we\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ADV", "PPOSAT", "NN", "VAPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Jetzt neue Feuer anzubrennen?\u00ab", "tokens": ["Jetzt", "neu\u00b7e", "Feu\u00b7er", "an\u00b7zu\u00b7bren\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJA", "NN", "VVIZU", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Diejenigen sollen's lesen,", "tokens": ["Die\u00b7je\u00b7ni\u00b7gen", "sol\u00b7len's", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die mich nicht mehr h\u00f6ren k\u00f6nnen.", "tokens": ["Die", "mich", "nicht", "mehr", "h\u00f6\u00b7ren", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKNEG", "ADV", "VVINF", "VMINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.59": {"line.1": {"text": "Einen langen Tag \u00fcber lebt ich sch\u00f6n,", "tokens": ["Ei\u00b7nen", "lan\u00b7gen", "Tag", "\u00fc\u00b7ber", "lebt", "ich", "sch\u00f6n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Eine kurze Nacht.", "tokens": ["Ei\u00b7ne", "kur\u00b7ze", "Nacht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Die Sonne war eben im Aufgehn,", "tokens": ["Die", "Son\u00b7ne", "war", "e\u00b7ben", "im", "Auf\u00b7gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Als ich zu neuem", "tokens": ["Als", "ich", "zu", "neu\u00b7em"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ADJA"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Tag erwacht.", "tokens": ["Tag", "er\u00b7wacht", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.60": {"line.1": {"text": "\u00bbdeine Z\u00f6glinge m\u00f6chten dich fragen:", "tokens": ["\u00bb", "dei\u00b7ne", "Z\u00f6g\u00b7lin\u00b7ge", "m\u00f6ch\u00b7ten", "dich", "fra\u00b7gen", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "VMFIN", "PRF", "VVINF", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Lange lebten wir gern auf Erden,", "tokens": ["Lan\u00b7ge", "leb\u00b7ten", "wir", "gern", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Was willst du uns f\u00fcr Lehre sagen?\u00ab", "tokens": ["Was", "willst", "du", "uns", "f\u00fcr", "Leh\u00b7re", "sa\u00b7gen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PRF", "APPR", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Keine Kunst ist's, alt zu werden,", "tokens": ["Kei\u00b7ne", "Kunst", "ist's", ",", "alt", "zu", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "$,", "ADJD", "PTKZU", "VAINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Es ist Kunst, es zu ertragen.", "tokens": ["Es", "ist", "Kunst", ",", "es", "zu", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Nachdem einer ringt,", "tokens": ["Nach\u00b7dem", "ei\u00b7ner", "ringt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Also ihm gelingt,", "tokens": ["Al\u00b7so", "ihm", "ge\u00b7lingt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wenn Manneskraft und Hab", "tokens": ["Wenn", "Man\u00b7nes\u00b7kraft", "und", "Hab"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Ihm Gott zum Willen gab.", "tokens": ["Ihm", "Gott", "zum", "Wil\u00b7len", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.62": {"line.1": {"text": "Den hochbestandnen F\u00f6hrenwald", "tokens": ["Den", "hoch\u00b7be\u00b7stand\u00b7nen", "F\u00f6h\u00b7ren\u00b7wald"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Pflanzt ich in jungen Tagen,", "tokens": ["Pflanzt", "ich", "in", "jun\u00b7gen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Er freut mich so! \u2013! \u2013! \u2013 Man wird ihn bald", "tokens": ["Er", "freut", "mich", "so", "!", "\u2013", "!", "\u2013", "!", "\u2013", "Man", "wird", "ihn", "bald"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$.", "$(", "$.", "$(", "$.", "$(", "PIS", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Brennholz niederschlagen.", "tokens": ["Als", "Brenn\u00b7holz", "nie\u00b7der\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.63": {"line.1": {"text": "Die Axt erklingt, da blinkt schon jedes Beil,", "tokens": ["Die", "Axt", "er\u00b7klingt", ",", "da", "blinkt", "schon", "je\u00b7des", "Beil", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ADV", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die Eiche f\u00e4llt, und jeder holzt sein Teil.", "tokens": ["Die", "Ei\u00b7che", "f\u00e4llt", ",", "und", "je\u00b7der", "holzt", "sein", "Teil", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PIS", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.64": {"line.1": {"text": "Ein alter Mann ist stets ein K\u00f6nig Lear! \u2013", "tokens": ["Ein", "al\u00b7ter", "Mann", "ist", "stets", "ein", "K\u00f6\u00b7nig", "Le\u00b7ar", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ART", "NN", "NE", "$.", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Was Hand in Hand mitwirkte, stritt,", "tokens": ["Was", "Hand", "in", "Hand", "mit\u00b7wirk\u00b7te", ",", "stritt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "NN", "APPR", "NN", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist l\u00e4ngst vorbeigegangen,", "tokens": ["Ist", "l\u00e4ngst", "vor\u00b7bei\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Was mit und an dir liebte, litt,", "tokens": ["Was", "mit", "und", "an", "dir", "lieb\u00b7te", ",", "litt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "APPR", "KON", "APPR", "PPER", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Hat sich woanders angehangen;", "tokens": ["Hat", "sich", "woan\u00b7ders", "an\u00b7ge\u00b7han\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Die Jugend ist um ihretwillen hier,", "tokens": ["Die", "Ju\u00b7gend", "ist", "um", "ih\u00b7ret\u00b7wil\u00b7len", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "PPOSAT", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Es w\u00e4re t\u00f6rig zu verlangen:", "tokens": ["Es", "w\u00e4\u00b7re", "t\u00f6\u00b7rig", "zu", "ver\u00b7lan\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Komm, \u00e4ltele du mit mir.", "tokens": ["Komm", ",", "\u00e4l\u00b7te\u00b7le", "du", "mit", "mir", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.65": {"line.1": {"text": "Gutes zu empfangen, zu erweisen,", "tokens": ["Gu\u00b7tes", "zu", "emp\u00b7fan\u00b7gen", ",", "zu", "er\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Alter! geh auf Reisen. \u2013", "tokens": ["Al\u00b7ter", "!", "geh", "auf", "Rei\u00b7sen", ".", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "VVFIN", "APPR", "NN", "$.", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Meine Freunde Sind aus einer Mittelzeit,", "tokens": ["Mei\u00b7ne", "Freun\u00b7de", "Sind", "aus", "ei\u00b7ner", "Mit\u00b7tel\u00b7zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Eine sch\u00f6ne Gemeinde,", "tokens": ["Ei\u00b7ne", "sch\u00f6\u00b7ne", "Ge\u00b7mein\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.5": {"text": "Weit und breit,", "tokens": ["Weit", "und", "breit", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.6": {"text": "Auch entfernt,", "tokens": ["Auch", "ent\u00b7fernt", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Haben sie von mir gelernt,", "tokens": ["Ha\u00b7ben", "sie", "von", "mir", "ge\u00b7lernt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "In Gesinnung treu;", "tokens": ["In", "Ge\u00b7sin\u00b7nung", "treu", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.9": {"text": "Haben nicht an mir gelitten,", "tokens": ["Ha\u00b7ben", "nicht", "an", "mir", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "APPR", "PPER", "VVPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.10": {"text": "Ich hab ihnen nichts abzubitten;", "tokens": ["Ich", "hab", "ih\u00b7nen", "nichts", "ab\u00b7zu\u00b7bit\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Als Person komm ich neu.", "tokens": ["Als", "Per\u00b7son", "komm", "ich", "neu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "Wir haben kein Konto miteinander,", "tokens": ["Wir", "ha\u00b7ben", "kein", "Kon\u00b7to", "mi\u00b7tein\u00b7an\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "ADV", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Sind wie im Paradies selbander.", "tokens": ["Sind", "wie", "im", "Pa\u00b7ra\u00b7dies", "sel\u00b7ban\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "APPRART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Mit dieser Welt ist's keiner Wege richtig;", "tokens": ["Mit", "die\u00b7ser", "Welt", "ist's", "kei\u00b7ner", "We\u00b7ge", "rich\u00b7tig", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Vergebens bist du brav, vergebens t\u00fcchtig,", "tokens": ["Ver\u00b7ge\u00b7bens", "bist", "du", "brav", ",", "ver\u00b7ge\u00b7bens", "t\u00fcch\u00b7tig", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sie will uns zahm, sie will sogar uns nichtig!", "tokens": ["Sie", "will", "uns", "zahm", ",", "sie", "will", "so\u00b7gar", "uns", "nich\u00b7tig", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "$,", "PPER", "VMFIN", "ADV", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.67": {"line.1": {"text": "Von heiligen M\u00e4nnern und von weisen", "tokens": ["Von", "hei\u00b7li\u00b7gen", "M\u00e4n\u00b7nern", "und", "von", "wei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "KON", "APPR", "ADJA"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Lie\u00df' ich mich recht gern unterweisen,", "tokens": ["Lie\u00df'", "ich", "mich", "recht", "gern", "un\u00b7ter\u00b7wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PRF", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Aber es m\u00fc\u00dfte kurz geschehn,", "tokens": ["A\u00b7ber", "es", "m\u00fc\u00df\u00b7te", "kurz", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Langes Reden will mir nicht anstehn:", "tokens": ["Lan\u00b7ges", "Re\u00b7den", "will", "mir", "nicht", "an\u00b7stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Wornach soll man am Ende trachten?", "tokens": ["Wor\u00b7nach", "soll", "man", "am", "En\u00b7de", "trach\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PIS", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die Welt zu kennen und sie nicht verachten.", "tokens": ["Die", "Welt", "zu", "ken\u00b7nen", "und", "sie", "nicht", "ver\u00b7ach\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "KON", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.68": {"line.1": {"text": "Hast du es so lange wie ich getrieben,", "tokens": ["Hast", "du", "es", "so", "lan\u00b7ge", "wie", "ich", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "KOKOM", "PPER", "VVPP", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Versuche wie ich das Leben zu lieben.", "tokens": ["Ver\u00b7su\u00b7che", "wie", "ich", "das", "Le\u00b7ben", "zu", "lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.69": {"line.1": {"text": "Ruhig soll ich hier verpassen", "tokens": ["Ru\u00b7hig", "soll", "ich", "hier", "ver\u00b7pas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VMFIN", "PPER", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Meine M\u00fch und Flei\u00df;", "tokens": ["Mei\u00b7ne", "M\u00fch", "und", "Flei\u00df", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Alles soll ich gelten lassen,", "tokens": ["Al\u00b7les", "soll", "ich", "gel\u00b7ten", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Was ich besser wei\u00df.", "tokens": ["Was", "ich", "bes\u00b7ser", "wei\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.70": {"line.1": {"text": "H\u00f6r auf doch, mit Weisheit zu prahlen, zu prangen,", "tokens": ["H\u00f6r", "auf", "doch", ",", "mit", "Weis\u00b7heit", "zu", "prah\u00b7len", ",", "zu", "pran\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "APPR", "ADV", "$,", "APPR", "NN", "PTKZU", "VVINF", "$,", "PTKZU", "VVFIN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Bescheidenheit w\u00fcrde dir l\u00f6blicher stehn:", "tokens": ["Be\u00b7schei\u00b7den\u00b7heit", "w\u00fcr\u00b7de", "dir", "l\u00f6b\u00b7li\u00b7cher", "stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Kaum hast du die Fehler der Jugend begangen,", "tokens": ["Kaum", "hast", "du", "die", "Feh\u00b7ler", "der", "Ju\u00b7gend", "be\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.4": {"text": "So mu\u00dft du die Fehler des Alters begehn.", "tokens": ["So", "mu\u00dft", "du", "die", "Feh\u00b7ler", "des", "Al\u00b7ters", "be\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.71": {"line.1": {"text": "Liebe leidet nicht Gesellen,", "tokens": ["Lie\u00b7be", "lei\u00b7det", "nicht", "Ge\u00b7sel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKNEG", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aber Leiden sucht und hegt sie;", "tokens": ["A\u00b7ber", "Lei\u00b7den", "sucht", "und", "hegt", "sie", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "KON", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Lebenswoge, Well auf Wellen,", "tokens": ["Le\u00b7bens\u00b7wo\u00b7ge", ",", "Well", "auf", "Wel\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Einen wie den andern tr\u00e4gt sie.", "tokens": ["Ei\u00b7nen", "wie", "den", "an\u00b7dern", "tr\u00e4gt", "sie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "KOKOM", "ART", "ADJA", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.72": {"line.1": {"text": "Einsam oder auch selbander,", "tokens": ["Ein\u00b7sam", "o\u00b7der", "auch", "sel\u00b7ban\u00b7der", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADV", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unter Lieben, unter Leiden,", "tokens": ["Un\u00b7ter", "Lie\u00b7ben", ",", "un\u00b7ter", "Lei\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "$,", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Werden vor- und nacheinander", "tokens": ["Wer\u00b7den", "vor", "und", "na\u00b7ch\u00b7ein\u00b7an\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "TRUNC", "KON", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Einer mit dem andern scheiden.", "tokens": ["Ei\u00b7ner", "mit", "dem", "an\u00b7dern", "schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.73": {"line.1": {"text": "Wie es dir nicht im Leben ziemt,", "tokens": ["Wie", "es", "dir", "nicht", "im", "Le\u00b7ben", "ziemt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "PTKNEG", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft du nach Ruhm auch nicht am Ende jagen:", "tokens": ["Mu\u00dft", "du", "nach", "Ruhm", "auch", "nicht", "am", "En\u00b7de", "ja\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "NN", "ADV", "PTKNEG", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Denn bist du nur erst hundert Jahr ber\u00fchmt,", "tokens": ["Denn", "bist", "du", "nur", "erst", "hun\u00b7dert", "Jahr", "be\u00b7r\u00fchmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "CARD", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So wei\u00df kein Mensch mehr was von dir zu sagen.", "tokens": ["So", "wei\u00df", "kein", "Mensch", "mehr", "was", "von", "dir", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "ADV", "PWS", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.74": {"line.1": {"text": "Ins holde Leben wenn dich G\u00f6tter senden,", "tokens": ["Ins", "hol\u00b7de", "Le\u00b7ben", "wenn", "dich", "G\u00f6t\u00b7ter", "sen\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "KOUS", "PPER", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Genie\u00dfe wohlgemut und froh!", "tokens": ["Ge\u00b7nie\u00b7\u00dfe", "wohl\u00b7ge\u00b7mut", "und", "froh", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Scheint es bedenklich, dich hinaus zu wenden,", "tokens": ["Scheint", "es", "be\u00b7denk\u00b7lich", ",", "dich", "hin\u00b7aus", "zu", "wen\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "$,", "PRF", "APZR", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Nimm dir's nicht \u00fcbel: allen scheint es so.", "tokens": ["Nimm", "dir's", "nicht", "\u00fc\u00b7bel", ":", "al\u00b7len", "scheint", "es", "so", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PTKNEG", "ADJD", "$.", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.75": {"line.1": {"text": "Nichts vom Verg\u00e4nglichen,", "tokens": ["Nichts", "vom", "Ver\u00b7g\u00e4ng\u00b7li\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "APPRART", "NN", "$,"], "meter": "+--+--", "measure": "dactylic.di.plus"}, "line.2": {"text": "Wie's auch geschah!", "tokens": ["Wie's", "auch", "ge\u00b7schah", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Uns zu verewigen,", "tokens": ["Uns", "zu", "ve\u00b7re\u00b7wi\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Sind wir ja da.", "tokens": ["Sind", "wir", "ja", "da", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.76": {"line.1": {"text": "Hab ich gerechterweise verschuldet", "tokens": ["Hab", "ich", "ge\u00b7rech\u00b7ter\u00b7wei\u00b7se", "ver\u00b7schul\u00b7det"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PPER", "ADV", "VVFIN"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Diese Strafe in alten Tagen?", "tokens": ["Die\u00b7se", "Stra\u00b7fe", "in", "al\u00b7ten", "Ta\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Erst hab ich's an den V\u00e4tern erduldet,", "tokens": ["Erst", "hab", "ich's", "an", "den", "V\u00e4\u00b7tern", "er\u00b7dul\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Jetzt mu\u00df ich's an den Enkeln ertragen.", "tokens": ["Jetzt", "mu\u00df", "ich's", "an", "den", "En\u00b7keln", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.77": {"line.1": {"text": "\u00bbwer will der Menge widerstehn?\u00ab", "tokens": ["\u00bb", "wer", "will", "der", "Men\u00b7ge", "wi\u00b7der\u00b7stehn", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich widerstreb ihr nicht, ich la\u00df sie gehn:", "tokens": ["Ich", "wi\u00b7der\u00b7streb", "ihr", "nicht", ",", "ich", "la\u00df", "sie", "gehn", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,", "PPER", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Sie schwebt und webt und schwankt und schwirrt,", "tokens": ["Sie", "schwebt", "und", "webt", "und", "schwankt", "und", "schwirrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bis sie endlich wieder Einheit wird.", "tokens": ["Bis", "sie", "end\u00b7lich", "wie\u00b7der", "Ein\u00b7heit", "wird", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.78": {"line.1": {"text": "\u00bbwarum erkl\u00e4rst du's nicht und l\u00e4\u00dft sie gehn?\u00ab", "tokens": ["\u00bb", "wa\u00b7rum", "er\u00b7kl\u00e4rst", "du's", "nicht", "und", "l\u00e4\u00dft", "sie", "gehn", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PIS", "PTKNEG", "KON", "VVFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Geht's mich denn an, wenn sie mich nicht verstehn?", "tokens": ["Geht's", "mich", "denn", "an", ",", "wenn", "sie", "mich", "nicht", "ver\u00b7stehn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "PTKVZ", "$,", "KOUS", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.79": {"line.1": {"text": "\u00bbsag nur, wie tr\u00e4gst du so beh\u00e4glich", "tokens": ["\u00bb", "sag", "nur", ",", "wie", "tr\u00e4gst", "du", "so", "be\u00b7h\u00e4g\u00b7lich"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "ADV", "$,", "PWAV", "VVFIN", "PPER", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der tollen Jugend anma\u00dfliches Wesen?\u00ab", "tokens": ["Der", "tol\u00b7len", "Ju\u00b7gend", "an\u00b7ma\u00df\u00b7li\u00b7ches", "We\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$.", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "F\u00fcrwahr, sie w\u00e4ren unertr\u00e4glich,", "tokens": ["F\u00fcr\u00b7wahr", ",", "sie", "w\u00e4\u00b7ren", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "W\u00e4r ich nicht auch unertr\u00e4glich gewesen.", "tokens": ["W\u00e4r", "ich", "nicht", "auch", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "VAPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.80": {"line.1": {"text": "Ich h\u00f6r es gern, wenn auch die Jugend plappert;", "tokens": ["Ich", "h\u00f6r", "es", "gern", ",", "wenn", "auch", "die", "Ju\u00b7gend", "plap\u00b7pert", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+---", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Das Neue klingt, das Alte klappert.", "tokens": ["Das", "Neu\u00b7e", "klingt", ",", "das", "Al\u00b7te", "klap\u00b7pert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "\u00bbwarum willst du nicht mit Gewalt", "tokens": ["\u00bb", "wa\u00b7rum", "willst", "du", "nicht", "mit", "Ge\u00b7walt"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "PTKNEG", "APPR", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Unter die Toren, die Neulinge schlagen!\u00ab", "tokens": ["Un\u00b7ter", "die", "To\u00b7ren", ",", "die", "Neu\u00b7lin\u00b7ge", "schla\u00b7gen", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ART", "NN", "VVINF", "$.", "$("], "meter": "+--+--++-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "W\u00e4r ich nicht mit Ehren alt,", "tokens": ["W\u00e4r", "ich", "nicht", "mit", "Eh\u00b7ren", "alt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "APPR", "NN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie wollt ich die Jugend ertragen!", "tokens": ["Wie", "wollt", "ich", "die", "Ju\u00b7gend", "er\u00b7tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.82": {"line.1": {"text": "\u00bbwas wir denn sollen?", "tokens": ["\u00bb", "was", "wir", "denn", "sol\u00b7len", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "PPER", "ADV", "VMFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Sag uns, in diesen Tagen.\u00ab", "tokens": ["Sag", "uns", ",", "in", "die\u00b7sen", "Ta\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "$,", "APPR", "PDAT", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie machen, was sie wollen,", "tokens": ["Sie", "ma\u00b7chen", ",", "was", "sie", "wol\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PRELS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Nur sollen sie mich nicht fragen.", "tokens": ["Nur", "sol\u00b7len", "sie", "mich", "nicht", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "PTKNEG", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.83": {"line.1": {"text": "\u00bbwie doch, betriegerischer Wicht,", "tokens": ["\u00bb", "wie", "doch", ",", "be\u00b7trie\u00b7ge\u00b7ri\u00b7scher", "Wicht", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "KOKOM", "ADV", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vertr\u00e4gst du dich mit allen?\u00ab", "tokens": ["Ver\u00b7tr\u00e4gst", "du", "dich", "mit", "al\u00b7len", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "PIAT", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich leugne die Talente nicht,", "tokens": ["Ich", "leug\u00b7ne", "die", "Ta\u00b7len\u00b7te", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Wenn sie mir auch mi\u00dffallen.", "tokens": ["Wenn", "sie", "mir", "auch", "mi\u00df\u00b7fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.84": {"line.1": {"text": "Wenn einer auch sich \u00fcbersch\u00e4tzt,", "tokens": ["Wenn", "ei\u00b7ner", "auch", "sich", "\u00fc\u00b7bersc\u00b7h\u00e4tzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Sterne kann er nicht erreichen,", "tokens": ["Die", "Ster\u00b7ne", "kann", "er", "nicht", "er\u00b7rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zu tief wird er herabgesetzt,", "tokens": ["Zu", "tief", "wird", "er", "her\u00b7ab\u00b7ge\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da ist denn alles bald im gleichen.", "tokens": ["Da", "ist", "denn", "al\u00b7les", "bald", "im", "glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIS", "ADV", "APPRART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.85": {"line.1": {"text": "Fahrt nur fort, nach eurer Weise", "tokens": ["Fahrt", "nur", "fort", ",", "nach", "eu\u00b7rer", "Wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Welt zu \u00fcberspinnen!", "tokens": ["Die", "Welt", "zu", "\u00fc\u00b7bers\u00b7pin\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich in meinem lebendigen Kreise", "tokens": ["Ich", "in", "mei\u00b7nem", "le\u00b7ben\u00b7di\u00b7gen", "Krei\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Wei\u00df das Leben zu gewinnen.", "tokens": ["Wei\u00df", "das", "Le\u00b7ben", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.86": {"line.1": {"text": "Mir will das kranke Zeug nicht munden,", "tokens": ["Mir", "will", "das", "kran\u00b7ke", "Zeug", "nicht", "mun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Autoren sollten erst gesunden.", "tokens": ["Au\u00b7to\u00b7ren", "soll\u00b7ten", "erst", "ge\u00b7sun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.87": {"line.1": {"text": "Zeig ich die Fehler des Geschlechts,", "tokens": ["Zeig", "ich", "die", "Feh\u00b7ler", "des", "Ge\u00b7schlechts", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So hei\u00dft es: Tue selbst was Rechts.", "tokens": ["So", "hei\u00dft", "es", ":", "Tue", "selbst", "was", "Rechts", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "NN", "ADV", "PWS", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.88": {"line.1": {"text": "\u00bbdu Kr\u00e4ftiger, sei nicht so still,", "tokens": ["\u00bb", "du", "Kr\u00e4f\u00b7ti\u00b7ger", ",", "sei", "nicht", "so", "still", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "NN", "$,", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn auch sich andere scheuen.\u00ab", "tokens": ["Wenn", "auch", "sich", "an\u00b7de\u00b7re", "scheu\u00b7en", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADV", "PRF", "PIS", "VVINF", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wer den Teufel erschrecken will,", "tokens": ["Wer", "den", "Teu\u00b7fel", "er\u00b7schre\u00b7cken", "will", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der mu\u00df laut schreien.", "tokens": ["Der", "mu\u00df", "laut", "schrei\u00b7en", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADJD", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.89": {"line.1": {"text": "\u00bbdu hast an sch\u00f6nen Tagen", "tokens": ["\u00bb", "du", "hast", "an", "sch\u00f6\u00b7nen", "Ta\u00b7gen"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Dich manchmal abgequ\u00e4lt!\u00ab", "tokens": ["Dich", "manch\u00b7mal", "ab\u00b7ge\u00b7qu\u00e4lt", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich habe mich nie verrechnet,", "tokens": ["Ich", "ha\u00b7be", "mich", "nie", "ver\u00b7rech\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Aber oft verz\u00e4hlt.", "tokens": ["A\u00b7ber", "oft", "ver\u00b7z\u00e4hlt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.90": {"line.1": {"text": "\u00dcber Berg und Tal,", "tokens": ["\u00dc\u00b7ber", "Berg", "und", "Tal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Irrtum \u00fcber Irrtum allzumal,", "tokens": ["Irr\u00b7tum", "\u00fc\u00b7ber", "Irr\u00b7tum", "all\u00b7zu\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Kommen wir wieder ins Freie;", "tokens": ["Kom\u00b7men", "wir", "wie\u00b7der", "ins", "Frei\u00b7e", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Doch da ist's gar zu weit und breit,", "tokens": ["Doch", "da", "ist's", "gar", "zu", "weit", "und", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "PTKA", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nun suchen wir in kurzer Zeit", "tokens": ["Nun", "su\u00b7chen", "wir", "in", "kur\u00b7zer", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Irrgang und Berg aufs neue.", "tokens": ["Irr\u00b7gang", "und", "Berg", "aufs", "neu\u00b7e", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPRART", "ADJA", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.91": {"line.1": {"text": "Gibt's ein Gespr\u00e4ch, wenn wir uns nicht betr\u00fcgen,", "tokens": ["Gibt's", "ein", "Ge\u00b7spr\u00e4ch", ",", "wenn", "wir", "uns", "nicht", "be\u00b7tr\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$,", "KOUS", "PPER", "PRF", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mehr oder weniger versteckt?", "tokens": ["Mehr", "o\u00b7der", "we\u00b7ni\u00b7ger", "ver\u00b7steckt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "KON", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So ein Ragout von Wahrheit und von L\u00fcgen,", "tokens": ["So", "ein", "Ra\u00b7gout", "von", "Wahr\u00b7heit", "und", "von", "L\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das ist die K\u00f6cherei, die mir am besten schmeckt.", "tokens": ["Das", "ist", "die", "K\u00f6\u00b7che\u00b7rei", ",", "die", "mir", "am", "bes\u00b7ten", "schmeckt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "PRELS", "PPER", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.92": {"line.1": {"text": "Kennst du das Spiel, wo man im lust'gen Kreis", "tokens": ["Kennst", "du", "das", "Spiel", ",", "wo", "man", "im", "lust'\u00b7gen", "Kreis"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "PWAV", "PIS", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Das Pfeifchen sucht und niemals findet,", "tokens": ["Das", "Pfei\u00b7fchen", "sucht", "und", "nie\u00b7mals", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KON", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Weil man's dem Sucher, ohn da\u00df er's wei\u00df,", "tokens": ["Weil", "man's", "dem", "Su\u00b7cher", ",", "ohn", "da\u00df", "er's", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "$,", "KOUI", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "In seines Rockes hintre Falten bindet,", "tokens": ["In", "sei\u00b7nes", "Ro\u00b7ckes", "hin\u00b7tre", "Fal\u00b7ten", "bin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Das hei\u00dft: an seinen Stei\u00df?", "tokens": ["Das", "hei\u00dft", ":", "an", "sei\u00b7nen", "Stei\u00df", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$.", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.93": {"line.1": {"text": "Mit Narren leben wird dir gar nicht schwer,", "tokens": ["Mit", "Nar\u00b7ren", "le\u00b7ben", "wird", "dir", "gar", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Versammle nur ein Tollhaus um dich her.", "tokens": ["Ver\u00b7samm\u00b7le", "nur", "ein", "Toll\u00b7haus", "um", "dich", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bedenke dann, das macht dich gleich gelind,", "tokens": ["Be\u00b7den\u00b7ke", "dann", ",", "das", "macht", "dich", "gleich", "ge\u00b7lind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "PDS", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df Narrenw\u00e4rter selbst auch Narren sind.", "tokens": ["Da\u00df", "Nar\u00b7ren\u00b7w\u00e4r\u00b7ter", "selbst", "auch", "Nar\u00b7ren", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "ADV", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.94": {"line.1": {"text": "Wo recht viel Widerspr\u00fcche schwirren,", "tokens": ["Wo", "recht", "viel", "Wi\u00b7der\u00b7spr\u00fc\u00b7che", "schwir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mag ich am liebsten wandern;", "tokens": ["Mag", "ich", "am", "liebs\u00b7ten", "wan\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Niemand g\u00f6nnt dem andern \u2013", "tokens": ["Nie\u00b7mand", "g\u00f6nnt", "dem", "an\u00b7dern", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Wie lustig! \u2013 das Recht zu irren.", "tokens": ["Wie", "lus\u00b7tig", "!", "\u2013", "das", "Recht", "zu", "ir\u00b7ren", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "$.", "$(", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.95": {"line.1": {"text": "St\u00e4mme wollen gegen St\u00e4mme pochen,", "tokens": ["St\u00e4m\u00b7me", "wol\u00b7len", "ge\u00b7gen", "St\u00e4m\u00b7me", "po\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Kann doch einer, was der andere kann!", "tokens": ["Kann", "doch", "ei\u00b7ner", ",", "was", "der", "an\u00b7de\u00b7re", "kann", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PIS", "$,", "PRELS", "ART", "PIS", "VMFIN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Steckt doch Mark in jedem Knochen,", "tokens": ["Steckt", "doch", "Mark", "in", "je\u00b7dem", "Kno\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und in jedem Hemde steckt ein Mann.", "tokens": ["Und", "in", "je\u00b7dem", "Hem\u00b7de", "steckt", "ein", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.96": {"line.1": {"text": "Hat Welscher-Hahn an seinem Kropf,", "tokens": ["Hat", "Wel\u00b7scher\u00b7Hahn", "an", "sei\u00b7nem", "Kropf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Storch an dem Langhals Freude;", "tokens": ["Storch", "an", "dem", "Lang\u00b7hals", "Freu\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Der Kessel schilt den Ofentopf,", "tokens": ["Der", "Kes\u00b7sel", "schilt", "den", "O\u00b7fen\u00b7topf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Schwarz sind sie alle beide.", "tokens": ["Schwarz", "sind", "sie", "al\u00b7le", "bei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "PIAT", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.97": {"line.1": {"text": "\u00bbwie gerne s\u00e4h ich jeden stolzieren,", "tokens": ["\u00bb", "wie", "ger\u00b7ne", "s\u00e4h", "ich", "je\u00b7den", "stol\u00b7zie\u00b7ren", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOKOM", "ADV", "VVFIN", "PPER", "PIAT", "ADJA", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "K\u00f6nnt er das Pfauenrad vollf\u00fchren.", "tokens": ["K\u00f6nnt", "er", "das", "Pfau\u00b7en\u00b7rad", "voll\u00b7f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.98": {"line.1": {"text": "Warum nur die h\u00fcbschen Leute", "tokens": ["Wa\u00b7rum", "nur", "die", "h\u00fcb\u00b7schen", "Leu\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Mir nicht gefallen sollen?\u00ab", "tokens": ["Mir", "nicht", "ge\u00b7fal\u00b7len", "sol\u00b7len", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "PTKNEG", "VVINF", "VMFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Manchen h\u00e4lt man f\u00fcr fett,", "tokens": ["Man\u00b7chen", "h\u00e4lt", "man", "f\u00fcr", "fett", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "APPR", "ADJD", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Er ist nur geschwollen.", "tokens": ["Er", "ist", "nur", "ge\u00b7schwol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.99": {"line.1": {"text": "Da reiten sie hin! wer hemmt den Lauf!", "tokens": ["Da", "rei\u00b7ten", "sie", "hin", "!", "wer", "hemmt", "den", "Lauf", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$.", "PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wer reitet denn? Stolz und Unwissenheit.", "tokens": ["Wer", "rei\u00b7tet", "denn", "?", "Stolz", "und", "Un\u00b7wis\u00b7sen\u00b7heit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "$.", "NN", "KON", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "La\u00df sie reiten! da ist gute Zeit,", "tokens": ["La\u00df", "sie", "rei\u00b7ten", "!", "da", "ist", "gu\u00b7te", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "VVFIN", "$.", "ADV", "VAFIN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Schimpf und Schade sitzen hinten auf.", "tokens": ["Schimpf", "und", "Scha\u00b7de", "sit\u00b7zen", "hin\u00b7ten", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.100": {"line.1": {"text": "\u00bbwie ist dir's doch so balde", "tokens": ["\u00bb", "wie", "ist", "dir's", "doch", "so", "bal\u00b7de"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KOKOM", "VAFIN", "PIS", "ADV", "ADV", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Zur Ehr und Schmach gediehn?\u00ab", "tokens": ["Zur", "Ehr", "und", "Schmach", "ge\u00b7diehn", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Blieb' der Wolf im Walde,", "tokens": ["Blieb'", "der", "Wolf", "im", "Wal\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "APPRART", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "So w\u00fcrd er nicht beschrien.", "tokens": ["So", "w\u00fcrd", "er", "nicht", "be\u00b7schri\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}