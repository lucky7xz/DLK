{"textgrid.poem.24300": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "Die Reise ohne Fahrplan", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In diese r\u00e4tselhafte Welt", "tokens": ["In", "die\u00b7se", "r\u00e4t\u00b7sel\u00b7haf\u00b7te", "Welt"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind wir alle als R\u00e4tsel gestellt;", "tokens": ["Sind", "wir", "al\u00b7le", "als", "R\u00e4t\u00b7sel", "ge\u00b7stellt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "KOKOM", "NN", "VVPP", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Bilden Charaden.", "tokens": ["Bil\u00b7den", "Cha\u00b7ra\u00b7den", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wer sucht den Sinn, wer findet Verstand", "tokens": ["Wer", "sucht", "den", "Sinn", ",", "wer", "fin\u00b7det", "Ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "ART", "NN", "$,", "PWS", "VVFIN", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "In diesem wimmelnden Allerhand?", "tokens": ["In", "die\u00b7sem", "wim\u00b7meln\u00b7den", "Al\u00b7ler\u00b7hand", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Wer kann uns erraten?", "tokens": ["Wer", "kann", "uns", "er\u00b7ra\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.2": {"line.1": {"text": "Wir selber? Kaum. Wir tauschen nichts als Zeichen,", "tokens": ["Wir", "sel\u00b7ber", "?", "Kaum", ".", "Wir", "tau\u00b7schen", "nichts", "als", "Zei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$.", "ADV", "$.", "PPER", "VVFIN", "PIS", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Andeutungen geheimnisvoller Art;", "tokens": ["An\u00b7deu\u00b7tun\u00b7gen", "ge\u00b7heim\u00b7nis\u00b7vol\u00b7ler", "Art", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ziehn uns Signale auf und stellen Weichen,", "tokens": ["Ziehn", "uns", "Sig\u00b7na\u00b7le", "auf", "und", "stel\u00b7len", "Wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "PTKVZ", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df keiner st\u00f6ren mag des andern Fahrt,", "tokens": ["Da\u00df", "kei\u00b7ner", "st\u00f6\u00b7ren", "mag", "des", "an\u00b7dern", "Fahrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVINF", "VMFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Die ach auf str\u00e4flich unsoliden Speichen", "tokens": ["Die", "ach", "auf", "str\u00e4f\u00b7lich", "un\u00b7so\u00b7li\u00b7den", "Spei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Uns an ein Loch f\u00fchrt, keinem noch erspart:", "tokens": ["Uns", "an", "ein", "Loch", "f\u00fchrt", ",", "kei\u00b7nem", "noch", "er\u00b7spart", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "VVFIN", "$,", "PIS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "An den bekannten Tunneleingang, der,", "tokens": ["An", "den", "be\u00b7kann\u00b7ten", "Tun\u00b7nel\u00b7ein\u00b7gang", ",", "der", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "PRELS", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Wenn wir es k\u00f6nnten, l\u00e4ngst vermauert w\u00e4r.", "tokens": ["Wenn", "wir", "es", "k\u00f6nn\u00b7ten", ",", "l\u00e4ngst", "ver\u00b7mau\u00b7ert", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VMFIN", "$,", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Vielleicht studiert ein Gott das wirre Wesen,", "tokens": ["Viel\u00b7leicht", "stu\u00b7diert", "ein", "Gott", "das", "wir\u00b7re", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wie ein Professor dies und das studiert:", "tokens": ["Wie", "ein", "Pro\u00b7fes\u00b7sor", "dies", "und", "das", "stu\u00b7diert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PDS", "KON", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bakterien, unters Mikroskop gelesen;", "tokens": ["Bak\u00b7te\u00b7ri\u00b7en", ",", "un\u00b7ters", "Mik\u00b7ros\u00b7kop", "ge\u00b7le\u00b7sen", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "APPRART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "Zahlenkolumnen, m\u00e4chtig aufmarschiert;", "tokens": ["Zah\u00b7len\u00b7ko\u00b7lum\u00b7nen", ",", "m\u00e4ch\u00b7tig", "auf\u00b7mar\u00b7schiert", ";"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "VVPP", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.5": {"text": "Vokabeln eines Dichters; welche Spesen,", "tokens": ["Vo\u00b7ka\u00b7beln", "ei\u00b7nes", "Dich\u00b7ters", ";", "wel\u00b7che", "Spe\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$.", "PWAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Im Haushalt der Natur die Kraft summiert.", "tokens": ["Im", "Haus\u00b7halt", "der", "Na\u00b7tur", "die", "Kraft", "sum\u00b7miert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wer wei\u00df, was einen Gott dran interessiert, \u2013", "tokens": ["Wer", "wei\u00df", ",", "was", "ei\u00b7nen", "Gott", "dran", "in\u00b7ter\u00b7es\u00b7siert", ",", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$,", "PRELS", "ART", "NN", "PAV", "VVFIN", "$,", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "Bis er, gelangweilt, mit dem Sturmesbesen", "tokens": ["Bis", "er", ",", "ge\u00b7lang\u00b7weilt", ",", "mit", "dem", "Stur\u00b7mes\u00b7be\u00b7sen"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "VVPP", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Das r\u00e4tselhafte Zeug beiseite wischt:", "tokens": ["Das", "r\u00e4t\u00b7sel\u00b7haf\u00b7te", "Zeug", "bei\u00b7sei\u00b7te", "wischt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Da\u00df Laus wie Elefant zugleich verschwinden,", "tokens": ["Da\u00df", "Laus", "wie", "E\u00b7le\u00b7fant", "zu\u00b7gleich", "ver\u00b7schwin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KOKOM", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Die ganze Weltgeschichte Kehricht ist,", "tokens": ["Die", "gan\u00b7ze", "Welt\u00b7ge\u00b7schich\u00b7te", "Keh\u00b7richt", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Napoleon nicht und Goethe mehr zu finden", "tokens": ["Na\u00b7po\u00b7le\u00b7on", "nicht", "und", "Goe\u00b7the", "mehr", "zu", "fin\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "PTKNEG", "KON", "NE", "ADV", "PTKZU", "VVINF"], "meter": "----+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.13": {"text": "Im gro\u00dfen schwarzen Weltentintengischt,", "tokens": ["Im", "gro\u00b7\u00dfen", "schwar\u00b7zen", "Wel\u00b7ten\u00b7tin\u00b7ten\u00b7gischt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Durch das die Zeit sich ruhig weiter fri\u00dft.", "tokens": ["Durch", "das", "die", "Zeit", "sich", "ru\u00b7hig", "wei\u00b7ter", "fri\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "NN", "PRF", "ADJD", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Doch kanns auch sein: Es kennt die Hieroglyphen", "tokens": ["Doch", "kanns", "auch", "sein", ":", "Es", "kennt", "die", "Hie\u00b7ro\u00b7gly\u00b7phen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "VAINF", "$.", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der Irgendwer, der diese R\u00e4tsel schrieb,", "tokens": ["Der", "Ir\u00b7gend\u00b7wer", ",", "der", "die\u00b7se", "R\u00e4t\u00b7sel", "schrieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Die nebenbei auch uns ins Leben riefen.", "tokens": ["Die", "ne\u00b7ben\u00b7bei", "auch", "uns", "ins", "Le\u00b7ben", "rie\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wer wei\u00df, vielleicht sind wir ihm wirklich lieb,", "tokens": ["Wer", "wei\u00df", ",", "viel\u00b7leicht", "sind", "wir", "ihm", "wirk\u00b7lich", "lieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "ADV", "VAFIN", "PPER", "PPER", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Und, was uns weh tut, jeder Schicksalshieb,", "tokens": ["Und", ",", "was", "uns", "weh", "tut", ",", "je\u00b7der", "Schick\u00b7sals\u00b7hieb", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$,", "PRELS", "PPER", "ADV", "VVFIN", "$,", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Will uns, prost Mahlzeit, will uns blo\u00df vertiefen.", "tokens": ["Will", "uns", ",", "prost", "Mahl\u00b7zeit", ",", "will", "uns", "blo\u00df", "ver\u00b7tie\u00b7fen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "VVFIN", "NN", "$,", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Es kann ja sein. Was kann nicht sein auf Erden?", "tokens": ["Es", "kann", "ja", "sein", ".", "Was", "kann", "nicht", "sein", "auf", "Er\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VAINF", "$.", "PWS", "VMFIN", "PTKNEG", "VAINF", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Wir k\u00f6nnen in der Tat noch alle Engel werden.", "tokens": ["Wir", "k\u00f6n\u00b7nen", "in", "der", "Tat", "noch", "al\u00b7le", "En\u00b7gel", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ART", "NN", "ADV", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Wei\u00df Gott: Gott wei\u00df es! Unser ist allein", "tokens": ["Wei\u00df", "Gott", ":", "Gott", "wei\u00df", "es", "!", "Un\u00b7ser", "ist", "al\u00b7lein"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "NN", "$.", "NN", "VVFIN", "PPER", "$.", "ADJD", "VAFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die Pflicht, ihm ein gef\u00fcger Stoff zu sein,", "tokens": ["Die", "Pflicht", ",", "ihm", "ein", "ge\u00b7f\u00fc\u00b7ger", "Stoff", "zu", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Auf da\u00df uns selbst die wunderliche Erde", "tokens": ["Auf", "da\u00df", "uns", "selbst", "die", "wun\u00b7der\u00b7li\u00b7che", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Kein Nadelkissen oder Kantenstein,", "tokens": ["Kein", "Na\u00b7del\u00b7kis\u00b7sen", "o\u00b7der", "Kan\u00b7ten\u00b7stein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sondern ein Garten voller Fr\u00fcchte werde.", "tokens": ["Son\u00b7dern", "ein", "Gar\u00b7ten", "vol\u00b7ler", "Fr\u00fcch\u00b7te", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "VAFIN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Und geht es dann ins Tunnelloch hinein,", "tokens": ["Und", "geht", "es", "dann", "ins", "Tun\u00b7nel\u00b7loch", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Soll wenigstens die Lebewohlgeb\u00e4rde", "tokens": ["Soll", "we\u00b7nigs\u00b7tens", "die", "Le\u00b7be\u00b7wohl\u00b7ge\u00b7b\u00e4r\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Den weiter R\u00e4tselnden kein schlechter Anblick sein.", "tokens": ["Den", "wei\u00b7ter", "R\u00e4t\u00b7seln\u00b7den", "kein", "schlech\u00b7ter", "An\u00b7blick", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PIAT", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "In diese r\u00e4tselhafte Welt", "tokens": ["In", "die\u00b7se", "r\u00e4t\u00b7sel\u00b7haf\u00b7te", "Welt"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind wir alle als R\u00e4tsel gestellt;", "tokens": ["Sind", "wir", "al\u00b7le", "als", "R\u00e4t\u00b7sel", "ge\u00b7stellt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "KOKOM", "NN", "VVPP", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Bilden Charaden.", "tokens": ["Bil\u00b7den", "Cha\u00b7ra\u00b7den", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wer sucht den Sinn, wer findet Verstand", "tokens": ["Wer", "sucht", "den", "Sinn", ",", "wer", "fin\u00b7det", "Ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "ART", "NN", "$,", "PWS", "VVFIN", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "In diesem wimmelnden Allerhand?", "tokens": ["In", "die\u00b7sem", "wim\u00b7meln\u00b7den", "Al\u00b7ler\u00b7hand", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Wer kann uns erraten?", "tokens": ["Wer", "kann", "uns", "er\u00b7ra\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.7": {"line.1": {"text": "Wir selber? Kaum. Wir tauschen nichts als Zeichen,", "tokens": ["Wir", "sel\u00b7ber", "?", "Kaum", ".", "Wir", "tau\u00b7schen", "nichts", "als", "Zei\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$.", "ADV", "$.", "PPER", "VVFIN", "PIS", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Andeutungen geheimnisvoller Art;", "tokens": ["An\u00b7deu\u00b7tun\u00b7gen", "ge\u00b7heim\u00b7nis\u00b7vol\u00b7ler", "Art", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ziehn uns Signale auf und stellen Weichen,", "tokens": ["Ziehn", "uns", "Sig\u00b7na\u00b7le", "auf", "und", "stel\u00b7len", "Wei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "PTKVZ", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df keiner st\u00f6ren mag des andern Fahrt,", "tokens": ["Da\u00df", "kei\u00b7ner", "st\u00f6\u00b7ren", "mag", "des", "an\u00b7dern", "Fahrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVINF", "VMFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Die ach auf str\u00e4flich unsoliden Speichen", "tokens": ["Die", "ach", "auf", "str\u00e4f\u00b7lich", "un\u00b7so\u00b7li\u00b7den", "Spei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Uns an ein Loch f\u00fchrt, keinem noch erspart:", "tokens": ["Uns", "an", "ein", "Loch", "f\u00fchrt", ",", "kei\u00b7nem", "noch", "er\u00b7spart", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "VVFIN", "$,", "PIS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "An den bekannten Tunneleingang, der,", "tokens": ["An", "den", "be\u00b7kann\u00b7ten", "Tun\u00b7nel\u00b7ein\u00b7gang", ",", "der", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "PRELS", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Wenn wir es k\u00f6nnten, l\u00e4ngst vermauert w\u00e4r.", "tokens": ["Wenn", "wir", "es", "k\u00f6nn\u00b7ten", ",", "l\u00e4ngst", "ver\u00b7mau\u00b7ert", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VMFIN", "$,", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Vielleicht studiert ein Gott das wirre Wesen,", "tokens": ["Viel\u00b7leicht", "stu\u00b7diert", "ein", "Gott", "das", "wir\u00b7re", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wie ein Professor dies und das studiert:", "tokens": ["Wie", "ein", "Pro\u00b7fes\u00b7sor", "dies", "und", "das", "stu\u00b7diert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PDS", "KON", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bakterien, unters Mikroskop gelesen;", "tokens": ["Bak\u00b7te\u00b7ri\u00b7en", ",", "un\u00b7ters", "Mik\u00b7ros\u00b7kop", "ge\u00b7le\u00b7sen", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "APPRART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "Zahlenkolumnen, m\u00e4chtig aufmarschiert;", "tokens": ["Zah\u00b7len\u00b7ko\u00b7lum\u00b7nen", ",", "m\u00e4ch\u00b7tig", "auf\u00b7mar\u00b7schiert", ";"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "VVPP", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.5": {"text": "Vokabeln eines Dichters; welche Spesen,", "tokens": ["Vo\u00b7ka\u00b7beln", "ei\u00b7nes", "Dich\u00b7ters", ";", "wel\u00b7che", "Spe\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$.", "PWAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Im Haushalt der Natur die Kraft summiert.", "tokens": ["Im", "Haus\u00b7halt", "der", "Na\u00b7tur", "die", "Kraft", "sum\u00b7miert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wer wei\u00df, was einen Gott dran interessiert, \u2013", "tokens": ["Wer", "wei\u00df", ",", "was", "ei\u00b7nen", "Gott", "dran", "in\u00b7ter\u00b7es\u00b7siert", ",", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "$,", "PRELS", "ART", "NN", "PAV", "VVFIN", "$,", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "Bis er, gelangweilt, mit dem Sturmesbesen", "tokens": ["Bis", "er", ",", "ge\u00b7lang\u00b7weilt", ",", "mit", "dem", "Stur\u00b7mes\u00b7be\u00b7sen"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "VVPP", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Das r\u00e4tselhafte Zeug beiseite wischt:", "tokens": ["Das", "r\u00e4t\u00b7sel\u00b7haf\u00b7te", "Zeug", "bei\u00b7sei\u00b7te", "wischt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Da\u00df Laus wie Elefant zugleich verschwinden,", "tokens": ["Da\u00df", "Laus", "wie", "E\u00b7le\u00b7fant", "zu\u00b7gleich", "ver\u00b7schwin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KOKOM", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Die ganze Weltgeschichte Kehricht ist,", "tokens": ["Die", "gan\u00b7ze", "Welt\u00b7ge\u00b7schich\u00b7te", "Keh\u00b7richt", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Napoleon nicht und Goethe mehr zu finden", "tokens": ["Na\u00b7po\u00b7le\u00b7on", "nicht", "und", "Goe\u00b7the", "mehr", "zu", "fin\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "PTKNEG", "KON", "NE", "ADV", "PTKZU", "VVINF"], "meter": "----+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.13": {"text": "Im gro\u00dfen schwarzen Weltentintengischt,", "tokens": ["Im", "gro\u00b7\u00dfen", "schwar\u00b7zen", "Wel\u00b7ten\u00b7tin\u00b7ten\u00b7gischt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Durch das die Zeit sich ruhig weiter fri\u00dft.", "tokens": ["Durch", "das", "die", "Zeit", "sich", "ru\u00b7hig", "wei\u00b7ter", "fri\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "NN", "PRF", "ADJD", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Doch kanns auch sein: Es kennt die Hieroglyphen", "tokens": ["Doch", "kanns", "auch", "sein", ":", "Es", "kennt", "die", "Hie\u00b7ro\u00b7gly\u00b7phen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "VAINF", "$.", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der Irgendwer, der diese R\u00e4tsel schrieb,", "tokens": ["Der", "Ir\u00b7gend\u00b7wer", ",", "der", "die\u00b7se", "R\u00e4t\u00b7sel", "schrieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Die nebenbei auch uns ins Leben riefen.", "tokens": ["Die", "ne\u00b7ben\u00b7bei", "auch", "uns", "ins", "Le\u00b7ben", "rie\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wer wei\u00df, vielleicht sind wir ihm wirklich lieb,", "tokens": ["Wer", "wei\u00df", ",", "viel\u00b7leicht", "sind", "wir", "ihm", "wirk\u00b7lich", "lieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "ADV", "VAFIN", "PPER", "PPER", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Und, was uns weh tut, jeder Schicksalshieb,", "tokens": ["Und", ",", "was", "uns", "weh", "tut", ",", "je\u00b7der", "Schick\u00b7sals\u00b7hieb", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "$,", "PRELS", "PPER", "ADV", "VVFIN", "$,", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Will uns, prost Mahlzeit, will uns blo\u00df vertiefen.", "tokens": ["Will", "uns", ",", "prost", "Mahl\u00b7zeit", ",", "will", "uns", "blo\u00df", "ver\u00b7tie\u00b7fen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "VVFIN", "NN", "$,", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Es kann ja sein. Was kann nicht sein auf Erden?", "tokens": ["Es", "kann", "ja", "sein", ".", "Was", "kann", "nicht", "sein", "auf", "Er\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VAINF", "$.", "PWS", "VMFIN", "PTKNEG", "VAINF", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Wir k\u00f6nnen in der Tat noch alle Engel werden.", "tokens": ["Wir", "k\u00f6n\u00b7nen", "in", "der", "Tat", "noch", "al\u00b7le", "En\u00b7gel", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ART", "NN", "ADV", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Wei\u00df Gott: Gott wei\u00df es! Unser ist allein", "tokens": ["Wei\u00df", "Gott", ":", "Gott", "wei\u00df", "es", "!", "Un\u00b7ser", "ist", "al\u00b7lein"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "NN", "$.", "NN", "VVFIN", "PPER", "$.", "ADJD", "VAFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die Pflicht, ihm ein gef\u00fcger Stoff zu sein,", "tokens": ["Die", "Pflicht", ",", "ihm", "ein", "ge\u00b7f\u00fc\u00b7ger", "Stoff", "zu", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "ART", "ADJA", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Auf da\u00df uns selbst die wunderliche Erde", "tokens": ["Auf", "da\u00df", "uns", "selbst", "die", "wun\u00b7der\u00b7li\u00b7che", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Kein Nadelkissen oder Kantenstein,", "tokens": ["Kein", "Na\u00b7del\u00b7kis\u00b7sen", "o\u00b7der", "Kan\u00b7ten\u00b7stein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sondern ein Garten voller Fr\u00fcchte werde.", "tokens": ["Son\u00b7dern", "ein", "Gar\u00b7ten", "vol\u00b7ler", "Fr\u00fcch\u00b7te", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "VAFIN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Und geht es dann ins Tunnelloch hinein,", "tokens": ["Und", "geht", "es", "dann", "ins", "Tun\u00b7nel\u00b7loch", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Soll wenigstens die Lebewohlgeb\u00e4rde", "tokens": ["Soll", "we\u00b7nigs\u00b7tens", "die", "Le\u00b7be\u00b7wohl\u00b7ge\u00b7b\u00e4r\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Den weiter R\u00e4tselnden kein schlechter Anblick sein.", "tokens": ["Den", "wei\u00b7ter", "R\u00e4t\u00b7seln\u00b7den", "kein", "schlech\u00b7ter", "An\u00b7blick", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PIAT", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}