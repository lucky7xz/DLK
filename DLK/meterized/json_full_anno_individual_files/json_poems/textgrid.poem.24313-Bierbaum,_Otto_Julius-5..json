{"textgrid.poem.24313": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "5.", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "T\u00e4glich fahr ich mit Pietro,", "tokens": ["T\u00e4g\u00b7lich", "fahr", "ich", "mit", "Pie\u00b7tro", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Meinem wohlbeleibten Kutscher", "tokens": ["Mei\u00b7nem", "wohl\u00b7be\u00b7leib\u00b7ten", "Kut\u00b7scher"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "(und mit seinem Pferdchen Palle,", "tokens": ["(", "und", "mit", "sei\u00b7nem", "Pferd\u00b7chen", "Pal\u00b7le", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welches auch nicht mager ist),", "tokens": ["Wel\u00b7ches", "auch", "nicht", "ma\u00b7ger", "ist", ")", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "ADV", "PTKNEG", "ADJD", "VAFIN", "$(", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "T\u00e4glich nachmittags um dreie", "tokens": ["T\u00e4g\u00b7lich", "nach\u00b7mit\u00b7tags", "um", "drei\u00b7e"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "ADV", "APPR", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fahr ich auf der alten Stra\u00dfe,", "tokens": ["Fahr", "ich", "auf", "der", "al\u00b7ten", "Stra\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die sehr steil ist und sehr holprig,", "tokens": ["Die", "sehr", "steil", "ist", "und", "sehr", "holp\u00b7rig", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "KON", "ADV", "ADJD", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Erst nach San Domenico", "tokens": ["Erst", "nach", "San", "Do\u00b7me\u00b7ni\u00b7co"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "NE"], "meter": "+---+--", "measure": "dactylic.init"}}, "stanza.3": {"line.1": {"text": "Und sodann, vorbei der Villa,", "tokens": ["Und", "so\u00b7dann", ",", "vor\u00b7bei", "der", "Vil\u00b7la", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "ART", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Wo Herr Dante einst verliebt war,", "tokens": ["Wo", "Herr", "Dan\u00b7te", "einst", "ver\u00b7liebt", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NE", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zwischen hohen Gartenmauern", "tokens": ["Zwi\u00b7schen", "ho\u00b7hen", "Gar\u00b7ten\u00b7mau\u00b7ern"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach Florenz. Dort trink ich Tee.", "tokens": ["Nach", "Flo\u00b7renz", ".", "Dort", "trink", "ich", "Tee", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$.", "ADV", "VVFIN", "PPER", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "\u00bbwie? Und der Palazzo Pitti?", "tokens": ["\u00bb", "wie", "?", "Und", "der", "Pa\u00b7laz\u00b7zo", "Pit\u00b7ti", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "$.", "KON", "ART", "NN", "NE", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Accademia? Uffizien?", "tokens": ["Ac\u00b7ca\u00b7de\u00b7mia", "?", "Uf\u00b7fi\u00b7zi\u00b7en", "?"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bibliotheca Laurenziana?", "tokens": ["Bib\u00b7liot\u00b7he\u00b7ca", "Lau\u00b7ren\u00b7zi\u00b7a\u00b7na", "?"], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hast du nicht nach Sch\u00f6nheit Durst?\u00ab", "tokens": ["Hast", "du", "nicht", "nach", "Sch\u00f6n\u00b7heit", "Durst", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "APPR", "NN", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Oh ja. Aber f\u00fcr Museen", "tokens": ["Oh", "ja", ".", "A\u00b7ber", "f\u00fcr", "Mu\u00b7se\u00b7en"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "ADV", "$.", "KON", "APPR", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Bin ich selten nur in Stimmung;", "tokens": ["Bin", "ich", "sel\u00b7ten", "nur", "in", "Stim\u00b7mung", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn es sind Konservenb\u00fcchsen;", "tokens": ["Denn", "es", "sind", "Kon\u00b7ser\u00b7ven\u00b7b\u00fcch\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihre Sch\u00f6nheit schmeckt nach Blech.", "tokens": ["Ih\u00b7re", "Sch\u00f6n\u00b7heit", "schmeckt", "nach", "Blech", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "\u00bbwie? Die himmlische Tribuna?", "tokens": ["\u00bb", "wie", "?", "Die", "himm\u00b7li\u00b7sche", "Tri\u00b7bu\u00b7na", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "$.", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Alessandro Botticelli?", "tokens": ["A\u00b7les\u00b7sand\u00b7ro", "Bot\u00b7ti\u00b7cel\u00b7li", "?"], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Cimabue? Donatello?\u00ab", "tokens": ["Ci\u00b7ma\u00b7bue", "?", "Do\u00b7na\u00b7tel\u00b7lo", "?", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "NE", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alle schmecken dort nach Blech.", "tokens": ["Al\u00b7le", "schme\u00b7cken", "dort", "nach", "Blech", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Lieber wandre ich durch dunkle", "tokens": ["Lie\u00b7ber", "wand\u00b7re", "ich", "durch", "dunk\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kirchen mit dem Operngucker", "tokens": ["Kir\u00b7chen", "mit", "dem", "O\u00b7pern\u00b7gu\u00b7cker"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und verrenke Hals und Kopf mir", "tokens": ["Und", "ver\u00b7ren\u00b7ke", "Hals", "und", "Kopf", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "KON", "NN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach der dort verstecken Kunst.", "tokens": ["Nach", "der", "dort", "ver\u00b7ste\u00b7cken", "Kunst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Da nur wirkt sie noch ins Leben,", "tokens": ["Da", "nur", "wirkt", "sie", "noch", "ins", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Thront sie noch auf ihrem Throne,", "tokens": ["Thront", "sie", "noch", "auf", "ih\u00b7rem", "Thro\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frei, gebietend, nicht gefangen:", "tokens": ["Frei", ",", "ge\u00b7bie\u00b7tend", ",", "nicht", "ge\u00b7fan\u00b7gen", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,", "PTKNEG", "PTKVZ", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Atmet aus und atmet ein.", "tokens": ["At\u00b7met", "aus", "und", "at\u00b7met", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Denn ein Kunstwerk braucht den Atem,", "tokens": ["Denn", "ein", "Kunst\u00b7werk", "braucht", "den", "A\u00b7tem", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Braucht die Luft des t\u00e4tigen Lebens;", "tokens": ["Braucht", "die", "Luft", "des", "t\u00e4\u00b7ti\u00b7gen", "Le\u00b7bens", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Seine Sch\u00f6nheit wird zum Schemen,", "tokens": ["Sei\u00b7ne", "Sch\u00f6n\u00b7heit", "wird", "zum", "Sche\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sperrt man sie vom Leben ab.", "tokens": ["Sperrt", "man", "sie", "vom", "Le\u00b7ben", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "APPRART", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "St\u00fcnde David noch im Freien,", "tokens": ["St\u00fcn\u00b7de", "Da\u00b7vid", "noch", "im", "Frei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dort, wohin ihn schuf sein Sch\u00f6pfer,", "tokens": ["Dort", ",", "wo\u00b7hin", "ihn", "schuf", "sein", "Sch\u00f6p\u00b7fer", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wohl, er w\u00e4re nicht so gl\u00e4nzend", "tokens": ["Wohl", ",", "er", "w\u00e4\u00b7re", "nicht", "so", "gl\u00e4n\u00b7zend"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PPER", "VAFIN", "PTKNEG", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wei\u00df wie jetzt und \u00bbfast wie neu\u00ab,", "tokens": ["Wei\u00df", "wie", "jetzt", "und", "\u00bb", "fast", "wie", "neu", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "KOKOM", "ADV", "KON", "$(", "ADV", "KOKOM", "ADJD", "$(", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Aber, grau vielleicht und rissig,", "tokens": ["A\u00b7ber", ",", "grau", "viel\u00b7leicht", "und", "ris\u00b7sig", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "ADV", "KON", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mitgenommen von Frost und Feuchte,", "tokens": ["Mit\u00b7ge\u00b7nom\u00b7men", "von", "Frost", "und", "Feuch\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Leidend, wie das Leben immer", "tokens": ["Lei\u00b7dend", ",", "wie", "das", "Le\u00b7ben", "im\u00b7mer"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "PWAV", "ART", "NN", "ADV"], "meter": "+-+-+-++", "measure": "unknown.measure.penta"}, "line.4": {"text": "Leiden mu\u00df, um ", "tokens": ["Lei\u00b7den", "mu\u00df", ",", "um"], "token_info": ["word", "word", "punct", "word"], "pos": ["NN", "VMFIN", "$,", "KOUI"], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.12": {"line.1": {"text": "St\u00fcnd er heldenhaft lebendig,", "tokens": ["St\u00fcnd", "er", "hel\u00b7den\u00b7haft", "le\u00b7ben\u00b7dig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sterbend st\u00fcnd er noch lebendiger,", "tokens": ["Ster\u00b7bend", "st\u00fcnd", "er", "noch", "le\u00b7ben\u00b7di\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "ADJA", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Herrlicher, strahlender da, als jetzt im", "tokens": ["Herr\u00b7li\u00b7cher", ",", "strah\u00b7len\u00b7der", "da", ",", "als", "jetzt", "im"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "ADV", "ADV", "$,", "KOUS", "ADV", "APPRART"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Abgemessenen Oberlicht.", "tokens": ["Ab\u00b7ge\u00b7mes\u00b7se\u00b7nen", "O\u00b7ber\u00b7licht", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.13": {"line.1": {"text": "\u00bbund verd\u00fcrbe.\u00ab Freilich. Alles", "tokens": ["\u00bb", "und", "ver\u00b7d\u00fcr\u00b7be", ".", "\u00ab", "Frei\u00b7lich", ".", "Al\u00b7les"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "punct", "word"], "pos": ["$(", "KON", "VVFIN", "$.", "$(", "ADV", "$.", "PIS"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Leben mu\u00df einmal verderben.", "tokens": ["Le\u00b7ben", "mu\u00df", "ein\u00b7mal", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber leben soll es, leben:", "tokens": ["A\u00b7ber", "le\u00b7ben", "soll", "es", ",", "le\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wirklich leben, bis es stirbt.", "tokens": ["Wirk\u00b7lich", "le\u00b7ben", ",", "bis", "es", "stirbt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Denkt nicht immer an die Enkel!", "tokens": ["Denkt", "nicht", "im\u00b7mer", "an", "die", "En\u00b7kel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denkt an euch, wie jene taten,", "tokens": ["Denkt", "an", "euch", ",", "wie", "je\u00b7ne", "ta\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPER", "$,", "PWAV", "PDS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die ihr Leben sich verkl\u00e4rten,", "tokens": ["Die", "ihr", "Le\u00b7ben", "sich", "ver\u00b7kl\u00e4r\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bildner ihrer Gegenwart.", "tokens": ["Bild\u00b7ner", "ih\u00b7rer", "Ge\u00b7gen\u00b7wart", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Dann erst h\u00e4ttet ihr ein Recht, sie", "tokens": ["Dann", "erst", "h\u00e4t\u00b7tet", "ihr", "ein", "Recht", ",", "sie"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ART", "NN", "$,", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In die heiligen Leichenkammern", "tokens": ["In", "die", "hei\u00b7li\u00b7gen", "Lei\u00b7chen\u00b7kam\u00b7mern"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Eurer Piet\u00e4t zu stecken,", "tokens": ["Eu\u00b7rer", "Pi\u00b7e\u00b7t\u00e4t", "zu", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Brauchtet ihr f\u00fcr Eignes Platz.", "tokens": ["Brauch\u00b7tet", "ihr", "f\u00fcr", "Eig\u00b7nes", "Platz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Doch genug. Ich geh zu Gilli,", "tokens": ["Doch", "ge\u00b7nug", ".", "Ich", "geh", "zu", "Gil\u00b7li", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "PPER", "VVFIN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Trinke Tee und esse Kuchen.", "tokens": ["Trin\u00b7ke", "Tee", "und", "es\u00b7se", "Ku\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Leider bin ich manchmal schwach und", "tokens": ["Lei\u00b7der", "bin", "ich", "manch\u00b7mal", "schwach", "und"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "KON"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Lese Zeitungen dazu.", "tokens": ["Le\u00b7se", "Zei\u00b7tun\u00b7gen", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Heiliger Marsyas! Noch immer,", "tokens": ["Hei\u00b7li\u00b7ger", "Mar\u00b7syas", "!", "Noch", "im\u00b7mer", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADV", "ADV", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Simson Deutschland, sind Philister,", "tokens": ["Sim\u00b7son", "Deutschland", ",", "sind", "Phi\u00b7lis\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "VAFIN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ach, und was f\u00fcr eine Sorte", "tokens": ["Ach", ",", "und", "was", "f\u00fcr", "ei\u00b7ne", "Sor\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "KON", "PWS", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "(frech und bieder), \u00fcber dir.", "tokens": ["(", "frech", "und", "bie\u00b7der", ")", ",", "\u00fc\u00b7ber", "dir", "."], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "ADJD", "KON", "ADJD", "$(", "$,", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Deine Delila hei\u00dft Wohlstand.", "tokens": ["Dei\u00b7ne", "De\u00b7li\u00b7la", "hei\u00dft", "Wohl\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "VVFIN", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "\u00dcppigst hast du zugenommen.", "tokens": ["\u00dcp\u00b7pigst", "hast", "du", "zu\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wohl bekommt dein Fett dem Bauche,", "tokens": ["Wohl", "be\u00b7kommt", "dein", "Fett", "dem", "Bau\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch dem Hirn bekommt es schlecht.", "tokens": ["Doch", "dem", "Hirn", "be\u00b7kommt", "es", "schlecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.19": {"line.1": {"text": "Und der Seele, ach, der edlen", "tokens": ["Und", "der", "See\u00b7le", ",", "ach", ",", "der", "ed\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ART", "NN", "$,", "ITJ", "$,", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deutschen Seele fehlts am Raume,", "tokens": ["Deut\u00b7schen", "See\u00b7le", "fehlts", "am", "Rau\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Scheint es, in dem kolossalen", "tokens": ["Scheint", "es", ",", "in", "dem", "ko\u00b7los\u00b7sa\u00b7len"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Korpus, der ganz Masse ist.", "tokens": ["Kor\u00b7pus", ",", "der", "ganz", "Mas\u00b7se", "ist", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PRELS", "ADV", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "Bocke, bocke nicht, Troch\u00e4us!", "tokens": ["Bo\u00b7cke", ",", "bo\u00b7cke", "nicht", ",", "Troc\u00b7h\u00e4us", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PTKNEG", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Jetzo mu\u00dft du Zahlen buckeln.", "tokens": ["Jet\u00b7zo", "mu\u00dft", "du", "Zah\u00b7len", "bu\u00b7ckeln", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schwer f\u00e4llt wohl dabei das Tanzen,", "tokens": ["Schwer", "f\u00e4llt", "wohl", "da\u00b7bei", "das", "Tan\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "PAV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch dein Kriechen k\u00fcndet Ruhm:", "tokens": ["Doch", "dein", "Krie\u00b7chen", "k\u00fcn\u00b7det", "Ruhm", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.21": {"line.1": {"text": "Seit dem Jahre achtzehnhundert-", "tokens": ["Seit", "dem", "Jah\u00b7re", "acht\u00b7zehn\u00b7hun\u00b7der\u00b7t"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "TRUNC"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Achtzig stieg von einunddrei\u00dfig", "tokens": ["Acht\u00b7zig", "stieg", "von", "ein\u00b7und\u00b7drei\u00b7\u00dfig"], "token_info": ["word", "word", "word", "word"], "pos": ["CARD", "VVFIN", "APPR", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Teilen unser Kohlenkonsum", "tokens": ["Tei\u00b7len", "un\u00b7ser", "Koh\u00b7len\u00b7kon\u00b7sum"], "token_info": ["word", "word", "word"], "pos": ["NN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bis auf hundert heut. Respekt!", "tokens": ["Bis", "auf", "hun\u00b7dert", "heut", ".", "Res\u00b7pekt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "APPR", "CARD", "ADV", "$.", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Der Verbrauch von Weizen hat sich", "tokens": ["Der", "Ver\u00b7brauch", "von", "Wei\u00b7zen", "hat", "sich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "VAFIN", "PRF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In derselben Zeit verdoppelt.", "tokens": ["In", "der\u00b7sel\u00b7ben", "Zeit", "ver\u00b7dop\u00b7pelt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Apfelsinen i\u00dft man ditto", "tokens": ["Ap\u00b7fel\u00b7si\u00b7nen", "i\u00dft", "man", "dit\u00b7to"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PIS", "ADV"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doppelt mehr als dazumal.", "tokens": ["Dop\u00b7pelt", "mehr", "als", "da\u00b7zu\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "KOKOM", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Und nun gar der Heckepfennig,", "tokens": ["Und", "nun", "gar", "der", "He\u00b7cke\u00b7pfen\u00b7nig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Symbolum des h\u00f6heren Lebens,", "tokens": ["Sym\u00b7bo\u00b7lum", "des", "h\u00f6\u00b7he\u00b7ren", "Le\u00b7bens", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Hat um zweiundachtzig Hundert-", "tokens": ["Hat", "um", "zwei\u00b7un\u00b7dacht\u00b7zig", "Hun\u00b7der\u00b7t"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "CARD", "TRUNC"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Teile l\u00f6blich sich vermehrt.", "tokens": ["Tei\u00b7le", "l\u00f6b\u00b7lich", "sich", "ver\u00b7mehrt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "PRF", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Simson! Simson! Wahr die Haare!", "tokens": ["Sim\u00b7son", "!", "Sim\u00b7son", "!", "Wahr", "die", "Haa\u00b7re", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NE", "$.", "NE", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Delilachen liebt die Glatzen!", "tokens": ["De\u00b7li\u00b7la\u00b7chen", "liebt", "die", "Glat\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Selbst die Haare auf den Z\u00e4hnen", "tokens": ["Selbst", "die", "Haa\u00b7re", "auf", "den", "Z\u00e4h\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "K\u00fc\u00dft sie, f\u00fcrcht ich, dir noch weg.", "tokens": ["K\u00fc\u00dft", "sie", ",", "f\u00fcrcht", "ich", ",", "dir", "noch", "weg", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "VVFIN", "PPER", "$,", "PPER", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Schon hast du das Byzantinern", "tokens": ["Schon", "hast", "du", "das", "By\u00b7zan\u00b7ti\u00b7nern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN"], "meter": "-+--++--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Allzurasch gelernt, schon zieht dein", "tokens": ["All\u00b7zu\u00b7rasch", "ge\u00b7lernt", ",", "schon", "zieht", "dein"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VVPP", "$,", "ADV", "VVFIN", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bauch dich tiefer auf die Erde,", "tokens": ["Bauch", "dich", "tie\u00b7fer", "auf", "die", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Als es Ehrerbietung heischt.", "tokens": ["Als", "es", "Ehr\u00b7er\u00b7bie\u00b7tung", "heischt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Treibe andere Gymnastik,", "tokens": ["Trei\u00b7be", "an\u00b7de\u00b7re", "Gym\u00b7nas\u00b7tik", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Als nach vorn die R\u00fcckenbeuge!", "tokens": ["Als", "nach", "vorn", "die", "R\u00fc\u00b7cken\u00b7beu\u00b7ge", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Steige, Simson, wie du stiegst, als", "tokens": ["Stei\u00b7ge", ",", "Sim\u00b7son", ",", "wie", "du", "stiegst", ",", "als"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["NN", "$,", "NE", "$,", "PWAV", "PPER", "VVFIN", "$,", "KOUS"], "meter": "+--+-++-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Michel Deutsch noch mager war!", "tokens": ["Mi\u00b7chel", "Deutsch", "noch", "ma\u00b7ger", "war", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "Cameriere! Cameriere!", "tokens": ["Ca\u00b7me\u00b7rie\u00b7re", "!", "Ca\u00b7me\u00b7rie\u00b7re", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbsubito!\u00ab \u2013 Pagare! \u2013 \u00bbGrazie!\u00ab", "tokens": ["\u00bb", "su\u00b7bi\u00b7to", "!", "\u00ab", "\u2013", "Pa\u00b7ga\u00b7re", "!", "\u2013", "\u00bb", "Gra\u00b7zie", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "punct", "punct", "punct", "word", "punct", "punct"], "pos": ["$(", "ADV", "$.", "$(", "$(", "NN", "$.", "$(", "$(", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "So. Jetzt geh ich zum Lungarno,", "tokens": ["So", ".", "Jetzt", "geh", "ich", "zum", "Lun\u00b7gar\u00b7no", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ADV", "VVFIN", "PPER", "APPRART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sch\u00f6ne Damen anzusehn.", "tokens": ["Sch\u00f6\u00b7ne", "Da\u00b7men", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVIZU", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Warum nicht? Ich kanns vergn\u00fcglich,", "tokens": ["Wa\u00b7rum", "nicht", "?", "Ich", "kanns", "ver\u00b7gn\u00fcg\u00b7lich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$.", "PPER", "VMFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn ich habe eine sch\u00f6nre.", "tokens": ["Denn", "ich", "ha\u00b7be", "ei\u00b7ne", "sch\u00f6n\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Treue ist f\u00fcr den kein Kunstst\u00fcck,", "tokens": ["Treu\u00b7e", "ist", "f\u00fcr", "den", "kein", "Kunst\u00b7st\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "ART", "PIAT", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Der bei jedem Tausch verliert.", "tokens": ["Der", "bei", "je\u00b7dem", "Tausch", "ver\u00b7liert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Ah, die Gr\u00e4fin Montignoso!", "tokens": ["Ah", ",", "die", "Gr\u00e4\u00b7fin", "Mon\u00b7tig\u00b7no\u00b7so", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Na, so, so. Da: die Geliebte", "tokens": ["Na", ",", "so", ",", "so", ".", "Da", ":", "die", "Ge\u00b7lieb\u00b7te"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["ITJ", "$,", "ADV", "$,", "ADV", "$.", "ADV", "$.", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Des viel sch\u00f6nren Gabriele.", "tokens": ["Des", "viel", "sch\u00f6n\u00b7ren", "Ga\u00b7bri\u00b7e\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "(\u00bbR\u00fcbchen\u00ab hei\u00dft er eigentlich.)", "tokens": ["(", "\u00bb", "R\u00fcb\u00b7chen", "\u00ab", "hei\u00dft", "er", "ei\u00b7gent\u00b7lich", ".", ")"], "token_info": ["punct", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "$(", "NN", "$(", "VVFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.30": {"line.1": {"text": "Nun, nicht \u00fcbel: Rasse, Feuer,", "tokens": ["Nun", ",", "nicht", "\u00fc\u00b7bel", ":", "Ras\u00b7se", ",", "Feu\u00b7er", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "PTKNEG", "ADJD", "$.", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gertenbiegsam, gro\u00dfe Augen,", "tokens": ["Ger\u00b7ten\u00b7bieg\u00b7sam", ",", "gro\u00b7\u00dfe", "Au\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie sie f\u00fcr die weite B\u00fchnen-", "tokens": ["Wie", "sie", "f\u00fcr", "die", "wei\u00b7te", "B\u00fch\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ART", "ADJA", "TRUNC"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Perspektive n\u00fctzlich sind.", "tokens": ["Pers\u00b7pek\u00b7ti\u00b7ve", "n\u00fctz\u00b7lich", "sind", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJD", "VAFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.31": {"line.1": {"text": "Dort: Amerika. Das ist nun", "tokens": ["Dort", ":", "A\u00b7me\u00b7ri\u00b7ka", ".", "Das", "ist", "nun"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$.", "NE", "$.", "PDS", "VAFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mein Fall. Protzt Hygiene.", "tokens": ["Nicht", "mein", "Fall", ".", "Protzt", "Hy\u00b7gi\u00b7e\u00b7ne", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "PPOSAT", "NN", "$.", "FM.la", "FM.la", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Resultat der Speisekarte.", "tokens": ["Re\u00b7sul\u00b7tat", "der", "Spei\u00b7se\u00b7kar\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenig Anmut, viel Effekt.", "tokens": ["We\u00b7nig", "An\u00b7mut", ",", "viel", "Ef\u00b7fekt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "England. Aoh! Noch immer schw\u00e4rmt die", "tokens": ["En\u00b7gland", ".", "A\u00b7oh", "!", "Noch", "im\u00b7mer", "schw\u00e4rmt", "die"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NN", "$.", "ADV", "ADV", "VVFIN", "ART"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mi\u00df f\u00fcr \u00bbihren\u00ab Botticelli.", "tokens": ["Mi\u00df", "f\u00fcr", "\u00bb", "ih\u00b7ren", "\u00ab", "Bot\u00b7ti\u00b7cel\u00b7li", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "APPR", "$(", "PPOSAT", "$(", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Engelhaft und englisch gibt ein", "tokens": ["En\u00b7gel\u00b7haft", "und", "eng\u00b7lisch", "gibt", "ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ADJD", "VVFIN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wunderliches Mischprodukt.", "tokens": ["Wun\u00b7der\u00b7li\u00b7ches", "Mischpro\u00b7dukt", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.33": {"line.1": {"text": "Endlich kommt, der ich schon lange", "tokens": ["End\u00b7lich", "kommt", ",", "der", "ich", "schon", "lan\u00b7ge"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "PRELS", "PPER", "ADV", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aufgelauert habe, kommt die", "tokens": ["Auf\u00b7ge\u00b7lau\u00b7ert", "ha\u00b7be", ",", "kommt", "die"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVPP", "VAFIN", "$,", "VVFIN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Gro\u00dfe Modekurtisane,", "tokens": ["Gro\u00b7\u00dfe", "Mo\u00b7de\u00b7kur\u00b7ti\u00b7sa\u00b7ne", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die Bellezza von Florenz.", "tokens": ["Die", "Bel\u00b7lez\u00b7za", "von", "Flo\u00b7renz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.34": {"line.1": {"text": "La Signora Millelire", "tokens": ["La", "Sig\u00b7no\u00b7ra", "Mil\u00b7le\u00b7li\u00b7re"], "token_info": ["word", "word", "word"], "pos": ["NE", "NE", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hei\u00dft man sie. Des zum Beweise", "tokens": ["Hei\u00dft", "man", "sie", ".", "Des", "zum", "Be\u00b7wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PPER", "$.", "ART", "APPRART", "NN"], "meter": "+-+++-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Tr\u00e4gt sie eine Perlenkette,", "tokens": ["Tr\u00e4gt", "sie", "ei\u00b7ne", "Per\u00b7len\u00b7ket\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die gewi\u00df nicht billig ist.", "tokens": ["Die", "ge\u00b7wi\u00df", "nicht", "bil\u00b7lig", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PTKNEG", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.35": {"line.1": {"text": "Sonst: Ich danke. Blo\u00df Bellezza.", "tokens": ["Sonst", ":", "Ich", "dan\u00b7ke", ".", "Blo\u00df", "Bel\u00b7lez\u00b7za", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VVFIN", "$.", "ADV", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Ansichtskarten-Sch\u00f6nheitstypus;", "tokens": ["An\u00b7sichts\u00b7kar\u00b7ten\u00b7Sch\u00f6n\u00b7heits\u00b7ty\u00b7pus", ";"], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Gut genug f\u00fcr jene Beutel,", "tokens": ["Gut", "ge\u00b7nug", "f\u00fcr", "je\u00b7ne", "Beu\u00b7tel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die voll mille lire sind.", "tokens": ["Die", "voll", "mil\u00b7le", "li\u00b7re", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "Aber nun: Oh teure Heimat!", "tokens": ["A\u00b7ber", "nun", ":", "Oh", "teu\u00b7re", "Hei\u00b7mat", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "FM", "FM", "FM", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommt da nicht das s\u00fc\u00dfe Gretchen,", "tokens": ["Kommt", "da", "nicht", "das", "s\u00fc\u00b7\u00dfe", "Gret\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das, weils seinen Hans gefunden,", "tokens": ["Das", ",", "weils", "sei\u00b7nen", "Hans", "ge\u00b7fun\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "KOUS", "PPOSAT", "NE", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schleunigst nach Florenz gemu\u00dft?", "tokens": ["Schleu\u00b7nigst", "nach", "Flo\u00b7renz", "ge\u00b7mu\u00dft", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "VVPP", "$."], "meter": "+-----+", "measure": "dactylic.init"}}, "stanza.37": {"line.1": {"text": "Ja, sie kommt, und ja, sie l\u00e4chelt,", "tokens": ["Ja", ",", "sie", "kommt", ",", "und", "ja", ",", "sie", "l\u00e4\u00b7chelt", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "$,", "KON", "PTKANT", "$,", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ja, sie ist ganz hin vor Selig-", "tokens": ["Ja", ",", "sie", "ist", "ganz", "hin", "vor", "Se\u00b7lig"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VAFIN", "ADV", "ADV", "APPR", "TRUNC"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Keit und gro\u00dfem Gl\u00fccke, weil sie", "tokens": ["Keit", "und", "gro\u00b7\u00dfem", "Gl\u00fc\u00b7cke", ",", "weil", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN", "$,", "KOUS", "PPER"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wirklich in Italien ist.", "tokens": ["Wirk\u00b7lich", "in", "I\u00b7ta\u00b7li\u00b7en", "ist", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "VAFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.38": {"line.1": {"text": "Spotte nicht, verruchter Knabe!", "tokens": ["Spot\u00b7te", "nicht", ",", "ver\u00b7ruch\u00b7ter", "Kna\u00b7be", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "La\u00df ihr auch das jugendstilig", "tokens": ["La\u00df", "ihr", "auch", "das", "ju\u00b7gend\u00b7sti\u00b7lig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "ADV", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "K\u00fcnstlerich empfundne, aber", "tokens": ["K\u00fcnst\u00b7le\u00b7rich", "emp\u00b7fund\u00b7ne", ",", "a\u00b7ber"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADJD", "ADJA", "$,", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Praktische Reformkost\u00fcm.", "tokens": ["Prak\u00b7ti\u00b7sche", "Re\u00b7form\u00b7kos\u00b7t\u00fcm", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.39": {"line.1": {"text": "Ist sie trotzdem nicht recht niedlich?", "tokens": ["Ist", "sie", "trotz\u00b7dem", "nicht", "recht", "nied\u00b7lich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PAV", "PTKNEG", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Frage dich: wie viele solche", "tokens": ["Fra\u00b7ge", "dich", ":", "wie", "vie\u00b7le", "sol\u00b7che"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "PPER", "$.", "PWAV", "PIS", "PIAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fcndchen, \u00c4ugelchen und N\u00e4schen", "tokens": ["M\u00fcnd\u00b7chen", ",", "\u00c4u\u00b7ge\u00b7lchen", "und", "N\u00e4\u00b7schen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Haben ehmals dich entflammt?", "tokens": ["Ha\u00b7ben", "eh\u00b7mals", "dich", "ent\u00b7flammt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.40": {"line.1": {"text": "Au\u00dferdem: \u00bbFr\u00fchlings Erwachen\u00ab", "tokens": ["Au\u00b7\u00dfer\u00b7dem", ":", "\u00bb", "Fr\u00fch\u00b7lings", "Er\u00b7wa\u00b7chen", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "punct"], "pos": ["PAV", "$.", "$(", "NN", "NN", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Hat auch diese tief begriffen,", "tokens": ["Hat", "auch", "die\u00b7se", "tief", "be\u00b7grif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PDAT", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie ist durchaus kein Gretchen", "tokens": ["Und", "sie", "ist", "durc\u00b7haus", "kein", "Gret\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie das alte Gretchen mehr.", "tokens": ["Wie", "das", "al\u00b7te", "Gret\u00b7chen", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.41": {"line.1": {"text": "Neue Jugend! \u2013 \u00bbJugend\u00ab! Pr\u00e4ge", "tokens": ["Neu\u00b7e", "Ju\u00b7gend", "!", "\u2013", "\u00bb", "Ju\u00b7gend", "\u00ab", "!", "Pr\u00e4\u00b7ge"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "punct", "punct", "word"], "pos": ["ADJA", "NN", "$.", "$(", "$(", "NN", "$(", "$.", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tief es dir in dein Gem\u00fcte:", "tokens": ["Tief", "es", "dir", "in", "dein", "Ge\u00b7m\u00fc\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der alten \u00bbGartenlaube\u00ab", "tokens": ["Von", "der", "al\u00b7ten", "\u00bb", "Gar\u00b7ten\u00b7lau\u00b7be", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "$(", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind wir absolut befreit.", "tokens": ["Sind", "wir", "ab\u00b7so\u00b7lut", "be\u00b7freit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.42": {"line.1": {"text": "Auf, und greife in die Harfe!", "tokens": ["Auf", ",", "und", "grei\u00b7fe", "in", "die", "Har\u00b7fe", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unser Gretchen ist verwandelt,", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "ver\u00b7wan\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unser Gretchen ist \u00e4sthetisch,", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "\u00e4s\u00b7the\u00b7tisch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Unser Gretchen ist modern.", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "mo\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.43": {"line.1": {"text": "Sieh, sie geht in einen Laden,", "tokens": ["Sieh", ",", "sie", "geht", "in", "ei\u00b7nen", "La\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo man sch\u00f6ne Marmorsachen", "tokens": ["Wo", "man", "sch\u00f6\u00b7ne", "Mar\u00b7mor\u00b7sa\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Billig kauft. Nun: was erstand sie?", "tokens": ["Bil\u00b7lig", "kauft", ".", "Nun", ":", "was", "er\u00b7stand", "sie", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$.", "ADV", "$.", "PWS", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ha! Ein nacktes Frauenbild!", "tokens": ["Ha", "!", "Ein", "nack\u00b7tes", "Frau\u00b7en\u00b7bild", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.44": {"line.1": {"text": "Schlag die Harfe! Schlag die Harfe!", "tokens": ["Schlag", "die", "Har\u00b7fe", "!", "Schlag", "die", "Har\u00b7fe", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$.", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn Germania ist gerettet.", "tokens": ["Denn", "Ger\u00b7ma\u00b7nia", "ist", "ge\u00b7ret\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zwar: sie kaufte einen Kitsch, doch,", "tokens": ["Zwar", ":", "sie", "kauf\u00b7te", "ei\u00b7nen", "Kitsch", ",", "doch", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VVFIN", "ART", "NN", "$,", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Heil, es war ein nackter Kitsch!", "tokens": ["Heil", ",", "es", "war", "ein", "nack\u00b7ter", "Kitsch", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.45": {"line.1": {"text": "Vetturino! \u00bbSissignore\u00ab.", "tokens": ["Vet\u00b7tu\u00b7ri\u00b7no", "!", "\u00bb", "Sis\u00b7sig\u00b7no\u00b7re", "\u00ab", "."], "token_info": ["word", "punct", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "$(", "NE", "$(", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nach Fiesole! \u2013 Die G\u00e4ulchen", "tokens": ["Nach", "Fie\u00b7so\u00b7le", "!", "\u2013", "Die", "G\u00e4ul\u00b7chen"], "token_info": ["word", "word", "punct", "punct", "word", "word"], "pos": ["APPR", "NN", "$.", "$(", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Brauchen Gott sei Dank zwei Stunden,", "tokens": ["Brau\u00b7chen", "Gott", "sei", "Dank", "zwei", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "APPR", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bis ich wieder oben bin.", "tokens": ["Bis", "ich", "wie\u00b7der", "o\u00b7ben", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.46": {"line.1": {"text": "Denn es ist ein sch\u00f6nes Fahren,", "tokens": ["Denn", "es", "ist", "ein", "sch\u00f6\u00b7nes", "Fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Langsam, langsam, bis zur H\u00f6he.", "tokens": ["Lang\u00b7sam", ",", "lang\u00b7sam", ",", "bis", "zur", "H\u00f6\u00b7he", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "KOUS", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unten liegt, wie eine Muschel,", "tokens": ["Un\u00b7ten", "liegt", ",", "wie", "ei\u00b7ne", "Mu\u00b7schel", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PWAV", "ART", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Rosafleischig \u00fcberhaucht,", "tokens": ["Ro\u00b7sa\u00b7flei\u00b7schig", "\u00fc\u00b7ber\u00b7haucht", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.47": {"line.1": {"text": "Traumhaft, wesenlos, ein sanftes,", "tokens": ["Traum\u00b7haft", ",", "we\u00b7sen\u00b7los", ",", "ein", "sanf\u00b7tes", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zages Blinken, liegt phantomisch", "tokens": ["Za\u00b7ges", "Blin\u00b7ken", ",", "liegt", "phan\u00b7to\u00b7misch"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "VVFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Diese Stadt der alten, edlen", "tokens": ["Die\u00b7se", "Stadt", "der", "al\u00b7ten", ",", "ed\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PDAT", "NN", "ART", "ADJA", "$,", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Phrasenfeindlichen Kultur.", "tokens": ["Phra\u00b7sen\u00b7feind\u00b7li\u00b7chen", "Kul\u00b7tur", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.48": {"line.1": {"text": "T\u00e4glich fahr ich mit Pietro,", "tokens": ["T\u00e4g\u00b7lich", "fahr", "ich", "mit", "Pie\u00b7tro", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Meinem wohlbeleibten Kutscher", "tokens": ["Mei\u00b7nem", "wohl\u00b7be\u00b7leib\u00b7ten", "Kut\u00b7scher"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "(und mit seinem Pferdchen Palle,", "tokens": ["(", "und", "mit", "sei\u00b7nem", "Pferd\u00b7chen", "Pal\u00b7le", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welches auch nicht mager ist),", "tokens": ["Wel\u00b7ches", "auch", "nicht", "ma\u00b7ger", "ist", ")", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "ADV", "PTKNEG", "ADJD", "VAFIN", "$(", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.49": {"line.1": {"text": "T\u00e4glich nachmittags um dreie", "tokens": ["T\u00e4g\u00b7lich", "nach\u00b7mit\u00b7tags", "um", "drei\u00b7e"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "ADV", "APPR", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fahr ich auf der alten Stra\u00dfe,", "tokens": ["Fahr", "ich", "auf", "der", "al\u00b7ten", "Stra\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die sehr steil ist und sehr holprig,", "tokens": ["Die", "sehr", "steil", "ist", "und", "sehr", "holp\u00b7rig", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "KON", "ADV", "ADJD", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Erst nach San Domenico", "tokens": ["Erst", "nach", "San", "Do\u00b7me\u00b7ni\u00b7co"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "NE"], "meter": "+---+--", "measure": "dactylic.init"}}, "stanza.50": {"line.1": {"text": "Und sodann, vorbei der Villa,", "tokens": ["Und", "so\u00b7dann", ",", "vor\u00b7bei", "der", "Vil\u00b7la", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "ART", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Wo Herr Dante einst verliebt war,", "tokens": ["Wo", "Herr", "Dan\u00b7te", "einst", "ver\u00b7liebt", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NE", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zwischen hohen Gartenmauern", "tokens": ["Zwi\u00b7schen", "ho\u00b7hen", "Gar\u00b7ten\u00b7mau\u00b7ern"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach Florenz. Dort trink ich Tee.", "tokens": ["Nach", "Flo\u00b7renz", ".", "Dort", "trink", "ich", "Tee", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$.", "ADV", "VVFIN", "PPER", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.51": {"line.1": {"text": "\u00bbwie? Und der Palazzo Pitti?", "tokens": ["\u00bb", "wie", "?", "Und", "der", "Pa\u00b7laz\u00b7zo", "Pit\u00b7ti", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "$.", "KON", "ART", "NN", "NE", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Accademia? Uffizien?", "tokens": ["Ac\u00b7ca\u00b7de\u00b7mia", "?", "Uf\u00b7fi\u00b7zi\u00b7en", "?"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bibliotheca Laurenziana?", "tokens": ["Bib\u00b7liot\u00b7he\u00b7ca", "Lau\u00b7ren\u00b7zi\u00b7a\u00b7na", "?"], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hast du nicht nach Sch\u00f6nheit Durst?\u00ab", "tokens": ["Hast", "du", "nicht", "nach", "Sch\u00f6n\u00b7heit", "Durst", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "APPR", "NN", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.52": {"line.1": {"text": "Oh ja. Aber f\u00fcr Museen", "tokens": ["Oh", "ja", ".", "A\u00b7ber", "f\u00fcr", "Mu\u00b7se\u00b7en"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "ADV", "$.", "KON", "APPR", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Bin ich selten nur in Stimmung;", "tokens": ["Bin", "ich", "sel\u00b7ten", "nur", "in", "Stim\u00b7mung", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn es sind Konservenb\u00fcchsen;", "tokens": ["Denn", "es", "sind", "Kon\u00b7ser\u00b7ven\u00b7b\u00fcch\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihre Sch\u00f6nheit schmeckt nach Blech.", "tokens": ["Ih\u00b7re", "Sch\u00f6n\u00b7heit", "schmeckt", "nach", "Blech", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.53": {"line.1": {"text": "\u00bbwie? Die himmlische Tribuna?", "tokens": ["\u00bb", "wie", "?", "Die", "himm\u00b7li\u00b7sche", "Tri\u00b7bu\u00b7na", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "$.", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Alessandro Botticelli?", "tokens": ["A\u00b7les\u00b7sand\u00b7ro", "Bot\u00b7ti\u00b7cel\u00b7li", "?"], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Cimabue? Donatello?\u00ab", "tokens": ["Ci\u00b7ma\u00b7bue", "?", "Do\u00b7na\u00b7tel\u00b7lo", "?", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "NE", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alle schmecken dort nach Blech.", "tokens": ["Al\u00b7le", "schme\u00b7cken", "dort", "nach", "Blech", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.54": {"line.1": {"text": "Lieber wandre ich durch dunkle", "tokens": ["Lie\u00b7ber", "wand\u00b7re", "ich", "durch", "dunk\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kirchen mit dem Operngucker", "tokens": ["Kir\u00b7chen", "mit", "dem", "O\u00b7pern\u00b7gu\u00b7cker"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und verrenke Hals und Kopf mir", "tokens": ["Und", "ver\u00b7ren\u00b7ke", "Hals", "und", "Kopf", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "KON", "NN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach der dort verstecken Kunst.", "tokens": ["Nach", "der", "dort", "ver\u00b7ste\u00b7cken", "Kunst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.55": {"line.1": {"text": "Da nur wirkt sie noch ins Leben,", "tokens": ["Da", "nur", "wirkt", "sie", "noch", "ins", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Thront sie noch auf ihrem Throne,", "tokens": ["Thront", "sie", "noch", "auf", "ih\u00b7rem", "Thro\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frei, gebietend, nicht gefangen:", "tokens": ["Frei", ",", "ge\u00b7bie\u00b7tend", ",", "nicht", "ge\u00b7fan\u00b7gen", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,", "PTKNEG", "PTKVZ", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Atmet aus und atmet ein.", "tokens": ["At\u00b7met", "aus", "und", "at\u00b7met", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "KON", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.56": {"line.1": {"text": "Denn ein Kunstwerk braucht den Atem,", "tokens": ["Denn", "ein", "Kunst\u00b7werk", "braucht", "den", "A\u00b7tem", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Braucht die Luft des t\u00e4tigen Lebens;", "tokens": ["Braucht", "die", "Luft", "des", "t\u00e4\u00b7ti\u00b7gen", "Le\u00b7bens", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Seine Sch\u00f6nheit wird zum Schemen,", "tokens": ["Sei\u00b7ne", "Sch\u00f6n\u00b7heit", "wird", "zum", "Sche\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sperrt man sie vom Leben ab.", "tokens": ["Sperrt", "man", "sie", "vom", "Le\u00b7ben", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "APPRART", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.57": {"line.1": {"text": "St\u00fcnde David noch im Freien,", "tokens": ["St\u00fcn\u00b7de", "Da\u00b7vid", "noch", "im", "Frei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dort, wohin ihn schuf sein Sch\u00f6pfer,", "tokens": ["Dort", ",", "wo\u00b7hin", "ihn", "schuf", "sein", "Sch\u00f6p\u00b7fer", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wohl, er w\u00e4re nicht so gl\u00e4nzend", "tokens": ["Wohl", ",", "er", "w\u00e4\u00b7re", "nicht", "so", "gl\u00e4n\u00b7zend"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PPER", "VAFIN", "PTKNEG", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wei\u00df wie jetzt und \u00bbfast wie neu\u00ab,", "tokens": ["Wei\u00df", "wie", "jetzt", "und", "\u00bb", "fast", "wie", "neu", "\u00ab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "KOKOM", "ADV", "KON", "$(", "ADV", "KOKOM", "ADJD", "$(", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.58": {"line.1": {"text": "Aber, grau vielleicht und rissig,", "tokens": ["A\u00b7ber", ",", "grau", "viel\u00b7leicht", "und", "ris\u00b7sig", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADJD", "ADV", "KON", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mitgenommen von Frost und Feuchte,", "tokens": ["Mit\u00b7ge\u00b7nom\u00b7men", "von", "Frost", "und", "Feuch\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Leidend, wie das Leben immer", "tokens": ["Lei\u00b7dend", ",", "wie", "das", "Le\u00b7ben", "im\u00b7mer"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "$,", "PWAV", "ART", "NN", "ADV"], "meter": "+-+-+-++", "measure": "unknown.measure.penta"}, "line.4": {"text": "Leiden mu\u00df, um ", "tokens": ["Lei\u00b7den", "mu\u00df", ",", "um"], "token_info": ["word", "word", "punct", "word"], "pos": ["NN", "VMFIN", "$,", "KOUI"], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.59": {"line.1": {"text": "St\u00fcnd er heldenhaft lebendig,", "tokens": ["St\u00fcnd", "er", "hel\u00b7den\u00b7haft", "le\u00b7ben\u00b7dig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sterbend st\u00fcnd er noch lebendiger,", "tokens": ["Ster\u00b7bend", "st\u00fcnd", "er", "noch", "le\u00b7ben\u00b7di\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "ADJA", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Herrlicher, strahlender da, als jetzt im", "tokens": ["Herr\u00b7li\u00b7cher", ",", "strah\u00b7len\u00b7der", "da", ",", "als", "jetzt", "im"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "ADV", "ADV", "$,", "KOUS", "ADV", "APPRART"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Abgemessenen Oberlicht.", "tokens": ["Ab\u00b7ge\u00b7mes\u00b7se\u00b7nen", "O\u00b7ber\u00b7licht", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.60": {"line.1": {"text": "\u00bbund verd\u00fcrbe.\u00ab Freilich. Alles", "tokens": ["\u00bb", "und", "ver\u00b7d\u00fcr\u00b7be", ".", "\u00ab", "Frei\u00b7lich", ".", "Al\u00b7les"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "punct", "word"], "pos": ["$(", "KON", "VVFIN", "$.", "$(", "ADV", "$.", "PIS"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Leben mu\u00df einmal verderben.", "tokens": ["Le\u00b7ben", "mu\u00df", "ein\u00b7mal", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber leben soll es, leben:", "tokens": ["A\u00b7ber", "le\u00b7ben", "soll", "es", ",", "le\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wirklich leben, bis es stirbt.", "tokens": ["Wirk\u00b7lich", "le\u00b7ben", ",", "bis", "es", "stirbt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Denkt nicht immer an die Enkel!", "tokens": ["Denkt", "nicht", "im\u00b7mer", "an", "die", "En\u00b7kel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denkt an euch, wie jene taten,", "tokens": ["Denkt", "an", "euch", ",", "wie", "je\u00b7ne", "ta\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPER", "$,", "PWAV", "PDS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die ihr Leben sich verkl\u00e4rten,", "tokens": ["Die", "ihr", "Le\u00b7ben", "sich", "ver\u00b7kl\u00e4r\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bildner ihrer Gegenwart.", "tokens": ["Bild\u00b7ner", "ih\u00b7rer", "Ge\u00b7gen\u00b7wart", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.62": {"line.1": {"text": "Dann erst h\u00e4ttet ihr ein Recht, sie", "tokens": ["Dann", "erst", "h\u00e4t\u00b7tet", "ihr", "ein", "Recht", ",", "sie"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ART", "NN", "$,", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In die heiligen Leichenkammern", "tokens": ["In", "die", "hei\u00b7li\u00b7gen", "Lei\u00b7chen\u00b7kam\u00b7mern"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Eurer Piet\u00e4t zu stecken,", "tokens": ["Eu\u00b7rer", "Pi\u00b7e\u00b7t\u00e4t", "zu", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Brauchtet ihr f\u00fcr Eignes Platz.", "tokens": ["Brauch\u00b7tet", "ihr", "f\u00fcr", "Eig\u00b7nes", "Platz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.63": {"line.1": {"text": "Doch genug. Ich geh zu Gilli,", "tokens": ["Doch", "ge\u00b7nug", ".", "Ich", "geh", "zu", "Gil\u00b7li", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "PPER", "VVFIN", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Trinke Tee und esse Kuchen.", "tokens": ["Trin\u00b7ke", "Tee", "und", "es\u00b7se", "Ku\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Leider bin ich manchmal schwach und", "tokens": ["Lei\u00b7der", "bin", "ich", "manch\u00b7mal", "schwach", "und"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "KON"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Lese Zeitungen dazu.", "tokens": ["Le\u00b7se", "Zei\u00b7tun\u00b7gen", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.64": {"line.1": {"text": "Heiliger Marsyas! Noch immer,", "tokens": ["Hei\u00b7li\u00b7ger", "Mar\u00b7syas", "!", "Noch", "im\u00b7mer", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADV", "ADV", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Simson Deutschland, sind Philister,", "tokens": ["Sim\u00b7son", "Deutschland", ",", "sind", "Phi\u00b7lis\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "VAFIN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ach, und was f\u00fcr eine Sorte", "tokens": ["Ach", ",", "und", "was", "f\u00fcr", "ei\u00b7ne", "Sor\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "KON", "PWS", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "(frech und bieder), \u00fcber dir.", "tokens": ["(", "frech", "und", "bie\u00b7der", ")", ",", "\u00fc\u00b7ber", "dir", "."], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "ADJD", "KON", "ADJD", "$(", "$,", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.65": {"line.1": {"text": "Deine Delila hei\u00dft Wohlstand.", "tokens": ["Dei\u00b7ne", "De\u00b7li\u00b7la", "hei\u00dft", "Wohl\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "VVFIN", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "\u00dcppigst hast du zugenommen.", "tokens": ["\u00dcp\u00b7pigst", "hast", "du", "zu\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wohl bekommt dein Fett dem Bauche,", "tokens": ["Wohl", "be\u00b7kommt", "dein", "Fett", "dem", "Bau\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch dem Hirn bekommt es schlecht.", "tokens": ["Doch", "dem", "Hirn", "be\u00b7kommt", "es", "schlecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.66": {"line.1": {"text": "Und der Seele, ach, der edlen", "tokens": ["Und", "der", "See\u00b7le", ",", "ach", ",", "der", "ed\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ART", "NN", "$,", "ITJ", "$,", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deutschen Seele fehlts am Raume,", "tokens": ["Deut\u00b7schen", "See\u00b7le", "fehlts", "am", "Rau\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Scheint es, in dem kolossalen", "tokens": ["Scheint", "es", ",", "in", "dem", "ko\u00b7los\u00b7sa\u00b7len"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Korpus, der ganz Masse ist.", "tokens": ["Kor\u00b7pus", ",", "der", "ganz", "Mas\u00b7se", "ist", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PRELS", "ADV", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.67": {"line.1": {"text": "Bocke, bocke nicht, Troch\u00e4us!", "tokens": ["Bo\u00b7cke", ",", "bo\u00b7cke", "nicht", ",", "Troc\u00b7h\u00e4us", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PTKNEG", "$,", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Jetzo mu\u00dft du Zahlen buckeln.", "tokens": ["Jet\u00b7zo", "mu\u00dft", "du", "Zah\u00b7len", "bu\u00b7ckeln", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schwer f\u00e4llt wohl dabei das Tanzen,", "tokens": ["Schwer", "f\u00e4llt", "wohl", "da\u00b7bei", "das", "Tan\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ADV", "PAV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch dein Kriechen k\u00fcndet Ruhm:", "tokens": ["Doch", "dein", "Krie\u00b7chen", "k\u00fcn\u00b7det", "Ruhm", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.68": {"line.1": {"text": "Seit dem Jahre achtzehnhundert-", "tokens": ["Seit", "dem", "Jah\u00b7re", "acht\u00b7zehn\u00b7hun\u00b7der\u00b7t"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "TRUNC"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Achtzig stieg von einunddrei\u00dfig", "tokens": ["Acht\u00b7zig", "stieg", "von", "ein\u00b7und\u00b7drei\u00b7\u00dfig"], "token_info": ["word", "word", "word", "word"], "pos": ["CARD", "VVFIN", "APPR", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Teilen unser Kohlenkonsum", "tokens": ["Tei\u00b7len", "un\u00b7ser", "Koh\u00b7len\u00b7kon\u00b7sum"], "token_info": ["word", "word", "word"], "pos": ["NN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bis auf hundert heut. Respekt!", "tokens": ["Bis", "auf", "hun\u00b7dert", "heut", ".", "Res\u00b7pekt", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "APPR", "CARD", "ADV", "$.", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.69": {"line.1": {"text": "Der Verbrauch von Weizen hat sich", "tokens": ["Der", "Ver\u00b7brauch", "von", "Wei\u00b7zen", "hat", "sich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "VAFIN", "PRF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In derselben Zeit verdoppelt.", "tokens": ["In", "der\u00b7sel\u00b7ben", "Zeit", "ver\u00b7dop\u00b7pelt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Apfelsinen i\u00dft man ditto", "tokens": ["Ap\u00b7fel\u00b7si\u00b7nen", "i\u00dft", "man", "dit\u00b7to"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PIS", "ADV"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doppelt mehr als dazumal.", "tokens": ["Dop\u00b7pelt", "mehr", "als", "da\u00b7zu\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "KOKOM", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.70": {"line.1": {"text": "Und nun gar der Heckepfennig,", "tokens": ["Und", "nun", "gar", "der", "He\u00b7cke\u00b7pfen\u00b7nig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Symbolum des h\u00f6heren Lebens,", "tokens": ["Sym\u00b7bo\u00b7lum", "des", "h\u00f6\u00b7he\u00b7ren", "Le\u00b7bens", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Hat um zweiundachtzig Hundert-", "tokens": ["Hat", "um", "zwei\u00b7un\u00b7dacht\u00b7zig", "Hun\u00b7der\u00b7t"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "CARD", "TRUNC"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Teile l\u00f6blich sich vermehrt.", "tokens": ["Tei\u00b7le", "l\u00f6b\u00b7lich", "sich", "ver\u00b7mehrt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "PRF", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.71": {"line.1": {"text": "Simson! Simson! Wahr die Haare!", "tokens": ["Sim\u00b7son", "!", "Sim\u00b7son", "!", "Wahr", "die", "Haa\u00b7re", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NE", "$.", "NE", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Delilachen liebt die Glatzen!", "tokens": ["De\u00b7li\u00b7la\u00b7chen", "liebt", "die", "Glat\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Selbst die Haare auf den Z\u00e4hnen", "tokens": ["Selbst", "die", "Haa\u00b7re", "auf", "den", "Z\u00e4h\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "K\u00fc\u00dft sie, f\u00fcrcht ich, dir noch weg.", "tokens": ["K\u00fc\u00dft", "sie", ",", "f\u00fcrcht", "ich", ",", "dir", "noch", "weg", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "VVFIN", "PPER", "$,", "PPER", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.72": {"line.1": {"text": "Schon hast du das Byzantinern", "tokens": ["Schon", "hast", "du", "das", "By\u00b7zan\u00b7ti\u00b7nern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN"], "meter": "-+--++--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Allzurasch gelernt, schon zieht dein", "tokens": ["All\u00b7zu\u00b7rasch", "ge\u00b7lernt", ",", "schon", "zieht", "dein"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VVPP", "$,", "ADV", "VVFIN", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bauch dich tiefer auf die Erde,", "tokens": ["Bauch", "dich", "tie\u00b7fer", "auf", "die", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Als es Ehrerbietung heischt.", "tokens": ["Als", "es", "Ehr\u00b7er\u00b7bie\u00b7tung", "heischt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.73": {"line.1": {"text": "Treibe andere Gymnastik,", "tokens": ["Trei\u00b7be", "an\u00b7de\u00b7re", "Gym\u00b7nas\u00b7tik", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Als nach vorn die R\u00fcckenbeuge!", "tokens": ["Als", "nach", "vorn", "die", "R\u00fc\u00b7cken\u00b7beu\u00b7ge", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Steige, Simson, wie du stiegst, als", "tokens": ["Stei\u00b7ge", ",", "Sim\u00b7son", ",", "wie", "du", "stiegst", ",", "als"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["NN", "$,", "NE", "$,", "PWAV", "PPER", "VVFIN", "$,", "KOUS"], "meter": "+--+-++-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Michel Deutsch noch mager war!", "tokens": ["Mi\u00b7chel", "Deutsch", "noch", "ma\u00b7ger", "war", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.74": {"line.1": {"text": "Cameriere! Cameriere!", "tokens": ["Ca\u00b7me\u00b7rie\u00b7re", "!", "Ca\u00b7me\u00b7rie\u00b7re", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$.", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbsubito!\u00ab \u2013 Pagare! \u2013 \u00bbGrazie!\u00ab", "tokens": ["\u00bb", "su\u00b7bi\u00b7to", "!", "\u00ab", "\u2013", "Pa\u00b7ga\u00b7re", "!", "\u2013", "\u00bb", "Gra\u00b7zie", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "punct", "punct", "punct", "word", "punct", "punct"], "pos": ["$(", "ADV", "$.", "$(", "$(", "NN", "$.", "$(", "$(", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "So. Jetzt geh ich zum Lungarno,", "tokens": ["So", ".", "Jetzt", "geh", "ich", "zum", "Lun\u00b7gar\u00b7no", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ADV", "VVFIN", "PPER", "APPRART", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sch\u00f6ne Damen anzusehn.", "tokens": ["Sch\u00f6\u00b7ne", "Da\u00b7men", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVIZU", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.75": {"line.1": {"text": "Warum nicht? Ich kanns vergn\u00fcglich,", "tokens": ["Wa\u00b7rum", "nicht", "?", "Ich", "kanns", "ver\u00b7gn\u00fcg\u00b7lich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$.", "PPER", "VMFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn ich habe eine sch\u00f6nre.", "tokens": ["Denn", "ich", "ha\u00b7be", "ei\u00b7ne", "sch\u00f6n\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Treue ist f\u00fcr den kein Kunstst\u00fcck,", "tokens": ["Treu\u00b7e", "ist", "f\u00fcr", "den", "kein", "Kunst\u00b7st\u00fcck", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "ART", "PIAT", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Der bei jedem Tausch verliert.", "tokens": ["Der", "bei", "je\u00b7dem", "Tausch", "ver\u00b7liert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.76": {"line.1": {"text": "Ah, die Gr\u00e4fin Montignoso!", "tokens": ["Ah", ",", "die", "Gr\u00e4\u00b7fin", "Mon\u00b7tig\u00b7no\u00b7so", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Na, so, so. Da: die Geliebte", "tokens": ["Na", ",", "so", ",", "so", ".", "Da", ":", "die", "Ge\u00b7lieb\u00b7te"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["ITJ", "$,", "ADV", "$,", "ADV", "$.", "ADV", "$.", "ART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Des viel sch\u00f6nren Gabriele.", "tokens": ["Des", "viel", "sch\u00f6n\u00b7ren", "Ga\u00b7bri\u00b7e\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "(\u00bbR\u00fcbchen\u00ab hei\u00dft er eigentlich.)", "tokens": ["(", "\u00bb", "R\u00fcb\u00b7chen", "\u00ab", "hei\u00dft", "er", "ei\u00b7gent\u00b7lich", ".", ")"], "token_info": ["punct", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "$(", "NN", "$(", "VVFIN", "PPER", "ADV", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.77": {"line.1": {"text": "Nun, nicht \u00fcbel: Rasse, Feuer,", "tokens": ["Nun", ",", "nicht", "\u00fc\u00b7bel", ":", "Ras\u00b7se", ",", "Feu\u00b7er", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "PTKNEG", "ADJD", "$.", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gertenbiegsam, gro\u00dfe Augen,", "tokens": ["Ger\u00b7ten\u00b7bieg\u00b7sam", ",", "gro\u00b7\u00dfe", "Au\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie sie f\u00fcr die weite B\u00fchnen-", "tokens": ["Wie", "sie", "f\u00fcr", "die", "wei\u00b7te", "B\u00fch\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ART", "ADJA", "TRUNC"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Perspektive n\u00fctzlich sind.", "tokens": ["Pers\u00b7pek\u00b7ti\u00b7ve", "n\u00fctz\u00b7lich", "sind", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJD", "VAFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.78": {"line.1": {"text": "Dort: Amerika. Das ist nun", "tokens": ["Dort", ":", "A\u00b7me\u00b7ri\u00b7ka", ".", "Das", "ist", "nun"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$.", "NE", "$.", "PDS", "VAFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht mein Fall. Protzt Hygiene.", "tokens": ["Nicht", "mein", "Fall", ".", "Protzt", "Hy\u00b7gi\u00b7e\u00b7ne", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "PPOSAT", "NN", "$.", "FM.la", "FM.la", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Resultat der Speisekarte.", "tokens": ["Re\u00b7sul\u00b7tat", "der", "Spei\u00b7se\u00b7kar\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenig Anmut, viel Effekt.", "tokens": ["We\u00b7nig", "An\u00b7mut", ",", "viel", "Ef\u00b7fekt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.79": {"line.1": {"text": "England. Aoh! Noch immer schw\u00e4rmt die", "tokens": ["En\u00b7gland", ".", "A\u00b7oh", "!", "Noch", "im\u00b7mer", "schw\u00e4rmt", "die"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NN", "$.", "ADV", "ADV", "VVFIN", "ART"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mi\u00df f\u00fcr \u00bbihren\u00ab Botticelli.", "tokens": ["Mi\u00df", "f\u00fcr", "\u00bb", "ih\u00b7ren", "\u00ab", "Bot\u00b7ti\u00b7cel\u00b7li", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "APPR", "$(", "PPOSAT", "$(", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Engelhaft und englisch gibt ein", "tokens": ["En\u00b7gel\u00b7haft", "und", "eng\u00b7lisch", "gibt", "ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ADJD", "VVFIN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wunderliches Mischprodukt.", "tokens": ["Wun\u00b7der\u00b7li\u00b7ches", "Mischpro\u00b7dukt", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.80": {"line.1": {"text": "Endlich kommt, der ich schon lange", "tokens": ["End\u00b7lich", "kommt", ",", "der", "ich", "schon", "lan\u00b7ge"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "PRELS", "PPER", "ADV", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aufgelauert habe, kommt die", "tokens": ["Auf\u00b7ge\u00b7lau\u00b7ert", "ha\u00b7be", ",", "kommt", "die"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVPP", "VAFIN", "$,", "VVFIN", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Gro\u00dfe Modekurtisane,", "tokens": ["Gro\u00b7\u00dfe", "Mo\u00b7de\u00b7kur\u00b7ti\u00b7sa\u00b7ne", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die Bellezza von Florenz.", "tokens": ["Die", "Bel\u00b7lez\u00b7za", "von", "Flo\u00b7renz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.81": {"line.1": {"text": "La Signora Millelire", "tokens": ["La", "Sig\u00b7no\u00b7ra", "Mil\u00b7le\u00b7li\u00b7re"], "token_info": ["word", "word", "word"], "pos": ["NE", "NE", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hei\u00dft man sie. Des zum Beweise", "tokens": ["Hei\u00dft", "man", "sie", ".", "Des", "zum", "Be\u00b7wei\u00b7se"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PPER", "$.", "ART", "APPRART", "NN"], "meter": "+-+++-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Tr\u00e4gt sie eine Perlenkette,", "tokens": ["Tr\u00e4gt", "sie", "ei\u00b7ne", "Per\u00b7len\u00b7ket\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die gewi\u00df nicht billig ist.", "tokens": ["Die", "ge\u00b7wi\u00df", "nicht", "bil\u00b7lig", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PTKNEG", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.82": {"line.1": {"text": "Sonst: Ich danke. Blo\u00df Bellezza.", "tokens": ["Sonst", ":", "Ich", "dan\u00b7ke", ".", "Blo\u00df", "Bel\u00b7lez\u00b7za", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VVFIN", "$.", "ADV", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Ansichtskarten-Sch\u00f6nheitstypus;", "tokens": ["An\u00b7sichts\u00b7kar\u00b7ten\u00b7Sch\u00f6n\u00b7heits\u00b7ty\u00b7pus", ";"], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Gut genug f\u00fcr jene Beutel,", "tokens": ["Gut", "ge\u00b7nug", "f\u00fcr", "je\u00b7ne", "Beu\u00b7tel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die voll mille lire sind.", "tokens": ["Die", "voll", "mil\u00b7le", "li\u00b7re", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.83": {"line.1": {"text": "Aber nun: Oh teure Heimat!", "tokens": ["A\u00b7ber", "nun", ":", "Oh", "teu\u00b7re", "Hei\u00b7mat", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "FM", "FM", "FM", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommt da nicht das s\u00fc\u00dfe Gretchen,", "tokens": ["Kommt", "da", "nicht", "das", "s\u00fc\u00b7\u00dfe", "Gret\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "PTKNEG", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das, weils seinen Hans gefunden,", "tokens": ["Das", ",", "weils", "sei\u00b7nen", "Hans", "ge\u00b7fun\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "KOUS", "PPOSAT", "NE", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schleunigst nach Florenz gemu\u00dft?", "tokens": ["Schleu\u00b7nigst", "nach", "Flo\u00b7renz", "ge\u00b7mu\u00dft", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "VVPP", "$."], "meter": "+-----+", "measure": "dactylic.init"}}, "stanza.84": {"line.1": {"text": "Ja, sie kommt, und ja, sie l\u00e4chelt,", "tokens": ["Ja", ",", "sie", "kommt", ",", "und", "ja", ",", "sie", "l\u00e4\u00b7chelt", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "$,", "KON", "PTKANT", "$,", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ja, sie ist ganz hin vor Selig-", "tokens": ["Ja", ",", "sie", "ist", "ganz", "hin", "vor", "Se\u00b7lig"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VAFIN", "ADV", "ADV", "APPR", "TRUNC"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Keit und gro\u00dfem Gl\u00fccke, weil sie", "tokens": ["Keit", "und", "gro\u00b7\u00dfem", "Gl\u00fc\u00b7cke", ",", "weil", "sie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN", "$,", "KOUS", "PPER"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Wirklich in Italien ist.", "tokens": ["Wirk\u00b7lich", "in", "I\u00b7ta\u00b7li\u00b7en", "ist", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "VAFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.85": {"line.1": {"text": "Spotte nicht, verruchter Knabe!", "tokens": ["Spot\u00b7te", "nicht", ",", "ver\u00b7ruch\u00b7ter", "Kna\u00b7be", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "La\u00df ihr auch das jugendstilig", "tokens": ["La\u00df", "ihr", "auch", "das", "ju\u00b7gend\u00b7sti\u00b7lig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "ADV", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "K\u00fcnstlerich empfundne, aber", "tokens": ["K\u00fcnst\u00b7le\u00b7rich", "emp\u00b7fund\u00b7ne", ",", "a\u00b7ber"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADJD", "ADJA", "$,", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Praktische Reformkost\u00fcm.", "tokens": ["Prak\u00b7ti\u00b7sche", "Re\u00b7form\u00b7kos\u00b7t\u00fcm", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.86": {"line.1": {"text": "Ist sie trotzdem nicht recht niedlich?", "tokens": ["Ist", "sie", "trotz\u00b7dem", "nicht", "recht", "nied\u00b7lich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PAV", "PTKNEG", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Frage dich: wie viele solche", "tokens": ["Fra\u00b7ge", "dich", ":", "wie", "vie\u00b7le", "sol\u00b7che"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "PPER", "$.", "PWAV", "PIS", "PIAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fcndchen, \u00c4ugelchen und N\u00e4schen", "tokens": ["M\u00fcnd\u00b7chen", ",", "\u00c4u\u00b7ge\u00b7lchen", "und", "N\u00e4\u00b7schen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Haben ehmals dich entflammt?", "tokens": ["Ha\u00b7ben", "eh\u00b7mals", "dich", "ent\u00b7flammt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.87": {"line.1": {"text": "Au\u00dferdem: \u00bbFr\u00fchlings Erwachen\u00ab", "tokens": ["Au\u00b7\u00dfer\u00b7dem", ":", "\u00bb", "Fr\u00fch\u00b7lings", "Er\u00b7wa\u00b7chen", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "punct"], "pos": ["PAV", "$.", "$(", "NN", "NN", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Hat auch diese tief begriffen,", "tokens": ["Hat", "auch", "die\u00b7se", "tief", "be\u00b7grif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PDAT", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie ist durchaus kein Gretchen", "tokens": ["Und", "sie", "ist", "durc\u00b7haus", "kein", "Gret\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie das alte Gretchen mehr.", "tokens": ["Wie", "das", "al\u00b7te", "Gret\u00b7chen", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.88": {"line.1": {"text": "Neue Jugend! \u2013 \u00bbJugend\u00ab! Pr\u00e4ge", "tokens": ["Neu\u00b7e", "Ju\u00b7gend", "!", "\u2013", "\u00bb", "Ju\u00b7gend", "\u00ab", "!", "Pr\u00e4\u00b7ge"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "punct", "punct", "word"], "pos": ["ADJA", "NN", "$.", "$(", "$(", "NN", "$(", "$.", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tief es dir in dein Gem\u00fcte:", "tokens": ["Tief", "es", "dir", "in", "dein", "Ge\u00b7m\u00fc\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der alten \u00bbGartenlaube\u00ab", "tokens": ["Von", "der", "al\u00b7ten", "\u00bb", "Gar\u00b7ten\u00b7lau\u00b7be", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "$(", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind wir absolut befreit.", "tokens": ["Sind", "wir", "ab\u00b7so\u00b7lut", "be\u00b7freit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.89": {"line.1": {"text": "Auf, und greife in die Harfe!", "tokens": ["Auf", ",", "und", "grei\u00b7fe", "in", "die", "Har\u00b7fe", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unser Gretchen ist verwandelt,", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "ver\u00b7wan\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unser Gretchen ist \u00e4sthetisch,", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "\u00e4s\u00b7the\u00b7tisch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Unser Gretchen ist modern.", "tokens": ["Un\u00b7ser", "Gret\u00b7chen", "ist", "mo\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.90": {"line.1": {"text": "Sieh, sie geht in einen Laden,", "tokens": ["Sieh", ",", "sie", "geht", "in", "ei\u00b7nen", "La\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo man sch\u00f6ne Marmorsachen", "tokens": ["Wo", "man", "sch\u00f6\u00b7ne", "Mar\u00b7mor\u00b7sa\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Billig kauft. Nun: was erstand sie?", "tokens": ["Bil\u00b7lig", "kauft", ".", "Nun", ":", "was", "er\u00b7stand", "sie", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$.", "ADV", "$.", "PWS", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ha! Ein nacktes Frauenbild!", "tokens": ["Ha", "!", "Ein", "nack\u00b7tes", "Frau\u00b7en\u00b7bild", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.91": {"line.1": {"text": "Schlag die Harfe! Schlag die Harfe!", "tokens": ["Schlag", "die", "Har\u00b7fe", "!", "Schlag", "die", "Har\u00b7fe", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$.", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn Germania ist gerettet.", "tokens": ["Denn", "Ger\u00b7ma\u00b7nia", "ist", "ge\u00b7ret\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zwar: sie kaufte einen Kitsch, doch,", "tokens": ["Zwar", ":", "sie", "kauf\u00b7te", "ei\u00b7nen", "Kitsch", ",", "doch", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$.", "PPER", "VVFIN", "ART", "NN", "$,", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Heil, es war ein nackter Kitsch!", "tokens": ["Heil", ",", "es", "war", "ein", "nack\u00b7ter", "Kitsch", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.92": {"line.1": {"text": "Vetturino! \u00bbSissignore\u00ab.", "tokens": ["Vet\u00b7tu\u00b7ri\u00b7no", "!", "\u00bb", "Sis\u00b7sig\u00b7no\u00b7re", "\u00ab", "."], "token_info": ["word", "punct", "punct", "word", "punct", "punct"], "pos": ["NE", "$.", "$(", "NE", "$(", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nach Fiesole! \u2013 Die G\u00e4ulchen", "tokens": ["Nach", "Fie\u00b7so\u00b7le", "!", "\u2013", "Die", "G\u00e4ul\u00b7chen"], "token_info": ["word", "word", "punct", "punct", "word", "word"], "pos": ["APPR", "NN", "$.", "$(", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Brauchen Gott sei Dank zwei Stunden,", "tokens": ["Brau\u00b7chen", "Gott", "sei", "Dank", "zwei", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "APPR", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bis ich wieder oben bin.", "tokens": ["Bis", "ich", "wie\u00b7der", "o\u00b7ben", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.93": {"line.1": {"text": "Denn es ist ein sch\u00f6nes Fahren,", "tokens": ["Denn", "es", "ist", "ein", "sch\u00f6\u00b7nes", "Fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Langsam, langsam, bis zur H\u00f6he.", "tokens": ["Lang\u00b7sam", ",", "lang\u00b7sam", ",", "bis", "zur", "H\u00f6\u00b7he", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "$,", "KOUS", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unten liegt, wie eine Muschel,", "tokens": ["Un\u00b7ten", "liegt", ",", "wie", "ei\u00b7ne", "Mu\u00b7schel", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PWAV", "ART", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Rosafleischig \u00fcberhaucht,", "tokens": ["Ro\u00b7sa\u00b7flei\u00b7schig", "\u00fc\u00b7ber\u00b7haucht", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.94": {"line.1": {"text": "Traumhaft, wesenlos, ein sanftes,", "tokens": ["Traum\u00b7haft", ",", "we\u00b7sen\u00b7los", ",", "ein", "sanf\u00b7tes", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zages Blinken, liegt phantomisch", "tokens": ["Za\u00b7ges", "Blin\u00b7ken", ",", "liegt", "phan\u00b7to\u00b7misch"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "VVFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Diese Stadt der alten, edlen", "tokens": ["Die\u00b7se", "Stadt", "der", "al\u00b7ten", ",", "ed\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PDAT", "NN", "ART", "ADJA", "$,", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Phrasenfeindlichen Kultur.", "tokens": ["Phra\u00b7sen\u00b7feind\u00b7li\u00b7chen", "Kul\u00b7tur", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}