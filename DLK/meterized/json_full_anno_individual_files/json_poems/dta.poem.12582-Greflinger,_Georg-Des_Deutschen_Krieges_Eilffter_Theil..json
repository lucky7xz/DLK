{"dta.poem.12582": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Eilffter Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Der neue Feld-Marschalck/ vor den Bannier zu", "tokens": ["Der", "neu\u00b7e", "Feld\u00b7Mar\u00b7schalck", "/", "vor", "den", "Ban\u00b7nier", "zu"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$(", "APPR", "ART", "NN", "PTKZU"], "meter": "-+-++-+-+-+", "measure": "unknown.measure.hexa"}, "line.2": {"text": "stehen/", "tokens": ["ste\u00b7hen", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Herr Linnert Torsten Sohn/ kam in das Feld zu", "tokens": ["Herr", "Lin\u00b7nert", "Tors\u00b7ten", "Sohn", "/", "kam", "in", "das", "Feld", "zu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "NE", "NE", "NN", "$(", "VVFIN", "APPR", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "gehen.", "tokens": ["ge\u00b7hen", "."], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Sag an Geschicht-G\u00f6ttin/ was that nun dieser Held?", "tokens": ["Sag", "an", "Ge\u00b7schicht\u00b7G\u00f6t\u00b7tin", "/", "was", "that", "nun", "die\u00b7ser", "Held", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$(", "PWS", "VVFIN", "ADV", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Er bracht acht tausend Mann aus Schweden in das Feld/", "tokens": ["Er", "bracht", "acht", "tau\u00b7send", "Mann", "aus", "Schwe\u00b7den", "in", "das", "Feld", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "CARD", "CARD", "NN", "APPR", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Darbey ein gro\u00dfes Geld und eine M\u00e4nge St\u00fccke/", "tokens": ["Dar\u00b7bey", "ein", "gro\u00b7\u00dfes", "Geld", "und", "ei\u00b7ne", "M\u00e4n\u00b7ge", "St\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "KON", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Mit vielem Kraut und Loth. Es hat das gute Gl\u00fccke", "tokens": ["Mit", "vie\u00b7lem", "Kraut", "und", "Loth", ".", "Es", "hat", "das", "gu\u00b7te", "Gl\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "NN", "KON", "NN", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Jhm stets die Hand gereicht. Es war voran gedacht/", "tokens": ["Jhm", "stets", "die", "Hand", "ge\u00b7reicht", ".", "Es", "war", "vo\u00b7ran", "ge\u00b7dacht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ART", "NN", "VVPP", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Da\u00df sich die-sonder-H\u00e4upt-gela\u00dfne Schweden-Macht", "tokens": ["Da\u00df", "sich", "die\u00b7son\u00b7der\u00b7H\u00e4up\u00b7tge\u00b7la\u00df\u00b7ne", "Schwe\u00b7den\u00b7Macht"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PRF", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "um Giffhorn und dahin bi\u00df Zell hatt\u2019 eingeleget/", "tokens": ["um", "Giff\u00b7horn", "und", "da\u00b7hin", "bi\u00df", "Zell", "hatt'", "ein\u00b7ge\u00b7le\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "PAV", "APPR", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "und sich ein wenig Zeit daselbsten wol verpfleget/", "tokens": ["und", "sich", "ein", "we\u00b7nig", "Zeit", "da\u00b7selbs\u00b7ten", "wol", "ver\u00b7pfle\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ART", "PIAT", "NN", "VVFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Dahin gieng Torsten Sohn mit seiner neuen Schaar", "tokens": ["Da\u00b7hin", "gieng", "Tors\u00b7ten", "Sohn", "mit", "sei\u00b7ner", "neu\u00b7en", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ADJA", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "und machte so ein Heer/ das gro\u00df zu f\u00f6rchten war.", "tokens": ["und", "mach\u00b7te", "so", "ein", "Heer", "/", "das", "gro\u00df", "zu", "f\u00f6rch\u00b7ten", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "$(", "ART", "ADJD", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Worauf die K\u00e4ysrischen sich bald zu r\u00fccke gaben/", "tokens": ["Wo\u00b7rauf", "die", "K\u00e4y\u00b7sri\u00b7schen", "sich", "bald", "zu", "r\u00fc\u00b7cke", "ga\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "PRF", "ADV", "PTKZU", "VVFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "und muste Th\u00fcringen sie neu zu Gaste haben.", "tokens": ["und", "mus\u00b7te", "Th\u00fc\u00b7rin\u00b7gen", "sie", "neu", "zu", "Gas\u00b7te", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "PPER", "ADJD", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.17": {"text": "Ob schon die Meynung war/ es w\u00fcrde Torsten Sohn", "tokens": ["Ob", "schon", "die", "Mey\u00b7nung", "war", "/", "es", "w\u00fcr\u00b7de", "Tors\u00b7ten", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "VAFIN", "$(", "PPER", "VAFIN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Denselben folgend seyn/ so lie\u00df er doch davon", "tokens": ["Den\u00b7sel\u00b7ben", "fol\u00b7gend", "seyn", "/", "so", "lie\u00df", "er", "doch", "da\u00b7von"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDAT", "ADJD", "VAINF", "$(", "ADV", "VVFIN", "PPER", "ADV", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "und wandte sich dafur (nach dem Jhm seine Schaaren", "tokens": ["und", "wand\u00b7te", "sich", "da\u00b7fur", "(", "nach", "dem", "Jhm", "sei\u00b7ne", "Schaa\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "PAV", "$(", "APPR", "PRELS", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "In Richtigkeit gebracht und neu beeydet waren.", "tokens": ["In", "Rich\u00b7tig\u00b7keit", "ge\u00b7bracht", "und", "neu", "be\u00b7ey\u00b7det", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "KON", "ADJD", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Nach dem auch Seckendorff/ ein Obrister/ der es", "tokens": ["Nach", "dem", "auch", "Se\u00b7cken\u00b7dorff", "/", "ein", "O\u00b7bris\u00b7ter", "/", "der", "es"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PRELS", "ADV", "NE", "$(", "ART", "NN", "$(", "PRELS", "PPER"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.22": {"text": "Mit seinem Feinde hielt/ dem Krieges-Recht gem\u00e4\u00df/", "tokens": ["Mit", "sei\u00b7nem", "Fein\u00b7de", "hielt", "/", "dem", "Krie\u00b7ge\u00b7sRecht", "ge\u00b7m\u00e4\u00df", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$(", "ART", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Daf\u00fcr enth\u00e4uptet war. Nach dem bey Aschersleben", "tokens": ["Da\u00b7f\u00fcr", "ent\u00b7h\u00e4up\u00b7tet", "war", ".", "Nach", "dem", "bey", "A\u00b7schers\u00b7le\u00b7ben"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "VVPP", "VAFIN", "$.", "APPR", "ART", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Ein tausend K\u00e4ysrische das Leben musten geben/", "tokens": ["Ein", "tau\u00b7send", "K\u00e4y\u00b7sri\u00b7sche", "das", "Le\u00b7ben", "mus\u00b7ten", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "ART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "und auch ein rothes Schwerdt in einer starcken Hand/", "tokens": ["und", "auch", "ein", "ro\u00b7thes", "Schwerdt", "in", "ei\u00b7ner", "star\u00b7cken", "Hand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Das seine Spitze hatt\u2019 auf Schlesien gewand/", "tokens": ["Das", "sei\u00b7ne", "Spit\u00b7ze", "hatt'", "auf", "Schle\u00b7si\u00b7en", "ge\u00b7wand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "VAFIN", "APPR", "NN", "VAPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Am Himmel war gesehn) zur Lau\u00dfnitz und von dannen", "tokens": ["Am", "Him\u00b7mel", "war", "ge\u00b7sehn", ")", "zur", "Lau\u00df\u00b7nitz", "und", "von", "dan\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VAFIN", "VVPP", "$(", "APPRART", "NN", "KON", "APPR", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Nach Schlesjen/ Stalhanschs Heer von neuen zu bemannen/", "tokens": ["Nach", "Schles\u00b7jen", "/", "Stal\u00b7hanschs", "Heer", "von", "neu\u00b7en", "zu", "be\u00b7man\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "NN", "NN", "APPR", "ADJA", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Weil es im Abgang war/ und wegen einer Macht/", "tokens": ["Weil", "es", "im", "Ab\u00b7gang", "war", "/", "und", "we\u00b7gen", "ei\u00b7ner", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VAFIN", "$(", "KON", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Von S\u00e4ch\u00df- und K\u00e4ysrischen darwider aufgebracht/", "tokens": ["Von", "S\u00e4ch\u00df", "und", "K\u00e4y\u00b7sri\u00b7schen", "dar\u00b7wi\u00b7der", "auf\u00b7ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "TRUNC", "KON", "NN", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "In gro\u00dfen N\u00f6then stundt\u2019. Es gieng die Hilff von statten/", "tokens": ["In", "gro\u00b7\u00dfen", "N\u00f6\u00b7then", "stundt'", ".", "Es", "gieng", "die", "Hilff", "von", "stat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NN", "APPR", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Ob schon die K\u00e4ysrischen die Lust zu st\u00fctzen hatten/", "tokens": ["Ob", "schon", "die", "K\u00e4y\u00b7sri\u00b7schen", "die", "Lust", "zu", "st\u00fct\u00b7zen", "hat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "So kunten sie doch nicht von wegen jener Schlacht/", "tokens": ["So", "kun\u00b7ten", "sie", "doch", "nicht", "von", "we\u00b7gen", "je\u00b7ner", "Schlacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "APPR", "APPR", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Die der Lambey verlohr/ wie oben war gedacht.", "tokens": ["Die", "der", "Lam\u00b7bey", "ver\u00b7lohr", "/", "wie", "o\u00b7ben", "war", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVFIN", "$(", "KOKOM", "ADV", "VAFIN", "VVPP", "$."], "meter": "--++-+-+-+-+", "measure": "anapaest.init"}, "line.35": {"text": "Sie musten h\u00e4uffig fort/ den Weymarschen und Hessen", "tokens": ["Sie", "mus\u00b7ten", "h\u00e4uf\u00b7fig", "fort", "/", "den", "Wey\u00b7mar\u00b7schen", "und", "Hes\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADJD", "PTKVZ", "$(", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Den Widerstand zu thun/ wiewol nicht gro\u00df. In dessen", "tokens": ["Den", "Wi\u00b7der\u00b7stand", "zu", "thun", "/", "wie\u00b7wol", "nicht", "gro\u00df", ".", "In", "des\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$(", "KOUS", "PTKNEG", "ADJD", "$.", "APPR", "PRELAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "Gieng Torsten Sohn gantz frey in Schlesien hinein.", "tokens": ["Gieng", "Tors\u00b7ten", "Sohn", "gantz", "frey", "in", "Schle\u00b7si\u00b7en", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "NN", "ADV", "ADJD", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "Und man sah nun das Gl\u00fcck von neuem Schwedisch seyn.", "tokens": ["Und", "man", "sah", "nun", "das", "Gl\u00fcck", "von", "neu\u00b7em", "Schwe\u00b7disch", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "VAINF", "$."], "meter": "-++--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.39": {"text": "Wo sich sein Heer erwie\u00df/ da sprungen Thor und Angel/", "tokens": ["Wo", "sich", "sein", "Heer", "er\u00b7wie\u00df", "/", "da", "sprun\u00b7gen", "Thor", "und", "An\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "PPOSAT", "NN", "VVFIN", "$(", "ADV", "VVFIN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Worzu auch Stallhansch fich/ anitzo sonder Mangel/", "tokens": ["Wor\u00b7zu", "auch", "Stall\u00b7hansch", "fich", "/", "a\u00b7nit\u00b7zo", "son\u00b7der", "Man\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "NN", "ADJD", "$(", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Getreulich brauchen lie\u00df. Grosglogau/ eine Stadt", "tokens": ["Ge\u00b7treu\u00b7lich", "brau\u00b7chen", "lie\u00df", ".", "Gros\u00b7glo\u00b7gau", "/", "ei\u00b7ne", "Stadt"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ADJD", "VVINF", "VVFIN", "$.", "NE", "$(", "ART", "NN"], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.42": {"text": "Woselbst der Gegentheil viel Krieges-Mittel hatt\u2019", "tokens": ["Wo\u00b7selbst", "der", "Ge\u00b7gen\u00b7theil", "viel", "Krie\u00b7ge\u00b7sMit\u00b7tel", "hatt'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "PIAT", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "und starck von Volcke war/ bekam die gr\u00f6sten Wunden.", "tokens": ["und", "starck", "von", "Vol\u00b7cke", "war", "/", "be\u00b7kam", "die", "gr\u00f6s\u00b7ten", "Wun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "NN", "VAFIN", "$(", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Sie nahmen sie ", "tokens": ["Sie", "nah\u00b7men", "sie"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER"], "meter": "-+-+", "measure": "iambic.di"}, "line.45": {"text": "Erw\u00fcrgten sie/ wie dann ein achtmal hundert Mann", "tokens": ["Er\u00b7w\u00fcrg\u00b7ten", "sie", "/", "wie", "dann", "ein", "acht\u00b7mal", "hun\u00b7dert", "Mann"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$(", "KOKOM", "ADV", "ART", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Verfielen/ fast so viel nahm man gefangen an.", "tokens": ["Ver\u00b7fie\u00b7len", "/", "fast", "so", "viel", "nahm", "man", "ge\u00b7fan\u00b7gen", "an", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADV", "ADV", "ADV", "VVFIN", "PIS", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.47": {"text": "Acht tausend Malter Meel/ mit etlich tausend Pferden/", "tokens": ["Acht", "tau\u00b7send", "Mal\u00b7ter", "Meel", "/", "mit", "et\u00b7lich", "tau\u00b7send", "Pfer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "NN", "NN", "$(", "APPR", "ADJD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.48": {"text": "Die musten allesamt der Schweden Beuthe werden.", "tokens": ["Die", "mus\u00b7ten", "al\u00b7le\u00b7samt", "der", "Schwe\u00b7den", "Beu\u00b7the", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Was man an Pulver/ Bleu/ an S\u00e4tteln/ B\u00fcchsen/ Stahl", "tokens": ["Was", "man", "an", "Pul\u00b7ver", "/", "Bleu", "/", "an", "S\u00e4t\u00b7teln", "/", "B\u00fcch\u00b7sen", "/", "Stahl"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["PWS", "PIS", "APPR", "NN", "$(", "NN", "$(", "APPR", "NN", "$(", "NN", "$(", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "und andern mehr bekam/ war einer gro\u00dfen Zahl.", "tokens": ["und", "an\u00b7dern", "mehr", "be\u00b7kam", "/", "war", "ei\u00b7ner", "gro\u00b7\u00dfen", "Zahl", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADV", "VVFIN", "$(", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "Wil uns das Gl\u00fccke wol so mu\u00df man nicht verweilen/", "tokens": ["Wil", "uns", "das", "Gl\u00fc\u00b7cke", "wol", "so", "mu\u00df", "man", "nicht", "ver\u00b7wei\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "ADV", "ADV", "VMFIN", "PIS", "PTKNEG", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.52": {"text": "und nehmen weil es giebt/ dann es pflegt sehr zu eilen.", "tokens": ["und", "neh\u00b7men", "weil", "es", "giebt", "/", "dann", "es", "pflegt", "sehr", "zu", "ei\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOUS", "PPER", "VVFIN", "$(", "ADV", "PPER", "VVFIN", "ADV", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.53": {"text": "Di\u00df nahm der Schwed in acht und nahm fast Fl\u00fcgel an/", "tokens": ["Di\u00df", "nahm", "der", "Schwed", "in", "acht", "und", "nahm", "fast", "Fl\u00fc\u00b7gel", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "CARD", "KON", "VVFIN", "ADV", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Der fl\u00fcgenden G\u00f6ttin nach dem und jenen Plan", "tokens": ["Der", "fl\u00fc\u00b7gen\u00b7den", "G\u00f6t\u00b7tin", "nach", "dem", "und", "je\u00b7nen", "Plan"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "KON", "PDAT", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.55": {"text": "Zu folgen. Trachenberg das wurde gantz nicht sauer/", "tokens": ["Zu", "fol\u00b7gen", ".", "Tra\u00b7chen\u00b7berg", "das", "wur\u00b7de", "gantz", "nicht", "sau\u00b7er", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "NN", "PDS", "VAFIN", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Es gab sich g\u00fctig auf. Strig aber/ Wohlau/ Jauer", "tokens": ["Es", "gab", "sich", "g\u00fc\u00b7tig", "auf", ".", "Strig", "a\u00b7ber", "/", "Wo\u00b7hlau", "/", "Jau\u00b7er"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "PRF", "ADJD", "PTKVZ", "$.", "NE", "ADV", "$(", "NE", "$(", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "und Grottau f\u00fchleten des Stahls und Brands Gewalt.", "tokens": ["und", "Grot\u00b7tau", "f\u00fch\u00b7le\u00b7ten", "des", "Stahls", "und", "Brands", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "KON", "NN", "NN", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.58": {"text": "Dieweil auch Schweinitz noch ein gro\u00dfer Aufenthalt", "tokens": ["Die\u00b7weil", "auch", "Schwei\u00b7nitz", "noch", "ein", "gro\u00b7\u00dfer", "Auf\u00b7ent\u00b7halt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "NE", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "Des Gegners war/ und sich die Schweden dero W\u00e4llen", "tokens": ["Des", "Geg\u00b7ners", "war", "/", "und", "sich", "die", "Schwe\u00b7den", "de\u00b7ro", "W\u00e4l\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "$(", "KON", "PRF", "ART", "NN", "PRELAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Tag-t\u00e4glich n\u00e4herten/ sie unter sie zu st\u00e4llen/", "tokens": ["Tag\u00b7t\u00e4g\u00b7lich", "n\u00e4\u00b7her\u00b7ten", "/", "sie", "un\u00b7ter", "sie", "zu", "st\u00e4l\u00b7len", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$(", "PPER", "APPR", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "Erhob der Sassen-F\u00fcrst/ Frantz Albrecht/ seine Schaar/", "tokens": ["Er\u00b7hob", "der", "Sas\u00b7sen\u00b7F\u00fcrst", "/", "Frantz", "Al\u00b7brecht", "/", "sei\u00b7ne", "Schaar", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$(", "NE", "NE", "$(", "PPOSAT", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.62": {"text": "Weil er in Schlesien des K\u00e4ysers Feld-Herr war/", "tokens": ["Weil", "er", "in", "Schle\u00b7si\u00b7en", "des", "K\u00e4y\u00b7sers", "Feld\u00b7Herr", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "ART", "NN", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "und gieng nach solcher Stadt/ ihr vor den Schweden Waf-", "tokens": ["und", "gieng", "nach", "sol\u00b7cher", "Stadt", "/", "ihr", "vor", "den", "Schwe\u00b7den", "Waf"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "$(", "PPER", "APPR", "ART", "NE", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.64": {"text": "Von Mitteln/ so an Volck als anderm/ Hilff zu schaffen.", "tokens": ["Von", "Mit\u00b7teln", "/", "so", "an", "Volck", "als", "an\u00b7derm", "/", "Hilff", "zu", "schaf\u00b7fen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "ADV", "APPR", "NN", "KOUS", "PIS", "$(", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Eh er den Ort betrat/ kam K\u00f6nigsmarck auf jhn/", "tokens": ["Eh", "er", "den", "Ort", "be\u00b7trat", "/", "kam", "K\u00f6\u00b7nigs\u00b7marck", "auf", "jhn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$(", "VVFIN", "NN", "APPR", "PPER", "$("], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.66": {"text": "und hielt jhn also fest/ da\u00df er/ dahin zu ziehn/", "tokens": ["und", "hielt", "jhn", "al\u00b7so", "fest", "/", "da\u00df", "er", "/", "da\u00b7hin", "zu", "ziehn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKVZ", "$(", "KOUS", "PPER", "$(", "PAV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.67": {"text": "Noch Weg/ noch Vorthel sah. In dem sie also stritten", "tokens": ["Noch", "Weg", "/", "noch", "Vor\u00b7thel", "sah", ".", "In", "dem", "sie", "al\u00b7so", "strit\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "$(", "ADV", "NN", "VVFIN", "$.", "APPR", "PRELS", "PPER", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.68": {"text": "Kam Torsten Sohn und nahm den Feind recht in die Mit-", "tokens": ["Kam", "Tors\u00b7ten", "Sohn", "und", "nahm", "den", "Feind", "recht", "in", "die", "Mit"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "NN", "KON", "VVFIN", "ART", "NN", "ADJD", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "Es kam zu gro\u00dfen Streit/ ", "tokens": ["Es", "kam", "zu", "gro\u00b7\u00dfen", "Streit", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.70": {"text": "und blieben von dem Feind ein achtzehn hundert Mann.", "tokens": ["und", "blie\u00b7ben", "von", "dem", "Feind", "ein", "acht\u00b7zehn", "hun\u00b7dert", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ART", "CARD", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "Er selbst/ der Sassen-F\u00fcrst/ empfieng zwo Kugel-Wunden/", "tokens": ["Er", "selbst", "/", "der", "Sas\u00b7sen\u00b7F\u00fcrst", "/", "emp\u00b7fi\u00b7eng", "zwo", "Ku\u00b7gel\u00b7Wun\u00b7den", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$(", "ART", "NN", "$(", "VVFIN", "CARD", "NN", "$("], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.72": {"text": "Wovon er bald hernach entseelet wurd\u2019 erfunden.", "tokens": ["Wo\u00b7von", "er", "bald", "her\u00b7nach", "ent\u00b7see\u00b7let", "wurd'", "er\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "ADV", "VVPP", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Zwey tausend blieben fest/ f\u00fcnff Obersten darbey/", "tokens": ["Zwey", "tau\u00b7send", "blie\u00b7ben", "fest", "/", "f\u00fcnff", "O\u00b7bers\u00b7ten", "dar\u00b7bey", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "VVFIN", "PTKVZ", "$(", "CARD", "NN", "PAV", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.74": {"text": "Man bracht\u2019 auch viermal zehn von Fahnen nach der Rey", "tokens": ["Man", "bracht'", "auch", "vier\u00b7mal", "zehn", "von", "Fah\u00b7nen", "nach", "der", "Rey"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "ADV", "CARD", "APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "Dem Sieger ins Gesicht. Zwey Thiere lang von Ohren/", "tokens": ["Dem", "Sie\u00b7ger", "ins", "Ge\u00b7sicht", ".", "Zwey", "Thie\u00b7re", "lang", "von", "Oh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$.", "CARD", "NN", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.76": {"text": "Mit Geld und andrem mehr/ die giengenhier verlohren/", "tokens": ["Mit", "Geld", "und", "an\u00b7drem", "mehr", "/", "die", "gien\u00b7gen\u00b7hier", "ver\u00b7loh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "PIS", "ADV", "$(", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Auch Schweinitz gab sich auf. Der Schwed gab GOtt den", "tokens": ["Auch", "Schwei\u00b7nitz", "gab", "sich", "auf", ".", "Der", "Schwed", "gab", "Gott", "den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "NE", "VVFIN", "PRF", "PTKVZ", "$.", "ART", "NN", "VVFIN", "NN", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.78": {"text": "Prei\u00df/", "tokens": ["Prei\u00df", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+", "measure": "single.up"}, "line.79": {"text": "und satzte sich hierauf mit aller Macht vor Neu\u00df/", "tokens": ["und", "satz\u00b7te", "sich", "hier\u00b7auf", "mit", "al\u00b7ler", "Macht", "vor", "Neu\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PAV", "APPR", "PIAT", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "Worin acht hundert Mann in Waffen r\u00fcstig stunden/", "tokens": ["Wo\u00b7rin", "acht", "hun\u00b7dert", "Mann", "in", "Waf\u00b7fen", "r\u00fcs\u00b7tig", "stun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "CARD", "CARD", "NN", "APPR", "NN", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Sie wurden aber bald mit st\u00fcrmen \u00fcberwunden", "tokens": ["Sie", "wur\u00b7den", "a\u00b7ber", "bald", "mit", "st\u00fcr\u00b7men", "\u00fc\u00b7berw\u00b7un\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "APPR", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.82": {"text": "und um den Geist gebracht. Damit so war der Krieg", "tokens": ["und", "um", "den", "Geist", "ge\u00b7bracht", ".", "Da\u00b7mit", "so", "war", "der", "Krieg"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "VVPP", "$.", "PAV", "ADV", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "Mit Schlesjen fast gethan/ nur Bre\u00dflau/ Lignitz/ Brieg", "tokens": ["Mit", "Schles\u00b7jen", "fast", "ge\u00b7than", "/", "nur", "Bre\u00df\u00b7lau", "/", "Lig\u00b7nitz", "/", "Brieg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NN", "ADV", "VVPP", "$(", "ADV", "NE", "$(", "NE", "$(", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "und Oppeln stellten sich noch zimlich hart darwider.", "tokens": ["und", "Op\u00b7peln", "stell\u00b7ten", "sich", "noch", "zim\u00b7lich", "hart", "dar\u00b7wi\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PRF", "ADV", "ADV", "ADJD", "PAV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Doch Oppeln fiel hieraus durch seine Flammen nider/", "tokens": ["Doch", "Op\u00b7peln", "fiel", "hier\u00b7aus", "durch", "sei\u00b7ne", "Flam\u00b7men", "ni\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PAV", "APPR", "PPOSAT", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "und Bre\u00dflau reichte Geld. Hierauf nahm man den Zug", "tokens": ["und", "Bre\u00df\u00b7lau", "reich\u00b7te", "Geld", ".", "Hier\u00b7auf", "nahm", "man", "den", "Zug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NE", "VVFIN", "NN", "$.", "PAV", "VVFIN", "PIS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.87": {"text": "Nach Olm\u00fctz/ welcher Ort von allerley genug", "tokens": ["Nach", "Ol\u00b7m\u00fctz", "/", "wel\u00b7cher", "Ort", "von", "al\u00b7ler\u00b7ley", "ge\u00b7nug"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "PWAT", "NN", "APPR", "PIAT", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "In sich beschlossen hielt. Kaum da/ war man darinnen. ", "tokens": ["In", "sich", "be\u00b7schlos\u00b7sen", "hielt", ".", "Kaum", "da", "/", "war", "man", "da\u00b7rin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "VVPP", "VVFIN", "$.", "ADV", "ADV", "$(", "VAFIN", "PIS", "ADV", "$."], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.89": {"text": "Es mahnte Krafft und Muth ein mehrers zu gewinnen.", "tokens": ["Es", "mahn\u00b7te", "Krafft", "und", "Muth", "ein", "meh\u00b7rers", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "ART", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Wanck/ der sich vormals hatt\u2019 in G\u00f6rlitz so gewehrt/", "tokens": ["Wanck", "/", "der", "sich", "vor\u00b7mals", "hatt'", "in", "G\u00f6r\u00b7litz", "so", "ge\u00b7wehrt", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PRELS", "PRF", "ADV", "VAFIN", "APPR", "NE", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "Da\u00df jhn so Freind so Feind vor tapfer hat geehrt/", "tokens": ["Da\u00df", "jhn", "so", "Freind", "so", "Feind", "vor", "tap\u00b7fer", "hat", "ge\u00b7ehrt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "ADV", "NN", "APPR", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "Wurd\u2019 hier zum H\u00e4upt erkiest/ den Ort wol zu bewachen/", "tokens": ["Wurd'", "hier", "zum", "H\u00e4upt", "er\u00b7kiest", "/", "den", "Ort", "wol", "zu", "be\u00b7wa\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "VVFIN", "$(", "ART", "NN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.93": {"text": "Wie auch/ nach seiner Kunst zu bauen/ fest zu machen.", "tokens": ["Wie", "auch", "/", "nach", "sei\u00b7ner", "Kunst", "zu", "bau\u00b7en", "/", "fest", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "$(", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$(", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.94": {"text": "Er hatte neben sich drey tausend Mann zur Wacht.", "tokens": ["Er", "hat\u00b7te", "ne\u00b7ben", "sich", "drey", "tau\u00b7send", "Mann", "zur", "Wacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PRF", "CARD", "CARD", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Es war die Stadt einmal in einer duncklen Nacht", "tokens": ["Es", "war", "die", "Stadt", "ein\u00b7mal", "in", "ei\u00b7ner", "dunck\u00b7len", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "Verr\u00e4therisch bedacht sie alle zu ermorden.", "tokens": ["Ver\u00b7r\u00e4\u00b7the\u00b7risch", "be\u00b7dacht", "sie", "al\u00b7le", "zu", "er\u00b7mor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "Di\u00df ", "tokens": ["Di\u00df"], "token_info": ["word"], "pos": ["PDS"], "meter": "+", "measure": "single.up"}, "line.98": {"text": "und musten 4 mal 6 daf\u00fcr enth\u00e4uptet seyn.", "tokens": ["und", "mus\u00b7ten", "4", "mal", "6", "da\u00b7f\u00fcr", "ent\u00b7h\u00e4up\u00b7tet", "seyn", "."], "token_info": ["word", "word", "number", "word", "number", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "CARD", "ADV", "CARD", "PAV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.99": {"text": "Man lasse sich ja nie in solche Bo\u00dfheit ein/", "tokens": ["Man", "las\u00b7se", "sich", "ja", "nie", "in", "sol\u00b7che", "Bo\u00df\u00b7heit", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADV", "ADV", "APPR", "PIAT", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Dann sie gar selten kan hinaus gef\u00fchret werden.", "tokens": ["Dann", "sie", "gar", "sel\u00b7ten", "kan", "hin\u00b7aus", "ge\u00b7f\u00fch\u00b7ret", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ADV", "ADJD", "VMFIN", "APZR", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Selbst den ", "tokens": ["Selbst", "den"], "token_info": ["word", "word"], "pos": ["ADV", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.102": {"text": "Zerrei\u00dfen. Also lag die Macht dem Mordenob.", "tokens": ["Zer\u00b7rei\u00b7\u00dfen", ".", "Al\u00b7so", "lag", "die", "Macht", "dem", "Mor\u00b7de\u00b7nob", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "Hief\u00fcr hat eine Magd nechst GOtt jhr gro\u00dfes Lob/", "tokens": ["Hie\u00b7f\u00fcr", "hat", "ei\u00b7ne", "Magd", "nechst", "Gott", "jhr", "gro\u00b7\u00dfes", "Lob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "VVFIN", "NN", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Sie wuste von dem Mord\u2019 und gab es zu verstehen/", "tokens": ["Sie", "wus\u00b7te", "von", "dem", "Mord'", "und", "gab", "es", "zu", "ver\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "VVFIN", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "Damit jhr Liebster nicht hiedurch m\u00f6cht\u2019 untergehen.", "tokens": ["Da\u00b7mit", "jhr", "Liebs\u00b7ter", "nicht", "hie\u00b7durch", "m\u00f6cht'", "un\u00b7ter\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "PTKNEG", "PAV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.107": {"text": "Da\u00df jhres Schmiedes Werck jhm nicht zu \u00fcbel thu.", "tokens": ["Da\u00df", "jhres", "Schmie\u00b7des", "Werck", "jhm", "nicht", "zu", "\u00fc\u00b7bel", "thu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "NN", "PPER", "PTKNEG", "PTKA", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.108": {"text": "Nu wieder in das Feld/ woselbst die Schweden-Schaaren", "tokens": ["Nu", "wie\u00b7der", "in", "das", "Feld", "/", "wo\u00b7selbst", "die", "Schwe\u00b7den\u00b7Schaa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$(", "PWAV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Bi\u00df nach ber Donau hin/ wie St\u00f6hme sch\u00fc\u00dfend waren.", "tokens": ["Bi\u00df", "nach", "ber", "Do\u00b7nau", "hin", "/", "wie", "St\u00f6h\u00b7me", "sch\u00fc\u00b7\u00dfend", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "NE", "NE", "PTKVZ", "$(", "KOKOM", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "Helm Wrangel gieng nach Wien/ ein anderer nach Prag/", "tokens": ["Helm", "Wran\u00b7gel", "gieng", "nach", "Wi\u00b7en", "/", "ein", "an\u00b7de\u00b7rer", "nach", "Prag", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "NE", "$(", "ART", "ADJA", "APPR", "NE", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.111": {"text": "Sie zapften alles an/ was vorn und hinten lag.", "tokens": ["Sie", "zapf\u00b7ten", "al\u00b7les", "an", "/", "was", "vorn", "und", "hin\u00b7ten", "lag", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$(", "PWS", "ADV", "KON", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "So ungehaltner Macht mit Macht zu widerstehen/", "tokens": ["So", "un\u00b7ge\u00b7halt\u00b7ner", "Macht", "mit", "Macht", "zu", "wi\u00b7der\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "APPR", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "Must\u2019 alles K\u00e4yser-Volck in einem Hauffen gehen", "tokens": ["Must'", "al\u00b7les", "K\u00e4y\u00b7ser\u00b7Volck", "in", "ei\u00b7nem", "Hauf\u00b7fen", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PIAT", "NN", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Mit Piccolomini/ als seinem General/", "tokens": ["Mit", "Pic\u00b7co\u00b7lo\u00b7mi\u00b7ni", "/", "als", "sei\u00b7nem", "Ge\u00b7ne\u00b7ral", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KOUS", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "Die Schwedische Gewalt zu br\u00e4chen. Seine Zahl", "tokens": ["Die", "Schwe\u00b7di\u00b7sche", "Ge\u00b7walt", "zu", "br\u00e4\u00b7chen", ".", "Sei\u00b7ne", "Zahl"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$.", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "War viertzig tausend Mann. Ein Theil must Olmitz schl\u00fcs-", "tokens": ["War", "viert\u00b7zig", "tau\u00b7send", "Mann", ".", "Ein", "Theil", "must", "Ol\u00b7mitz", "schl\u00fcs"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "CARD", "CARD", "NN", "$.", "ART", "NN", "VMFIN", "NE", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.117": {"text": "Ein Theil/ von welchen man das b\u00e4ste mochte wissen/", "tokens": ["Ein", "Theil", "/", "von", "wel\u00b7chen", "man", "das", "b\u00e4s\u00b7te", "moch\u00b7te", "wis\u00b7sen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "APPR", "PWAT", "PIS", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "Des Torsten Sohns sein Heer beziehen/ dessen Strich", "tokens": ["Des", "Tors\u00b7ten", "Sohns", "sein", "Heer", "be\u00b7zie\u00b7hen", "/", "des\u00b7sen", "Strich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "NN", "PPOSAT", "NN", "VVINF", "$(", "PRELAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.119": {"text": "Zu r\u00fcck in Schlesjen war/ weil er der M\u00e4nge wich.", "tokens": ["Zu", "r\u00fcck", "in", "Schles\u00b7jen", "war", "/", "weil", "er", "der", "M\u00e4n\u00b7ge", "wich", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "NN", "VAFIN", "$(", "KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Di\u00df nahm den Krieg von Brieg/ das lange Zeit beschlossen", "tokens": ["Di\u00df", "nahm", "den", "Krieg", "von", "Brieg", "/", "das", "lan\u00b7ge", "Zeit", "be\u00b7schlos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "NN", "$(", "ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "und auch beschossen war. Kam dieses Heer geflossen", "tokens": ["und", "auch", "be\u00b7schos\u00b7sen", "war", ".", "Kam", "die\u00b7ses", "Heer", "ge\u00b7flos\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVPP", "VAFIN", "$.", "NE", "PDAT", "NN", "VVPP"], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}, "line.122": {"text": "Als eine Flut/ so gieng es jetzund ebbend ab/", "tokens": ["Als", "ei\u00b7ne", "Flut", "/", "so", "gieng", "es", "je\u00b7tzund", "eb\u00b7bend", "ab", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$(", "ADV", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.123": {"text": "Da\u00df sein Verfolger sich bi\u00df vor Gro\u00dfglogau gab/", "tokens": ["Da\u00df", "sein", "Ver\u00b7fol\u00b7ger", "sich", "bi\u00df", "vor", "Gro\u00df\u00b7glo\u00b7gau", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PRF", "ADV", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.124": {"text": "und solchen Ort beschlo\u00df/ jedoch nicht b\u00e4ster ma\u00dfen/", "tokens": ["und", "sol\u00b7chen", "Ort", "be\u00b7schlo\u00df", "/", "je\u00b7doch", "nicht", "b\u00e4s\u00b7ter", "ma\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "$(", "ADV", "PTKNEG", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "Er muste wieder ab und viel darvor verla\u00dfen/", "tokens": ["Er", "mus\u00b7te", "wie\u00b7der", "ab", "und", "viel", "dar\u00b7vor", "ver\u00b7la\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PTKVZ", "KON", "ADV", "PAV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.126": {"text": "Die jhm des Wrangels Volck in einem Au\u00dffall schlug.", "tokens": ["Die", "jhm", "des", "Wran\u00b7gels", "Volck", "in", "ei\u00b7nem", "Au\u00df\u00b7fall", "schlug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "Man wei\u00df es was Begier Bannier nach Leipzig trug/", "tokens": ["Man", "wei\u00df", "es", "was", "Be\u00b7gier", "Ban\u00b7nier", "nach", "Leip\u00b7zig", "trug", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PIS", "ADJA", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Man wei\u00df auch wol wie schlecht sein Wollen angegangen.", "tokens": ["Man", "wei\u00df", "auch", "wol", "wie", "schlecht", "sein", "Wol\u00b7len", "an\u00b7ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "ADV", "KOKOM", "ADJD", "VAINF", "VMFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Und nun kam Torsten Sohn/ dergleichen anzufangen.", "tokens": ["Und", "nun", "kam", "Tors\u00b7ten", "Sohn", "/", "derg\u00b7lei\u00b7chen", "an\u00b7zu\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADJA", "NN", "$(", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Er gieng durch Schlesien dahin ins breite Feld/", "tokens": ["Er", "gieng", "durch", "Schle\u00b7si\u00b7en", "da\u00b7hin", "ins", "brei\u00b7te", "Feld", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "PAV", "APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "und hielt die gute Stadt mit gro\u00dfer Macht umst\u00e4llt.", "tokens": ["und", "hielt", "die", "gu\u00b7te", "Stadt", "mit", "gro\u00b7\u00dfer", "Macht", "um\u00b7st\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.132": {"text": "Kaum da/ kam auch sein Feind/ das Leipzig zu entsetzen/", "tokens": ["Kaum", "da", "/", "kam", "auch", "sein", "Feind", "/", "das", "Leip\u00b7zig", "zu", "ent\u00b7set\u00b7zen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$(", "VVFIN", "ADV", "PPOSAT", "NN", "$(", "ART", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "und es gerieth hierob ", "tokens": ["und", "es", "ge\u00b7rieth", "hier\u00b7ob"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.134": {"text": "So/ da\u00df der K\u00e4ysrischen bey f\u00fcnffmal tausend Mann", "tokens": ["So", "/", "da\u00df", "der", "K\u00e4y\u00b7sri\u00b7schen", "bey", "f\u00fcnff\u00b7mal", "tau\u00b7send", "Mann"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$(", "KOUS", "ART", "NN", "APPR", "ADV", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Verfielen/ fast so viel nahm man gefangen an/", "tokens": ["Ver\u00b7fie\u00b7len", "/", "fast", "so", "viel", "nahm", "man", "ge\u00b7fan\u00b7gen", "an", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADV", "ADV", "ADV", "VVFIN", "PIS", "ADJD", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.136": {"text": "und blieben neben dem ein 46. St\u00fccke/", "tokens": ["und", "blie\u00b7ben", "ne\u00b7ben", "dem", "ein", "46.", "St\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "ordinal", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "ART", "ADJA", "NN", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.137": {"text": "Ein 90. Fahnen und das meiste Gut zu r\u00fccke.", "tokens": ["Ein", "90.", "Fah\u00b7nen", "und", "das", "meis\u00b7te", "Gut", "zu", "r\u00fc\u00b7cke", "."], "token_info": ["word", "ordinal", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ART", "ADJA", "NN", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.138": {"text": "Des Siegers sein Verlust war kein halb tausend Mann.", "tokens": ["Des", "Sie\u00b7gers", "sein", "Ver\u00b7lust", "war", "kein", "halb", "tau\u00b7send", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VAFIN", "PIAT", "ADJD", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.139": {"text": "Damit so klopfet\u2019 er vor Leipzig wieder an/", "tokens": ["Da\u00b7mit", "so", "klop\u00b7fet'", "er", "vor", "Leip\u00b7zig", "wie\u00b7der", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "APPR", "NE", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "und zwar mit solchem Ernst/ da\u00df es sich must\u2019", "tokens": ["und", "zwar", "mit", "sol\u00b7chem", "Ernst", "/", "da\u00df", "es", "sich", "must'"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "PIAT", "NN", "$(", "KOUS", "PPER", "PRF", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.141": {"text": "und bi\u00df zum Friedens-Schlu\u00df in seinen Diensten leben.", "tokens": ["und", "bi\u00df", "zum", "Frie\u00b7dens\u00b7Schlu\u00df", "in", "sei\u00b7nen", "Diens\u00b7ten", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "APPRART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "Das machte diese Schlacht. Ists nicht ein Wunderding!", "tokens": ["Das", "mach\u00b7te", "die\u00b7se", "Schlacht", ".", "Ists", "nicht", "ein", "Wun\u00b7der\u00b7ding", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PDAT", "NN", "$.", "NE", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "So offte man sich hier des schlagens unterfieng/", "tokens": ["So", "off\u00b7te", "man", "sich", "hier", "des", "schla\u00b7gens", "un\u00b7ter\u00b7fieng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "ADV", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "So offte blieb der Sieg den Kriegs-gelehrten Schweden.", "tokens": ["So", "off\u00b7te", "blieb", "der", "Sieg", "den", "Kriegs\u00b7ge\u00b7lehr\u00b7ten", "Schwe\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "NE", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "Man mag es sonder Scheu/ dieweil es wahr ist/ reden/", "tokens": ["Man", "mag", "es", "son\u00b7der", "Scheu", "/", "die\u00b7weil", "es", "wahr", "ist", "/", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "PIAT", "NN", "$(", "KOUS", "PPER", "ADJD", "VAFIN", "$(", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.146": {"text": "Da\u00df von den K\u00e4ysrischen ein zwantzig tausend Mann/", "tokens": ["Da\u00df", "von", "den", "K\u00e4y\u00b7sri\u00b7schen", "ein", "zwant\u00b7zig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "ART", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "In dreyen Schlachtungen vom Leben abgethan/", "tokens": ["In", "drey\u00b7en", "Schlach\u00b7tun\u00b7gen", "vom", "Le\u00b7ben", "ab\u00b7ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "Hierum verscharret seyn. Die Erde gl\u00e4ntzt von Knochen.", "tokens": ["Hie\u00b7rum", "ver\u00b7schar\u00b7ret", "seyn", ".", "Die", "Er\u00b7de", "gl\u00e4ntzt", "von", "Kno\u00b7chen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "VAINF", "$.", "ART", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.149": {"text": "Nach dem der Torsten Sohn in f\u00fcnff und zwantzig Wochen", "tokens": ["Nach", "dem", "der", "Tors\u00b7ten", "Sohn", "in", "f\u00fcnff", "und", "zwant\u00b7zig", "Wo\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "NN", "NN", "APPR", "CARD", "KON", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Zweymal das Gl\u00fccke hatt\u2019 in offenbarer Schlacht", "tokens": ["Zwey\u00b7mal", "das", "Gl\u00fc\u00b7cke", "hatt'", "in", "of\u00b7fen\u00b7ba\u00b7rer", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "VAFIN", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.151": {"text": "Zu siegen und dabey manch Ort in seine Macht", "tokens": ["Zu", "sie\u00b7gen", "und", "da\u00b7bey", "manch", "Ort", "in", "sei\u00b7ne", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "KON", "PAV", "PIAT", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "Zu kriegen/ trieb es jhn zu noch viel andern Dingen/", "tokens": ["Zu", "krie\u00b7gen", "/", "trieb", "es", "jhn", "zu", "noch", "viel", "an\u00b7dern", "Din\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "PPER", "PPER", "APPR", "ADV", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Und sih jhn nun gesinnt auch Freyberg zu bezwingen.", "tokens": ["Und", "sih", "jhn", "nun", "ge\u00b7sinnt", "auch", "Frey\u00b7berg", "zu", "be\u00b7zwin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "ADV", "VVPP", "ADV", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Er brauchte gro\u00dfer Macht/ zog aber endlich ab/", "tokens": ["Er", "brauch\u00b7te", "gro\u00b7\u00dfer", "Macht", "/", "zog", "a\u00b7ber", "end\u00b7lich", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJA", "NN", "$(", "VVFIN", "ADV", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Worzu ein neues Heer vom K\u00e4yser ", "tokens": ["Wor\u00b7zu", "ein", "neu\u00b7es", "Heer", "vom", "K\u00e4y\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.156": {"text": "Zwey tausend sollen jhm hiervor seyn umgekommen.", "tokens": ["Zwey", "tau\u00b7send", "sol\u00b7len", "jhm", "hier\u00b7vor", "seyn", "um\u00b7ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "VMFIN", "PPER", "PAV", "VAINF", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.157": {"text": "Worauf sein andrer Zug nach Olm\u00fctz war genommen.", "tokens": ["Wo\u00b7rauf", "sein", "an\u00b7drer", "Zug", "nach", "Ol\u00b7m\u00fctz", "war", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "ADJA", "NN", "APPR", "NE", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "Es war sehr hart bedr\u00e4ngt/ und schrieb um den Entsatz/", "tokens": ["Es", "war", "sehr", "hart", "be\u00b7dr\u00e4ngt", "/", "und", "schrieb", "um", "den", "Ent\u00b7satz", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "VVFIN", "$(", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "Er kam auch schleunig an und rettete den Platz.", "tokens": ["Er", "kam", "auch", "schleu\u00b7nig", "an", "und", "ret\u00b7te\u00b7te", "den", "Platz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "PTKVZ", "KON", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Bekam auch \u00fcber di\u00df Cremsier in seine Klauen/", "tokens": ["Be\u00b7kam", "auch", "\u00fc\u00b7ber", "di\u00df", "Crem\u00b7sier", "in", "sei\u00b7ne", "Klau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PDS", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.161": {"text": "und lie\u00df zw\u00f6lff hundert Mann hierum darnieder hauen/", "tokens": ["und", "lie\u00df", "zw\u00f6lff", "hun\u00b7dert", "Mann", "hie\u00b7rum", "dar\u00b7nie\u00b7der", "hau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "CARD", "CARD", "NN", "ADV", "PAV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Worzu Helm Wrangel sich sehr tapfer brauchen lie\u00df/", "tokens": ["Wor\u00b7zu", "Helm", "Wran\u00b7gel", "sich", "sehr", "tap\u00b7fer", "brau\u00b7chen", "lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "PRF", "ADV", "ADJD", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "Der wie ein schneller Strohm bi\u00df an die Donau ri\u00df/", "tokens": ["Der", "wie", "ein", "schnel\u00b7ler", "Strohm", "bi\u00df", "an", "die", "Do\u00b7nau", "ri\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "KOKOM", "ART", "ADJA", "NN", "APPR", "APPR", "ART", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "und mit drey tausend Mann die Wiener Br\u00fccken trutzte/", "tokens": ["und", "mit", "drey", "tau\u00b7send", "Mann", "die", "Wie\u00b7ner", "Br\u00fc\u00b7cken", "trutz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "CARD", "NN", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.165": {"text": "So/ da\u00df die gro\u00dfe Stadt hierob nicht wenig stutzte.", "tokens": ["So", "/", "da\u00df", "die", "gro\u00b7\u00dfe", "Stadt", "hier\u00b7ob", "nicht", "we\u00b7nig", "stutz\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KOUS", "ART", "ADJA", "NN", "ADV", "PTKNEG", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.166": {"text": "Indessen satzten sich die K\u00e4ysrischen bey Brinn/", "tokens": ["In\u00b7des\u00b7sen", "satz\u00b7ten", "sich", "die", "K\u00e4y\u00b7sri\u00b7schen", "bey", "Brinn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "und sahen zu wie es den Schwedischen nach Sinn\u2019", "tokens": ["und", "sa\u00b7hen", "zu", "wie", "es", "den", "Schwe\u00b7di\u00b7schen", "nach", "Sinn'"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PWAV", "PPER", "ART", "NN", "APPR", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.168": {"text": "und Hertzens-Wundsch ergieng. Sie kunten es nicht \u00e4n-", "tokens": ["und", "Hert\u00b7zens\u00b7Wundsch", "er\u00b7gieng", ".", "Sie", "kun\u00b7ten", "es", "nicht", "\u00e4n"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVFIN", "$.", "PPER", "VMFIN", "PPER", "PTKNEG", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.169": {"text": "Nicht b\u00e4sser hatten es die in den Ober-L\u00e4ndern/", "tokens": ["Nicht", "b\u00e4s\u00b7ser", "hat\u00b7ten", "es", "die", "in", "den", "O\u00b7ber\u00b7L\u00e4n\u00b7dern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VAFIN", "PPER", "ART", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Dann jhnen der Frantzo\u00df und He\u00df zu m\u00e4chtig kam/", "tokens": ["Dann", "jh\u00b7nen", "der", "Frant\u00b7zo\u00df", "und", "He\u00df", "zu", "m\u00e4ch\u00b7tig", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "KON", "NE", "PTKA", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.171": {"text": "Das meiste Franckenland in seine Waffen nahm", "tokens": ["Das", "meis\u00b7te", "Fran\u00b7cken\u00b7land", "in", "sei\u00b7ne", "Waf\u00b7fen", "nahm"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "und gro\u00dfes Geld erhob. Man stritt fast in die Wettte/", "tokens": ["und", "gro\u00b7\u00dfes", "Geld", "er\u00b7hob", ".", "Man", "stritt", "fast", "in", "die", "Wett\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVFIN", "$.", "PIS", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.173": {"text": "Wer unser Deutsches Reich/ auch dessen Haab und St\u00e4dte", "tokens": ["Wer", "un\u00b7ser", "Deut\u00b7sches", "Reich", "/", "auch", "des\u00b7sen", "Ha\u00b7ab", "und", "St\u00e4d\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPOSAT", "ADJA", "NN", "$(", "ADV", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-++-+-", "measure": "unknown.measure.septa"}, "line.174": {"text": "Am b\u00e4sten rupffen k\u00f6nnt. Hielt M\u00e4hren sattsam aus/", "tokens": ["Am", "b\u00e4s\u00b7ten", "rupf\u00b7fen", "k\u00f6nnt", ".", "Hielt", "M\u00e4h\u00b7ren", "satt\u00b7sam", "aus", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "$.", "VVFIN", "NN", "ADJD", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "Noch \u00e4rger hielt der Frantz und He\u00df mit Francken Hau\u00df.", "tokens": ["Noch", "\u00e4r\u00b7ger", "hielt", "der", "Frantz", "und", "He\u00df", "mit", "Fran\u00b7cken", "Hau\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "KON", "NE", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.176": {"text": "Selbst W\u00fcrtzburg muste sich zum Geld-bezahlen beugen/", "tokens": ["Selbst", "W\u00fcrtz\u00b7burg", "mus\u00b7te", "sich", "zum", "Geld\u00b7be\u00b7zah\u00b7len", "beu\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VMFIN", "PRF", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "und dort sah man Krumau in einem Sturm besteigen/", "tokens": ["und", "dort", "sah", "man", "Kru\u00b7mau", "in", "ei\u00b7nem", "Sturm", "be\u00b7stei\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "NN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+--++-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.178": {"text": "Wo man viel Adel-Volck mit gro\u00dfem Haab bekam/", "tokens": ["Wo", "man", "viel", "A\u00b7del\u00b7Volck", "mit", "gro\u00b7\u00dfem", "Ha\u00b7ab", "be\u00b7kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PIAT", "NN", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.179": {"text": "und dann auch Klempenau also mit st\u00fcrmen nahm.", "tokens": ["und", "dann", "auch", "Klem\u00b7pe\u00b7nau", "al\u00b7so", "mit", "st\u00fcr\u00b7men", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "NE", "ADV", "APPR", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "Es fundten sich auch jetzt viel tausend von Wallachen/", "tokens": ["Es", "fund\u00b7ten", "sich", "auch", "jetzt", "viel", "tau\u00b7send", "von", "Wal\u00b7la\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "ADV", "ADV", "CARD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+++-", "measure": "unknown.measure.septa"}, "line.181": {"text": "Der Schweden gro\u00dfe Macht noch m\u00e4chtiger zu machen.", "tokens": ["Der", "Schwe\u00b7den", "gro\u00b7\u00dfe", "Macht", "noch", "m\u00e4ch\u00b7ti\u00b7ger", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADJA", "NN", "ADV", "ADJA", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Doch es war viel Geschrey und wenig Woll hierbey/", "tokens": ["Doch", "es", "war", "viel", "Ge\u00b7schrey", "und", "we\u00b7nig", "Woll", "hier\u00b7bey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PIAT", "NN", "KON", "PIS", "VMFIN", "ADV", "$("], "meter": "-++--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.183": {"text": "und sie verstoben auch so leichtlich als die Spreu.", "tokens": ["und", "sie", "ver\u00b7sto\u00b7ben", "auch", "so", "leicht\u00b7lich", "als", "die", "Spreu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADV", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Da\u00df gleichwol auch der Schwed nicht gar zu sicher siegte/", "tokens": ["Da\u00df", "gleich\u00b7wol", "auch", "der", "Schwed", "nicht", "gar", "zu", "si\u00b7cher", "sieg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "ART", "NN", "PTKNEG", "ADV", "PTKA", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.185": {"text": "Kam eine Feinds-Parthey/ die unter Bucheim kriegte/", "tokens": ["Kam", "ei\u00b7ne", "Feinds\u00b7Par\u00b7they", "/", "die", "un\u00b7ter", "Buc\u00b7heim", "krieg\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$(", "ART", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "und schlug mit leichter M\u00fch drey Regimenter ab.", "tokens": ["und", "schlug", "mit", "leich\u00b7ter", "M\u00fch", "drey", "Re\u00b7gi\u00b7men\u00b7ter", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Warauf der Torsten Sohn dem Feind ein Billichs gab/", "tokens": ["Wa\u00b7rauf", "der", "Tors\u00b7ten", "Sohn", "dem", "Feind", "ein", "Bil\u00b7lichs", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "ART", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "Zween Obristen hiervon/ bey jhnen noch gefangen/", "tokens": ["Zween", "O\u00b7bris\u00b7ten", "hier\u00b7von", "/", "bey", "jh\u00b7nen", "noch", "ge\u00b7fan\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "PAV", "$(", "APPR", "PPER", "ADV", "ADJD", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.189": {"text": "Zu l\u00f6sen. Es geschah. ", "tokens": ["Zu", "l\u00f6\u00b7sen", ".", "Es", "ge\u00b7schah", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "PPER", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.190": {"text": "Der ander aber must\u2019 entehret von den Heer.", "tokens": ["Der", "an\u00b7der", "a\u00b7ber", "must'", "en\u00b7teh\u00b7ret", "von", "den", "Heer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "VMFIN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.191": {"text": "Das war so viel gesagt r Es thu es keiner mehr/", "tokens": ["Das", "war", "so", "viel", "ge\u00b7sagt", "r", "Es", "thu", "es", "kei\u00b7ner", "mehr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "VVPP", "VVFIN", "PPER", "VVFIN", "PPER", "PIS", "ADV", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.192": {"text": "Da\u00df er sein Leben rett\u2019 und seinen Ruhm verl\u00fchre.", "tokens": ["Da\u00df", "er", "sein", "Le\u00b7ben", "rett'", "und", "sei\u00b7nen", "Ruhm", "ver\u00b7l\u00fch\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVFIN", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "Denckt/ da\u00df man sein Gewehr um Ehr\u2019 und Leben f\u00fchre.", "tokens": ["Denckt", "/", "da\u00df", "man", "sein", "Ge\u00b7wehr", "um", "Ehr'", "und", "Le\u00b7ben", "f\u00fch\u00b7re", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "KOUS", "PIS", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Um dieser Zeiten Lauff erhob in Schweinitz sich", "tokens": ["Um", "die\u00b7ser", "Zei\u00b7ten", "Lauff", "er\u00b7hob", "in", "Schwei\u00b7nitz", "sich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PDAT", "NN", "NN", "VVFIN", "APPR", "NE", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Ein gro\u00df Verr\u00e4ther-Werck/ die Schweden j\u00e4mmerlich", "tokens": ["Ein", "gro\u00df", "Ver\u00b7r\u00e4ther\u00b7\u00b7Werck", "/", "die", "Schwe\u00b7den", "j\u00e4m\u00b7mer\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "$(", "ART", "NE", "ADJD"], "meter": "-+-++-+-+-+", "measure": "unknown.measure.hexa"}, "line.196": {"text": "Zu morden. Doch es wurd in Zeiten kundt und machte/", "tokens": ["Zu", "mor\u00b7den", ".", "Doch", "es", "wurd", "in", "Zei\u00b7ten", "kundt", "und", "mach\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "KON", "PPER", "VAFIN", "APPR", "NN", "PTKVZ", "KON", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Da\u00df man ein zwantzig Mann in das Gef\u00e4ngni\u00df brachte/", "tokens": ["Da\u00df", "man", "ein", "zwant\u00b7zig", "Mann", "in", "das", "Ge\u00b7f\u00e4ng\u00b7ni\u00df", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "CARD", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.198": {"text": "und sieben t\u00f6dten lie\u00df. Ich bin von M\u00e4hren ab/", "tokens": ["und", "sie\u00b7ben", "t\u00f6d\u00b7ten", "lie\u00df", ".", "Ich", "bin", "von", "M\u00e4h\u00b7ren", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "VVINF", "VVFIN", "$.", "PPER", "VAFIN", "APPR", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "und seh herum wie sich des. Gubrians Heer gehab/", "tokens": ["und", "seh", "he\u00b7rum", "wie", "sich", "des", ".", "Gu\u00b7bri\u00b7ans", "Heer", "ge\u00b7hab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "KOKOM", "PRF", "ART", "$.", "NE", "NN", "VVFIN", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.200": {"text": "Und wie Jean de W\u00f6rt/ ein Krieg\u00dfman b\u00e4ster ma\u00dfen/", "tokens": ["Und", "wie", "Jean", "de", "W\u00f6rt", "/", "ein", "Krieg\u00df\u00b7man", "b\u00e4s\u00b7ter", "ma\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "NE", "NE", "NE", "$(", "ART", "NN", "ADJD", "VVFIN", "$("], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}, "line.201": {"text": "Nu gegen Gustav Horn von Franckreich lo\u00df gela\u00dfen/", "tokens": ["Nu", "ge\u00b7gen", "Gus\u00b7tav", "Horn", "von", "Fran\u00b7ck\u00b7reich", "lo\u00df", "ge\u00b7la\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "NE", "APPR", "NE", "ADV", "VVINF", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.202": {"text": "Sich neu entgegen st\u00e4ll\u2019. Als sich die Frantzen-Schaar", "tokens": ["Sich", "neu", "ent\u00b7ge\u00b7gen", "st\u00e4ll'", ".", "Als", "sich", "die", "Frant\u00b7zen\u00b7Schaar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "PTKVZ", "VVFIN", "$.", "KOUS", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Aus Franckten weg begab und auf dem Wege war", "tokens": ["Aus", "Franck\u00b7ten", "weg", "be\u00b7gab", "und", "auf", "dem", "We\u00b7ge", "war"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "ADV", "VVFIN", "KON", "APPR", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "Nach W\u00fcrtenberg zu gehn/ gieng jhr de W\u00f6rt in R\u00fccken", "tokens": ["Nach", "W\u00fcr\u00b7ten\u00b7berg", "zu", "gehn", "/", "gieng", "jhr", "de", "W\u00f6rt", "in", "R\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKZU", "VVINF", "$(", "VVFIN", "PPER", "NE", "NE", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "und schlug bey Schorendorff/ mit ziemlichen begl\u00fccken/", "tokens": ["und", "schlug", "bey", "Scho\u00b7ren\u00b7dorff", "/", "mit", "ziem\u00b7li\u00b7chen", "be\u00b7gl\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "$(", "APPR", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Ein gutes Theil davon. Er nahm auch Gopping ein/", "tokens": ["Ein", "gu\u00b7tes", "Theil", "da\u00b7von", ".", "Er", "nahm", "auch", "Gop\u00b7ping", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "ADV", "NE", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Hief\u00fcr must Reutlingen der Frantzen Beuthe seyn.", "tokens": ["Hie\u00b7f\u00fcr", "must", "Reut\u00b7lin\u00b7gen", "der", "Frant\u00b7zen", "Beu\u00b7the", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "NN", "ART", "NN", "VVFIN", "VAINF", "$."], "meter": "----+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.208": {"text": "Noch nahm der schnelle W\u00f6rt acht hundert Frantzen Wa-", "tokens": ["Noch", "nahm", "der", "schnel\u00b7le", "W\u00f6rt", "acht", "hun\u00b7dert", "Frant\u00b7zen", "Wa"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "CARD", "CARD", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Hergegen wurden jhm vier hundert Mann erschlagen.", "tokens": ["Her\u00b7ge\u00b7gen", "wur\u00b7den", "jhm", "vier", "hun\u00b7dert", "Mann", "er\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "CARD", "CARD", "NN", "VVPP", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.210": {"text": "Noch schlug er sehr begl\u00fcckt drey Regimenter ab/", "tokens": ["Noch", "schlug", "er", "sehr", "be\u00b7gl\u00fcckt", "drey", "Re\u00b7gi\u00b7men\u00b7ter", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "VVPP", "CARD", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.211": {"text": "Da doch der Rosa das jhm schleunig wieder gab/", "tokens": ["Da", "doch", "der", "Ro\u00b7sa", "das", "jhm", "schleu\u00b7nig", "wie\u00b7der", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "ART", "PPER", "ADJD", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "und zwar in gleicher Zahl. Noch war auf beyden Seiten", "tokens": ["und", "zwar", "in", "glei\u00b7cher", "Zahl", ".", "Noch", "war", "auf", "bey\u00b7den", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "$.", "ADV", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Nicht weit von Ravenspurg ein zimlich hitzig streiten/", "tokens": ["Nicht", "weit", "von", "Ra\u00b7ven\u00b7spurg", "ein", "zim\u00b7lich", "hit\u00b7zig", "strei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "APPR", "NE", "ART", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Da\u00df jeder gleich verlohr. Worauf de Gubrian", "tokens": ["Da\u00df", "je\u00b7der", "gleich", "ver\u00b7lohr", ".", "Wo\u00b7rauf", "de", "Gu\u00b7bri\u00b7an"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "VVFIN", "$.", "PAV", "NE", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "Das Rothweil rund beschlo\u00df. Es wurd\u2019 umsonst gethan/", "tokens": ["Das", "Roth\u00b7weil", "rund", "be\u00b7schlo\u00df", ".", "Es", "wurd'", "um\u00b7sonst", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVFIN", "$.", "PPER", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Weil es de W\u00f6rt entsatzt. Ein Monat kaum verflossen/", "tokens": ["Weil", "es", "de", "W\u00f6rt", "ent\u00b7satzt", ".", "Ein", "Mo\u00b7nat", "kaum", "ver\u00b7flos\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NE", "NE", "VVPP", "$.", "ART", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "War es zum andernmahl von Gubrian beschlossen.", "tokens": ["War", "es", "zum", "an\u00b7dern\u00b7mahl", "von", "Gu\u00b7bri\u00b7an", "be\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPRART", "ADV", "APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "De W\u00f6rt that wiederum so viel jhm m\u00f6glich war/", "tokens": ["De", "W\u00f6rt", "that", "wie\u00b7de\u00b7rum", "so", "viel", "jhm", "m\u00f6g\u00b7lich", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "ADV", "ADV", "ADV", "PPER", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.219": {"text": "Dem Ort Entsatz zu thun/ doch es war seine Schaar", "tokens": ["Dem", "Ort", "Ent\u00b7satz", "zu", "thun", "/", "doch", "es", "war", "sei\u00b7ne", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$(", "KON", "PPER", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.220": {"text": "Zu wenig/ Rothweil must an Gubrian sich ergeben/", "tokens": ["Zu", "we\u00b7nig", "/", "Roth\u00b7weil", "must", "an", "Gu\u00b7bri\u00b7an", "sich", "er\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$(", "NN", "VMFIN", "APPR", "NE", "PRF", "VVPP", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.221": {"text": "Es galt jhm aber selbst den Arm und auch das Leben.", "tokens": ["Es", "galt", "jhm", "a\u00b7ber", "selbst", "den", "Arm", "und", "auch", "das", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADV", "ART", "NN", "KON", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Hierauf vertheilte sich das Weymar-Frantzen Heer", "tokens": ["Hier\u00b7auf", "ver\u00b7theil\u00b7te", "sich", "das", "Wey\u00b7ma\u00b7rFrant\u00b7zen", "Heer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PRF", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "(die Hessen hielten sich um diese Zeit nicht mehr", "tokens": ["(", "die", "Hes\u00b7sen", "hiel\u00b7ten", "sich", "um", "die\u00b7se", "Zeit", "nicht", "mehr"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ART", "NN", "VVFIN", "PRF", "APPR", "PDAT", "NN", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.224": {"text": "Bey diesen V\u00f6lckern auf/ sie kriegten gantz besunders", "tokens": ["Bey", "die\u00b7sen", "V\u00f6l\u00b7ckern", "auf", "/", "sie", "krieg\u00b7ten", "gantz", "be\u00b7sun\u00b7ders"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "APPR", "$(", "PPER", "VVFIN", "ADV", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "und machten dort und da den P\u00e4pstischen vtel Wunders)", "tokens": ["und", "mach\u00b7ten", "dort", "und", "da", "den", "P\u00e4ps\u00b7ti\u00b7schen", "vtel", "Wun\u00b7ders", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "KON", "ADV", "ART", "NN", "NE", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "um D\u00fcttlingen herum/ und lag in guter Ruh.", "tokens": ["um", "D\u00fctt\u00b7lin\u00b7gen", "he\u00b7rum", "/", "und", "lag", "in", "gu\u00b7ter", "Ruh", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKVZ", "$(", "KON", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.227": {"text": "Doch es schlug unverhofft ein gro\u00dfes ", "tokens": ["Doch", "es", "schlug", "un\u00b7ver\u00b7hofft", "ein", "gro\u00b7\u00dfes"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.228": {"text": "In dem die Hatzfeld-W\u00f6rt- und Lotheringsche kamen", "tokens": ["In", "dem", "die", "Hatz\u00b7feld\u00b7W\u00f6r\u00b7t", "und", "Lo\u00b7the\u00b7ring\u00b7sche", "ka\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "TRUNC", "KON", "NN", "VVFIN"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.229": {"text": "und fast das gantze Heer ", "tokens": ["und", "fast", "das", "gant\u00b7ze", "Heer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.230": {"text": "Nur Rosa kam davon und etwas Reuterey.", "tokens": ["Nur", "Ro\u00b7sa", "kam", "da\u00b7von", "und", "et\u00b7was", "Reu\u00b7te\u00b7rey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PAV", "KON", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Damit kam Rotweil auch von seinen Feinden frey.", "tokens": ["Da\u00b7mit", "kam", "Rot\u00b7weil", "auch", "von", "sei\u00b7nen", "Fein\u00b7den", "frey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NN", "ADV", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "In dem di\u00df hier verlieff/ hielt Torsten Sohn in M\u00e4hren", "tokens": ["In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "/", "hielt", "Tors\u00b7ten", "Sohn", "in", "M\u00e4h\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "PDS", "ADV", "VVFIN", "$(", "VVFIN", "NN", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Bey Neustadt sieben Tag\u2019 im Felde/ mit begehren/", "tokens": ["Bey", "Neu\u00b7stadt", "sie\u00b7ben", "Tag'", "im", "Fel\u00b7de", "/", "mit", "be\u00b7geh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "CARD", "NN", "APPRART", "NN", "$(", "APPR", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "Da\u00df Gallas/ welcher nun von neuem Feldherr war/", "tokens": ["Da\u00df", "Gal\u00b7las", "/", "wel\u00b7cher", "nun", "von", "neu\u00b7em", "Feld\u00b7herr", "war", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "$(", "PRELS", "ADV", "APPR", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "(dann Piccolomini gieng nach der Spannschen Schaar", "tokens": ["(", "dann", "Pic\u00b7co\u00b7lo\u00b7mi\u00b7ni", "gieng", "nach", "der", "Spann\u00b7schen", "Schaar"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "NE", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "In Catnlonien/ den Frantzen abzuwehren)", "tokens": ["In", "Catn\u00b7lo\u00b7ni\u00b7en", "/", "den", "Frant\u00b7zen", "ab\u00b7zu\u00b7weh\u00b7ren", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "ART", "NN", "VVIZU", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.237": {"text": "Sich einmal schlagen m\u00f6cht\u2019/ hier aber war kein h\u00f6ren.", "tokens": ["Sich", "ein\u00b7mal", "schla\u00b7gen", "m\u00f6cht'", "/", "hier", "a\u00b7ber", "war", "kein", "h\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVINF", "VMFIN", "$(", "ADV", "ADV", "VAFIN", "PIAT", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "Daher er sich mit Macht nach Eulenburg erhob/", "tokens": ["Da\u00b7her", "er", "sich", "mit", "Macht", "nach", "Eu\u00b7len\u00b7burg", "er\u00b7hob", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "PRF", "APPR", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "und lag dem festen Ort in wenig Tagen ob.", "tokens": ["und", "lag", "dem", "fes\u00b7ten", "Ort", "in", "we\u00b7nig", "Ta\u00b7gen", "ob", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "KOUS", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "Wo eine Million von Geld und andern Sachen", "tokens": ["Wo", "ei\u00b7ne", "Mil\u00b7li\u00b7on", "von", "Geld", "und", "an\u00b7dern", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "APPR", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "In seine H\u00e4nde fiel. Hie\u00df das nicht Beuthe machen?", "tokens": ["In", "sei\u00b7ne", "H\u00e4n\u00b7de", "fiel", ".", "Hie\u00df", "das", "nicht", "Beu\u00b7the", "ma\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "VVFIN", "PDS", "PTKNEG", "NN", "VVINF", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.242": {"text": "Fast halb so viel bekam der Wrangel auch vor Brinn.", "tokens": ["Fast", "halb", "so", "viel", "be\u00b7kam", "der", "Wran\u00b7gel", "auch", "vor", "Brinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "ADV", "VVFIN", "ART", "NN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Das Land war sehr ersch\u00f6pfft/ noch war solch Gut darin.", "tokens": ["Das", "Land", "war", "sehr", "er\u00b7sch\u00f6pfft", "/", "noch", "war", "solch", "Gut", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVPP", "$(", "ADV", "VAFIN", "PIAT", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.244": {"text": "Als di\u00df geschehen war lie\u00df er viel Pl\u00e4tze st\u00e4rcken/", "tokens": ["Als", "di\u00df", "ge\u00b7sche\u00b7hen", "war", "lie\u00df", "er", "viel", "Pl\u00e4t\u00b7ze", "st\u00e4r\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "VVPP", "VAFIN", "VVFIN", "PPER", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.245": {"text": "So wol mit gutem Volck/ als auch mit festen Wercken/", "tokens": ["So", "wol", "mit", "gu\u00b7tem", "Volck", "/", "als", "auch", "mit", "fes\u00b7ten", "Wer\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ADJA", "NN", "$(", "KOKOM", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.246": {"text": "und gieng nach Schlesien mit einer gro\u00dfen Beuth\u2019.", "tokens": ["und", "gieng", "nach", "Schle\u00b7si\u00b7en", "mit", "ei\u00b7ner", "gro\u00b7\u00dfen", "Beuth'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "Es hieb jhm Ger\u00dfdorff nach/ wor\u00fcber solcher Streit", "tokens": ["Es", "hieb", "jhm", "Ger\u00df\u00b7dorff", "nach", "/", "wo\u00b7r\u00fc\u00b7ber", "sol\u00b7cher", "Streit"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "NE", "APPR", "$(", "PWAV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "Entstunde/ da\u00df nicht viel zu r\u00fccke sind gekommen/", "tokens": ["Ent\u00b7stun\u00b7de", "/", "da\u00df", "nicht", "viel", "zu", "r\u00fc\u00b7cke", "sind", "ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KOUS", "PTKNEG", "ADV", "PTKZU", "VVFIN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Halb wurd en sie entsehlt/ halb in Verhafft genommen.", "tokens": ["Halb", "wurd", "en", "sie", "ent\u00b7sehlt", "/", "halb", "in", "Ver\u00b7hafft", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "FM", "PPER", "VVFIN", "$(", "ADJD", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Also ergieng es auch dem Buchheim/ dessen Schaar", "tokens": ["Al\u00b7so", "er\u00b7gieng", "es", "auch", "dem", "Buch\u00b7heim", "/", "des\u00b7sen", "Schaar"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "$(", "PRELAT", "NN"], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}, "line.251": {"text": "Von funffzehnhundert Mann des Gerstorffs Meynung", "tokens": ["Von", "funff\u00b7zehn\u00b7hun\u00b7dert", "Mann", "des", "Ger\u00b7storffs", "Mey\u00b7nung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "NN", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.252": {"text": "war.", "tokens": ["war", "."], "token_info": ["word", "punct"], "pos": ["VAFIN", "$."], "meter": "-", "measure": "single.down"}, "line.253": {"text": "Ein tausend blieben todt/ bey Masteritz erschlagen/", "tokens": ["Ein", "tau\u00b7send", "blie\u00b7ben", "todt", "/", "bey", "Mas\u00b7te\u00b7ritz", "er\u00b7schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "VVFIN", "ADJD", "$(", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Er selbst vermochte kaum dem ", "tokens": ["Er", "selbst", "ver\u00b7moch\u00b7te", "kaum", "dem"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "ADV", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.255": {"text": "Er kam aufs dritie Pferd. Als nun der Torsten Sohn", "tokens": ["Er", "kam", "aufs", "dri\u00b7tie", "Pferd", ".", "Als", "nun", "der", "Tors\u00b7ten", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "ADJA", "NN", "$.", "KOUS", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Das Schlesjen hatt\u2019 erreicht/ lie\u00df er das Brieg/ wovon", "tokens": ["Das", "Schles\u00b7jen", "hatt'", "er\u00b7reicht", "/", "lie\u00df", "er", "das", "Brieg", "/", "wo\u00b7von"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$(", "VVFIN", "PPER", "ART", "NN", "$(", "PWAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "Er vormahls nichts erhielt/ von neuem starck besch\u00fc\u00dfen/", "tokens": ["Er", "vor\u00b7mahls", "nichts", "er\u00b7hielt", "/", "von", "neu\u00b7em", "starck", "be\u00b7sch\u00fc\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PIS", "VVFIN", "$(", "APPR", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "Doch es wolt auch anjetzt gantz nichts zu Willen wissen/", "tokens": ["Doch", "es", "wolt", "auch", "an\u00b7jetzt", "gantz", "nichts", "zu", "Wil\u00b7len", "wis\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "ADV", "ADV", "ADV", "PIS", "APPR", "NN", "VVINF", "$("], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.259": {"text": "und wiesen Brinn und Brieg die gr\u00f6ste Gegenwehr.", "tokens": ["und", "wie\u00b7sen", "Brinn", "und", "Brieg", "die", "gr\u00f6s\u00b7te", "Ge\u00b7gen\u00b7wehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "KON", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "Ich la\u00df jhn hier vor Brieg und sehe nach dem Heer/", "tokens": ["Ich", "la\u00df", "jhn", "hier", "vor", "Brieg", "und", "se\u00b7he", "nach", "dem", "Heer", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "NN", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.261": {"text": "Das sich nach Pommern gab/ vom Crakau hingef\u00fchret.", "tokens": ["Das", "sich", "nach", "Pom\u00b7mern", "gab", "/", "vom", "Cra\u00b7kau", "hin\u00b7ge\u00b7f\u00fch\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "PRF", "APPR", "NE", "VVFIN", "$(", "APPRART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Es hatte dieses Land sich wieder au\u00dfgezieret/", "tokens": ["Es", "hat\u00b7te", "die\u00b7ses", "Land", "sich", "wie\u00b7der", "au\u00df\u00b7ge\u00b7zie\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PDAT", "NN", "PRF", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "Dann es des Krieges-Volcks sehr wol enthoben war.", "tokens": ["Dann", "es", "des", "Krie\u00b7ge\u00b7sVolcks", "sehr", "wol", "ent\u00b7ho\u00b7ben", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "ADV", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Di\u00df nahm sein Feind in acht und lie\u00df des Crakau Schaar/", "tokens": ["Di\u00df", "nahm", "sein", "Feind", "in", "acht", "und", "lie\u00df", "des", "Cra\u00b7kau", "Schaar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "APPR", "CARD", "KON", "VVFIN", "ART", "NE", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "Von etlich tausend Mann/ dasselbig \u00fcberziehen/", "tokens": ["Von", "et\u00b7lich", "tau\u00b7send", "Mann", "/", "das\u00b7sel\u00b7big", "\u00fc\u00b7ber\u00b7zie\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJD", "CARD", "NN", "$(", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.266": {"text": "Des Trosts/ es w\u00fcrde sich der Torsten Sohn bem\u00fchen", "tokens": ["Des", "Trosts", "/", "es", "w\u00fcr\u00b7de", "sich", "der", "Tors\u00b7ten", "Sohn", "be\u00b7m\u00fc\u00b7hen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "PPER", "VAFIN", "PRF", "ART", "NN", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.267": {"text": "Dem Pommern Hilff zu thun/ und so der M\u00e4hrer Land", "tokens": ["Dem", "Pom\u00b7mern", "Hilff", "zu", "thun", "/", "und", "so", "der", "M\u00e4h\u00b7rer", "Land"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$(", "KON", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Verlassen. ", "tokens": ["Ver\u00b7las\u00b7sen", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.269": {"text": "Doch wenig mit Bestand/ als es zu kommen pfleget/", "tokens": ["Doch", "we\u00b7nig", "mit", "Be\u00b7stand", "/", "als", "es", "zu", "kom\u00b7men", "pfle\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "$(", "KOUS", "PPER", "PTKZU", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Wann man sich nur allein auf Raub und Pl\u00fcndern leget.", "tokens": ["Wann", "man", "sich", "nur", "al\u00b7lein", "auf", "Raub", "und", "Pl\u00fcn\u00b7dern", "le\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "ADV", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Er nahm bey Belgard Stand und satzt\u2019 ein Lager auff.", "tokens": ["Er", "nahm", "bey", "Bel\u00b7gard", "Stand", "und", "satzt'", "ein", "La\u00b7ger", "auff", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "NN", "KON", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Kaum da/ kam K\u00f6nigsmarck in einem vollem Lauff", "tokens": ["Kaum", "da", "/", "kam", "K\u00f6\u00b7nigs\u00b7marck", "in", "ei\u00b7nem", "vol\u00b7lem", "Lauff"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$(", "VVFIN", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Auf jhn und satzte sich nechst seinem Lager nieder.", "tokens": ["Auf", "jhn", "und", "satz\u00b7te", "sich", "nechst", "sei\u00b7nem", "La\u00b7ger", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "KON", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "Der Krakau lang besetzt/ sah endlich hin und wieder", "tokens": ["Der", "Kra\u00b7kau", "lang", "be\u00b7setzt", "/", "sah", "end\u00b7lich", "hin", "und", "wie\u00b7der"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "VVPP", "$(", "VVFIN", "ADV", "PTKVZ", "KON", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "Wie er entkommen m\u00f6cht\u2019/ zur Schlacht hierau\u00df zu gehn/", "tokens": ["Wie", "er", "ent\u00b7kom\u00b7men", "m\u00f6cht'", "/", "zur", "Schlacht", "hier\u00b7au\u00df", "zu", "gehn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVINF", "VMFIN", "$(", "APPRART", "NN", "PAV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "War gantz nicht Raht/ es war kein Vothel zu ersehn.", "tokens": ["War", "gantz", "nicht", "Raht", "/", "es", "war", "kein", "Vo\u00b7thel", "zu", "er\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "NN", "$(", "PPER", "VAFIN", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Er wust\u2019 auch \u00fcber di\u00df/ da\u00df Torsten Sohn sich regte/", "tokens": ["Er", "wust'", "auch", "\u00fc\u00b7ber", "di\u00df", "/", "da\u00df", "Tors\u00b7ten", "Sohn", "sich", "reg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDS", "$(", "KOUS", "NN", "NN", "PRF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.278": {"text": "Daher er/ eh man jhm die P\u00e4sse gantz verlegte/", "tokens": ["Da\u00b7her", "er", "/", "eh", "man", "jhm", "die", "P\u00e4s\u00b7se", "gantz", "ver\u00b7leg\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "$(", "KOUS", "PIS", "PPER", "ART", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "Sich recht durch Polen zog/ die Br\u00fccken niderri\u00df/", "tokens": ["Sich", "recht", "durch", "Po\u00b7len", "zog", "/", "die", "Br\u00fc\u00b7cken", "ni\u00b7der\u00b7ri\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "APPR", "NE", "VVFIN", "$(", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "und so in Schlesjen kam. Was er des Nachts verlie\u00df/", "tokens": ["und", "so", "in", "Schles\u00b7jen", "kam", ".", "Was", "er", "des", "Nachts", "ver\u00b7lie\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NE", "VVFIN", "$.", "PWS", "PPER", "ART", "ADV", "VVFIN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.281": {"text": "Nahm K\u00f6nigsmarck am Tag\u2019 Er war jhm auf den Solen/", "tokens": ["Nahm", "K\u00f6\u00b7nigs\u00b7marck", "am", "Tag'", "Er", "war", "jhm", "auf", "den", "So\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPRART", "NN", "PPER", "VAFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Vermocht\u2019 jhn aber doch zur Schlacht nicht einzuholen.", "tokens": ["Ver\u00b7mocht'", "jhn", "a\u00b7ber", "doch", "zur", "Schlacht", "nicht", "ein\u00b7zu\u00b7ho\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "APPRART", "NN", "PTKNEG", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "Weil jhn der Br\u00fcckenbruch zu viel verhinderte.", "tokens": ["Weil", "jhn", "der", "Br\u00fc\u00b7cken\u00b7bruch", "zu", "viel", "ver\u00b7hin\u00b7der\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Er wandte sich hierauf zu r\u00fccke nach der See/", "tokens": ["Er", "wand\u00b7te", "sich", "hier\u00b7auf", "zu", "r\u00fc\u00b7cke", "nach", "der", "See", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PAV", "PTKZU", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "und st\u00fcrmte Belgarts Wall/ den R\u00fccken rein zu haben/", "tokens": ["und", "st\u00fcrm\u00b7te", "Belg\u00b7arts", "Wall", "/", "den", "R\u00fc\u00b7cken", "rein", "zu", "ha\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "NE", "$(", "ART", "NN", "ADJD", "PTKZU", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.286": {"text": "Woranf die Feinde sich in seine Gunst ergaben.", "tokens": ["Wo\u00b7ranf", "die", "Fein\u00b7de", "sich", "in", "sei\u00b7ne", "Gunst", "er\u00b7ga\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.287": {"text": "In dem ein jederman der festen Meynung war/", "tokens": ["In", "dem", "ein", "je\u00b7der\u00b7man", "der", "fes\u00b7ten", "Mey\u00b7nung", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "PIS", "ART", "ADJA", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.288": {"text": "Es w\u00fcrde Torsten Sohn die kalte Zeit vom Jahr", "tokens": ["Es", "w\u00fcr\u00b7de", "Tors\u00b7ten", "Sohn", "die", "kal\u00b7te", "Zeit", "vom", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJA", "NN", "ART", "ADJA", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.289": {"text": "Jm Mei\u00dfen oder noch in Schlesien verbringen/", "tokens": ["Jm", "Mei\u00b7\u00dfen", "o\u00b7der", "noch", "in", "Schle\u00b7si\u00b7en", "ver\u00b7brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "ADV", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Sah man sein gantzes Heer sich schnell nach Holstein schwin-", "tokens": ["Sah", "man", "sein", "gant\u00b7zes", "Heer", "sich", "schnell", "nach", "Hol\u00b7stein", "schwin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PPOSAT", "ADJA", "NN", "PRF", "ADJD", "APPR", "NE", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Es war ein solcher Zug/ der kaum zu glauben schien.", "tokens": ["Es", "war", "ein", "sol\u00b7cher", "Zug", "/", "der", "kaum", "zu", "glau\u00b7ben", "schien", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "PIAT", "NN", "$(", "ART", "ADV", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Kiel hatte kaum die Post/ da war er schon darin.", "tokens": ["Kiel", "hat\u00b7te", "kaum", "die", "Post", "/", "da", "war", "er", "schon", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ART", "NN", "$(", "ADV", "VAFIN", "PPER", "ADV", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Da war kein Widerstand/ man wuste nichts als Frieden.", "tokens": ["Da", "war", "kein", "Wi\u00b7der\u00b7stand", "/", "man", "wus\u00b7te", "nichts", "als", "Frie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "$(", "PIS", "VVFIN", "PIS", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "So schleunig ist das Gl\u00fcck von manchem abgeschieden!", "tokens": ["So", "schleu\u00b7nig", "ist", "das", "Gl\u00fcck", "von", "man\u00b7chem", "ab\u00b7ge\u00b7schie\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "APPR", "PIAT", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.296": {"text": "Beschreib\u2019 ein andrer Kiel/ ich halte meinen ab.", "tokens": ["Be\u00b7schreib'", "ein", "an\u00b7drer", "Kiel", "/", "ich", "hal\u00b7te", "mei\u00b7nen", "ab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NE", "$(", "PPER", "VVFIN", "PPOSAT", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "Warum? Das bleibt bey mir. Ich habe doch zu reden.", "tokens": ["Wa\u00b7rum", "?", "Das", "bleibt", "bey", "mir", ".", "Ich", "ha\u00b7be", "doch", "zu", "re\u00b7den", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PDS", "VVFIN", "APPR", "PPER", "$.", "PPER", "VAFIN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Nu sich die gantze Macht der Siegs-gewohnten Schweden", "tokens": ["Nu", "sich", "die", "gant\u00b7ze", "Macht", "der", "Siegs\u00b7ge\u00b7wohn\u00b7ten", "Schwe\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PRF", "ART", "ADJA", "NN", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.299": {"text": "So weit vom Reiche gab/ war doch noch jemand da", "tokens": ["So", "weit", "vom", "Rei\u00b7che", "gab", "/", "war", "doch", "noch", "je\u00b7mand", "da"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "APPRART", "NE", "VVFIN", "$(", "VAFIN", "ADV", "ADV", "PIS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Der auf den K\u00e4yser gieng und jhn bekriegte? Ja.", "tokens": ["Der", "auf", "den", "K\u00e4y\u00b7ser", "gieng", "und", "jhn", "be\u00b7krieg\u00b7te", "?", "Ja", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "VVFIN", "KON", "PPER", "VVFIN", "$.", "PTKANT", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Das ", "tokens": ["Das"], "token_info": ["word"], "pos": ["PDS"], "meter": "-", "measure": "single.down"}, "line.302": {"text": "Sieh den Ragotzky da den scharffen S\u00e4bel fassen/", "tokens": ["Sieh", "den", "Ra\u00b7gotz\u00b7ky", "da", "den", "scharf\u00b7fen", "S\u00e4\u00b7bel", "fas\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NE", "ADV", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.303": {"text": "Mit drey\u00dfigtausend Mann in ", "tokens": ["Mit", "drey\u00b7\u00dfig\u00b7tau\u00b7send", "Mann", "in"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "CARD", "NN", "APPR"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.304": {"text": "und an der Schweden Statt vor einen Feind zu stehn.", "tokens": ["und", "an", "der", "Schwe\u00b7den", "Statt", "vor", "ei\u00b7nen", "Feind", "zu", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.305": {"text": "Er schlo\u00df ", "tokens": ["Er", "schlo\u00df"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+", "measure": "iambic.single"}, "line.306": {"text": "und nahm vom T\u00fcrcken Trost/ jhm alle Hilff zu schaffen/", "tokens": ["und", "nahm", "vom", "T\u00fcr\u00b7cken", "Trost", "/", "jhm", "al\u00b7le", "Hilff", "zu", "schaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "NN", "$(", "PPER", "PIAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Wann sie von n\u00f6then w\u00e4r\u2019. Ein zwantzig tausend Mann", "tokens": ["Wann", "sie", "von", "n\u00f6\u00b7then", "w\u00e4r'", ".", "Ein", "zwant\u00b7zig", "tau\u00b7send", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ADJA", "VAFIN", "$.", "ART", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Die solten allezeit jhm seyn zur Hand gethan.", "tokens": ["Die", "sol\u00b7ten", "al\u00b7le\u00b7zeit", "jhm", "seyn", "zur", "Hand", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "PPER", "VAINF", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Di\u00df neue Feuer bald in seiner Glut zu d\u00e4mpfen/", "tokens": ["Di\u00df", "neu\u00b7e", "Feu\u00b7er", "bald", "in", "sei\u00b7ner", "Glut", "zu", "d\u00e4mp\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "ADV", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.310": {"text": "Zog der von Buchheim auf/ mit solcher Macht zu k\u00e4mpfen.", "tokens": ["Zog", "der", "von", "Buch\u00b7heim", "auf", "/", "mit", "sol\u00b7cher", "Macht", "zu", "k\u00e4mp\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "NN", "APPR", "$(", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.311": {"text": "Ragozky zog zu r\u00fcck\u2019/ Er/ Buchheimb/ folgte nach/", "tokens": ["Ra\u00b7goz\u00b7ky", "zog", "zu", "r\u00fcck", "/", "Er", "/", "Buch\u00b7heimb", "/", "folg\u00b7te", "nach", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPR", "NE", "$(", "PPER", "$(", "NN", "$(", "VVFIN", "APPR", "$("], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}, "line.312": {"text": "Wurd\u2019 aber bald umringt/ jedoch/ ob schon so schwach/", "tokens": ["Wurd'", "a\u00b7ber", "bald", "um\u00b7ringt", "/", "je\u00b7doch", "/", "ob", "schon", "so", "schwach", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "VVPP", "$(", "ADV", "$(", "KOUS", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "Ohn einen gro\u00dfen Schlag vom Gl\u00fccke durch gef\u00fchret.", "tokens": ["Ohn", "ei\u00b7nen", "gro\u00b7\u00dfen", "Schlag", "vom", "Gl\u00fc\u00b7cke", "durch", "ge\u00b7f\u00fch\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPRART", "NN", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.314": {"text": "Worauf er dann Villeck/ von Feindes Macht ber\u00fchret/", "tokens": ["Wo\u00b7rauf", "er", "dann", "Vil\u00b7leck", "/", "von", "Fein\u00b7des", "Macht", "be\u00b7r\u00fch\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "NE", "$(", "APPR", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Entsatzt/ und neben dem viertausend Mann erschlug.", "tokens": ["Ent\u00b7satzt", "/", "und", "ne\u00b7ben", "dem", "vier\u00b7tau\u00b7send", "Mann", "er\u00b7schlug", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "KON", "APPR", "ART", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}, "line.316": {"text": "Ragozky hatt hiemit des Krieges fast genug/", "tokens": ["Ra\u00b7goz\u00b7ky", "hatt", "hie\u00b7mit", "des", "Krie\u00b7ges", "fast", "ge\u00b7nug", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PAV", "ART", "NN", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.317": {"text": "und dennoch kunte man zu keinem Frieden kommen/", "tokens": ["und", "den\u00b7noch", "kun\u00b7te", "man", "zu", "kei\u00b7nem", "Frie\u00b7den", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PIS", "APPR", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.318": {"text": "Was Mittel man hierzu hatt\u2019 immer vorgenommen.", "tokens": ["Was", "Mit\u00b7tel", "man", "hier\u00b7zu", "hatt'", "im\u00b7mer", "vor\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "PIS", "PAV", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.320": {"text": "Das lang-geplagte Land von seiner Last und M\u00fch", "tokens": ["Das", "lang\u00b7ge\u00b7plag\u00b7te", "Land", "von", "sei\u00b7ner", "Last", "und", "M\u00fch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.321": {"text": "Zu freyen. Dann es war jhr Vortheil in den Waffen/", "tokens": ["Zu", "frey\u00b7en", ".", "Dann", "es", "war", "jhr", "Vor\u00b7theil", "in", "den", "Waf\u00b7fen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "ADV", "PPER", "VAFIN", "PPOSAT", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.322": {"text": "und darum gab sie dort und da genug zu schaffen.", "tokens": ["und", "da\u00b7rum", "gab", "sie", "dort", "und", "da", "ge\u00b7nug", "zu", "schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "ADV", "KON", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.323": {"text": "Wir lassen ", "tokens": ["Wir", "las\u00b7sen"], "token_info": ["word", "word"], "pos": ["PPER", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.324": {"text": "Zu sehen/ ob auch da noch Feind\u2019 und Gegner seyn.", "tokens": ["Zu", "se\u00b7hen", "/", "ob", "auch", "da", "noch", "Feind'", "und", "Geg\u00b7ner", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KOUS", "ADV", "ADV", "ADV", "NN", "KON", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.325": {"text": "Erinnre dich/ wie man bey D\u00fcttlingen gestritten/", "tokens": ["E\u00b7rinn\u00b7re", "dich", "/", "wie", "man", "bey", "D\u00fctt\u00b7lin\u00b7gen", "ge\u00b7strit\u00b7ten", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$(", "PWAV", "PIS", "APPR", "NN", "VVPP", "$("], "meter": "-+----+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.326": {"text": "und was die Frantzen Macht daselbsten hab\u2019 erlltten.", "tokens": ["und", "was", "die", "Frant\u00b7zen", "Macht", "da\u00b7selbs\u00b7ten", "hab'", "erllt\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "NN", "VVINF", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.327": {"text": "Di\u00df trieb des Siegers Macht vor ", "tokens": ["Di\u00df", "trieb", "des", "Sie\u00b7gers", "Macht", "vor"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "NN", "APPR"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.328": {"text": "Wie auch vor andre Pl\u00e4tz\u2019 und er wurd Herr darin.", "tokens": ["Wie", "auch", "vor", "and\u00b7re", "Pl\u00e4tz'", "und", "er", "wurd", "Herr", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "APPR", "ADJA", "NN", "KON", "PPER", "VAFIN", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.329": {"text": "Er satzt\u2019 auch Freyburg zu/ da wolten sich die Frantzen", "tokens": ["Er", "satzt'", "auch", "Frey\u00b7burg", "zu", "/", "da", "wol\u00b7ten", "sich", "die", "Frant\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "NN", "PTKZU", "$(", "ADV", "VMFIN", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "Beweisen/ wie sie dann mit Sturm des Feindes Schantzen", "tokens": ["Be\u00b7wei\u00b7sen", "/", "wie", "sie", "dann", "mit", "Sturm", "des", "Fein\u00b7des", "Schant\u00b7zen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$(", "PWAV", "PPER", "ADV", "APPR", "NN", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Befielen/ und es kam zu einer grimmen Schlacht/", "tokens": ["Be\u00b7fie\u00b7len", "/", "und", "es", "kam", "zu", "ei\u00b7ner", "grim\u00b7men", "Schlacht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.332": {"text": "Der Hertzog von ", "tokens": ["Der", "Hert\u00b7zog", "von"], "token_info": ["word", "word", "word"], "pos": ["ART", "NE", "APPR"], "meter": "-+-+", "measure": "iambic.di"}, "line.333": {"text": "Fiel alles eufrig an/ und sparte keiner Knechte/", "tokens": ["Fiel", "al\u00b7les", "euf\u00b7rig", "an", "/", "und", "spar\u00b7te", "kei\u00b7ner", "Knech\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADJD", "PTKVZ", "$(", "KON", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Sah aber auch dabey/ da\u00df jhm in dem Gefechte", "tokens": ["Sah", "a\u00b7ber", "auch", "da\u00b7bey", "/", "da\u00df", "jhm", "in", "dem", "Ge\u00b7fech\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADV", "PAV", "$(", "KOUS", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Ein zw\u00f6lffmal hundert Mann verfielen. Der die Stirn", "tokens": ["Ein", "zw\u00f6lff\u00b7mal", "hun\u00b7dert", "Mann", "ver\u00b7fie\u00b7len", ".", "Der", "die", "Stirn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADV", "CARD", "NN", "VVINF", "$.", "ART", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.336": {"text": "An einem Felsen st\u00f6\u00dft/ verletzet das Gehirn.", "tokens": ["An", "ei\u00b7nem", "Fel\u00b7sen", "st\u00f6\u00dft", "/", "ver\u00b7let\u00b7zet", "das", "Ge\u00b7hirn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$(", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "Und dennoch kam es so/ da\u00df sich die Beyer-Schaaren/", "tokens": ["Und", "den\u00b7noch", "kam", "es", "so", "/", "da\u00df", "sich", "die", "Beyer\u00b7Schaa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "$(", "KOUS", "PRF", "ART", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.338": {"text": "Weil sie durch diesen Schlag selbst auch gebr\u00e4chlich waren/", "tokens": ["Weil", "sie", "durch", "die\u00b7sen", "Schlag", "selbst", "auch", "ge\u00b7br\u00e4ch\u00b7lich", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PDAT", "NN", "ADV", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.339": {"text": "Weit von besagter Stadt begaben/ dessen sich", "tokens": ["Weit", "von", "be\u00b7sag\u00b7ter", "Stadt", "be\u00b7ga\u00b7ben", "/", "des\u00b7sen", "sich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADJD", "APPR", "ADJA", "NN", "VVFIN", "$(", "PDS", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "Das Weymar-Frantzen-Heer sehr wol und meisterlich", "tokens": ["Das", "Wey\u00b7ma\u00b7rFrant\u00b7zen\u00b7Heer", "sehr", "wol", "und", "meis\u00b7ter\u00b7lich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "ADV", "KON", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.341": {"text": "Bediente/ seinen Zug l\u00e4ngst nach dem Reyhne setzte/", "tokens": ["Be\u00b7dien\u00b7te", "/", "sei\u00b7nen", "Zug", "l\u00e4ngst", "nach", "dem", "Reyh\u00b7ne", "setz\u00b7te", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.342": {"text": "und was jhm widrig fiel mit Macht darnider m\u00e4tzte.", "tokens": ["und", "was", "jhm", "wid\u00b7rig", "fiel", "mit", "Macht", "dar\u00b7ni\u00b7der", "m\u00e4tz\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADJD", "VVFIN", "APPR", "NN", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.343": {"text": "Die meisten St\u00e4dt hinab zur ", "tokens": ["Die", "meis\u00b7ten", "St\u00e4dt", "hin\u00b7ab", "zur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "ADV", "APPRART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.344": {"text": "Da jhm auch Philipsburg zur guten Beuthe blieb.", "tokens": ["Da", "jhm", "auch", "Phi\u00b7lips\u00b7burg", "zur", "gu\u00b7ten", "Beu\u00b7the", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NE", "APPRART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.345": {"text": "Indessen hatte sich der Torsten Sohn den Dehnen", "tokens": ["In\u00b7des\u00b7sen", "hat\u00b7te", "sich", "der", "Tors\u00b7ten", "Sohn", "den", "Deh\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PRF", "ART", "NN", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.346": {"text": "und Cymbren so bezeigt/ da\u00df man in gro\u00dfem sehnen", "tokens": ["und", "Cym\u00b7bren", "so", "be\u00b7zeigt", "/", "da\u00df", "man", "in", "gro\u00b7\u00dfem", "seh\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "ADV", "VVPP", "$(", "KOUS", "PIS", "APPR", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.347": {"text": "Neu frey zu leben war. ", "tokens": ["Neu", "frey", "zu", "le\u00b7ben", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.348": {"text": "Die K\u00e4yserliche Macht mit Gallas recht herzu/", "tokens": ["Die", "K\u00e4y\u00b7ser\u00b7li\u00b7che", "Macht", "mit", "Gal\u00b7las", "recht", "her\u00b7zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.349": {"text": "Die Schweden/ wie man sprach/ in einen Sack zu kriegen.", "tokens": ["Die", "Schwe\u00b7den", "/", "wie", "man", "sprach", "/", "in", "ei\u00b7nen", "Sack", "zu", "krie\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$(", "PWAV", "PIS", "VVFIN", "$(", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.350": {"text": "Wahr ists/ man sah die See an dreyen Seiten ligen/", "tokens": ["Wahr", "ists", "/", "man", "sah", "die", "See", "an", "drey\u00b7en", "Sei\u00b7ten", "li\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "$(", "PIS", "VVFIN", "ART", "NN", "APPR", "CARD", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.351": {"text": "Man sah der Dehnen Heer/ von vornen/ da das Loch", "tokens": ["Man", "sah", "der", "Deh\u00b7nen", "Heer", "/", "von", "vor\u00b7nen", "/", "da", "das", "Loch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "NN", "$(", "APPR", "VVINF", "$(", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.352": {"text": "Noch offen war/ da lag des Gallas Macht/ und doch", "tokens": ["Noch", "of\u00b7fen", "war", "/", "da", "lag", "des", "Gal\u00b7las", "Macht", "/", "und", "doch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "$(", "ADV", "VVFIN", "ART", "NN", "NN", "$(", "KON", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.353": {"text": "Gieng Linnert Torsten Sohn Meer/ Dehnen/ Gallas/", "tokens": ["Gieng", "Lin\u00b7nert", "Tors\u00b7ten", "Sohn", "Meer", "/", "Deh\u00b7nen", "/", "Gal\u00b7las", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "NE", "NN", "NN", "$(", "NN", "$(", "NE", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.354": {"text": "St\u00fccke", "tokens": ["St\u00fc\u00b7cke"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.355": {"text": "und anders mehr vorbey. Er kam also zu r\u00fccke/", "tokens": ["und", "an\u00b7ders", "mehr", "vor\u00b7bey", ".", "Er", "kam", "al\u00b7so", "zu", "r\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "PTKVZ", "$.", "PPER", "VVFIN", "ADV", "PTKZU", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.356": {"text": "Da\u00df es vor m\u00e4nniglch ein Wunder-Zug erschien.", "tokens": ["Da\u00df", "es", "vor", "m\u00e4n\u00b7niglch", "ein", "Wun\u00b7der\u00b7Zug", "er\u00b7schien", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.357": {"text": "Helm Wrangel aber blieb an seiner Stat darin/", "tokens": ["Helm", "Wran\u00b7gel", "a\u00b7ber", "blieb", "an", "sei\u00b7ner", "Stat", "da\u00b7rin", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "VVFIN", "APPR", "PPOSAT", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.358": {"text": "und schaffte viel zu thun/ wie K\u00f6nigsmarck in Brehmen/", "tokens": ["und", "schaff\u00b7te", "viel", "zu", "thun", "/", "wie", "K\u00f6\u00b7nigs\u00b7marck", "in", "Breh\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PTKZU", "VVINF", "$(", "KOKOM", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.359": {"text": "Da man jhn Boxtehud und Stade weg sah nehmen/", "tokens": ["Da", "man", "jhn", "Box\u00b7te\u00b7hud", "und", "Sta\u00b7de", "weg", "sah", "neh\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "NE", "KON", "NN", "ADV", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.360": {"text": "Wie auch das gantze Land. Weil Hertzog Friederich/", "tokens": ["Wie", "auch", "das", "gant\u00b7ze", "Land", ".", "Weil", "Hert\u00b7zog", "Frie\u00b7de\u00b7rich", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "$.", "KOUS", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.361": {"text": "Der Bischoff dieses Ort/ des lieben Vaters sich/", "tokens": ["Der", "Bi\u00b7schoff", "die\u00b7ses", "Ort", "/", "des", "lie\u00b7ben", "Va\u00b7ters", "sich", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PDAT", "NN", "$(", "ART", "ADJA", "NN", "PRF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.362": {"text": "Des K\u00f6nigs Christians/ hatt\u2019 hertzlich angenommen/", "tokens": ["Des", "K\u00f6\u00b7nigs", "Chris\u00b7ti\u00b7ans", "/", "hatt'", "hertz\u00b7lich", "an\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$(", "VAFIN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.363": {"text": "und nach der M\u00f6gligkeit zu Hilffe war gekommen/", "tokens": ["und", "nach", "der", "M\u00f6g\u00b7lig\u00b7keit", "zu", "Hilf\u00b7fe", "war", "ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.364": {"text": "Kam jhm di\u00df ", "tokens": ["Kam", "jhm", "di\u00df"], "token_info": ["word", "word", "word"], "pos": ["NE", "PPER", "PDS"], "meter": "+-+", "measure": "trochaic.di"}, "line.365": {"text": "Erlassen. Doch es kam nach diesem eine Lust/", "tokens": ["Er\u00b7las\u00b7sen", ".", "Doch", "es", "kam", "nach", "die\u00b7sem", "ei\u00b7ne", "Lust", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "KON", "PPER", "VVFIN", "APPR", "PDAT", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.366": {"text": "Die dieses Lands Verlust ersetzte. Meine Seele", "tokens": ["Die", "die\u00b7ses", "Lands", "Ver\u00b7lust", "er\u00b7setz\u00b7te", ".", "Mei\u00b7ne", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PDAT", "NN", "NN", "VVFIN", "$.", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.367": {"text": "W\u00fcndscht hertzlich/ da\u00df es Jhm niemal am Frieden fehle.", "tokens": ["W\u00fcnd\u00b7scht", "hertz\u00b7lich", "/", "da\u00df", "es", "Jhm", "nie\u00b7mal", "am", "Frie\u00b7den", "feh\u00b7le", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "$(", "KOUS", "PPER", "PPER", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.368": {"text": "In dem der K\u00f6nigsmarck das Brehmer-Stifft bekam/", "tokens": ["In", "dem", "der", "K\u00f6\u00b7nigs\u00b7marck", "das", "Breh\u00b7mer\u00b7Stifft", "be\u00b7kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.369": {"text": "Geschah es/ da\u00df der Sachs viel St\u00e4dte wieder nahm.", "tokens": ["Ge\u00b7schah", "es", "/", "da\u00df", "der", "Sachs", "viel", "St\u00e4d\u00b7te", "wie\u00b7der", "nahm", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$(", "KOUS", "ART", "NN", "PIAT", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.370": {"text": "Als Kemnitz/ Rochlitz/ Grimm und andere. Di\u00df brachte/", "tokens": ["Als", "Kem\u00b7nitz", "/", "Roch\u00b7litz", "/", "Grimm", "und", "an\u00b7de\u00b7re", ".", "Di\u00df", "brach\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "NE", "$(", "NE", "$(", "NE", "KON", "PIS", "$.", "PDS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.371": {"text": "Da\u00df sich der K\u00f6nig\u00dfmarck ins Halberst\u00e4dtsche machte/", "tokens": ["Da\u00df", "sich", "der", "K\u00f6\u00b7nig\u00df\u00b7marck", "ins", "Hal\u00b7ber\u00b7st\u00e4dt\u00b7sche", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.372": {"text": "und f\u00f6rders nach der Elb\u2019 an Torgau/ welches sich", "tokens": ["und", "f\u00f6r\u00b7ders", "nach", "der", "Elb'", "an", "Tor\u00b7gau", "/", "wel\u00b7ches", "sich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "APPR", "NE", "$(", "PRELS", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.373": {"text": "An jhn ergeben must\u2019/ auch Egeln blieb im Stich.", "tokens": ["An", "jhn", "er\u00b7ge\u00b7ben", "must'", "/", "auch", "E\u00b7geln", "blieb", "im", "Stich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVINF", "VMFIN", "$(", "ADV", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.374": {"text": "Hierauf begab er sich nach Halberstadt zu r\u00fccke/", "tokens": ["Hier\u00b7auf", "be\u00b7gab", "er", "sich", "nach", "Hal\u00b7ber\u00b7stadt", "zu", "r\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "APPR", "NN", "PTKZU", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.375": {"text": "Woselbst jhm ungefehr ein zimlich ", "tokens": ["Wo\u00b7selbst", "jhm", "un\u00b7ge\u00b7fehr", "ein", "zim\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.376": {"text": "Auf seine V\u00f6lcker stie\u00df. Es kam des Gallas Heer/", "tokens": ["Auf", "sei\u00b7ne", "V\u00f6l\u00b7cker", "stie\u00df", ".", "Es", "kam", "des", "Gal\u00b7las", "Heer", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.377": {"text": "Nach welches R\u00fccken sich der Torsten Sohn so sehr", "tokens": ["Nach", "wel\u00b7ches", "R\u00fc\u00b7cken", "sich", "der", "Tors\u00b7ten", "Sohn", "so", "sehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "PRF", "ART", "NN", "NN", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.378": {"text": "Bem\u00fchte/ schleuniger als man es hier gedachte/", "tokens": ["Be\u00b7m\u00fch\u00b7te", "/", "schleu\u00b7ni\u00b7ger", "als", "man", "es", "hier", "ge\u00b7dach\u00b7te", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "ADJA", "KOUS", "PIS", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.379": {"text": "Wodurch Graf Broy viel Volcks geschwind zu nichte machte/", "tokens": ["Wo\u00b7durch", "Graf", "Broy", "viel", "Volcks", "ge\u00b7schwind", "zu", "nich\u00b7te", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "PIAT", "NN", "ADJD", "APPR", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.380": {"text": "Er schlug dem K\u00f6nigsmarck drey Regimenter ab.", "tokens": ["Er", "schlug", "dem", "K\u00f6\u00b7nigs\u00b7marck", "drey", "Re\u00b7gi\u00b7men\u00b7ter", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.381": {"text": "Wiewol er solchen Schlag jhm schleunig wieder gab/", "tokens": ["Wie\u00b7wol", "er", "sol\u00b7chen", "Schlag", "jhm", "schleu\u00b7nig", "wie\u00b7der", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "PPER", "ADJD", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.382": {"text": "und wenig schuldig blieb. Auf dieses lagen beyde/", "tokens": ["und", "we\u00b7nig", "schul\u00b7dig", "blieb", ".", "Auf", "die\u00b7ses", "la\u00b7gen", "bey\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVFIN", "$.", "APPR", "PDAT", "VVFIN", "PIS", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.383": {"text": "Gallas und Torsten Sohn/ zu einem gro\u00dfen Leyde", "tokens": ["Gal\u00b7las", "und", "Tors\u00b7ten", "Sohn", "/", "zu", "ei\u00b7nem", "gro\u00b7\u00dfen", "Ley\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "KON", "NN", "NN", "$(", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.384": {"text": "Des Landes/ lange Zeit bey Bernburg an der Sahl/", "tokens": ["Des", "Lan\u00b7des", "/", "lan\u00b7ge", "Zeit", "bey", "Bern\u00b7burg", "an", "der", "Sahl", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ADJA", "NN", "APPR", "NE", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.385": {"text": "Bi\u00df sich die K\u00e4ysrischen aus gro\u00dfer Hungers-Qual", "tokens": ["Bi\u00df", "sich", "die", "K\u00e4y\u00b7sri\u00b7schen", "aus", "gro\u00b7\u00dfer", "Hun\u00b7ger\u00b7sQual"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRF", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.386": {"text": "Erhoben und den Weg ins Magdeburgsche nahmen/", "tokens": ["Er\u00b7ho\u00b7ben", "und", "den", "Weg", "ins", "Mag\u00b7de\u00b7burg\u00b7sche", "nah\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ART", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.387": {"text": "Da doch die Schwedischen bald in den R\u00fccken kamen/", "tokens": ["Da", "doch", "die", "Schwe\u00b7di\u00b7schen", "bald", "in", "den", "R\u00fc\u00b7cken", "ka\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.388": {"text": "Dem Broy und Enckefort die meiste Reuterey", "tokens": ["Dem", "Broy", "und", "En\u00b7cke\u00b7fort", "die", "meis\u00b7te", "Reu\u00b7te\u00b7rey"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.389": {"text": "Erschlugen/ und hiemit war Nieder-Sachsen frey.", "tokens": ["Er\u00b7schlu\u00b7gen", "/", "und", "hie\u00b7mit", "war", "Nie\u00b7der\u00b7Sach\u00b7sen", "frey", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "PAV", "VAFIN", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.390": {"text": "Der Feldherr/ Gallas/ gieng hierauf nach B\u00f6h\u00e4ims Grentze\u0303-", "tokens": ["Der", "Feld\u00b7herr", "/", "Gal\u00b7las", "/", "gieng", "hier\u00b7auf", "nach", "B\u00f6\u00b7h\u00e4i\u00b7ms", "Grent\u00b7z\u1ebd"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "NE", "$(", "VVFIN", "PAV", "APPR", "NE", "TRUNC"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.391": {"text": "Kaum da/ lie\u00df auch d\u2019 Schwed sein Schwert daselbste\u0303 gl\u00e4ntze\u0303", "tokens": ["Kaum", "da", "/", "lie\u00df", "auch", "d'", "Schwed", "sein", "Schwert", "da\u00b7selbst\u1ebd", "gl\u00e4nt\u00b7z\u1ebd"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$(", "VVFIN", "ADV", "NE", "NE", "PPOSAT", "NN", "ART", "NN"], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.392": {"text": "und satzte sich bey Saatz. Bald brach er wieder auff", "tokens": ["und", "satz\u00b7te", "sich", "bey", "Saatz", ".", "Bald", "brach", "er", "wie\u00b7der", "auff"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "$.", "ADV", "VVFIN", "PPER", "ADV", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.393": {"text": "und gieng nach Pilsen hin/ nicht achtend/ da\u00df sein Lauff", "tokens": ["und", "gieng", "nach", "Pil\u00b7sen", "hin", "/", "nicht", "ach\u00b7tend", "/", "da\u00df", "sein", "Lauff"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NN", "PTKVZ", "$(", "PTKNEG", "ADJD", "$(", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.394": {"text": "Dem Feind\u2019 im Auge war. Er folgte stets zur Seiten.", "tokens": ["Dem", "Feind'", "im", "Au\u00b7ge", "war", ".", "Er", "folg\u00b7te", "stets", "zur", "Sei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VAFIN", "$.", "PPER", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.395": {"text": "Es kam hier\u00fcber auch bey Jankau ", "tokens": ["Es", "kam", "hier\u00b7\u00fc\u00b7ber", "auch", "bey", "Jan\u00b7kau"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PAV", "ADV", "APPR", "NE"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.396": {"text": "Da es den K\u00e4ysrischen vom Anfang wol ergieng.", "tokens": ["Da", "es", "den", "K\u00e4y\u00b7sri\u00b7schen", "vom", "An\u00b7fang", "wol", "er\u00b7gieng", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPRART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.397": {"text": "Als aber derer Hand zu fr\u00fch nach Beuthen hieng/", "tokens": ["Als", "a\u00b7ber", "de\u00b7rer", "Hand", "zu", "fr\u00fch", "nach", "Beu\u00b7then", "hieng", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PDS", "NN", "PTKA", "ADJD", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.398": {"text": "Verlohren sie die Schlacht/ da\u00df ein dreytausend blieben/", "tokens": ["Ver\u00b7loh\u00b7ren", "sie", "die", "Schlacht", "/", "da\u00df", "ein", "drey\u00b7tau\u00b7send", "blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$(", "KOUS", "ART", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.399": {"text": "Ich schweige was man hat im fliehen aufgerieben", "tokens": ["Ich", "schwei\u00b7ge", "was", "man", "hat", "im", "flie\u00b7hen", "auf\u00b7ge\u00b7rie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PWS", "PIS", "VAFIN", "APPRART", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.400": {"text": "und in Verhafft gebracht. Es blieb manch tapfrer Held", "tokens": ["und", "in", "Ver\u00b7hafft", "ge\u00b7bracht", ".", "Es", "blieb", "manch", "tapf\u00b7rer", "Held"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "VVPP", "$.", "PPER", "VVFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.401": {"text": "So wol in dem Verhafft als durch das Schwerdt gef\u00e4llt.", "tokens": ["So", "wol", "in", "dem", "Ver\u00b7hafft", "als", "durch", "das", "Schwerdt", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "KOKOM", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.402": {"text": "Damit verf\u00fcgte sich der Sieger neu in M\u00e4hren/", "tokens": ["Da\u00b7mit", "ver\u00b7f\u00fcg\u00b7te", "sich", "der", "Sie\u00b7ger", "neu", "in", "M\u00e4h\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PRF", "ART", "NN", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.403": {"text": "Dem Olm\u00fctz zum Entsatz und mehrers zu begehren.", "tokens": ["Dem", "Ol\u00b7m\u00fctz", "zum", "Ent\u00b7satz", "und", "meh\u00b7rers", "zu", "be\u00b7geh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "KON", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.404": {"text": "Die Donau gieng nicht frey. Crem\u00df/ Neuburg/ Modern/", "tokens": ["Die", "Do\u00b7nau", "gieng", "nicht", "frey", ".", "Crem\u00df", "/", "Neu\u00b7burg", "/", "Mo\u00b7dern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "PTKNEG", "ADJD", "$.", "NN", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.405": {"text": "Die musten mit Gewalt d\u2019 Schweden Freunde seyn. (Stein/", "tokens": ["Die", "mus\u00b7ten", "mit", "Ge\u00b7walt", "d'", "Schwe\u00b7den", "Freun\u00b7de", "seyn", ".", "(", "Stein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["PDS", "VMFIN", "APPR", "NN", "NE", "NE", "NN", "VAINF", "$.", "$(", "NN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.406": {"text": "Und nun sah man auch Brinn zum andern mal beschl\u00fc\u00dfen/", "tokens": ["Und", "nun", "sah", "man", "auch", "Brinn", "zum", "an\u00b7dern", "mal", "be\u00b7schl\u00fc\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "ADV", "NN", "APPRART", "PIS", "ADV", "VVINF", "$("], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.407": {"text": "Wo die Ragotzgische sehr starck zum Schweden stie\u00dfen", "tokens": ["Wo", "die", "Ra\u00b7gotz\u00b7gi\u00b7sche", "sehr", "starck", "zum", "Schwe\u00b7den", "stie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ADV", "ADJD", "APPRART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.408": {"text": "und mit gesamter Macht die wolverwahrte Stadt", "tokens": ["und", "mit", "ge\u00b7sam\u00b7ter", "Macht", "die", "wol\u00b7ver\u00b7wahr\u00b7te", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ADJA", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.409": {"text": "Best\u00fcrmten/ welches jhr doch wenig Schaden that.", "tokens": ["Be\u00b7st\u00fcrm\u00b7ten", "/", "wel\u00b7ches", "jhr", "doch", "we\u00b7nig", "Scha\u00b7den", "that", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWS", "PPER", "ADV", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.410": {"text": "Sie kam auch endlich frey/ und des Ragotzky Schaaren", "tokens": ["Sie", "kam", "auch", "end\u00b7lich", "frey", "/", "und", "des", "Ra\u00b7gotz\u00b7ky", "Schaa\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADJD", "$(", "KON", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.411": {"text": "Entwichen/ weil sie stets im Sturm die ersten waren.", "tokens": ["Ent\u00b7wi\u00b7chen", "/", "weil", "sie", "stets", "im", "Sturm", "die", "ers\u00b7ten", "wa\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KOUS", "PPER", "ADV", "APPRART", "NN", "ART", "ADJA", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.412": {"text": "Sie giengen wieder heim und lebten in der Ruh.", "tokens": ["Sie", "gien\u00b7gen", "wie\u00b7der", "heim", "und", "leb\u00b7ten", "in", "der", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.413": {"text": "Man schrieb auch aus Bysantz de\u00dfhalben dr\u00f6uend zu/", "tokens": ["Man", "schrieb", "auch", "aus", "By\u00b7santz", "de\u00df\u00b7hal\u00b7ben", "dr\u00f6u\u00b7end", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "NE", "PAV", "ADJD", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.414": {"text": "Mit unserm K\u00e4yserthum in guter Ruh zu leben.", "tokens": ["Mit", "un\u00b7serm", "K\u00e4y\u00b7ser\u00b7thum", "in", "gu\u00b7ter", "Ruh", "zu", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.415": {"text": "In dem die Schwedischen dem Brinn zu schaffen geben", "tokens": ["In", "dem", "die", "Schwe\u00b7di\u00b7schen", "dem", "Brinn", "zu", "schaf\u00b7fen", "ge\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.416": {"text": "Wil ich wo anders hin/ und erst nach Mei\u00dfen zu/", "tokens": ["Wil", "ich", "wo", "an\u00b7ders", "hin", "/", "und", "erst", "nach", "Mei\u00b7\u00dfen", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PWAV", "ADV", "PTKVZ", "$(", "KON", "ADV", "APPR", "NN", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.417": {"text": "Zu sehn/ was K\u00f6nigsmarck in diesem Lande thu.", "tokens": ["Zu", "sehn", "/", "was", "K\u00f6\u00b7nigs\u00b7marck", "in", "die\u00b7sem", "Lan\u00b7de", "thu", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "NN", "APPR", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.418": {"text": "Es gieng jhm recht nach Wundsch/ also/ dz sich Cur-Sachsen/", "tokens": ["Es", "gieng", "jhm", "recht", "nach", "Wund\u00b7sch", "/", "al\u00b7so", "/", "dz", "sich", "Cur\u00b7Sach\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "NN", "$(", "ADV", "$(", "KOUS", "PRF", "NE", "$("], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.419": {"text": "Des langen Krieges m\u00fcd\u2019 und Schweden nicht gewachsen/", "tokens": ["Des", "lan\u00b7gen", "Krie\u00b7ges", "m\u00fcd'", "und", "Schwe\u00b7den", "nicht", "ge\u00b7wach\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "KON", "NE", "PTKNEG", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.420": {"text": "Neutral bezeugen must. Hiermit war Mei\u00dfen frey", "tokens": ["Neut\u00b7ral", "be\u00b7zeu\u00b7gen", "must", ".", "Hier\u00b7mit", "war", "Mei\u00b7\u00dfen", "frey"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "VMFIN", "$.", "ADV", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.421": {"text": "Vor fernerer Gewalt/ und Brandenburg dabey/", "tokens": ["Vor", "fer\u00b7ne\u00b7rer", "Ge\u00b7walt", "/", "und", "Bran\u00b7den\u00b7burg", "da\u00b7bey", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$(", "KON", "NE", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.422": {"text": "Weil es dergleichen that. Nun/ hier nicht mehr zu stehen/", "tokens": ["Weil", "es", "derg\u00b7lei\u00b7chen", "that", ".", "Nun", "/", "hier", "nicht", "mehr", "zu", "ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "VVFIN", "$.", "ADV", "$(", "ADV", "PTKNEG", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.423": {"text": "So wollen wir dann fort bi\u00df an den Reynstrohm gehen/", "tokens": ["So", "wol\u00b7len", "wir", "dann", "fort", "bi\u00df", "an", "den", "Reyn\u00b7strohm", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PTKVZ", "KON", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.424": {"text": "und der Frantzosen Thun beaugen. Von ", "tokens": ["und", "der", "Frant\u00b7zo\u00b7sen", "Thun", "be\u00b7au\u00b7gen", ".", "Von"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVINF", "$.", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.425": {"text": "War weg noch mehr von Volck aus Franckreich her zu ziehn", "tokens": ["War", "weg", "noch", "mehr", "von", "Volck", "aus", "Fran\u00b7ck\u00b7reich", "her", "zu", "ziehn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "APPR", "NN", "APPR", "NE", "APZR", "PTKZU", "VVINF"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.426": {"text": "Indessen gieng sein Heer/ nach des Turaine leiten/", "tokens": ["In\u00b7des\u00b7sen", "gieng", "sein", "Heer", "/", "nach", "des", "Tur\u00b7ai\u00b7ne", "lei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "$(", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+--++-+-", "measure": "iambic.hexa.relaxed"}, "line.427": {"text": "Ins W\u00fcrtenberger Land/ dasselbe zu bestreiten.", "tokens": ["Ins", "W\u00fcr\u00b7ten\u00b7ber\u00b7ger", "Land", "/", "das\u00b7sel\u00b7be", "zu", "be\u00b7strei\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$(", "PDAT", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.428": {"text": "Sie nahmen Rotenburg/ Hall/ Kreil\u00dfheim und noch mehr/", "tokens": ["Sie", "nah\u00b7men", "Ro\u00b7ten\u00b7burg", "/", "Hall", "/", "Kreil\u00df\u00b7heim", "und", "noch", "mehr", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "$(", "NE", "$(", "NN", "KON", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.429": {"text": "Gedachten auch bereits an keine Gegenwehr/", "tokens": ["Ge\u00b7dach\u00b7ten", "auch", "be\u00b7reits", "an", "kei\u00b7ne", "Ge\u00b7gen\u00b7wehr", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.430": {"text": "Die doch sehe schleunig kam. Der Curf\u00fcrst von den Beyern", "tokens": ["Die", "doch", "se\u00b7he", "schleu\u00b7nig", "kam", ".", "Der", "Cur\u00b7f\u00fcrst", "von", "den", "Be\u00b7yern"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVFIN", "ADJD", "VVFIN", "$.", "ART", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.431": {"text": "Sah es vor n\u00f6thig an nach M\u00f6gligkeit zu steuern.", "tokens": ["Sah", "es", "vor", "n\u00f6\u00b7thig", "an", "nach", "M\u00f6g\u00b7lig\u00b7keit", "zu", "steu\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJD", "APPR", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.432": {"text": "Sie kriegten auch hierauf ", "tokens": ["Sie", "krieg\u00b7ten", "auch", "hier\u00b7auf"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PAV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.433": {"text": "Da\u00df um Herbsthausen r\u00fcm jhr meistes Fu\u00dfvolck blieb.", "tokens": ["Da\u00df", "um", "Herbst\u00b7hau\u00b7sen", "r\u00fcm", "jhr", "meis\u00b7tes", "Fu\u00df\u00b7volck", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.434": {"text": "Auch war die Reuterey fast halb darauf gegangen/", "tokens": ["Auch", "war", "die", "Reu\u00b7te\u00b7rey", "fast", "halb", "da\u00b7rauf", "ge\u00b7gan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "ADJD", "PAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.435": {"text": "Man nahm noch \u00fcber di\u00df zwey tausend Mann gefangen.", "tokens": ["Man", "nahm", "noch", "\u00fc\u00b7ber", "di\u00df", "zwey", "tau\u00b7send", "Mann", "ge\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "PDS", "CARD", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.436": {"text": "Der Rest nahm seine Flucht ins ", "tokens": ["Der", "Rest", "nahm", "sei\u00b7ne", "Flucht", "ins"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "APPRART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.437": {"text": "und Cassel halff hierauf zu einem neuen Stand/", "tokens": ["und", "Cas\u00b7sel", "halff", "hier\u00b7auf", "zu", "ei\u00b7nem", "neu\u00b7en", "Stand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PAV", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.438": {"text": "Also auch K\u00f6nigsmarck. Die Hilff war kaum geschehen/", "tokens": ["Al\u00b7so", "auch", "K\u00f6\u00b7nigs\u00b7marck", ".", "Die", "Hilff", "war", "kaum", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "$.", "ART", "NN", "VAFIN", "ADV", "VVPP", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.439": {"text": "Sah man sie wiederum den Reyn hinaufwertz gehen/", "tokens": ["Sah", "man", "sie", "wie\u00b7de\u00b7rum", "den", "Reyn", "hin\u00b7auf\u00b7wertz", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "ADV", "ART", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.440": {"text": "Die neue Frantzen Hilff mit diesem von ", "tokens": ["Die", "neu\u00b7e", "Frant\u00b7zen", "Hilff", "mit", "die\u00b7sem", "von"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN", "APPR", "PDAT", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.441": {"text": "Daselbsten angelangt/ an jhre Macht zu ziehn.", "tokens": ["Da\u00b7selbs\u00b7ten", "an\u00b7ge\u00b7langt", "/", "an", "jhre", "Macht", "zu", "ziehn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$(", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.442": {"text": "Nichts minders hatten sich die Beyerischen Hauffen", "tokens": ["Nichts", "min\u00b7ders", "hat\u00b7ten", "sich", "die", "Be\u00b7ye\u00b7ri\u00b7schen", "Hauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "ADV", "VAFIN", "PRF", "ART", "ADJA", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.443": {"text": "Mit dem Geleen verst\u00e4rckt/ iu einem neuen rauffen", "tokens": ["Mit", "dem", "Ge\u00b7leen", "ver\u00b7st\u00e4rckt", "/", "i\u00b7u", "ei\u00b7nem", "neu\u00b7en", "rauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVPP", "$(", "NE", "ART", "ADJA", "NN"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.444": {"text": "Genugsem Mann zu seyn. Der K\u00f6nigsmarck gieng ab/", "tokens": ["Ge\u00b7nug\u00b7sem", "Mann", "zu", "seyn", ".", "Der", "K\u00f6\u00b7nigs\u00b7marck", "gieng", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKZU", "VAINF", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.445": {"text": "Welch Zug den Beyrischen nicht wenig Vorthel gab/", "tokens": ["Welch", "Zug", "den", "Bey\u00b7ri\u00b7schen", "nicht", "we\u00b7nig", "Vor\u00b7thel", "gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "PTKNEG", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.446": {"text": "Die Frantzen anzugehn. Es kam ", "tokens": ["Die", "Frant\u00b7zen", "an\u00b7zu\u00b7gehn", ".", "Es", "kam"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVIZU", "$.", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.447": {"text": "Worin das Beyer Heer sich tapffer hat getragen.", "tokens": ["Wo\u00b7rin", "das", "Be\u00b7yer", "Heer", "sich", "tapf\u00b7fer", "hat", "ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "PRF", "ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.448": {"text": "Und gieng der Frantzen Macht das meiste Fu\u00dfvolck auff/", "tokens": ["Und", "gieng", "der", "Frant\u00b7zen", "Macht", "das", "meis\u00b7te", "Fu\u00df\u00b7volck", "auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "ART", "ADJA", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.449": {"text": "Doch endlich trieb der He\u00df den Beyer auf den Lauff/", "tokens": ["Doch", "end\u00b7lich", "trieb", "der", "He\u00df", "den", "Be\u00b7yer", "auf", "den", "Lauff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.450": {"text": "und gab den Frantzen Sieg/ jhm aber blieb die Ehre/", "tokens": ["und", "gab", "den", "Frant\u00b7zen", "Sieg", "/", "jhm", "a\u00b7ber", "blieb", "die", "Eh\u00b7re", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "$(", "PPER", "ADV", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.451": {"text": "Weil der Frantzo\u00df ohn\u2019 jhn der Beyern Gegenwehre", "tokens": ["Weil", "der", "Frant\u00b7zo\u00df", "ohn'", "jhn", "der", "Be\u00b7yern", "Ge\u00b7gen\u00b7weh\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "APPR", "PPER", "ART", "NN", "NN"], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.452": {"text": "Nicht Manns genug erschien. ", "tokens": ["Nicht", "Manns", "ge\u00b7nug", "er\u00b7schien", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.453": {"text": "Geschah bey Allersheim/ welch Dorff bi\u00df diesen Tag", "tokens": ["Ge\u00b7schah", "bey", "Al\u00b7ler\u00b7sheim", "/", "welch", "Dorff", "bi\u00df", "die\u00b7sen", "Tag"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "NE", "$(", "PWAT", "NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.454": {"text": "Hiervon bek\u00e4nntlich ist. Auf dieses muste Schwaben", "tokens": ["Hier\u00b7von", "be\u00b7k\u00e4nnt\u00b7lich", "ist", ".", "Auf", "die\u00b7ses", "mus\u00b7te", "Schwa\u00b7ben"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "ADJD", "VAFIN", "$.", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.455": {"text": "Den gantzen Frantzen-Schwarm mit Macht zu Gaste habe\u0303/", "tokens": ["Den", "gant\u00b7zen", "Frant\u00b7zen\u00b7Schwarm", "mit", "Macht", "zu", "Gas\u00b7te", "hab\u1ebd", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "APPR", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.456": {"text": "Voraus das N\u00f6rdlingen/ welch Ort sehr viel erlit/", "tokens": ["Vo\u00b7raus", "das", "N\u00f6rd\u00b7lin\u00b7gen", "/", "welch", "Ort", "sehr", "viel", "er\u00b7lit", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$(", "PWAT", "NN", "ADV", "ADV", "VVPP", "$("], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.457": {"text": "Weil dies und jenes Theil so offt um solches stritt.", "tokens": ["Weil", "dies", "und", "je\u00b7nes", "Theil", "so", "offt", "um", "sol\u00b7ches", "stritt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "KON", "PDAT", "NN", "ADV", "ADV", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.458": {"text": "La\u00df uns nun wiederum nach M\u00e4hren zu den Schweden/", "tokens": ["La\u00df", "uns", "nun", "wie\u00b7de\u00b7rum", "nach", "M\u00e4h\u00b7ren", "zu", "den", "Schwe\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "APPR", "NN", "APPR", "ART", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.459": {"text": "und von derselben Thun mit kurtzen Worten reden.", "tokens": ["und", "von", "der\u00b7sel\u00b7ben", "Thun", "mit", "kurt\u00b7zen", "Wor\u00b7ten", "re\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PDAT", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.460": {"text": "Sie wusten nichts an Brinn zu haben als Verlust/", "tokens": ["Sie", "wus\u00b7ten", "nichts", "an", "Brinn", "zu", "ha\u00b7ben", "als", "Ver\u00b7lust", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "NN", "PTKZU", "VAINF", "KOKOM", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.461": {"text": "Daher sich derer Macht von dar erheben must\u2019/", "tokens": ["Da\u00b7her", "sich", "de\u00b7rer", "Macht", "von", "dar", "er\u00b7he\u00b7ben", "must'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PRF", "PDS", "NN", "APPR", "PTKVZ", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.462": {"text": "und weil sehr viel gebrach/ begab sie sich zu r\u00fccke", "tokens": ["und", "weil", "sehr", "viel", "ge\u00b7brach", "/", "be\u00b7gab", "sie", "sich", "zu", "r\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ADV", "ADV", "VVFIN", "$(", "VVFIN", "PPER", "PRF", "PTKZU", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.463": {"text": "Nach B\u00f6h\u00e4imb/ pflantzete vor K\u00f6nigsgr\u00e4tz die St\u00fccke", "tokens": ["Nach", "B\u00f6\u00b7h\u00e4i\u00b7mb", "/", "pflant\u00b7ze\u00b7te", "vor", "K\u00f6\u00b7nigs\u00b7gr\u00e4tz", "die", "St\u00fc\u00b7cke"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "VVFIN", "APPR", "NN", "ART", "NN"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.464": {"text": "und that jhm viel Gewalt/ wie auch dem Bardowitz/", "tokens": ["und", "that", "jhm", "viel", "Ge\u00b7walt", "/", "wie", "auch", "dem", "Bar\u00b7do\u00b7witz", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PIAT", "NN", "$(", "KOKOM", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.465": {"text": "Doch alles sonder Frucht. Hief\u00fcr kam Leutmaritz", "tokens": ["Doch", "al\u00b7les", "son\u00b7der", "Frucht", ".", "Hie\u00b7f\u00fcr", "kam", "Leut\u00b7ma\u00b7ritz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PIS", "ADJA", "NN", "$.", "PAV", "VVFIN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.466": {"text": "und Friedland unter Sie. Als dieses war geschehen/", "tokens": ["und", "Fried\u00b7land", "un\u00b7ter", "Sie", ".", "Als", "die\u00b7ses", "war", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "PPER", "$.", "KOUS", "PDS", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.467": {"text": "Sah man das gantze Heer aus diesem Reiche gehen/", "tokens": ["Sah", "man", "das", "gant\u00b7ze", "Heer", "aus", "die\u00b7sem", "Rei\u00b7che", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "APPR", "PDAT", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.468": {"text": "und sich nach Th\u00fcringen verlegen. Torsten Sohn/", "tokens": ["und", "sich", "nach", "Th\u00fc\u00b7rin\u00b7gen", "ver\u00b7le\u00b7gen", ".", "Tors\u00b7ten", "Sohn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "NN", "VVINF", "$.", "NN", "NN", "$("], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}, "line.469": {"text": "Der in so kurtzer Zeit so manche Sieges-Krohn", "tokens": ["Der", "in", "so", "kurt\u00b7zer", "Zeit", "so", "man\u00b7che", "Sie\u00b7ges\u00b7Krohn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ADV", "ADJA", "NN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.470": {"text": "Erwarb/ verlie\u00df hierauf den Krieg/ und gieng in Mei\u00dfen/", "tokens": ["Er\u00b7warb", "/", "ver\u00b7lie\u00df", "hier\u00b7auf", "den", "Krieg", "/", "und", "gieng", "in", "Mei\u00b7\u00dfen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "VVFIN", "PAV", "ART", "NN", "$(", "KON", "VVFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.471": {"text": "Sich guter Kr\u00e4ffte da/ wo m\u00f6glich/ zu beflei\u00dfen.", "tokens": ["Sich", "gu\u00b7ter", "Kr\u00e4ff\u00b7te", "da", "/", "wo", "m\u00f6g\u00b7lich", "/", "zu", "be\u00b7flei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PRF", "ADJA", "NN", "ADV", "$(", "PWAV", "ADJD", "$(", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.472": {"text": "Dann es war jhm sein Leib viel anders als der Muht/", "tokens": ["Dann", "es", "war", "jhm", "sein", "Leib", "viel", "an\u00b7ders", "als", "der", "Muht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PPER", "PPOSAT", "NN", "ADV", "ADV", "KOKOM", "ART", "NN", "$("], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.473": {"text": "Es wurd auch kurtz hierauf mit jhm (doch ewig) gut.", "tokens": ["Es", "wurd", "auch", "kurtz", "hier\u00b7auf", "mit", "jhm", "(", "doch", "e\u00b7wig", ")", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "PAV", "APPR", "PPER", "$(", "ADV", "ADJD", "$(", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}