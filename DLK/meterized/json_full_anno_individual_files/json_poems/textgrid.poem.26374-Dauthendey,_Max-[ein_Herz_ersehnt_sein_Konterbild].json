{"textgrid.poem.26374": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[ein Herz ersehnt sein Konterbild]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Herz ersehnt sein Konterbild,", "tokens": ["Ein", "Herz", "er\u00b7sehnt", "sein", "Kon\u00b7ter\u00b7bild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kriegt man's nicht, so macht das wild.", "tokens": ["Und", "kriegt", "man's", "nicht", ",", "so", "macht", "das", "wild", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PTKNEG", "$,", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Die Sehnsucht ist ein tolles Weib,", "tokens": ["Die", "Sehn\u00b7sucht", "ist", "ein", "tol\u00b7les", "Weib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie boxt den Mann zum Zeitvertreib,", "tokens": ["Sie", "boxt", "den", "Mann", "zum", "Zeit\u00b7ver\u00b7treib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und willst du nicht gleich mit ihr gehn,", "tokens": ["Und", "willst", "du", "nicht", "gleich", "mit", "ihr", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PTKNEG", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So l\u00e4\u00dft sie schwarze N\u00e4gel sehn.", "tokens": ["So", "l\u00e4\u00dft", "sie", "schwar\u00b7ze", "N\u00e4\u00b7gel", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Die Sehnsucht schleift dich durch die Gassen,", "tokens": ["Die", "Sehn\u00b7sucht", "schleift", "dich", "durch", "die", "Gas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lehrt dich solide Menschen hassen,", "tokens": ["Lehrt", "dich", "so\u00b7li\u00b7de", "Men\u00b7schen", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.5": {"line.1": {"text": "Willst nicht auf Trottoiren gehn,", "tokens": ["Willst", "nicht", "auf", "Trot\u00b7toi\u00b7ren", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu langsam tut die Welt sich drehn.", "tokens": ["Zu", "lang\u00b7sam", "tut", "die", "Welt", "sich", "drehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VVFIN", "ART", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Die Sehnsucht ist ein Nadel\u00f6hr,", "tokens": ["Die", "Sehn\u00b7sucht", "ist", "ein", "Na\u00b7de\u00b7l\u00f6hr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hindurch mu\u00df jeder, ist's auch schwer,", "tokens": ["Hin\u00b7durch", "mu\u00df", "je\u00b7der", ",", "ist's", "auch", "schwer", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "$,", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Und hat sie dich ganz d\u00fcnn bekommen", "tokens": ["Und", "hat", "sie", "dich", "ganz", "d\u00fcnn", "be\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "PRF", "ADV", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles \u00dcberfett genommen,", "tokens": ["Und", "al\u00b7les", "\u00dc\u00b7ber\u00b7fett", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Hast still verzichtet und verflucht,", "tokens": ["Hast", "still", "ver\u00b7zich\u00b7tet", "und", "ver\u00b7flucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da naht sich sanft, was du gesucht.", "tokens": ["Da", "naht", "sich", "sanft", ",", "was", "du", "ge\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ADJD", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wei\u00dft nicht, warum der L\u00e4rm geschah,", "tokens": ["Wei\u00dft", "nicht", ",", "wa\u00b7rum", "der", "L\u00e4rm", "ge\u00b7schah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Scheinbar war l\u00e4ngst schon alles da,", "tokens": ["Schein\u00b7bar", "war", "l\u00e4ngst", "schon", "al\u00b7les", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "ADV", "PIS", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Hast \u00fcberhungert deinen Durst,", "tokens": ["Hast", "\u00fc\u00b7ber\u00b7hun\u00b7gert", "dei\u00b7nen", "Durst", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles ist dir beinah Wurst.", "tokens": ["Und", "al\u00b7les", "ist", "dir", "bei\u00b7nah", "Wurst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PPER", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "So ging es mir, dem Balthasar,", "tokens": ["So", "ging", "es", "mir", ",", "dem", "Balt\u00b7ha\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der gar so lang' gerudert war,", "tokens": ["Der", "gar", "so", "lang'", "ge\u00b7ru\u00b7dert", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Der immer nur nach Sehnsucht frug", "tokens": ["Der", "im\u00b7mer", "nur", "nach", "Sehn\u00b7sucht", "frug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eine Dornenkrone trug.", "tokens": ["Und", "ei\u00b7ne", "Dor\u00b7nen\u00b7kro\u00b7ne", "trug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Die Sehnsucht dr\u00e4ngte mich zur Stadt,", "tokens": ["Die", "Sehn\u00b7sucht", "dr\u00e4ng\u00b7te", "mich", "zur", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo alles einst verdr\u00e4ngt mich hat,", "tokens": ["Wo", "al\u00b7les", "einst", "ver\u00b7dr\u00e4ngt", "mich", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADV", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Zwar traf ich nicht Frau K\u00f6nigin,", "tokens": ["Zwar", "traf", "ich", "nicht", "Frau", "K\u00f6\u00b7ni\u00b7gin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch P.T. kam gleich zu mir hin.", "tokens": ["Doch", "P.", "T.", "kam", "gleich", "zu", "mir", "hin", "."], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "S\u00fc\u00df war der Abend wie Rosinen,", "tokens": ["S\u00fc\u00df", "war", "der", "A\u00b7bend", "wie", "Ro\u00b7si\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fixsterne haben stark geschienen,", "tokens": ["Fix\u00b7ster\u00b7ne", "ha\u00b7ben", "stark", "ge\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Der Flu\u00df schwamm sacht zur Seite fort,", "tokens": ["Der", "Flu\u00df", "schwamm", "sacht", "zur", "Sei\u00b7te", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "P.T., der stand am Wasser dort,", "tokens": ["P.", "T.", ",", "der", "stand", "am", "Was\u00b7ser", "dort", ","], "token_info": ["abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "VVFIN", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "P.T. zeigte mir leer die H\u00e4nde,", "tokens": ["P.", "T.", "zeig\u00b7te", "mir", "leer", "die", "H\u00e4n\u00b7de", ","], "token_info": ["abbreviation", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "ADJD", "ART", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Fragend, ob ich daran was f\u00e4nde,", "tokens": ["Fra\u00b7gend", ",", "ob", "ich", "da\u00b7ran", "was", "f\u00e4n\u00b7de", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "PPER", "PAV", "PWS", "VVFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.18": {"line.1": {"text": "Da neulich er zum Pfandhaus ging,", "tokens": ["Da", "neu\u00b7lich", "er", "zum", "Pfand\u00b7haus", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So tr\u00fcg' er jetzo keinen Ring.", "tokens": ["So", "tr\u00fcg'", "er", "jet\u00b7zo", "kei\u00b7nen", "Ring", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "\u00bbdenn sieh, ich konnt' es nicht mehr tragen,", "tokens": ["\u00bb", "denn", "sieh", ",", "ich", "konnt'", "es", "nicht", "mehr", "tra\u00b7gen", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VVIMP", "$,", "PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lieb' nicht den Brautstand sozusagen;", "tokens": ["Lieb'", "nicht", "den", "Brauts\u00b7tand", "so\u00b7zu\u00b7sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Zur Heirat fehlt mir jeder Halt,", "tokens": ["Zur", "Hei\u00b7rat", "fehlt", "mir", "je\u00b7der", "Halt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So brauchte einfach ich Gewalt.", "tokens": ["So", "brauch\u00b7te", "ein\u00b7fach", "ich", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PPER", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.21": {"line.1": {"text": "F\u00fcr eine frohe freie Nacht.", "tokens": ["F\u00fcr", "ei\u00b7ne", "fro\u00b7he", "frei\u00b7e", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' ich den Ring zu Wein gemacht.", "tokens": ["Hab'", "ich", "den", "Ring", "zu", "Wein", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Ein \u00dcbermensch soll niemals frei'n", "tokens": ["Ein", "\u00dc\u00b7ber\u00b7mensch", "soll", "nie\u00b7mals", "frei'n"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sollte mehr geschmackvoll sein.\u00ab", "tokens": ["Und", "soll\u00b7te", "mehr", "ge\u00b7schmack\u00b7voll", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VMFIN", "ADV", "ADJD", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "\u00bbp.T., dein Mund gef\u00e4llt mir nicht,", "tokens": ["\u00bb", "p.", "T.", ",", "dein", "Mund", "ge\u00b7f\u00e4llt", "mir", "nicht", ","], "token_info": ["punct", "abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "NE", "$,", "PPOSAT", "NN", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Da er so ganz respektlos spricht.", "tokens": ["Da", "er", "so", "ganz", "res\u00b7pekt\u00b7los", "spricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Die \u00c4rmste, die du jetzt verlassen,", "tokens": ["Die", "\u00c4rms\u00b7te", ",", "die", "du", "jetzt", "ver\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie wird mit Weinen sich befassen.\u00ab", "tokens": ["Sie", "wird", "mit", "Wei\u00b7nen", "sich", "be\u00b7fas\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "PRF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "\u00bbja, siehst du, Balzer, mein Gebaren", "tokens": ["\u00bb", "ja", ",", "siehst", "du", ",", "Bal\u00b7zer", ",", "mein", "Ge\u00b7ba\u00b7ren"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["$(", "PTKANT", "$,", "VVFIN", "PPER", "$,", "NN", "$,", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rei\u00dft mich verflucht jetzt in den Haaren,", "tokens": ["Rei\u00dft", "mich", "ver\u00b7flucht", "jetzt", "in", "den", "Haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.26": {"line.1": {"text": "Kaum hat sie keinen Ring gesehn,", "tokens": ["Kaum", "hat", "sie", "kei\u00b7nen", "Ring", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So lie\u00df sie mich stillschweigend stehn.", "tokens": ["So", "lie\u00df", "sie", "mich", "still\u00b7schwei\u00b7gend", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Sie sprach nicht und sie schrie nicht laut,", "tokens": ["Sie", "sprach", "nicht", "und", "sie", "schrie", "nicht", "laut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "KON", "PPER", "VVFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Schweigen hat mich durchgehaut,", "tokens": ["Ihr", "Schwei\u00b7gen", "hat", "mich", "durch\u00b7ge\u00b7haut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Sie schrieb, sie wolle nichts mehr wissen,", "tokens": ["Sie", "schrieb", ",", "sie", "wol\u00b7le", "nichts", "mehr", "wis\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hielte nichts und h\u00e4tt's zerrissen.", "tokens": ["Ich", "hiel\u00b7te", "nichts", "und", "h\u00e4tt's", "zer\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "KON", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Sie h\u00e4lt auf dich, mein Freundesknochen,", "tokens": ["Sie", "h\u00e4lt", "auf", "dich", ",", "mein", "Freun\u00b7des\u00b7kno\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Stets hat sie hoch von dir gesprochen,", "tokens": ["Stets", "hat", "sie", "hoch", "von", "dir", "ge\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Sch\u00f6n war sie, wenn sie von dir sprach.", "tokens": ["Sch\u00f6n", "war", "sie", ",", "wenn", "sie", "von", "dir", "sprach", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und denke ich dar\u00fcber nach,", "tokens": ["Und", "den\u00b7ke", "ich", "da\u00b7r\u00fc\u00b7ber", "nach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Blind ist des Weibes Lebenslauf,", "tokens": ["Blind", "ist", "des", "Wei\u00b7bes", "Le\u00b7bens\u00b7lauf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hoffe noch, sie sucht dich auf.\u00ab", "tokens": ["Ich", "hof\u00b7fe", "noch", ",", "sie", "sucht", "dich", "auf", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "\u00bbdann, P.T., mu\u00df ich dir gesteht,", "tokens": ["\u00bb", "dann", ",", "P.", "T.", ",", "mu\u00df", "ich", "dir", "ge\u00b7steht", ","], "token_info": ["punct", "word", "punct", "abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NE", "NE", "$,", "VMFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Nicht l\u00e4nger w\u00fcrd' ich seitw\u00e4rts gehn.", "tokens": ["Nicht", "l\u00e4n\u00b7ger", "w\u00fcrd'", "ich", "seit\u00b7w\u00e4rts", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Wohl m\u00f6cht' ich heut schon bei ihr weilen,", "tokens": ["Wohl", "m\u00f6cht'", "ich", "heut", "schon", "bei", "ihr", "wei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch peinlich ist's, sich jetzt zu eilen.", "tokens": ["Doch", "pein\u00b7lich", "ist's", ",", "sich", "jetzt", "zu", "ei\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "$,", "PRF", "ADV", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Erst soll ihr Schmerz vor\u00fcber sein,", "tokens": ["Erst", "soll", "ihr", "Schmerz", "vor\u00b7\u00fc\u00b7ber", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPOSAT", "NN", "ADV", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann stelle ich mich liebend ein.\u00ab", "tokens": ["Dann", "stel\u00b7le", "ich", "mich", "lie\u00b7bend", "ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJD", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Gern w\u00e4re ich vor Lust geflogen,", "tokens": ["Gern", "w\u00e4\u00b7re", "ich", "vor", "Lust", "ge\u00b7flo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Venusstern hat's mich gezogen,", "tokens": ["Zum", "Ve\u00b7nuss\u00b7tern", "hat's", "mich", "ge\u00b7zo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.36": {"line.1": {"text": "Ich durfte es mir eingestehn:", "tokens": ["Ich", "durf\u00b7te", "es", "mir", "ein\u00b7ge\u00b7stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben ist doch wundersch\u00f6n.", "tokens": ["Das", "Le\u00b7ben", "ist", "doch", "wun\u00b7der\u00b7sch\u00f6n", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "P.T. verfluchte sich und schrie,", "tokens": ["P.", "T.", "ver\u00b7fluch\u00b7te", "sich", "und", "schrie", ","], "token_info": ["abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PRF", "KON", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Er sei kein Mensch, ein \u00dcbervieh,", "tokens": ["Er", "sei", "kein", "Mensch", ",", "ein", "\u00dc\u00b7ber\u00b7vieh", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Ungl\u00fccklich sei er bis zum Rand", "tokens": ["Un\u00b7gl\u00fcck\u00b7lich", "sei", "er", "bis", "zum", "Rand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wolle schleunigst aus dem Land.", "tokens": ["Und", "wol\u00b7le", "schleu\u00b7nigst", "aus", "dem", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Wir sprachen dies auf einer Br\u00fccke,", "tokens": ["Wir", "spra\u00b7chen", "dies", "auf", "ei\u00b7ner", "Br\u00fc\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den P.T. hielt ich kaum zur\u00fccke,", "tokens": ["Den", "P.", "T.", "hielt", "ich", "kaum", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.40": {"line.1": {"text": "Am liebsten sprang er in den Strom,", "tokens": ["Am", "liebs\u00b7ten", "sprang", "er", "in", "den", "Strom", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich nahm ihn fest und sagte: \u00bbKomm.\u00ab", "tokens": ["Ich", "nahm", "ihn", "fest", "und", "sag\u00b7te", ":", "\u00bb", "Kom\u00b7m.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "abbreviation", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "$.", "$(", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.41": {"line.1": {"text": "\u00bbnein, la\u00df mich,\u00ab schrie er wie verwirrt,", "tokens": ["\u00bb", "nein", ",", "la\u00df", "mich", ",", "\u00ab", "schrie", "er", "wie", "ver\u00b7wirrt", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "VVIMP", "PPER", "$,", "$(", "VVFIN", "PPER", "KOKOM", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbich habe mich in mir geirrt,", "tokens": ["\u00bb", "ich", "ha\u00b7be", "mich", "in", "mir", "ge\u00b7irrt", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Das Schlimmste, was der Mensch erf\u00e4hrt", "tokens": ["Das", "Schlimms\u00b7te", ",", "was", "der", "Mensch", "er\u00b7f\u00e4hrt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist, wenn er f\u00fchlt, er ist nichts wert.\u00ab", "tokens": ["Ist", ",", "wenn", "er", "f\u00fchlt", ",", "er", "ist", "nichts", "wert", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "PIS", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Er sprang nicht in das Wasser 'runter,", "tokens": ["Er", "sprang", "nicht", "in", "das", "Was\u00b7ser", "'r\u00b7un\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "ART", "NN", "NE", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Warf nur den Regenschirm hinunter.", "tokens": ["Warf", "nur", "den", "Re\u00b7gen\u00b7schirm", "hin\u00b7un\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.44": {"line.1": {"text": "Die ganze Welt hat ihn ge\u00f6det,", "tokens": ["Die", "gan\u00b7ze", "Welt", "hat", "ihn", "ge\u00b7\u00f6\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Symbolisch hat er sich get\u00f6tet.", "tokens": ["Sym\u00b7bo\u00b7lisch", "hat", "er", "sich", "ge\u00b7t\u00f6\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Ein Herz ersehnt sein Konterbild,", "tokens": ["Ein", "Herz", "er\u00b7sehnt", "sein", "Kon\u00b7ter\u00b7bild", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kriegt man's nicht, so macht das wild.", "tokens": ["Und", "kriegt", "man's", "nicht", ",", "so", "macht", "das", "wild", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "PTKNEG", "$,", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Die Sehnsucht ist ein tolles Weib,", "tokens": ["Die", "Sehn\u00b7sucht", "ist", "ein", "tol\u00b7les", "Weib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie boxt den Mann zum Zeitvertreib,", "tokens": ["Sie", "boxt", "den", "Mann", "zum", "Zeit\u00b7ver\u00b7treib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Und willst du nicht gleich mit ihr gehn,", "tokens": ["Und", "willst", "du", "nicht", "gleich", "mit", "ihr", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PTKNEG", "ADV", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So l\u00e4\u00dft sie schwarze N\u00e4gel sehn.", "tokens": ["So", "l\u00e4\u00dft", "sie", "schwar\u00b7ze", "N\u00e4\u00b7gel", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Die Sehnsucht schleift dich durch die Gassen,", "tokens": ["Die", "Sehn\u00b7sucht", "schleift", "dich", "durch", "die", "Gas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lehrt dich solide Menschen hassen,", "tokens": ["Lehrt", "dich", "so\u00b7li\u00b7de", "Men\u00b7schen", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.49": {"line.1": {"text": "Willst nicht auf Trottoiren gehn,", "tokens": ["Willst", "nicht", "auf", "Trot\u00b7toi\u00b7ren", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu langsam tut die Welt sich drehn.", "tokens": ["Zu", "lang\u00b7sam", "tut", "die", "Welt", "sich", "drehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VVFIN", "ART", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Die Sehnsucht ist ein Nadel\u00f6hr,", "tokens": ["Die", "Sehn\u00b7sucht", "ist", "ein", "Na\u00b7de\u00b7l\u00f6hr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hindurch mu\u00df jeder, ist's auch schwer,", "tokens": ["Hin\u00b7durch", "mu\u00df", "je\u00b7der", ",", "ist's", "auch", "schwer", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "$,", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Und hat sie dich ganz d\u00fcnn bekommen", "tokens": ["Und", "hat", "sie", "dich", "ganz", "d\u00fcnn", "be\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "PRF", "ADV", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles \u00dcberfett genommen,", "tokens": ["Und", "al\u00b7les", "\u00dc\u00b7ber\u00b7fett", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Hast still verzichtet und verflucht,", "tokens": ["Hast", "still", "ver\u00b7zich\u00b7tet", "und", "ver\u00b7flucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da naht sich sanft, was du gesucht.", "tokens": ["Da", "naht", "sich", "sanft", ",", "was", "du", "ge\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ADJD", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.53": {"line.1": {"text": "Wei\u00dft nicht, warum der L\u00e4rm geschah,", "tokens": ["Wei\u00dft", "nicht", ",", "wa\u00b7rum", "der", "L\u00e4rm", "ge\u00b7schah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Scheinbar war l\u00e4ngst schon alles da,", "tokens": ["Schein\u00b7bar", "war", "l\u00e4ngst", "schon", "al\u00b7les", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "ADV", "PIS", "ADV", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.54": {"line.1": {"text": "Hast \u00fcberhungert deinen Durst,", "tokens": ["Hast", "\u00fc\u00b7ber\u00b7hun\u00b7gert", "dei\u00b7nen", "Durst", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles ist dir beinah Wurst.", "tokens": ["Und", "al\u00b7les", "ist", "dir", "bei\u00b7nah", "Wurst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PPER", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "So ging es mir, dem Balthasar,", "tokens": ["So", "ging", "es", "mir", ",", "dem", "Balt\u00b7ha\u00b7sar", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der gar so lang' gerudert war,", "tokens": ["Der", "gar", "so", "lang'", "ge\u00b7ru\u00b7dert", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Der immer nur nach Sehnsucht frug", "tokens": ["Der", "im\u00b7mer", "nur", "nach", "Sehn\u00b7sucht", "frug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eine Dornenkrone trug.", "tokens": ["Und", "ei\u00b7ne", "Dor\u00b7nen\u00b7kro\u00b7ne", "trug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Die Sehnsucht dr\u00e4ngte mich zur Stadt,", "tokens": ["Die", "Sehn\u00b7sucht", "dr\u00e4ng\u00b7te", "mich", "zur", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo alles einst verdr\u00e4ngt mich hat,", "tokens": ["Wo", "al\u00b7les", "einst", "ver\u00b7dr\u00e4ngt", "mich", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADV", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Zwar traf ich nicht Frau K\u00f6nigin,", "tokens": ["Zwar", "traf", "ich", "nicht", "Frau", "K\u00f6\u00b7ni\u00b7gin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch P.T. kam gleich zu mir hin.", "tokens": ["Doch", "P.", "T.", "kam", "gleich", "zu", "mir", "hin", "."], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.59": {"line.1": {"text": "S\u00fc\u00df war der Abend wie Rosinen,", "tokens": ["S\u00fc\u00df", "war", "der", "A\u00b7bend", "wie", "Ro\u00b7si\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fixsterne haben stark geschienen,", "tokens": ["Fix\u00b7ster\u00b7ne", "ha\u00b7ben", "stark", "ge\u00b7schie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Der Flu\u00df schwamm sacht zur Seite fort,", "tokens": ["Der", "Flu\u00df", "schwamm", "sacht", "zur", "Sei\u00b7te", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "P.T., der stand am Wasser dort,", "tokens": ["P.", "T.", ",", "der", "stand", "am", "Was\u00b7ser", "dort", ","], "token_info": ["abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "VVFIN", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.61": {"line.1": {"text": "P.T. zeigte mir leer die H\u00e4nde,", "tokens": ["P.", "T.", "zeig\u00b7te", "mir", "leer", "die", "H\u00e4n\u00b7de", ","], "token_info": ["abbreviation", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "ADJD", "ART", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Fragend, ob ich daran was f\u00e4nde,", "tokens": ["Fra\u00b7gend", ",", "ob", "ich", "da\u00b7ran", "was", "f\u00e4n\u00b7de", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KOUS", "PPER", "PAV", "PWS", "VVFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.62": {"line.1": {"text": "Da neulich er zum Pfandhaus ging,", "tokens": ["Da", "neu\u00b7lich", "er", "zum", "Pfand\u00b7haus", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So tr\u00fcg' er jetzo keinen Ring.", "tokens": ["So", "tr\u00fcg'", "er", "jet\u00b7zo", "kei\u00b7nen", "Ring", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "\u00bbdenn sieh, ich konnt' es nicht mehr tragen,", "tokens": ["\u00bb", "denn", "sieh", ",", "ich", "konnt'", "es", "nicht", "mehr", "tra\u00b7gen", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VVIMP", "$,", "PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lieb' nicht den Brautstand sozusagen;", "tokens": ["Lieb'", "nicht", "den", "Brauts\u00b7tand", "so\u00b7zu\u00b7sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Zur Heirat fehlt mir jeder Halt,", "tokens": ["Zur", "Hei\u00b7rat", "fehlt", "mir", "je\u00b7der", "Halt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So brauchte einfach ich Gewalt.", "tokens": ["So", "brauch\u00b7te", "ein\u00b7fach", "ich", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PPER", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.65": {"line.1": {"text": "F\u00fcr eine frohe freie Nacht.", "tokens": ["F\u00fcr", "ei\u00b7ne", "fro\u00b7he", "frei\u00b7e", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab' ich den Ring zu Wein gemacht.", "tokens": ["Hab'", "ich", "den", "Ring", "zu", "Wein", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Ein \u00dcbermensch soll niemals frei'n", "tokens": ["Ein", "\u00dc\u00b7ber\u00b7mensch", "soll", "nie\u00b7mals", "frei'n"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sollte mehr geschmackvoll sein.\u00ab", "tokens": ["Und", "soll\u00b7te", "mehr", "ge\u00b7schmack\u00b7voll", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VMFIN", "ADV", "ADJD", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "\u00bbp.T., dein Mund gef\u00e4llt mir nicht,", "tokens": ["\u00bb", "p.", "T.", ",", "dein", "Mund", "ge\u00b7f\u00e4llt", "mir", "nicht", ","], "token_info": ["punct", "abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "NE", "$,", "PPOSAT", "NN", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Da er so ganz respektlos spricht.", "tokens": ["Da", "er", "so", "ganz", "res\u00b7pekt\u00b7los", "spricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Die \u00c4rmste, die du jetzt verlassen,", "tokens": ["Die", "\u00c4rms\u00b7te", ",", "die", "du", "jetzt", "ver\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie wird mit Weinen sich befassen.\u00ab", "tokens": ["Sie", "wird", "mit", "Wei\u00b7nen", "sich", "be\u00b7fas\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "PRF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "\u00bbja, siehst du, Balzer, mein Gebaren", "tokens": ["\u00bb", "ja", ",", "siehst", "du", ",", "Bal\u00b7zer", ",", "mein", "Ge\u00b7ba\u00b7ren"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["$(", "PTKANT", "$,", "VVFIN", "PPER", "$,", "NN", "$,", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rei\u00dft mich verflucht jetzt in den Haaren,", "tokens": ["Rei\u00dft", "mich", "ver\u00b7flucht", "jetzt", "in", "den", "Haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.70": {"line.1": {"text": "Kaum hat sie keinen Ring gesehn,", "tokens": ["Kaum", "hat", "sie", "kei\u00b7nen", "Ring", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So lie\u00df sie mich stillschweigend stehn.", "tokens": ["So", "lie\u00df", "sie", "mich", "still\u00b7schwei\u00b7gend", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.71": {"line.1": {"text": "Sie sprach nicht und sie schrie nicht laut,", "tokens": ["Sie", "sprach", "nicht", "und", "sie", "schrie", "nicht", "laut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "KON", "PPER", "VVFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr Schweigen hat mich durchgehaut,", "tokens": ["Ihr", "Schwei\u00b7gen", "hat", "mich", "durch\u00b7ge\u00b7haut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Sie schrieb, sie wolle nichts mehr wissen,", "tokens": ["Sie", "schrieb", ",", "sie", "wol\u00b7le", "nichts", "mehr", "wis\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hielte nichts und h\u00e4tt's zerrissen.", "tokens": ["Ich", "hiel\u00b7te", "nichts", "und", "h\u00e4tt's", "zer\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "KON", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.73": {"line.1": {"text": "Sie h\u00e4lt auf dich, mein Freundesknochen,", "tokens": ["Sie", "h\u00e4lt", "auf", "dich", ",", "mein", "Freun\u00b7des\u00b7kno\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Stets hat sie hoch von dir gesprochen,", "tokens": ["Stets", "hat", "sie", "hoch", "von", "dir", "ge\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.74": {"line.1": {"text": "Sch\u00f6n war sie, wenn sie von dir sprach.", "tokens": ["Sch\u00f6n", "war", "sie", ",", "wenn", "sie", "von", "dir", "sprach", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und denke ich dar\u00fcber nach,", "tokens": ["Und", "den\u00b7ke", "ich", "da\u00b7r\u00fc\u00b7ber", "nach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Blind ist des Weibes Lebenslauf,", "tokens": ["Blind", "ist", "des", "Wei\u00b7bes", "Le\u00b7bens\u00b7lauf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hoffe noch, sie sucht dich auf.\u00ab", "tokens": ["Ich", "hof\u00b7fe", "noch", ",", "sie", "sucht", "dich", "auf", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.76": {"line.1": {"text": "\u00bbdann, P.T., mu\u00df ich dir gesteht,", "tokens": ["\u00bb", "dann", ",", "P.", "T.", ",", "mu\u00df", "ich", "dir", "ge\u00b7steht", ","], "token_info": ["punct", "word", "punct", "abbreviation", "abbreviation", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "NE", "NE", "$,", "VMFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Nicht l\u00e4nger w\u00fcrd' ich seitw\u00e4rts gehn.", "tokens": ["Nicht", "l\u00e4n\u00b7ger", "w\u00fcrd'", "ich", "seit\u00b7w\u00e4rts", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.77": {"line.1": {"text": "Wohl m\u00f6cht' ich heut schon bei ihr weilen,", "tokens": ["Wohl", "m\u00f6cht'", "ich", "heut", "schon", "bei", "ihr", "wei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch peinlich ist's, sich jetzt zu eilen.", "tokens": ["Doch", "pein\u00b7lich", "ist's", ",", "sich", "jetzt", "zu", "ei\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "$,", "PRF", "ADV", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.78": {"line.1": {"text": "Erst soll ihr Schmerz vor\u00fcber sein,", "tokens": ["Erst", "soll", "ihr", "Schmerz", "vor\u00b7\u00fc\u00b7ber", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPOSAT", "NN", "ADV", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dann stelle ich mich liebend ein.\u00ab", "tokens": ["Dann", "stel\u00b7le", "ich", "mich", "lie\u00b7bend", "ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJD", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Gern w\u00e4re ich vor Lust geflogen,", "tokens": ["Gern", "w\u00e4\u00b7re", "ich", "vor", "Lust", "ge\u00b7flo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Venusstern hat's mich gezogen,", "tokens": ["Zum", "Ve\u00b7nuss\u00b7tern", "hat's", "mich", "ge\u00b7zo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.80": {"line.1": {"text": "Ich durfte es mir eingestehn:", "tokens": ["Ich", "durf\u00b7te", "es", "mir", "ein\u00b7ge\u00b7stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Leben ist doch wundersch\u00f6n.", "tokens": ["Das", "Le\u00b7ben", "ist", "doch", "wun\u00b7der\u00b7sch\u00f6n", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.81": {"line.1": {"text": "P.T. verfluchte sich und schrie,", "tokens": ["P.", "T.", "ver\u00b7fluch\u00b7te", "sich", "und", "schrie", ","], "token_info": ["abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PRF", "KON", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Er sei kein Mensch, ein \u00dcbervieh,", "tokens": ["Er", "sei", "kein", "Mensch", ",", "ein", "\u00dc\u00b7ber\u00b7vieh", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.82": {"line.1": {"text": "Ungl\u00fccklich sei er bis zum Rand", "tokens": ["Un\u00b7gl\u00fcck\u00b7lich", "sei", "er", "bis", "zum", "Rand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wolle schleunigst aus dem Land.", "tokens": ["Und", "wol\u00b7le", "schleu\u00b7nigst", "aus", "dem", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.83": {"line.1": {"text": "Wir sprachen dies auf einer Br\u00fccke,", "tokens": ["Wir", "spra\u00b7chen", "dies", "auf", "ei\u00b7ner", "Br\u00fc\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den P.T. hielt ich kaum zur\u00fccke,", "tokens": ["Den", "P.", "T.", "hielt", "ich", "kaum", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "VVFIN", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.84": {"line.1": {"text": "Am liebsten sprang er in den Strom,", "tokens": ["Am", "liebs\u00b7ten", "sprang", "er", "in", "den", "Strom", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich nahm ihn fest und sagte: \u00bbKomm.\u00ab", "tokens": ["Ich", "nahm", "ihn", "fest", "und", "sag\u00b7te", ":", "\u00bb", "Kom\u00b7m.", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "abbreviation", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "$.", "$(", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.85": {"line.1": {"text": "\u00bbnein, la\u00df mich,\u00ab schrie er wie verwirrt,", "tokens": ["\u00bb", "nein", ",", "la\u00df", "mich", ",", "\u00ab", "schrie", "er", "wie", "ver\u00b7wirrt", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$,", "VVIMP", "PPER", "$,", "$(", "VVFIN", "PPER", "KOKOM", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbich habe mich in mir geirrt,", "tokens": ["\u00bb", "ich", "ha\u00b7be", "mich", "in", "mir", "ge\u00b7irrt", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.86": {"line.1": {"text": "Das Schlimmste, was der Mensch erf\u00e4hrt", "tokens": ["Das", "Schlimms\u00b7te", ",", "was", "der", "Mensch", "er\u00b7f\u00e4hrt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist, wenn er f\u00fchlt, er ist nichts wert.\u00ab", "tokens": ["Ist", ",", "wenn", "er", "f\u00fchlt", ",", "er", "ist", "nichts", "wert", ".", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "PIS", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.87": {"line.1": {"text": "Er sprang nicht in das Wasser 'runter,", "tokens": ["Er", "sprang", "nicht", "in", "das", "Was\u00b7ser", "'r\u00b7un\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "ART", "NN", "NE", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Warf nur den Regenschirm hinunter.", "tokens": ["Warf", "nur", "den", "Re\u00b7gen\u00b7schirm", "hin\u00b7un\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.88": {"line.1": {"text": "Die ganze Welt hat ihn ge\u00f6det,", "tokens": ["Die", "gan\u00b7ze", "Welt", "hat", "ihn", "ge\u00b7\u00f6\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Symbolisch hat er sich get\u00f6tet.", "tokens": ["Sym\u00b7bo\u00b7lisch", "hat", "er", "sich", "ge\u00b7t\u00f6\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}