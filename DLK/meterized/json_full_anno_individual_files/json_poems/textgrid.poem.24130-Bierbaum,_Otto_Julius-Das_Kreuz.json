{"textgrid.poem.24130": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "Das Kreuz", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "J\u00fcngst war ich auf einem Tr\u00f6delmarkt", "tokens": ["J\u00fcngst", "war", "ich", "auf", "ei\u00b7nem", "Tr\u00f6\u00b7del\u00b7markt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Und sah was das Leben zusammenharckt", "tokens": ["Und", "sah", "was", "das", "Le\u00b7ben", "zu\u00b7sam\u00b7men\u00b7harckt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PWS", "ART", "NN", "VVPP"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Auf dem gro\u00dfen Ger\u00fcmpelhaufen:", "tokens": ["Auf", "dem", "gro\u00b7\u00dfen", "Ge\u00b7r\u00fcm\u00b7pel\u00b7hau\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Lumpen und Plunder, Geraffel und Tand,", "tokens": ["Lum\u00b7pen", "und", "Plun\u00b7der", ",", "Ge\u00b7raf\u00b7fel", "und", "Tand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.5": {"text": "In Schmutz und Scherben allerhand;", "tokens": ["In", "Schmutz", "und", "Scher\u00b7ben", "al\u00b7ler\u00b7hand", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wer Geld hat kann sichs kaufen.", "tokens": ["Wer", "Geld", "hat", "kann", "sichs", "kau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Da, unter altem Ger\u00fcst und Ger\u00e4t,", "tokens": ["Da", ",", "un\u00b7ter", "al\u00b7tem", "Ge\u00b7r\u00fcst", "und", "Ge\u00b7r\u00e4t", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hab ich ein h\u00f6lzernes Kreuz ersp\u00e4ht.", "tokens": ["Hab", "ich", "ein", "h\u00f6l\u00b7zer\u00b7nes", "Kreuz", "er\u00b7sp\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Zwei H\u00e4nde lang wars, aus Fichtenholz schlicht;", "tokens": ["Zwei", "H\u00e4n\u00b7de", "lang", "wars", ",", "aus", "Fich\u00b7ten\u00b7holz", "schlicht", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "VAFIN", "$,", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ich machte mir gleich ein r\u00fchrsam Gedicht,", "tokens": ["Ich", "mach\u00b7te", "mir", "gleich", "ein", "r\u00fchr\u00b7sam", "Ge\u00b7dicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ART", "ADJD", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "An welcher Andachtsst\u00e4tte", "tokens": ["An", "wel\u00b7cher", "An\u00b7dachts\u00b7st\u00e4t\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PWAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Es einst gehangen h\u00e4tte.", "tokens": ["Es", "einst", "ge\u00b7han\u00b7gen", "h\u00e4t\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Dacht eine Bauernstube mir,", "tokens": ["Dacht", "ei\u00b7ne", "Bau\u00b7ern\u00b7stu\u00b7be", "mir", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In ders die d\u00fcrftige fromme Zier", "tokens": ["In", "ders", "die", "d\u00fcrf\u00b7ti\u00b7ge", "from\u00b7me", "Zier"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ART", "ADJA", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Vielleicht gewesen w\u00e4re;", "tokens": ["Viel\u00b7leicht", "ge\u00b7we\u00b7sen", "w\u00e4\u00b7re", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAPP", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Hing in der Eck an der wei\u00dfen Wand,", "tokens": ["Hing", "in", "der", "Eck", "an", "der", "wei\u00b7\u00dfen", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Und manche harte Bauernhand", "tokens": ["Und", "man\u00b7che", "har\u00b7te", "Bau\u00b7ern\u00b7hand"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Schlug vor ihm ihre schwere", "tokens": ["Schlug", "vor", "ihm", "ih\u00b7re", "schwe\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "PPER", "PPOSAT", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Bekreuzung \u00fcber Brust und Gesicht.", "tokens": ["Be\u00b7kreu\u00b7zung", "\u00fc\u00b7ber", "Brust", "und", "Ge\u00b7sicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "So dacht ich, aber 's war so nicht.", "tokens": ["So", "dacht", "ich", ",", "a\u00b7ber", "'s", "war", "so", "nicht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KON", "PPER", "VAFIN", "ADV", "PTKNEG", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Denn sieh, als sch\u00e4rfer hin ich sah:", "tokens": ["Denn", "sieh", ",", "als", "sch\u00e4r\u00b7fer", "hin", "ich", "sah", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "$,", "KOUS", "ADJD", "PTKVZ", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Am Querholz war ein Einschnitt da,", "tokens": ["Am", "Quer\u00b7holz", "war", "ein", "Ein\u00b7schnitt", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und, als ich leicht darauf gedr\u00fcckt,", "tokens": ["Und", ",", "als", "ich", "leicht", "da\u00b7rauf", "ge\u00b7dr\u00fcckt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "ADJD", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hat sich ein Dolch heraus gez\u00fcckt.", "tokens": ["Hat", "sich", "ein", "Dolch", "he\u00b7raus", "ge\u00b7z\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ART", "NN", "APZR", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Erschrocken schier sah ich das Eisen", "tokens": ["Er\u00b7schro\u00b7cken", "schier", "sah", "ich", "das", "Ei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "VVFIN", "PPER", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "Des Kreuzes in der Sonne glei\u00dfen.", "tokens": ["Des", "Kreu\u00b7zes", "in", "der", "Son\u00b7ne", "glei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "War eine Blutrinn eingeschnitten,", "tokens": ["War", "ei\u00b7ne", "Blut\u00b7rinn", "ein\u00b7ge\u00b7schnit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und dieses las ich ihr inmitten:", "tokens": ["Und", "die\u00b7ses", "las", "ich", "ihr", "in\u00b7mit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kreutz und Messer Aines worden", "tokens": ["Kreutz", "und", "Mes\u00b7ser", "Ai\u00b7nes", "wor\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "NE", "VAPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In der Messerkreutzer Orden.", "tokens": ["In", "der", "Mes\u00b7ser\u00b7kreut\u00b7zer", "Or\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Schlecht bin ich leider nur beschlagen", "tokens": ["Schlecht", "bin", "ich", "lei\u00b7der", "nur", "be\u00b7schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In Wissenschaft aus alten Tagen,", "tokens": ["In", "Wis\u00b7sen\u00b7schaft", "aus", "al\u00b7ten", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auch konnte, wie ich um mich that,", "tokens": ["Auch", "konn\u00b7te", ",", "wie", "ich", "um", "mich", "that", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "$,", "PWAV", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir keiner sichere Kunde sagen,", "tokens": ["Mir", "kei\u00b7ner", "si\u00b7che\u00b7re", "Kun\u00b7de", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Was f\u00fcr ein Orden es gewesen,", "tokens": ["Was", "f\u00fcr", "ein", "Or\u00b7den", "es", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "PPER", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der Kreuz und Messer sich erlesen", "tokens": ["Der", "Kreuz", "und", "Mes\u00b7ser", "sich", "er\u00b7le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "NN", "PRF", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Als Waffe und als Namen hat.", "tokens": ["Als", "Waf\u00b7fe", "und", "als", "Na\u00b7men", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "KOUS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Doch hab das Kreuz ich mitgenommen", "tokens": ["Doch", "hab", "das", "Kreuz", "ich", "mit\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und geb es, wenn sie zu mir kommen,", "tokens": ["Und", "geb", "es", ",", "wenn", "sie", "zu", "mir", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als R\u00e4tsel gerne denen auf,", "tokens": ["Als", "R\u00e4t\u00b7sel", "ger\u00b7ne", "de\u00b7nen", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "PDS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So da geh\u00f6ren zu den Frommen.", "tokens": ["So", "da", "ge\u00b7h\u00f6\u00b7ren", "zu", "den", "From\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Mir scheints, als obs ein Sinnbild w\u00e4re", "tokens": ["Mir", "scheints", ",", "als", "obs", "ein", "Sinn\u00b7bild", "w\u00e4\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "KOUS", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr jenen wundersamen Lauf,", "tokens": ["F\u00fcr", "je\u00b7nen", "wun\u00b7der\u00b7sa\u00b7men", "Lauf", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den des Erl\u00f6sers milde Lehre", "tokens": ["Den", "des", "Er\u00b7l\u00f6\u00b7sers", "mil\u00b7de", "Leh\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von Golgatha bis heut genommen.", "tokens": ["Von", "Gol\u00b7ga\u00b7tha", "bis", "heut", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "J\u00fcngst war ich auf einem Tr\u00f6delmarkt", "tokens": ["J\u00fcngst", "war", "ich", "auf", "ei\u00b7nem", "Tr\u00f6\u00b7del\u00b7markt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Und sah was das Leben zusammenharckt", "tokens": ["Und", "sah", "was", "das", "Le\u00b7ben", "zu\u00b7sam\u00b7men\u00b7harckt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PWS", "ART", "NN", "VVPP"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Auf dem gro\u00dfen Ger\u00fcmpelhaufen:", "tokens": ["Auf", "dem", "gro\u00b7\u00dfen", "Ge\u00b7r\u00fcm\u00b7pel\u00b7hau\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Lumpen und Plunder, Geraffel und Tand,", "tokens": ["Lum\u00b7pen", "und", "Plun\u00b7der", ",", "Ge\u00b7raf\u00b7fel", "und", "Tand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.5": {"text": "In Schmutz und Scherben allerhand;", "tokens": ["In", "Schmutz", "und", "Scher\u00b7ben", "al\u00b7ler\u00b7hand", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wer Geld hat kann sichs kaufen.", "tokens": ["Wer", "Geld", "hat", "kann", "sichs", "kau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Da, unter altem Ger\u00fcst und Ger\u00e4t,", "tokens": ["Da", ",", "un\u00b7ter", "al\u00b7tem", "Ge\u00b7r\u00fcst", "und", "Ge\u00b7r\u00e4t", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hab ich ein h\u00f6lzernes Kreuz ersp\u00e4ht.", "tokens": ["Hab", "ich", "ein", "h\u00f6l\u00b7zer\u00b7nes", "Kreuz", "er\u00b7sp\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Zwei H\u00e4nde lang wars, aus Fichtenholz schlicht;", "tokens": ["Zwei", "H\u00e4n\u00b7de", "lang", "wars", ",", "aus", "Fich\u00b7ten\u00b7holz", "schlicht", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "VAFIN", "$,", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ich machte mir gleich ein r\u00fchrsam Gedicht,", "tokens": ["Ich", "mach\u00b7te", "mir", "gleich", "ein", "r\u00fchr\u00b7sam", "Ge\u00b7dicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ART", "ADJD", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "An welcher Andachtsst\u00e4tte", "tokens": ["An", "wel\u00b7cher", "An\u00b7dachts\u00b7st\u00e4t\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PWAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Es einst gehangen h\u00e4tte.", "tokens": ["Es", "einst", "ge\u00b7han\u00b7gen", "h\u00e4t\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Dacht eine Bauernstube mir,", "tokens": ["Dacht", "ei\u00b7ne", "Bau\u00b7ern\u00b7stu\u00b7be", "mir", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In ders die d\u00fcrftige fromme Zier", "tokens": ["In", "ders", "die", "d\u00fcrf\u00b7ti\u00b7ge", "from\u00b7me", "Zier"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ART", "ADJA", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Vielleicht gewesen w\u00e4re;", "tokens": ["Viel\u00b7leicht", "ge\u00b7we\u00b7sen", "w\u00e4\u00b7re", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAPP", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Hing in der Eck an der wei\u00dfen Wand,", "tokens": ["Hing", "in", "der", "Eck", "an", "der", "wei\u00b7\u00dfen", "Wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Und manche harte Bauernhand", "tokens": ["Und", "man\u00b7che", "har\u00b7te", "Bau\u00b7ern\u00b7hand"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Schlug vor ihm ihre schwere", "tokens": ["Schlug", "vor", "ihm", "ih\u00b7re", "schwe\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "PPER", "PPOSAT", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Bekreuzung \u00fcber Brust und Gesicht.", "tokens": ["Be\u00b7kreu\u00b7zung", "\u00fc\u00b7ber", "Brust", "und", "Ge\u00b7sicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.13": {"line.1": {"text": "So dacht ich, aber 's war so nicht.", "tokens": ["So", "dacht", "ich", ",", "a\u00b7ber", "'s", "war", "so", "nicht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KON", "PPER", "VAFIN", "ADV", "PTKNEG", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Denn sieh, als sch\u00e4rfer hin ich sah:", "tokens": ["Denn", "sieh", ",", "als", "sch\u00e4r\u00b7fer", "hin", "ich", "sah", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "$,", "KOUS", "ADJD", "PTKVZ", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Am Querholz war ein Einschnitt da,", "tokens": ["Am", "Quer\u00b7holz", "war", "ein", "Ein\u00b7schnitt", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und, als ich leicht darauf gedr\u00fcckt,", "tokens": ["Und", ",", "als", "ich", "leicht", "da\u00b7rauf", "ge\u00b7dr\u00fcckt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "ADJD", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hat sich ein Dolch heraus gez\u00fcckt.", "tokens": ["Hat", "sich", "ein", "Dolch", "he\u00b7raus", "ge\u00b7z\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ART", "NN", "APZR", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Erschrocken schier sah ich das Eisen", "tokens": ["Er\u00b7schro\u00b7cken", "schier", "sah", "ich", "das", "Ei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "VVFIN", "PPER", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "Des Kreuzes in der Sonne glei\u00dfen.", "tokens": ["Des", "Kreu\u00b7zes", "in", "der", "Son\u00b7ne", "glei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "War eine Blutrinn eingeschnitten,", "tokens": ["War", "ei\u00b7ne", "Blut\u00b7rinn", "ein\u00b7ge\u00b7schnit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und dieses las ich ihr inmitten:", "tokens": ["Und", "die\u00b7ses", "las", "ich", "ihr", "in\u00b7mit\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kreutz und Messer Aines worden", "tokens": ["Kreutz", "und", "Mes\u00b7ser", "Ai\u00b7nes", "wor\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "NE", "VAPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In der Messerkreutzer Orden.", "tokens": ["In", "der", "Mes\u00b7ser\u00b7kreut\u00b7zer", "Or\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Schlecht bin ich leider nur beschlagen", "tokens": ["Schlecht", "bin", "ich", "lei\u00b7der", "nur", "be\u00b7schla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In Wissenschaft aus alten Tagen,", "tokens": ["In", "Wis\u00b7sen\u00b7schaft", "aus", "al\u00b7ten", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auch konnte, wie ich um mich that,", "tokens": ["Auch", "konn\u00b7te", ",", "wie", "ich", "um", "mich", "that", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "$,", "PWAV", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir keiner sichere Kunde sagen,", "tokens": ["Mir", "kei\u00b7ner", "si\u00b7che\u00b7re", "Kun\u00b7de", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Was f\u00fcr ein Orden es gewesen,", "tokens": ["Was", "f\u00fcr", "ein", "Or\u00b7den", "es", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "PPER", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der Kreuz und Messer sich erlesen", "tokens": ["Der", "Kreuz", "und", "Mes\u00b7ser", "sich", "er\u00b7le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "NN", "PRF", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Als Waffe und als Namen hat.", "tokens": ["Als", "Waf\u00b7fe", "und", "als", "Na\u00b7men", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "KOUS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Doch hab das Kreuz ich mitgenommen", "tokens": ["Doch", "hab", "das", "Kreuz", "ich", "mit\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "PPER", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und geb es, wenn sie zu mir kommen,", "tokens": ["Und", "geb", "es", ",", "wenn", "sie", "zu", "mir", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als R\u00e4tsel gerne denen auf,", "tokens": ["Als", "R\u00e4t\u00b7sel", "ger\u00b7ne", "de\u00b7nen", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "PDS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So da geh\u00f6ren zu den Frommen.", "tokens": ["So", "da", "ge\u00b7h\u00f6\u00b7ren", "zu", "den", "From\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Mir scheints, als obs ein Sinnbild w\u00e4re", "tokens": ["Mir", "scheints", ",", "als", "obs", "ein", "Sinn\u00b7bild", "w\u00e4\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "KOUS", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr jenen wundersamen Lauf,", "tokens": ["F\u00fcr", "je\u00b7nen", "wun\u00b7der\u00b7sa\u00b7men", "Lauf", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den des Erl\u00f6sers milde Lehre", "tokens": ["Den", "des", "Er\u00b7l\u00f6\u00b7sers", "mil\u00b7de", "Leh\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von Golgatha bis heut genommen.", "tokens": ["Von", "Gol\u00b7ga\u00b7tha", "bis", "heut", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}