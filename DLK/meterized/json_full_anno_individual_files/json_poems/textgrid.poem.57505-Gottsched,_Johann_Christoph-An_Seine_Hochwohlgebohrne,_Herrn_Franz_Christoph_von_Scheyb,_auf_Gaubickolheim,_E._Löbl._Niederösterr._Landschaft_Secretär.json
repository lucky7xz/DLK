{"textgrid.poem.57505": {"metadata": {"author": {"name": "Gottsched, Johann Christoph", "birth": "N.A.", "death": "N.A."}, "title": "An Seine Hochwohlgebohrne, Herrn Franz Christoph von Scheyb, auf Gaubickolheim, E. L\u00f6bl. Nieder\u00f6sterr. Landschaft Secret\u00e4r", "genre": "verse", "period": "N.A.", "pub_year": 1733, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Denn dazu hat Dich mir des Schicksals Huld gegeben;", "tokens": ["Denn", "da\u00b7zu", "hat", "Dich", "mir", "des", "Schick\u00b7sals", "Huld", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VAFIN", "PPER", "PPER", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nachdem zwey Drittheil schon des Laufs vor\u00fcber sind,", "tokens": ["Nach\u00b7dem", "zwey", "Dritt\u00b7heil", "schon", "des", "Laufs", "vor\u00b7\u00fc\u00b7ber", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "NN", "ADV", "ART", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und meiner Scheitel H\u00f6h schon Reif und Schnee gewinnt.", "tokens": ["Und", "mei\u00b7ner", "Schei\u00b7tel", "H\u00f6h", "schon", "Reif", "und", "Schnee", "ge\u00b7winnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "ADV", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Aus Costnitz! h\u00e4tte mirs auch jemals tr\u00e4umen k\u00f6nnen?", "tokens": ["Aus", "Cost\u00b7nitz", "!", "h\u00e4t\u00b7te", "mirs", "auch", "je\u00b7mals", "tr\u00e4u\u00b7men", "k\u00f6n\u00b7nen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$.", "VAFIN", "NE", "ADV", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Aus Schwaben sollte mir ", "tokens": ["Aus", "Schwa\u00b7ben", "soll\u00b7te", "mir"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "VMFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Wer h\u00e4tte das geglaubt? seit dem ein b\u00f6ser Schwab,", "tokens": ["Wer", "h\u00e4t\u00b7te", "das", "ge\u00b7glaubt", "?", "seit", "dem", "ein", "b\u00f6\u00b7ser", "Schwab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PDS", "VVPP", "$.", "APPR", "ART", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Mir lebenslang von Stolz und Ha\u00df die Proben gab;", "tokens": ["Mir", "le\u00b7bens\u00b7lang", "von", "Stolz", "und", "Ha\u00df", "die", "Pro\u00b7ben", "gab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Der bittern Rachgier Gift f\u00fcr ungeschehne Sachen,", "tokens": ["Der", "bit\u00b7tern", "Rach\u00b7gier", "Gift", "f\u00fcr", "un\u00b7ge\u00b7scheh\u00b7ne", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch h\u00f6hern Arm gesch\u00fctzt, mir wu\u00dfte schwer zu machen.", "tokens": ["Durch", "h\u00f6\u00b7hern", "Arm", "ge\u00b7sch\u00fctzt", ",", "mir", "wu\u00df\u00b7te", "schwer", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$,", "PPER", "VVFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Nun liegt er in der Gruft; begl\u00fcckt, wie er geglaubt,", "tokens": ["Nun", "liegt", "er", "in", "der", "Gruft", ";", "be\u00b7gl\u00fcckt", ",", "wie", "er", "ge\u00b7glaubt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$.", "VVPP", "$,", "PWAV", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wenn ihm an Geist und Leib der Tod das Seyn geraubt.", "tokens": ["Wenn", "ihm", "an", "Geist", "und", "Leib", "der", "Tod", "das", "Seyn", "ge\u00b7raubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "KON", "NN", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "So lern ich denn an Dir, und wenig andern Proben", "tokens": ["So", "lern", "ich", "denn", "an", "Dir", ",", "und", "we\u00b7nig", "an\u00b7dern", "Pro\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$,", "KON", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kein Volk sey \u00fcberhaupt zu schelten und zu loben.", "tokens": ["Kein", "Volk", "sey", "\u00fc\u00b7ber\u00b7haupt", "zu", "schel\u00b7ten", "und", "zu", "lo\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein jedes Land erzeugt Gem\u00fcther edler Art;", "tokens": ["Ein", "je\u00b7des", "Land", "er\u00b7zeugt", "Ge\u00b7m\u00fc\u00b7ther", "ed\u00b7ler", "Art", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wohl dem! dem eins davon in Freundschaft g\u00fcnstig ward.", "tokens": ["Wohl", "dem", "!", "dem", "eins", "da\u00b7von", "in", "Freund\u00b7schaft", "g\u00fcns\u00b7tig", "ward", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "$.", "ART", "PIS", "PAV", "APPR", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die\u00df Gl\u00fcck ertheilest Du mir ferngebohrnem Preu\u00dfen;", "tokens": ["Die\u00df", "Gl\u00fcck", "er\u00b7thei\u00b7lest", "Du", "mir", "fern\u00b7ge\u00b7bohr\u00b7nem", "Preu\u00b7\u00dfen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VVFIN", "PPER", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Den jener Bernsteinstrand kann seinen Z\u00f6gling hei\u00dfen,", "tokens": ["Den", "je\u00b7ner", "Bern\u00b7stein\u00b7strand", "kann", "sei\u00b7nen", "Z\u00f6g\u00b7ling", "hei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Bis ihn das Gl\u00fcck hieher in Deutschlands Kern gebracht.", "tokens": ["Bis", "ihn", "das", "Gl\u00fcck", "hie\u00b7her", "in", "Deutschlands", "Kern", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "PAV", "APPR", "NE", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Hier hab ich Geist und Witz noch feiner ausgeschliffen,", "tokens": ["Hier", "hab", "ich", "Geist", "und", "Witz", "noch", "fei\u00b7ner", "aus\u00b7ge\u00b7schlif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "KON", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Was ", "tokens": ["Was"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.11": {"text": "Durch fremder Sprachen Licht das Deutsche mehr gest\u00e4rkt,", "tokens": ["Durch", "frem\u00b7der", "Spra\u00b7chen", "Licht", "das", "Deut\u00b7sche", "mehr", "ge\u00b7st\u00e4rkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und aus der Alten H\u00f6h der Neuern Fall bemerkt.", "tokens": ["Und", "aus", "der", "Al\u00b7ten", "H\u00f6h", "der", "Neu\u00b7ern", "Fall", "be\u00b7merkt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Hier fand ich ", "tokens": ["Hier", "fand", "ich"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.14": {"text": "So hab ich nach und nach die Wahrheit mehr verstanden:", "tokens": ["So", "hab", "ich", "nach", "und", "nach", "die", "Wahr\u00b7heit", "mehr", "ver\u00b7stan\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "KON", "APPR", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Da\u00df auch das beste Feld von selbst nur Unkraut tr\u00e4gt,", "tokens": ["Da\u00df", "auch", "das", "bes\u00b7te", "Feld", "von", "selbst", "nur", "Un\u00b7kraut", "tr\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "APPR", "ADV", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Wenn keines G\u00e4rtners Hand den Flei\u00df daran gelegt.", "tokens": ["Wenn", "kei\u00b7nes", "G\u00e4rt\u00b7ners", "Hand", "den", "Flei\u00df", "da\u00b7ran", "ge\u00b7legt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "NN", "ART", "NN", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wie th\u00f6richt ist es denn, von Sonn und Luft zu sprechen,", "tokens": ["Wie", "th\u00f6\u00b7richt", "ist", "es", "denn", ",", "von", "Sonn", "und", "Luft", "zu", "spre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PPER", "ADV", "$,", "APPR", "NN", "KON", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Da Griechenland und Rom der Regel Nachdruck schw\u00e4chen?", "tokens": ["Da", "Grie\u00b7chen\u00b7land", "und", "Rom", "der", "Re\u00b7gel", "Nach\u00b7druck", "schw\u00e4\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "KON", "NE", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Behaupte wie du willst, hochweiser ", "tokens": ["Be\u00b7haup\u00b7te", "wie", "du", "willst", ",", "hoch\u00b7wei\u00b7ser"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "KOKOM", "PPER", "VMFIN", "$,", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Das Clima mache klug. Ein Kluger lacht dazu!", "tokens": ["Das", "Cli\u00b7ma", "ma\u00b7che", "klug", ".", "Ein", "Klu\u00b7ger", "lacht", "da\u00b7zu", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$.", "ART", "NN", "VVFIN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Und l\u00e4\u00dft zur Probe, dich die Menschen, gleich den Bl\u00fcthen,", "tokens": ["Und", "l\u00e4\u00dft", "zur", "Pro\u00b7be", ",", "dich", "die", "Men\u00b7schen", ",", "gleich", "den", "Bl\u00fc\u00b7then", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$,", "PRF", "ART", "NN", "$,", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}}, "stanza.3": {"line.1": {"text": "Allein best\u00e4tigt nicht, Dein Beyspiel, ", "tokens": ["Al\u00b7lein", "be\u00b7st\u00e4\u00b7tigt", "nicht", ",", "Dein", "Bey\u00b7spiel", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was, \u00fcberhaupt gesagt, so widersinnisch scheint?", "tokens": ["Was", ",", "\u00fc\u00b7ber\u00b7haupt", "ge\u00b7sagt", ",", "so", "wi\u00b7der\u00b7sin\u00b7nisch", "scheint", "?"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "$,", "ADV", "VVPP", "$,", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein w\u00e4rmer Land hat Dir Empfindung Geist und Leben;", "tokens": ["Ein", "w\u00e4r\u00b7mer", "Land", "hat", "Dir", "Emp\u00b7fin\u00b7dung", "Geist", "und", "Le\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "NN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Mir nur der kalte Belt ein F\u00fcnckchen Witz gegeben.", "tokens": ["Mir", "nur", "der", "kal\u00b7te", "Belt", "ein", "F\u00fcn\u00b7ck\u00b7chen", "Witz", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Mit n\u00e4hern Blicken scho\u00df die Sonne Dir zu gut,", "tokens": ["Mit", "n\u00e4\u00b7hern", "Bli\u00b7cken", "scho\u00df", "die", "Son\u00b7ne", "Dir", "zu", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "PPER", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Viel mildre Stralen ab, als sie am Pregel thut.", "tokens": ["Viel", "mild\u00b7re", "Stra\u00b7len", "ab", ",", "als", "sie", "am", "Pre\u00b7gel", "thut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "PTKVZ", "$,", "KOUS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Kein Wunder! da\u00df Dein Geist sich \u00fcber mich geschwungen,", "tokens": ["Kein", "Wun\u00b7der", "!", "da\u00df", "Dein", "Geist", "sich", "\u00fc\u00b7ber", "mich", "ge\u00b7schwun\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$.", "KOUS", "PPOSAT", "NN", "PRF", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Als Du die Lust der Welt, ", "tokens": ["Als", "Du", "die", "Lust", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "So gern ich die\u00df gesteh, so falsch ist jens dabey.", "tokens": ["So", "gern", "ich", "die\u00df", "ge\u00b7steh", ",", "so", "falsch", "ist", "jens", "da\u00b7bey", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PDS", "VVFIN", "$,", "ADV", "ADJD", "VAFIN", "ADV", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Schuff denn der Sonnenstral in Costnitz einerley?", "tokens": ["Schuff", "denn", "der", "Son\u00b7nen\u00b7stral", "in", "Cost\u00b7nitz", "ei\u00b7ner\u00b7ley", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ART", "NN", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wie kams, da\u00df auf der Bank, wo ", "tokens": ["Wie", "kams", ",", "da\u00df", "auf", "der", "Bank", ",", "wo"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["PWAV", "NE", "$,", "KOUS", "APPR", "ART", "NN", "$,", "PWAV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.12": {"text": "Sie allen Sch\u00fclern nicht gleichviel Witz zugemessen?", "tokens": ["Sie", "al\u00b7len", "Sch\u00fc\u00b7lern", "nicht", "gleich\u00b7viel", "Witz", "zu\u00b7ge\u00b7mes\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "PTKNEG", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-++-+--+-", "measure": "iambic.hexa.relaxed"}, "line.13": {"text": "Und hat sie das gethan; wo sind die andern nun?", "tokens": ["Und", "hat", "sie", "das", "ge\u00b7than", ";", "wo", "sind", "die", "an\u00b7dern", "nun", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PDS", "VVPP", "$.", "PWAV", "VAFIN", "ART", "ADJA", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Warum verr\u00e4th sie nicht ihr Denken, Schreiben, Thun?", "tokens": ["Wa\u00b7rum", "ver\u00b7r\u00e4\u00b7th", "sie", "nicht", "ihr", "Den\u00b7ken", ",", "Schrei\u00b7ben", ",", "Thun", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "Wer kennet sie in Wien? O! wer kann das ergr\u00fcnden?", "tokens": ["Wer", "ken\u00b7net", "sie", "in", "Wi\u00b7en", "?", "O", "!", "wer", "kann", "das", "er\u00b7gr\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "APPR", "NE", "$.", "NE", "$.", "PWS", "VMFIN", "PDS", "VVINF", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "In soviel L\u00e4ndern ist doch nur ", "tokens": ["In", "so\u00b7viel", "L\u00e4n\u00b7dern", "ist", "doch", "nur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "So wie mein Vaterland nur ", "tokens": ["So", "wie", "mein", "Va\u00b7ter\u00b7land", "nur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "PPOSAT", "NN", "ADV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.18": {"text": "Der durch erhabne Glut auch w\u00e4lsche Geister beugt.", "tokens": ["Der", "durch", "er\u00b7hab\u00b7ne", "Glut", "auch", "w\u00e4l\u00b7sche", "Geis\u00b7ter", "beugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ADJA", "NN", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Wohlauf, ", "tokens": ["Wohl\u00b7auf", ","], "token_info": ["word", "punct"], "pos": ["ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Komm, wage noch einmal Kalliopens Gesch\u00e4ffte.", "tokens": ["Komm", ",", "wa\u00b7ge", "noch", "ein\u00b7mal", "Kal\u00b7li\u00b7o\u00b7pens", "Ge\u00b7sch\u00e4ff\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "ADV", "ADV", "NE", "NN", "$."], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.3": {"text": "Nimm ihr heroisch Rohr der G\u00f6ttinn aus der Hand,", "tokens": ["Nimm", "ihr", "he\u00b7ro\u00b7isch", "Rohr", "der", "G\u00f6t\u00b7tinn", "aus", "der", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADJD", "NN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und mach uns abermal ", "tokens": ["Und", "mach", "uns", "a\u00b7ber\u00b7mal"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die so viel Thronen ziert; Die das Geschick erkohren,", "tokens": ["Die", "so", "viel", "Thro\u00b7nen", "ziert", ";", "Die", "das", "Ge\u00b7schick", "er\u00b7koh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIAT", "NN", "VVFIN", "$.", "ART", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Zeiten Schmuck zu seyn, die Sie zur Welt gebohren.", "tokens": ["Der", "Zei\u00b7ten", "Schmuck", "zu", "seyn", ",", "die", "Sie", "zur", "Welt", "ge\u00b7boh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VAINF", "$,", "PRELS", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dein Vorsatz ist so sch\u00f6n, als edel und gerecht:", "tokens": ["Dein", "Vor\u00b7satz", "ist", "so", "sch\u00f6n", ",", "als", "e\u00b7del", "und", "ge\u00b7recht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,", "KOUS", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Denn wo der Gegenstand des Dichters Kraft nicht schw\u00e4cht,", "tokens": ["Denn", "wo", "der", "Ge\u00b7gen\u00b7stand", "des", "Dich\u00b7ters", "Kraft", "nicht", "schw\u00e4cht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "ART", "NN", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ja sie vielmehr erh\u00f6ht; da mu\u00df es ihm gelingen,", "tokens": ["Ja", "sie", "viel\u00b7mehr", "er\u00b7h\u00f6ht", ";", "da", "mu\u00df", "es", "ihm", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PPER", "ADV", "VVPP", "$.", "ADV", "VMFIN", "PPER", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der Dichtkunst h\u00f6chsten Preis sich spielend zu erringen.", "tokens": ["Der", "Dicht\u00b7kunst", "h\u00f6chs\u00b7ten", "Preis", "sich", "spie\u00b7lend", "zu", "er\u00b7rin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "PRF", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Nur eins bek\u00fcmmert mich von allem was Du schreibst;", "tokens": ["Nur", "eins", "be\u00b7k\u00fcm\u00b7mert", "mich", "von", "al\u00b7lem", "was", "Du", "schreibst", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "PRF", "APPR", "PIS", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df Du voll Eigensinn bey jenen Mustern bleibst,", "tokens": ["Da\u00df", "Du", "voll", "Ei\u00b7gen\u00b7sinn", "bey", "je\u00b7nen", "Mus\u00b7tern", "bleibst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "NN", "APPR", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Griechenland und Rom der Welt zuerst gewiesen,", "tokens": ["Die", "Grie\u00b7chen\u00b7land", "und", "Rom", "der", "Welt", "zu\u00b7erst", "ge\u00b7wie\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn sie der Helden Lob nach der Natur gepriesen.", "tokens": ["Wenn", "sie", "der", "Hel\u00b7den", "Lob", "nach", "der", "Na\u00b7tur", "ge\u00b7prie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Du liesest den ", "tokens": ["Du", "lie\u00b7sest", "den"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ART"], "meter": "+---", "measure": "dactylic.init"}, "line.6": {"text": "Eh er die Stifter Roms, ", "tokens": ["Eh", "er", "die", "Stif\u00b7ter", "Roms", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NE", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.7": {"text": "Nach Latien gef\u00fchrt. Der hie\u00df ja wohl vor Jahren,", "tokens": ["Nach", "La\u00b7ti\u00b7en", "ge\u00b7f\u00fchrt", ".", "Der", "hie\u00df", "ja", "wohl", "vor", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$.", "ART", "VVFIN", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Als Geist und Dichtkunst noch in ihrer Wiege waren,", "tokens": ["Als", "Geist", "und", "Dicht\u00b7kunst", "noch", "in", "ih\u00b7rer", "Wie\u00b7ge", "wa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ADV", "APPR", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Das Augenmerk der Kunst, der Vater von dem Witz,", "tokens": ["Das", "Au\u00b7gen\u00b7merk", "der", "Kunst", ",", "der", "Va\u00b7ter", "von", "dem", "Witz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der alles aufgekl\u00e4rt, als noch der Musen Sitz", "tokens": ["Der", "al\u00b7les", "auf\u00b7ge\u00b7kl\u00e4rt", ",", "als", "noch", "der", "Mu\u00b7sen", "Sitz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVPP", "$,", "KOUS", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Im Grajerlande lag. Jedoch zu unsern Zeiten", "tokens": ["Im", "Gra\u00b7jer\u00b7lan\u00b7de", "lag", ".", "Je\u00b7doch", "zu", "un\u00b7sern", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "$.", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Hat alles sich verkehrt, bis auf der Dichter Seyten.", "tokens": ["Hat", "al\u00b7les", "sich", "ver\u00b7kehrt", ",", "bis", "auf", "der", "Dich\u00b7ter", "Sey\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PRF", "VVPP", "$,", "KOUS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Ich glaubte sonst wie Du: bis ich nur j\u00fcngst gelernt,", "tokens": ["Ich", "glaub\u00b7te", "sonst", "wie", "Du", ":", "bis", "ich", "nur", "j\u00fcngst", "ge\u00b7lernt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "KOKOM", "PPER", "$.", "KOUS", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Da\u00df man durchs Alterthum sich von dem Ruhm entfernt,", "tokens": ["Da\u00df", "man", "durchs", "Al\u00b7ter\u00b7thum", "sich", "von", "dem", "Ruhm", "ent\u00b7fernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "PRF", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Ein Muster selbst zu seyn; da\u00df man die Geister hindert,", "tokens": ["Ein", "Mus\u00b7ter", "selbst", "zu", "seyn", ";", "da\u00df", "man", "die", "Geis\u00b7ter", "hin\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKZU", "VAINF", "$.", "KOUS", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Wenn die Vernunft den Flug der Phantasey vermindert,", "tokens": ["Wenn", "die", "Ver\u00b7nunft", "den", "Flug", "der", "Phan\u00b7ta\u00b7sey", "ver\u00b7min\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und kl\u00fcglich schreiben lehrt. Drum gib ein wenig acht,", "tokens": ["Und", "kl\u00fcg\u00b7lich", "schrei\u00b7ben", "lehrt", ".", "Drum", "gib", "ein", "we\u00b7nig", "acht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVINF", "VVFIN", "$.", "PAV", "VVIMP", "ART", "PIAT", "CARD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Was mich seit kurzer Zeit auf andern Sinn gebracht.", "tokens": ["Was", "mich", "seit", "kur\u00b7zer", "Zeit", "auf", "an\u00b7dern", "Sinn", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ADJA", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Wer zwanzig Jahre schon der Dichtkunst Regeln lehret,", "tokens": ["Wer", "zwan\u00b7zig", "Jah\u00b7re", "schon", "der", "Dicht\u00b7kunst", "Re\u00b7geln", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "CARD", "NN", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Verdient vieleicht ein Ohr! das ihn geduldig h\u00f6ret.", "tokens": ["Ver\u00b7di\u00b7ent", "vie\u00b7leicht", "ein", "Ohr", "!", "das", "ihn", "ge\u00b7dul\u00b7dig", "h\u00f6\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ADV", "ART", "NN", "$.", "PRELS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.6": {"line.1": {"text": "Als ", "tokens": ["Als"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Drey Jahr, eh er erblich, mir dieses Amt befahl;", "tokens": ["Drey", "Jahr", ",", "eh", "er", "er\u00b7blich", ",", "mir", "die\u00b7ses", "Amt", "be\u00b7fahl", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "KOUS", "PPER", "ADJD", "$,", "PPER", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "(vieleicht weil ich sehr oft, des Helden Gnadenproben", "tokens": ["(", "vie\u00b7leicht", "weil", "ich", "sehr", "oft", ",", "des", "Hel\u00b7den", "Gna\u00b7den\u00b7pro\u00b7ben"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ADV", "KOUS", "PPER", "ADV", "ADV", "$,", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "An Musen und Parna\u00df, der Wahrheit nach, erhoben)", "tokens": ["An", "Mu\u00b7sen", "und", "Par\u00b7na\u00df", ",", "der", "Wahr\u00b7heit", "nach", ",", "er\u00b7ho\u00b7ben", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "$,", "ART", "NN", "PTKVZ", "$,", "VVPP", "$("], "meter": "-+-+++-+-+-+-", "measure": "unknown.measure.septa"}, "line.5": {"text": "Beherrschte leider mich noch der verj\u00e4hrte Wahn:", "tokens": ["Be\u00b7herrschte", "lei\u00b7der", "mich", "noch", "der", "ver\u00b7j\u00e4hr\u00b7te", "Wahn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.6": {"text": "(wie ", "tokens": ["(", "wie"], "token_info": ["punct", "word"], "pos": ["$(", "KOKOM"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Man m\u00fcsse nach der Spur der alten Regeln gehen,", "tokens": ["Man", "m\u00fcs\u00b7se", "nach", "der", "Spur", "der", "al\u00b7ten", "Re\u00b7geln", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPR", "ART", "NN", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Dichtkunst auf den Grad der Griechen zu erh\u00f6hen.", "tokens": ["Die", "Dicht\u00b7kunst", "auf", "den", "Grad", "der", "Grie\u00b7chen", "zu", "er\u00b7h\u00f6\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und den vermi\u00dfte man. Ein dummes Quodlibet,", "tokens": ["Und", "den", "ver\u00b7mi\u00df\u00b7te", "man", ".", "Ein", "dum\u00b7mes", "Quod\u00b7li\u00b7bet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "PIS", "$.", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Wo weder Kopf noch Schweif am rechten Ende steht,", "tokens": ["Wo", "we\u00b7der", "Kopf", "noch", "Schweif", "am", "rech\u00b7ten", "En\u00b7de", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "KON", "NN", "ADV", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "War damals Mei\u00dfens Lust. Ein l\u00e4ppisch Zotenwesen", "tokens": ["War", "da\u00b7mals", "Mei\u00b7\u00dfens", "Lust", ".", "Ein", "l\u00e4p\u00b7pisch", "Zo\u00b7ten\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "NN", "$.", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Voll Unvernunft und Schmutz ward \u00fcberall gelesen.", "tokens": ["Voll", "Un\u00b7ver\u00b7nunft", "und", "Schmutz", "ward", "\u00fc\u00b7be\u00b7rall", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Satiren nannte man, was doch Pasquille sind;", "tokens": ["Sa\u00b7ti\u00b7ren", "nann\u00b7te", "man", ",", "was", "doch", "Pas\u00b7quil\u00b7le", "sind", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "$,", "PRELS", "ADV", "NN", "VAFIN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.14": {"text": "Ein Trauerspiel, ein St\u00fcck, wo Harlekin gewinnt;", "tokens": ["Ein", "Trau\u00b7er\u00b7spiel", ",", "ein", "St\u00fcck", ",", "wo", "Har\u00b7le\u00b7kin", "ge\u00b7winnt", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "PWAV", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Ein Lustspiel, wo Pandolf nebst zwanzig andern Thoren,", "tokens": ["Ein", "Lust\u00b7spiel", ",", "wo", "Pan\u00b7dolf", "nebst", "zwan\u00b7zig", "an\u00b7dern", "Tho\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWAV", "NE", "APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.16": {"text": "Des Lederh\u00e4ndlers Zweck zu hindern sich verschworen;", "tokens": ["Des", "Le\u00b7der\u00b7h\u00e4nd\u00b7lers", "Zweck", "zu", "hin\u00b7dern", "sich", "ver\u00b7schwo\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wo sich ein Poltergeist auf hundert Arten zeigt,", "tokens": ["Wo", "sich", "ein", "Pol\u00b7ter\u00b7geist", "auf", "hun\u00b7dert", "Ar\u00b7ten", "zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "APPR", "CARD", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Und Doctor Faust das Volk zu Zauberk\u00fcnsten neigt.", "tokens": ["Und", "Doc\u00b7tor", "Faust", "das", "Volk", "zu", "Zau\u00b7ber\u00b7k\u00fcns\u00b7ten", "neigt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das epische Gedicht war vollends gar vergessen:", "tokens": ["Das", "e\u00b7pisc\u00b7he", "Ge\u00b7dicht", "war", "vol\u00b7lends", "gar", "ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Warum? solch hohes Zeug bringt keinem was zu essen.", "tokens": ["Wa\u00b7rum", "?", "solch", "ho\u00b7hes", "Zeug", "bringt", "kei\u00b7nem", "was", "zu", "es\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PIAT", "ADJA", "NN", "VVFIN", "PIS", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Brautsuppen kochte man f\u00fcr Braut und Br\u00e4utigam;", "tokens": ["Brau\u00b7tsup\u00b7pen", "koch\u00b7te", "man", "f\u00fcr", "Braut", "und", "Br\u00e4u\u00b7ti\u00b7gam", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "APPR", "NN", "KON", "NE", "$."], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}, "line.22": {"text": "Ein Chronodistichon, ein k\u00fcnstlich Anagramm,", "tokens": ["Ein", "Chro\u00b7no\u00b7dis\u00b7ti\u00b7chon", ",", "ein", "k\u00fcnst\u00b7lich", "A\u00b7na\u00b7gramm", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Ein Cabbalisticum, und, da\u00df wir nichts vers\u00e4umen,", "tokens": ["Ein", "Cab\u00b7ba\u00b7li\u00b7sti\u00b7cum", ",", "und", ",", "da\u00df", "wir", "nichts", "ver\u00b7s\u00e4u\u00b7men", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$,", "KON", "$,", "KOUS", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Manch R\u00e4thsel voller Schmutz, nebst Bild- und Leberreimen.", "tokens": ["Manch", "R\u00e4th\u00b7sel", "vol\u00b7ler", "Schmutz", ",", "nebst", "Bild", "und", "Le\u00b7ber\u00b7rei\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJA", "NN", "$,", "APPR", "TRUNC", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Hier brach mein Eifer los! der Weise von ", "tokens": ["Hier", "brach", "mein", "Ei\u00b7fer", "los", "!", "der", "Wei\u00b7se", "von"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$.", "ART", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und sein unsterblich Buch vom Dichten, winkten mir.", "tokens": ["Und", "sein", "uns\u00b7terb\u00b7lich", "Buch", "vom", "Dich\u00b7ten", ",", "wink\u00b7ten", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJD", "NN", "APPRART", "NN", "$,", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich las es \u00f6ffentlich, und sucht es einzusch\u00e4rfen,", "tokens": ["Ich", "las", "es", "\u00f6f\u00b7fent\u00b7lich", ",", "und", "sucht", "es", "ein\u00b7zu\u00b7sch\u00e4r\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "$,", "KON", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.4": {"text": "Und lehrte den Geschmack des P\u00f6belvolks verwerfen.", "tokens": ["Und", "lehr\u00b7te", "den", "Ge\u00b7schmack", "des", "P\u00f6\u00b7bel\u00b7volks", "ver\u00b7wer\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zum Muster wies ich an, die Sch\u00f6nheit der Natur;", "tokens": ["Zum", "Mus\u00b7ter", "wies", "ich", "an", ",", "die", "Sch\u00f6n\u00b7heit", "der", "Na\u00b7tur", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie meine Dichtkunst schon auf der Lateiner Spur", "tokens": ["Wie", "mei\u00b7ne", "Dicht\u00b7kunst", "schon", "auf", "der", "La\u00b7tei\u00b7ner", "Spur"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Aus dem ", "tokens": ["Aus", "dem"], "token_info": ["word", "word"], "pos": ["APPR", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.8": {"text": "Der kurz vorher Vernunft und Tugend fast bezwungen.", "tokens": ["Der", "kurz", "vor\u00b7her", "Ver\u00b7nunft", "und", "Tu\u00b7gend", "fast", "be\u00b7zwun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "NN", "KON", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ganz Leipzig dankte mir; man that die Augen auf;", "tokens": ["Ganz", "Leip\u00b7zig", "dank\u00b7te", "mir", ";", "man", "that", "die", "Au\u00b7gen", "auf", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PPER", "$.", "PIS", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der richtige Geschmack gewann nun freyern Lauf,", "tokens": ["Der", "rich\u00b7ti\u00b7ge", "Ge\u00b7schmack", "ge\u00b7wann", "nun", "frey\u00b7ern", "Lauf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Halb Deutschland fiel uns bey, und eiferte mit Sachsen", "tokens": ["Halb", "Deutschland", "fiel", "uns", "bey", ",", "und", "ei\u00b7fer\u00b7te", "mit", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,", "KON", "VVFIN", "APPR", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Wo Geist, Vernunft und Witz am sch\u00f6nsten k\u00f6nnte wachsen.", "tokens": ["Wo", "Geist", ",", "Ver\u00b7nunft", "und", "Witz", "am", "sch\u00f6ns\u00b7ten", "k\u00f6nn\u00b7te", "wach\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "NN", "KON", "NN", "APPRART", "ADJA", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Doch leider! nur umsonst! Ein ungleich heller Licht,", "tokens": ["Doch", "lei\u00b7der", "!", "nur", "um\u00b7sonst", "!", "Ein", "un\u00b7gleich", "hel\u00b7ler", "Licht", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "ADV", "ADV", "$.", "ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das aus den Alpen quillt, und durch die Nebel bricht,", "tokens": ["Das", "aus", "den", "Al\u00b7pen", "quillt", ",", "und", "durch", "die", "Ne\u00b7bel", "bricht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "VVFIN", "$,", "KON", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die unsre Geister noch mit Wahn und Irrthum deckten,", "tokens": ["Die", "uns\u00b7re", "Geis\u00b7ter", "noch", "mit", "Wahn", "und", "Irr\u00b7thum", "deck\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bestralt der Dichter Heer, die noch im Dunkeln steckten.", "tokens": ["Be\u00b7stralt", "der", "Dich\u00b7ter", "Heer", ",", "die", "noch", "im", "Dun\u00b7keln", "steck\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NN", "$,", "PRELS", "ADV", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Man sucht den ", "tokens": ["Man", "sucht", "den"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Man lehrt ihn Schweizerdeutsch, man sucht ihn anzupreisen,", "tokens": ["Man", "lehrt", "ihn", "Schwei\u00b7zer\u00b7deutsch", ",", "man", "sucht", "ihn", "an\u00b7zu\u00b7prei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "$,", "PIS", "VVFIN", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und seine Sch\u00f6nheit recht der blinden Welt zu weisen.", "tokens": ["Und", "sei\u00b7ne", "Sch\u00f6n\u00b7heit", "recht", "der", "blin\u00b7den", "Welt", "zu", "wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ADV", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ein Auge blinzelt nur, das man aus dicker Nacht", "tokens": ["Ein", "Au\u00b7ge", "blin\u00b7zelt", "nur", ",", "das", "man", "aus", "di\u00b7cker", "Nacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "$,", "PRELS", "PIS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "In helle Zimmer f\u00fchrt, vor vieler Kerzen Pracht.", "tokens": ["In", "hel\u00b7le", "Zim\u00b7mer", "f\u00fchrt", ",", "vor", "vie\u00b7ler", "Ker\u00b7zen", "Pracht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$,", "APPR", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wenn Spiegel ohne Zahl der Stralen Glanz verst\u00e4rken;", "tokens": ["Wenn", "Spie\u00b7gel", "oh\u00b7ne", "Zahl", "der", "Stra\u00b7len", "Glanz", "ver\u00b7st\u00e4r\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "NN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So wird es anfangs blind und kann fast nichts bemerken.", "tokens": ["So", "wird", "es", "an\u00b7fangs", "blind", "und", "kann", "fast", "nichts", "be\u00b7mer\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "KON", "VMFIN", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "So schien uns ", "tokens": ["So", "schien", "uns"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.13": {"text": "Wer es zuerst erblickt, empfand die Sch\u00f6nheit nicht,", "tokens": ["Wer", "es", "zu\u00b7erst", "er\u00b7blickt", ",", "emp\u00b7fand", "die", "Sch\u00f6n\u00b7heit", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVPP", "$,", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wo Satan wider Gott erst ", "tokens": ["Wo", "Sa\u00b7tan", "wi\u00b7der", "Gott", "erst"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NE", "APPR", "NN", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.15": {"text": "Bis Gott und Mensch verspielt und Satan herrlich sieget.", "tokens": ["Bis", "Gott", "und", "Mensch", "ver\u00b7spielt", "und", "Sa\u00b7tan", "herr\u00b7lich", "sie\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "KON", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Doch endlich fiengen wir, auch in der finstern Kluft", "tokens": ["Doch", "end\u00b7lich", "fi\u00b7en\u00b7gen", "wir", ",", "auch", "in", "der", "fins\u00b7tern", "Kluft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "$,", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.17": {"text": "Wo Tod und S\u00fcnde haust, und von der heitern Luft", "tokens": ["Wo", "Tod", "und", "S\u00fcn\u00b7de", "haust", ",", "und", "von", "der", "hei\u00b7tern", "Luft"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "KON", "NN", "VVFIN", "$,", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Sich durch ein neunfach Thor, und soviel Mauren trennet,", "tokens": ["Sich", "durch", "ein", "neun\u00b7fach", "Thor", ",", "und", "so\u00b7viel", "Mau\u00b7ren", "tren\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "ADJA", "NN", "$,", "KON", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das Feuer anzusehn, das ", "tokens": ["Das", "Feu\u00b7er", "an\u00b7zu\u00b7sehn", ",", "das"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VVIZU", "$,", "PRELS"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.20": {"text": "Ein Kind scheut anfangs nichts von der Gespenster Macht,", "tokens": ["Ein", "Kind", "scheut", "an\u00b7fangs", "nichts", "von", "der", "Ge\u00b7spens\u00b7ter", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIS", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Es sieht, es h\u00f6rt sie nicht: doch, giebt es flei\u00dfig acht,", "tokens": ["Es", "sieht", ",", "es", "h\u00f6rt", "sie", "nicht", ":", "doch", ",", "giebt", "es", "flei\u00b7\u00dfig", "acht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "$,", "VVFIN", "PPER", "ADJD", "CARD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Was kluge Vetteln uns von Poltergeistern lehren:", "tokens": ["Was", "klu\u00b7ge", "Vet\u00b7teln", "uns", "von", "Pol\u00b7ter\u00b7geis\u00b7tern", "leh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJA", "NN", "PPER", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "So f\u00e4ngt es an zu sehn, so f\u00e4ngt es an zu h\u00f6ren.", "tokens": ["So", "f\u00e4ngt", "es", "an", "zu", "sehn", ",", "so", "f\u00e4ngt", "es", "an", "zu", "h\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$,", "ADV", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Wie Eulen auch bey Nacht mehr als am Tage sehn,", "tokens": ["Wie", "Eu\u00b7len", "auch", "bey", "Nacht", "mehr", "als", "am", "Ta\u00b7ge", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "ADV", "APPR", "NN", "PIAT", "KOKOM", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "So kann es itzt von uns ", "tokens": ["So", "kann", "es", "itzt", "von", "uns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "APPR", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.26": {"text": "Zumal seit dem man uns \u00e4sthetisch denken lehret,", "tokens": ["Zu\u00b7mal", "seit", "dem", "man", "uns", "\u00e4s\u00b7the\u00b7tisch", "den\u00b7ken", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "PIS", "PPER", "ADJD", "VVINF", "VVFIN", "$,"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "Vernunft und Licht verwirft, die Dunkelheit verehret.", "tokens": ["Ver\u00b7nunft", "und", "Licht", "ver\u00b7wirft", ",", "die", "Dun\u00b7kel\u00b7heit", "ver\u00b7eh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Jedoch ein gr\u00f6\u00dfrer Geist, als Milton zeiget sich.", "tokens": ["Je\u00b7doch", "ein", "gr\u00f6\u00df\u00b7rer", "Geist", ",", "als", "Mil\u00b7ton", "zei\u00b7get", "sich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "KOUS", "NE", "VVFIN", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "O ", "tokens": ["O"], "token_info": ["word"], "pos": ["NE"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Ein deutsches Meisterst\u00fcck, die Frucht von ", "tokens": ["Ein", "deut\u00b7sches", "Meis\u00b7ter\u00b7st\u00fcck", ",", "die", "Frucht", "von"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "APPR"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Z\u00fcrch der Welt geschenkt, zu sehen und zu ehren.", "tokens": ["Die", "Z\u00fcrch", "der", "Welt", "ge\u00b7schenkt", ",", "zu", "se\u00b7hen", "und", "zu", "eh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVPP", "$,", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Schon seit so langer Zeit zu sehn begierig war;", "tokens": ["Schon", "seit", "so", "lan\u00b7ger", "Zeit", "zu", "sehn", "be\u00b7gie\u00b7rig", "war", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nein, den ein ander Chor von unbeschnittnen Ohren,", "tokens": ["Nein", ",", "den", "ein", "an\u00b7der", "Chor", "von", "un\u00b7be\u00b7schnitt\u00b7nen", "Oh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Sich in Gedanken l\u00e4ngst zum Trost und Heil erkohren.", "tokens": ["Sich", "in", "Ge\u00b7dan\u00b7ken", "l\u00e4ngst", "zum", "Trost", "und", "Heil", "er\u00b7koh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "ADV", "APPRART", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Das aller Britten Stolz durch deutsche Kr\u00e4fte bricht;", "tokens": ["Das", "al\u00b7ler", "Brit\u00b7ten", "Stolz", "durch", "deut\u00b7sche", "Kr\u00e4f\u00b7te", "bricht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Weit mehr als St. ", "tokens": ["Weit", "mehr", "als", "St."], "token_info": ["word", "word", "word", "abbreviation"], "pos": ["ADJD", "PIAT", "KOKOM", "NE"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "Den ", "tokens": ["Den"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.11": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}, "line.12": {"text": "Der Lehrer selbst erstaunt vor dem zu gro\u00dfen Sch\u00fcler,", "tokens": ["Der", "Leh\u00b7rer", "selbst", "er\u00b7staunt", "vor", "dem", "zu", "gro\u00b7\u00dfen", "Sch\u00fc\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "APPR", "ART", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und bethet ihn fast an. Der hei\u00dfe Wunsch so vieler,", "tokens": ["Und", "be\u00b7thet", "ihn", "fast", "an", ".", "Der", "hei\u00b7\u00dfe", "Wunsch", "so", "vie\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKVZ", "$.", "ART", "ADJA", "NN", "ADV", "PIAT", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Ein deutsches Heldenwerk von solchem Schrot zu sehn,", "tokens": ["Ein", "deut\u00b7sches", "Hel\u00b7den\u00b7werk", "von", "sol\u00b7chem", "Schrot", "zu", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Dem Himmel sey gedankt! ist nicht umsonst geschehn.", "tokens": ["Dem", "Him\u00b7mel", "sey", "ge\u00b7dankt", "!", "ist", "nicht", "um\u00b7sonst", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$.", "VAFIN", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Hier stralt ein dunkler Glanz. Hier st\u00fctzet man den Glauben", "tokens": ["Hier", "stralt", "ein", "dunk\u00b7ler", "Glanz", ".", "Hier", "st\u00fct\u00b7zet", "man", "den", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$.", "ADV", "VVFIN", "PIS", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Mit Fabeln neuer Art: wer will ihn uns nun rauben?", "tokens": ["Mit", "Fa\u00b7beln", "neu\u00b7er", "Art", ":", "wer", "will", "ihn", "uns", "nun", "rau\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$.", "PWS", "VMFIN", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Was kein Prophet gesehn und kein Evangelist,", "tokens": ["Was", "kein", "Pro\u00b7phet", "ge\u00b7sehn", "und", "kein", "E\u00b7van\u00b7ge\u00b7list", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVPP", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Was kein Apostel wu\u00dft, das lernst du hier, mein Christ!", "tokens": ["Was", "kein", "A\u00b7pos\u00b7tel", "wu\u00dft", ",", "das", "lernst", "du", "hier", ",", "mein", "Christ", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Der Schriftgelehrten Witz wird uns, mit tiefen Schl\u00fcssen,", "tokens": ["Der", "Schrift\u00b7ge\u00b7lehr\u00b7ten", "Witz", "wird", "uns", ",", "mit", "tie\u00b7fen", "Schl\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Die\u00df neue Bibelbuch hinfort erkl\u00e4ren m\u00fcssen.", "tokens": ["Die\u00df", "neu\u00b7e", "Bi\u00b7bel\u00b7buch", "hin\u00b7fort", "er\u00b7kl\u00e4\u00b7ren", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Auf nun, ", "tokens": ["Auf", "nun", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Und stelle Dir die\u00df Werk zum Musterbilde vor.", "tokens": ["Und", "stel\u00b7le", "Dir", "die\u00df", "Werk", "zum", "Mus\u00b7ter\u00b7bil\u00b7de", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PDS", "NN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wer ihm nicht \u00e4hnlich schreibt, kann Deutschland nicht gefallen;", "tokens": ["Wer", "ihm", "nicht", "\u00e4hn\u00b7lich", "schreibt", ",", "kann", "Deutschland", "nicht", "ge\u00b7fal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "ADJD", "VVFIN", "$,", "VMFIN", "NE", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dagegen ", "tokens": ["Da\u00b7ge\u00b7gen"], "token_info": ["word"], "pos": ["PAV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Ein wenig hat ", "tokens": ["Ein", "we\u00b7nig", "hat"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIS", "VAFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Noch mehr war ", "tokens": ["Noch", "mehr", "war"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "Der Grieche ", "tokens": ["Der", "Grie\u00b7che"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "Wies noch am leidlichsten ein recht \u00e4sthetisch Bild.", "tokens": ["Wies", "noch", "am", "leid\u00b7lichs\u00b7ten", "ein", "recht", "\u00e4s\u00b7the\u00b7tisch", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "ADJA", "ART", "ADJD", "ADJD", "NN", "$."], "meter": "+--+--+-+--+", "measure": "dactylic.di.plus"}, "line.10": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.11": {"text": "Sind der Verg\u00f6ttrung werth, und m\u00fcssen Tempel haben.", "tokens": ["Sind", "der", "Ver\u00b7g\u00f6t\u00b7trung", "werth", ",", "und", "m\u00fcs\u00b7sen", "Tem\u00b7pel", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADJD", "$,", "KON", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Wiewohl ich sehe schon: Du bleibst auf Deinem Sinn!", "tokens": ["Wie\u00b7wohl", "ich", "se\u00b7he", "schon", ":", "Du", "bleibst", "auf", "Dei\u00b7nem", "Sinn", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ADV", "$.", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gehst Du von dem nicht ab, dem seit dreytausend Jahren,", "tokens": ["Gehst", "Du", "von", "dem", "nicht", "ab", ",", "dem", "seit", "drey\u00b7tau\u00b7send", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PRELS", "PTKNEG", "PTKVZ", "$,", "PRELS", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die gr\u00f6\u00dften Geister auch zu folgen eifrig waren;", "tokens": ["Die", "gr\u00f6\u00df\u00b7ten", "Geis\u00b7ter", "auch", "zu", "fol\u00b7gen", "eif\u00b7rig", "wa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Gut! folge Deinem Kopf. Du liebst ein deutlich Wesen?", "tokens": ["Gut", "!", "fol\u00b7ge", "Dei\u00b7nem", "Kopf", ".", "Du", "liebst", "ein", "deut\u00b7lich", "We\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$.", "VVFIN", "PPOSAT", "NN", "$.", "PPER", "VVFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Vernimm das Donnerwort: ", "tokens": ["Ver\u00b7nimm", "das", "Don\u00b7ner\u00b7wort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVIMP", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Denn dazu hat Dich mir des Schicksals Huld gegeben;", "tokens": ["Denn", "da\u00b7zu", "hat", "Dich", "mir", "des", "Schick\u00b7sals", "Huld", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VAFIN", "PPER", "PPER", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nachdem zwey Drittheil schon des Laufs vor\u00fcber sind,", "tokens": ["Nach\u00b7dem", "zwey", "Dritt\u00b7heil", "schon", "des", "Laufs", "vor\u00b7\u00fc\u00b7ber", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "NN", "ADV", "ART", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und meiner Scheitel H\u00f6h schon Reif und Schnee gewinnt.", "tokens": ["Und", "mei\u00b7ner", "Schei\u00b7tel", "H\u00f6h", "schon", "Reif", "und", "Schnee", "ge\u00b7winnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "NN", "ADV", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Aus Costnitz! h\u00e4tte mirs auch jemals tr\u00e4umen k\u00f6nnen?", "tokens": ["Aus", "Cost\u00b7nitz", "!", "h\u00e4t\u00b7te", "mirs", "auch", "je\u00b7mals", "tr\u00e4u\u00b7men", "k\u00f6n\u00b7nen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$.", "VAFIN", "NE", "ADV", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Aus Schwaben sollte mir ", "tokens": ["Aus", "Schwa\u00b7ben", "soll\u00b7te", "mir"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "VMFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Wer h\u00e4tte das geglaubt? seit dem ein b\u00f6ser Schwab,", "tokens": ["Wer", "h\u00e4t\u00b7te", "das", "ge\u00b7glaubt", "?", "seit", "dem", "ein", "b\u00f6\u00b7ser", "Schwab", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PDS", "VVPP", "$.", "APPR", "ART", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Mir lebenslang von Stolz und Ha\u00df die Proben gab;", "tokens": ["Mir", "le\u00b7bens\u00b7lang", "von", "Stolz", "und", "Ha\u00df", "die", "Pro\u00b7ben", "gab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Der bittern Rachgier Gift f\u00fcr ungeschehne Sachen,", "tokens": ["Der", "bit\u00b7tern", "Rach\u00b7gier", "Gift", "f\u00fcr", "un\u00b7ge\u00b7scheh\u00b7ne", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Durch h\u00f6hern Arm gesch\u00fctzt, mir wu\u00dfte schwer zu machen.", "tokens": ["Durch", "h\u00f6\u00b7hern", "Arm", "ge\u00b7sch\u00fctzt", ",", "mir", "wu\u00df\u00b7te", "schwer", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$,", "PPER", "VVFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Nun liegt er in der Gruft; begl\u00fcckt, wie er geglaubt,", "tokens": ["Nun", "liegt", "er", "in", "der", "Gruft", ";", "be\u00b7gl\u00fcckt", ",", "wie", "er", "ge\u00b7glaubt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$.", "VVPP", "$,", "PWAV", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wenn ihm an Geist und Leib der Tod das Seyn geraubt.", "tokens": ["Wenn", "ihm", "an", "Geist", "und", "Leib", "der", "Tod", "das", "Seyn", "ge\u00b7raubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "KON", "NN", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "So lern ich denn an Dir, und wenig andern Proben", "tokens": ["So", "lern", "ich", "denn", "an", "Dir", ",", "und", "we\u00b7nig", "an\u00b7dern", "Pro\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$,", "KON", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kein Volk sey \u00fcberhaupt zu schelten und zu loben.", "tokens": ["Kein", "Volk", "sey", "\u00fc\u00b7ber\u00b7haupt", "zu", "schel\u00b7ten", "und", "zu", "lo\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADV", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein jedes Land erzeugt Gem\u00fcther edler Art;", "tokens": ["Ein", "je\u00b7des", "Land", "er\u00b7zeugt", "Ge\u00b7m\u00fc\u00b7ther", "ed\u00b7ler", "Art", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wohl dem! dem eins davon in Freundschaft g\u00fcnstig ward.", "tokens": ["Wohl", "dem", "!", "dem", "eins", "da\u00b7von", "in", "Freund\u00b7schaft", "g\u00fcns\u00b7tig", "ward", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "$.", "ART", "PIS", "PAV", "APPR", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die\u00df Gl\u00fcck ertheilest Du mir ferngebohrnem Preu\u00dfen;", "tokens": ["Die\u00df", "Gl\u00fcck", "er\u00b7thei\u00b7lest", "Du", "mir", "fern\u00b7ge\u00b7bohr\u00b7nem", "Preu\u00b7\u00dfen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VVFIN", "PPER", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Den jener Bernsteinstrand kann seinen Z\u00f6gling hei\u00dfen,", "tokens": ["Den", "je\u00b7ner", "Bern\u00b7stein\u00b7strand", "kann", "sei\u00b7nen", "Z\u00f6g\u00b7ling", "hei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Bis ihn das Gl\u00fcck hieher in Deutschlands Kern gebracht.", "tokens": ["Bis", "ihn", "das", "Gl\u00fcck", "hie\u00b7her", "in", "Deutschlands", "Kern", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "PAV", "APPR", "NE", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Hier hab ich Geist und Witz noch feiner ausgeschliffen,", "tokens": ["Hier", "hab", "ich", "Geist", "und", "Witz", "noch", "fei\u00b7ner", "aus\u00b7ge\u00b7schlif\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "KON", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Was ", "tokens": ["Was"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.11": {"text": "Durch fremder Sprachen Licht das Deutsche mehr gest\u00e4rkt,", "tokens": ["Durch", "frem\u00b7der", "Spra\u00b7chen", "Licht", "das", "Deut\u00b7sche", "mehr", "ge\u00b7st\u00e4rkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und aus der Alten H\u00f6h der Neuern Fall bemerkt.", "tokens": ["Und", "aus", "der", "Al\u00b7ten", "H\u00f6h", "der", "Neu\u00b7ern", "Fall", "be\u00b7merkt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Hier fand ich ", "tokens": ["Hier", "fand", "ich"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.14": {"text": "So hab ich nach und nach die Wahrheit mehr verstanden:", "tokens": ["So", "hab", "ich", "nach", "und", "nach", "die", "Wahr\u00b7heit", "mehr", "ver\u00b7stan\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "KON", "APPR", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Da\u00df auch das beste Feld von selbst nur Unkraut tr\u00e4gt,", "tokens": ["Da\u00df", "auch", "das", "bes\u00b7te", "Feld", "von", "selbst", "nur", "Un\u00b7kraut", "tr\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "APPR", "ADV", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Wenn keines G\u00e4rtners Hand den Flei\u00df daran gelegt.", "tokens": ["Wenn", "kei\u00b7nes", "G\u00e4rt\u00b7ners", "Hand", "den", "Flei\u00df", "da\u00b7ran", "ge\u00b7legt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "NN", "ART", "NN", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wie th\u00f6richt ist es denn, von Sonn und Luft zu sprechen,", "tokens": ["Wie", "th\u00f6\u00b7richt", "ist", "es", "denn", ",", "von", "Sonn", "und", "Luft", "zu", "spre\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PPER", "ADV", "$,", "APPR", "NN", "KON", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Da Griechenland und Rom der Regel Nachdruck schw\u00e4chen?", "tokens": ["Da", "Grie\u00b7chen\u00b7land", "und", "Rom", "der", "Re\u00b7gel", "Nach\u00b7druck", "schw\u00e4\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "KON", "NE", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Behaupte wie du willst, hochweiser ", "tokens": ["Be\u00b7haup\u00b7te", "wie", "du", "willst", ",", "hoch\u00b7wei\u00b7ser"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "KOKOM", "PPER", "VMFIN", "$,", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Das Clima mache klug. Ein Kluger lacht dazu!", "tokens": ["Das", "Cli\u00b7ma", "ma\u00b7che", "klug", ".", "Ein", "Klu\u00b7ger", "lacht", "da\u00b7zu", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$.", "ART", "NN", "VVFIN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Und l\u00e4\u00dft zur Probe, dich die Menschen, gleich den Bl\u00fcthen,", "tokens": ["Und", "l\u00e4\u00dft", "zur", "Pro\u00b7be", ",", "dich", "die", "Men\u00b7schen", ",", "gleich", "den", "Bl\u00fc\u00b7then", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$,", "PRF", "ART", "NN", "$,", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}}, "stanza.14": {"line.1": {"text": "Allein best\u00e4tigt nicht, Dein Beyspiel, ", "tokens": ["Al\u00b7lein", "be\u00b7st\u00e4\u00b7tigt", "nicht", ",", "Dein", "Bey\u00b7spiel", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was, \u00fcberhaupt gesagt, so widersinnisch scheint?", "tokens": ["Was", ",", "\u00fc\u00b7ber\u00b7haupt", "ge\u00b7sagt", ",", "so", "wi\u00b7der\u00b7sin\u00b7nisch", "scheint", "?"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "$,", "ADV", "VVPP", "$,", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein w\u00e4rmer Land hat Dir Empfindung Geist und Leben;", "tokens": ["Ein", "w\u00e4r\u00b7mer", "Land", "hat", "Dir", "Emp\u00b7fin\u00b7dung", "Geist", "und", "Le\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "NN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Mir nur der kalte Belt ein F\u00fcnckchen Witz gegeben.", "tokens": ["Mir", "nur", "der", "kal\u00b7te", "Belt", "ein", "F\u00fcn\u00b7ck\u00b7chen", "Witz", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Mit n\u00e4hern Blicken scho\u00df die Sonne Dir zu gut,", "tokens": ["Mit", "n\u00e4\u00b7hern", "Bli\u00b7cken", "scho\u00df", "die", "Son\u00b7ne", "Dir", "zu", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "PPER", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Viel mildre Stralen ab, als sie am Pregel thut.", "tokens": ["Viel", "mild\u00b7re", "Stra\u00b7len", "ab", ",", "als", "sie", "am", "Pre\u00b7gel", "thut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "PTKVZ", "$,", "KOUS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Kein Wunder! da\u00df Dein Geist sich \u00fcber mich geschwungen,", "tokens": ["Kein", "Wun\u00b7der", "!", "da\u00df", "Dein", "Geist", "sich", "\u00fc\u00b7ber", "mich", "ge\u00b7schwun\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$.", "KOUS", "PPOSAT", "NN", "PRF", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Als Du die Lust der Welt, ", "tokens": ["Als", "Du", "die", "Lust", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "So gern ich die\u00df gesteh, so falsch ist jens dabey.", "tokens": ["So", "gern", "ich", "die\u00df", "ge\u00b7steh", ",", "so", "falsch", "ist", "jens", "da\u00b7bey", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PDS", "VVFIN", "$,", "ADV", "ADJD", "VAFIN", "ADV", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Schuff denn der Sonnenstral in Costnitz einerley?", "tokens": ["Schuff", "denn", "der", "Son\u00b7nen\u00b7stral", "in", "Cost\u00b7nitz", "ei\u00b7ner\u00b7ley", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ART", "NN", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Wie kams, da\u00df auf der Bank, wo ", "tokens": ["Wie", "kams", ",", "da\u00df", "auf", "der", "Bank", ",", "wo"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["PWAV", "NE", "$,", "KOUS", "APPR", "ART", "NN", "$,", "PWAV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.12": {"text": "Sie allen Sch\u00fclern nicht gleichviel Witz zugemessen?", "tokens": ["Sie", "al\u00b7len", "Sch\u00fc\u00b7lern", "nicht", "gleich\u00b7viel", "Witz", "zu\u00b7ge\u00b7mes\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "NN", "PTKNEG", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-++-+--+-", "measure": "iambic.hexa.relaxed"}, "line.13": {"text": "Und hat sie das gethan; wo sind die andern nun?", "tokens": ["Und", "hat", "sie", "das", "ge\u00b7than", ";", "wo", "sind", "die", "an\u00b7dern", "nun", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PDS", "VVPP", "$.", "PWAV", "VAFIN", "ART", "ADJA", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Warum verr\u00e4th sie nicht ihr Denken, Schreiben, Thun?", "tokens": ["Wa\u00b7rum", "ver\u00b7r\u00e4\u00b7th", "sie", "nicht", "ihr", "Den\u00b7ken", ",", "Schrei\u00b7ben", ",", "Thun", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "Wer kennet sie in Wien? O! wer kann das ergr\u00fcnden?", "tokens": ["Wer", "ken\u00b7net", "sie", "in", "Wi\u00b7en", "?", "O", "!", "wer", "kann", "das", "er\u00b7gr\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "APPR", "NE", "$.", "NE", "$.", "PWS", "VMFIN", "PDS", "VVINF", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "In soviel L\u00e4ndern ist doch nur ", "tokens": ["In", "so\u00b7viel", "L\u00e4n\u00b7dern", "ist", "doch", "nur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "So wie mein Vaterland nur ", "tokens": ["So", "wie", "mein", "Va\u00b7ter\u00b7land", "nur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "PPOSAT", "NN", "ADV"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.18": {"text": "Der durch erhabne Glut auch w\u00e4lsche Geister beugt.", "tokens": ["Der", "durch", "er\u00b7hab\u00b7ne", "Glut", "auch", "w\u00e4l\u00b7sche", "Geis\u00b7ter", "beugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ADJA", "NN", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Wohlauf, ", "tokens": ["Wohl\u00b7auf", ","], "token_info": ["word", "punct"], "pos": ["ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Komm, wage noch einmal Kalliopens Gesch\u00e4ffte.", "tokens": ["Komm", ",", "wa\u00b7ge", "noch", "ein\u00b7mal", "Kal\u00b7li\u00b7o\u00b7pens", "Ge\u00b7sch\u00e4ff\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "ADV", "ADV", "NE", "NN", "$."], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.3": {"text": "Nimm ihr heroisch Rohr der G\u00f6ttinn aus der Hand,", "tokens": ["Nimm", "ihr", "he\u00b7ro\u00b7isch", "Rohr", "der", "G\u00f6t\u00b7tinn", "aus", "der", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADJD", "NN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und mach uns abermal ", "tokens": ["Und", "mach", "uns", "a\u00b7ber\u00b7mal"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die so viel Thronen ziert; Die das Geschick erkohren,", "tokens": ["Die", "so", "viel", "Thro\u00b7nen", "ziert", ";", "Die", "das", "Ge\u00b7schick", "er\u00b7koh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIAT", "NN", "VVFIN", "$.", "ART", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Zeiten Schmuck zu seyn, die Sie zur Welt gebohren.", "tokens": ["Der", "Zei\u00b7ten", "Schmuck", "zu", "seyn", ",", "die", "Sie", "zur", "Welt", "ge\u00b7boh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VAINF", "$,", "PRELS", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dein Vorsatz ist so sch\u00f6n, als edel und gerecht:", "tokens": ["Dein", "Vor\u00b7satz", "ist", "so", "sch\u00f6n", ",", "als", "e\u00b7del", "und", "ge\u00b7recht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,", "KOUS", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Denn wo der Gegenstand des Dichters Kraft nicht schw\u00e4cht,", "tokens": ["Denn", "wo", "der", "Ge\u00b7gen\u00b7stand", "des", "Dich\u00b7ters", "Kraft", "nicht", "schw\u00e4cht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "ART", "NN", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ja sie vielmehr erh\u00f6ht; da mu\u00df es ihm gelingen,", "tokens": ["Ja", "sie", "viel\u00b7mehr", "er\u00b7h\u00f6ht", ";", "da", "mu\u00df", "es", "ihm", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PPER", "ADV", "VVPP", "$.", "ADV", "VMFIN", "PPER", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der Dichtkunst h\u00f6chsten Preis sich spielend zu erringen.", "tokens": ["Der", "Dicht\u00b7kunst", "h\u00f6chs\u00b7ten", "Preis", "sich", "spie\u00b7lend", "zu", "er\u00b7rin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "PRF", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Nur eins bek\u00fcmmert mich von allem was Du schreibst;", "tokens": ["Nur", "eins", "be\u00b7k\u00fcm\u00b7mert", "mich", "von", "al\u00b7lem", "was", "Du", "schreibst", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "PRF", "APPR", "PIS", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df Du voll Eigensinn bey jenen Mustern bleibst,", "tokens": ["Da\u00df", "Du", "voll", "Ei\u00b7gen\u00b7sinn", "bey", "je\u00b7nen", "Mus\u00b7tern", "bleibst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "NN", "APPR", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Griechenland und Rom der Welt zuerst gewiesen,", "tokens": ["Die", "Grie\u00b7chen\u00b7land", "und", "Rom", "der", "Welt", "zu\u00b7erst", "ge\u00b7wie\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NE", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn sie der Helden Lob nach der Natur gepriesen.", "tokens": ["Wenn", "sie", "der", "Hel\u00b7den", "Lob", "nach", "der", "Na\u00b7tur", "ge\u00b7prie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Du liesest den ", "tokens": ["Du", "lie\u00b7sest", "den"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ART"], "meter": "+---", "measure": "dactylic.init"}, "line.6": {"text": "Eh er die Stifter Roms, ", "tokens": ["Eh", "er", "die", "Stif\u00b7ter", "Roms", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "NE", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.7": {"text": "Nach Latien gef\u00fchrt. Der hie\u00df ja wohl vor Jahren,", "tokens": ["Nach", "La\u00b7ti\u00b7en", "ge\u00b7f\u00fchrt", ".", "Der", "hie\u00df", "ja", "wohl", "vor", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$.", "ART", "VVFIN", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Als Geist und Dichtkunst noch in ihrer Wiege waren,", "tokens": ["Als", "Geist", "und", "Dicht\u00b7kunst", "noch", "in", "ih\u00b7rer", "Wie\u00b7ge", "wa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ADV", "APPR", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Das Augenmerk der Kunst, der Vater von dem Witz,", "tokens": ["Das", "Au\u00b7gen\u00b7merk", "der", "Kunst", ",", "der", "Va\u00b7ter", "von", "dem", "Witz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der alles aufgekl\u00e4rt, als noch der Musen Sitz", "tokens": ["Der", "al\u00b7les", "auf\u00b7ge\u00b7kl\u00e4rt", ",", "als", "noch", "der", "Mu\u00b7sen", "Sitz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVPP", "$,", "KOUS", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Im Grajerlande lag. Jedoch zu unsern Zeiten", "tokens": ["Im", "Gra\u00b7jer\u00b7lan\u00b7de", "lag", ".", "Je\u00b7doch", "zu", "un\u00b7sern", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "$.", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Hat alles sich verkehrt, bis auf der Dichter Seyten.", "tokens": ["Hat", "al\u00b7les", "sich", "ver\u00b7kehrt", ",", "bis", "auf", "der", "Dich\u00b7ter", "Sey\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PRF", "VVPP", "$,", "KOUS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Ich glaubte sonst wie Du: bis ich nur j\u00fcngst gelernt,", "tokens": ["Ich", "glaub\u00b7te", "sonst", "wie", "Du", ":", "bis", "ich", "nur", "j\u00fcngst", "ge\u00b7lernt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "KOKOM", "PPER", "$.", "KOUS", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Da\u00df man durchs Alterthum sich von dem Ruhm entfernt,", "tokens": ["Da\u00df", "man", "durchs", "Al\u00b7ter\u00b7thum", "sich", "von", "dem", "Ruhm", "ent\u00b7fernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "PRF", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Ein Muster selbst zu seyn; da\u00df man die Geister hindert,", "tokens": ["Ein", "Mus\u00b7ter", "selbst", "zu", "seyn", ";", "da\u00df", "man", "die", "Geis\u00b7ter", "hin\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKZU", "VAINF", "$.", "KOUS", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Wenn die Vernunft den Flug der Phantasey vermindert,", "tokens": ["Wenn", "die", "Ver\u00b7nunft", "den", "Flug", "der", "Phan\u00b7ta\u00b7sey", "ver\u00b7min\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und kl\u00fcglich schreiben lehrt. Drum gib ein wenig acht,", "tokens": ["Und", "kl\u00fcg\u00b7lich", "schrei\u00b7ben", "lehrt", ".", "Drum", "gib", "ein", "we\u00b7nig", "acht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVINF", "VVFIN", "$.", "PAV", "VVIMP", "ART", "PIAT", "CARD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Was mich seit kurzer Zeit auf andern Sinn gebracht.", "tokens": ["Was", "mich", "seit", "kur\u00b7zer", "Zeit", "auf", "an\u00b7dern", "Sinn", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ADJA", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Wer zwanzig Jahre schon der Dichtkunst Regeln lehret,", "tokens": ["Wer", "zwan\u00b7zig", "Jah\u00b7re", "schon", "der", "Dicht\u00b7kunst", "Re\u00b7geln", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "CARD", "NN", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Verdient vieleicht ein Ohr! das ihn geduldig h\u00f6ret.", "tokens": ["Ver\u00b7di\u00b7ent", "vie\u00b7leicht", "ein", "Ohr", "!", "das", "ihn", "ge\u00b7dul\u00b7dig", "h\u00f6\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ADV", "ART", "NN", "$.", "PRELS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.17": {"line.1": {"text": "Als ", "tokens": ["Als"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Drey Jahr, eh er erblich, mir dieses Amt befahl;", "tokens": ["Drey", "Jahr", ",", "eh", "er", "er\u00b7blich", ",", "mir", "die\u00b7ses", "Amt", "be\u00b7fahl", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "KOUS", "PPER", "ADJD", "$,", "PPER", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "(vieleicht weil ich sehr oft, des Helden Gnadenproben", "tokens": ["(", "vie\u00b7leicht", "weil", "ich", "sehr", "oft", ",", "des", "Hel\u00b7den", "Gna\u00b7den\u00b7pro\u00b7ben"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ADV", "KOUS", "PPER", "ADV", "ADV", "$,", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "An Musen und Parna\u00df, der Wahrheit nach, erhoben)", "tokens": ["An", "Mu\u00b7sen", "und", "Par\u00b7na\u00df", ",", "der", "Wahr\u00b7heit", "nach", ",", "er\u00b7ho\u00b7ben", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "KON", "NE", "$,", "ART", "NN", "PTKVZ", "$,", "VVPP", "$("], "meter": "-+-+++-+-+-+-", "measure": "unknown.measure.septa"}, "line.5": {"text": "Beherrschte leider mich noch der verj\u00e4hrte Wahn:", "tokens": ["Be\u00b7herrschte", "lei\u00b7der", "mich", "noch", "der", "ver\u00b7j\u00e4hr\u00b7te", "Wahn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.6": {"text": "(wie ", "tokens": ["(", "wie"], "token_info": ["punct", "word"], "pos": ["$(", "KOKOM"], "meter": "-", "measure": "single.down"}, "line.7": {"text": "Man m\u00fcsse nach der Spur der alten Regeln gehen,", "tokens": ["Man", "m\u00fcs\u00b7se", "nach", "der", "Spur", "der", "al\u00b7ten", "Re\u00b7geln", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPR", "ART", "NN", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Dichtkunst auf den Grad der Griechen zu erh\u00f6hen.", "tokens": ["Die", "Dicht\u00b7kunst", "auf", "den", "Grad", "der", "Grie\u00b7chen", "zu", "er\u00b7h\u00f6\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und den vermi\u00dfte man. Ein dummes Quodlibet,", "tokens": ["Und", "den", "ver\u00b7mi\u00df\u00b7te", "man", ".", "Ein", "dum\u00b7mes", "Quod\u00b7li\u00b7bet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "PIS", "$.", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Wo weder Kopf noch Schweif am rechten Ende steht,", "tokens": ["Wo", "we\u00b7der", "Kopf", "noch", "Schweif", "am", "rech\u00b7ten", "En\u00b7de", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "KON", "NN", "ADV", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "War damals Mei\u00dfens Lust. Ein l\u00e4ppisch Zotenwesen", "tokens": ["War", "da\u00b7mals", "Mei\u00b7\u00dfens", "Lust", ".", "Ein", "l\u00e4p\u00b7pisch", "Zo\u00b7ten\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "NN", "$.", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Voll Unvernunft und Schmutz ward \u00fcberall gelesen.", "tokens": ["Voll", "Un\u00b7ver\u00b7nunft", "und", "Schmutz", "ward", "\u00fc\u00b7be\u00b7rall", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Satiren nannte man, was doch Pasquille sind;", "tokens": ["Sa\u00b7ti\u00b7ren", "nann\u00b7te", "man", ",", "was", "doch", "Pas\u00b7quil\u00b7le", "sind", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "$,", "PRELS", "ADV", "NN", "VAFIN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.14": {"text": "Ein Trauerspiel, ein St\u00fcck, wo Harlekin gewinnt;", "tokens": ["Ein", "Trau\u00b7er\u00b7spiel", ",", "ein", "St\u00fcck", ",", "wo", "Har\u00b7le\u00b7kin", "ge\u00b7winnt", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "PWAV", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Ein Lustspiel, wo Pandolf nebst zwanzig andern Thoren,", "tokens": ["Ein", "Lust\u00b7spiel", ",", "wo", "Pan\u00b7dolf", "nebst", "zwan\u00b7zig", "an\u00b7dern", "Tho\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWAV", "NE", "APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.16": {"text": "Des Lederh\u00e4ndlers Zweck zu hindern sich verschworen;", "tokens": ["Des", "Le\u00b7der\u00b7h\u00e4nd\u00b7lers", "Zweck", "zu", "hin\u00b7dern", "sich", "ver\u00b7schwo\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Wo sich ein Poltergeist auf hundert Arten zeigt,", "tokens": ["Wo", "sich", "ein", "Pol\u00b7ter\u00b7geist", "auf", "hun\u00b7dert", "Ar\u00b7ten", "zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "APPR", "CARD", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Und Doctor Faust das Volk zu Zauberk\u00fcnsten neigt.", "tokens": ["Und", "Doc\u00b7tor", "Faust", "das", "Volk", "zu", "Zau\u00b7ber\u00b7k\u00fcns\u00b7ten", "neigt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das epische Gedicht war vollends gar vergessen:", "tokens": ["Das", "e\u00b7pisc\u00b7he", "Ge\u00b7dicht", "war", "vol\u00b7lends", "gar", "ver\u00b7ges\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Warum? solch hohes Zeug bringt keinem was zu essen.", "tokens": ["Wa\u00b7rum", "?", "solch", "ho\u00b7hes", "Zeug", "bringt", "kei\u00b7nem", "was", "zu", "es\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PIAT", "ADJA", "NN", "VVFIN", "PIS", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Brautsuppen kochte man f\u00fcr Braut und Br\u00e4utigam;", "tokens": ["Brau\u00b7tsup\u00b7pen", "koch\u00b7te", "man", "f\u00fcr", "Braut", "und", "Br\u00e4u\u00b7ti\u00b7gam", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "APPR", "NN", "KON", "NE", "$."], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}, "line.22": {"text": "Ein Chronodistichon, ein k\u00fcnstlich Anagramm,", "tokens": ["Ein", "Chro\u00b7no\u00b7dis\u00b7ti\u00b7chon", ",", "ein", "k\u00fcnst\u00b7lich", "A\u00b7na\u00b7gramm", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Ein Cabbalisticum, und, da\u00df wir nichts vers\u00e4umen,", "tokens": ["Ein", "Cab\u00b7ba\u00b7li\u00b7sti\u00b7cum", ",", "und", ",", "da\u00df", "wir", "nichts", "ver\u00b7s\u00e4u\u00b7men", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$,", "KON", "$,", "KOUS", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Manch R\u00e4thsel voller Schmutz, nebst Bild- und Leberreimen.", "tokens": ["Manch", "R\u00e4th\u00b7sel", "vol\u00b7ler", "Schmutz", ",", "nebst", "Bild", "und", "Le\u00b7ber\u00b7rei\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJA", "NN", "$,", "APPR", "TRUNC", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Hier brach mein Eifer los! der Weise von ", "tokens": ["Hier", "brach", "mein", "Ei\u00b7fer", "los", "!", "der", "Wei\u00b7se", "von"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$.", "ART", "NN", "APPR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und sein unsterblich Buch vom Dichten, winkten mir.", "tokens": ["Und", "sein", "uns\u00b7terb\u00b7lich", "Buch", "vom", "Dich\u00b7ten", ",", "wink\u00b7ten", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJD", "NN", "APPRART", "NN", "$,", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich las es \u00f6ffentlich, und sucht es einzusch\u00e4rfen,", "tokens": ["Ich", "las", "es", "\u00f6f\u00b7fent\u00b7lich", ",", "und", "sucht", "es", "ein\u00b7zu\u00b7sch\u00e4r\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "$,", "KON", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.4": {"text": "Und lehrte den Geschmack des P\u00f6belvolks verwerfen.", "tokens": ["Und", "lehr\u00b7te", "den", "Ge\u00b7schmack", "des", "P\u00f6\u00b7bel\u00b7volks", "ver\u00b7wer\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zum Muster wies ich an, die Sch\u00f6nheit der Natur;", "tokens": ["Zum", "Mus\u00b7ter", "wies", "ich", "an", ",", "die", "Sch\u00f6n\u00b7heit", "der", "Na\u00b7tur", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie meine Dichtkunst schon auf der Lateiner Spur", "tokens": ["Wie", "mei\u00b7ne", "Dicht\u00b7kunst", "schon", "auf", "der", "La\u00b7tei\u00b7ner", "Spur"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "NN"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Aus dem ", "tokens": ["Aus", "dem"], "token_info": ["word", "word"], "pos": ["APPR", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.8": {"text": "Der kurz vorher Vernunft und Tugend fast bezwungen.", "tokens": ["Der", "kurz", "vor\u00b7her", "Ver\u00b7nunft", "und", "Tu\u00b7gend", "fast", "be\u00b7zwun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "NN", "KON", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ganz Leipzig dankte mir; man that die Augen auf;", "tokens": ["Ganz", "Leip\u00b7zig", "dank\u00b7te", "mir", ";", "man", "that", "die", "Au\u00b7gen", "auf", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PPER", "$.", "PIS", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der richtige Geschmack gewann nun freyern Lauf,", "tokens": ["Der", "rich\u00b7ti\u00b7ge", "Ge\u00b7schmack", "ge\u00b7wann", "nun", "frey\u00b7ern", "Lauf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Halb Deutschland fiel uns bey, und eiferte mit Sachsen", "tokens": ["Halb", "Deutschland", "fiel", "uns", "bey", ",", "und", "ei\u00b7fer\u00b7te", "mit", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,", "KON", "VVFIN", "APPR", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Wo Geist, Vernunft und Witz am sch\u00f6nsten k\u00f6nnte wachsen.", "tokens": ["Wo", "Geist", ",", "Ver\u00b7nunft", "und", "Witz", "am", "sch\u00f6ns\u00b7ten", "k\u00f6nn\u00b7te", "wach\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "NN", "KON", "NN", "APPRART", "ADJA", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Doch leider! nur umsonst! Ein ungleich heller Licht,", "tokens": ["Doch", "lei\u00b7der", "!", "nur", "um\u00b7sonst", "!", "Ein", "un\u00b7gleich", "hel\u00b7ler", "Licht", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "ADV", "ADV", "$.", "ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das aus den Alpen quillt, und durch die Nebel bricht,", "tokens": ["Das", "aus", "den", "Al\u00b7pen", "quillt", ",", "und", "durch", "die", "Ne\u00b7bel", "bricht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "VVFIN", "$,", "KON", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die unsre Geister noch mit Wahn und Irrthum deckten,", "tokens": ["Die", "uns\u00b7re", "Geis\u00b7ter", "noch", "mit", "Wahn", "und", "Irr\u00b7thum", "deck\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADV", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bestralt der Dichter Heer, die noch im Dunkeln steckten.", "tokens": ["Be\u00b7stralt", "der", "Dich\u00b7ter", "Heer", ",", "die", "noch", "im", "Dun\u00b7keln", "steck\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NN", "$,", "PRELS", "ADV", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Man sucht den ", "tokens": ["Man", "sucht", "den"], "token_info": ["word", "word", "word"], "pos": ["PIS", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Man lehrt ihn Schweizerdeutsch, man sucht ihn anzupreisen,", "tokens": ["Man", "lehrt", "ihn", "Schwei\u00b7zer\u00b7deutsch", ",", "man", "sucht", "ihn", "an\u00b7zu\u00b7prei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "$,", "PIS", "VVFIN", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und seine Sch\u00f6nheit recht der blinden Welt zu weisen.", "tokens": ["Und", "sei\u00b7ne", "Sch\u00f6n\u00b7heit", "recht", "der", "blin\u00b7den", "Welt", "zu", "wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ADV", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ein Auge blinzelt nur, das man aus dicker Nacht", "tokens": ["Ein", "Au\u00b7ge", "blin\u00b7zelt", "nur", ",", "das", "man", "aus", "di\u00b7cker", "Nacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "$,", "PRELS", "PIS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "In helle Zimmer f\u00fchrt, vor vieler Kerzen Pracht.", "tokens": ["In", "hel\u00b7le", "Zim\u00b7mer", "f\u00fchrt", ",", "vor", "vie\u00b7ler", "Ker\u00b7zen", "Pracht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$,", "APPR", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wenn Spiegel ohne Zahl der Stralen Glanz verst\u00e4rken;", "tokens": ["Wenn", "Spie\u00b7gel", "oh\u00b7ne", "Zahl", "der", "Stra\u00b7len", "Glanz", "ver\u00b7st\u00e4r\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "NN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So wird es anfangs blind und kann fast nichts bemerken.", "tokens": ["So", "wird", "es", "an\u00b7fangs", "blind", "und", "kann", "fast", "nichts", "be\u00b7mer\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "KON", "VMFIN", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "So schien uns ", "tokens": ["So", "schien", "uns"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.13": {"text": "Wer es zuerst erblickt, empfand die Sch\u00f6nheit nicht,", "tokens": ["Wer", "es", "zu\u00b7erst", "er\u00b7blickt", ",", "emp\u00b7fand", "die", "Sch\u00f6n\u00b7heit", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVPP", "$,", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wo Satan wider Gott erst ", "tokens": ["Wo", "Sa\u00b7tan", "wi\u00b7der", "Gott", "erst"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NE", "APPR", "NN", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.15": {"text": "Bis Gott und Mensch verspielt und Satan herrlich sieget.", "tokens": ["Bis", "Gott", "und", "Mensch", "ver\u00b7spielt", "und", "Sa\u00b7tan", "herr\u00b7lich", "sie\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "KON", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Doch endlich fiengen wir, auch in der finstern Kluft", "tokens": ["Doch", "end\u00b7lich", "fi\u00b7en\u00b7gen", "wir", ",", "auch", "in", "der", "fins\u00b7tern", "Kluft"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "$,", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.17": {"text": "Wo Tod und S\u00fcnde haust, und von der heitern Luft", "tokens": ["Wo", "Tod", "und", "S\u00fcn\u00b7de", "haust", ",", "und", "von", "der", "hei\u00b7tern", "Luft"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "KON", "NN", "VVFIN", "$,", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Sich durch ein neunfach Thor, und soviel Mauren trennet,", "tokens": ["Sich", "durch", "ein", "neun\u00b7fach", "Thor", ",", "und", "so\u00b7viel", "Mau\u00b7ren", "tren\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "ADJA", "NN", "$,", "KON", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das Feuer anzusehn, das ", "tokens": ["Das", "Feu\u00b7er", "an\u00b7zu\u00b7sehn", ",", "das"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VVIZU", "$,", "PRELS"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.20": {"text": "Ein Kind scheut anfangs nichts von der Gespenster Macht,", "tokens": ["Ein", "Kind", "scheut", "an\u00b7fangs", "nichts", "von", "der", "Ge\u00b7spens\u00b7ter", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PIS", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Es sieht, es h\u00f6rt sie nicht: doch, giebt es flei\u00dfig acht,", "tokens": ["Es", "sieht", ",", "es", "h\u00f6rt", "sie", "nicht", ":", "doch", ",", "giebt", "es", "flei\u00b7\u00dfig", "acht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "$.", "ADV", "$,", "VVFIN", "PPER", "ADJD", "CARD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Was kluge Vetteln uns von Poltergeistern lehren:", "tokens": ["Was", "klu\u00b7ge", "Vet\u00b7teln", "uns", "von", "Pol\u00b7ter\u00b7geis\u00b7tern", "leh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJA", "NN", "PPER", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "So f\u00e4ngt es an zu sehn, so f\u00e4ngt es an zu h\u00f6ren.", "tokens": ["So", "f\u00e4ngt", "es", "an", "zu", "sehn", ",", "so", "f\u00e4ngt", "es", "an", "zu", "h\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$,", "ADV", "VVFIN", "PPER", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Wie Eulen auch bey Nacht mehr als am Tage sehn,", "tokens": ["Wie", "Eu\u00b7len", "auch", "bey", "Nacht", "mehr", "als", "am", "Ta\u00b7ge", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "ADV", "APPR", "NN", "PIAT", "KOKOM", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "So kann es itzt von uns ", "tokens": ["So", "kann", "es", "itzt", "von", "uns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "APPR", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.26": {"text": "Zumal seit dem man uns \u00e4sthetisch denken lehret,", "tokens": ["Zu\u00b7mal", "seit", "dem", "man", "uns", "\u00e4s\u00b7the\u00b7tisch", "den\u00b7ken", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "PIS", "PPER", "ADJD", "VVINF", "VVFIN", "$,"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "Vernunft und Licht verwirft, die Dunkelheit verehret.", "tokens": ["Ver\u00b7nunft", "und", "Licht", "ver\u00b7wirft", ",", "die", "Dun\u00b7kel\u00b7heit", "ver\u00b7eh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Jedoch ein gr\u00f6\u00dfrer Geist, als Milton zeiget sich.", "tokens": ["Je\u00b7doch", "ein", "gr\u00f6\u00df\u00b7rer", "Geist", ",", "als", "Mil\u00b7ton", "zei\u00b7get", "sich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "KOUS", "NE", "VVFIN", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "O ", "tokens": ["O"], "token_info": ["word"], "pos": ["NE"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Ein deutsches Meisterst\u00fcck, die Frucht von ", "tokens": ["Ein", "deut\u00b7sches", "Meis\u00b7ter\u00b7st\u00fcck", ",", "die", "Frucht", "von"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "APPR"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Z\u00fcrch der Welt geschenkt, zu sehen und zu ehren.", "tokens": ["Die", "Z\u00fcrch", "der", "Welt", "ge\u00b7schenkt", ",", "zu", "se\u00b7hen", "und", "zu", "eh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVPP", "$,", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Schon seit so langer Zeit zu sehn begierig war;", "tokens": ["Schon", "seit", "so", "lan\u00b7ger", "Zeit", "zu", "sehn", "be\u00b7gie\u00b7rig", "war", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nein, den ein ander Chor von unbeschnittnen Ohren,", "tokens": ["Nein", ",", "den", "ein", "an\u00b7der", "Chor", "von", "un\u00b7be\u00b7schnitt\u00b7nen", "Oh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PRELS", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Sich in Gedanken l\u00e4ngst zum Trost und Heil erkohren.", "tokens": ["Sich", "in", "Ge\u00b7dan\u00b7ken", "l\u00e4ngst", "zum", "Trost", "und", "Heil", "er\u00b7koh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "ADV", "APPRART", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Das aller Britten Stolz durch deutsche Kr\u00e4fte bricht;", "tokens": ["Das", "al\u00b7ler", "Brit\u00b7ten", "Stolz", "durch", "deut\u00b7sche", "Kr\u00e4f\u00b7te", "bricht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Weit mehr als St. ", "tokens": ["Weit", "mehr", "als", "St."], "token_info": ["word", "word", "word", "abbreviation"], "pos": ["ADJD", "PIAT", "KOKOM", "NE"], "meter": "+-+", "measure": "trochaic.di"}, "line.10": {"text": "Den ", "tokens": ["Den"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.11": {"text": "Wie ", "tokens": ["Wie"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}, "line.12": {"text": "Der Lehrer selbst erstaunt vor dem zu gro\u00dfen Sch\u00fcler,", "tokens": ["Der", "Leh\u00b7rer", "selbst", "er\u00b7staunt", "vor", "dem", "zu", "gro\u00b7\u00dfen", "Sch\u00fc\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "APPR", "ART", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und bethet ihn fast an. Der hei\u00dfe Wunsch so vieler,", "tokens": ["Und", "be\u00b7thet", "ihn", "fast", "an", ".", "Der", "hei\u00b7\u00dfe", "Wunsch", "so", "vie\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKVZ", "$.", "ART", "ADJA", "NN", "ADV", "PIAT", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Ein deutsches Heldenwerk von solchem Schrot zu sehn,", "tokens": ["Ein", "deut\u00b7sches", "Hel\u00b7den\u00b7werk", "von", "sol\u00b7chem", "Schrot", "zu", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Dem Himmel sey gedankt! ist nicht umsonst geschehn.", "tokens": ["Dem", "Him\u00b7mel", "sey", "ge\u00b7dankt", "!", "ist", "nicht", "um\u00b7sonst", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$.", "VAFIN", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Hier stralt ein dunkler Glanz. Hier st\u00fctzet man den Glauben", "tokens": ["Hier", "stralt", "ein", "dunk\u00b7ler", "Glanz", ".", "Hier", "st\u00fct\u00b7zet", "man", "den", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$.", "ADV", "VVFIN", "PIS", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Mit Fabeln neuer Art: wer will ihn uns nun rauben?", "tokens": ["Mit", "Fa\u00b7beln", "neu\u00b7er", "Art", ":", "wer", "will", "ihn", "uns", "nun", "rau\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$.", "PWS", "VMFIN", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Was kein Prophet gesehn und kein Evangelist,", "tokens": ["Was", "kein", "Pro\u00b7phet", "ge\u00b7sehn", "und", "kein", "E\u00b7van\u00b7ge\u00b7list", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVPP", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Was kein Apostel wu\u00dft, das lernst du hier, mein Christ!", "tokens": ["Was", "kein", "A\u00b7pos\u00b7tel", "wu\u00dft", ",", "das", "lernst", "du", "hier", ",", "mein", "Christ", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Der Schriftgelehrten Witz wird uns, mit tiefen Schl\u00fcssen,", "tokens": ["Der", "Schrift\u00b7ge\u00b7lehr\u00b7ten", "Witz", "wird", "uns", ",", "mit", "tie\u00b7fen", "Schl\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Die\u00df neue Bibelbuch hinfort erkl\u00e4ren m\u00fcssen.", "tokens": ["Die\u00df", "neu\u00b7e", "Bi\u00b7bel\u00b7buch", "hin\u00b7fort", "er\u00b7kl\u00e4\u00b7ren", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Auf nun, ", "tokens": ["Auf", "nun", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "ADV", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Und stelle Dir die\u00df Werk zum Musterbilde vor.", "tokens": ["Und", "stel\u00b7le", "Dir", "die\u00df", "Werk", "zum", "Mus\u00b7ter\u00b7bil\u00b7de", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PDS", "NN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wer ihm nicht \u00e4hnlich schreibt, kann Deutschland nicht gefallen;", "tokens": ["Wer", "ihm", "nicht", "\u00e4hn\u00b7lich", "schreibt", ",", "kann", "Deutschland", "nicht", "ge\u00b7fal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "ADJD", "VVFIN", "$,", "VMFIN", "NE", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dagegen ", "tokens": ["Da\u00b7ge\u00b7gen"], "token_info": ["word"], "pos": ["PAV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Ein wenig hat ", "tokens": ["Ein", "we\u00b7nig", "hat"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIS", "VAFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Noch mehr war ", "tokens": ["Noch", "mehr", "war"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN"], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "Der Grieche ", "tokens": ["Der", "Grie\u00b7che"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "Wies noch am leidlichsten ein recht \u00e4sthetisch Bild.", "tokens": ["Wies", "noch", "am", "leid\u00b7lichs\u00b7ten", "ein", "recht", "\u00e4s\u00b7the\u00b7tisch", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "ADJA", "ART", "ADJD", "ADJD", "NN", "$."], "meter": "+--+--+-+--+", "measure": "dactylic.di.plus"}, "line.10": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.11": {"text": "Sind der Verg\u00f6ttrung werth, und m\u00fcssen Tempel haben.", "tokens": ["Sind", "der", "Ver\u00b7g\u00f6t\u00b7trung", "werth", ",", "und", "m\u00fcs\u00b7sen", "Tem\u00b7pel", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADJD", "$,", "KON", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Wiewohl ich sehe schon: Du bleibst auf Deinem Sinn!", "tokens": ["Wie\u00b7wohl", "ich", "se\u00b7he", "schon", ":", "Du", "bleibst", "auf", "Dei\u00b7nem", "Sinn", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ADV", "$.", "PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gehst Du von dem nicht ab, dem seit dreytausend Jahren,", "tokens": ["Gehst", "Du", "von", "dem", "nicht", "ab", ",", "dem", "seit", "drey\u00b7tau\u00b7send", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PRELS", "PTKNEG", "PTKVZ", "$,", "PRELS", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die gr\u00f6\u00dften Geister auch zu folgen eifrig waren;", "tokens": ["Die", "gr\u00f6\u00df\u00b7ten", "Geis\u00b7ter", "auch", "zu", "fol\u00b7gen", "eif\u00b7rig", "wa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "PTKZU", "VVINF", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dem ", "tokens": ["Dem"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Gut! folge Deinem Kopf. Du liebst ein deutlich Wesen?", "tokens": ["Gut", "!", "fol\u00b7ge", "Dei\u00b7nem", "Kopf", ".", "Du", "liebst", "ein", "deut\u00b7lich", "We\u00b7sen", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$.", "VVFIN", "PPOSAT", "NN", "$.", "PPER", "VVFIN", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Vernimm das Donnerwort: ", "tokens": ["Ver\u00b7nimm", "das", "Don\u00b7ner\u00b7wort", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVIMP", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}