{"textgrid.poem.57055": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Der Wasseresel", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Wasseresel taucht empor", "tokens": ["Der", "Was\u00b7se\u00b7re\u00b7sel", "taucht", "em\u00b7por"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und legt sich r\u00fccklings auf das Moor.", "tokens": ["und", "legt", "sich", "r\u00fcck\u00b7lings", "auf", "das", "Moor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Und ordnet k\u00fcnstlich sein Gebein,", "tokens": ["Und", "ord\u00b7net", "k\u00fcnst\u00b7lich", "sein", "Ge\u00b7bein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "im Hinblick auf den Mondenschein:", "tokens": ["im", "Hin\u00b7blick", "auf", "den", "Mon\u00b7den\u00b7schein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "So da\u00df der Mond ein Ornament", "tokens": ["So", "da\u00df", "der", "Mond", "ein", "Or\u00b7na\u00b7ment"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "auf seines Bauches W\u00f6lbung brennt ...", "tokens": ["auf", "sei\u00b7nes", "Bau\u00b7ches", "W\u00f6l\u00b7bung", "brennt", "..."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Mit diesem Ornamente naht", "tokens": ["Mit", "die\u00b7sem", "Or\u00b7na\u00b7men\u00b7te", "naht"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "er sich der Fingur Wasserstaat.", "tokens": ["er", "sich", "der", "Fin\u00b7gur", "Was\u00b7ser\u00b7staat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PRF", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Und wird von dieser, rings beneidet,", "tokens": ["Und", "wird", "von", "die\u00b7ser", ",", "rings", "be\u00b7nei\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PDAT", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "mit einem Doktorhut bekleidet.", "tokens": ["mit", "ei\u00b7nem", "Dok\u00b7tor\u00b7hut", "be\u00b7klei\u00b7det", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Als Lehrer list er nun am Pult,", "tokens": ["Als", "Leh\u00b7rer", "list", "er", "nun", "am", "Pult", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wie man durch Geist, Licht und Geduld,", "tokens": ["wie", "man", "durch", "Geist", ",", "Licht", "und", "Ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.7": {"line.1": {"text": "versch\u00f6nern k\u00f6nne, was sonst nicht", "tokens": ["ver\u00b7sch\u00f6\u00b7nern", "k\u00f6n\u00b7ne", ",", "was", "sonst", "nicht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVINF", "VMFIN", "$,", "PRELS", "ADV", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "in allem dem Geschmack entspricht.", "tokens": ["in", "al\u00b7lem", "dem", "Ge\u00b7schmack", "ent\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Er stellt zuletzt mit viel Humor", "tokens": ["Er", "stellt", "zu\u00b7letzt", "mit", "viel", "Hu\u00b7mor"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "sich selbst als lehrreich Beispiel vor.", "tokens": ["sich", "selbst", "als", "lehr\u00b7reich", "Bei\u00b7spiel", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "KOUS", "ADJD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "\u00bbeinst war ich meiner Dummheit Beute\u00ab,", "tokens": ["\u00bb", "einst", "war", "ich", "mei\u00b7ner", "Dumm\u00b7heit", "Beu\u00b7te", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "PPOSAT", "NN", "NN", "$(", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "so spricht er \u2013 \u00bbund was bin ich heute?", "tokens": ["so", "spricht", "er", "\u2013", "\u00bb", "und", "was", "bin", "ich", "heu\u00b7te", "?"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$(", "$(", "KON", "PWS", "VAFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Ein Kunstwerk der Kulturbegierde,", "tokens": ["Ein", "Kunst\u00b7werk", "der", "Kul\u00b7tur\u00b7be\u00b7gier\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "des Waldes Stolz, des Weihers Zierde!", "tokens": ["des", "Wal\u00b7des", "Stolz", ",", "des", "Wei\u00b7hers", "Zier\u00b7de", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Seht her, ich bing euch in Person", "tokens": ["Seht", "her", ",", "ich", "bing", "euch", "in", "Per\u00b7son"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "das Kunsthandwerk als Religion.\u00ab", "tokens": ["das", "Kunst\u00b7hand\u00b7werk", "als", "Re\u00b7li\u00b7gi\u00b7on", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Der Wasseresel taucht empor", "tokens": ["Der", "Was\u00b7se\u00b7re\u00b7sel", "taucht", "em\u00b7por"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und legt sich r\u00fccklings auf das Moor.", "tokens": ["und", "legt", "sich", "r\u00fcck\u00b7lings", "auf", "das", "Moor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und ordnet k\u00fcnstlich sein Gebein,", "tokens": ["Und", "ord\u00b7net", "k\u00fcnst\u00b7lich", "sein", "Ge\u00b7bein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "im Hinblick auf den Mondenschein:", "tokens": ["im", "Hin\u00b7blick", "auf", "den", "Mon\u00b7den\u00b7schein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "So da\u00df der Mond ein Ornament", "tokens": ["So", "da\u00df", "der", "Mond", "ein", "Or\u00b7na\u00b7ment"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "auf seines Bauches W\u00f6lbung brennt ...", "tokens": ["auf", "sei\u00b7nes", "Bau\u00b7ches", "W\u00f6l\u00b7bung", "brennt", "..."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Mit diesem Ornamente naht", "tokens": ["Mit", "die\u00b7sem", "Or\u00b7na\u00b7men\u00b7te", "naht"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "er sich der Fingur Wasserstaat.", "tokens": ["er", "sich", "der", "Fin\u00b7gur", "Was\u00b7ser\u00b7staat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PRF", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Und wird von dieser, rings beneidet,", "tokens": ["Und", "wird", "von", "die\u00b7ser", ",", "rings", "be\u00b7nei\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PDAT", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "mit einem Doktorhut bekleidet.", "tokens": ["mit", "ei\u00b7nem", "Dok\u00b7tor\u00b7hut", "be\u00b7klei\u00b7det", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Als Lehrer list er nun am Pult,", "tokens": ["Als", "Leh\u00b7rer", "list", "er", "nun", "am", "Pult", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wie man durch Geist, Licht und Geduld,", "tokens": ["wie", "man", "durch", "Geist", ",", "Licht", "und", "Ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}}, "stanza.18": {"line.1": {"text": "versch\u00f6nern k\u00f6nne, was sonst nicht", "tokens": ["ver\u00b7sch\u00f6\u00b7nern", "k\u00f6n\u00b7ne", ",", "was", "sonst", "nicht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVINF", "VMFIN", "$,", "PRELS", "ADV", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "in allem dem Geschmack entspricht.", "tokens": ["in", "al\u00b7lem", "dem", "Ge\u00b7schmack", "ent\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Er stellt zuletzt mit viel Humor", "tokens": ["Er", "stellt", "zu\u00b7letzt", "mit", "viel", "Hu\u00b7mor"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "sich selbst als lehrreich Beispiel vor.", "tokens": ["sich", "selbst", "als", "lehr\u00b7reich", "Bei\u00b7spiel", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "KOUS", "ADJD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "\u00bbeinst war ich meiner Dummheit Beute\u00ab,", "tokens": ["\u00bb", "einst", "war", "ich", "mei\u00b7ner", "Dumm\u00b7heit", "Beu\u00b7te", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "PPOSAT", "NN", "NN", "$(", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "so spricht er \u2013 \u00bbund was bin ich heute?", "tokens": ["so", "spricht", "er", "\u2013", "\u00bb", "und", "was", "bin", "ich", "heu\u00b7te", "?"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$(", "$(", "KON", "PWS", "VAFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Ein Kunstwerk der Kulturbegierde,", "tokens": ["Ein", "Kunst\u00b7werk", "der", "Kul\u00b7tur\u00b7be\u00b7gier\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "des Waldes Stolz, des Weihers Zierde!", "tokens": ["des", "Wal\u00b7des", "Stolz", ",", "des", "Wei\u00b7hers", "Zier\u00b7de", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Seht her, ich bing euch in Person", "tokens": ["Seht", "her", ",", "ich", "bing", "euch", "in", "Per\u00b7son"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKVZ", "$,", "PPER", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "das Kunsthandwerk als Religion.\u00ab", "tokens": ["das", "Kunst\u00b7hand\u00b7werk", "als", "Re\u00b7li\u00b7gi\u00b7on", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "KOUS", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}