{"textgrid.poem.31762": {"metadata": {"author": {"name": "Weerth, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Heute morgen fuhr ich nach D\u00fcsseldorf", "genre": "verse", "period": "N.A.", "pub_year": 1839, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Heute morgen fuhr ich nach D\u00fcsseldorf", "tokens": ["Heu\u00b7te", "mor\u00b7gen", "fuhr", "ich", "nach", "D\u00fcs\u00b7sel\u00b7dorf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "APPR", "NE"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In sehr honetter Begleitung:", "tokens": ["In", "sehr", "ho\u00b7net\u00b7ter", "Be\u00b7glei\u00b7tung", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ein Regierungsrat \u2013 er schimpfte sehr", "tokens": ["Ein", "Re\u00b7gie\u00b7rungs\u00b7rat", "\u2013", "er", "schimpf\u00b7te", "sehr"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$(", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Auf die Neue Rheinische Zeitung.", "tokens": ["Auf", "die", "Neu\u00b7e", "Rhei\u00b7ni\u00b7sche", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "\u00bbdie Redakteure dieses Blatts\u00ab,", "tokens": ["\u00bb", "die", "Re\u00b7dak\u00b7teu\u00b7re", "die\u00b7ses", "Blatts", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "PDAT", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So sprach er, \u00bbsind s\u00e4mtlich Teufel;", "tokens": ["So", "sprach", "er", ",", "\u00bb", "sind", "s\u00e4mt\u00b7lich", "Teu\u00b7fel", ";"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "$(", "VAFIN", "ADJD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie f\u00fcrchten weder den lieben Gott", "tokens": ["Sie", "f\u00fcrch\u00b7ten", "we\u00b7der", "den", "lie\u00b7ben", "Gott"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KON", "ART", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Noch den Ober-Prokurator Zweiffel.", "tokens": ["Noch", "den", "O\u00b7ber\u00b7Pro\u00b7ku\u00b7ra\u00b7tor", "Zweif\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NN", "$."], "meter": "+-+--++-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "F\u00fcr alles irdische Mi\u00dfgeschick", "tokens": ["F\u00fcr", "al\u00b7les", "ir\u00b7di\u00b7sche", "Mi\u00df\u00b7ge\u00b7schick"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sehn sie die einzige Heilung", "tokens": ["Sehn", "sie", "die", "ein\u00b7zi\u00b7ge", "Hei\u00b7lung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In der rosenr\u00f6tlichen Republik", "tokens": ["In", "der", "ro\u00b7sen\u00b7r\u00f6t\u00b7li\u00b7chen", "Re\u00b7pub\u00b7lik"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und vollkommener G\u00fcterteilung.", "tokens": ["Und", "voll\u00b7kom\u00b7me\u00b7ner", "G\u00fc\u00b7ter\u00b7tei\u00b7lung", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Die ganze Welt wird eingeteilt", "tokens": ["Die", "gan\u00b7ze", "Welt", "wird", "ein\u00b7ge\u00b7teilt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In tausend Millionen Parzellen;", "tokens": ["In", "tau\u00b7send", "Mil\u00b7lion\u00b7en", "Par\u00b7zel\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "In so viel Land, in so viel Sand", "tokens": ["In", "so", "viel", "Land", ",", "in", "so", "viel", "Sand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "PIAT", "NN", "$,", "APPR", "ADV", "PIAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Und in so viel Meereswellen.", "tokens": ["Und", "in", "so", "viel", "Mee\u00b7res\u00b7wel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "PIAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Und alle Menschen bekommen ein St\u00fcck", "tokens": ["Und", "al\u00b7le", "Men\u00b7schen", "be\u00b7kom\u00b7men", "ein", "St\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu ihrer speziellen Erheitrung \u2013", "tokens": ["Zu", "ih\u00b7rer", "spe\u00b7zi\u00b7el\u00b7len", "Er\u00b7heit\u00b7rung", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Die besten Brocken: die Redakteur'", "tokens": ["Die", "bes\u00b7ten", "Bro\u00b7cken", ":", "die", "Re\u00b7dak\u00b7teur'"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Neuen Rheinischen Zeitung.", "tokens": ["Der", "Neu\u00b7en", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Auch nach Weibergemeinschaft steht ihr Sinn.", "tokens": ["Auch", "nach", "Wei\u00b7ber\u00b7ge\u00b7mein\u00b7schaft", "steht", "ihr", "Sinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Abschaffen wolln sie die Ehe:", "tokens": ["Ab\u00b7schaf\u00b7fen", "wolln", "sie", "die", "E\u00b7he", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df alles in Zukunft ad libitum", "tokens": ["Da\u00df", "al\u00b7les", "in", "Zu\u00b7kunft", "ad", "li\u00b7bi\u00b7tum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "NN", "FM", "FM"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Miteinander nach Bette gehe:", "tokens": ["Mi\u00b7tein\u00b7an\u00b7der", "nach", "Bet\u00b7te", "ge\u00b7he", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Tartar und Mongole mit Griechenfraun,", "tokens": ["Tar\u00b7tar", "und", "Mon\u00b7go\u00b7le", "mit", "Grie\u00b7chen\u00b7fraun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Cherusker mit gelben Chinesen,", "tokens": ["Che\u00b7rus\u00b7ker", "mit", "gel\u00b7ben", "Chi\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Eisb\u00e4ren mit schwedischen Nachtigalln,", "tokens": ["Eis\u00b7b\u00e4\u00b7ren", "mit", "schwe\u00b7di\u00b7schen", "Nach\u00b7ti\u00b7galln", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "++--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "T\u00fcrkinnen mit Irokesen.", "tokens": ["T\u00fcr\u00b7kin\u00b7nen", "mit", "I\u00b7ro\u00b7ke\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Tranduftende Samojedinnen solln", "tokens": ["Tran\u00b7duf\u00b7ten\u00b7de", "Sa\u00b7mo\u00b7je\u00b7din\u00b7nen", "solln"], "token_info": ["word", "word", "word"], "pos": ["ADJA", "NN", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Zu Briten und R\u00f6mern sich betten,", "tokens": ["Zu", "Bri\u00b7ten", "und", "R\u00f6\u00b7mern", "sich", "bet\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PRF", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Plattnasige d\u00fcstre Kaffern zu", "tokens": ["Platt\u00b7na\u00b7si\u00b7ge", "d\u00fcst\u00b7re", "Kaf\u00b7fern", "zu"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "NN", "PTKZU"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Alabasterwei\u00dfen Grisetten.", "tokens": ["A\u00b7la\u00b7bas\u00b7ter\u00b7wei\u00b7\u00dfen", "Gri\u00b7set\u00b7ten", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.9": {"line.1": {"text": "Ja, \u00e4ndern wird sich die ganze Welt", "tokens": ["Ja", ",", "\u00e4n\u00b7dern", "wird", "sich", "die", "gan\u00b7ze", "Welt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "VVINF", "VAFIN", "PRF", "ART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch, diese moderne Leitung \u2013", "tokens": ["Durch", ",", "die\u00b7se", "mo\u00b7der\u00b7ne", "Lei\u00b7tung", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "PDAT", "ADJA", "NN", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Doch die sch\u00f6nsten Weiber bekommen die", "tokens": ["Doch", "die", "sch\u00f6ns\u00b7ten", "Wei\u00b7ber", "be\u00b7kom\u00b7men", "die"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN", "ART"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Redakteure der Rheinischen Zeitung!", "tokens": ["Re\u00b7dak\u00b7teu\u00b7re", "der", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "Aufl\u00f6sen wollen sie alles schier;", "tokens": ["Auf\u00b7l\u00f6\u00b7sen", "wol\u00b7len", "sie", "al\u00b7les", "schier", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PIS", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Oh, L\u00e4strer sind sie und Sp\u00f6tter;", "tokens": ["Oh", ",", "L\u00e4st\u00b7rer", "sind", "sie", "und", "Sp\u00f6t\u00b7ter", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "VAFIN", "PPER", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Kein Mensch soll in Zukunft besitzen mehr", "tokens": ["Kein", "Mensch", "soll", "in", "Zu\u00b7kunft", "be\u00b7sit\u00b7zen", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VMFIN", "APPR", "NN", "VVFIN", "ADV"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Privateigent\u00fcmliche G\u00f6tter.", "tokens": ["Pri\u00b7va\u00b7tei\u00b7gen\u00b7t\u00fcm\u00b7li\u00b7che", "G\u00f6t\u00b7ter", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Die Religion wird abgeschafft,", "tokens": ["Die", "Re\u00b7li\u00b7gi\u00b7on", "wird", "ab\u00b7ge\u00b7schafft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nicht glauben mehr soll man an Rhenus,", "tokens": ["Nicht", "glau\u00b7ben", "mehr", "soll", "man", "an", "Rhe\u00b7nus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "ADV", "VMFIN", "PIS", "APPR", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "An den nu\u00dflaub- und rebenbekr\u00e4nzten, und nicht", "tokens": ["An", "den", "nu\u00df\u00b7laub", "und", "re\u00b7ben\u00b7be\u00b7kr\u00e4nz\u00b7ten", ",", "und", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "ART", "TRUNC", "KON", "VVFIN", "$,", "KON", "PTKNEG"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "An die Mediceische Venus.", "tokens": ["An", "die", "Me\u00b7di\u00b7ce\u00b7i\u00b7sche", "Ve\u00b7nus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.12": {"line.1": {"text": "Nicht glauben an Kastor und Pollux \u2013 nicht", "tokens": ["Nicht", "glau\u00b7ben", "an", "Kas\u00b7tor", "und", "Pol\u00b7lux", "\u2013", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKNEG", "VVFIN", "APPR", "NN", "KON", "NN", "$(", "PTKNEG"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "An Juno und Zeus Kronion,", "tokens": ["An", "Ju\u00b7no", "und", "Zeus", "Kro\u00b7ni\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "NE", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "An Isis nicht und Osiris nicht", "tokens": ["An", "I\u00b7sis", "nicht", "und", "O\u00b7si\u00b7ris", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "PTKNEG", "KON", "NE", "PTKNEG"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und an deine Mauern, o Zion!", "tokens": ["Und", "an", "dei\u00b7ne", "Mau\u00b7ern", ",", "o", "Zi\u00b7on", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "$,", "FM", "NE", "$."], "meter": "--+-+--++", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "Ja, weder an Odin glauben noch Thor,", "tokens": ["Ja", ",", "we\u00b7der", "an", "O\u00b7din", "glau\u00b7ben", "noch", "Thor", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KON", "APPR", "NE", "VVFIN", "ADV", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "An Allah nicht und an Brahma \u2013", "tokens": ["An", "Al\u00b7lah", "nicht", "und", "an", "Brah\u00b7ma", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKNEG", "KON", "APPR", "NE", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Neue Rheinische Zeitung bleibt", "tokens": ["Die", "Neu\u00b7e", "Rhei\u00b7ni\u00b7sche", "Zei\u00b7tung", "bleibt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN", "VVFIN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der einzige Dalai-Lama.\u00ab", "tokens": ["Der", "ein\u00b7zi\u00b7ge", "Da\u00b7lai\u00b7La\u00b7ma", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "$("], "meter": "-+---+-+", "measure": "dactylic.init"}}, "stanza.14": {"line.1": {"text": "Da schwieg der Herr Regierungsrat,", "tokens": ["Da", "schwieg", "der", "Herr", "Re\u00b7gie\u00b7rungs\u00b7rat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nicht wenig war ich verwundert:", "tokens": ["Und", "nicht", "we\u00b7nig", "war", "ich", "ver\u00b7wun\u00b7dert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "VAFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Sie scheinen ein sehr gescheiter Mann", "tokens": ["Sie", "schei\u00b7nen", "ein", "sehr", "ge\u00b7schei\u00b7ter", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADV", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "F\u00fcr unser verr\u00fcckt Jahrhundert!", "tokens": ["F\u00fcr", "un\u00b7ser", "ver\u00b7r\u00fcckt", "Jahr\u00b7hun\u00b7dert", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Ich bin entz\u00fcckt, mein werter Herr,", "tokens": ["Ich", "bin", "ent\u00b7z\u00fcckt", ",", "mein", "wer\u00b7ter", "Herr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von Ihrer honetten Begleitung \u2013", "tokens": ["Von", "Ih\u00b7rer", "ho\u00b7net\u00b7ten", "Be\u00b7glei\u00b7tung", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Ich selber bin ein Redakteur", "tokens": ["Ich", "sel\u00b7ber", "bin", "ein", "Re\u00b7dak\u00b7teur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von der Neuen Rheinischen Zeitung.", "tokens": ["Von", "der", "Neu\u00b7en", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.16": {"line.1": {"text": "Oh, fahren Sie fort, so unsern Ruhm", "tokens": ["Oh", ",", "fah\u00b7ren", "Sie", "fort", ",", "so", "un\u00b7sern", "Ruhm"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "ADV", "PPOSAT", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu tragen durch alle Lande \u2013", "tokens": ["Zu", "tra\u00b7gen", "durch", "al\u00b7le", "Lan\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "PIAT", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie sind als Mensch und Regierungsrat", "tokens": ["Sie", "sind", "als", "Mensch", "und", "Re\u00b7gie\u00b7rungs\u00b7rat"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "KOKOM", "NN", "KON", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von unbeschr\u00e4nktem Verstande.", "tokens": ["Von", "un\u00b7be\u00b7schr\u00e4nk\u00b7tem", "Ver\u00b7stan\u00b7de", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.17": {"line.1": {"text": "Oh, fahr er fort, mein guter Mann \u2013", "tokens": ["Oh", ",", "fahr", "er", "fort", ",", "mein", "gu\u00b7ter", "Mann", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich will ihm ein Denkmal setzen", "tokens": ["Ich", "will", "ihm", "ein", "Denk\u00b7mal", "set\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "In unserm heitern Feuilleton \u2013", "tokens": ["In", "un\u00b7serm", "hei\u00b7tern", "Feu\u00b7il\u00b7le\u00b7ton", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Sie wissen die Ehre zu sch\u00e4tzen.", "tokens": ["Sie", "wis\u00b7sen", "die", "Eh\u00b7re", "zu", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.18": {"line.1": {"text": "Ja, wahrlich, nicht jeder Gimpel bekommt", "tokens": ["Ja", ",", "wahr\u00b7lich", ",", "nicht", "je\u00b7der", "Gim\u00b7pel", "be\u00b7kommt"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "ADV", "$,", "PTKNEG", "PIAT", "NN", "VVFIN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Einen Tritt von unsern F\u00fc\u00dfen \u2013", "tokens": ["Ei\u00b7nen", "Tritt", "von", "un\u00b7sern", "F\u00fc\u00b7\u00dfen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich habe, mein lieber Regierungsrat,", "tokens": ["Ich", "ha\u00b7be", ",", "mein", "lie\u00b7ber", "Re\u00b7gie\u00b7rungs\u00b7rat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Die Ehre, Sie h\u00f6flich zu gr\u00fc\u00dfen.", "tokens": ["Die", "Eh\u00b7re", ",", "Sie", "h\u00f6f\u00b7lich", "zu", "gr\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.19": {"line.1": {"text": "Heute morgen fuhr ich nach D\u00fcsseldorf", "tokens": ["Heu\u00b7te", "mor\u00b7gen", "fuhr", "ich", "nach", "D\u00fcs\u00b7sel\u00b7dorf"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "APPR", "NE"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "In sehr honetter Begleitung:", "tokens": ["In", "sehr", "ho\u00b7net\u00b7ter", "Be\u00b7glei\u00b7tung", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ein Regierungsrat \u2013 er schimpfte sehr", "tokens": ["Ein", "Re\u00b7gie\u00b7rungs\u00b7rat", "\u2013", "er", "schimpf\u00b7te", "sehr"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$(", "PPER", "VVFIN", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Auf die Neue Rheinische Zeitung.", "tokens": ["Auf", "die", "Neu\u00b7e", "Rhei\u00b7ni\u00b7sche", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.20": {"line.1": {"text": "\u00bbdie Redakteure dieses Blatts\u00ab,", "tokens": ["\u00bb", "die", "Re\u00b7dak\u00b7teu\u00b7re", "die\u00b7ses", "Blatts", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "PDAT", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So sprach er, \u00bbsind s\u00e4mtlich Teufel;", "tokens": ["So", "sprach", "er", ",", "\u00bb", "sind", "s\u00e4mt\u00b7lich", "Teu\u00b7fel", ";"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "$(", "VAFIN", "ADJD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie f\u00fcrchten weder den lieben Gott", "tokens": ["Sie", "f\u00fcrch\u00b7ten", "we\u00b7der", "den", "lie\u00b7ben", "Gott"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "KON", "ART", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Noch den Ober-Prokurator Zweiffel.", "tokens": ["Noch", "den", "O\u00b7ber\u00b7Pro\u00b7ku\u00b7ra\u00b7tor", "Zweif\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NN", "$."], "meter": "+-+--++-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.21": {"line.1": {"text": "F\u00fcr alles irdische Mi\u00dfgeschick", "tokens": ["F\u00fcr", "al\u00b7les", "ir\u00b7di\u00b7sche", "Mi\u00df\u00b7ge\u00b7schick"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sehn sie die einzige Heilung", "tokens": ["Sehn", "sie", "die", "ein\u00b7zi\u00b7ge", "Hei\u00b7lung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In der rosenr\u00f6tlichen Republik", "tokens": ["In", "der", "ro\u00b7sen\u00b7r\u00f6t\u00b7li\u00b7chen", "Re\u00b7pub\u00b7lik"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und vollkommener G\u00fcterteilung.", "tokens": ["Und", "voll\u00b7kom\u00b7me\u00b7ner", "G\u00fc\u00b7ter\u00b7tei\u00b7lung", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Die ganze Welt wird eingeteilt", "tokens": ["Die", "gan\u00b7ze", "Welt", "wird", "ein\u00b7ge\u00b7teilt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In tausend Millionen Parzellen;", "tokens": ["In", "tau\u00b7send", "Mil\u00b7lion\u00b7en", "Par\u00b7zel\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "In so viel Land, in so viel Sand", "tokens": ["In", "so", "viel", "Land", ",", "in", "so", "viel", "Sand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "PIAT", "NN", "$,", "APPR", "ADV", "PIAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Und in so viel Meereswellen.", "tokens": ["Und", "in", "so", "viel", "Mee\u00b7res\u00b7wel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "PIAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.23": {"line.1": {"text": "Und alle Menschen bekommen ein St\u00fcck", "tokens": ["Und", "al\u00b7le", "Men\u00b7schen", "be\u00b7kom\u00b7men", "ein", "St\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "VVFIN", "ART", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu ihrer speziellen Erheitrung \u2013", "tokens": ["Zu", "ih\u00b7rer", "spe\u00b7zi\u00b7el\u00b7len", "Er\u00b7heit\u00b7rung", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Die besten Brocken: die Redakteur'", "tokens": ["Die", "bes\u00b7ten", "Bro\u00b7cken", ":", "die", "Re\u00b7dak\u00b7teur'"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Neuen Rheinischen Zeitung.", "tokens": ["Der", "Neu\u00b7en", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "Auch nach Weibergemeinschaft steht ihr Sinn.", "tokens": ["Auch", "nach", "Wei\u00b7ber\u00b7ge\u00b7mein\u00b7schaft", "steht", "ihr", "Sinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Abschaffen wolln sie die Ehe:", "tokens": ["Ab\u00b7schaf\u00b7fen", "wolln", "sie", "die", "E\u00b7he", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df alles in Zukunft ad libitum", "tokens": ["Da\u00df", "al\u00b7les", "in", "Zu\u00b7kunft", "ad", "li\u00b7bi\u00b7tum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "NN", "FM", "FM"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Miteinander nach Bette gehe:", "tokens": ["Mi\u00b7tein\u00b7an\u00b7der", "nach", "Bet\u00b7te", "ge\u00b7he", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.25": {"line.1": {"text": "Tartar und Mongole mit Griechenfraun,", "tokens": ["Tar\u00b7tar", "und", "Mon\u00b7go\u00b7le", "mit", "Grie\u00b7chen\u00b7fraun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "APPR", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Cherusker mit gelben Chinesen,", "tokens": ["Che\u00b7rus\u00b7ker", "mit", "gel\u00b7ben", "Chi\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Eisb\u00e4ren mit schwedischen Nachtigalln,", "tokens": ["Eis\u00b7b\u00e4\u00b7ren", "mit", "schwe\u00b7di\u00b7schen", "Nach\u00b7ti\u00b7galln", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "++--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "T\u00fcrkinnen mit Irokesen.", "tokens": ["T\u00fcr\u00b7kin\u00b7nen", "mit", "I\u00b7ro\u00b7ke\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.26": {"line.1": {"text": "Tranduftende Samojedinnen solln", "tokens": ["Tran\u00b7duf\u00b7ten\u00b7de", "Sa\u00b7mo\u00b7je\u00b7din\u00b7nen", "solln"], "token_info": ["word", "word", "word"], "pos": ["ADJA", "NN", "VMFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Zu Briten und R\u00f6mern sich betten,", "tokens": ["Zu", "Bri\u00b7ten", "und", "R\u00f6\u00b7mern", "sich", "bet\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PRF", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Plattnasige d\u00fcstre Kaffern zu", "tokens": ["Platt\u00b7na\u00b7si\u00b7ge", "d\u00fcst\u00b7re", "Kaf\u00b7fern", "zu"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "NN", "PTKZU"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Alabasterwei\u00dfen Grisetten.", "tokens": ["A\u00b7la\u00b7bas\u00b7ter\u00b7wei\u00b7\u00dfen", "Gri\u00b7set\u00b7ten", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.27": {"line.1": {"text": "Ja, \u00e4ndern wird sich die ganze Welt", "tokens": ["Ja", ",", "\u00e4n\u00b7dern", "wird", "sich", "die", "gan\u00b7ze", "Welt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "VVINF", "VAFIN", "PRF", "ART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch, diese moderne Leitung \u2013", "tokens": ["Durch", ",", "die\u00b7se", "mo\u00b7der\u00b7ne", "Lei\u00b7tung", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "PDAT", "ADJA", "NN", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Doch die sch\u00f6nsten Weiber bekommen die", "tokens": ["Doch", "die", "sch\u00f6ns\u00b7ten", "Wei\u00b7ber", "be\u00b7kom\u00b7men", "die"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN", "ART"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Redakteure der Rheinischen Zeitung!", "tokens": ["Re\u00b7dak\u00b7teu\u00b7re", "der", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.28": {"line.1": {"text": "Aufl\u00f6sen wollen sie alles schier;", "tokens": ["Auf\u00b7l\u00f6\u00b7sen", "wol\u00b7len", "sie", "al\u00b7les", "schier", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PIS", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Oh, L\u00e4strer sind sie und Sp\u00f6tter;", "tokens": ["Oh", ",", "L\u00e4st\u00b7rer", "sind", "sie", "und", "Sp\u00f6t\u00b7ter", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "VAFIN", "PPER", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Kein Mensch soll in Zukunft besitzen mehr", "tokens": ["Kein", "Mensch", "soll", "in", "Zu\u00b7kunft", "be\u00b7sit\u00b7zen", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VMFIN", "APPR", "NN", "VVFIN", "ADV"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Privateigent\u00fcmliche G\u00f6tter.", "tokens": ["Pri\u00b7va\u00b7tei\u00b7gen\u00b7t\u00fcm\u00b7li\u00b7che", "G\u00f6t\u00b7ter", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.29": {"line.1": {"text": "Die Religion wird abgeschafft,", "tokens": ["Die", "Re\u00b7li\u00b7gi\u00b7on", "wird", "ab\u00b7ge\u00b7schafft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nicht glauben mehr soll man an Rhenus,", "tokens": ["Nicht", "glau\u00b7ben", "mehr", "soll", "man", "an", "Rhe\u00b7nus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "ADV", "VMFIN", "PIS", "APPR", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "An den nu\u00dflaub- und rebenbekr\u00e4nzten, und nicht", "tokens": ["An", "den", "nu\u00df\u00b7laub", "und", "re\u00b7ben\u00b7be\u00b7kr\u00e4nz\u00b7ten", ",", "und", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "ART", "TRUNC", "KON", "VVFIN", "$,", "KON", "PTKNEG"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "An die Mediceische Venus.", "tokens": ["An", "die", "Me\u00b7di\u00b7ce\u00b7i\u00b7sche", "Ve\u00b7nus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.30": {"line.1": {"text": "Nicht glauben an Kastor und Pollux \u2013 nicht", "tokens": ["Nicht", "glau\u00b7ben", "an", "Kas\u00b7tor", "und", "Pol\u00b7lux", "\u2013", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKNEG", "VVFIN", "APPR", "NN", "KON", "NN", "$(", "PTKNEG"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "An Juno und Zeus Kronion,", "tokens": ["An", "Ju\u00b7no", "und", "Zeus", "Kro\u00b7ni\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "NE", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "An Isis nicht und Osiris nicht", "tokens": ["An", "I\u00b7sis", "nicht", "und", "O\u00b7si\u00b7ris", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "PTKNEG", "KON", "NE", "PTKNEG"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und an deine Mauern, o Zion!", "tokens": ["Und", "an", "dei\u00b7ne", "Mau\u00b7ern", ",", "o", "Zi\u00b7on", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "$,", "FM", "NE", "$."], "meter": "--+-+--++", "measure": "iambic.tetra.relaxed"}}, "stanza.31": {"line.1": {"text": "Ja, weder an Odin glauben noch Thor,", "tokens": ["Ja", ",", "we\u00b7der", "an", "O\u00b7din", "glau\u00b7ben", "noch", "Thor", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KON", "APPR", "NE", "VVFIN", "ADV", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "An Allah nicht und an Brahma \u2013", "tokens": ["An", "Al\u00b7lah", "nicht", "und", "an", "Brah\u00b7ma", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKNEG", "KON", "APPR", "NE", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Neue Rheinische Zeitung bleibt", "tokens": ["Die", "Neu\u00b7e", "Rhei\u00b7ni\u00b7sche", "Zei\u00b7tung", "bleibt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN", "VVFIN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der einzige Dalai-Lama.\u00ab", "tokens": ["Der", "ein\u00b7zi\u00b7ge", "Da\u00b7lai\u00b7La\u00b7ma", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "$("], "meter": "-+---+-+", "measure": "dactylic.init"}}, "stanza.32": {"line.1": {"text": "Da schwieg der Herr Regierungsrat,", "tokens": ["Da", "schwieg", "der", "Herr", "Re\u00b7gie\u00b7rungs\u00b7rat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nicht wenig war ich verwundert:", "tokens": ["Und", "nicht", "we\u00b7nig", "war", "ich", "ver\u00b7wun\u00b7dert", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "VAFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Sie scheinen ein sehr gescheiter Mann", "tokens": ["Sie", "schei\u00b7nen", "ein", "sehr", "ge\u00b7schei\u00b7ter", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADV", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "F\u00fcr unser verr\u00fcckt Jahrhundert!", "tokens": ["F\u00fcr", "un\u00b7ser", "ver\u00b7r\u00fcckt", "Jahr\u00b7hun\u00b7dert", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.33": {"line.1": {"text": "Ich bin entz\u00fcckt, mein werter Herr,", "tokens": ["Ich", "bin", "ent\u00b7z\u00fcckt", ",", "mein", "wer\u00b7ter", "Herr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von Ihrer honetten Begleitung \u2013", "tokens": ["Von", "Ih\u00b7rer", "ho\u00b7net\u00b7ten", "Be\u00b7glei\u00b7tung", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Ich selber bin ein Redakteur", "tokens": ["Ich", "sel\u00b7ber", "bin", "ein", "Re\u00b7dak\u00b7teur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von der Neuen Rheinischen Zeitung.", "tokens": ["Von", "der", "Neu\u00b7en", "Rhei\u00b7ni\u00b7schen", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.34": {"line.1": {"text": "Oh, fahren Sie fort, so unsern Ruhm", "tokens": ["Oh", ",", "fah\u00b7ren", "Sie", "fort", ",", "so", "un\u00b7sern", "Ruhm"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "ADV", "PPOSAT", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu tragen durch alle Lande \u2013", "tokens": ["Zu", "tra\u00b7gen", "durch", "al\u00b7le", "Lan\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "PIAT", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie sind als Mensch und Regierungsrat", "tokens": ["Sie", "sind", "als", "Mensch", "und", "Re\u00b7gie\u00b7rungs\u00b7rat"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "KOKOM", "NN", "KON", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von unbeschr\u00e4nktem Verstande.", "tokens": ["Von", "un\u00b7be\u00b7schr\u00e4nk\u00b7tem", "Ver\u00b7stan\u00b7de", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.35": {"line.1": {"text": "Oh, fahr er fort, mein guter Mann \u2013", "tokens": ["Oh", ",", "fahr", "er", "fort", ",", "mein", "gu\u00b7ter", "Mann", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "PTKVZ", "$,", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich will ihm ein Denkmal setzen", "tokens": ["Ich", "will", "ihm", "ein", "Denk\u00b7mal", "set\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "In unserm heitern Feuilleton \u2013", "tokens": ["In", "un\u00b7serm", "hei\u00b7tern", "Feu\u00b7il\u00b7le\u00b7ton", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Sie wissen die Ehre zu sch\u00e4tzen.", "tokens": ["Sie", "wis\u00b7sen", "die", "Eh\u00b7re", "zu", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.36": {"line.1": {"text": "Ja, wahrlich, nicht jeder Gimpel bekommt", "tokens": ["Ja", ",", "wahr\u00b7lich", ",", "nicht", "je\u00b7der", "Gim\u00b7pel", "be\u00b7kommt"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "ADV", "$,", "PTKNEG", "PIAT", "NN", "VVFIN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Einen Tritt von unsern F\u00fc\u00dfen \u2013", "tokens": ["Ei\u00b7nen", "Tritt", "von", "un\u00b7sern", "F\u00fc\u00b7\u00dfen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich habe, mein lieber Regierungsrat,", "tokens": ["Ich", "ha\u00b7be", ",", "mein", "lie\u00b7ber", "Re\u00b7gie\u00b7rungs\u00b7rat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Die Ehre, Sie h\u00f6flich zu gr\u00fc\u00dfen.", "tokens": ["Die", "Eh\u00b7re", ",", "Sie", "h\u00f6f\u00b7lich", "zu", "gr\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}}}}