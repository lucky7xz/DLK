{"textgrid.poem.61665": {"metadata": {"author": {"name": "Geibel, Emanuel", "birth": "N.A.", "death": "N.A."}, "title": "34.", "genre": "verse", "period": "N.A.", "pub_year": 1833, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbwoher so viel des Abgeschmackten,", "tokens": ["\u00bb", "wo\u00b7her", "so", "viel", "des", "Ab\u00b7ge\u00b7schmack\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das l\u00e4ngst erschien als abgetan?\u00ab \u2013", "tokens": ["Das", "l\u00e4ngst", "er\u00b7schien", "als", "ab\u00b7ge\u00b7tan", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PDS", "ADV", "VVFIN", "KOKOM", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir sind einmal Autodidakten,", "tokens": ["Wir", "sind", "ein\u00b7mal", "Au\u00b7to\u00b7di\u00b7dak\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und ganz von vorn f\u00e4ngt jeder an.", "tokens": ["Und", "ganz", "von", "vorn", "f\u00e4ngt", "je\u00b7der", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADV", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbwoher so viel des Abgeschmackten,", "tokens": ["\u00bb", "wo\u00b7her", "so", "viel", "des", "Ab\u00b7ge\u00b7schmack\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das l\u00e4ngst erschien als abgetan?\u00ab \u2013", "tokens": ["Das", "l\u00e4ngst", "er\u00b7schien", "als", "ab\u00b7ge\u00b7tan", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PDS", "ADV", "VVFIN", "KOKOM", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir sind einmal Autodidakten,", "tokens": ["Wir", "sind", "ein\u00b7mal", "Au\u00b7to\u00b7di\u00b7dak\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und ganz von vorn f\u00e4ngt jeder an.", "tokens": ["Und", "ganz", "von", "vorn", "f\u00e4ngt", "je\u00b7der", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADV", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}