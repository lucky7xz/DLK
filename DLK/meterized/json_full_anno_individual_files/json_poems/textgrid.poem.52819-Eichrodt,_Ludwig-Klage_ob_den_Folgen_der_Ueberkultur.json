{"textgrid.poem.52819": {"metadata": {"author": {"name": "Eichrodt, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Klage ob den Folgen der Ueberkultur", "genre": "verse", "period": "N.A.", "pub_year": 1859, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Als Gott aus seinem Paradies", "tokens": ["Als", "Gott", "aus", "sei\u00b7nem", "Pa\u00b7ra\u00b7dies"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Adam und die Eva stie\u00df,", "tokens": ["Den", "A\u00b7dam", "und", "die", "E\u00b7va", "stie\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat er es wohl sogleich bereut", "tokens": ["Hat", "er", "es", "wohl", "sog\u00b7leich", "be\u00b7reut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In seiner Unerforschlichkeit.", "tokens": ["In", "sei\u00b7ner", "Un\u00b7er\u00b7for\u00b7schlich\u00b7keit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Denn jetzo ward der Mensch bekannt", "tokens": ["Denn", "jet\u00b7zo", "ward", "der", "Mensch", "be\u00b7kannt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit seinem eigenen Verstand,", "tokens": ["Mit", "sei\u00b7nem", "ei\u00b7ge\u00b7nen", "Ver\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er wurde stolz, bezwang die Noth,", "tokens": ["Er", "wur\u00b7de", "stolz", ",", "be\u00b7zwang", "die", "Noth", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und machte ich's bequem im Koth.", "tokens": ["Und", "mach\u00b7te", "ich's", "be\u00b7quem", "im", "Koth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Zwar anfangs forchte man ", "tokens": ["Zwar", "an\u00b7fangs", "forch\u00b7te", "man"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PIS"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Man opfert', winselte und kroch,", "tokens": ["Man", "op\u00b7fert'", ",", "win\u00b7sel\u00b7te", "und", "kroch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch bald erlebt er gro\u00dfe Schand;", "tokens": ["Doch", "bald", "er\u00b7lebt", "er", "gro\u00b7\u00dfe", "Schand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es lebt der ", "tokens": ["Es", "lebt", "der"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.4": {"line.1": {"text": "Er log, er mordete, er stahl,", "tokens": ["Er", "log", ",", "er", "mor\u00b7de\u00b7te", ",", "er", "stahl", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er hauste wie ein Kannibal;", "tokens": ["Er", "haus\u00b7te", "wie", "ein", "Kan\u00b7ni\u00b7bal", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sobald das Heidenthum begann,", "tokens": ["So\u00b7bald", "das", "Hei\u00b7den\u00b7thum", "be\u00b7gann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fand man auch sein Vergn\u00fcgen dran.", "tokens": ["Fand", "man", "auch", "sein", "Ver\u00b7gn\u00fc\u00b7gen", "dran", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Es traten Leute auf sogar,", "tokens": ["Es", "tra\u00b7ten", "Leu\u00b7te", "auf", "so\u00b7gar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die es bewiesen bis auf's Haar,", "tokens": ["Die", "es", "be\u00b7wie\u00b7sen", "bis", "auf's", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVPP", "APPR", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es existire gar kein Gott,", "tokens": ["Es", "e\u00b7xis\u00b7ti\u00b7re", "gar", "kein", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und f\u00fcgten zu dem Ungl\u00fcck Spott.", "tokens": ["Und", "f\u00fcg\u00b7ten", "zu", "dem", "Un\u00b7gl\u00fcck", "Spott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Zuletzt erlitt ", "tokens": ["Zu\u00b7letzt", "er\u00b7litt"], "token_info": ["word", "word"], "pos": ["ADV", "VVFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Da\u00df Mancher war von selber brav;", "tokens": ["Da\u00df", "Man\u00b7cher", "war", "von", "sel\u00b7ber", "brav", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VAFIN", "APPR", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und jetzt erlebt ", "tokens": ["Und", "jetzt", "er\u00b7lebt"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADV", "VVPP"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Man macht ihm seine Sachen nach!", "tokens": ["Man", "macht", "ihm", "sei\u00b7ne", "Sa\u00b7chen", "nach", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Als Gott aus seinem Paradies", "tokens": ["Als", "Gott", "aus", "sei\u00b7nem", "Pa\u00b7ra\u00b7dies"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Adam und die Eva stie\u00df,", "tokens": ["Den", "A\u00b7dam", "und", "die", "E\u00b7va", "stie\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat er es wohl sogleich bereut", "tokens": ["Hat", "er", "es", "wohl", "sog\u00b7leich", "be\u00b7reut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In seiner Unerforschlichkeit.", "tokens": ["In", "sei\u00b7ner", "Un\u00b7er\u00b7for\u00b7schlich\u00b7keit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Denn jetzo ward der Mensch bekannt", "tokens": ["Denn", "jet\u00b7zo", "ward", "der", "Mensch", "be\u00b7kannt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit seinem eigenen Verstand,", "tokens": ["Mit", "sei\u00b7nem", "ei\u00b7ge\u00b7nen", "Ver\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er wurde stolz, bezwang die Noth,", "tokens": ["Er", "wur\u00b7de", "stolz", ",", "be\u00b7zwang", "die", "Noth", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und machte ich's bequem im Koth.", "tokens": ["Und", "mach\u00b7te", "ich's", "be\u00b7quem", "im", "Koth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Zwar anfangs forchte man ", "tokens": ["Zwar", "an\u00b7fangs", "forch\u00b7te", "man"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PIS"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Man opfert', winselte und kroch,", "tokens": ["Man", "op\u00b7fert'", ",", "win\u00b7sel\u00b7te", "und", "kroch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch bald erlebt er gro\u00dfe Schand;", "tokens": ["Doch", "bald", "er\u00b7lebt", "er", "gro\u00b7\u00dfe", "Schand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es lebt der ", "tokens": ["Es", "lebt", "der"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.10": {"line.1": {"text": "Er log, er mordete, er stahl,", "tokens": ["Er", "log", ",", "er", "mor\u00b7de\u00b7te", ",", "er", "stahl", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er hauste wie ein Kannibal;", "tokens": ["Er", "haus\u00b7te", "wie", "ein", "Kan\u00b7ni\u00b7bal", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sobald das Heidenthum begann,", "tokens": ["So\u00b7bald", "das", "Hei\u00b7den\u00b7thum", "be\u00b7gann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fand man auch sein Vergn\u00fcgen dran.", "tokens": ["Fand", "man", "auch", "sein", "Ver\u00b7gn\u00fc\u00b7gen", "dran", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Es traten Leute auf sogar,", "tokens": ["Es", "tra\u00b7ten", "Leu\u00b7te", "auf", "so\u00b7gar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPR", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die es bewiesen bis auf's Haar,", "tokens": ["Die", "es", "be\u00b7wie\u00b7sen", "bis", "auf's", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVPP", "APPR", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es existire gar kein Gott,", "tokens": ["Es", "e\u00b7xis\u00b7ti\u00b7re", "gar", "kein", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und f\u00fcgten zu dem Ungl\u00fcck Spott.", "tokens": ["Und", "f\u00fcg\u00b7ten", "zu", "dem", "Un\u00b7gl\u00fcck", "Spott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Zuletzt erlitt ", "tokens": ["Zu\u00b7letzt", "er\u00b7litt"], "token_info": ["word", "word"], "pos": ["ADV", "VVFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Da\u00df Mancher war von selber brav;", "tokens": ["Da\u00df", "Man\u00b7cher", "war", "von", "sel\u00b7ber", "brav", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VAFIN", "APPR", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und jetzt erlebt ", "tokens": ["Und", "jetzt", "er\u00b7lebt"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADV", "VVPP"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Man macht ihm seine Sachen nach!", "tokens": ["Man", "macht", "ihm", "sei\u00b7ne", "Sa\u00b7chen", "nach", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}