{"textgrid.poem.67713": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "Pal\u00e4stina", "genre": "verse", "period": "N.A.", "pub_year": 1777, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Da liegst Du nun, ver\u00f6det Land,", "tokens": ["Da", "liegst", "Du", "nun", ",", "ver\u00b7\u00f6\u00b7det", "Land", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo Gottes Fu\u00dftritt stand,", "tokens": ["Wo", "Got\u00b7tes", "Fu\u00df\u00b7tritt", "stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo er erschien, der Ewige,", "tokens": ["Wo", "er", "er\u00b7schien", ",", "der", "E\u00b7wi\u00b7ge", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "ART", "ADJA", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "Ein Mensch und wandelte,", "tokens": ["Ein", "Mensch", "und", "wan\u00b7del\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Geheimni\u00df sprach und Wunder that;", "tokens": ["Ge\u00b7heim\u00b7ni\u00df", "sprach", "und", "Wun\u00b7der", "that", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da liegt in Dir ver\u00f6det nun sein Pfad.", "tokens": ["Da", "liegt", "in", "Dir", "ver\u00b7\u00f6\u00b7det", "nun", "sein", "Pfad", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Sie zeigen jeden Schritt und Tritt,", "tokens": ["Sie", "zei\u00b7gen", "je\u00b7den", "Schritt", "und", "Tritt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur nicht den Wandler mit.", "tokens": ["Nur", "nicht", "den", "Wand\u00b7ler", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sein Dasein, Gegenwart und Kraft \u2013", "tokens": ["Sein", "Da\u00b7sein", ",", "Ge\u00b7gen\u00b7wart", "und", "Kraft", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Alles hingerafft.", "tokens": ["Ist", "Al\u00b7les", "hin\u00b7ge\u00b7rafft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die \u00f6de Stelle trauert da", "tokens": ["Die", "\u00f6\u00b7de", "Stel\u00b7le", "trau\u00b7ert", "da"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und \u00e4chzt: \u00bbHier bin ich, und er ist nicht da!\u00ab", "tokens": ["Und", "\u00e4chzt", ":", "\u00bb", "Hier", "bin", "ich", ",", "und", "er", "ist", "nicht", "da", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "ADV", "VAFIN", "PPER", "$,", "KON", "PPER", "VAFIN", "PTKNEG", "ADV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Und was er sprach, ist leeres Wort,", "tokens": ["Und", "was", "er", "sprach", ",", "ist", "lee\u00b7res", "Wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und was er hie und dort", "tokens": ["Und", "was", "er", "hie", "und", "dort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PPER", "ADV", "KON", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So geist-, so liebevoll einst that,", "tokens": ["So", "geist", ",", "so", "lie\u00b7be\u00b7voll", "einst", "that", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "TRUNC", "$,", "ADV", "ADJD", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Wahn, Betrug und Staat.", "tokens": ["Ist", "Wahn", ",", "Be\u00b7trug", "und", "Staat", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Sie bauen da sein leeres Grab;", "tokens": ["Sie", "bau\u00b7en", "da", "sein", "lee\u00b7res", "Grab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und selbst, sie selbst sind ja sein \u00e4rgstes Grab.", "tokens": ["Und", "selbst", ",", "sie", "selbst", "sind", "ja", "sein", "\u00e4rgs\u00b7tes", "Grab", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "PPER", "ADV", "VAFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "O Trauer! Trauer! Weine, Herz,", "tokens": ["O", "Trau\u00b7er", "!", "Trau\u00b7er", "!", "Wei\u00b7ne", ",", "Herz", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "NN", "$.", "NN", "$.", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den tiefsten Menschenschmerz!", "tokens": ["Den", "tiefs\u00b7ten", "Men\u00b7schen\u00b7schmerz", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo Licht einst war und kam nun Nacht,", "tokens": ["Wo", "Licht", "einst", "war", "und", "kam", "nun", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ADV", "VAFIN", "KON", "VVFIN", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird \u00e4rgre Mitternacht;", "tokens": ["Wird", "\u00e4r\u00b7gre", "Mit\u00b7ter\u00b7nacht", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Wo Altar Gottes einmal stand,", "tokens": ["Wo", "Al\u00b7tar", "Got\u00b7tes", "ein\u00b7mal", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wird zweifach Leichengruft und M\u00f6rderland.", "tokens": ["Wird", "zwei\u00b7fach", "Lei\u00b7chen\u00b7gruft", "und", "M\u00f6r\u00b7der\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Sie kau'n an H\u00fclsen, letzen sich", "tokens": ["Sie", "kau'n", "an", "H\u00fcl\u00b7sen", ",", "let\u00b7zen", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,", "VVFIN", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Schall elendiglich,", "tokens": ["Mit", "Schall", "e\u00b7len\u00b7dig\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Verwehn den Athem vor sich her", "tokens": ["Ver\u00b7wehn", "den", "A\u00b7them", "vor", "sich", "her"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "APPR", "PRF", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und d\u00fcrsten, ach, im Meer!", "tokens": ["Und", "d\u00fcrs\u00b7ten", ",", "ach", ",", "im", "Meer", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VMFIN", "$,", "ITJ", "$,", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "So bist Du Land und Christenthum", "tokens": ["So", "bist", "Du", "Land", "und", "Chris\u00b7ten\u00b7thum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und Griechenland und Rom und \u2013 liebes Lutherthum.", "tokens": ["Und", "Grie\u00b7chen\u00b7land", "und", "Rom", "und", "\u2013", "lie\u00b7bes", "Lu\u00b7ther\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NE", "KON", "NE", "KON", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Da liegst Du nun, ver\u00f6det Land,", "tokens": ["Da", "liegst", "Du", "nun", ",", "ver\u00b7\u00f6\u00b7det", "Land", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo Gottes Fu\u00dftritt stand,", "tokens": ["Wo", "Got\u00b7tes", "Fu\u00df\u00b7tritt", "stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo er erschien, der Ewige,", "tokens": ["Wo", "er", "er\u00b7schien", ",", "der", "E\u00b7wi\u00b7ge", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "ART", "ADJA", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "Ein Mensch und wandelte,", "tokens": ["Ein", "Mensch", "und", "wan\u00b7del\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Geheimni\u00df sprach und Wunder that;", "tokens": ["Ge\u00b7heim\u00b7ni\u00df", "sprach", "und", "Wun\u00b7der", "that", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da liegt in Dir ver\u00f6det nun sein Pfad.", "tokens": ["Da", "liegt", "in", "Dir", "ver\u00b7\u00f6\u00b7det", "nun", "sein", "Pfad", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Sie zeigen jeden Schritt und Tritt,", "tokens": ["Sie", "zei\u00b7gen", "je\u00b7den", "Schritt", "und", "Tritt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur nicht den Wandler mit.", "tokens": ["Nur", "nicht", "den", "Wand\u00b7ler", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sein Dasein, Gegenwart und Kraft \u2013", "tokens": ["Sein", "Da\u00b7sein", ",", "Ge\u00b7gen\u00b7wart", "und", "Kraft", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Alles hingerafft.", "tokens": ["Ist", "Al\u00b7les", "hin\u00b7ge\u00b7rafft", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die \u00f6de Stelle trauert da", "tokens": ["Die", "\u00f6\u00b7de", "Stel\u00b7le", "trau\u00b7ert", "da"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und \u00e4chzt: \u00bbHier bin ich, und er ist nicht da!\u00ab", "tokens": ["Und", "\u00e4chzt", ":", "\u00bb", "Hier", "bin", "ich", ",", "und", "er", "ist", "nicht", "da", "!", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "ADV", "VAFIN", "PPER", "$,", "KON", "PPER", "VAFIN", "PTKNEG", "ADV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Und was er sprach, ist leeres Wort,", "tokens": ["Und", "was", "er", "sprach", ",", "ist", "lee\u00b7res", "Wort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und was er hie und dort", "tokens": ["Und", "was", "er", "hie", "und", "dort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PPER", "ADV", "KON", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So geist-, so liebevoll einst that,", "tokens": ["So", "geist", ",", "so", "lie\u00b7be\u00b7voll", "einst", "that", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "TRUNC", "$,", "ADV", "ADJD", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Wahn, Betrug und Staat.", "tokens": ["Ist", "Wahn", ",", "Be\u00b7trug", "und", "Staat", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Sie bauen da sein leeres Grab;", "tokens": ["Sie", "bau\u00b7en", "da", "sein", "lee\u00b7res", "Grab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und selbst, sie selbst sind ja sein \u00e4rgstes Grab.", "tokens": ["Und", "selbst", ",", "sie", "selbst", "sind", "ja", "sein", "\u00e4rgs\u00b7tes", "Grab", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "PPER", "ADV", "VAFIN", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "O Trauer! Trauer! Weine, Herz,", "tokens": ["O", "Trau\u00b7er", "!", "Trau\u00b7er", "!", "Wei\u00b7ne", ",", "Herz", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "NN", "$.", "NN", "$.", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den tiefsten Menschenschmerz!", "tokens": ["Den", "tiefs\u00b7ten", "Men\u00b7schen\u00b7schmerz", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wo Licht einst war und kam nun Nacht,", "tokens": ["Wo", "Licht", "einst", "war", "und", "kam", "nun", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ADV", "VAFIN", "KON", "VVFIN", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird \u00e4rgre Mitternacht;", "tokens": ["Wird", "\u00e4r\u00b7gre", "Mit\u00b7ter\u00b7nacht", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Wo Altar Gottes einmal stand,", "tokens": ["Wo", "Al\u00b7tar", "Got\u00b7tes", "ein\u00b7mal", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wird zweifach Leichengruft und M\u00f6rderland.", "tokens": ["Wird", "zwei\u00b7fach", "Lei\u00b7chen\u00b7gruft", "und", "M\u00f6r\u00b7der\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Sie kau'n an H\u00fclsen, letzen sich", "tokens": ["Sie", "kau'n", "an", "H\u00fcl\u00b7sen", ",", "let\u00b7zen", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,", "VVFIN", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Schall elendiglich,", "tokens": ["Mit", "Schall", "e\u00b7len\u00b7dig\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Verwehn den Athem vor sich her", "tokens": ["Ver\u00b7wehn", "den", "A\u00b7them", "vor", "sich", "her"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "APPR", "PRF", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und d\u00fcrsten, ach, im Meer!", "tokens": ["Und", "d\u00fcrs\u00b7ten", ",", "ach", ",", "im", "Meer", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VMFIN", "$,", "ITJ", "$,", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "So bist Du Land und Christenthum", "tokens": ["So", "bist", "Du", "Land", "und", "Chris\u00b7ten\u00b7thum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und Griechenland und Rom und \u2013 liebes Lutherthum.", "tokens": ["Und", "Grie\u00b7chen\u00b7land", "und", "Rom", "und", "\u2013", "lie\u00b7bes", "Lu\u00b7ther\u00b7thum", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NE", "KON", "NE", "KON", "$(", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}