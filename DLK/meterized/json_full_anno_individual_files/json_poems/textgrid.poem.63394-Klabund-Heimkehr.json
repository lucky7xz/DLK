{"textgrid.poem.63394": {"metadata": {"author": {"name": "Klabund", "birth": "N.A.", "death": "N.A."}, "title": "Heimkehr", "genre": "verse", "period": "N.A.", "pub_year": 1909, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich bin geboren in einem W\u00e4schekorb,", "tokens": ["Ich", "bin", "ge\u00b7bo\u00b7ren", "in", "ei\u00b7nem", "W\u00e4\u00b7sche\u00b7korb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Aufgewachsen in einem kleinen gr\u00fcnen Garten.", "tokens": ["Auf\u00b7ge\u00b7wach\u00b7sen", "in", "ei\u00b7nem", "klei\u00b7nen", "gr\u00fc\u00b7nen", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "F\u00fcnf Meter lang, f\u00fcnf Meter breit \u2013", "tokens": ["F\u00fcnf", "Me\u00b7ter", "lang", ",", "f\u00fcnf", "Me\u00b7ter", "breit", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "$,", "CARD", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mein Sarg wird wohl noch enger sein.", "tokens": ["Mein", "Sarg", "wird", "wohl", "noch", "en\u00b7ger", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Kohlrabi, Apfelreis, Radieschen,", "tokens": ["Kohl\u00b7ra\u00b7bi", ",", "Ap\u00b7fel\u00b7reis", ",", "Ra\u00b7die\u00b7schen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Waren meine Lieblingsspeisen.", "tokens": ["Wa\u00b7ren", "mei\u00b7ne", "Lieb\u00b7lings\u00b7spei\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das M\u00e4dchen, das mich wartete, hie\u00df Berta Jaensch.", "tokens": ["Das", "M\u00e4d\u00b7chen", ",", "das", "mich", "war\u00b7te\u00b7te", ",", "hie\u00df", "Ber\u00b7ta", "Jaen\u00b7sch", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVFIN", "NE", "NE", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "In den Johannisbeerstr\u00e4uchern am Gartenrand", "tokens": ["In", "den", "Jo\u00b7han\u00b7nis\u00b7beer\u00b7str\u00e4u\u00b7chern", "am", "Gar\u00b7ten\u00b7rand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPRART", "NN"], "meter": "+-+---+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Lebten gute Gnomen und b\u00f6se Eschen.", "tokens": ["Leb\u00b7ten", "gu\u00b7te", "Gno\u00b7men", "und", "b\u00f6\u00b7se", "E\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}}, "stanza.3": {"line.1": {"text": "F\u00fcnfzehn Jahre war ich, da ich von Hause wegging.", "tokens": ["F\u00fcnf\u00b7zehn", "Jah\u00b7re", "war", "ich", ",", "da", "ich", "von", "Hau\u00b7se", "weg\u00b7ging", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Hochtrabend trabte ich zu Ro\u00df aus dem Glog'schen Tor.", "tokens": ["Hoch\u00b7tra\u00b7bend", "trab\u00b7te", "ich", "zu", "Ro\u00df", "aus", "dem", "Glo\u00b7g'\u00b7schen", "Tor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "NN", "APPR", "ART", "NN", "NE", "$."], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Dreiunddrei\u00dfig Jahre bin ich, da ich nach Hause zur\u00fcckkehre", "tokens": ["Drei\u00b7und\u00b7drei\u00b7\u00dfig", "Jah\u00b7re", "bin", "ich", ",", "da", "ich", "nach", "Hau\u00b7se", "zu\u00b7r\u00fcck\u00b7keh\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN"], "meter": "+-+-+--+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.4": {"text": "Auf einem knatternden Motorrad.", "tokens": ["Auf", "ei\u00b7nem", "knat\u00b7tern\u00b7den", "Mo\u00b7tor\u00b7rad", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Die alte h\u00f6lzerne Zugbr\u00fccke ist niedergerissen.", "tokens": ["Die", "al\u00b7te", "h\u00f6l\u00b7zer\u00b7ne", "Zug\u00b7br\u00fc\u00b7cke", "ist", "nie\u00b7der\u00b7ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--++--+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Jetzt bezwingen die Oder Eisen und Beton.", "tokens": ["Jetzt", "be\u00b7zwin\u00b7gen", "die", "O\u00b7der", "Ei\u00b7sen", "und", "Be\u00b7ton", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Nur der Flu\u00df darunter, er flie\u00dft wie vor tausend Jahren", "tokens": ["Nur", "der", "Flu\u00df", "da\u00b7run\u00b7ter", ",", "er", "flie\u00dft", "wie", "vor", "tau\u00b7send", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "PAV", "$,", "PPER", "VVFIN", "KOKOM", "APPR", "CARD", "NN"], "meter": "+-+-+--++-+-+-", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": "So auch heute.", "tokens": ["So", "auch", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Ich gehe durch die Gassen und niemand kennt mich.", "tokens": ["Ich", "ge\u00b7he", "durch", "die", "Gas\u00b7sen", "und", "nie\u00b7mand", "kennt", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "PIS", "VVFIN", "PPER", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich trage Knickerbocker und man h\u00e4lt mich", "tokens": ["Ich", "tra\u00b7ge", "Kni\u00b7cker\u00b7bo\u00b7cker", "und", "man", "h\u00e4lt", "mich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "KON", "PIS", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "F\u00fcr einen reisenden Engl\u00e4nder.", "tokens": ["F\u00fcr", "ei\u00b7nen", "rei\u00b7sen\u00b7den", "En\u00b7gl\u00e4n\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "An der Schmiede, wo ich als Kind ins lohende Feuer sah,", "tokens": ["An", "der", "Schmie\u00b7de", ",", "wo", "ich", "als", "Kind", "ins", "lo\u00b7hen\u00b7de", "Feu\u00b7er", "sah", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PWAV", "PPER", "KOUS", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "--+-++-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "Bleibe ich stehn und starre in Asche und Ru\u00df.", "tokens": ["Blei\u00b7be", "ich", "stehn", "und", "star\u00b7re", "in", "A\u00b7sche", "und", "Ru\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVINF", "KON", "VVFIN", "APPR", "NN", "KON", "NN", "$."], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}}, "stanza.6": {"line.1": {"text": "Oben auf dem Bergfriedhof bin ich nicht allein.", "tokens": ["O\u00b7ben", "auf", "dem", "Berg\u00b7fried\u00b7hof", "bin", "ich", "nicht", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN", "PPER", "PTKNEG", "ADV", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Hier liegen viele, die ich einst gekannt habe.", "tokens": ["Hier", "lie\u00b7gen", "vie\u00b7le", ",", "die", "ich", "einst", "ge\u00b7kannt", "ha\u00b7be", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "PRELS", "PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.3": {"text": "Der alte Professor,", "tokens": ["Der", "al\u00b7te", "Pro\u00b7fes\u00b7sor", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Bei dem ich lateinischen Nachhilfeunterricht hatte,", "tokens": ["Bei", "dem", "ich", "la\u00b7tei\u00b7ni\u00b7schen", "Nach\u00b7hil\u00b7fe\u00b7un\u00b7ter\u00b7richt", "hat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+--+--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Und mein kleiner Bruder.", "tokens": ["Und", "mein", "klei\u00b7ner", "Bru\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.7": {"line.1": {"text": "Jetzt stehe ich am Grabmal eines Generals,", "tokens": ["Jetzt", "ste\u00b7he", "ich", "am", "Grab\u00b7mal", "ei\u00b7nes", "Ge\u00b7ne\u00b7rals", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der unter Friedrich dem Gro\u00dfen focht.", "tokens": ["Der", "un\u00b7ter", "Fried\u00b7rich", "dem", "Gro\u00b7\u00dfen", "focht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Seinen Namen verwitterte das Gestein.", "tokens": ["Sei\u00b7nen", "Na\u00b7men", "ver\u00b7wit\u00b7ter\u00b7te", "das", "Ge\u00b7stein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Was wollte er, was konnte er?", "tokens": ["Was", "woll\u00b7te", "er", ",", "was", "konn\u00b7te", "er", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "$,", "PWS", "VMFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Niemand wei\u00df es.", "tokens": ["Nie\u00b7mand", "wei\u00df", "es", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.8": {"line.1": {"text": "Er f\u00fchrte in der Schlacht von Kunersdorf", "tokens": ["Er", "f\u00fchr\u00b7te", "in", "der", "Schlacht", "von", "Ku\u00b7ners\u00b7dorf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein Grenadierregiment \u2013 und? \u2013", "tokens": ["Ein", "Gre\u00b7na\u00b7dier\u00b7re\u00b7gi\u00b7ment", "\u2013", "und", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["ART", "NN", "$(", "KON", "$.", "$("], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Schritt mit dem Degen in der Faust voran. \u2013 Seine Pflicht. \u2013", "tokens": ["Schritt", "mit", "dem", "De\u00b7gen", "in", "der", "Faust", "vo\u00b7ran", ".", "\u2013", "Sei\u00b7ne", "Pflicht", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "APPR", "ART", "NN", "APPR", "ART", "NN", "PTKVZ", "$.", "$(", "PPOSAT", "NN", "$.", "$("], "meter": "+--+-+-+--+-+", "measure": "iambic.hexa.invert"}, "line.4": {"text": "Er hatte au\u00dfer dem preu\u00dfischen Exerzierreglement", "tokens": ["Er", "hat\u00b7te", "au\u00b7\u00dfer", "dem", "preu\u00b7\u00dfi\u00b7schen", "Ex\u00b7er\u00b7zier\u00b7reg\u00b7le\u00b7ment"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Nie ein Buch gelesen, und war stolz darauf. \u2013", "tokens": ["Nie", "ein", "Buch", "ge\u00b7le\u00b7sen", ",", "und", "war", "stolz", "da\u00b7rauf", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ART", "NN", "VVPP", "$,", "KON", "VAFIN", "ADJD", "PAV", "$.", "$("], "meter": "+-+-+--++-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.9": {"line.1": {"text": "Wir haben alle B\u00fccher gelesen und keine Schlacht geschlagen.", "tokens": ["Wir", "ha\u00b7ben", "al\u00b7le", "B\u00fc\u00b7cher", "ge\u00b7le\u00b7sen", "und", "kei\u00b7ne", "Schlacht", "ge\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "VVPP", "KON", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.2": {"text": "Es ist eines so wenig wert als das andere.", "tokens": ["Es", "ist", "ei\u00b7nes", "so", "we\u00b7nig", "wert", "als", "das", "an\u00b7de\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADV", "ADV", "ADJD", "KOKOM", "ART", "ADJA", "$."], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Einmal werden vor meinem Grab die Leute stehn.", "tokens": ["Ein\u00b7mal", "wer\u00b7den", "vor", "mei\u00b7nem", "Grab", "die", "Leu\u00b7te", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Was wollte er, was konnte er?", "tokens": ["Was", "woll\u00b7te", "er", ",", "was", "konn\u00b7te", "er", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "$,", "PWS", "VMFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Niemand wei\u00df es.", "tokens": ["Nie\u00b7mand", "wei\u00df", "es", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.10": {"line.1": {"text": "Hoppla, Bruder, steh auf,", "tokens": ["Hopp\u00b7la", ",", "Bru\u00b7der", ",", "steh", "auf", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Du hast schon lange genug geschlafen.", "tokens": ["Du", "hast", "schon", "lan\u00b7ge", "ge\u00b7nug", "ge\u00b7schla\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Jetzt bin ich an der Reihe.", "tokens": ["Jetzt", "bin", "ich", "an", "der", "Rei\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da hast du meinen Stock, Esche, Natur, ungebeizt, Hornspitze.", "tokens": ["Da", "hast", "du", "mei\u00b7nen", "Stock", ",", "E\u00b7sche", ",", "Na\u00b7tur", ",", "un\u00b7ge\u00b7beizt", ",", "Horns\u00b7pit\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "$,", "ADJA", "$,", "NN", "$,", "ADJD", "$,", "NN", "$."], "meter": "-+-+-+--+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "Geh an meiner Stelle hinunter in die Stadt.", "tokens": ["Geh", "an", "mei\u00b7ner", "Stel\u00b7le", "hin\u00b7un\u00b7ter", "in", "die", "Stadt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PPOSAT", "NN", "APPO", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.11": {"line.1": {"text": "Es d\u00e4mmert. Ehe die erste Gaslaterne aufflammt,", "tokens": ["Es", "d\u00e4m\u00b7mert", ".", "E\u00b7he", "die", "ers\u00b7te", "Gas\u00b7la\u00b7ter\u00b7ne", "auf\u00b7flammt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Wirst du am Marktplatz sein.", "tokens": ["Wirst", "du", "am", "Markt\u00b7platz", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dort steht die K\u00f6nigl. Preu\u00dfische Adlerapotheke.", "tokens": ["Dort", "steht", "die", "K\u00f6\u00b7nigl", ".", "Preu\u00b7\u00dfi\u00b7sche", "Ad\u00b7le\u00b7ra\u00b7po\u00b7the\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "ADJA", "NN", "$."], "meter": "-+-+-+---+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Bringe Vater und Mutter einen Gru\u00df von mir.", "tokens": ["Brin\u00b7ge", "Va\u00b7ter", "und", "Mut\u00b7ter", "ei\u00b7nen", "Gru\u00df", "von", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "NN", "ART", "NN", "APPR", "PPER", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.12": {"line.1": {"text": "Sag ihnen, ich h\u00e4tte mich zur ewigen Ruh begeben", "tokens": ["Sag", "ih\u00b7nen", ",", "ich", "h\u00e4t\u00b7te", "mich", "zur", "e\u00b7wi\u00b7gen", "Ruh", "be\u00b7ge\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "$,", "PPER", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVINF"], "meter": "-+--+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Und mich lebendig begraben.", "tokens": ["Und", "mich", "le\u00b7ben\u00b7dig", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Drei H\u00e4nde Erde auf mein Grab,", "tokens": ["Drei", "H\u00e4n\u00b7de", "Er\u00b7de", "auf", "mein", "Grab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Drei Seufzer, drei Tr\u00e4nen und damit basta.", "tokens": ["Drei", "Seuf\u00b7zer", ",", "drei", "Tr\u00e4\u00b7nen", "und", "da\u00b7mit", "bas\u00b7ta", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "CARD", "NN", "KON", "PAV", "NE", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Bitte, Vater, la\u00df dich in der sachgem\u00e4\u00dfen Herstellung", "tokens": ["Bit\u00b7te", ",", "Va\u00b7ter", ",", "la\u00df", "dich", "in", "der", "sach\u00b7ge\u00b7m\u00e4\u00b7\u00dfen", "Her\u00b7stel\u00b7lung"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "NN", "$,", "VVIMP", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+--+-", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "Von Dr. A. Henschkes Restitutionsfluid nicht st\u00f6ren.", "tokens": ["Von", "Dr.", "A.", "Henschkes", "Res\u00b7ti\u00b7tu\u00b7ti\u00b7ons\u00b7flu\u00b7id", "nicht", "st\u00f6\u00b7ren", "."], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPRART", "ADJA", "NN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.13": {"line.1": {"text": "Ich bin geboren in einem W\u00e4schekorb,", "tokens": ["Ich", "bin", "ge\u00b7bo\u00b7ren", "in", "ei\u00b7nem", "W\u00e4\u00b7sche\u00b7korb", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Aufgewachsen in einem kleinen gr\u00fcnen Garten.", "tokens": ["Auf\u00b7ge\u00b7wach\u00b7sen", "in", "ei\u00b7nem", "klei\u00b7nen", "gr\u00fc\u00b7nen", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "F\u00fcnf Meter lang, f\u00fcnf Meter breit \u2013", "tokens": ["F\u00fcnf", "Me\u00b7ter", "lang", ",", "f\u00fcnf", "Me\u00b7ter", "breit", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "$,", "CARD", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mein Sarg wird wohl noch enger sein.", "tokens": ["Mein", "Sarg", "wird", "wohl", "noch", "en\u00b7ger", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Kohlrabi, Apfelreis, Radieschen,", "tokens": ["Kohl\u00b7ra\u00b7bi", ",", "Ap\u00b7fel\u00b7reis", ",", "Ra\u00b7die\u00b7schen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Waren meine Lieblingsspeisen.", "tokens": ["Wa\u00b7ren", "mei\u00b7ne", "Lieb\u00b7lings\u00b7spei\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das M\u00e4dchen, das mich wartete, hie\u00df Berta Jaensch.", "tokens": ["Das", "M\u00e4d\u00b7chen", ",", "das", "mich", "war\u00b7te\u00b7te", ",", "hie\u00df", "Ber\u00b7ta", "Jaen\u00b7sch", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "VVFIN", "NE", "NE", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.4": {"text": "In den Johannisbeerstr\u00e4uchern am Gartenrand", "tokens": ["In", "den", "Jo\u00b7han\u00b7nis\u00b7beer\u00b7str\u00e4u\u00b7chern", "am", "Gar\u00b7ten\u00b7rand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPRART", "NN"], "meter": "+-+---+--+-+", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Lebten gute Gnomen und b\u00f6se Eschen.", "tokens": ["Leb\u00b7ten", "gu\u00b7te", "Gno\u00b7men", "und", "b\u00f6\u00b7se", "E\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}}, "stanza.15": {"line.1": {"text": "F\u00fcnfzehn Jahre war ich, da ich von Hause wegging.", "tokens": ["F\u00fcnf\u00b7zehn", "Jah\u00b7re", "war", "ich", ",", "da", "ich", "von", "Hau\u00b7se", "weg\u00b7ging", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Hochtrabend trabte ich zu Ro\u00df aus dem Glog'schen Tor.", "tokens": ["Hoch\u00b7tra\u00b7bend", "trab\u00b7te", "ich", "zu", "Ro\u00df", "aus", "dem", "Glo\u00b7g'\u00b7schen", "Tor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "NN", "APPR", "ART", "NN", "NE", "$."], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Dreiunddrei\u00dfig Jahre bin ich, da ich nach Hause zur\u00fcckkehre", "tokens": ["Drei\u00b7und\u00b7drei\u00b7\u00dfig", "Jah\u00b7re", "bin", "ich", ",", "da", "ich", "nach", "Hau\u00b7se", "zu\u00b7r\u00fcck\u00b7keh\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VAFIN", "PPER", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN"], "meter": "+-+-+--+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.4": {"text": "Auf einem knatternden Motorrad.", "tokens": ["Auf", "ei\u00b7nem", "knat\u00b7tern\u00b7den", "Mo\u00b7tor\u00b7rad", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "Die alte h\u00f6lzerne Zugbr\u00fccke ist niedergerissen.", "tokens": ["Die", "al\u00b7te", "h\u00f6l\u00b7zer\u00b7ne", "Zug\u00b7br\u00fc\u00b7cke", "ist", "nie\u00b7der\u00b7ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+--++--+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Jetzt bezwingen die Oder Eisen und Beton.", "tokens": ["Jetzt", "be\u00b7zwin\u00b7gen", "die", "O\u00b7der", "Ei\u00b7sen", "und", "Be\u00b7ton", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Nur der Flu\u00df darunter, er flie\u00dft wie vor tausend Jahren", "tokens": ["Nur", "der", "Flu\u00df", "da\u00b7run\u00b7ter", ",", "er", "flie\u00dft", "wie", "vor", "tau\u00b7send", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "PAV", "$,", "PPER", "VVFIN", "KOKOM", "APPR", "CARD", "NN"], "meter": "+-+-+--++-+-+-", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": "So auch heute.", "tokens": ["So", "auch", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.17": {"line.1": {"text": "Ich gehe durch die Gassen und niemand kennt mich.", "tokens": ["Ich", "ge\u00b7he", "durch", "die", "Gas\u00b7sen", "und", "nie\u00b7mand", "kennt", "mich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "PIS", "VVFIN", "PPER", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich trage Knickerbocker und man h\u00e4lt mich", "tokens": ["Ich", "tra\u00b7ge", "Kni\u00b7cker\u00b7bo\u00b7cker", "und", "man", "h\u00e4lt", "mich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "KON", "PIS", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "F\u00fcr einen reisenden Engl\u00e4nder.", "tokens": ["F\u00fcr", "ei\u00b7nen", "rei\u00b7sen\u00b7den", "En\u00b7gl\u00e4n\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "An der Schmiede, wo ich als Kind ins lohende Feuer sah,", "tokens": ["An", "der", "Schmie\u00b7de", ",", "wo", "ich", "als", "Kind", "ins", "lo\u00b7hen\u00b7de", "Feu\u00b7er", "sah", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PWAV", "PPER", "KOUS", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "--+-++-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "Bleibe ich stehn und starre in Asche und Ru\u00df.", "tokens": ["Blei\u00b7be", "ich", "stehn", "und", "star\u00b7re", "in", "A\u00b7sche", "und", "Ru\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVINF", "KON", "VVFIN", "APPR", "NN", "KON", "NN", "$."], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}}, "stanza.18": {"line.1": {"text": "Oben auf dem Bergfriedhof bin ich nicht allein.", "tokens": ["O\u00b7ben", "auf", "dem", "Berg\u00b7fried\u00b7hof", "bin", "ich", "nicht", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VAFIN", "PPER", "PTKNEG", "ADV", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Hier liegen viele, die ich einst gekannt habe.", "tokens": ["Hier", "lie\u00b7gen", "vie\u00b7le", ",", "die", "ich", "einst", "ge\u00b7kannt", "ha\u00b7be", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "PRELS", "PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.3": {"text": "Der alte Professor,", "tokens": ["Der", "al\u00b7te", "Pro\u00b7fes\u00b7sor", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Bei dem ich lateinischen Nachhilfeunterricht hatte,", "tokens": ["Bei", "dem", "ich", "la\u00b7tei\u00b7ni\u00b7schen", "Nach\u00b7hil\u00b7fe\u00b7un\u00b7ter\u00b7richt", "hat\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADJA", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+--+--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Und mein kleiner Bruder.", "tokens": ["Und", "mein", "klei\u00b7ner", "Bru\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.19": {"line.1": {"text": "Jetzt stehe ich am Grabmal eines Generals,", "tokens": ["Jetzt", "ste\u00b7he", "ich", "am", "Grab\u00b7mal", "ei\u00b7nes", "Ge\u00b7ne\u00b7rals", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der unter Friedrich dem Gro\u00dfen focht.", "tokens": ["Der", "un\u00b7ter", "Fried\u00b7rich", "dem", "Gro\u00b7\u00dfen", "focht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NE", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Seinen Namen verwitterte das Gestein.", "tokens": ["Sei\u00b7nen", "Na\u00b7men", "ver\u00b7wit\u00b7ter\u00b7te", "das", "Ge\u00b7stein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Was wollte er, was konnte er?", "tokens": ["Was", "woll\u00b7te", "er", ",", "was", "konn\u00b7te", "er", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "$,", "PWS", "VMFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Niemand wei\u00df es.", "tokens": ["Nie\u00b7mand", "wei\u00df", "es", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.20": {"line.1": {"text": "Er f\u00fchrte in der Schlacht von Kunersdorf", "tokens": ["Er", "f\u00fchr\u00b7te", "in", "der", "Schlacht", "von", "Ku\u00b7ners\u00b7dorf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein Grenadierregiment \u2013 und? \u2013", "tokens": ["Ein", "Gre\u00b7na\u00b7dier\u00b7re\u00b7gi\u00b7ment", "\u2013", "und", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["ART", "NN", "$(", "KON", "$.", "$("], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Schritt mit dem Degen in der Faust voran. \u2013 Seine Pflicht. \u2013", "tokens": ["Schritt", "mit", "dem", "De\u00b7gen", "in", "der", "Faust", "vo\u00b7ran", ".", "\u2013", "Sei\u00b7ne", "Pflicht", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "APPR", "ART", "NN", "APPR", "ART", "NN", "PTKVZ", "$.", "$(", "PPOSAT", "NN", "$.", "$("], "meter": "+--+-+-+--+-+", "measure": "iambic.hexa.invert"}, "line.4": {"text": "Er hatte au\u00dfer dem preu\u00dfischen Exerzierreglement", "tokens": ["Er", "hat\u00b7te", "au\u00b7\u00dfer", "dem", "preu\u00b7\u00dfi\u00b7schen", "Ex\u00b7er\u00b7zier\u00b7reg\u00b7le\u00b7ment"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Nie ein Buch gelesen, und war stolz darauf. \u2013", "tokens": ["Nie", "ein", "Buch", "ge\u00b7le\u00b7sen", ",", "und", "war", "stolz", "da\u00b7rauf", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ART", "NN", "VVPP", "$,", "KON", "VAFIN", "ADJD", "PAV", "$.", "$("], "meter": "+-+-+--++-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.21": {"line.1": {"text": "Wir haben alle B\u00fccher gelesen und keine Schlacht geschlagen.", "tokens": ["Wir", "ha\u00b7ben", "al\u00b7le", "B\u00fc\u00b7cher", "ge\u00b7le\u00b7sen", "und", "kei\u00b7ne", "Schlacht", "ge\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "VVPP", "KON", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.2": {"text": "Es ist eines so wenig wert als das andere.", "tokens": ["Es", "ist", "ei\u00b7nes", "so", "we\u00b7nig", "wert", "als", "das", "an\u00b7de\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADV", "ADV", "ADJD", "KOKOM", "ART", "ADJA", "$."], "meter": "--+--+-+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Einmal werden vor meinem Grab die Leute stehn.", "tokens": ["Ein\u00b7mal", "wer\u00b7den", "vor", "mei\u00b7nem", "Grab", "die", "Leu\u00b7te", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPR", "PPOSAT", "NN", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Was wollte er, was konnte er?", "tokens": ["Was", "woll\u00b7te", "er", ",", "was", "konn\u00b7te", "er", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "$,", "PWS", "VMFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Niemand wei\u00df es.", "tokens": ["Nie\u00b7mand", "wei\u00df", "es", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.22": {"line.1": {"text": "Hoppla, Bruder, steh auf,", "tokens": ["Hopp\u00b7la", ",", "Bru\u00b7der", ",", "steh", "auf", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Du hast schon lange genug geschlafen.", "tokens": ["Du", "hast", "schon", "lan\u00b7ge", "ge\u00b7nug", "ge\u00b7schla\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Jetzt bin ich an der Reihe.", "tokens": ["Jetzt", "bin", "ich", "an", "der", "Rei\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da hast du meinen Stock, Esche, Natur, ungebeizt, Hornspitze.", "tokens": ["Da", "hast", "du", "mei\u00b7nen", "Stock", ",", "E\u00b7sche", ",", "Na\u00b7tur", ",", "un\u00b7ge\u00b7beizt", ",", "Horns\u00b7pit\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "$,", "ADJA", "$,", "NN", "$,", "ADJD", "$,", "NN", "$."], "meter": "-+-+-+--+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "Geh an meiner Stelle hinunter in die Stadt.", "tokens": ["Geh", "an", "mei\u00b7ner", "Stel\u00b7le", "hin\u00b7un\u00b7ter", "in", "die", "Stadt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PPOSAT", "NN", "APPO", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.23": {"line.1": {"text": "Es d\u00e4mmert. Ehe die erste Gaslaterne aufflammt,", "tokens": ["Es", "d\u00e4m\u00b7mert", ".", "E\u00b7he", "die", "ers\u00b7te", "Gas\u00b7la\u00b7ter\u00b7ne", "auf\u00b7flammt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "NN", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Wirst du am Marktplatz sein.", "tokens": ["Wirst", "du", "am", "Markt\u00b7platz", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Dort steht die K\u00f6nigl. Preu\u00dfische Adlerapotheke.", "tokens": ["Dort", "steht", "die", "K\u00f6\u00b7nigl", ".", "Preu\u00b7\u00dfi\u00b7sche", "Ad\u00b7le\u00b7ra\u00b7po\u00b7the\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "ADJA", "NN", "$."], "meter": "-+-+-+---+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Bringe Vater und Mutter einen Gru\u00df von mir.", "tokens": ["Brin\u00b7ge", "Va\u00b7ter", "und", "Mut\u00b7ter", "ei\u00b7nen", "Gru\u00df", "von", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "NN", "ART", "NN", "APPR", "PPER", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.24": {"line.1": {"text": "Sag ihnen, ich h\u00e4tte mich zur ewigen Ruh begeben", "tokens": ["Sag", "ih\u00b7nen", ",", "ich", "h\u00e4t\u00b7te", "mich", "zur", "e\u00b7wi\u00b7gen", "Ruh", "be\u00b7ge\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "$,", "PPER", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVINF"], "meter": "-+--+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Und mich lebendig begraben.", "tokens": ["Und", "mich", "le\u00b7ben\u00b7dig", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Drei H\u00e4nde Erde auf mein Grab,", "tokens": ["Drei", "H\u00e4n\u00b7de", "Er\u00b7de", "auf", "mein", "Grab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Drei Seufzer, drei Tr\u00e4nen und damit basta.", "tokens": ["Drei", "Seuf\u00b7zer", ",", "drei", "Tr\u00e4\u00b7nen", "und", "da\u00b7mit", "bas\u00b7ta", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "CARD", "NN", "KON", "PAV", "NE", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Bitte, Vater, la\u00df dich in der sachgem\u00e4\u00dfen Herstellung", "tokens": ["Bit\u00b7te", ",", "Va\u00b7ter", ",", "la\u00df", "dich", "in", "der", "sach\u00b7ge\u00b7m\u00e4\u00b7\u00dfen", "Her\u00b7stel\u00b7lung"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "NN", "$,", "VVIMP", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+--+-", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "Von Dr. A. Henschkes Restitutionsfluid nicht st\u00f6ren.", "tokens": ["Von", "Dr.", "A.", "Henschkes", "Res\u00b7ti\u00b7tu\u00b7ti\u00b7ons\u00b7flu\u00b7id", "nicht", "st\u00f6\u00b7ren", "."], "token_info": ["word", "abbreviation", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPRART", "ADJA", "NN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}}}}