{"textgrid.poem.26406": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "1L: Bett an Bett, zwei Betten standen,", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Bett an Bett, zwei Betten standen,", "tokens": ["Bett", "an", "Bett", ",", "zwei", "Bet\u00b7ten", "stan\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "CARD", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Drauf sich zwei Pakete fanden.", "tokens": ["Drauf", "sich", "zwei", "Pa\u00b7ke\u00b7te", "fan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PRF", "CARD", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frische W\u00e4sche war darin.", "tokens": ["Fri\u00b7sche", "W\u00e4\u00b7sche", "war", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Zwei legten sie vorhin hin.", "tokens": ["Zwei", "leg\u00b7ten", "sie", "vor\u00b7hin", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Diese Zwei sind dann gegangen.", "tokens": ["Die\u00b7se", "Zwei", "sind", "dann", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "CARD", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und mit Blicken, mit schmerzlangen,", "tokens": ["Und", "mit", "Bli\u00b7cken", ",", "mit", "schmerz\u00b7lan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,", "APPR", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schieden sie vom Zimmerort.", "tokens": ["Schie\u00b7den", "sie", "vom", "Zim\u00b7mer\u00b7ort", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Denn man ging f\u00fcr immer fort.", "tokens": ["Denn", "man", "ging", "f\u00fcr", "im\u00b7mer", "fort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Und in den Paketen drinnen", "tokens": ["Und", "in", "den", "Pa\u00b7ke\u00b7ten", "drin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADV"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lagen Hemden, zwei aus Linnen.", "tokens": ["La\u00b7gen", "Hem\u00b7den", ",", "zwei", "aus", "Lin\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "CARD", "APPR", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eines davon war ein Herr,", "tokens": ["Ei\u00b7nes", "da\u00b7von", "war", "ein", "Herr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PAV", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch das andere weiblicher.", "tokens": ["Doch", "das", "an\u00b7de\u00b7re", "weib\u00b7li\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "ADJA", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Beide Hemden sich gut kannten", "tokens": ["Bei\u00b7de", "Hem\u00b7den", "sich", "gut", "kann\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "PRF", "ADJD", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und sich laut beim Namen nannten.", "tokens": ["Und", "sich", "laut", "beim", "Na\u00b7men", "nann\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADJD", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das, vom Herrn, hie\u00df Heinerich,", "tokens": ["Das", ",", "vom", "Herrn", ",", "hie\u00df", "Hei\u00b7ne\u00b7rich", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "$,", "APPRART", "NN", "$,", "VVFIN", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Frida rief das andere sich.", "tokens": ["Fri\u00b7da", "rief", "das", "an\u00b7de\u00b7re", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "ADJA", "PRF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "\u00bbheinerich, was soll's bedeuten?", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "was", "soll's", "be\u00b7deu\u00b7ten", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "PRELS", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was geht vor mit unsern Leuten?", "tokens": ["Was", "geht", "vor", "mit", "un\u00b7sern", "Leu\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Etwas mu\u00df im Gange sein,", "tokens": ["Et\u00b7was", "mu\u00df", "im", "Gan\u00b7ge", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "APPRART", "NN", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Man schlie\u00dft uns hier seltsam ein.\u00ab", "tokens": ["Man", "schlie\u00dft", "uns", "hier", "selt\u00b7sam", "ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbfrida, mu\u00dft Dich nicht erschrecken,", "tokens": ["\u00bb", "fri\u00b7da", ",", "mu\u00dft", "Dich", "nicht", "er\u00b7schre\u00b7cken", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$,", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sonst kriegt Leinewand Stockflecken.", "tokens": ["Sonst", "kriegt", "Lei\u00b7ne\u00b7wand", "Stock\u00b7fle\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sieht mans Leben zu genau,", "tokens": ["Sieht", "mans", "Le\u00b7ben", "zu", "ge\u00b7nau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PTKA", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wird leicht jeder Faden grau.\u00ab", "tokens": ["Wird", "leicht", "je\u00b7der", "Fa\u00b7den", "grau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADJD", "PIAT", "NN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "\u00bbheinerich, mir wird's so eigen!\u00ab", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "mir", "wird's", "so", "ei\u00b7gen", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbfrida, sp\u00e4ter wird sich's zeigen.\u00ab", "tokens": ["\u00bb", "fri\u00b7da", ",", "sp\u00e4\u00b7ter", "wird", "sich's", "zei\u00b7gen", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM.la", "$,", "ADJD", "VAFIN", "PIS", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbheinerich, mir wird so schw\u00fcl!\u00ab", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "mir", "wird", "so", "schw\u00fcl", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbechte Leinwand, Weib, bleibt k\u00fchl.\u00ab", "tokens": ["\u00bb", "ech\u00b7te", "Lein\u00b7wand", ",", "Weib", ",", "bleibt", "k\u00fchl", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "NN", "$,", "VVFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "\u00bbheinerich, ich f\u00fchl ohn' Ende", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "ich", "f\u00fchl", "ohn'", "En\u00b7de"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$,", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jene Finger jener H\u00e4nde,", "tokens": ["Je\u00b7ne", "Fin\u00b7ger", "je\u00b7ner", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jenes Blut, das mich sonst w\u00e4rmt,", "tokens": ["Je\u00b7nes", "Blut", ",", "das", "mich", "sonst", "w\u00e4rmt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "PRELS", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Schien mir heute so verh\u00e4rmt.", "tokens": ["Schien", "mir", "heu\u00b7te", "so", "ver\u00b7h\u00e4rmt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Heute stehn wir vor dem Tode!", "tokens": ["Heu\u00b7te", "stehn", "wir", "vor", "dem", "To\u00b7de", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gestern noch in der Kommode \u2013", "tokens": ["Ge\u00b7stern", "noch", "in", "der", "Kom\u00b7mo\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Heinerich, wer h\u00e4tt's gedacht,", "tokens": ["Hei\u00b7ne\u00b7rich", ",", "wer", "h\u00e4tt's", "ge\u00b7dacht", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PWS", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Leichen schm\u00fccken wir zur Nacht.\u00ab", "tokens": ["Lei\u00b7chen", "schm\u00fc\u00b7cken", "wir", "zur", "Nacht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "\u00bbfrida, was treibt Dich zu Schl\u00fcssen,", "tokens": ["\u00bb", "fri\u00b7da", ",", "was", "treibt", "Dich", "zu", "Schl\u00fcs\u00b7sen", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$,", "PWS", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die sich erst beweisen m\u00fcssen?", "tokens": ["Die", "sich", "erst", "be\u00b7wei\u00b7sen", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ADV", "VVINF", "VMINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unser Leben war nie schwer,", "tokens": ["Un\u00b7ser", "Le\u00b7ben", "war", "nie", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Lustig ging's stets in uns her.", "tokens": ["Lus\u00b7tig", "ging's", "stets", "in", "uns", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Denk' nur an die Hochzeitshitzen,", "tokens": ["Denk'", "nur", "an", "die", "Hoch\u00b7zeits\u00b7hit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die in unseren F\u00e4den sitzen!", "tokens": ["Die", "in", "un\u00b7se\u00b7ren", "F\u00e4\u00b7den", "sit\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Dampfend lag Dein Spitzenlatz", "tokens": ["Damp\u00b7fend", "lag", "Dein", "Spit\u00b7zen\u00b7latz"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Oft an meinem Brusteinsatz.", "tokens": ["Oft", "an", "mei\u00b7nem", "Brus\u00b7tein\u00b7satz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Frida, mach' nicht mehr Grimassen!", "tokens": ["Fri\u00b7da", ",", "mach'", "nicht", "mehr", "Gri\u00b7mas\u00b7sen", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PTKNEG", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Man mu\u00df Dich sonst b\u00fcgeln lassen\u00ab.", "tokens": ["Man", "mu\u00df", "Dich", "sonst", "b\u00fc\u00b7geln", "las\u00b7sen", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VMFIN", "PPER", "ADV", "VVINF", "VVINF", "$(", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbheinz, etwas ins Zimmer tritt,", "tokens": ["\u00bb", "heinz", ",", "et\u00b7was", "ins", "Zim\u00b7mer", "tritt", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Kalt wie einer Schere Schnitt.", "tokens": ["Kalt", "wie", "ei\u00b7ner", "Sche\u00b7re", "Schnitt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Eiskalt werden meine Spitzen.", "tokens": ["Eis\u00b7kalt", "wer\u00b7den", "mei\u00b7ne", "Spit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Siehst Du es am Bett nicht sitzen?", "tokens": ["Siehst", "Du", "es", "am", "Bett", "nicht", "sit\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "APPRART", "NN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ach, ich sehe es genau,", "tokens": ["Ach", ",", "ich", "se\u00b7he", "es", "ge\u00b7nau", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "'s ist der Tod mit seiner Frau.\u00ab", "tokens": ["'s", "ist", "der", "Tod", "mit", "sei\u00b7ner", "Frau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Beide Hemden, unter Zittern,", "tokens": ["Bei\u00b7de", "Hem\u00b7den", ",", "un\u00b7ter", "Zit\u00b7tern", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bei dem Bett den Tod kalt wittern.", "tokens": ["Bei", "dem", "Bett", "den", "Tod", "kalt", "wit\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und es krachte jede Naht,", "tokens": ["Und", "es", "krach\u00b7te", "je\u00b7de", "Naht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Weil der Tod sie sch\u00fctteln tat.", "tokens": ["Weil", "der", "Tod", "sie", "sch\u00fct\u00b7teln", "tat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Horch! Ein Poltern auf den Treppen.", "tokens": ["Horch", "!", "Ein", "Pol\u00b7tern", "auf", "den", "Trep\u00b7pen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "M\u00e4nner jetzt zwei Leichen schleppen.", "tokens": ["M\u00e4n\u00b7ner", "jetzt", "zwei", "Lei\u00b7chen", "schlep\u00b7pen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "CARD", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frida kannte sie sogleich,", "tokens": ["Fri\u00b7da", "kann\u00b7te", "sie", "sog\u00b7leich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und auch Heinerich wird bleich.", "tokens": ["Und", "auch", "Hei\u00b7ne\u00b7rich", "wird", "bleich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Denn der guten Hemden Leute,", "tokens": ["Denn", "der", "gu\u00b7ten", "Hem\u00b7den", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gar zu mies ging's ihnen heute.", "tokens": ["Gar", "zu", "mies", "ging's", "ih\u00b7nen", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKA", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Elend sucht den Tod, weil's mu\u00df \u2013", "tokens": ["E\u00b7lend", "sucht", "den", "Tod", ",", "weil's", "mu\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "$,", "PIS", "VMFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Tot zog man sie aus dem Flu\u00df.", "tokens": ["Tot", "zog", "man", "sie", "aus", "dem", "Flu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.17": {"line.1": {"text": "In derselben Nacht im Zimmer", "tokens": ["In", "der\u00b7sel\u00b7ben", "Nacht", "im", "Zim\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lagen Zwei im Bett wie immer;", "tokens": ["La\u00b7gen", "Zwei", "im", "Bett", "wie", "im\u00b7mer", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "CARD", "APPRART", "NN", "KOKOM", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eins im Tode, au\u00dfen fremd,", "tokens": ["Eins", "im", "To\u00b7de", ",", "au\u00b7\u00dfen", "fremd", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Schm\u00fcckte sie ihr Hochzeitshemd.", "tokens": ["Schm\u00fcck\u00b7te", "sie", "ihr", "Hoch\u00b7zeits\u00b7hemd", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Bett an Bett, zwei Betten standen,", "tokens": ["Bett", "an", "Bett", ",", "zwei", "Bet\u00b7ten", "stan\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "CARD", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Drauf sich zwei Pakete fanden.", "tokens": ["Drauf", "sich", "zwei", "Pa\u00b7ke\u00b7te", "fan\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PRF", "CARD", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frische W\u00e4sche war darin.", "tokens": ["Fri\u00b7sche", "W\u00e4\u00b7sche", "war", "da\u00b7rin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Zwei legten sie vorhin hin.", "tokens": ["Zwei", "leg\u00b7ten", "sie", "vor\u00b7hin", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.19": {"line.1": {"text": "Diese Zwei sind dann gegangen.", "tokens": ["Die\u00b7se", "Zwei", "sind", "dann", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "CARD", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und mit Blicken, mit schmerzlangen,", "tokens": ["Und", "mit", "Bli\u00b7cken", ",", "mit", "schmerz\u00b7lan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,", "APPR", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schieden sie vom Zimmerort.", "tokens": ["Schie\u00b7den", "sie", "vom", "Zim\u00b7mer\u00b7ort", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Denn man ging f\u00fcr immer fort.", "tokens": ["Denn", "man", "ging", "f\u00fcr", "im\u00b7mer", "fort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "Und in den Paketen drinnen", "tokens": ["Und", "in", "den", "Pa\u00b7ke\u00b7ten", "drin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ADV"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lagen Hemden, zwei aus Linnen.", "tokens": ["La\u00b7gen", "Hem\u00b7den", ",", "zwei", "aus", "Lin\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "CARD", "APPR", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eines davon war ein Herr,", "tokens": ["Ei\u00b7nes", "da\u00b7von", "war", "ein", "Herr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PAV", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch das andere weiblicher.", "tokens": ["Doch", "das", "an\u00b7de\u00b7re", "weib\u00b7li\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "ADJA", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}}, "stanza.21": {"line.1": {"text": "Beide Hemden sich gut kannten", "tokens": ["Bei\u00b7de", "Hem\u00b7den", "sich", "gut", "kann\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "PRF", "ADJD", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und sich laut beim Namen nannten.", "tokens": ["Und", "sich", "laut", "beim", "Na\u00b7men", "nann\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADJD", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das, vom Herrn, hie\u00df Heinerich,", "tokens": ["Das", ",", "vom", "Herrn", ",", "hie\u00df", "Hei\u00b7ne\u00b7rich", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "$,", "APPRART", "NN", "$,", "VVFIN", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Frida rief das andere sich.", "tokens": ["Fri\u00b7da", "rief", "das", "an\u00b7de\u00b7re", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "ADJA", "PRF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "\u00bbheinerich, was soll's bedeuten?", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "was", "soll's", "be\u00b7deu\u00b7ten", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "PRELS", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was geht vor mit unsern Leuten?", "tokens": ["Was", "geht", "vor", "mit", "un\u00b7sern", "Leu\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Etwas mu\u00df im Gange sein,", "tokens": ["Et\u00b7was", "mu\u00df", "im", "Gan\u00b7ge", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "APPRART", "NN", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Man schlie\u00dft uns hier seltsam ein.\u00ab", "tokens": ["Man", "schlie\u00dft", "uns", "hier", "selt\u00b7sam", "ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.23": {"line.1": {"text": "\u00bbfrida, mu\u00dft Dich nicht erschrecken,", "tokens": ["\u00bb", "fri\u00b7da", ",", "mu\u00dft", "Dich", "nicht", "er\u00b7schre\u00b7cken", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$,", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sonst kriegt Leinewand Stockflecken.", "tokens": ["Sonst", "kriegt", "Lei\u00b7ne\u00b7wand", "Stock\u00b7fle\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sieht mans Leben zu genau,", "tokens": ["Sieht", "mans", "Le\u00b7ben", "zu", "ge\u00b7nau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PTKA", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wird leicht jeder Faden grau.\u00ab", "tokens": ["Wird", "leicht", "je\u00b7der", "Fa\u00b7den", "grau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADJD", "PIAT", "NN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "\u00bbheinerich, mir wird's so eigen!\u00ab", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "mir", "wird's", "so", "ei\u00b7gen", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbfrida, sp\u00e4ter wird sich's zeigen.\u00ab", "tokens": ["\u00bb", "fri\u00b7da", ",", "sp\u00e4\u00b7ter", "wird", "sich's", "zei\u00b7gen", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM.la", "$,", "ADJD", "VAFIN", "PIS", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbheinerich, mir wird so schw\u00fcl!\u00ab", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "mir", "wird", "so", "schw\u00fcl", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbechte Leinwand, Weib, bleibt k\u00fchl.\u00ab", "tokens": ["\u00bb", "ech\u00b7te", "Lein\u00b7wand", ",", "Weib", ",", "bleibt", "k\u00fchl", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "NN", "$,", "VVFIN", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "\u00bbheinerich, ich f\u00fchl ohn' Ende", "tokens": ["\u00bb", "hei\u00b7ne\u00b7rich", ",", "ich", "f\u00fchl", "ohn'", "En\u00b7de"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "$,", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jene Finger jener H\u00e4nde,", "tokens": ["Je\u00b7ne", "Fin\u00b7ger", "je\u00b7ner", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jenes Blut, das mich sonst w\u00e4rmt,", "tokens": ["Je\u00b7nes", "Blut", ",", "das", "mich", "sonst", "w\u00e4rmt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "PRELS", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Schien mir heute so verh\u00e4rmt.", "tokens": ["Schien", "mir", "heu\u00b7te", "so", "ver\u00b7h\u00e4rmt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Heute stehn wir vor dem Tode!", "tokens": ["Heu\u00b7te", "stehn", "wir", "vor", "dem", "To\u00b7de", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gestern noch in der Kommode \u2013", "tokens": ["Ge\u00b7stern", "noch", "in", "der", "Kom\u00b7mo\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Heinerich, wer h\u00e4tt's gedacht,", "tokens": ["Hei\u00b7ne\u00b7rich", ",", "wer", "h\u00e4tt's", "ge\u00b7dacht", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PWS", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Leichen schm\u00fccken wir zur Nacht.\u00ab", "tokens": ["Lei\u00b7chen", "schm\u00fc\u00b7cken", "wir", "zur", "Nacht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "\u00bbfrida, was treibt Dich zu Schl\u00fcssen,", "tokens": ["\u00bb", "fri\u00b7da", ",", "was", "treibt", "Dich", "zu", "Schl\u00fcs\u00b7sen", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$,", "PWS", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die sich erst beweisen m\u00fcssen?", "tokens": ["Die", "sich", "erst", "be\u00b7wei\u00b7sen", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ADV", "VVINF", "VMINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unser Leben war nie schwer,", "tokens": ["Un\u00b7ser", "Le\u00b7ben", "war", "nie", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Lustig ging's stets in uns her.", "tokens": ["Lus\u00b7tig", "ging's", "stets", "in", "uns", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Denk' nur an die Hochzeitshitzen,", "tokens": ["Denk'", "nur", "an", "die", "Hoch\u00b7zeits\u00b7hit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die in unseren F\u00e4den sitzen!", "tokens": ["Die", "in", "un\u00b7se\u00b7ren", "F\u00e4\u00b7den", "sit\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Dampfend lag Dein Spitzenlatz", "tokens": ["Damp\u00b7fend", "lag", "Dein", "Spit\u00b7zen\u00b7latz"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Oft an meinem Brusteinsatz.", "tokens": ["Oft", "an", "mei\u00b7nem", "Brus\u00b7tein\u00b7satz", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Frida, mach' nicht mehr Grimassen!", "tokens": ["Fri\u00b7da", ",", "mach'", "nicht", "mehr", "Gri\u00b7mas\u00b7sen", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PTKNEG", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Man mu\u00df Dich sonst b\u00fcgeln lassen\u00ab.", "tokens": ["Man", "mu\u00df", "Dich", "sonst", "b\u00fc\u00b7geln", "las\u00b7sen", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VMFIN", "PPER", "ADV", "VVINF", "VVINF", "$(", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbheinz, etwas ins Zimmer tritt,", "tokens": ["\u00bb", "heinz", ",", "et\u00b7was", "ins", "Zim\u00b7mer", "tritt", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Kalt wie einer Schere Schnitt.", "tokens": ["Kalt", "wie", "ei\u00b7ner", "Sche\u00b7re", "Schnitt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.30": {"line.1": {"text": "Eiskalt werden meine Spitzen.", "tokens": ["Eis\u00b7kalt", "wer\u00b7den", "mei\u00b7ne", "Spit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Siehst Du es am Bett nicht sitzen?", "tokens": ["Siehst", "Du", "es", "am", "Bett", "nicht", "sit\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "APPRART", "NN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ach, ich sehe es genau,", "tokens": ["Ach", ",", "ich", "se\u00b7he", "es", "ge\u00b7nau", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "'s ist der Tod mit seiner Frau.\u00ab", "tokens": ["'s", "ist", "der", "Tod", "mit", "sei\u00b7ner", "Frau", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Beide Hemden, unter Zittern,", "tokens": ["Bei\u00b7de", "Hem\u00b7den", ",", "un\u00b7ter", "Zit\u00b7tern", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bei dem Bett den Tod kalt wittern.", "tokens": ["Bei", "dem", "Bett", "den", "Tod", "kalt", "wit\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und es krachte jede Naht,", "tokens": ["Und", "es", "krach\u00b7te", "je\u00b7de", "Naht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Weil der Tod sie sch\u00fctteln tat.", "tokens": ["Weil", "der", "Tod", "sie", "sch\u00fct\u00b7teln", "tat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "Horch! Ein Poltern auf den Treppen.", "tokens": ["Horch", "!", "Ein", "Pol\u00b7tern", "auf", "den", "Trep\u00b7pen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "M\u00e4nner jetzt zwei Leichen schleppen.", "tokens": ["M\u00e4n\u00b7ner", "jetzt", "zwei", "Lei\u00b7chen", "schlep\u00b7pen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "CARD", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Frida kannte sie sogleich,", "tokens": ["Fri\u00b7da", "kann\u00b7te", "sie", "sog\u00b7leich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und auch Heinerich wird bleich.", "tokens": ["Und", "auch", "Hei\u00b7ne\u00b7rich", "wird", "bleich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NE", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Denn der guten Hemden Leute,", "tokens": ["Denn", "der", "gu\u00b7ten", "Hem\u00b7den", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gar zu mies ging's ihnen heute.", "tokens": ["Gar", "zu", "mies", "ging's", "ih\u00b7nen", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKA", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Elend sucht den Tod, weil's mu\u00df \u2013", "tokens": ["E\u00b7lend", "sucht", "den", "Tod", ",", "weil's", "mu\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "$,", "PIS", "VMFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Tot zog man sie aus dem Flu\u00df.", "tokens": ["Tot", "zog", "man", "sie", "aus", "dem", "Flu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.34": {"line.1": {"text": "In derselben Nacht im Zimmer", "tokens": ["In", "der\u00b7sel\u00b7ben", "Nacht", "im", "Zim\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lagen Zwei im Bett wie immer;", "tokens": ["La\u00b7gen", "Zwei", "im", "Bett", "wie", "im\u00b7mer", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "CARD", "APPRART", "NN", "KOKOM", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eins im Tode, au\u00dfen fremd,", "tokens": ["Eins", "im", "To\u00b7de", ",", "au\u00b7\u00dfen", "fremd", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Schm\u00fcckte sie ihr Hochzeitshemd.", "tokens": ["Schm\u00fcck\u00b7te", "sie", "ihr", "Hoch\u00b7zeits\u00b7hemd", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}