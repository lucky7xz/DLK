{"textgrid.poem.24163": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "1L: Um einen gro\u00dfen Tisch", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Um einen gro\u00dfen Tisch", "tokens": ["Um", "ei\u00b7nen", "gro\u00b7\u00dfen", "Tisch"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUI", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sind wir herumgesessen", "tokens": ["Sind", "wir", "her\u00b7um\u00b7ge\u00b7ses\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und haben ausgezeichnet", "tokens": ["Und", "ha\u00b7ben", "aus\u00b7ge\u00b7zeich\u00b7net"], "token_info": ["word", "word", "word"], "pos": ["KON", "VAFIN", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Getrunken und gegessen;", "tokens": ["Ge\u00b7trun\u00b7ken", "und", "ge\u00b7ges\u00b7sen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Geistreiche Leute waren auch dabei.", "tokens": ["Geist\u00b7rei\u00b7che", "Leu\u00b7te", "wa\u00b7ren", "auch", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "ADV", "PAV", "$."], "meter": "++-+-+-+-+", "measure": "iambic.penta.spondeus"}, "line.6": {"text": "Wei\u00df Gott, da konnte man merken,", "tokens": ["Wei\u00df", "Gott", ",", "da", "konn\u00b7te", "man", "mer\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "KOUS", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Was Witz, und Bosheit sei.", "tokens": ["Was", "Witz", ",", "und", "Bos\u00b7heit", "sei", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$,", "KON", "NN", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Zu Suppe, Braten, Fisch, Kompot,", "tokens": ["Zu", "Sup\u00b7pe", ",", "Bra\u00b7ten", ",", "Fisch", ",", "Kom\u00b7pot", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Salat und s\u00fc\u00dfer Speise", "tokens": ["Sa\u00b7lat", "und", "s\u00fc\u00b7\u00dfer", "Spei\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.10": {"text": "Maultrommelte Kritik und Spott,", "tokens": ["Mault\u00b7rom\u00b7mel\u00b7te", "Kri\u00b7tik", "und", "Spott", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$,"], "meter": "----+--+", "measure": "iambic.di.chol"}, "line.11": {"text": "Es reimte Teufel sich auf Gott", "tokens": ["Es", "reim\u00b7te", "Teu\u00b7fel", "sich", "auf", "Gott"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "PRF", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "In dieser muntern Weise.", "tokens": ["In", "die\u00b7ser", "mun\u00b7tern", "Wei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Von der Suppe bis zum Schnapse", "tokens": ["Von", "der", "Sup\u00b7pe", "bis", "zum", "Schnap\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sa\u00df ich sprachlos da,", "tokens": ["Sa\u00df", "ich", "sprach\u00b7los", "da", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wie getroffen vom Collapse,", "tokens": ["Wie", "ge\u00b7trof\u00b7fen", "vom", "Col\u00b7lap\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "APPRART", "NN", "$,"], "meter": "+-+-+++-", "measure": "unknown.measure.penta"}, "line.4": {"text": "Wu\u00dfte nicht, wie mir geschah.", "tokens": ["Wu\u00df\u00b7te", "nicht", ",", "wie", "mir", "ge\u00b7schah", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Tournedos, Kaviar, Lampreten,", "tokens": ["Tour\u00b7ne\u00b7dos", ",", "Ka\u00b7vi\u00b7ar", ",", "Lam\u00b7pre\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Rindfleisch \u00e0 la Bordelaise,", "tokens": ["Rind\u00b7fleisch", "\u00e0", "la", "Bor\u00b7de\u00b7laise", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "FM", "FM", "FM", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Stilton-, Schweizer-, Chesterk\u00e4s,", "tokens": ["Stil\u00b7ton", ",", "Schwei\u00b7zer", ",", "Ches\u00b7ter\u00b7k\u00e4s", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["TRUNC", "$,", "TRUNC", "$,", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Und dazwischen immer Reden!:", "tokens": ["Und", "da\u00b7zwi\u00b7schen", "im\u00b7mer", "Re\u00b7den", "!", ":"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PAV", "ADV", "NN", "$.", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.3": {"line.1": {"text": "Bismarck, Harden, Stinde, Goethe,", "tokens": ["Bis\u00b7marck", ",", "Har\u00b7den", ",", "Stin\u00b7de", ",", "Goe\u00b7the", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wagner, Bungert, Dahn, Homer,", "tokens": ["Wag\u00b7ner", ",", "Bun\u00b7gert", ",", "Dahn", ",", "Ho\u00b7mer", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Fledermaus und Zauberfl\u00f6te,", "tokens": ["Fle\u00b7der\u00b7maus", "und", "Zau\u00b7ber\u00b7fl\u00f6\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ludolf Waldmann, Meyerbeer;", "tokens": ["Lu\u00b7dolf", "Wald\u00b7mann", ",", "Me\u00b7yer\u00b7beer", ";"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.5": {"text": "China, Japan, B\u00f6cklin, Thumann,", "tokens": ["Chi\u00b7na", ",", "Ja\u00b7pan", ",", "B\u00f6c\u00b7klin", ",", "Thu\u00b7mann", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NE", "$,", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Thoma, Werner, Stuck und Knaus,", "tokens": ["Tho\u00b7ma", ",", "Wer\u00b7ner", ",", "Stuck", "und", "Knaus", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Johann, Eduard, Richard Strau\u00df,", "tokens": ["Jo\u00b7hann", ",", "E\u00b7duard", ",", "Ric\u00b7hard", "Strau\u00df", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NE", "NE", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Kaiser Wilhelm, Robert Schumann ...", "tokens": ["Kai\u00b7ser", "Wil\u00b7helm", ",", "Ro\u00b7bert", "Schu\u00b7mann", "..."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.9": {"text": "Mahlzeit! Mahlzeit!! La\u00dfts mi aus!!!", "tokens": ["Mahl\u00b7zeit", "!", "Mahl\u00b7zeit", "!!", "La\u00dfts", "mi", "aus", "!!!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "NE", "NE", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Um einen gro\u00dfen Tisch", "tokens": ["Um", "ei\u00b7nen", "gro\u00b7\u00dfen", "Tisch"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUI", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sind wir herumgesessen", "tokens": ["Sind", "wir", "her\u00b7um\u00b7ge\u00b7ses\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und haben ausgezeichnet", "tokens": ["Und", "ha\u00b7ben", "aus\u00b7ge\u00b7zeich\u00b7net"], "token_info": ["word", "word", "word"], "pos": ["KON", "VAFIN", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Getrunken und gegessen;", "tokens": ["Ge\u00b7trun\u00b7ken", "und", "ge\u00b7ges\u00b7sen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Geistreiche Leute waren auch dabei.", "tokens": ["Geist\u00b7rei\u00b7che", "Leu\u00b7te", "wa\u00b7ren", "auch", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "ADV", "PAV", "$."], "meter": "++-+-+-+-+", "measure": "iambic.penta.spondeus"}, "line.6": {"text": "Wei\u00df Gott, da konnte man merken,", "tokens": ["Wei\u00df", "Gott", ",", "da", "konn\u00b7te", "man", "mer\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "KOUS", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Was Witz, und Bosheit sei.", "tokens": ["Was", "Witz", ",", "und", "Bos\u00b7heit", "sei", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$,", "KON", "NN", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Zu Suppe, Braten, Fisch, Kompot,", "tokens": ["Zu", "Sup\u00b7pe", ",", "Bra\u00b7ten", ",", "Fisch", ",", "Kom\u00b7pot", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Salat und s\u00fc\u00dfer Speise", "tokens": ["Sa\u00b7lat", "und", "s\u00fc\u00b7\u00dfer", "Spei\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "KON", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.10": {"text": "Maultrommelte Kritik und Spott,", "tokens": ["Mault\u00b7rom\u00b7mel\u00b7te", "Kri\u00b7tik", "und", "Spott", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "NN", "$,"], "meter": "----+--+", "measure": "iambic.di.chol"}, "line.11": {"text": "Es reimte Teufel sich auf Gott", "tokens": ["Es", "reim\u00b7te", "Teu\u00b7fel", "sich", "auf", "Gott"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "PRF", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "In dieser muntern Weise.", "tokens": ["In", "die\u00b7ser", "mun\u00b7tern", "Wei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Von der Suppe bis zum Schnapse", "tokens": ["Von", "der", "Sup\u00b7pe", "bis", "zum", "Schnap\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sa\u00df ich sprachlos da,", "tokens": ["Sa\u00df", "ich", "sprach\u00b7los", "da", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wie getroffen vom Collapse,", "tokens": ["Wie", "ge\u00b7trof\u00b7fen", "vom", "Col\u00b7lap\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "APPRART", "NN", "$,"], "meter": "+-+-+++-", "measure": "unknown.measure.penta"}, "line.4": {"text": "Wu\u00dfte nicht, wie mir geschah.", "tokens": ["Wu\u00df\u00b7te", "nicht", ",", "wie", "mir", "ge\u00b7schah", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Tournedos, Kaviar, Lampreten,", "tokens": ["Tour\u00b7ne\u00b7dos", ",", "Ka\u00b7vi\u00b7ar", ",", "Lam\u00b7pre\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Rindfleisch \u00e0 la Bordelaise,", "tokens": ["Rind\u00b7fleisch", "\u00e0", "la", "Bor\u00b7de\u00b7laise", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "FM", "FM", "FM", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Stilton-, Schweizer-, Chesterk\u00e4s,", "tokens": ["Stil\u00b7ton", ",", "Schwei\u00b7zer", ",", "Ches\u00b7ter\u00b7k\u00e4s", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["TRUNC", "$,", "TRUNC", "$,", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Und dazwischen immer Reden!:", "tokens": ["Und", "da\u00b7zwi\u00b7schen", "im\u00b7mer", "Re\u00b7den", "!", ":"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PAV", "ADV", "NN", "$.", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.6": {"line.1": {"text": "Bismarck, Harden, Stinde, Goethe,", "tokens": ["Bis\u00b7marck", ",", "Har\u00b7den", ",", "Stin\u00b7de", ",", "Goe\u00b7the", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wagner, Bungert, Dahn, Homer,", "tokens": ["Wag\u00b7ner", ",", "Bun\u00b7gert", ",", "Dahn", ",", "Ho\u00b7mer", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "$,", "NE", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Fledermaus und Zauberfl\u00f6te,", "tokens": ["Fle\u00b7der\u00b7maus", "und", "Zau\u00b7ber\u00b7fl\u00f6\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ludolf Waldmann, Meyerbeer;", "tokens": ["Lu\u00b7dolf", "Wald\u00b7mann", ",", "Me\u00b7yer\u00b7beer", ";"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.5": {"text": "China, Japan, B\u00f6cklin, Thumann,", "tokens": ["Chi\u00b7na", ",", "Ja\u00b7pan", ",", "B\u00f6c\u00b7klin", ",", "Thu\u00b7mann", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NE", "$,", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Thoma, Werner, Stuck und Knaus,", "tokens": ["Tho\u00b7ma", ",", "Wer\u00b7ner", ",", "Stuck", "und", "Knaus", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Johann, Eduard, Richard Strau\u00df,", "tokens": ["Jo\u00b7hann", ",", "E\u00b7duard", ",", "Ric\u00b7hard", "Strau\u00df", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "NE", "NE", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Kaiser Wilhelm, Robert Schumann ...", "tokens": ["Kai\u00b7ser", "Wil\u00b7helm", ",", "Ro\u00b7bert", "Schu\u00b7mann", "..."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NE", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.9": {"text": "Mahlzeit! Mahlzeit!! La\u00dfts mi aus!!!", "tokens": ["Mahl\u00b7zeit", "!", "Mahl\u00b7zeit", "!!", "La\u00dfts", "mi", "aus", "!!!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "NE", "NE", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}