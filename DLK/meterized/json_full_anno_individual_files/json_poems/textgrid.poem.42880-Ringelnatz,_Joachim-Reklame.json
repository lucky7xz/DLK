{"textgrid.poem.42880": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Reklame", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich wollte von gar nichts wissen.", "tokens": ["Ich", "woll\u00b7te", "von", "gar", "nichts", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ADV", "PIS", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da habe ich eine Reklame erblickt,", "tokens": ["Da", "ha\u00b7be", "ich", "ei\u00b7ne", "Re\u00b7kla\u00b7me", "er\u00b7blickt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Die hat mich in die Augen gezwickt", "tokens": ["Die", "hat", "mich", "in", "die", "Au\u00b7gen", "ge\u00b7zwickt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und ins Ged\u00e4chtnis gebissen.", "tokens": ["Und", "ins", "Ge\u00b7d\u00e4cht\u00b7nis", "ge\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVPP", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Sie predigte mir von fr\u00fch bis sp\u00e4t", "tokens": ["Sie", "pre\u00b7dig\u00b7te", "mir", "von", "fr\u00fch", "bis", "sp\u00e4t"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ADJD", "APPR", "ADJD"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Laut \u00f6ffentlich wie im stillen", "tokens": ["Laut", "\u00f6f\u00b7fent\u00b7lich", "wie", "im", "stil\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJD", "KOKOM", "APPRART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der vorz\u00fcglichen Qualit\u00e4t", "tokens": ["Von", "der", "vor\u00b7z\u00fcg\u00b7li\u00b7chen", "Qua\u00b7li\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Gewisser Bettn\u00e4sser-Pillen.", "tokens": ["Ge\u00b7wis\u00b7ser", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7len", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Ich sagte: \u00bbMag sein! Doch f\u00fcr mich nicht! Nein, nein!", "tokens": ["Ich", "sag\u00b7te", ":", "\u00bb", "Mag", "sein", "!", "Doch", "f\u00fcr", "mich", "nicht", "!", "Nein", ",", "nein", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VMFIN", "VAINF", "$.", "KON", "APPR", "PPER", "PTKNEG", "$.", "PTKANT", "$,", "PTKANT", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Mein Bett und mein Gewissen sind rein!\u00ab", "tokens": ["Mein", "Bett", "und", "mein", "Ge\u00b7wis\u00b7sen", "sind", "rein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "KON", "PPOSAT", "NN", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "Doch sie lief weiter hinter mir her.", "tokens": ["Doch", "sie", "lief", "wei\u00b7ter", "hin\u00b7ter", "mir", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sie folgte mir bis an die Brille.", "tokens": ["Sie", "folg\u00b7te", "mir", "bis", "an", "die", "Bril\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie kam mir aus jedem Journal in die Quer", "tokens": ["Sie", "kam", "mir", "aus", "je\u00b7dem", "Jour\u00b7nal", "in", "die", "Quer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PIAT", "NN", "APPR", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und s\u00e4uselte: \u00bbBettn\u00e4sser-Pille.\u00ab", "tokens": ["Und", "s\u00e4u\u00b7sel\u00b7te", ":", "\u00bb", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7le", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Sie war bald rosa, bald lieblich gr\u00fcn.", "tokens": ["Sie", "war", "bald", "ro\u00b7sa", ",", "bald", "lieb\u00b7lich", "gr\u00fcn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sie sprach in Reimen von Dichtem.", "tokens": ["Sie", "sprach", "in", "Rei\u00b7men", "von", "Dich\u00b7tem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie fuhr in der Trambahn und kletterte k\u00fchn", "tokens": ["Sie", "fuhr", "in", "der", "Tram\u00b7bahn", "und", "klet\u00b7ter\u00b7te", "k\u00fchn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "VVFIN", "ADJD"], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Nachts auf die D\u00e4cher mit Lichtern.", "tokens": ["Nachts", "auf", "die", "D\u00e4\u00b7cher", "mit", "Lich\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.6": {"line.1": {"text": "Und weil sie so z\u00e4he und k\u00fcnstlerisch", "tokens": ["Und", "weil", "sie", "so", "z\u00e4\u00b7he", "und", "k\u00fcnst\u00b7le\u00b7risch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "VVFIN", "KON", "ADJD"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Blieb, war ich ihr endlich zu Willen.", "tokens": ["Blieb", ",", "war", "ich", "ihr", "end\u00b7lich", "zu", "Wil\u00b7len", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VAFIN", "PPER", "PPER", "ADV", "APPR", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Es liegen auf meinem Fr\u00fchst\u00fcckstisch", "tokens": ["Es", "lie\u00b7gen", "auf", "mei\u00b7nem", "Fr\u00fch\u00b7st\u00fccks\u00b7tisch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nun t\u00e4glich zwei Bettn\u00e4sser-Pillen.", "tokens": ["Nun", "t\u00e4g\u00b7lich", "zwei", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "CARD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Die i\u00dft meine Frau als \u00bbEntfettungsbonbon\u00ab.", "tokens": ["Die", "i\u00dft", "mei\u00b7ne", "Frau", "als", "\u00bb", "Ent\u00b7fet\u00b7tungs\u00b7bon\u00b7bon", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "KOUS", "$(", "NN", "$(", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ich habe die Frau belogen.", "tokens": ["Ich", "ha\u00b7be", "die", "Frau", "be\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ein holder Frieden ist in den Salon", "tokens": ["Ein", "hol\u00b7der", "Frie\u00b7den", "ist", "in", "den", "Sa\u00b7lon"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+++-+", "measure": "zehnsilber"}, "line.4": {"text": "Meiner Seele eingezogen.", "tokens": ["Mei\u00b7ner", "See\u00b7le", "ein\u00b7ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Ich wollte von gar nichts wissen.", "tokens": ["Ich", "woll\u00b7te", "von", "gar", "nichts", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ADV", "PIS", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da habe ich eine Reklame erblickt,", "tokens": ["Da", "ha\u00b7be", "ich", "ei\u00b7ne", "Re\u00b7kla\u00b7me", "er\u00b7blickt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Die hat mich in die Augen gezwickt", "tokens": ["Die", "hat", "mich", "in", "die", "Au\u00b7gen", "ge\u00b7zwickt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und ins Ged\u00e4chtnis gebissen.", "tokens": ["Und", "ins", "Ge\u00b7d\u00e4cht\u00b7nis", "ge\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVPP", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.9": {"line.1": {"text": "Sie predigte mir von fr\u00fch bis sp\u00e4t", "tokens": ["Sie", "pre\u00b7dig\u00b7te", "mir", "von", "fr\u00fch", "bis", "sp\u00e4t"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ADJD", "APPR", "ADJD"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Laut \u00f6ffentlich wie im stillen", "tokens": ["Laut", "\u00f6f\u00b7fent\u00b7lich", "wie", "im", "stil\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJD", "KOKOM", "APPRART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der vorz\u00fcglichen Qualit\u00e4t", "tokens": ["Von", "der", "vor\u00b7z\u00fcg\u00b7li\u00b7chen", "Qua\u00b7li\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Gewisser Bettn\u00e4sser-Pillen.", "tokens": ["Ge\u00b7wis\u00b7ser", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7len", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.10": {"line.1": {"text": "Ich sagte: \u00bbMag sein! Doch f\u00fcr mich nicht! Nein, nein!", "tokens": ["Ich", "sag\u00b7te", ":", "\u00bb", "Mag", "sein", "!", "Doch", "f\u00fcr", "mich", "nicht", "!", "Nein", ",", "nein", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VMFIN", "VAINF", "$.", "KON", "APPR", "PPER", "PTKNEG", "$.", "PTKANT", "$,", "PTKANT", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Mein Bett und mein Gewissen sind rein!\u00ab", "tokens": ["Mein", "Bett", "und", "mein", "Ge\u00b7wis\u00b7sen", "sind", "rein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "KON", "PPOSAT", "NN", "VAFIN", "ADJD", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "Doch sie lief weiter hinter mir her.", "tokens": ["Doch", "sie", "lief", "wei\u00b7ter", "hin\u00b7ter", "mir", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sie folgte mir bis an die Brille.", "tokens": ["Sie", "folg\u00b7te", "mir", "bis", "an", "die", "Bril\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie kam mir aus jedem Journal in die Quer", "tokens": ["Sie", "kam", "mir", "aus", "je\u00b7dem", "Jour\u00b7nal", "in", "die", "Quer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "PIAT", "NN", "APPR", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und s\u00e4uselte: \u00bbBettn\u00e4sser-Pille.\u00ab", "tokens": ["Und", "s\u00e4u\u00b7sel\u00b7te", ":", "\u00bb", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7le", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.12": {"line.1": {"text": "Sie war bald rosa, bald lieblich gr\u00fcn.", "tokens": ["Sie", "war", "bald", "ro\u00b7sa", ",", "bald", "lieb\u00b7lich", "gr\u00fcn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sie sprach in Reimen von Dichtem.", "tokens": ["Sie", "sprach", "in", "Rei\u00b7men", "von", "Dich\u00b7tem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie fuhr in der Trambahn und kletterte k\u00fchn", "tokens": ["Sie", "fuhr", "in", "der", "Tram\u00b7bahn", "und", "klet\u00b7ter\u00b7te", "k\u00fchn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "VVFIN", "ADJD"], "meter": "-+--++-+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Nachts auf die D\u00e4cher mit Lichtern.", "tokens": ["Nachts", "auf", "die", "D\u00e4\u00b7cher", "mit", "Lich\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.13": {"line.1": {"text": "Und weil sie so z\u00e4he und k\u00fcnstlerisch", "tokens": ["Und", "weil", "sie", "so", "z\u00e4\u00b7he", "und", "k\u00fcnst\u00b7le\u00b7risch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "VVFIN", "KON", "ADJD"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Blieb, war ich ihr endlich zu Willen.", "tokens": ["Blieb", ",", "war", "ich", "ihr", "end\u00b7lich", "zu", "Wil\u00b7len", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VAFIN", "PPER", "PPER", "ADV", "APPR", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Es liegen auf meinem Fr\u00fchst\u00fcckstisch", "tokens": ["Es", "lie\u00b7gen", "auf", "mei\u00b7nem", "Fr\u00fch\u00b7st\u00fccks\u00b7tisch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nun t\u00e4glich zwei Bettn\u00e4sser-Pillen.", "tokens": ["Nun", "t\u00e4g\u00b7lich", "zwei", "Bett\u00b7n\u00e4s\u00b7ser\u00b7Pil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "CARD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Die i\u00dft meine Frau als \u00bbEntfettungsbonbon\u00ab.", "tokens": ["Die", "i\u00dft", "mei\u00b7ne", "Frau", "als", "\u00bb", "Ent\u00b7fet\u00b7tungs\u00b7bon\u00b7bon", "\u00ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "KOUS", "$(", "NN", "$(", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ich habe die Frau belogen.", "tokens": ["Ich", "ha\u00b7be", "die", "Frau", "be\u00b7lo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ein holder Frieden ist in den Salon", "tokens": ["Ein", "hol\u00b7der", "Frie\u00b7den", "ist", "in", "den", "Sa\u00b7lon"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+++-+", "measure": "zehnsilber"}, "line.4": {"text": "Meiner Seele eingezogen.", "tokens": ["Mei\u00b7ner", "See\u00b7le", "ein\u00b7ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}