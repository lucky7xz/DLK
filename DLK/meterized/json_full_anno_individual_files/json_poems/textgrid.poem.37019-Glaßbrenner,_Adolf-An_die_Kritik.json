{"textgrid.poem.37019": {"metadata": {"author": {"name": "Gla\u00dfbrenner, Adolf", "birth": "N.A.", "death": "N.A."}, "title": "An die Kritik", "genre": "verse", "period": "N.A.", "pub_year": 1843, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wohl flammte im Herzen mir fort und fort", "tokens": ["Wohl", "flamm\u00b7te", "im", "Her\u00b7zen", "mir", "fort", "und", "fort"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "PPER", "PTKVZ", "KON", "PTKVZ"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "F\u00fcr ", "tokens": ["F\u00fcr"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Doch folgt' ich als einstiger Deutscher trotzdem", "tokens": ["Doch", "folgt'", "ich", "als", "eins\u00b7ti\u00b7ger", "Deut\u00b7scher", "trotz\u00b7dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "KOUS", "ADJA", "NN", "PAV"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Meinem Kenntni\u00df verlangenden Triebe.", "tokens": ["Mei\u00b7nem", "Kennt\u00b7ni\u00df", "ver\u00b7lan\u00b7gen\u00b7den", "Trie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJA", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Und ich sahe und h\u00f6rte und forschte und nahm", "tokens": ["Und", "ich", "sa\u00b7he", "und", "h\u00f6r\u00b7te", "und", "forschte", "und", "nahm"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "KON", "VVFIN"], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Notiz mir von Jenem und Diesem,", "tokens": ["No\u00b7tiz", "mir", "von", "Je\u00b7nem", "und", "Die\u00b7sem", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PDAT", "KON", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und theil' es hier mit in gef\u00e4lligem Styl", "tokens": ["Und", "theil'", "es", "hier", "mit", "in", "ge\u00b7f\u00e4l\u00b7li\u00b7gem", "Styl"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Und in m\u00f6glichst con- und pr\u00e4cisem.", "tokens": ["Und", "in", "m\u00f6g\u00b7lichst", "con", "und", "pr\u00e4\u00b7ci\u00b7sem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "TRUNC", "KON", "NE", "$."], "meter": "--+-+-+--", "measure": "anapaest.init"}}, "stanza.3": {"line.1": {"text": "Und wie Du bei Manchem auch sch\u00fctteln magst", "tokens": ["Und", "wie", "Du", "bei", "Man\u00b7chem", "auch", "sch\u00fct\u00b7teln", "magst"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "PPER", "APPR", "PIS", "ADV", "VVINF", "VMFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Dein sceptisches Haupt, Recensente,", "tokens": ["Dein", "scep\u00b7ti\u00b7sches", "Haupt", ",", "Re\u00b7cen\u00b7sen\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mein k\u00f6niglich Wort darauf: Alles ist ", "tokens": ["Mein", "k\u00f6\u00b7nig\u00b7lich", "Wort", "da\u00b7rauf", ":", "Al\u00b7les", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJD", "NN", "PAV", "$.", "PIS", "VAFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Nicht das Kleinste ist Puff oder Ente.", "tokens": ["Nicht", "das", "Kleins\u00b7te", "ist", "Puff", "o\u00b7der", "En\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "VAFIN", "NN", "KON", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.4": {"line.1": {"text": "Und stie\u00dfe der pureste Nonsens Dir auf,", "tokens": ["Und", "stie\u00b7\u00dfe", "der", "pu\u00b7res\u00b7te", "Non\u00b7sens", "Dir", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Du ", "tokens": ["Du"], "token_info": ["word"], "pos": ["PPER"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Weder Logik vorhanden noch sonst was.", "tokens": ["We\u00b7der", "Lo\u00b7gik", "vor\u00b7han\u00b7den", "noch", "sonst", "was", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJD", "ADV", "ADV", "PIS", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Da\u00df Alles und Alles sich hier widerspricht,", "tokens": ["Da\u00df", "Al\u00b7les", "und", "Al\u00b7les", "sich", "hier", "wi\u00b7der\u00b7spricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "KON", "PIS", "PRF", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und drum Spott auch und Hohn unvermeidbar;", "tokens": ["Und", "drum", "Spott", "auch", "und", "Hohn", "un\u00b7ver\u00b7meid\u00b7bar", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "NN", "ADV", "KON", "NN", "ADJD", "$."], "meter": "--+--++-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Da\u00df die h\u00f6cheste, tiefeste Philosophie", "tokens": ["Da\u00df", "die", "h\u00f6\u00b7ches\u00b7te", ",", "tie\u00b7fes\u00b7te", "Phi\u00b7lo\u00b7so\u00b7phie"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "$,", "ADJA", "NN"], "meter": "+-+--+--++-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Von dem Bl\u00f6dsinne kaum unterscheidbar.", "tokens": ["Von", "dem", "Bl\u00f6d\u00b7sin\u00b7ne", "kaum", "un\u00b7ter\u00b7scheid\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "ADJD", "$."], "meter": "+-++--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Und da\u00df just wenn man staunt ob des g\u00f6ttichen Geist's,", "tokens": ["Und", "da\u00df", "just", "wenn", "man", "staunt", "ob", "des", "g\u00f6t\u00b7ti\u00b7chen", "Geist's", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "KOUS", "PIS", "VVFIN", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.2": {"text": "Der der fernesten Sterne Gewicht w\u00e4gt,", "tokens": ["Der", "der", "fer\u00b7nes\u00b7ten", "Ster\u00b7ne", "Ge\u00b7wicht", "w\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "NN", "VVFIN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Uns die aufgeblasenste Fach-Unvernunft", "tokens": ["Uns", "die", "auf\u00b7ge\u00b7bla\u00b7sens\u00b7te", "Fach\u00b7Un\u00b7ver\u00b7nunft"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "In demselben Moment in's Gesicht schl\u00e4gt!", "tokens": ["In", "dem\u00b7sel\u00b7ben", "Mo\u00b7ment", "in's", "Ge\u00b7sicht", "schl\u00e4gt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.7": {"line.1": {"text": "O, bedenke Das g\u00fctigst, o, Rezensent", "tokens": ["O", ",", "be\u00b7den\u00b7ke", "Das", "g\u00fc\u00b7tigst", ",", "o", ",", "Re\u00b7zen\u00b7sent"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "VVFIN", "ART", "NN", "$,", "FM", "$,", "NN"], "meter": "+-+--++-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Und des Doctor-Diplomes Besitzer,", "tokens": ["Und", "des", "Doc\u00b7tor\u00b7Di\u00b7plo\u00b7mes", "Be\u00b7sit\u00b7zer", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Und beweise nicht weise, mein Buch sei nur Lug", "tokens": ["Und", "be\u00b7wei\u00b7se", "nicht", "wei\u00b7se", ",", "mein", "Buch", "sei", "nur", "Lug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PTKNEG", "VVFIN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "NE"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Und voll gr\u00e4\u00dflicher logischer Schnitzer.", "tokens": ["Und", "voll", "gr\u00e4\u00df\u00b7li\u00b7cher", "lo\u00b7gi\u00b7scher", "Schnit\u00b7zer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJA", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.8": {"line.1": {"text": "Und thust Du es doch, nun so nennst Du doch nicht", "tokens": ["Und", "thust", "Du", "es", "doch", ",", "nun", "so", "nennst", "Du", "doch", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADV", "$,", "ADV", "ADV", "VVFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Den Inhalt kurzweg: Larifari!", "tokens": ["Den", "In\u00b7halt", "kurz\u00b7weg", ":", "La\u00b7ri\u00b7fa\u00b7ri", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$.", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie Gewisse, die sich auf den Standpunkt stell'n,", "tokens": ["Wie", "Ge\u00b7wis\u00b7se", ",", "die", "sich", "auf", "den", "Stand\u00b7punkt", "stell'n", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "PRELS", "PRF", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Den erhab'nen des ", "tokens": ["Den", "er\u00b7hab'\u00b7nen", "des"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "ART"], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Ach, Nichts nicht schmerzt uns Poeten so sehr,", "tokens": ["Ach", ",", "Nichts", "nicht", "schmerzt", "uns", "Po\u00b7et\u00b7en", "so", "sehr", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PIS", "PTKNEG", "VVFIN", "PPER", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Als wenn mit 'nem Werke wir Wunder", "tokens": ["Als", "wenn", "mit", "'nem", "Wer\u00b7ke", "wir", "Wun\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "APPR", "ART", "NN", "PPER", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was zu bringen geglaubt und dann hinterher ein", "tokens": ["Was", "zu", "brin\u00b7gen", "ge\u00b7glaubt", "und", "dann", "hin\u00b7ter\u00b7her", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PTKZU", "VVINF", "VVPP", "KON", "ADV", "ADV", "ART"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Kritik\u00fcschen es wegwirft wie Plunder.", "tokens": ["Kri\u00b7ti\u00b7k\u00fc\u00b7schen", "es", "weg\u00b7wirft", "wie", "Plun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVFIN", "KOKOM", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "Denn, ach! in die Thr\u00e4ne, geweint um uns selbst,", "tokens": ["Denn", ",", "ach", "!", "in", "die", "Thr\u00e4\u00b7ne", ",", "ge\u00b7weint", "um", "uns", "selbst", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ITJ", "$.", "APPR", "ART", "NN", "$,", "VVPP", "APPR", "PPER", "ADV", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mischt sich auch noch die um den Richter,", "tokens": ["Mischt", "sich", "auch", "noch", "die", "um", "den", "Rich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "ART", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Der (vom Geist abgesehn) f\u00fcnf Minuten verlor", "tokens": ["Der", "(", "vom", "Geist", "ab\u00b7ge\u00b7sehn", ")", "f\u00fcnf", "Mi\u00b7nu\u00b7ten", "ver\u00b7lor"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "$(", "APPRART", "NN", "VVPP", "$(", "CARD", "NN", "VVFIN"], "meter": "+-++-+--+--+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Um zu werden des Werkes Vernichter!", "tokens": ["Um", "zu", "wer\u00b7den", "des", "Wer\u00b7kes", "Ver\u00b7nich\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VAINF", "ART", "NN", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Doch was schwatz' von Kritik ich und werde mit ihr", "tokens": ["Doch", "was", "schwatz'", "von", "Kri\u00b7tik", "ich", "und", "wer\u00b7de", "mit", "ihr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VVFIN", "APPR", "NN", "PPER", "KON", "VAFIN", "APPR", "PPOSAT"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Fast wie'n Kom\u00f6diant so krakeelig!", "tokens": ["Fast", "wie'n", "Ko\u00b7m\u00f6\u00b7di\u00b7ant", "so", "kra\u00b7kee\u00b7lig", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Wenn man unter der Erde ist: ", "tokens": ["Wenn", "man", "un\u00b7ter", "der", "Er\u00b7de", "ist", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.12": {"line.1": {"text": "Was k\u00fcmmert es mich, ob das St\u00e4ubchen Herr X.", "tokens": ["Was", "k\u00fcm\u00b7mert", "es", "mich", ",", "ob", "das", "St\u00e4ub\u00b7chen", "Herr", "X."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "abbreviation"], "pos": ["PWS", "VVFIN", "PPER", "PRF", "$,", "KOUS", "ART", "ADJA", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Auf dem Staubkorne Erde mich tadelt!", "tokens": ["Auf", "dem", "Staub\u00b7kor\u00b7ne", "Er\u00b7de", "mich", "ta\u00b7delt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "PPER", "VVFIN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Und de\u00df Psyche schon kosmisch geadelt!", "tokens": ["Und", "de\u00df", "Psy\u00b7che", "schon", "kos\u00b7misch", "ge\u00b7a\u00b7delt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.13": {"line.1": {"text": "Mich, dessen Gef\u00fchl und de\u00df Denken so rein,", "tokens": ["Mich", ",", "des\u00b7sen", "Ge\u00b7f\u00fchl", "und", "de\u00df", "Den\u00b7ken", "so", "rein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELAT", "NN", "KON", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "So ganz absolut und total ist,", "tokens": ["So", "ganz", "ab\u00b7so\u00b7lut", "und", "to\u00b7tal", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "KON", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da\u00df sein Ruhm auf 'nem einzelnen lumpigen Stern", "tokens": ["Da\u00df", "sein", "Ruhm", "auf", "'nem", "ein\u00b7zel\u00b7nen", "lum\u00b7pi\u00b7gen", "Stern"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "ART", "ADJA", "ADJA", "NN"], "meter": "--+-+-+-++-+", "measure": "anapaest.init"}, "line.4": {"text": "Ihm totaliter schaal und egal ist!", "tokens": ["Ihm", "to\u00b7ta\u00b7li\u00b7ter", "schaal", "und", "e\u00b7gal", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "KON", "ADV", "VAFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.14": {"line.1": {"text": "Nein, nein! und selbst wenn durch dieses Gedicht", "tokens": ["Nein", ",", "nein", "!", "und", "selbst", "wenn", "durch", "die\u00b7ses", "Ge\u00b7dicht"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PTKANT", "$.", "KON", "ADV", "KOUS", "APPR", "PDAT", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sich in Deutschland mein Ruf noch erh\u00f6hte,", "tokens": ["Sich", "in", "Deutschland", "mein", "Ruf", "noch", "er\u00b7h\u00f6h\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NE", "PPOSAT", "NN", "ADV", "VVFIN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und ich w\u00fcrde verherrlicht in \u00e4hnlicher Art", "tokens": ["Und", "ich", "w\u00fcr\u00b7de", "ver\u00b7herr\u00b7licht", "in", "\u00e4hn\u00b7li\u00b7cher", "Art"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "VVPP", "APPR", "ADJA", "NN"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Wie Schiller, Pepita und G\u00f6the:", "tokens": ["Wie", "Schil\u00b7ler", ",", "Pe\u00b7pi\u00b7ta", "und", "G\u00f6\u00b7the", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "$,", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Es bewegte mein Ich doch, mein seliges, kaum", "tokens": ["Es", "be\u00b7weg\u00b7te", "mein", "Ich", "doch", ",", "mein", "se\u00b7li\u00b7ges", ",", "kaum"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "PPER", "ADV", "$,", "PPOSAT", "ADJA", "$,", "ADV"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.2": {"text": "Wie den t\u00fcrkischen Siebenten Himmel", "tokens": ["Wie", "den", "t\u00fcr\u00b7ki\u00b7schen", "Sie\u00b7ben\u00b7ten", "Him\u00b7mel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN", "NN"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Ein zur Anbetung lieblich einladendes", "tokens": ["Ein", "zur", "An\u00b7be\u00b7tung", "lieb\u00b7lich", "ein\u00b7la\u00b7den\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPRART", "NN", "ADJD", "ART"], "meter": "+-+--+--+--", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Christkirchliches Glockengebimmel!", "tokens": ["Christ\u00b7kirch\u00b7li\u00b7ches", "Glo\u00b7cken\u00b7ge\u00b7bim\u00b7mel", "!"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.16": {"line.1": {"text": "Und so theil' ich im n\u00e4chsten Kapitel denn mit \u2013", "tokens": ["Und", "so", "theil'", "ich", "im", "n\u00e4chs\u00b7ten", "Ka\u00b7pi\u00b7tel", "denn", "mit", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPRART", "ADJA", "NN", "ADV", "APPR", "$("], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Da dieses (de\u00df Raum doch begrenzt ist)", "tokens": ["Da", "die\u00b7ses", "(", "de\u00df", "Raum", "doch", "be\u00b7grenzt", "ist", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "$(", "ART", "NN", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Durch unn\u00fctzes Plaudern (wie dr\u00fcck' ich es aus?)", "tokens": ["Durch", "un\u00b7n\u00fct\u00b7zes", "Plau\u00b7dern", "(", "wie", "dr\u00fcck", "ich", "es", "aus", "?", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "$(", "PWAV", "VVFIN", "PPER", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ver-kammert und ver-conferenzt ist \u2013", "tokens": ["Ver\u00b7kam\u00b7mert", "und", "ver\u00b7con\u00b7fe\u00b7renzt", "ist", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.17": {"line.1": {"text": "Um Lob und Kritik unbek\u00fcmmert all Das,", "tokens": ["Um", "Lob", "und", "Kri\u00b7tik", "un\u00b7be\u00b7k\u00fcm\u00b7mert", "all", "Das", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "KON", "NN", "ADJD", "PIAT", "PDS", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Was im Merkbuch bis heute notirt zwar,", "tokens": ["Was", "im", "Merk\u00b7buch", "bis", "heu\u00b7te", "no\u00b7tirt", "zwar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPRART", "NN", "APPR", "ADV", "VVFIN", "ADV", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Doch bishero noch nicht ", "tokens": ["Doch", "bis\u00b7he\u00b7ro", "noch", "nicht"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "PTKNEG"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "(hier in meiner Verkehrten) fixirt war.", "tokens": ["(", "hier", "in", "mei\u00b7ner", "Ver\u00b7kehr\u00b7ten", ")", "fi\u00b7xirt", "war", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "PPOSAT", "NN", "$(", "VVPP", "VAFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "Wohl flammte im Herzen mir fort und fort", "tokens": ["Wohl", "flamm\u00b7te", "im", "Her\u00b7zen", "mir", "fort", "und", "fort"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "PPER", "PTKVZ", "KON", "PTKVZ"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "F\u00fcr ", "tokens": ["F\u00fcr"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Doch folgt' ich als einstiger Deutscher trotzdem", "tokens": ["Doch", "folgt'", "ich", "als", "eins\u00b7ti\u00b7ger", "Deut\u00b7scher", "trotz\u00b7dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "KOUS", "ADJA", "NN", "PAV"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Meinem Kenntni\u00df verlangenden Triebe.", "tokens": ["Mei\u00b7nem", "Kennt\u00b7ni\u00df", "ver\u00b7lan\u00b7gen\u00b7den", "Trie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJA", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.19": {"line.1": {"text": "Und ich sahe und h\u00f6rte und forschte und nahm", "tokens": ["Und", "ich", "sa\u00b7he", "und", "h\u00f6r\u00b7te", "und", "forschte", "und", "nahm"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "KON", "VVFIN", "KON", "VVFIN", "KON", "VVFIN"], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Notiz mir von Jenem und Diesem,", "tokens": ["No\u00b7tiz", "mir", "von", "Je\u00b7nem", "und", "Die\u00b7sem", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PDAT", "KON", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und theil' es hier mit in gef\u00e4lligem Styl", "tokens": ["Und", "theil'", "es", "hier", "mit", "in", "ge\u00b7f\u00e4l\u00b7li\u00b7gem", "Styl"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Und in m\u00f6glichst con- und pr\u00e4cisem.", "tokens": ["Und", "in", "m\u00f6g\u00b7lichst", "con", "und", "pr\u00e4\u00b7ci\u00b7sem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "TRUNC", "KON", "NE", "$."], "meter": "--+-+-+--", "measure": "anapaest.init"}}, "stanza.20": {"line.1": {"text": "Und wie Du bei Manchem auch sch\u00fctteln magst", "tokens": ["Und", "wie", "Du", "bei", "Man\u00b7chem", "auch", "sch\u00fct\u00b7teln", "magst"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "PPER", "APPR", "PIS", "ADV", "VVINF", "VMFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Dein sceptisches Haupt, Recensente,", "tokens": ["Dein", "scep\u00b7ti\u00b7sches", "Haupt", ",", "Re\u00b7cen\u00b7sen\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mein k\u00f6niglich Wort darauf: Alles ist ", "tokens": ["Mein", "k\u00f6\u00b7nig\u00b7lich", "Wort", "da\u00b7rauf", ":", "Al\u00b7les", "ist"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "ADJD", "NN", "PAV", "$.", "PIS", "VAFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Nicht das Kleinste ist Puff oder Ente.", "tokens": ["Nicht", "das", "Kleins\u00b7te", "ist", "Puff", "o\u00b7der", "En\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "VAFIN", "NN", "KON", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.21": {"line.1": {"text": "Und stie\u00dfe der pureste Nonsens Dir auf,", "tokens": ["Und", "stie\u00b7\u00dfe", "der", "pu\u00b7res\u00b7te", "Non\u00b7sens", "Dir", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Du ", "tokens": ["Du"], "token_info": ["word"], "pos": ["PPER"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Weder Logik vorhanden noch sonst was.", "tokens": ["We\u00b7der", "Lo\u00b7gik", "vor\u00b7han\u00b7den", "noch", "sonst", "was", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADJD", "ADV", "ADV", "PIS", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Da\u00df Alles und Alles sich hier widerspricht,", "tokens": ["Da\u00df", "Al\u00b7les", "und", "Al\u00b7les", "sich", "hier", "wi\u00b7der\u00b7spricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "KON", "PIS", "PRF", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und drum Spott auch und Hohn unvermeidbar;", "tokens": ["Und", "drum", "Spott", "auch", "und", "Hohn", "un\u00b7ver\u00b7meid\u00b7bar", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "NN", "ADV", "KON", "NN", "ADJD", "$."], "meter": "--+--++-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Da\u00df die h\u00f6cheste, tiefeste Philosophie", "tokens": ["Da\u00df", "die", "h\u00f6\u00b7ches\u00b7te", ",", "tie\u00b7fes\u00b7te", "Phi\u00b7lo\u00b7so\u00b7phie"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "$,", "ADJA", "NN"], "meter": "+-+--+--++-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Von dem Bl\u00f6dsinne kaum unterscheidbar.", "tokens": ["Von", "dem", "Bl\u00f6d\u00b7sin\u00b7ne", "kaum", "un\u00b7ter\u00b7scheid\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "ADJD", "$."], "meter": "+-++--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.23": {"line.1": {"text": "Und da\u00df just wenn man staunt ob des g\u00f6ttichen Geist's,", "tokens": ["Und", "da\u00df", "just", "wenn", "man", "staunt", "ob", "des", "g\u00f6t\u00b7ti\u00b7chen", "Geist's", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "KOUS", "PIS", "VVFIN", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.2": {"text": "Der der fernesten Sterne Gewicht w\u00e4gt,", "tokens": ["Der", "der", "fer\u00b7nes\u00b7ten", "Ster\u00b7ne", "Ge\u00b7wicht", "w\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "NN", "VVFIN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Uns die aufgeblasenste Fach-Unvernunft", "tokens": ["Uns", "die", "auf\u00b7ge\u00b7bla\u00b7sens\u00b7te", "Fach\u00b7Un\u00b7ver\u00b7nunft"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "In demselben Moment in's Gesicht schl\u00e4gt!", "tokens": ["In", "dem\u00b7sel\u00b7ben", "Mo\u00b7ment", "in's", "Ge\u00b7sicht", "schl\u00e4gt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.24": {"line.1": {"text": "O, bedenke Das g\u00fctigst, o, Rezensent", "tokens": ["O", ",", "be\u00b7den\u00b7ke", "Das", "g\u00fc\u00b7tigst", ",", "o", ",", "Re\u00b7zen\u00b7sent"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "VVFIN", "ART", "NN", "$,", "FM", "$,", "NN"], "meter": "+-+--++-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Und des Doctor-Diplomes Besitzer,", "tokens": ["Und", "des", "Doc\u00b7tor\u00b7Di\u00b7plo\u00b7mes", "Be\u00b7sit\u00b7zer", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Und beweise nicht weise, mein Buch sei nur Lug", "tokens": ["Und", "be\u00b7wei\u00b7se", "nicht", "wei\u00b7se", ",", "mein", "Buch", "sei", "nur", "Lug"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PTKNEG", "VVFIN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "NE"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Und voll gr\u00e4\u00dflicher logischer Schnitzer.", "tokens": ["Und", "voll", "gr\u00e4\u00df\u00b7li\u00b7cher", "lo\u00b7gi\u00b7scher", "Schnit\u00b7zer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJA", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.25": {"line.1": {"text": "Und thust Du es doch, nun so nennst Du doch nicht", "tokens": ["Und", "thust", "Du", "es", "doch", ",", "nun", "so", "nennst", "Du", "doch", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PPER", "ADV", "$,", "ADV", "ADV", "VVFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Den Inhalt kurzweg: Larifari!", "tokens": ["Den", "In\u00b7halt", "kurz\u00b7weg", ":", "La\u00b7ri\u00b7fa\u00b7ri", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$.", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie Gewisse, die sich auf den Standpunkt stell'n,", "tokens": ["Wie", "Ge\u00b7wis\u00b7se", ",", "die", "sich", "auf", "den", "Stand\u00b7punkt", "stell'n", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "PRELS", "PRF", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Den erhab'nen des ", "tokens": ["Den", "er\u00b7hab'\u00b7nen", "des"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "ART"], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.26": {"line.1": {"text": "Ach, Nichts nicht schmerzt uns Poeten so sehr,", "tokens": ["Ach", ",", "Nichts", "nicht", "schmerzt", "uns", "Po\u00b7et\u00b7en", "so", "sehr", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PIS", "PTKNEG", "VVFIN", "PPER", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Als wenn mit 'nem Werke wir Wunder", "tokens": ["Als", "wenn", "mit", "'nem", "Wer\u00b7ke", "wir", "Wun\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "APPR", "ART", "NN", "PPER", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was zu bringen geglaubt und dann hinterher ein", "tokens": ["Was", "zu", "brin\u00b7gen", "ge\u00b7glaubt", "und", "dann", "hin\u00b7ter\u00b7her", "ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PTKZU", "VVINF", "VVPP", "KON", "ADV", "ADV", "ART"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Kritik\u00fcschen es wegwirft wie Plunder.", "tokens": ["Kri\u00b7ti\u00b7k\u00fc\u00b7schen", "es", "weg\u00b7wirft", "wie", "Plun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVFIN", "KOKOM", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.27": {"line.1": {"text": "Denn, ach! in die Thr\u00e4ne, geweint um uns selbst,", "tokens": ["Denn", ",", "ach", "!", "in", "die", "Thr\u00e4\u00b7ne", ",", "ge\u00b7weint", "um", "uns", "selbst", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ITJ", "$.", "APPR", "ART", "NN", "$,", "VVPP", "APPR", "PPER", "ADV", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Mischt sich auch noch die um den Richter,", "tokens": ["Mischt", "sich", "auch", "noch", "die", "um", "den", "Rich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "ART", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Der (vom Geist abgesehn) f\u00fcnf Minuten verlor", "tokens": ["Der", "(", "vom", "Geist", "ab\u00b7ge\u00b7sehn", ")", "f\u00fcnf", "Mi\u00b7nu\u00b7ten", "ver\u00b7lor"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "$(", "APPRART", "NN", "VVPP", "$(", "CARD", "NN", "VVFIN"], "meter": "+-++-+--+--+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Um zu werden des Werkes Vernichter!", "tokens": ["Um", "zu", "wer\u00b7den", "des", "Wer\u00b7kes", "Ver\u00b7nich\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VAINF", "ART", "NN", "NN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.28": {"line.1": {"text": "Doch was schwatz' von Kritik ich und werde mit ihr", "tokens": ["Doch", "was", "schwatz'", "von", "Kri\u00b7tik", "ich", "und", "wer\u00b7de", "mit", "ihr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VVFIN", "APPR", "NN", "PPER", "KON", "VAFIN", "APPR", "PPOSAT"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Fast wie'n Kom\u00f6diant so krakeelig!", "tokens": ["Fast", "wie'n", "Ko\u00b7m\u00f6\u00b7di\u00b7ant", "so", "kra\u00b7kee\u00b7lig", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Wenn man unter der Erde ist: ", "tokens": ["Wenn", "man", "un\u00b7ter", "der", "Er\u00b7de", "ist", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.29": {"line.1": {"text": "Was k\u00fcmmert es mich, ob das St\u00e4ubchen Herr X.", "tokens": ["Was", "k\u00fcm\u00b7mert", "es", "mich", ",", "ob", "das", "St\u00e4ub\u00b7chen", "Herr", "X."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "abbreviation"], "pos": ["PWS", "VVFIN", "PPER", "PRF", "$,", "KOUS", "ART", "ADJA", "NN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Auf dem Staubkorne Erde mich tadelt!", "tokens": ["Auf", "dem", "Staub\u00b7kor\u00b7ne", "Er\u00b7de", "mich", "ta\u00b7delt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "PPER", "VVFIN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Und de\u00df Psyche schon kosmisch geadelt!", "tokens": ["Und", "de\u00df", "Psy\u00b7che", "schon", "kos\u00b7misch", "ge\u00b7a\u00b7delt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.30": {"line.1": {"text": "Mich, dessen Gef\u00fchl und de\u00df Denken so rein,", "tokens": ["Mich", ",", "des\u00b7sen", "Ge\u00b7f\u00fchl", "und", "de\u00df", "Den\u00b7ken", "so", "rein", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELAT", "NN", "KON", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "So ganz absolut und total ist,", "tokens": ["So", "ganz", "ab\u00b7so\u00b7lut", "und", "to\u00b7tal", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "KON", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da\u00df sein Ruhm auf 'nem einzelnen lumpigen Stern", "tokens": ["Da\u00df", "sein", "Ruhm", "auf", "'nem", "ein\u00b7zel\u00b7nen", "lum\u00b7pi\u00b7gen", "Stern"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "ART", "ADJA", "ADJA", "NN"], "meter": "--+-+-+-++-+", "measure": "anapaest.init"}, "line.4": {"text": "Ihm totaliter schaal und egal ist!", "tokens": ["Ihm", "to\u00b7ta\u00b7li\u00b7ter", "schaal", "und", "e\u00b7gal", "ist", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "KON", "ADV", "VAFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.31": {"line.1": {"text": "Nein, nein! und selbst wenn durch dieses Gedicht", "tokens": ["Nein", ",", "nein", "!", "und", "selbst", "wenn", "durch", "die\u00b7ses", "Ge\u00b7dicht"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PTKANT", "$.", "KON", "ADV", "KOUS", "APPR", "PDAT", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sich in Deutschland mein Ruf noch erh\u00f6hte,", "tokens": ["Sich", "in", "Deutschland", "mein", "Ruf", "noch", "er\u00b7h\u00f6h\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NE", "PPOSAT", "NN", "ADV", "VVFIN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und ich w\u00fcrde verherrlicht in \u00e4hnlicher Art", "tokens": ["Und", "ich", "w\u00fcr\u00b7de", "ver\u00b7herr\u00b7licht", "in", "\u00e4hn\u00b7li\u00b7cher", "Art"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "VVPP", "APPR", "ADJA", "NN"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "Wie Schiller, Pepita und G\u00f6the:", "tokens": ["Wie", "Schil\u00b7ler", ",", "Pe\u00b7pi\u00b7ta", "und", "G\u00f6\u00b7the", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "$,", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Es bewegte mein Ich doch, mein seliges, kaum", "tokens": ["Es", "be\u00b7weg\u00b7te", "mein", "Ich", "doch", ",", "mein", "se\u00b7li\u00b7ges", ",", "kaum"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "PPER", "ADV", "$,", "PPOSAT", "ADJA", "$,", "ADV"], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.2": {"text": "Wie den t\u00fcrkischen Siebenten Himmel", "tokens": ["Wie", "den", "t\u00fcr\u00b7ki\u00b7schen", "Sie\u00b7ben\u00b7ten", "Him\u00b7mel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN", "NN"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Ein zur Anbetung lieblich einladendes", "tokens": ["Ein", "zur", "An\u00b7be\u00b7tung", "lieb\u00b7lich", "ein\u00b7la\u00b7den\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPRART", "NN", "ADJD", "ART"], "meter": "+-+--+--+--", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Christkirchliches Glockengebimmel!", "tokens": ["Christ\u00b7kirch\u00b7li\u00b7ches", "Glo\u00b7cken\u00b7ge\u00b7bim\u00b7mel", "!"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.33": {"line.1": {"text": "Und so theil' ich im n\u00e4chsten Kapitel denn mit \u2013", "tokens": ["Und", "so", "theil'", "ich", "im", "n\u00e4chs\u00b7ten", "Ka\u00b7pi\u00b7tel", "denn", "mit", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPRART", "ADJA", "NN", "ADV", "APPR", "$("], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Da dieses (de\u00df Raum doch begrenzt ist)", "tokens": ["Da", "die\u00b7ses", "(", "de\u00df", "Raum", "doch", "be\u00b7grenzt", "ist", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "$(", "ART", "NN", "ADV", "VVPP", "VAFIN", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Durch unn\u00fctzes Plaudern (wie dr\u00fcck' ich es aus?)", "tokens": ["Durch", "un\u00b7n\u00fct\u00b7zes", "Plau\u00b7dern", "(", "wie", "dr\u00fcck", "ich", "es", "aus", "?", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "$(", "PWAV", "VVFIN", "PPER", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ver-kammert und ver-conferenzt ist \u2013", "tokens": ["Ver\u00b7kam\u00b7mert", "und", "ver\u00b7con\u00b7fe\u00b7renzt", "ist", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAFIN", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.34": {"line.1": {"text": "Um Lob und Kritik unbek\u00fcmmert all Das,", "tokens": ["Um", "Lob", "und", "Kri\u00b7tik", "un\u00b7be\u00b7k\u00fcm\u00b7mert", "all", "Das", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "KON", "NN", "ADJD", "PIAT", "PDS", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Was im Merkbuch bis heute notirt zwar,", "tokens": ["Was", "im", "Merk\u00b7buch", "bis", "heu\u00b7te", "no\u00b7tirt", "zwar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPRART", "NN", "APPR", "ADV", "VVFIN", "ADV", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Doch bishero noch nicht ", "tokens": ["Doch", "bis\u00b7he\u00b7ro", "noch", "nicht"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "PTKNEG"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "(hier in meiner Verkehrten) fixirt war.", "tokens": ["(", "hier", "in", "mei\u00b7ner", "Ver\u00b7kehr\u00b7ten", ")", "fi\u00b7xirt", "war", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "PPOSAT", "NN", "$(", "VVPP", "VAFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}}}}