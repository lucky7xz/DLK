{"textgrid.poem.63908": {"metadata": {"author": {"name": "Heyse, Paul", "birth": "N.A.", "death": "N.A."}, "title": "1L: H\u00fcte dich, wahllos einzustimmen,", "genre": "verse", "period": "N.A.", "pub_year": 1872, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "H\u00fcte dich, wahllos einzustimmen,", "tokens": ["H\u00fc\u00b7te", "dich", ",", "wahl\u00b7los", "ein\u00b7zu\u00b7stim\u00b7men", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wenn L\u00e4sterzungen die Frauen kr\u00e4nken!", "tokens": ["Wenn", "L\u00e4s\u00b7ter\u00b7zun\u00b7gen", "die", "Frau\u00b7en", "kr\u00e4n\u00b7ken", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Man kann nicht schlimm genug von den schlimmen,", "tokens": ["Man", "kann", "nicht", "schlimm", "ge\u00b7nug", "von", "den", "schlim\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PTKNEG", "ADJD", "ADV", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nicht gut genug von den guten denken.", "tokens": ["Nicht", "gut", "ge\u00b7nug", "von", "den", "gu\u00b7ten", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "ADV", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Durch Trinken loben wir den Wein", "tokens": ["Durch", "Trin\u00b7ken", "lo\u00b7ben", "wir", "den", "Wein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sch\u00f6nen Mund durch K\u00fcssen.", "tokens": ["Und", "sch\u00f6\u00b7nen", "Mund", "durch", "K\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was k\u00f6nnt' auch wohl beredter sein", "tokens": ["Was", "k\u00f6nnt'", "auch", "wohl", "be\u00b7red\u00b7ter", "sein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ADV", "ADV", "ADJD", "VAINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als so verstummen m\u00fcssen?", "tokens": ["Als", "so", "ver\u00b7stum\u00b7men", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Wich, was du liebst, in weite Fernen,", "tokens": ["Wich", ",", "was", "du", "liebst", ",", "in", "wei\u00b7te", "Fer\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWS", "PPER", "VVFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Mu\u00dft du vorlieb zu nehmen lernen;", "tokens": ["Mu\u00dft", "du", "vor\u00b7lieb", "zu", "neh\u00b7men", "ler\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADJD", "PTKZU", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch tu nur keinem Surrogat die Ehre,", "tokens": ["Doch", "tu", "nur", "kei\u00b7nem", "Sur\u00b7ro\u00b7gat", "die", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "ADV", "PIAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Zu glauben, da\u00df es das Echte w\u00e4re.", "tokens": ["Zu", "glau\u00b7ben", ",", "da\u00df", "es", "das", "Ech\u00b7te", "w\u00e4\u00b7re", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Leidenschaft ist ein s\u00fc\u00dfer Wein,", "tokens": ["Lei\u00b7den\u00b7schaft", "ist", "ein", "s\u00fc\u00b7\u00dfer", "Wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Geschl\u00fcrft aus gl\u00fchendem Becher.", "tokens": ["Ge\u00b7schl\u00fcrft", "aus", "gl\u00fc\u00b7hen\u00b7dem", "Be\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Er labt bis ins innerste Mark hinein", "tokens": ["Er", "labt", "bis", "ins", "in\u00b7ners\u00b7te", "Mark", "hin\u00b7ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "ADJA", "NN", "APZR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und versenkt die Lippe dem Zecher.", "tokens": ["Und", "ver\u00b7senkt", "die", "Lip\u00b7pe", "dem", "Ze\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Nie wird ein Weib sich ganz dir weihn,", "tokens": ["Nie", "wird", "ein", "Weib", "sich", "ganz", "dir", "weihn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PRF", "ADV", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat es dir nie was zu verzeihn.", "tokens": ["Hat", "es", "dir", "nie", "was", "zu", "ver\u00b7zeihn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wie trefflich Weib und Mann", "tokens": ["Wie", "treff\u00b7lich", "Weib", "und", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sich miteinander st\u00e4nden,", "tokens": ["Sich", "mi\u00b7tein\u00b7an\u00b7der", "st\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Fingen wir schwerer an,", "tokens": ["Fin\u00b7gen", "wir", "schwe\u00b7rer", "an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.4": {"text": "Und k\u00f6nnten sie leichter enden!", "tokens": ["Und", "k\u00f6nn\u00b7ten", "sie", "leich\u00b7ter", "en\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Klug ist, wer seinen Witz verhehlt", "tokens": ["Klug", "ist", ",", "wer", "sei\u00b7nen", "Witz", "ver\u00b7hehlt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "$,", "PWS", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und bei den Frauen spielt den Toren.", "tokens": ["Und", "bei", "den", "Frau\u00b7en", "spielt", "den", "To\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie denken, wenn's an Verstand uns fehlt,", "tokens": ["Sie", "den\u00b7ken", ",", "wenn's", "an", "Ver\u00b7stand", "uns", "fehlt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "KOUS", "APPR", "NN", "PPER", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir h\u00e4tten ihn um sie verloren.", "tokens": ["Wir", "h\u00e4t\u00b7ten", "ihn", "um", "sie", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wie du gesinnt zu sch\u00f6nen Frauen,", "tokens": ["Wie", "du", "ge\u00b7sinnt", "zu", "sch\u00f6\u00b7nen", "Frau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVPP", "PTKZU", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft ja nicht dem Papier vertrauen.", "tokens": ["Mu\u00dft", "ja", "nicht", "dem", "Pa\u00b7pier", "ver\u00b7trau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Viel Federlesens magst du sparen:", "tokens": ["Viel", "Fe\u00b7der\u00b7le\u00b7sens", "magst", "du", "spa\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Halt' dich ans m\u00fcndliche Verfahren.", "tokens": ["Halt'", "dich", "ans", "m\u00fcnd\u00b7li\u00b7che", "Ver\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Da\u00df es dir nur nicht gleich Bedenken mache,", "tokens": ["Da\u00df", "es", "dir", "nur", "nicht", "gleich", "Be\u00b7den\u00b7ken", "ma\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PTKNEG", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Horcht eine Frau zerstreut auf deiner Stimme Ton.", "tokens": ["Horcht", "ei\u00b7ne", "Frau", "zer\u00b7streut", "auf", "dei\u00b7ner", "Stim\u00b7me", "Ton", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vielleicht ist sie nicht v\u00f6llig bei der Sache,", "tokens": ["Viel\u00b7leicht", "ist", "sie", "nicht", "v\u00f6l\u00b7lig", "bei", "der", "Sa\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Doch desto mehr bei der Person.", "tokens": ["Doch", "des\u00b7to", "mehr", "bei", "der", "Per\u00b7son", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.10": {"line.1": {"text": "Das sind die Traurigen, Flachen,", "tokens": ["Das", "sind", "die", "Trau\u00b7ri\u00b7gen", ",", "Fla\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die tief und stark sich scheinen:", "tokens": ["Die", "tief", "und", "stark", "sich", "schei\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJD", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Frauen, die nicht lachen,", "tokens": ["Die", "Frau\u00b7en", ",", "die", "nicht", "la\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die M\u00e4nner, die nicht weinen.", "tokens": ["Die", "M\u00e4n\u00b7ner", ",", "die", "nicht", "wei\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Nie wird das zartere Geschlecht", "tokens": ["Nie", "wird", "das", "zar\u00b7te\u00b7re", "Ge\u00b7schlecht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Amt der Richter passen.", "tokens": ["Zum", "Amt", "der", "Rich\u00b7ter", "pas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie glauben schon, sie seien h\u00f6chst gerecht,", "tokens": ["Sie", "glau\u00b7ben", "schon", ",", "sie", "sei\u00b7en", "h\u00f6chst", "ge\u00b7recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wenn sie verdammen, ohne zu hassen.", "tokens": ["Wenn", "sie", "ver\u00b7dam\u00b7men", ",", "oh\u00b7ne", "zu", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "$,", "KOUI", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Wie weit ein Weib auch dann und wann", "tokens": ["Wie", "weit", "ein", "Weib", "auch", "dann", "und", "wann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN", "ADV", "ADV", "KON", "PWAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Kultus der Person mag treiben,", "tokens": ["Den", "Kul\u00b7tus", "der", "Per\u00b7son", "mag", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das M\u00e4nnliche im Mann", "tokens": ["Das", "M\u00e4nn\u00b7li\u00b7che", "im", "Mann"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Wird stets des Tempels Gottheit bleiben.", "tokens": ["Wird", "stets", "des", "Tem\u00b7pels", "Got\u00b7theit", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Kommt in ein Frauenlos ein Bruch,", "tokens": ["Kommt", "in", "ein", "Frau\u00b7en\u00b7los", "ein", "Bruch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fchlt sich das Herz getrieben", "tokens": ["F\u00fchlt", "sich", "das", "Herz", "ge\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ART", "NN", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und sch\u00fcttet in ein kleines Buch", "tokens": ["Und", "sch\u00fct\u00b7tet", "in", "ein", "klei\u00b7nes", "Buch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein Leiden und sein Lieben.", "tokens": ["Sein", "Lei\u00b7den", "und", "sein", "Lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Doch was zuerst ein Herzenstrieb,", "tokens": ["Doch", "was", "zu\u00b7erst", "ein", "Her\u00b7zen\u00b7strieb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wird bald bequeme Sitte,", "tokens": ["Wird", "bald", "be\u00b7que\u00b7me", "Sit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und nur, weil sie das erste schrieb,", "tokens": ["Und", "nur", ",", "weil", "sie", "das", "ers\u00b7te", "schrieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "PPER", "ART", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Schreibt sie das zweit' und dritte.", "tokens": ["Schreibt", "sie", "das", "zweit'", "und", "drit\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PDS", "VVFIN", "KON", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Fraun sind oft R\u00e4tsel von jener Art,", "tokens": ["Fraun", "sind", "oft", "R\u00e4t\u00b7sel", "von", "je\u00b7ner", "Art", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Die, wenn wir die L\u00f6sung wissen,", "tokens": ["Die", ",", "wenn", "wir", "die", "L\u00f6\u00b7sung", "wis\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bereuen lassen, da\u00df wir so hart", "tokens": ["Be\u00b7reu\u00b7en", "las\u00b7sen", ",", "da\u00df", "wir", "so", "hart"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VVINF", "$,", "KOUS", "PPER", "ADV", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Z\u00e4hne daran zerbissen.", "tokens": ["Die", "Z\u00e4h\u00b7ne", "da\u00b7ran", "zer\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.16": {"line.1": {"text": "Aus Lieb' oder aus Vernunft zu frei'n?", "tokens": ["Aus", "Lieb'", "o\u00b7der", "aus", "Ver\u00b7nunft", "zu", "frei'n", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie sollte das nicht dasselbe sein,", "tokens": ["Wie", "soll\u00b7te", "das", "nicht", "das\u00b7sel\u00b7be", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PDS", "PTKNEG", "PDAT", "VAINF", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da es doch nichts Vern\u00fcnft'gers gibt,", "tokens": ["Da", "es", "doch", "nichts", "Ver\u00b7n\u00fcnft'\u00b7gers", "gibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PIS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als eine nehmen, die man liebt.", "tokens": ["Als", "ei\u00b7ne", "neh\u00b7men", ",", "die", "man", "liebt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "VVINF", "$,", "PRELS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Wenn die Weiber nicht eitel w\u00e4ren,", "tokens": ["Wenn", "die", "Wei\u00b7ber", "nicht", "ei\u00b7tel", "w\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKNEG", "ADJD", "VAFIN", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Die M\u00e4nner k\u00f6nnten sie's lehren.", "tokens": ["Die", "M\u00e4n\u00b7ner", "k\u00f6nn\u00b7ten", "sie's", "leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.18": {"line.1": {"text": "Wie Mann und Weib verschieden von Natur,", "tokens": ["Wie", "Mann", "und", "Weib", "ver\u00b7schie\u00b7den", "von", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "VVPP", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wird dir ihr Opfermut enth\u00fcllen:", "tokens": ["Wird", "dir", "ihr", "Op\u00b7fer\u00b7mut", "ent\u00b7h\u00fcl\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Es opfert sich der Mann erkannten Zwecken nur,", "tokens": ["Es", "op\u00b7fert", "sich", "der", "Mann", "er\u00b7kann\u00b7ten", "Zwe\u00b7cken", "nur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das Weib des blo\u00dfen Opfers willen.", "tokens": ["Das", "Weib", "des", "blo\u00b7\u00dfen", "Op\u00b7fers", "wil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Das ist unselige Minne,", "tokens": ["Das", "ist", "un\u00b7se\u00b7li\u00b7ge", "Min\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wenn Weiber das Herz dir r\u00fchren,", "tokens": ["Wenn", "Wei\u00b7ber", "das", "Herz", "dir", "r\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bei denen Gem\u00fct und Sinne", "tokens": ["Bei", "de\u00b7nen", "Ge\u00b7m\u00fct", "und", "Sin\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "NN", "KON", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Getrennte Wirtschaft f\u00fchren.", "tokens": ["Ge\u00b7trenn\u00b7te", "Wirt\u00b7schaft", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Nicht, welches Weib dem Mann gef\u00e4llt,", "tokens": ["Nicht", ",", "wel\u00b7ches", "Weib", "dem", "Mann", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "$,", "PWAT", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist seines Wertes Messer.", "tokens": ["Ist", "sei\u00b7nes", "Wer\u00b7tes", "Mes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von Weibern denkt auch mancher Held:", "tokens": ["Von", "Wei\u00b7bern", "denkt", "auch", "man\u00b7cher", "Held", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Je schlimmer, desto besser.", "tokens": ["Je", "schlim\u00b7mer", ",", "des\u00b7to", "bes\u00b7ser", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Wer sich mit Seelenkunde befa\u00dft,", "tokens": ["Wer", "sich", "mit", "See\u00b7len\u00b7kun\u00b7de", "be\u00b7fa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Wird manch verborgnen Schatz entsiegeln;", "tokens": ["Wird", "manch", "ver\u00b7borg\u00b7nen", "Schatz", "ent\u00b7sie\u00b7geln", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch welcher Mann zu welchem Weibe pa\u00dft,", "tokens": ["Doch", "wel\u00b7cher", "Mann", "zu", "wel\u00b7chem", "Wei\u00b7be", "pa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAT", "NN", "APPR", "PWAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Kein Psychologe wird's erkl\u00fcgeln.", "tokens": ["Kein", "Psy\u00b7cho\u00b7lo\u00b7ge", "wird's", "er\u00b7kl\u00fc\u00b7geln", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Liebe bringt uns um allerhand:", "tokens": ["Lie\u00b7be", "bringt", "uns", "um", "al\u00b7ler\u00b7hand", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPR", "PIS", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Um Zeit, Geld, Reputation und Verstand.", "tokens": ["Um", "Zeit", ",", "Geld", ",", "Re\u00b7pu\u00b7ta\u00b7ti\u00b7on", "und", "Ver\u00b7stand", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Wer nur mit dem Bankrott nicht endet,", "tokens": ["Wer", "nur", "mit", "dem", "Bank\u00b7rott", "nicht", "en\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Hat nie eintr\u00e4glicher verschwendet.", "tokens": ["Hat", "nie", "ein\u00b7tr\u00e4g\u00b7li\u00b7cher", "ver\u00b7schwen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "H\u00fcte dich, wahllos einzustimmen,", "tokens": ["H\u00fc\u00b7te", "dich", ",", "wahl\u00b7los", "ein\u00b7zu\u00b7stim\u00b7men", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "ADJD", "VVPP", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wenn L\u00e4sterzungen die Frauen kr\u00e4nken!", "tokens": ["Wenn", "L\u00e4s\u00b7ter\u00b7zun\u00b7gen", "die", "Frau\u00b7en", "kr\u00e4n\u00b7ken", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Man kann nicht schlimm genug von den schlimmen,", "tokens": ["Man", "kann", "nicht", "schlimm", "ge\u00b7nug", "von", "den", "schlim\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PTKNEG", "ADJD", "ADV", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nicht gut genug von den guten denken.", "tokens": ["Nicht", "gut", "ge\u00b7nug", "von", "den", "gu\u00b7ten", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "ADV", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.24": {"line.1": {"text": "Durch Trinken loben wir den Wein", "tokens": ["Durch", "Trin\u00b7ken", "lo\u00b7ben", "wir", "den", "Wein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sch\u00f6nen Mund durch K\u00fcssen.", "tokens": ["Und", "sch\u00f6\u00b7nen", "Mund", "durch", "K\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was k\u00f6nnt' auch wohl beredter sein", "tokens": ["Was", "k\u00f6nnt'", "auch", "wohl", "be\u00b7red\u00b7ter", "sein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "ADV", "ADV", "ADJD", "VAINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als so verstummen m\u00fcssen?", "tokens": ["Als", "so", "ver\u00b7stum\u00b7men", "m\u00fcs\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Wich, was du liebst, in weite Fernen,", "tokens": ["Wich", ",", "was", "du", "liebst", ",", "in", "wei\u00b7te", "Fer\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWS", "PPER", "VVFIN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Mu\u00dft du vorlieb zu nehmen lernen;", "tokens": ["Mu\u00dft", "du", "vor\u00b7lieb", "zu", "neh\u00b7men", "ler\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADJD", "PTKZU", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch tu nur keinem Surrogat die Ehre,", "tokens": ["Doch", "tu", "nur", "kei\u00b7nem", "Sur\u00b7ro\u00b7gat", "die", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "ADV", "PIAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Zu glauben, da\u00df es das Echte w\u00e4re.", "tokens": ["Zu", "glau\u00b7ben", ",", "da\u00df", "es", "das", "Ech\u00b7te", "w\u00e4\u00b7re", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.26": {"line.1": {"text": "Leidenschaft ist ein s\u00fc\u00dfer Wein,", "tokens": ["Lei\u00b7den\u00b7schaft", "ist", "ein", "s\u00fc\u00b7\u00dfer", "Wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Geschl\u00fcrft aus gl\u00fchendem Becher.", "tokens": ["Ge\u00b7schl\u00fcrft", "aus", "gl\u00fc\u00b7hen\u00b7dem", "Be\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Er labt bis ins innerste Mark hinein", "tokens": ["Er", "labt", "bis", "ins", "in\u00b7ners\u00b7te", "Mark", "hin\u00b7ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "ADJA", "NN", "APZR"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und versenkt die Lippe dem Zecher.", "tokens": ["Und", "ver\u00b7senkt", "die", "Lip\u00b7pe", "dem", "Ze\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.27": {"line.1": {"text": "Nie wird ein Weib sich ganz dir weihn,", "tokens": ["Nie", "wird", "ein", "Weib", "sich", "ganz", "dir", "weihn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PRF", "ADV", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat es dir nie was zu verzeihn.", "tokens": ["Hat", "es", "dir", "nie", "was", "zu", "ver\u00b7zeihn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Wie trefflich Weib und Mann", "tokens": ["Wie", "treff\u00b7lich", "Weib", "und", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sich miteinander st\u00e4nden,", "tokens": ["Sich", "mi\u00b7tein\u00b7an\u00b7der", "st\u00e4n\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Fingen wir schwerer an,", "tokens": ["Fin\u00b7gen", "wir", "schwe\u00b7rer", "an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.4": {"text": "Und k\u00f6nnten sie leichter enden!", "tokens": ["Und", "k\u00f6nn\u00b7ten", "sie", "leich\u00b7ter", "en\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.29": {"line.1": {"text": "Klug ist, wer seinen Witz verhehlt", "tokens": ["Klug", "ist", ",", "wer", "sei\u00b7nen", "Witz", "ver\u00b7hehlt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "$,", "PWS", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und bei den Frauen spielt den Toren.", "tokens": ["Und", "bei", "den", "Frau\u00b7en", "spielt", "den", "To\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie denken, wenn's an Verstand uns fehlt,", "tokens": ["Sie", "den\u00b7ken", ",", "wenn's", "an", "Ver\u00b7stand", "uns", "fehlt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "KOUS", "APPR", "NN", "PPER", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir h\u00e4tten ihn um sie verloren.", "tokens": ["Wir", "h\u00e4t\u00b7ten", "ihn", "um", "sie", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Wie du gesinnt zu sch\u00f6nen Frauen,", "tokens": ["Wie", "du", "ge\u00b7sinnt", "zu", "sch\u00f6\u00b7nen", "Frau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVPP", "PTKZU", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft ja nicht dem Papier vertrauen.", "tokens": ["Mu\u00dft", "ja", "nicht", "dem", "Pa\u00b7pier", "ver\u00b7trau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Viel Federlesens magst du sparen:", "tokens": ["Viel", "Fe\u00b7der\u00b7le\u00b7sens", "magst", "du", "spa\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Halt' dich ans m\u00fcndliche Verfahren.", "tokens": ["Halt'", "dich", "ans", "m\u00fcnd\u00b7li\u00b7che", "Ver\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Da\u00df es dir nur nicht gleich Bedenken mache,", "tokens": ["Da\u00df", "es", "dir", "nur", "nicht", "gleich", "Be\u00b7den\u00b7ken", "ma\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PTKNEG", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Horcht eine Frau zerstreut auf deiner Stimme Ton.", "tokens": ["Horcht", "ei\u00b7ne", "Frau", "zer\u00b7streut", "auf", "dei\u00b7ner", "Stim\u00b7me", "Ton", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vielleicht ist sie nicht v\u00f6llig bei der Sache,", "tokens": ["Viel\u00b7leicht", "ist", "sie", "nicht", "v\u00f6l\u00b7lig", "bei", "der", "Sa\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Doch desto mehr bei der Person.", "tokens": ["Doch", "des\u00b7to", "mehr", "bei", "der", "Per\u00b7son", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.32": {"line.1": {"text": "Das sind die Traurigen, Flachen,", "tokens": ["Das", "sind", "die", "Trau\u00b7ri\u00b7gen", ",", "Fla\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die tief und stark sich scheinen:", "tokens": ["Die", "tief", "und", "stark", "sich", "schei\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJD", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Frauen, die nicht lachen,", "tokens": ["Die", "Frau\u00b7en", ",", "die", "nicht", "la\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die M\u00e4nner, die nicht weinen.", "tokens": ["Die", "M\u00e4n\u00b7ner", ",", "die", "nicht", "wei\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "Nie wird das zartere Geschlecht", "tokens": ["Nie", "wird", "das", "zar\u00b7te\u00b7re", "Ge\u00b7schlecht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Amt der Richter passen.", "tokens": ["Zum", "Amt", "der", "Rich\u00b7ter", "pas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie glauben schon, sie seien h\u00f6chst gerecht,", "tokens": ["Sie", "glau\u00b7ben", "schon", ",", "sie", "sei\u00b7en", "h\u00f6chst", "ge\u00b7recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wenn sie verdammen, ohne zu hassen.", "tokens": ["Wenn", "sie", "ver\u00b7dam\u00b7men", ",", "oh\u00b7ne", "zu", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "$,", "KOUI", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.34": {"line.1": {"text": "Wie weit ein Weib auch dann und wann", "tokens": ["Wie", "weit", "ein", "Weib", "auch", "dann", "und", "wann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN", "ADV", "ADV", "KON", "PWAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Kultus der Person mag treiben,", "tokens": ["Den", "Kul\u00b7tus", "der", "Per\u00b7son", "mag", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das M\u00e4nnliche im Mann", "tokens": ["Das", "M\u00e4nn\u00b7li\u00b7che", "im", "Mann"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Wird stets des Tempels Gottheit bleiben.", "tokens": ["Wird", "stets", "des", "Tem\u00b7pels", "Got\u00b7theit", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Kommt in ein Frauenlos ein Bruch,", "tokens": ["Kommt", "in", "ein", "Frau\u00b7en\u00b7los", "ein", "Bruch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fchlt sich das Herz getrieben", "tokens": ["F\u00fchlt", "sich", "das", "Herz", "ge\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ART", "NN", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und sch\u00fcttet in ein kleines Buch", "tokens": ["Und", "sch\u00fct\u00b7tet", "in", "ein", "klei\u00b7nes", "Buch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein Leiden und sein Lieben.", "tokens": ["Sein", "Lei\u00b7den", "und", "sein", "Lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.36": {"line.1": {"text": "Doch was zuerst ein Herzenstrieb,", "tokens": ["Doch", "was", "zu\u00b7erst", "ein", "Her\u00b7zen\u00b7strieb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wird bald bequeme Sitte,", "tokens": ["Wird", "bald", "be\u00b7que\u00b7me", "Sit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und nur, weil sie das erste schrieb,", "tokens": ["Und", "nur", ",", "weil", "sie", "das", "ers\u00b7te", "schrieb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "PPER", "ART", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Schreibt sie das zweit' und dritte.", "tokens": ["Schreibt", "sie", "das", "zweit'", "und", "drit\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PDS", "VVFIN", "KON", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.37": {"line.1": {"text": "Fraun sind oft R\u00e4tsel von jener Art,", "tokens": ["Fraun", "sind", "oft", "R\u00e4t\u00b7sel", "von", "je\u00b7ner", "Art", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Die, wenn wir die L\u00f6sung wissen,", "tokens": ["Die", ",", "wenn", "wir", "die", "L\u00f6\u00b7sung", "wis\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bereuen lassen, da\u00df wir so hart", "tokens": ["Be\u00b7reu\u00b7en", "las\u00b7sen", ",", "da\u00df", "wir", "so", "hart"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VVINF", "$,", "KOUS", "PPER", "ADV", "ADJD"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Z\u00e4hne daran zerbissen.", "tokens": ["Die", "Z\u00e4h\u00b7ne", "da\u00b7ran", "zer\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.38": {"line.1": {"text": "Aus Lieb' oder aus Vernunft zu frei'n?", "tokens": ["Aus", "Lieb'", "o\u00b7der", "aus", "Ver\u00b7nunft", "zu", "frei'n", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie sollte das nicht dasselbe sein,", "tokens": ["Wie", "soll\u00b7te", "das", "nicht", "das\u00b7sel\u00b7be", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PDS", "PTKNEG", "PDAT", "VAINF", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da es doch nichts Vern\u00fcnft'gers gibt,", "tokens": ["Da", "es", "doch", "nichts", "Ver\u00b7n\u00fcnft'\u00b7gers", "gibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PIS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als eine nehmen, die man liebt.", "tokens": ["Als", "ei\u00b7ne", "neh\u00b7men", ",", "die", "man", "liebt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "VVINF", "$,", "PRELS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Wenn die Weiber nicht eitel w\u00e4ren,", "tokens": ["Wenn", "die", "Wei\u00b7ber", "nicht", "ei\u00b7tel", "w\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKNEG", "ADJD", "VAFIN", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Die M\u00e4nner k\u00f6nnten sie's lehren.", "tokens": ["Die", "M\u00e4n\u00b7ner", "k\u00f6nn\u00b7ten", "sie's", "leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.40": {"line.1": {"text": "Wie Mann und Weib verschieden von Natur,", "tokens": ["Wie", "Mann", "und", "Weib", "ver\u00b7schie\u00b7den", "von", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "VVPP", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wird dir ihr Opfermut enth\u00fcllen:", "tokens": ["Wird", "dir", "ihr", "Op\u00b7fer\u00b7mut", "ent\u00b7h\u00fcl\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Es opfert sich der Mann erkannten Zwecken nur,", "tokens": ["Es", "op\u00b7fert", "sich", "der", "Mann", "er\u00b7kann\u00b7ten", "Zwe\u00b7cken", "nur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das Weib des blo\u00dfen Opfers willen.", "tokens": ["Das", "Weib", "des", "blo\u00b7\u00dfen", "Op\u00b7fers", "wil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Das ist unselige Minne,", "tokens": ["Das", "ist", "un\u00b7se\u00b7li\u00b7ge", "Min\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wenn Weiber das Herz dir r\u00fchren,", "tokens": ["Wenn", "Wei\u00b7ber", "das", "Herz", "dir", "r\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bei denen Gem\u00fct und Sinne", "tokens": ["Bei", "de\u00b7nen", "Ge\u00b7m\u00fct", "und", "Sin\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "NN", "KON", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Getrennte Wirtschaft f\u00fchren.", "tokens": ["Ge\u00b7trenn\u00b7te", "Wirt\u00b7schaft", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.42": {"line.1": {"text": "Nicht, welches Weib dem Mann gef\u00e4llt,", "tokens": ["Nicht", ",", "wel\u00b7ches", "Weib", "dem", "Mann", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "$,", "PWAT", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist seines Wertes Messer.", "tokens": ["Ist", "sei\u00b7nes", "Wer\u00b7tes", "Mes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von Weibern denkt auch mancher Held:", "tokens": ["Von", "Wei\u00b7bern", "denkt", "auch", "man\u00b7cher", "Held", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Je schlimmer, desto besser.", "tokens": ["Je", "schlim\u00b7mer", ",", "des\u00b7to", "bes\u00b7ser", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.43": {"line.1": {"text": "Wer sich mit Seelenkunde befa\u00dft,", "tokens": ["Wer", "sich", "mit", "See\u00b7len\u00b7kun\u00b7de", "be\u00b7fa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Wird manch verborgnen Schatz entsiegeln;", "tokens": ["Wird", "manch", "ver\u00b7borg\u00b7nen", "Schatz", "ent\u00b7sie\u00b7geln", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch welcher Mann zu welchem Weibe pa\u00dft,", "tokens": ["Doch", "wel\u00b7cher", "Mann", "zu", "wel\u00b7chem", "Wei\u00b7be", "pa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAT", "NN", "APPR", "PWAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Kein Psychologe wird's erkl\u00fcgeln.", "tokens": ["Kein", "Psy\u00b7cho\u00b7lo\u00b7ge", "wird's", "er\u00b7kl\u00fc\u00b7geln", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Liebe bringt uns um allerhand:", "tokens": ["Lie\u00b7be", "bringt", "uns", "um", "al\u00b7ler\u00b7hand", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPR", "PIS", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Um Zeit, Geld, Reputation und Verstand.", "tokens": ["Um", "Zeit", ",", "Geld", ",", "Re\u00b7pu\u00b7ta\u00b7ti\u00b7on", "und", "Ver\u00b7stand", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Wer nur mit dem Bankrott nicht endet,", "tokens": ["Wer", "nur", "mit", "dem", "Bank\u00b7rott", "nicht", "en\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Hat nie eintr\u00e4glicher verschwendet.", "tokens": ["Hat", "nie", "ein\u00b7tr\u00e4g\u00b7li\u00b7cher", "ver\u00b7schwen\u00b7det", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}