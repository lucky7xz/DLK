{"textgrid.poem.34961": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Disputation", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In der Aula zu Toledo", "tokens": ["In", "der", "Au\u00b7la", "zu", "To\u00b7le\u00b7do"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NE", "APPR", "NE"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Klingen schmetternd die Fanfaren;", "tokens": ["Klin\u00b7gen", "schmet\u00b7ternd", "die", "Fan\u00b7fa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu dem geistlichen Turnei", "tokens": ["Zu", "dem", "geist\u00b7li\u00b7chen", "Tur\u00b7nei"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wallt das Volk in bunten Scharen.", "tokens": ["Wallt", "das", "Volk", "in", "bun\u00b7ten", "Scha\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Das ist nicht ein weltlich Stechen,", "tokens": ["Das", "ist", "nicht", "ein", "welt\u00b7lich", "Ste\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKNEG", "ART", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Keine Eisenwaffe blitzet \u2013", "tokens": ["Kei\u00b7ne", "Ei\u00b7sen\u00b7waf\u00b7fe", "blit\u00b7zet", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eine Lanze ist das Wort,", "tokens": ["Ei\u00b7ne", "Lan\u00b7ze", "ist", "das", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das scholastisch scharf gespitzet.", "tokens": ["Das", "scho\u00b7las\u00b7tisch", "scharf", "ge\u00b7spit\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Nicht galante Paladins", "tokens": ["Nicht", "ga\u00b7lan\u00b7te", "Pa\u00b7la\u00b7dins"], "token_info": ["word", "word", "word"], "pos": ["PTKNEG", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Fechten hier, nicht Damendiener \u2013", "tokens": ["Fech\u00b7ten", "hier", ",", "nicht", "Da\u00b7men\u00b7die\u00b7ner", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "PTKNEG", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieses Kampfes Ritter sind", "tokens": ["Die\u00b7ses", "Kamp\u00b7fes", "Rit\u00b7ter", "sind"], "token_info": ["word", "word", "word", "word"], "pos": ["PDAT", "NN", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Kapuziner und Rabbiner.", "tokens": ["Ka\u00b7pu\u00b7zi\u00b7ner", "und", "Rab\u00b7bi\u00b7ner", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Statt des Helmes tragen sie", "tokens": ["Statt", "des", "Hel\u00b7mes", "tra\u00b7gen", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "VVFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Schabbesdeckel und Kapuzen;", "tokens": ["Schab\u00b7bes\u00b7de\u00b7ckel", "und", "Ka\u00b7pu\u00b7zen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Skapulier und Arbekanfe\u00df", "tokens": ["Ska\u00b7pu\u00b7lier", "und", "Ar\u00b7be\u00b7kan\u00b7fe\u00df"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Sind der Harnisch, drob sie trutzen.", "tokens": ["Sind", "der", "Har\u00b7nisch", ",", "drob", "sie", "trut\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "VVFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Welches ist der wahre Gott?", "tokens": ["Wel\u00b7ches", "ist", "der", "wah\u00b7re", "Gott", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist es der Hebr\u00e4er starrer", "tokens": ["Ist", "es", "der", "Heb\u00b7r\u00e4\u00b7er", "star\u00b7rer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADJA"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Gro\u00dfer Eingott, dessen K\u00e4mpe", "tokens": ["Gro\u00b7\u00dfer", "Ein\u00b7gott", ",", "des\u00b7sen", "K\u00e4m\u00b7pe"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "PRELAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Rabbi Juda, der Navarrer?", "tokens": ["Rab\u00b7bi", "Ju\u00b7da", ",", "der", "Na\u00b7var\u00b7rer", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Oder ist es der dreifalt'ge", "tokens": ["O\u00b7der", "ist", "es", "der", "drei\u00b7falt'\u00b7ge"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "ART", "ADJA"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Liebegott der Christianer,", "tokens": ["Lie\u00b7be\u00b7gott", "der", "Chris\u00b7ti\u00b7a\u00b7ner", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dessen K\u00e4mpe Frater Jose,", "tokens": ["Des\u00b7sen", "K\u00e4m\u00b7pe", "Fra\u00b7ter", "Jo\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "NE", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Gardian der Franziskaner?", "tokens": ["Gar\u00b7di\u00b7an", "der", "Fran\u00b7zis\u00b7ka\u00b7ner", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Durch die Macht der Argumente,", "tokens": ["Durch", "die", "Macht", "der", "Ar\u00b7gu\u00b7men\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Durch der Logik Kettenschl\u00fcsse", "tokens": ["Durch", "der", "Lo\u00b7gik", "Ket\u00b7ten\u00b7schl\u00fcs\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Zitate von Autoren,", "tokens": ["Und", "Zi\u00b7ta\u00b7te", "von", "Au\u00b7to\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die man anerkennen m\u00fcsse,", "tokens": ["Die", "man", "an\u00b7er\u00b7ken\u00b7nen", "m\u00fcs\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Will ein jeder K\u00e4mpe seinen", "tokens": ["Will", "ein", "je\u00b7der", "K\u00e4m\u00b7pe", "sei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ART", "PIAT", "NN", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gegner ad absurdum f\u00fchren", "tokens": ["Geg\u00b7ner", "ad", "ab\u00b7sur\u00b7dum", "f\u00fch\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "FM", "FM", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Und die wahre G\u00f6ttlichkeit", "tokens": ["Und", "die", "wah\u00b7re", "G\u00f6tt\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Seines Gottes demonstrieren.", "tokens": ["Sei\u00b7nes", "Got\u00b7tes", "de\u00b7monst\u00b7rie\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.9": {"line.1": {"text": "Festgestellt ist: da\u00df derjen'ge,", "tokens": ["Fest\u00b7ge\u00b7stellt", "ist", ":", "da\u00df", "der\u00b7jen'\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$.", "KOUS", "PDS", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der im Streit ward \u00fcberwunden,", "tokens": ["Der", "im", "Streit", "ward", "\u00fc\u00b7berw\u00b7un\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPRART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Seines Gegners Religion", "tokens": ["Sei\u00b7nes", "Geg\u00b7ners", "Re\u00b7li\u00b7gi\u00b7on"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Anzunehmen sei verbunden,", "tokens": ["An\u00b7zu\u00b7neh\u00b7men", "sei", "ver\u00b7bun\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Da\u00df der Jude sich der Taufe", "tokens": ["Da\u00df", "der", "Ju\u00b7de", "sich", "der", "Tau\u00b7fe"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PRF", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Heil'gem Sakramente f\u00fcge,", "tokens": ["Heil'\u00b7gem", "Sak\u00b7ra\u00b7men\u00b7te", "f\u00fc\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und im Gegenteil der Christ", "tokens": ["Und", "im", "Ge\u00b7gen\u00b7teil", "der", "Christ"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Beschneidung unterliege.", "tokens": ["Der", "Be\u00b7schnei\u00b7dung", "un\u00b7ter\u00b7lie\u00b7ge", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Jedem von den beiden K\u00e4mpen", "tokens": ["Je\u00b7dem", "von", "den", "bei\u00b7den", "K\u00e4m\u00b7pen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "APPR", "ART", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Beigesellt sind elf Genossen,", "tokens": ["Bei\u00b7ge\u00b7sellt", "sind", "elf", "Ge\u00b7nos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die zu teilen sein Geschick", "tokens": ["Die", "zu", "tei\u00b7len", "sein", "Ge\u00b7schick"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PTKZU", "VVINF", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind in Freud und Leid entschlossen.", "tokens": ["Sind", "in", "Freud", "und", "Leid", "ent\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Glaubenssicher sind die M\u00f6nche", "tokens": ["Glau\u00b7bens\u00b7si\u00b7cher", "sind", "die", "M\u00f6n\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Von des Gardians Geleitschaft,", "tokens": ["Von", "des", "Gar\u00b7di\u00b7ans", "Ge\u00b7leit\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Halten schon Weihwasserk\u00fcbel", "tokens": ["Hal\u00b7ten", "schon", "Weih\u00b7was\u00b7ser\u00b7k\u00fc\u00b7bel"], "token_info": ["word", "word", "word"], "pos": ["NN", "ADV", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "F\u00fcr die Taufe in Bereitschaft,", "tokens": ["F\u00fcr", "die", "Tau\u00b7fe", "in", "Be\u00b7reit\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Schwingen schon die Sprengelbesen", "tokens": ["Schwin\u00b7gen", "schon", "die", "Spren\u00b7gel\u00b7be\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die blanken R\u00e4ucherf\u00e4sser \u2013", "tokens": ["Und", "die", "blan\u00b7ken", "R\u00e4u\u00b7cher\u00b7f\u00e4s\u00b7ser", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihre Gegner unterdessen", "tokens": ["Ih\u00b7re", "Geg\u00b7ner", "un\u00b7ter\u00b7des\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wetzen die Beschneidungsmesser.", "tokens": ["Wet\u00b7zen", "die", "Be\u00b7schnei\u00b7dungs\u00b7mes\u00b7ser", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Beide Rotten stehn schlagfertig", "tokens": ["Bei\u00b7de", "Rot\u00b7ten", "stehn", "schlag\u00b7fer\u00b7tig"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor den Schranken in dem Saale,", "tokens": ["Vor", "den", "Schran\u00b7ken", "in", "dem", "Saa\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und das Volk mit Ungeduld", "tokens": ["Und", "das", "Volk", "mit", "Un\u00b7ge\u00b7duld"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Harret dr\u00e4ngend der Signale.", "tokens": ["Har\u00b7ret", "dr\u00e4n\u00b7gend", "der", "Sig\u00b7na\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Unterm g\u00fcldnen Baldachin", "tokens": ["Un\u00b7term", "g\u00fcld\u00b7nen", "Bal\u00b7da\u00b7ch\u00b7in"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und umrauscht vom Hofgesinde", "tokens": ["Und", "um\u00b7rauscht", "vom", "Hof\u00b7ge\u00b7sin\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sitzt der K\u00f6nig und die K\u00f6n'gin;", "tokens": ["Sitzt", "der", "K\u00f6\u00b7nig", "und", "die", "K\u00f6n'\u00b7gin", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Diese gleichet einem Kinde.", "tokens": ["Die\u00b7se", "glei\u00b7chet", "ei\u00b7nem", "Kin\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Ein franz\u00f6sisch stumpfes N\u00e4schen,", "tokens": ["Ein", "fran\u00b7z\u00f6\u00b7sisch", "stump\u00b7fes", "N\u00e4\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schalkheit kichert in den Mienen,", "tokens": ["Schalk\u00b7heit", "ki\u00b7chert", "in", "den", "Mie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch bezaubernd sind des Mundes", "tokens": ["Doch", "be\u00b7zau\u00b7bernd", "sind", "des", "Mun\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Immer l\u00e4chelnde Rubinen.", "tokens": ["Im\u00b7mer", "l\u00e4\u00b7cheln\u00b7de", "Ru\u00b7bi\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.17": {"line.1": {"text": "Sch\u00f6ne, flatterhafte Blume \u2013", "tokens": ["Sch\u00f6\u00b7ne", ",", "flat\u00b7ter\u00b7haf\u00b7te", "Blu\u00b7me", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df sich ihrer Gott erbarme \u2013", "tokens": ["Da\u00df", "sich", "ih\u00b7rer", "Gott", "er\u00b7bar\u00b7me", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPOSAT", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von dem heitern Seineufer", "tokens": ["Von", "dem", "hei\u00b7tern", "Sein\u00b7eu\u00b7fer"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wurde sie verpflanzt, die arme,", "tokens": ["Wur\u00b7de", "sie", "ver\u00b7pflanzt", ",", "die", "ar\u00b7me", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVPP", "$,", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Hierher in den steifen Boden", "tokens": ["Hier\u00b7her", "in", "den", "stei\u00b7fen", "Bo\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Der hispanischen Grandezza;", "tokens": ["Der", "his\u00b7pa\u00b7ni\u00b7schen", "Gran\u00b7dez\u00b7za", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Weiland hie\u00df sie Blanch' de Bourbon,", "tokens": ["Wei\u00b7land", "hie\u00df", "sie", "Blan\u00b7ch'", "de", "Bour\u00b7bon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "NE", "NE", "NE", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Do\u00f1a Blanka hei\u00dft sie jetzo.", "tokens": ["Do\u00f1a", "Blan\u00b7ka", "hei\u00dft", "sie", "jet\u00b7zo", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.19": {"line.1": {"text": "Pedro wird genannt der K\u00f6nig", "tokens": ["Ped\u00b7ro", "wird", "ge\u00b7nannt", "der", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "VVPP", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit dem Zusatz der Grausame;", "tokens": ["Mit", "dem", "Zu\u00b7satz", "der", "Grau\u00b7sa\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Aber heute, milden Sinnes,", "tokens": ["A\u00b7ber", "heu\u00b7te", ",", "mil\u00b7den", "Sin\u00b7nes", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist er besser als sein Name.", "tokens": ["Ist", "er", "bes\u00b7ser", "als", "sein", "Na\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "KOKOM", "PPOSAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.20": {"line.1": {"text": "Unterh\u00e4lt sich gut gelaunt", "tokens": ["Un\u00b7ter\u00b7h\u00e4lt", "sich", "gut", "ge\u00b7launt"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADJD", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit des Hofes Edelleuten;", "tokens": ["Mit", "des", "Ho\u00b7fes", "E\u00b7del\u00b7leu\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch den Juden und den Mohren", "tokens": ["Auch", "den", "Ju\u00b7den", "und", "den", "Moh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sagt er viele Artigkeiten.", "tokens": ["Sagt", "er", "vie\u00b7le", "Ar\u00b7tig\u00b7kei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.21": {"line.1": {"text": "Diese Ritter ohne Vorhaut", "tokens": ["Die\u00b7se", "Rit\u00b7ter", "oh\u00b7ne", "Vor\u00b7haut"], "token_info": ["word", "word", "word", "word"], "pos": ["PDAT", "NN", "APPR", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sind des K\u00f6nigs Lieblingsschranzen,", "tokens": ["Sind", "des", "K\u00f6\u00b7nigs", "Lieb\u00b7lings\u00b7schran\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie befehl'gen seine Heere,", "tokens": ["Sie", "be\u00b7fehl'\u00b7gen", "sei\u00b7ne", "Hee\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sie verwalten die Finanzen.", "tokens": ["Sie", "ver\u00b7wal\u00b7ten", "die", "Fi\u00b7nan\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Aber pl\u00f6tzlich Paukenschl\u00e4ge,", "tokens": ["A\u00b7ber", "pl\u00f6tz\u00b7lich", "Pau\u00b7ken\u00b7schl\u00e4\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und es melden die Trompeten,", "tokens": ["Und", "es", "mel\u00b7den", "die", "Trom\u00b7pe\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df begonnen hat der Maulkampf,", "tokens": ["Da\u00df", "be\u00b7gon\u00b7nen", "hat", "der", "Maul\u00b7kampf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVPP", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Disput der zwei Athleten.", "tokens": ["Der", "Dis\u00b7put", "der", "zwei", "Ath\u00b7le\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "CARD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.23": {"line.1": {"text": "Der Gardian der Franziskaner", "tokens": ["Der", "Gar\u00b7di\u00b7an", "der", "Fran\u00b7zis\u00b7ka\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bricht hervor mit frommem Grimme;", "tokens": ["Bricht", "her\u00b7vor", "mit", "from\u00b7mem", "Grim\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Polternd roh und widrig greinend", "tokens": ["Pol\u00b7ternd", "roh", "und", "wid\u00b7rig", "grei\u00b7nend"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "ADJD", "KON", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist abwechselnd seine Stimme.", "tokens": ["Ist", "ab\u00b7wech\u00b7selnd", "sei\u00b7ne", "Stim\u00b7me", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "In des Vaters und des Sohnes", "tokens": ["In", "des", "Va\u00b7ters", "und", "des", "Soh\u00b7nes"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und des Heil'gen Geistes Namen", "tokens": ["Und", "des", "Heil'\u00b7gen", "Geis\u00b7tes", "Na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Exorzieret er den Rabbi,", "tokens": ["Ex\u00b7or\u00b7zie\u00b7ret", "er", "den", "Rab\u00b7bi", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Jakobs maledeiten Samen.", "tokens": ["Ja\u00b7kobs", "ma\u00b7le\u00b7dei\u00b7ten", "Sa\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Denn bei solchen Kontroversen", "tokens": ["Denn", "bei", "sol\u00b7chen", "Kont\u00b7ro\u00b7ver\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sind oft Teufelchen verborgen", "tokens": ["Sind", "oft", "Teu\u00b7fel\u00b7chen", "ver\u00b7bor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "NN", "VVPP"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "In dem Juden, die mit Scharfsinn,", "tokens": ["In", "dem", "Ju\u00b7den", ",", "die", "mit", "Scharf\u00b7sinn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Witz und Gr\u00fcnden ihn versorgen.", "tokens": ["Witz", "und", "Gr\u00fcn\u00b7den", "ihn", "ver\u00b7sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Nun die Teufel ausgetrieben", "tokens": ["Nun", "die", "Teu\u00b7fel", "aus\u00b7ge\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Durch die Macht des Exorzismus,", "tokens": ["Durch", "die", "Macht", "des", "Ex\u00b7or\u00b7zis\u00b7mus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Kommt der M\u00f6nch auch zur Dogmatik,", "tokens": ["Kommt", "der", "M\u00f6nch", "auch", "zur", "Dog\u00b7ma\u00b7tik", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Kugelt ab den Katechismus.", "tokens": ["Ku\u00b7gelt", "ab", "den", "Ka\u00b7te\u00b7chis\u00b7mus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.27": {"line.1": {"text": "Er erz\u00e4hlt, da\u00df in der Gottheit", "tokens": ["Er", "er\u00b7z\u00e4hlt", ",", "da\u00df", "in", "der", "Got\u00b7theit"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Drei Personen sind enthalten,", "tokens": ["Drei", "Per\u00b7so\u00b7nen", "sind", "ent\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die jedoch zu einer einz'gen,", "tokens": ["Die", "je\u00b7doch", "zu", "ei\u00b7ner", "einz'\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn es passend, sich gestalten \u2013", "tokens": ["Wenn", "es", "pas\u00b7send", ",", "sich", "ge\u00b7stal\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "$,", "PRF", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Ein Mysterium, das nur", "tokens": ["Ein", "Mys\u00b7te\u00b7ri\u00b7um", ",", "das", "nur"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Von demjen'gen wird verstanden,", "tokens": ["Von", "demjen'\u00b7gen", "wird", "ver\u00b7stan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der entsprungen ist dem Kerker", "tokens": ["Der", "ent\u00b7sprun\u00b7gen", "ist", "dem", "Ker\u00b7ker"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Vernunft und ihren Banden.", "tokens": ["Der", "Ver\u00b7nunft", "und", "ih\u00b7ren", "Ban\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Er erz\u00e4hlt: wie Gott der Herr", "tokens": ["Er", "er\u00b7z\u00e4hlt", ":", "wie", "Gott", "der", "Herr"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ward zu Bethlehem geboren", "tokens": ["Ward", "zu", "Beth\u00b7le\u00b7hem", "ge\u00b7bo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der Jungfrau, welche niemals", "tokens": ["Von", "der", "Jung\u00b7frau", ",", "wel\u00b7che", "nie\u00b7mals"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihre Jungferschaft verloren;", "tokens": ["Ih\u00b7re", "Jung\u00b7fer\u00b7schaft", "ver\u00b7lo\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.30": {"line.1": {"text": "Wie der Herr der Welt gelegen", "tokens": ["Wie", "der", "Herr", "der", "Welt", "ge\u00b7le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In der Krippe, und ein K\u00fchlein", "tokens": ["In", "der", "Krip\u00b7pe", ",", "und", "ein", "K\u00fch\u00b7lein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "KON", "ART", "NN"], "meter": "--+--+--", "measure": "anapaest.di.plus"}, "line.3": {"text": "Und ein \u00d6chslein bei ihm stunden,", "tokens": ["Und", "ein", "\u00d6chs\u00b7lein", "bei", "ihm", "stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "PPER", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schier and\u00e4chtig, zwei Rindviehlein.", "tokens": ["Schier", "an\u00b7d\u00e4ch\u00b7tig", ",", "zwei", "Rind\u00b7vieh\u00b7lein", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "CARD", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.31": {"line.1": {"text": "Er erz\u00e4hlte: wie der Herr", "tokens": ["Er", "er\u00b7z\u00e4hl\u00b7te", ":", "wie", "der", "Herr"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor den Schergen des Herodes", "tokens": ["Vor", "den", "Scher\u00b7gen", "des", "He\u00b7ro\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nach \u00c4gypten floh, und sp\u00e4ter", "tokens": ["Nach", "\u00c4\u00b7gyp\u00b7ten", "floh", ",", "und", "sp\u00e4\u00b7ter"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "$,", "KON", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Litt die herbe Pein des Todes", "tokens": ["Litt", "die", "her\u00b7be", "Pein", "des", "To\u00b7des"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "Unter Pontio Pilato,", "tokens": ["Un\u00b7ter", "Pon\u00b7tio", "Pi\u00b7la\u00b7to", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Der das Urteil unterschrieben,", "tokens": ["Der", "das", "Ur\u00b7teil", "un\u00b7ter\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von den harten Pharis\u00e4ern,", "tokens": ["Von", "den", "har\u00b7ten", "Pha\u00b7ri\u00b7s\u00e4\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Von den Juden angetrieben.", "tokens": ["Von", "den", "Ju\u00b7den", "an\u00b7ge\u00b7trie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Er erz\u00e4hlte: wie der Herr,", "tokens": ["Er", "er\u00b7z\u00e4hl\u00b7te", ":", "wie", "der", "Herr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Der entstiegen seinem Grabe", "tokens": ["Der", "ent\u00b7stie\u00b7gen", "sei\u00b7nem", "Gra\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schon am dritten Tag, gen Himmel", "tokens": ["Schon", "am", "drit\u00b7ten", "Tag", ",", "gen", "Him\u00b7mel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$,", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Seinen Flug genommen habe;", "tokens": ["Sei\u00b7nen", "Flug", "ge\u00b7nom\u00b7men", "ha\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.34": {"line.1": {"text": "Wie er aber, wenn es Zeit ist,", "tokens": ["Wie", "er", "a\u00b7ber", ",", "wenn", "es", "Zeit", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "$,", "KOUS", "PPER", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wiederkehren auf die Erde", "tokens": ["Wie\u00b7der\u00b7keh\u00b7ren", "auf", "die", "Er\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und zu Josaphat die Toten", "tokens": ["Und", "zu", "Jo\u00b7sa\u00b7phat", "die", "To\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und Lebend'gen richten werde.", "tokens": ["Und", "Le\u00b7ben\u00b7d'\u00b7gen", "rich\u00b7ten", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "\u00bbzittert, Juden!\u00ab rief der M\u00f6nch,", "tokens": ["\u00bb", "zit\u00b7tert", ",", "Ju\u00b7den", "!", "\u00ab", "rief", "der", "M\u00f6nch", ","], "token_info": ["punct", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NN", "$.", "$(", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbvor dem Gott, den ihr mit Hieben", "tokens": ["\u00bb", "vor", "dem", "Gott", ",", "den", "ihr", "mit", "Hie\u00b7ben"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und mit Dornen habt gemartert,", "tokens": ["Und", "mit", "Dor\u00b7nen", "habt", "ge\u00b7mar\u00b7tert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Den ihr in den Tod getrieben.", "tokens": ["Den", "ihr", "in", "den", "Tod", "ge\u00b7trie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "Seine M\u00f6rder, Volk der Rachsucht,", "tokens": ["Sei\u00b7ne", "M\u00f6r\u00b7der", ",", "Volk", "der", "Rach\u00b7sucht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Juden, das seid ihr gewesen \u2013", "tokens": ["Ju\u00b7den", ",", "das", "seid", "ihr", "ge\u00b7we\u00b7sen", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PDS", "VAFIN", "PPER", "VAPP", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Immer meuchelt ihr den Heiland,", "tokens": ["Im\u00b7mer", "meu\u00b7chelt", "ihr", "den", "Hei\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welcher kommt, euch zu erl\u00f6sen.", "tokens": ["Wel\u00b7cher", "kommt", ",", "euch", "zu", "er\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAT", "VVFIN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.37": {"line.1": {"text": "Judenvolk, du bist ein Aas,", "tokens": ["Ju\u00b7den\u00b7volk", ",", "du", "bist", "ein", "Aas", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Worin hausen die D\u00e4monen;", "tokens": ["Wo\u00b7rin", "hau\u00b7sen", "die", "D\u00e4\u00b7mo\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eure Leiber sind Kasernen", "tokens": ["Eu\u00b7re", "Lei\u00b7ber", "sind", "Ka\u00b7ser\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr des Teufels Legionen.", "tokens": ["F\u00fcr", "des", "Teu\u00b7fels", "Le\u00b7gi\u00b7o\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.38": {"line.1": {"text": "Thomas von Aquino sagt es,", "tokens": ["Tho\u00b7mas", "von", "A\u00b7qui\u00b7no", "sagt", "es", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "VVFIN", "PPER", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Den man nennt den gro\u00dfen Ochsen", "tokens": ["Den", "man", "nennt", "den", "gro\u00b7\u00dfen", "Och\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der Gelehrsamkeit, er ist", "tokens": ["Der", "Ge\u00b7lehr\u00b7sam\u00b7keit", ",", "er", "ist"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PPER", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Licht und Lust der Orthodoxen.", "tokens": ["Licht", "und", "Lust", "der", "Or\u00b7tho\u00b7do\u00b7xen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.39": {"line.1": {"text": "Judenvolk, ihr seid Hy\u00e4nen,", "tokens": ["Ju\u00b7den\u00b7volk", ",", "ihr", "seid", "Hy\u00b7\u00e4\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "W\u00f6lfe, Schakals, die in Gr\u00e4bern", "tokens": ["W\u00f6l\u00b7fe", ",", "Scha\u00b7kals", ",", "die", "in", "Gr\u00e4\u00b7bern"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "PRELS", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fchlen, um der Toten Leichnam'", "tokens": ["W\u00fch\u00b7len", ",", "um", "der", "To\u00b7ten", "Leich\u00b7nam'"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUI", "ART", "NN", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Blutfra\u00dfgierig aufzust\u00f6bern.", "tokens": ["Blut\u00b7fra\u00df\u00b7gie\u00b7rig", "auf\u00b7zu\u00b7st\u00f6\u00b7bern", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.40": {"line.1": {"text": "Juden, Juden, ihr seid S\u00e4ue,", "tokens": ["Ju\u00b7den", ",", "Ju\u00b7den", ",", "ihr", "seid", "S\u00e4u\u00b7e", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Paviane, Nashorntiere,", "tokens": ["Pa\u00b7vi\u00b7a\u00b7ne", ",", "Nas\u00b7horn\u00b7tie\u00b7re", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die man nennt Rhinozerosse,", "tokens": ["Die", "man", "nennt", "Rhi\u00b7no\u00b7ze\u00b7ros\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Krokodile und Vampire.", "tokens": ["Kro\u00b7ko\u00b7di\u00b7le", "und", "Vam\u00b7pi\u00b7re", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.41": {"line.1": {"text": "Ihr seid Raben, Eulen, Uhus,", "tokens": ["Ihr", "seid", "Ra\u00b7ben", ",", "Eu\u00b7len", ",", "U\u00b7hus", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "NE", "$,", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Flederm\u00e4use, Wiedeh\u00f6pfe,", "tokens": ["Fle\u00b7der\u00b7m\u00e4u\u00b7se", ",", "Wie\u00b7de\u00b7h\u00f6p\u00b7fe", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Leichenh\u00fchner, Basilisken,", "tokens": ["Lei\u00b7chen\u00b7h\u00fch\u00b7ner", ",", "Ba\u00b7si\u00b7lis\u00b7ken", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Galgenv\u00f6gel, Nachtgesch\u00f6pfe.", "tokens": ["Gal\u00b7gen\u00b7v\u00f6\u00b7gel", ",", "Nacht\u00b7ge\u00b7sch\u00f6p\u00b7fe", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.42": {"line.1": {"text": "Ihr seid Vipern und Blindschleichen,", "tokens": ["Ihr", "seid", "Vi\u00b7pern", "und", "Blind\u00b7schlei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Klapperschlangen, gift'ge Kr\u00f6ten,", "tokens": ["Klap\u00b7per\u00b7schlan\u00b7gen", ",", "gift'\u00b7ge", "Kr\u00f6\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ottern, Nattern \u2013 Christus wird", "tokens": ["Ot\u00b7tern", ",", "Nat\u00b7tern", "\u2013", "Chris\u00b7tus", "wird"], "token_info": ["word", "punct", "word", "punct", "word", "word"], "pos": ["NE", "$,", "NN", "$(", "NE", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Eu'r verfluchtes Haupt zertreten.", "tokens": ["Eu'r", "ver\u00b7fluch\u00b7tes", "Haupt", "zer\u00b7tre\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.43": {"line.1": {"text": "Oder wollt ihr, Maledeiten,", "tokens": ["O\u00b7der", "wollt", "ihr", ",", "Ma\u00b7le\u00b7dei\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Eure armen Seelen retten?", "tokens": ["Eu\u00b7re", "ar\u00b7men", "See\u00b7len", "ret\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aus der Bosheit Synagoge", "tokens": ["Aus", "der", "Bos\u00b7heit", "Syn\u00b7a\u00b7go\u00b7ge"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Fl\u00fcchtet nach den frommen St\u00e4tten,", "tokens": ["Fl\u00fcch\u00b7tet", "nach", "den", "from\u00b7men", "St\u00e4t\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.44": {"line.1": {"text": "Nach der Liebe lichtem Dome,", "tokens": ["Nach", "der", "Lie\u00b7be", "lich\u00b7tem", "Do\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo im benedeiten Becken", "tokens": ["Wo", "im", "be\u00b7ne\u00b7dei\u00b7ten", "Be\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Euch der Quell der Gnade sprudelt \u2013", "tokens": ["Euch", "der", "Quell", "der", "Gna\u00b7de", "spru\u00b7delt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "ART", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Drin sollt ihr die K\u00f6pfe stecken \u2013", "tokens": ["Drin", "sollt", "ihr", "die", "K\u00f6p\u00b7fe", "ste\u00b7cken", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.45": {"line.1": {"text": "Wascht dort ab den alten Adam", "tokens": ["Wascht", "dort", "ab", "den", "al\u00b7ten", "A\u00b7dam"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die Laster, die ihn schw\u00e4rzen;", "tokens": ["Und", "die", "Las\u00b7ter", ",", "die", "ihn", "schw\u00e4r\u00b7zen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Des verj\u00e4hrten Grolles Schimmel,", "tokens": ["Des", "ver\u00b7j\u00e4hr\u00b7ten", "Grol\u00b7les", "Schim\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wascht ihn ab von euren Herzen!", "tokens": ["Wascht", "ihn", "ab", "von", "eu\u00b7ren", "Her\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.46": {"line.1": {"text": "H\u00f6rt ihr nicht des Heilands Stimme?", "tokens": ["H\u00f6rt", "ihr", "nicht", "des", "Hei\u00b7lands", "Stim\u00b7me", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Euren neuen Namen rief er \u2013", "tokens": ["Eu\u00b7ren", "neu\u00b7en", "Na\u00b7men", "rief", "er", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Lauset euch an Christi Brust", "tokens": ["Lau\u00b7set", "euch", "an", "Chris\u00b7ti", "Brust"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NE", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Von der S\u00fcnde Ungeziefer!", "tokens": ["Von", "der", "S\u00fcn\u00b7de", "Un\u00b7ge\u00b7zie\u00b7fer", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.47": {"line.1": {"text": "Unser Gott, der ist die Liebe,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "die", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und er gleichet einem Lamme;", "tokens": ["Und", "er", "glei\u00b7chet", "ei\u00b7nem", "Lam\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Um zu s\u00fchnen unsre Schuld,", "tokens": ["Um", "zu", "s\u00fch\u00b7nen", "uns\u00b7re", "Schuld", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Starb er an des Kreuzes Stamme.", "tokens": ["Starb", "er", "an", "des", "Kreu\u00b7zes", "Stam\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.48": {"line.1": {"text": "Unser Gott, der ist die Liebe,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "die", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jesus Christus ist sein Name;", "tokens": ["Je\u00b7sus", "Chris\u00b7tus", "ist", "sein", "Na\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Seine Duldsamkeit und Demut", "tokens": ["Sei\u00b7ne", "Duld\u00b7sam\u00b7keit", "und", "De\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Suchen wir stets nachzuahmen.", "tokens": ["Su\u00b7chen", "wir", "stets", "nach\u00b7zu\u00b7ah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.49": {"line.1": {"text": "Deshalb sind wir auch so sanft,", "tokens": ["Des\u00b7halb", "sind", "wir", "auch", "so", "sanft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So leutselig, ruhig, milde,", "tokens": ["So", "leut\u00b7se\u00b7lig", ",", "ru\u00b7hig", ",", "mil\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Hadern niemals, nach des Lammes,", "tokens": ["Ha\u00b7dern", "nie\u00b7mals", ",", "nach", "des", "Lam\u00b7mes", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Des Vers\u00f6hners, Musterbilde.", "tokens": ["Des", "Ver\u00b7s\u00f6h\u00b7ners", ",", "Mus\u00b7ter\u00b7bil\u00b7de", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.50": {"line.1": {"text": "Einst im Himmel werden wir", "tokens": ["Einst", "im", "Him\u00b7mel", "wer\u00b7den", "wir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPRART", "NN", "VAFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ganz verkl\u00e4rt zu frommen Englein,", "tokens": ["Ganz", "ver\u00b7kl\u00e4rt", "zu", "from\u00b7men", "En\u00b7glein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "PTKZU", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und wir wandeln dort gottselig,", "tokens": ["Und", "wir", "wan\u00b7deln", "dort", "gott\u00b7se\u00b7lig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In den H\u00e4nden Lilienstenglein.", "tokens": ["In", "den", "H\u00e4n\u00b7den", "Li\u00b7li\u00b7ens\u00b7ten\u00b7glein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.51": {"line.1": {"text": "Statt der groben Kutten tragen", "tokens": ["Statt", "der", "gro\u00b7ben", "Kut\u00b7ten", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wir die reinlichsten Gew\u00e4nder", "tokens": ["Wir", "die", "rein\u00b7lichs\u00b7ten", "Ge\u00b7w\u00e4n\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "ADJA", "NN"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.3": {"text": "Von Muss'lin, Brokat und Seide,", "tokens": ["Von", "Muss'\u00b7lin", ",", "Bro\u00b7kat", "und", "Sei\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "KON", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Goldne Troddeln, bunte B\u00e4nder.", "tokens": ["Gold\u00b7ne", "Trod\u00b7deln", ",", "bun\u00b7te", "B\u00e4n\u00b7der", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.52": {"line.1": {"text": "Keine Glatze mehr! Goldlocken", "tokens": ["Kei\u00b7ne", "Glat\u00b7ze", "mehr", "!", "Gold\u00b7lo\u00b7cken"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PIAT", "NN", "ADV", "$.", "NN"], "meter": "+-+--++-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Flattern dort um unsre K\u00f6pfe;", "tokens": ["Flat\u00b7tern", "dort", "um", "uns\u00b7re", "K\u00f6p\u00b7fe", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Allerliebste Jungfraun flechten", "tokens": ["Al\u00b7ler\u00b7liebs\u00b7te", "Jung\u00b7fraun", "flech\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Uns das Haar in h\u00fcbsche Z\u00f6pfe.", "tokens": ["Uns", "das", "Haar", "in", "h\u00fcb\u00b7sche", "Z\u00f6p\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.53": {"line.1": {"text": "Weinpokale wird es droben", "tokens": ["Wein\u00b7po\u00b7ka\u00b7le", "wird", "es", "dro\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Von viel weiterm Umfang geben,", "tokens": ["Von", "viel", "wei\u00b7term", "Um\u00b7fang", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als die Becher sind hier unten,", "tokens": ["Als", "die", "Be\u00b7cher", "sind", "hier", "un\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VAFIN", "ADV", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Worin sch\u00e4umt der Saft der Reben.", "tokens": ["Wo\u00b7rin", "sch\u00e4umt", "der", "Saft", "der", "Re\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.54": {"line.1": {"text": "Doch im Gegenteil viel enger", "tokens": ["Doch", "im", "Ge\u00b7gen\u00b7teil", "viel", "en\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ADV", "ADJD"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Als ein Weibermund hienieden,", "tokens": ["Als", "ein", "Wei\u00b7ber\u00b7mund", "hien\u00b7ie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wird das Frauenm\u00fcndchen sein,", "tokens": ["Wird", "das", "Frau\u00b7en\u00b7m\u00fcnd\u00b7chen", "sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das dort oben uns beschieden.", "tokens": ["Das", "dort", "o\u00b7ben", "uns", "be\u00b7schie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.55": {"line.1": {"text": "Trinkend, k\u00fcssend, lachend wollen", "tokens": ["Trin\u00b7kend", ",", "k\u00fcs\u00b7send", ",", "la\u00b7chend", "wol\u00b7len"], "token_info": ["word", "punct", "word", "punct", "word", "word"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "VMFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wir die Ewigkeit verbringen,", "tokens": ["Wir", "die", "E\u00b7wig\u00b7keit", "ver\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und verz\u00fcckt Halleluja,", "tokens": ["Und", "ver\u00b7z\u00fcckt", "Hal\u00b7le\u00b7lu\u00b7ja", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "$,"], "meter": "+-+---+", "measure": "unknown.measure.tri"}, "line.4": {"text": "Kyrie eleison singen.\u00ab", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "sin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "NE", "VVINF", "$.", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.56": {"line.1": {"text": "Also schlo\u00df der Christ. Die M\u00f6nchlein", "tokens": ["Al\u00b7so", "schlo\u00df", "der", "Christ", ".", "Die", "M\u00f6nch\u00b7lein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Glaubten schon, Erleuchtung tr\u00e4te", "tokens": ["Glaub\u00b7ten", "schon", ",", "Er\u00b7leuch\u00b7tung", "tr\u00e4\u00b7te"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In die Herzen, und sie schleppten", "tokens": ["In", "die", "Her\u00b7zen", ",", "und", "sie", "schlepp\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "KON", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Flink herbei das Taufger\u00e4te.", "tokens": ["Flink", "her\u00b7bei", "das", "Tauf\u00b7ge\u00b7r\u00e4\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.57": {"line.1": {"text": "Doch die wasserscheuen Juden", "tokens": ["Doch", "die", "was\u00b7ser\u00b7scheu\u00b7en", "Ju\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sch\u00fctteln sich und grinsen schn\u00f6de.", "tokens": ["Sch\u00fct\u00b7teln", "sich", "und", "grin\u00b7sen", "schn\u00f6\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "KON", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Rabbi Juda, der Navarrer,", "tokens": ["Rab\u00b7bi", "Ju\u00b7da", ",", "der", "Na\u00b7var\u00b7rer", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hub jetzt an die Gegenrede:", "tokens": ["Hub", "jetzt", "an", "die", "Ge\u00b7gen\u00b7re\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.58": {"line.1": {"text": "\u00bbum f\u00fcr deine Saat zu d\u00fcngen", "tokens": ["\u00bb", "um", "f\u00fcr", "dei\u00b7ne", "Saat", "zu", "d\u00fcn\u00b7gen"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KOUI", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Meines Geistes d\u00fcrren Acker,", "tokens": ["Mei\u00b7nes", "Geis\u00b7tes", "d\u00fcr\u00b7ren", "A\u00b7cker", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit Mistkarren voll Schimpfw\u00f6rter", "tokens": ["Mit", "Mist\u00b7kar\u00b7ren", "voll", "Schimpf\u00b7w\u00f6r\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hast du mich beschmissen wacker.", "tokens": ["Hast", "du", "mich", "be\u00b7schmis\u00b7sen", "wa\u00b7cker", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADJD", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.59": {"line.1": {"text": "So folgt jeder der Methode,", "tokens": ["So", "folgt", "je\u00b7der", "der", "Me\u00b7tho\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dran er nun einmal gew\u00f6hnet,", "tokens": ["Dran", "er", "nun", "ein\u00b7mal", "ge\u00b7w\u00f6h\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und anstatt dich drob zu schelten,", "tokens": ["Und", "an\u00b7statt", "dich", "drob", "zu", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sag ich Dank dir, wohlvers\u00f6hnet.", "tokens": ["Sag", "ich", "Dank", "dir", ",", "wohl\u00b7ver\u00b7s\u00f6h\u00b7net", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PPER", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.60": {"line.1": {"text": "Die Dreieinigkeitsdoktrin", "tokens": ["Die", "Drei\u00b7ei\u00b7nig\u00b7keits\u00b7dokt\u00b7rin"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kann f\u00fcr unsre Leut' nicht passen,", "tokens": ["Kann", "f\u00fcr", "uns\u00b7re", "Leut'", "nicht", "pas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PPOSAT", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die mit Regula-de-tri", "tokens": ["Die", "mit", "Re\u00b7gu\u00b7la\u00b7de\u00b7tri"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sich von Jugend auf befassen.", "tokens": ["Sich", "von", "Ju\u00b7gend", "auf", "be\u00b7fas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "APPR", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Da\u00df in deinem Gotte drei,", "tokens": ["Da\u00df", "in", "dei\u00b7nem", "Got\u00b7te", "drei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPOSAT", "NN", "CARD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Drei Personen sind enthalten,", "tokens": ["Drei", "Per\u00b7so\u00b7nen", "sind", "ent\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ist bescheiden noch, sechstausend", "tokens": ["Ist", "be\u00b7schei\u00b7den", "noch", ",", "sech\u00b7stau\u00b7send"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["VAFIN", "ADJD", "ADV", "$,", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "G\u00f6tter gab es bei den Alten.", "tokens": ["G\u00f6t\u00b7ter", "gab", "es", "bei", "den", "Al\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.62": {"line.1": {"text": "Unbekannt ist mir der Gott,", "tokens": ["Un\u00b7be\u00b7kannt", "ist", "mir", "der", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Den ihr Christum pflegt zu nennen;", "tokens": ["Den", "ihr", "Chris\u00b7tum", "pflegt", "zu", "nen\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Seine Jungfer Mutter gleichfalls", "tokens": ["Sei\u00b7ne", "Jung\u00b7fer", "Mut\u00b7ter", "gleich\u00b7falls"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hab ich nicht die Ehr' zu kennen.", "tokens": ["Hab", "ich", "nicht", "die", "Ehr'", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.63": {"line.1": {"text": "Ich bedaure, da\u00df er einst,", "tokens": ["Ich", "be\u00b7dau\u00b7re", ",", "da\u00df", "er", "einst", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor etwa zw\u00f6lfhundert Jahren,", "tokens": ["Vor", "et\u00b7wa", "zw\u00f6lf\u00b7hun\u00b7dert", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ein'ge Unannehmlichkeiten", "tokens": ["Ein'\u00b7ge", "Un\u00b7an\u00b7nehm\u00b7lich\u00b7kei\u00b7ten"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu Jerusalem erfahren.", "tokens": ["Zu", "Je\u00b7ru\u00b7sa\u00b7lem", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.64": {"line.1": {"text": "Ob die Juden ihn get\u00f6tet,", "tokens": ["Ob", "die", "Ju\u00b7den", "ihn", "ge\u00b7t\u00f6\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Das ist schwer jetzt zu erkunden,", "tokens": ["Das", "ist", "schwer", "jetzt", "zu", "er\u00b7kun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "ADV", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da ja das Corpus delicti", "tokens": ["Da", "ja", "das", "Cor\u00b7pus", "de\u00b7lic\u00b7ti"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NE", "NE"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Schon am dritten Tag verschwunden.", "tokens": ["Schon", "am", "drit\u00b7ten", "Tag", "ver\u00b7schwun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.65": {"line.1": {"text": "Da\u00df er ein Verwandter sei", "tokens": ["Da\u00df", "er", "ein", "Ver\u00b7wand\u00b7ter", "sei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Unsres Gottes, ist nicht minder", "tokens": ["Uns\u00b7res", "Got\u00b7tes", ",", "ist", "nicht", "min\u00b7der"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "VAFIN", "PTKNEG", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zweifelhaft; soviel wir wissen,", "tokens": ["Zwei\u00b7fel\u00b7haft", ";", "so\u00b7viel", "wir", "wis\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat der letztre keine Kinder.", "tokens": ["Hat", "der", "letz\u00b7tre", "kei\u00b7ne", "Kin\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.66": {"line.1": {"text": "Unser Gott ist nicht gestorben", "tokens": ["Un\u00b7ser", "Gott", "ist", "nicht", "ge\u00b7stor\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als ein armes L\u00e4mmerschw\u00e4nzchen", "tokens": ["Als", "ein", "ar\u00b7mes", "L\u00e4m\u00b7mer\u00b7schw\u00e4nz\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "F\u00fcr die Menschheit, ist kein s\u00fc\u00dfes", "tokens": ["F\u00fcr", "die", "Menschheit", ",", "ist", "kein", "s\u00fc\u00b7\u00dfes"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "VAFIN", "PIAT", "ADJA"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Philantr\u00f6pfchen, Faselh\u00e4nschen.", "tokens": ["Phil\u00b7an\u00b7tr\u00f6pf\u00b7chen", ",", "Fa\u00b7sel\u00b7h\u00e4n\u00b7schen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.67": {"line.1": {"text": "Unser Gott ist nicht die Liebe;", "tokens": ["Un\u00b7ser", "Gott", "ist", "nicht", "die", "Lie\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schn\u00e4beln ist nicht seine Sache,", "tokens": ["Schn\u00e4\u00b7beln", "ist", "nicht", "sei\u00b7ne", "Sa\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PTKNEG", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn er ist ein Donnergott", "tokens": ["Denn", "er", "ist", "ein", "Don\u00b7ner\u00b7gott"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er ist ein Gott der Rache.", "tokens": ["Und", "er", "ist", "ein", "Gott", "der", "Ra\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.68": {"line.1": {"text": "Seines Zornes Blitze treffen", "tokens": ["Sei\u00b7nes", "Zor\u00b7nes", "Blit\u00b7ze", "tref\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unerbittlich jeden S\u00fcnder,", "tokens": ["Un\u00b7er\u00b7bitt\u00b7lich", "je\u00b7den", "S\u00fcn\u00b7der", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und des Vaters Schulden b\u00fc\u00dfen", "tokens": ["Und", "des", "Va\u00b7ters", "Schul\u00b7den", "b\u00fc\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Oft die sp\u00e4ten Enkelkinder.", "tokens": ["Oft", "die", "sp\u00e4\u00b7ten", "En\u00b7kel\u00b7kin\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.69": {"line.1": {"text": "Unser Gott, der ist lebendig,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "le\u00b7ben\u00b7dig", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und in seiner Himmelshalle", "tokens": ["Und", "in", "sei\u00b7ner", "Him\u00b7mels\u00b7hal\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Existieret er drauflos", "tokens": ["E\u00b7xis\u00b7tie\u00b7ret", "er", "drauf\u00b7los"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Durch die Ewigkeiten alle.", "tokens": ["Durch", "die", "E\u00b7wig\u00b7kei\u00b7ten", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PIAT", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.70": {"line.1": {"text": "Unser Gott, und der ist auch", "tokens": ["Un\u00b7ser", "Gott", ",", "und", "der", "ist", "auch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KON", "ART", "VAFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ein gesunder Gott, kein Mythos", "tokens": ["Ein", "ge\u00b7sun\u00b7der", "Gott", ",", "kein", "My\u00b7thos"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PIAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Bleich und d\u00fcnne wie Oblaten", "tokens": ["Bleich", "und", "d\u00fcn\u00b7ne", "wie", "Ob\u00b7la\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "KOKOM", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Oder Schatten am Cocytos.", "tokens": ["O\u00b7der", "Schat\u00b7ten", "am", "Co\u00b7cy\u00b7tos", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPRART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.71": {"line.1": {"text": "Unser Gott ist stark. In H\u00e4nden", "tokens": ["Un\u00b7ser", "Gott", "ist", "stark", ".", "In", "H\u00e4n\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$.", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tr\u00e4gt er Sonne, Mond, Gestirne;", "tokens": ["Tr\u00e4gt", "er", "Son\u00b7ne", ",", "Mond", ",", "Ge\u00b7stir\u00b7ne", ";"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Throne brechen, V\u00f6lker schwinden,", "tokens": ["Thro\u00b7ne", "bre\u00b7chen", ",", "V\u00f6l\u00b7ker", "schwin\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn er runzelt seine Stirne.", "tokens": ["Wenn", "er", "run\u00b7zelt", "sei\u00b7ne", "Stir\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.72": {"line.1": {"text": "Und er ist ein gro\u00dfer Gott.", "tokens": ["Und", "er", "ist", "ein", "gro\u00b7\u00dfer", "Gott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "David singt: Ermessen lie\u00dfe", "tokens": ["Da\u00b7vid", "singt", ":", "Er\u00b7mes\u00b7sen", "lie\u00b7\u00dfe"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NE", "VVFIN", "$.", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sich die Gr\u00f6\u00dfe nicht, die Erde", "tokens": ["Sich", "die", "Gr\u00f6\u00b7\u00dfe", "nicht", ",", "die", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PRF", "ART", "NN", "PTKNEG", "$,", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sei der Schemel seiner F\u00fc\u00dfe.", "tokens": ["Sei", "der", "Sche\u00b7mel", "sei\u00b7ner", "F\u00fc\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.73": {"line.1": {"text": "Unser Gott liebt die Musik,", "tokens": ["Un\u00b7ser", "Gott", "liebt", "die", "Mu\u00b7sik", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Saitenspiel und Festges\u00e4nge;", "tokens": ["Sai\u00b7ten\u00b7spiel", "und", "Fest\u00b7ge\u00b7s\u00e4n\u00b7ge", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch wie Ferkelgrunzen sind", "tokens": ["Doch", "wie", "Fer\u00b7kel\u00b7grun\u00b7zen", "sind"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihm zuwider Glockenkl\u00e4nge.", "tokens": ["Ihm", "zu\u00b7wi\u00b7der", "Glo\u00b7cken\u00b7kl\u00e4n\u00b7ge", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.74": {"line.1": {"text": "Leviathan hei\u00dft der Fisch,", "tokens": ["Le\u00b7vi\u00b7a\u00b7than", "hei\u00dft", "der", "Fisch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Welcher haust im Meeresgrunde;", "tokens": ["Wel\u00b7cher", "haust", "im", "Mee\u00b7res\u00b7grun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit ihm spielet Gott der Herr", "tokens": ["Mit", "ihm", "spie\u00b7let", "Gott", "der", "Herr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "VVFIN", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alle Tage eine Stunde \u2013", "tokens": ["Al\u00b7le", "Ta\u00b7ge", "ei\u00b7ne", "Stun\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.75": {"line.1": {"text": "Ausgenommen an dem neunten", "tokens": ["Aus\u00b7ge\u00b7nom\u00b7men", "an", "dem", "neun\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tag des Monats Ab, wo n\u00e4mlich", "tokens": ["Tag", "des", "Mo\u00b7nats", "Ab", ",", "wo", "n\u00e4m\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "ART", "NN", "APPR", "$,", "PWAV", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einge\u00e4schert ward sein Tempel;", "tokens": ["Ein\u00b7ge\u00b7\u00e4\u00b7schert", "ward", "sein", "Tem\u00b7pel", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "An dem Tag ist er zu gr\u00e4mlich.", "tokens": ["An", "dem", "Tag", "ist", "er", "zu", "gr\u00e4m\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "PTKA", "ADJD", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.76": {"line.1": {"text": "Des Leviathans L\u00e4nge ist", "tokens": ["Des", "Le\u00b7vi\u00b7a\u00b7thans", "L\u00e4n\u00b7ge", "ist"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hundert Meilen, hat Flo\u00dffedern", "tokens": ["Hun\u00b7dert", "Mei\u00b7len", ",", "hat", "Flo\u00df\u00b7fe\u00b7dern"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["CARD", "NN", "$,", "VAFIN", "NN"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "Gro\u00df wie K\u00f6nig Ok von Basan,", "tokens": ["Gro\u00df", "wie", "K\u00f6\u00b7nig", "Ok", "von", "Ba\u00b7san", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "NN", "NE", "APPR", "NE", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und sein Schwanz ist wie ein Zedern.", "tokens": ["Und", "sein", "Schwanz", "ist", "wie", "ein", "Ze\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "KOKOM", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.77": {"line.1": {"text": "Doch sein Fleisch ist delikat,", "tokens": ["Doch", "sein", "Fleisch", "ist", "de\u00b7li\u00b7kat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Delikater als Schildkr\u00f6ten,", "tokens": ["De\u00b7li\u00b7ka\u00b7ter", "als", "Schild\u00b7kr\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KOUS", "NN", "$,"], "meter": "+-+-+++-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Und am Tag der Auferstehung", "tokens": ["Und", "am", "Tag", "der", "Auf\u00b7er\u00b7ste\u00b7hung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wird der Herr zu Tische beten", "tokens": ["Wird", "der", "Herr", "zu", "Ti\u00b7sche", "be\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "APPR", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.78": {"line.1": {"text": "Alle frommen Auserw\u00e4hlten,", "tokens": ["Al\u00b7le", "from\u00b7men", "Au\u00b7ser\u00b7w\u00e4hl\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Gerechten und die Weisen \u2013", "tokens": ["Die", "Ge\u00b7rech\u00b7ten", "und", "die", "Wei\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unsres Herrgotts Lieblingsfisch", "tokens": ["Uns\u00b7res", "Herr\u00b7gotts", "Lieb\u00b7lings\u00b7fisch"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Werden sie alsdann verspeisen,", "tokens": ["Wer\u00b7den", "sie", "als\u00b7dann", "ver\u00b7spei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.79": {"line.1": {"text": "Teils mit wei\u00dfer Knoblauchbr\u00fche,", "tokens": ["Teils", "mit", "wei\u00b7\u00dfer", "Knob\u00b7lauch\u00b7br\u00fc\u00b7he", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Teils auch braun in Wein gesotten,", "tokens": ["Teils", "auch", "braun", "in", "Wein", "ge\u00b7sot\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "APPR", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit Gew\u00fcrzen und Rosinen,", "tokens": ["Mit", "Ge\u00b7w\u00fcr\u00b7zen", "und", "Ro\u00b7si\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ungef\u00e4hr wie Mateloten.", "tokens": ["Un\u00b7ge\u00b7f\u00e4hr", "wie", "Ma\u00b7te\u00b7lo\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.80": {"line.1": {"text": "In der wei\u00dfen Knoblauchbr\u00fche", "tokens": ["In", "der", "wei\u00b7\u00dfen", "Knob\u00b7lauch\u00b7br\u00fc\u00b7he"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schwimmen kleine Sch\u00e4bchen Rettich \u2013", "tokens": ["Schwim\u00b7men", "klei\u00b7ne", "Sch\u00e4b\u00b7chen", "Ret\u00b7tich", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "NE", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "So bereitet, Frater Jose,", "tokens": ["So", "be\u00b7rei\u00b7tet", ",", "Fra\u00b7ter", "Jo\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "NN", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mundet dir das Fischlein, wett ich!", "tokens": ["Mun\u00b7det", "dir", "das", "Fisc\u00b7hlein", ",", "wett", "ich", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "$,", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.81": {"line.1": {"text": "Auch die braune ist so lecker,", "tokens": ["Auch", "die", "brau\u00b7ne", "ist", "so", "le\u00b7cker", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "N\u00e4mlich die Rosinensauce,", "tokens": ["N\u00e4m\u00b7lich", "die", "Ro\u00b7si\u00b7nen\u00b7sau\u00b7ce", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Sie wird himmlisch wohl behagen", "tokens": ["Sie", "wird", "himm\u00b7lisch", "wohl", "be\u00b7ha\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Deinem B\u00e4uchlein, Frater Jose.", "tokens": ["Dei\u00b7nem", "B\u00e4uch\u00b7lein", ",", "Fra\u00b7ter", "Jo\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.82": {"line.1": {"text": "Was Gott kocht, ist gut gekocht!", "tokens": ["Was", "Gott", "kocht", ",", "ist", "gut", "ge\u00b7kocht", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "ADJD", "VVPP", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "M\u00f6nchlein, nimm jetzt meinen Rat an,", "tokens": ["M\u00f6nch\u00b7lein", ",", "nimm", "jetzt", "mei\u00b7nen", "Rat", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVIMP", "ADV", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Opfre hin die alte Vorhaut", "tokens": ["Opf\u00b7re", "hin", "die", "al\u00b7te", "Vor\u00b7haut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und erquick dich am Leviathan.\u00ab", "tokens": ["Und", "er\u00b7quick", "dich", "am", "Le\u00b7vi\u00b7a\u00b7than", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVIMP", "PPER", "APPRART", "NN", "$.", "$("], "meter": "--+-+-+-+", "measure": "anapaest.init"}}, "stanza.83": {"line.1": {"text": "Also lockend sprach der Rabbi,", "tokens": ["Al\u00b7so", "lo\u00b7ckend", "sprach", "der", "Rab\u00b7bi", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lockend, k\u00f6dernd, heimlich schmunzelnd,", "tokens": ["Lo\u00b7ckend", ",", "k\u00f6\u00b7dernd", ",", "heim\u00b7lich", "schmun\u00b7zelnd", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und die Juden schwangen schon", "tokens": ["Und", "die", "Ju\u00b7den", "schwan\u00b7gen", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihre Messer wonnegrunzelnd,", "tokens": ["Ih\u00b7re", "Mes\u00b7ser", "won\u00b7ne\u00b7grun\u00b7zelnd", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.84": {"line.1": {"text": "Um als Sieger zu skalpieren", "tokens": ["Um", "als", "Sie\u00b7ger", "zu", "skal\u00b7pie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUI", "KOUS", "NN", "PTKZU", "VVINF"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Die verfallenen Vorh\u00e4ute,", "tokens": ["Die", "ver\u00b7fal\u00b7le\u00b7nen", "Vor\u00b7h\u00e4u\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "--+---+-", "measure": "anapaest.init"}, "line.3": {"text": "Wahre spolia opima", "tokens": ["Wah\u00b7re", "spo\u00b7lia", "o\u00b7pi\u00b7ma"], "token_info": ["word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "In dem wunderlichen Streite.", "tokens": ["In", "dem", "wun\u00b7der\u00b7li\u00b7chen", "Strei\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.85": {"line.1": {"text": "Doch die M\u00f6nche hielten fest", "tokens": ["Doch", "die", "M\u00f6n\u00b7che", "hiel\u00b7ten", "fest"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "PTKVZ"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "An dem v\u00e4terlichen Glauben", "tokens": ["An", "dem", "v\u00e4\u00b7ter\u00b7li\u00b7chen", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und an ihrer Vorhaut, lie\u00dfen", "tokens": ["Und", "an", "ih\u00b7rer", "Vor\u00b7haut", ",", "lie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["KON", "APPR", "PPOSAT", "NN", "$,", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sich derselben nicht berauben.", "tokens": ["Sich", "der\u00b7sel\u00b7ben", "nicht", "be\u00b7rau\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "PDS", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.86": {"line.1": {"text": "Nach dem Juden sprach aufs neue", "tokens": ["Nach", "dem", "Ju\u00b7den", "sprach", "aufs", "neu\u00b7e"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "APPRART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der katholische Bekehrer;", "tokens": ["Der", "ka\u00b7tho\u00b7li\u00b7sche", "Be\u00b7keh\u00b7rer", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wieder schimpft er, jedes Wort", "tokens": ["Wie\u00b7der", "schimpft", "er", ",", "je\u00b7des", "Wort"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist ein Nachttopf, und kein leerer.", "tokens": ["Ist", "ein", "Nacht\u00b7topf", ",", "und", "kein", "lee\u00b7rer", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "KON", "PIAT", "ADJA", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.87": {"line.1": {"text": "Darauf repliziert der Rabbi", "tokens": ["Da\u00b7rauf", "re\u00b7pli\u00b7ziert", "der", "Rab\u00b7bi"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Mit zur\u00fcckgehaltnem Eifer;", "tokens": ["Mit", "zu\u00b7r\u00fcck\u00b7ge\u00b7halt\u00b7nem", "Ei\u00b7fer", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie sein Herz auch \u00fcberkocht,", "tokens": ["Wie", "sein", "Herz", "auch", "\u00fc\u00b7ber\u00b7kocht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch verschluckt er seinen Geifer.", "tokens": ["Doch", "ver\u00b7schluckt", "er", "sei\u00b7nen", "Gei\u00b7fer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.88": {"line.1": {"text": "Er beruft sich auf die Mischna,", "tokens": ["Er", "be\u00b7ruft", "sich", "auf", "die", "Mischna", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommentare und Traktate;", "tokens": ["Kom\u00b7men\u00b7ta\u00b7re", "und", "Trak\u00b7ta\u00b7te", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "Bringt auch aus dem Tausves-Jontof", "tokens": ["Bringt", "auch", "aus", "dem", "Taus\u00b7ve\u00b7sJon\u00b7tof"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Viel beweisende Zitate.", "tokens": ["Viel", "be\u00b7wei\u00b7sen\u00b7de", "Zi\u00b7ta\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.89": {"line.1": {"text": "Aber welche Blasphemie", "tokens": ["A\u00b7ber", "wel\u00b7che", "Blas\u00b7phe\u00b7mie"], "token_info": ["word", "word", "word"], "pos": ["KON", "PWAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mu\u00dft er von dem M\u00f6nche h\u00f6ren!", "tokens": ["Mu\u00dft", "er", "von", "dem", "M\u00f6n\u00b7che", "h\u00f6\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieser sprach: der Tausves-Jontof", "tokens": ["Die\u00b7ser", "sprach", ":", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "$.", "ART", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "M\u00f6ge sich zum Teufel scheren.", "tokens": ["M\u00f6\u00b7ge", "sich", "zum", "Teu\u00b7fel", "sche\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPRART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.90": {"line.1": {"text": "\u00bbda h\u00f6rt alles auf, o Gott!\u00ab", "tokens": ["\u00bb", "da", "h\u00f6rt", "al\u00b7les", "auf", ",", "o", "Gott", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PIS", "PTKVZ", "$,", "FM", "NN", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kreischt der Rabbi jetzt entsetzlich;", "tokens": ["Kreischt", "der", "Rab\u00b7bi", "jetzt", "ent\u00b7setz\u00b7lich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und es rei\u00dft ihm die Geduld,", "tokens": ["Und", "es", "rei\u00dft", "ihm", "die", "Ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Rappelk\u00f6pfig wird er pl\u00f6tzlich.", "tokens": ["Rap\u00b7pel\u00b7k\u00f6p\u00b7fig", "wird", "er", "pl\u00f6tz\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.91": {"line.1": {"text": "\u00bbgilt nichts mehr der Tausves-Jontof,", "tokens": ["\u00bb", "gilt", "nichts", "mehr", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PIS", "ADV", "ART", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Was soll gelten? Zeter! Zeter!", "tokens": ["Was", "soll", "gel\u00b7ten", "?", "Ze\u00b7ter", "!", "Ze\u00b7ter", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWS", "VMFIN", "VVINF", "$.", "NN", "$.", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "R\u00e4che, Herr, die Missetat,", "tokens": ["R\u00e4\u00b7che", ",", "Herr", ",", "die", "Mis\u00b7se\u00b7tat", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Strafe, Herr, den \u00dcbelt\u00e4ter!", "tokens": ["Stra\u00b7fe", ",", "Herr", ",", "den", "\u00dc\u00b7belt\u00b7\u00e4\u00b7ter", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.92": {"line.1": {"text": "Denn der Tausves-Jontof, Gott,", "tokens": ["Denn", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof", ",", "Gott", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Das bist du! Und an dem frechen", "tokens": ["Das", "bist", "du", "!", "Und", "an", "dem", "fre\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PPER", "$.", "KON", "APPR", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Tausves-Jontof-Leugner mu\u00dft du", "tokens": ["Taus\u00b7ve\u00b7sJon\u00b7tof\u00b7Leug\u00b7ner", "mu\u00dft", "du"], "token_info": ["word", "word", "word"], "pos": ["NN", "VMFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Deines Namens Ehre r\u00e4chen.", "tokens": ["Dei\u00b7nes", "Na\u00b7mens", "Eh\u00b7re", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.93": {"line.1": {"text": "La\u00df den Abgrund ihn verschlingen,", "tokens": ["La\u00df", "den", "Ab\u00b7grund", "ihn", "ver\u00b7schlin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie des Korah b\u00f6se Rotte,", "tokens": ["Wie", "des", "Ko\u00b7rah", "b\u00f6\u00b7se", "Rot\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die sich wider dich emp\u00f6rt", "tokens": ["Die", "sich", "wi\u00b7der", "dich", "em\u00b7p\u00f6rt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PRF", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Durch Emeute und Komplotte.", "tokens": ["Durch", "E\u00b7meu\u00b7te", "und", "Kom\u00b7plot\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.94": {"line.1": {"text": "Donnre deinen besten Donner!", "tokens": ["Donn\u00b7re", "dei\u00b7nen", "bes\u00b7ten", "Don\u00b7ner", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Strafe, o mein Gott, den Frevel \u2013", "tokens": ["Stra\u00b7fe", ",", "o", "mein", "Gott", ",", "den", "Fre\u00b7vel", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "FM", "PPOSAT", "NN", "$,", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hattest du doch zu Sodoma", "tokens": ["Hat\u00b7test", "du", "doch", "zu", "So\u00b7do\u00b7ma"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und Gomorrha Pech und Schwefel!", "tokens": ["Und", "Go\u00b7morr\u00b7ha", "Pech", "und", "Schwe\u00b7fel", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.95": {"line.1": {"text": "Treffe, Herr, die Kapuziner,", "tokens": ["Tref\u00b7fe", ",", "Herr", ",", "die", "Ka\u00b7pu\u00b7zi\u00b7ner", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie du Pharaon getroffen,", "tokens": ["Wie", "du", "Pha\u00b7raon", "ge\u00b7trof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der uns nachgesetzt, als wir", "tokens": ["Der", "uns", "nach\u00b7ge\u00b7setzt", ",", "als", "wir"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PPER", "VVPP", "$,", "KOUS", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wohlbepackt davongeloffen.", "tokens": ["Wohl\u00b7be\u00b7packt", "da\u00b7von\u00b7ge\u00b7lof\u00b7fen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.96": {"line.1": {"text": "Hunderttausend Ritter folgten", "tokens": ["Hun\u00b7dert\u00b7tau\u00b7send", "Rit\u00b7ter", "folg\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["CARD", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Diesem K\u00f6nig von Mizrayim,", "tokens": ["Die\u00b7sem", "K\u00f6\u00b7nig", "von", "Miz\u00b7ra\u00b7yim", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "NE", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Stahlbepanzert, blanke Schwerter", "tokens": ["Stahl\u00b7be\u00b7pan\u00b7zert", ",", "blan\u00b7ke", "Schwer\u00b7ter"], "token_info": ["word", "punct", "word", "word"], "pos": ["VVFIN", "$,", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In den schrecklichen Jadayim.", "tokens": ["In", "den", "schreck\u00b7li\u00b7chen", "Ja\u00b7day\u00b7im", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.97": {"line.1": {"text": "Gott! da hast du ausgestreckt", "tokens": ["Gott", "!", "da", "hast", "du", "aus\u00b7ge\u00b7streckt"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "ADV", "VAFIN", "PPER", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Deine Jad, und samt dem Heere", "tokens": ["Dei\u00b7ne", "Jad", ",", "und", "samt", "dem", "Hee\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KON", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ward ertr\u00e4nkt, wie junge Katzen,", "tokens": ["Ward", "er\u00b7tr\u00e4nkt", ",", "wie", "jun\u00b7ge", "Kat\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "PWAV", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Pharao im Roten Meere.", "tokens": ["Pha\u00b7rao", "im", "Ro\u00b7ten", "Mee\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.98": {"line.1": {"text": "Treffe, Herr, die Kapuziner,", "tokens": ["Tref\u00b7fe", ",", "Herr", ",", "die", "Ka\u00b7pu\u00b7zi\u00b7ner", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zeige den infamen Schuften,", "tokens": ["Zei\u00b7ge", "den", "in\u00b7fa\u00b7men", "Schuf\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df die Blitze deines Zorns", "tokens": ["Da\u00df", "die", "Blit\u00b7ze", "dei\u00b7nes", "Zorns"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nicht verrauchten und verpufften.", "tokens": ["Nicht", "ver\u00b7rauch\u00b7ten", "und", "ver\u00b7puff\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "KON", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.99": {"line.1": {"text": "Deines Sieges Ruhm und Preis", "tokens": ["Dei\u00b7nes", "Sie\u00b7ges", "Ruhm", "und", "Preis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Will ich singen dann und sagen,", "tokens": ["Will", "ich", "sin\u00b7gen", "dann", "und", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "VVFIN", "ADV", "KON", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und dabei, wie Mirjam tat,", "tokens": ["Und", "da\u00b7bei", ",", "wie", "Mir\u00b7jam", "tat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "$,", "PWAV", "NE", "VVFIN", "$,"], "meter": "--+-+--", "measure": "anapaest.init"}, "line.4": {"text": "Tanzen und die Pauke schlagen.\u00ab", "tokens": ["Tan\u00b7zen", "und", "die", "Pau\u00b7ke", "schla\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "KON", "ART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.100": {"line.1": {"text": "In die Rede grimmig fiel", "tokens": ["In", "die", "Re\u00b7de", "grim\u00b7mig", "fiel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADJD", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Jetzt der M\u00f6nch dem Zornentflammten:", "tokens": ["Jetzt", "der", "M\u00f6nch", "dem", "Zor\u00b7nent\u00b7flamm\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbmag dich selbst der Herr verderben,", "tokens": ["\u00bb", "mag", "dich", "selbst", "der", "Herr", "ver\u00b7der\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dich Verfluchten und Verdammten!", "tokens": ["Dich", "Ver\u00b7fluch\u00b7ten", "und", "Ver\u00b7damm\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.101": {"line.1": {"text": "Trotzen kann ich deinen Teufeln,", "tokens": ["Trot\u00b7zen", "kann", "ich", "dei\u00b7nen", "Teu\u00b7feln", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deinem schmutz'gen Fliegengotte,", "tokens": ["Dei\u00b7nem", "schmutz'\u00b7gen", "Flie\u00b7gen\u00b7got\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Luzifer und Beelzebube,", "tokens": ["Lu\u00b7zi\u00b7fer", "und", "Beel\u00b7ze\u00b7bu\u00b7be", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Belial und Astarothe.", "tokens": ["Be\u00b7li\u00b7al", "und", "As\u00b7ta\u00b7ro\u00b7the", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.102": {"line.1": {"text": "Trotzen kann ich deinen Geistern,", "tokens": ["Trot\u00b7zen", "kann", "ich", "dei\u00b7nen", "Geis\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deinen dunkeln H\u00f6llenpossen,", "tokens": ["Dei\u00b7nen", "dun\u00b7keln", "H\u00f6l\u00b7len\u00b7pos\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn in mir ist Jesus Christus,", "tokens": ["Denn", "in", "mir", "ist", "Je\u00b7sus", "Chris\u00b7tus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "VAFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Habe seinen Leib genossen.", "tokens": ["Ha\u00b7be", "sei\u00b7nen", "Leib", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.103": {"line.1": {"text": "Christus ist mein Leibgericht,", "tokens": ["Chris\u00b7tus", "ist", "mein", "Leib\u00b7ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Schmeckt viel besser als Leviathan", "tokens": ["Schmeckt", "viel", "bes\u00b7ser", "als", "Le\u00b7vi\u00b7a\u00b7than"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADJD", "KOKOM", "NE"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Mit der wei\u00dfen Knoblauchsauce,", "tokens": ["Mit", "der", "wei\u00b7\u00dfen", "Knob\u00b7lauch\u00b7sau\u00b7ce", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Die vielleicht gekocht der Satan.", "tokens": ["Die", "viel\u00b7leicht", "ge\u00b7kocht", "der", "Sa\u00b7tan", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVPP", "ART", "NN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.104": {"line.1": {"text": "Ach! anstatt zu disputieren,", "tokens": ["Ach", "!", "an\u00b7statt", "zu", "dis\u00b7pu\u00b7tie\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "KOUI", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lieber m\u00f6cht ich schmoren, braten", "tokens": ["Lie\u00b7ber", "m\u00f6cht", "ich", "schmo\u00b7ren", ",", "bra\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ADJD", "VMFIN", "PPER", "VVINF", "$,", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf dem w\u00e4rmsten Scheiterhaufen", "tokens": ["Auf", "dem", "w\u00e4rms\u00b7ten", "Schei\u00b7ter\u00b7hau\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dich und deine Kameraden.\u00ab", "tokens": ["Dich", "und", "dei\u00b7ne", "Ka\u00b7me\u00b7ra\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "KON", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.105": {"line.1": {"text": "Also tost in Schimpf und Ernst", "tokens": ["Al\u00b7so", "tost", "in", "Schimpf", "und", "Ernst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Das Turnei f\u00fcr Gott und Glauben,", "tokens": ["Das", "Tur\u00b7nei", "f\u00fcr", "Gott", "und", "Glau\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch die K\u00e4mpen ganz vergeblich", "tokens": ["Doch", "die", "K\u00e4m\u00b7pen", "ganz", "ver\u00b7geb\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Kreischen, schelten, w\u00fcten, schnauben.", "tokens": ["Krei\u00b7schen", ",", "schel\u00b7ten", ",", "w\u00fc\u00b7ten", ",", "schnau\u00b7ben", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.106": {"line.1": {"text": "Schon zw\u00f6lf Stunden w\u00e4hrt der Kampf,", "tokens": ["Schon", "zw\u00f6lf", "Stun\u00b7den", "w\u00e4hrt", "der", "Kampf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Dem kein End' ist abzuschauen;", "tokens": ["Dem", "kein", "End'", "ist", "ab\u00b7zu\u00b7schau\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VAFIN", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fcde wird das Publikum,", "tokens": ["M\u00fc\u00b7de", "wird", "das", "Pub\u00b7li\u00b7kum", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und es schwitzen stark die Frauen.", "tokens": ["Und", "es", "schwit\u00b7zen", "stark", "die", "Frau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.107": {"line.1": {"text": "Auch der Hof wird ungeduldig,", "tokens": ["Auch", "der", "Hof", "wird", "un\u00b7ge\u00b7dul\u00b7dig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Manche Zofe g\u00e4hnt ein wenig.", "tokens": ["Man\u00b7che", "Zo\u00b7fe", "g\u00e4hnt", "ein", "we\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ART", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu der sch\u00f6nen K\u00f6nigin", "tokens": ["Zu", "der", "sch\u00f6\u00b7nen", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wendet fragend sich der K\u00f6nig:", "tokens": ["Wen\u00b7det", "fra\u00b7gend", "sich", "der", "K\u00f6\u00b7nig", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PRF", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.108": {"line.1": {"text": "\u00bbsagt mir, was ist Eure Meinung?", "tokens": ["\u00bb", "sagt", "mir", ",", "was", "ist", "Eu\u00b7re", "Mei\u00b7nung", "?"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PWS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wer hat recht von diesen beiden?", "tokens": ["Wer", "hat", "recht", "von", "die\u00b7sen", "bei\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "APPR", "PDAT", "PIAT", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Wollt Ihr f\u00fcr den Rabbi Euch", "tokens": ["Wollt", "Ihr", "f\u00fcr", "den", "Rab\u00b7bi", "Euch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "ART", "NN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Oder f\u00fcr den M\u00f6nch entscheiden?\u00ab", "tokens": ["O\u00b7der", "f\u00fcr", "den", "M\u00f6nch", "ent\u00b7schei\u00b7den", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.109": {"line.1": {"text": "Do\u00f1a Blanka schaut ihn an,", "tokens": ["Do\u00f1a", "Blan\u00b7ka", "schaut", "ihn", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Und wie sinnend ihre H\u00e4nde", "tokens": ["Und", "wie", "sin\u00b7nend", "ih\u00b7re", "H\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ADJD", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit verschr\u00e4nkten Fingern dr\u00fcckt sie", "tokens": ["Mit", "ver\u00b7schr\u00e4nk\u00b7ten", "Fin\u00b7gern", "dr\u00fcckt", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "An die Stirn und spricht am Ende:", "tokens": ["An", "die", "Stirn", "und", "spricht", "am", "En\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "VVFIN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.110": {"line.1": {"text": "\u00bbwelcher recht hat, wei\u00df ich nicht \u2013", "tokens": ["\u00bb", "wel\u00b7cher", "recht", "hat", ",", "wei\u00df", "ich", "nicht", "\u2013"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PRELS", "ADV", "VAFIN", "$,", "VVFIN", "PPER", "PTKNEG", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Doch es will mich schier bed\u00fcnken,", "tokens": ["Doch", "es", "will", "mich", "schier", "be\u00b7d\u00fcn\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PRF", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Rabbi und der M\u00f6nch,", "tokens": ["Da\u00df", "der", "Rab\u00b7bi", "und", "der", "M\u00f6nch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df sie alle beide stinken.\u00ab", "tokens": ["Da\u00df", "sie", "al\u00b7le", "bei\u00b7de", "stin\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PIAT", "PIS", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.111": {"line.1": {"text": "In der Aula zu Toledo", "tokens": ["In", "der", "Au\u00b7la", "zu", "To\u00b7le\u00b7do"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NE", "APPR", "NE"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Klingen schmetternd die Fanfaren;", "tokens": ["Klin\u00b7gen", "schmet\u00b7ternd", "die", "Fan\u00b7fa\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu dem geistlichen Turnei", "tokens": ["Zu", "dem", "geist\u00b7li\u00b7chen", "Tur\u00b7nei"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wallt das Volk in bunten Scharen.", "tokens": ["Wallt", "das", "Volk", "in", "bun\u00b7ten", "Scha\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.112": {"line.1": {"text": "Das ist nicht ein weltlich Stechen,", "tokens": ["Das", "ist", "nicht", "ein", "welt\u00b7lich", "Ste\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKNEG", "ART", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Keine Eisenwaffe blitzet \u2013", "tokens": ["Kei\u00b7ne", "Ei\u00b7sen\u00b7waf\u00b7fe", "blit\u00b7zet", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eine Lanze ist das Wort,", "tokens": ["Ei\u00b7ne", "Lan\u00b7ze", "ist", "das", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das scholastisch scharf gespitzet.", "tokens": ["Das", "scho\u00b7las\u00b7tisch", "scharf", "ge\u00b7spit\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.113": {"line.1": {"text": "Nicht galante Paladins", "tokens": ["Nicht", "ga\u00b7lan\u00b7te", "Pa\u00b7la\u00b7dins"], "token_info": ["word", "word", "word"], "pos": ["PTKNEG", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Fechten hier, nicht Damendiener \u2013", "tokens": ["Fech\u00b7ten", "hier", ",", "nicht", "Da\u00b7men\u00b7die\u00b7ner", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "PTKNEG", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieses Kampfes Ritter sind", "tokens": ["Die\u00b7ses", "Kamp\u00b7fes", "Rit\u00b7ter", "sind"], "token_info": ["word", "word", "word", "word"], "pos": ["PDAT", "NN", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Kapuziner und Rabbiner.", "tokens": ["Ka\u00b7pu\u00b7zi\u00b7ner", "und", "Rab\u00b7bi\u00b7ner", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.114": {"line.1": {"text": "Statt des Helmes tragen sie", "tokens": ["Statt", "des", "Hel\u00b7mes", "tra\u00b7gen", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "VVFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Schabbesdeckel und Kapuzen;", "tokens": ["Schab\u00b7bes\u00b7de\u00b7ckel", "und", "Ka\u00b7pu\u00b7zen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Skapulier und Arbekanfe\u00df", "tokens": ["Ska\u00b7pu\u00b7lier", "und", "Ar\u00b7be\u00b7kan\u00b7fe\u00df"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Sind der Harnisch, drob sie trutzen.", "tokens": ["Sind", "der", "Har\u00b7nisch", ",", "drob", "sie", "trut\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "VVFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.115": {"line.1": {"text": "Welches ist der wahre Gott?", "tokens": ["Wel\u00b7ches", "ist", "der", "wah\u00b7re", "Gott", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist es der Hebr\u00e4er starrer", "tokens": ["Ist", "es", "der", "Heb\u00b7r\u00e4\u00b7er", "star\u00b7rer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADJA"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Gro\u00dfer Eingott, dessen K\u00e4mpe", "tokens": ["Gro\u00b7\u00dfer", "Ein\u00b7gott", ",", "des\u00b7sen", "K\u00e4m\u00b7pe"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "PRELAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Rabbi Juda, der Navarrer?", "tokens": ["Rab\u00b7bi", "Ju\u00b7da", ",", "der", "Na\u00b7var\u00b7rer", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.116": {"line.1": {"text": "Oder ist es der dreifalt'ge", "tokens": ["O\u00b7der", "ist", "es", "der", "drei\u00b7falt'\u00b7ge"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "ART", "ADJA"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Liebegott der Christianer,", "tokens": ["Lie\u00b7be\u00b7gott", "der", "Chris\u00b7ti\u00b7a\u00b7ner", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dessen K\u00e4mpe Frater Jose,", "tokens": ["Des\u00b7sen", "K\u00e4m\u00b7pe", "Fra\u00b7ter", "Jo\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "NE", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Gardian der Franziskaner?", "tokens": ["Gar\u00b7di\u00b7an", "der", "Fran\u00b7zis\u00b7ka\u00b7ner", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.117": {"line.1": {"text": "Durch die Macht der Argumente,", "tokens": ["Durch", "die", "Macht", "der", "Ar\u00b7gu\u00b7men\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Durch der Logik Kettenschl\u00fcsse", "tokens": ["Durch", "der", "Lo\u00b7gik", "Ket\u00b7ten\u00b7schl\u00fcs\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Zitate von Autoren,", "tokens": ["Und", "Zi\u00b7ta\u00b7te", "von", "Au\u00b7to\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die man anerkennen m\u00fcsse,", "tokens": ["Die", "man", "an\u00b7er\u00b7ken\u00b7nen", "m\u00fcs\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.118": {"line.1": {"text": "Will ein jeder K\u00e4mpe seinen", "tokens": ["Will", "ein", "je\u00b7der", "K\u00e4m\u00b7pe", "sei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ART", "PIAT", "NN", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gegner ad absurdum f\u00fchren", "tokens": ["Geg\u00b7ner", "ad", "ab\u00b7sur\u00b7dum", "f\u00fch\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "FM", "FM", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Und die wahre G\u00f6ttlichkeit", "tokens": ["Und", "die", "wah\u00b7re", "G\u00f6tt\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Seines Gottes demonstrieren.", "tokens": ["Sei\u00b7nes", "Got\u00b7tes", "de\u00b7monst\u00b7rie\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.119": {"line.1": {"text": "Festgestellt ist: da\u00df derjen'ge,", "tokens": ["Fest\u00b7ge\u00b7stellt", "ist", ":", "da\u00df", "der\u00b7jen'\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$.", "KOUS", "PDS", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der im Streit ward \u00fcberwunden,", "tokens": ["Der", "im", "Streit", "ward", "\u00fc\u00b7berw\u00b7un\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPRART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Seines Gegners Religion", "tokens": ["Sei\u00b7nes", "Geg\u00b7ners", "Re\u00b7li\u00b7gi\u00b7on"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Anzunehmen sei verbunden,", "tokens": ["An\u00b7zu\u00b7neh\u00b7men", "sei", "ver\u00b7bun\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.120": {"line.1": {"text": "Da\u00df der Jude sich der Taufe", "tokens": ["Da\u00df", "der", "Ju\u00b7de", "sich", "der", "Tau\u00b7fe"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PRF", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Heil'gem Sakramente f\u00fcge,", "tokens": ["Heil'\u00b7gem", "Sak\u00b7ra\u00b7men\u00b7te", "f\u00fc\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und im Gegenteil der Christ", "tokens": ["Und", "im", "Ge\u00b7gen\u00b7teil", "der", "Christ"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Beschneidung unterliege.", "tokens": ["Der", "Be\u00b7schnei\u00b7dung", "un\u00b7ter\u00b7lie\u00b7ge", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.121": {"line.1": {"text": "Jedem von den beiden K\u00e4mpen", "tokens": ["Je\u00b7dem", "von", "den", "bei\u00b7den", "K\u00e4m\u00b7pen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "APPR", "ART", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Beigesellt sind elf Genossen,", "tokens": ["Bei\u00b7ge\u00b7sellt", "sind", "elf", "Ge\u00b7nos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die zu teilen sein Geschick", "tokens": ["Die", "zu", "tei\u00b7len", "sein", "Ge\u00b7schick"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PTKZU", "VVINF", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind in Freud und Leid entschlossen.", "tokens": ["Sind", "in", "Freud", "und", "Leid", "ent\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.122": {"line.1": {"text": "Glaubenssicher sind die M\u00f6nche", "tokens": ["Glau\u00b7bens\u00b7si\u00b7cher", "sind", "die", "M\u00f6n\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Von des Gardians Geleitschaft,", "tokens": ["Von", "des", "Gar\u00b7di\u00b7ans", "Ge\u00b7leit\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Halten schon Weihwasserk\u00fcbel", "tokens": ["Hal\u00b7ten", "schon", "Weih\u00b7was\u00b7ser\u00b7k\u00fc\u00b7bel"], "token_info": ["word", "word", "word"], "pos": ["NN", "ADV", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "F\u00fcr die Taufe in Bereitschaft,", "tokens": ["F\u00fcr", "die", "Tau\u00b7fe", "in", "Be\u00b7reit\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.123": {"line.1": {"text": "Schwingen schon die Sprengelbesen", "tokens": ["Schwin\u00b7gen", "schon", "die", "Spren\u00b7gel\u00b7be\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die blanken R\u00e4ucherf\u00e4sser \u2013", "tokens": ["Und", "die", "blan\u00b7ken", "R\u00e4u\u00b7cher\u00b7f\u00e4s\u00b7ser", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihre Gegner unterdessen", "tokens": ["Ih\u00b7re", "Geg\u00b7ner", "un\u00b7ter\u00b7des\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wetzen die Beschneidungsmesser.", "tokens": ["Wet\u00b7zen", "die", "Be\u00b7schnei\u00b7dungs\u00b7mes\u00b7ser", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.124": {"line.1": {"text": "Beide Rotten stehn schlagfertig", "tokens": ["Bei\u00b7de", "Rot\u00b7ten", "stehn", "schlag\u00b7fer\u00b7tig"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor den Schranken in dem Saale,", "tokens": ["Vor", "den", "Schran\u00b7ken", "in", "dem", "Saa\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und das Volk mit Ungeduld", "tokens": ["Und", "das", "Volk", "mit", "Un\u00b7ge\u00b7duld"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Harret dr\u00e4ngend der Signale.", "tokens": ["Har\u00b7ret", "dr\u00e4n\u00b7gend", "der", "Sig\u00b7na\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.125": {"line.1": {"text": "Unterm g\u00fcldnen Baldachin", "tokens": ["Un\u00b7term", "g\u00fcld\u00b7nen", "Bal\u00b7da\u00b7ch\u00b7in"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und umrauscht vom Hofgesinde", "tokens": ["Und", "um\u00b7rauscht", "vom", "Hof\u00b7ge\u00b7sin\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sitzt der K\u00f6nig und die K\u00f6n'gin;", "tokens": ["Sitzt", "der", "K\u00f6\u00b7nig", "und", "die", "K\u00f6n'\u00b7gin", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Diese gleichet einem Kinde.", "tokens": ["Die\u00b7se", "glei\u00b7chet", "ei\u00b7nem", "Kin\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.126": {"line.1": {"text": "Ein franz\u00f6sisch stumpfes N\u00e4schen,", "tokens": ["Ein", "fran\u00b7z\u00f6\u00b7sisch", "stump\u00b7fes", "N\u00e4\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schalkheit kichert in den Mienen,", "tokens": ["Schalk\u00b7heit", "ki\u00b7chert", "in", "den", "Mie\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch bezaubernd sind des Mundes", "tokens": ["Doch", "be\u00b7zau\u00b7bernd", "sind", "des", "Mun\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Immer l\u00e4chelnde Rubinen.", "tokens": ["Im\u00b7mer", "l\u00e4\u00b7cheln\u00b7de", "Ru\u00b7bi\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "$."], "meter": "+-+---+-", "measure": "unknown.measure.tri"}}, "stanza.127": {"line.1": {"text": "Sch\u00f6ne, flatterhafte Blume \u2013", "tokens": ["Sch\u00f6\u00b7ne", ",", "flat\u00b7ter\u00b7haf\u00b7te", "Blu\u00b7me", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df sich ihrer Gott erbarme \u2013", "tokens": ["Da\u00df", "sich", "ih\u00b7rer", "Gott", "er\u00b7bar\u00b7me", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PPOSAT", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von dem heitern Seineufer", "tokens": ["Von", "dem", "hei\u00b7tern", "Sein\u00b7eu\u00b7fer"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wurde sie verpflanzt, die arme,", "tokens": ["Wur\u00b7de", "sie", "ver\u00b7pflanzt", ",", "die", "ar\u00b7me", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVPP", "$,", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.128": {"line.1": {"text": "Hierher in den steifen Boden", "tokens": ["Hier\u00b7her", "in", "den", "stei\u00b7fen", "Bo\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Der hispanischen Grandezza;", "tokens": ["Der", "his\u00b7pa\u00b7ni\u00b7schen", "Gran\u00b7dez\u00b7za", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Weiland hie\u00df sie Blanch' de Bourbon,", "tokens": ["Wei\u00b7land", "hie\u00df", "sie", "Blan\u00b7ch'", "de", "Bour\u00b7bon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "NE", "NE", "NE", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Do\u00f1a Blanka hei\u00dft sie jetzo.", "tokens": ["Do\u00f1a", "Blan\u00b7ka", "hei\u00dft", "sie", "jet\u00b7zo", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.129": {"line.1": {"text": "Pedro wird genannt der K\u00f6nig", "tokens": ["Ped\u00b7ro", "wird", "ge\u00b7nannt", "der", "K\u00f6\u00b7nig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "VVPP", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit dem Zusatz der Grausame;", "tokens": ["Mit", "dem", "Zu\u00b7satz", "der", "Grau\u00b7sa\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Aber heute, milden Sinnes,", "tokens": ["A\u00b7ber", "heu\u00b7te", ",", "mil\u00b7den", "Sin\u00b7nes", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist er besser als sein Name.", "tokens": ["Ist", "er", "bes\u00b7ser", "als", "sein", "Na\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "KOKOM", "PPOSAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.130": {"line.1": {"text": "Unterh\u00e4lt sich gut gelaunt", "tokens": ["Un\u00b7ter\u00b7h\u00e4lt", "sich", "gut", "ge\u00b7launt"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADJD", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit des Hofes Edelleuten;", "tokens": ["Mit", "des", "Ho\u00b7fes", "E\u00b7del\u00b7leu\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch den Juden und den Mohren", "tokens": ["Auch", "den", "Ju\u00b7den", "und", "den", "Moh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sagt er viele Artigkeiten.", "tokens": ["Sagt", "er", "vie\u00b7le", "Ar\u00b7tig\u00b7kei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.131": {"line.1": {"text": "Diese Ritter ohne Vorhaut", "tokens": ["Die\u00b7se", "Rit\u00b7ter", "oh\u00b7ne", "Vor\u00b7haut"], "token_info": ["word", "word", "word", "word"], "pos": ["PDAT", "NN", "APPR", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sind des K\u00f6nigs Lieblingsschranzen,", "tokens": ["Sind", "des", "K\u00f6\u00b7nigs", "Lieb\u00b7lings\u00b7schran\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie befehl'gen seine Heere,", "tokens": ["Sie", "be\u00b7fehl'\u00b7gen", "sei\u00b7ne", "Hee\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sie verwalten die Finanzen.", "tokens": ["Sie", "ver\u00b7wal\u00b7ten", "die", "Fi\u00b7nan\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.132": {"line.1": {"text": "Aber pl\u00f6tzlich Paukenschl\u00e4ge,", "tokens": ["A\u00b7ber", "pl\u00f6tz\u00b7lich", "Pau\u00b7ken\u00b7schl\u00e4\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und es melden die Trompeten,", "tokens": ["Und", "es", "mel\u00b7den", "die", "Trom\u00b7pe\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df begonnen hat der Maulkampf,", "tokens": ["Da\u00df", "be\u00b7gon\u00b7nen", "hat", "der", "Maul\u00b7kampf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVPP", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Disput der zwei Athleten.", "tokens": ["Der", "Dis\u00b7put", "der", "zwei", "Ath\u00b7le\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "CARD", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.133": {"line.1": {"text": "Der Gardian der Franziskaner", "tokens": ["Der", "Gar\u00b7di\u00b7an", "der", "Fran\u00b7zis\u00b7ka\u00b7ner"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bricht hervor mit frommem Grimme;", "tokens": ["Bricht", "her\u00b7vor", "mit", "from\u00b7mem", "Grim\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Polternd roh und widrig greinend", "tokens": ["Pol\u00b7ternd", "roh", "und", "wid\u00b7rig", "grei\u00b7nend"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "ADJD", "KON", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist abwechselnd seine Stimme.", "tokens": ["Ist", "ab\u00b7wech\u00b7selnd", "sei\u00b7ne", "Stim\u00b7me", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.134": {"line.1": {"text": "In des Vaters und des Sohnes", "tokens": ["In", "des", "Va\u00b7ters", "und", "des", "Soh\u00b7nes"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und des Heil'gen Geistes Namen", "tokens": ["Und", "des", "Heil'\u00b7gen", "Geis\u00b7tes", "Na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Exorzieret er den Rabbi,", "tokens": ["Ex\u00b7or\u00b7zie\u00b7ret", "er", "den", "Rab\u00b7bi", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Jakobs maledeiten Samen.", "tokens": ["Ja\u00b7kobs", "ma\u00b7le\u00b7dei\u00b7ten", "Sa\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.135": {"line.1": {"text": "Denn bei solchen Kontroversen", "tokens": ["Denn", "bei", "sol\u00b7chen", "Kont\u00b7ro\u00b7ver\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sind oft Teufelchen verborgen", "tokens": ["Sind", "oft", "Teu\u00b7fel\u00b7chen", "ver\u00b7bor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "NN", "VVPP"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "In dem Juden, die mit Scharfsinn,", "tokens": ["In", "dem", "Ju\u00b7den", ",", "die", "mit", "Scharf\u00b7sinn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Witz und Gr\u00fcnden ihn versorgen.", "tokens": ["Witz", "und", "Gr\u00fcn\u00b7den", "ihn", "ver\u00b7sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.136": {"line.1": {"text": "Nun die Teufel ausgetrieben", "tokens": ["Nun", "die", "Teu\u00b7fel", "aus\u00b7ge\u00b7trie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Durch die Macht des Exorzismus,", "tokens": ["Durch", "die", "Macht", "des", "Ex\u00b7or\u00b7zis\u00b7mus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Kommt der M\u00f6nch auch zur Dogmatik,", "tokens": ["Kommt", "der", "M\u00f6nch", "auch", "zur", "Dog\u00b7ma\u00b7tik", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Kugelt ab den Katechismus.", "tokens": ["Ku\u00b7gelt", "ab", "den", "Ka\u00b7te\u00b7chis\u00b7mus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.137": {"line.1": {"text": "Er erz\u00e4hlt, da\u00df in der Gottheit", "tokens": ["Er", "er\u00b7z\u00e4hlt", ",", "da\u00df", "in", "der", "Got\u00b7theit"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Drei Personen sind enthalten,", "tokens": ["Drei", "Per\u00b7so\u00b7nen", "sind", "ent\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Die jedoch zu einer einz'gen,", "tokens": ["Die", "je\u00b7doch", "zu", "ei\u00b7ner", "einz'\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn es passend, sich gestalten \u2013", "tokens": ["Wenn", "es", "pas\u00b7send", ",", "sich", "ge\u00b7stal\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "$,", "PRF", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.138": {"line.1": {"text": "Ein Mysterium, das nur", "tokens": ["Ein", "Mys\u00b7te\u00b7ri\u00b7um", ",", "das", "nur"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Von demjen'gen wird verstanden,", "tokens": ["Von", "demjen'\u00b7gen", "wird", "ver\u00b7stan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der entsprungen ist dem Kerker", "tokens": ["Der", "ent\u00b7sprun\u00b7gen", "ist", "dem", "Ker\u00b7ker"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der Vernunft und ihren Banden.", "tokens": ["Der", "Ver\u00b7nunft", "und", "ih\u00b7ren", "Ban\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.139": {"line.1": {"text": "Er erz\u00e4hlt: wie Gott der Herr", "tokens": ["Er", "er\u00b7z\u00e4hlt", ":", "wie", "Gott", "der", "Herr"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ward zu Bethlehem geboren", "tokens": ["Ward", "zu", "Beth\u00b7le\u00b7hem", "ge\u00b7bo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von der Jungfrau, welche niemals", "tokens": ["Von", "der", "Jung\u00b7frau", ",", "wel\u00b7che", "nie\u00b7mals"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihre Jungferschaft verloren;", "tokens": ["Ih\u00b7re", "Jung\u00b7fer\u00b7schaft", "ver\u00b7lo\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.140": {"line.1": {"text": "Wie der Herr der Welt gelegen", "tokens": ["Wie", "der", "Herr", "der", "Welt", "ge\u00b7le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In der Krippe, und ein K\u00fchlein", "tokens": ["In", "der", "Krip\u00b7pe", ",", "und", "ein", "K\u00fch\u00b7lein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "KON", "ART", "NN"], "meter": "--+--+--", "measure": "anapaest.di.plus"}, "line.3": {"text": "Und ein \u00d6chslein bei ihm stunden,", "tokens": ["Und", "ein", "\u00d6chs\u00b7lein", "bei", "ihm", "stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "PPER", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Schier and\u00e4chtig, zwei Rindviehlein.", "tokens": ["Schier", "an\u00b7d\u00e4ch\u00b7tig", ",", "zwei", "Rind\u00b7vieh\u00b7lein", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADJD", "$,", "CARD", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.141": {"line.1": {"text": "Er erz\u00e4hlte: wie der Herr", "tokens": ["Er", "er\u00b7z\u00e4hl\u00b7te", ":", "wie", "der", "Herr"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor den Schergen des Herodes", "tokens": ["Vor", "den", "Scher\u00b7gen", "des", "He\u00b7ro\u00b7des"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nach \u00c4gypten floh, und sp\u00e4ter", "tokens": ["Nach", "\u00c4\u00b7gyp\u00b7ten", "floh", ",", "und", "sp\u00e4\u00b7ter"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "$,", "KON", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Litt die herbe Pein des Todes", "tokens": ["Litt", "die", "her\u00b7be", "Pein", "des", "To\u00b7des"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.142": {"line.1": {"text": "Unter Pontio Pilato,", "tokens": ["Un\u00b7ter", "Pon\u00b7tio", "Pi\u00b7la\u00b7to", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Der das Urteil unterschrieben,", "tokens": ["Der", "das", "Ur\u00b7teil", "un\u00b7ter\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von den harten Pharis\u00e4ern,", "tokens": ["Von", "den", "har\u00b7ten", "Pha\u00b7ri\u00b7s\u00e4\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Von den Juden angetrieben.", "tokens": ["Von", "den", "Ju\u00b7den", "an\u00b7ge\u00b7trie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.143": {"line.1": {"text": "Er erz\u00e4hlte: wie der Herr,", "tokens": ["Er", "er\u00b7z\u00e4hl\u00b7te", ":", "wie", "der", "Herr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Der entstiegen seinem Grabe", "tokens": ["Der", "ent\u00b7stie\u00b7gen", "sei\u00b7nem", "Gra\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schon am dritten Tag, gen Himmel", "tokens": ["Schon", "am", "drit\u00b7ten", "Tag", ",", "gen", "Him\u00b7mel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$,", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Seinen Flug genommen habe;", "tokens": ["Sei\u00b7nen", "Flug", "ge\u00b7nom\u00b7men", "ha\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.144": {"line.1": {"text": "Wie er aber, wenn es Zeit ist,", "tokens": ["Wie", "er", "a\u00b7ber", ",", "wenn", "es", "Zeit", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "$,", "KOUS", "PPER", "NN", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wiederkehren auf die Erde", "tokens": ["Wie\u00b7der\u00b7keh\u00b7ren", "auf", "die", "Er\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und zu Josaphat die Toten", "tokens": ["Und", "zu", "Jo\u00b7sa\u00b7phat", "die", "To\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und Lebend'gen richten werde.", "tokens": ["Und", "Le\u00b7ben\u00b7d'\u00b7gen", "rich\u00b7ten", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.145": {"line.1": {"text": "\u00bbzittert, Juden!\u00ab rief der M\u00f6nch,", "tokens": ["\u00bb", "zit\u00b7tert", ",", "Ju\u00b7den", "!", "\u00ab", "rief", "der", "M\u00f6nch", ","], "token_info": ["punct", "word", "punct", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NN", "$.", "$(", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbvor dem Gott, den ihr mit Hieben", "tokens": ["\u00bb", "vor", "dem", "Gott", ",", "den", "ihr", "mit", "Hie\u00b7ben"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und mit Dornen habt gemartert,", "tokens": ["Und", "mit", "Dor\u00b7nen", "habt", "ge\u00b7mar\u00b7tert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Den ihr in den Tod getrieben.", "tokens": ["Den", "ihr", "in", "den", "Tod", "ge\u00b7trie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.146": {"line.1": {"text": "Seine M\u00f6rder, Volk der Rachsucht,", "tokens": ["Sei\u00b7ne", "M\u00f6r\u00b7der", ",", "Volk", "der", "Rach\u00b7sucht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Juden, das seid ihr gewesen \u2013", "tokens": ["Ju\u00b7den", ",", "das", "seid", "ihr", "ge\u00b7we\u00b7sen", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PDS", "VAFIN", "PPER", "VAPP", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Immer meuchelt ihr den Heiland,", "tokens": ["Im\u00b7mer", "meu\u00b7chelt", "ihr", "den", "Hei\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welcher kommt, euch zu erl\u00f6sen.", "tokens": ["Wel\u00b7cher", "kommt", ",", "euch", "zu", "er\u00b7l\u00f6\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAT", "VVFIN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.147": {"line.1": {"text": "Judenvolk, du bist ein Aas,", "tokens": ["Ju\u00b7den\u00b7volk", ",", "du", "bist", "ein", "Aas", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Worin hausen die D\u00e4monen;", "tokens": ["Wo\u00b7rin", "hau\u00b7sen", "die", "D\u00e4\u00b7mo\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eure Leiber sind Kasernen", "tokens": ["Eu\u00b7re", "Lei\u00b7ber", "sind", "Ka\u00b7ser\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr des Teufels Legionen.", "tokens": ["F\u00fcr", "des", "Teu\u00b7fels", "Le\u00b7gi\u00b7o\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.148": {"line.1": {"text": "Thomas von Aquino sagt es,", "tokens": ["Tho\u00b7mas", "von", "A\u00b7qui\u00b7no", "sagt", "es", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NE", "VVFIN", "PPER", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Den man nennt den gro\u00dfen Ochsen", "tokens": ["Den", "man", "nennt", "den", "gro\u00b7\u00dfen", "Och\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der Gelehrsamkeit, er ist", "tokens": ["Der", "Ge\u00b7lehr\u00b7sam\u00b7keit", ",", "er", "ist"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PPER", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Licht und Lust der Orthodoxen.", "tokens": ["Licht", "und", "Lust", "der", "Or\u00b7tho\u00b7do\u00b7xen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.149": {"line.1": {"text": "Judenvolk, ihr seid Hy\u00e4nen,", "tokens": ["Ju\u00b7den\u00b7volk", ",", "ihr", "seid", "Hy\u00b7\u00e4\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "W\u00f6lfe, Schakals, die in Gr\u00e4bern", "tokens": ["W\u00f6l\u00b7fe", ",", "Scha\u00b7kals", ",", "die", "in", "Gr\u00e4\u00b7bern"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "PRELS", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fchlen, um der Toten Leichnam'", "tokens": ["W\u00fch\u00b7len", ",", "um", "der", "To\u00b7ten", "Leich\u00b7nam'"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUI", "ART", "NN", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Blutfra\u00dfgierig aufzust\u00f6bern.", "tokens": ["Blut\u00b7fra\u00df\u00b7gie\u00b7rig", "auf\u00b7zu\u00b7st\u00f6\u00b7bern", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.150": {"line.1": {"text": "Juden, Juden, ihr seid S\u00e4ue,", "tokens": ["Ju\u00b7den", ",", "Ju\u00b7den", ",", "ihr", "seid", "S\u00e4u\u00b7e", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Paviane, Nashorntiere,", "tokens": ["Pa\u00b7vi\u00b7a\u00b7ne", ",", "Nas\u00b7horn\u00b7tie\u00b7re", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die man nennt Rhinozerosse,", "tokens": ["Die", "man", "nennt", "Rhi\u00b7no\u00b7ze\u00b7ros\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Krokodile und Vampire.", "tokens": ["Kro\u00b7ko\u00b7di\u00b7le", "und", "Vam\u00b7pi\u00b7re", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.151": {"line.1": {"text": "Ihr seid Raben, Eulen, Uhus,", "tokens": ["Ihr", "seid", "Ra\u00b7ben", ",", "Eu\u00b7len", ",", "U\u00b7hus", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "NE", "$,", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Flederm\u00e4use, Wiedeh\u00f6pfe,", "tokens": ["Fle\u00b7der\u00b7m\u00e4u\u00b7se", ",", "Wie\u00b7de\u00b7h\u00f6p\u00b7fe", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Leichenh\u00fchner, Basilisken,", "tokens": ["Lei\u00b7chen\u00b7h\u00fch\u00b7ner", ",", "Ba\u00b7si\u00b7lis\u00b7ken", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Galgenv\u00f6gel, Nachtgesch\u00f6pfe.", "tokens": ["Gal\u00b7gen\u00b7v\u00f6\u00b7gel", ",", "Nacht\u00b7ge\u00b7sch\u00f6p\u00b7fe", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.152": {"line.1": {"text": "Ihr seid Vipern und Blindschleichen,", "tokens": ["Ihr", "seid", "Vi\u00b7pern", "und", "Blind\u00b7schlei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Klapperschlangen, gift'ge Kr\u00f6ten,", "tokens": ["Klap\u00b7per\u00b7schlan\u00b7gen", ",", "gift'\u00b7ge", "Kr\u00f6\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ottern, Nattern \u2013 Christus wird", "tokens": ["Ot\u00b7tern", ",", "Nat\u00b7tern", "\u2013", "Chris\u00b7tus", "wird"], "token_info": ["word", "punct", "word", "punct", "word", "word"], "pos": ["NE", "$,", "NN", "$(", "NE", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Eu'r verfluchtes Haupt zertreten.", "tokens": ["Eu'r", "ver\u00b7fluch\u00b7tes", "Haupt", "zer\u00b7tre\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.153": {"line.1": {"text": "Oder wollt ihr, Maledeiten,", "tokens": ["O\u00b7der", "wollt", "ihr", ",", "Ma\u00b7le\u00b7dei\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Eure armen Seelen retten?", "tokens": ["Eu\u00b7re", "ar\u00b7men", "See\u00b7len", "ret\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aus der Bosheit Synagoge", "tokens": ["Aus", "der", "Bos\u00b7heit", "Syn\u00b7a\u00b7go\u00b7ge"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Fl\u00fcchtet nach den frommen St\u00e4tten,", "tokens": ["Fl\u00fcch\u00b7tet", "nach", "den", "from\u00b7men", "St\u00e4t\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.154": {"line.1": {"text": "Nach der Liebe lichtem Dome,", "tokens": ["Nach", "der", "Lie\u00b7be", "lich\u00b7tem", "Do\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo im benedeiten Becken", "tokens": ["Wo", "im", "be\u00b7ne\u00b7dei\u00b7ten", "Be\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Euch der Quell der Gnade sprudelt \u2013", "tokens": ["Euch", "der", "Quell", "der", "Gna\u00b7de", "spru\u00b7delt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "ART", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Drin sollt ihr die K\u00f6pfe stecken \u2013", "tokens": ["Drin", "sollt", "ihr", "die", "K\u00f6p\u00b7fe", "ste\u00b7cken", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.155": {"line.1": {"text": "Wascht dort ab den alten Adam", "tokens": ["Wascht", "dort", "ab", "den", "al\u00b7ten", "A\u00b7dam"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und die Laster, die ihn schw\u00e4rzen;", "tokens": ["Und", "die", "Las\u00b7ter", ",", "die", "ihn", "schw\u00e4r\u00b7zen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Des verj\u00e4hrten Grolles Schimmel,", "tokens": ["Des", "ver\u00b7j\u00e4hr\u00b7ten", "Grol\u00b7les", "Schim\u00b7mel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wascht ihn ab von euren Herzen!", "tokens": ["Wascht", "ihn", "ab", "von", "eu\u00b7ren", "Her\u00b7zen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.156": {"line.1": {"text": "H\u00f6rt ihr nicht des Heilands Stimme?", "tokens": ["H\u00f6rt", "ihr", "nicht", "des", "Hei\u00b7lands", "Stim\u00b7me", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Euren neuen Namen rief er \u2013", "tokens": ["Eu\u00b7ren", "neu\u00b7en", "Na\u00b7men", "rief", "er", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Lauset euch an Christi Brust", "tokens": ["Lau\u00b7set", "euch", "an", "Chris\u00b7ti", "Brust"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NE", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Von der S\u00fcnde Ungeziefer!", "tokens": ["Von", "der", "S\u00fcn\u00b7de", "Un\u00b7ge\u00b7zie\u00b7fer", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.157": {"line.1": {"text": "Unser Gott, der ist die Liebe,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "die", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und er gleichet einem Lamme;", "tokens": ["Und", "er", "glei\u00b7chet", "ei\u00b7nem", "Lam\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Um zu s\u00fchnen unsre Schuld,", "tokens": ["Um", "zu", "s\u00fch\u00b7nen", "uns\u00b7re", "Schuld", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Starb er an des Kreuzes Stamme.", "tokens": ["Starb", "er", "an", "des", "Kreu\u00b7zes", "Stam\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.158": {"line.1": {"text": "Unser Gott, der ist die Liebe,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "die", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jesus Christus ist sein Name;", "tokens": ["Je\u00b7sus", "Chris\u00b7tus", "ist", "sein", "Na\u00b7me", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Seine Duldsamkeit und Demut", "tokens": ["Sei\u00b7ne", "Duld\u00b7sam\u00b7keit", "und", "De\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Suchen wir stets nachzuahmen.", "tokens": ["Su\u00b7chen", "wir", "stets", "nach\u00b7zu\u00b7ah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.159": {"line.1": {"text": "Deshalb sind wir auch so sanft,", "tokens": ["Des\u00b7halb", "sind", "wir", "auch", "so", "sanft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So leutselig, ruhig, milde,", "tokens": ["So", "leut\u00b7se\u00b7lig", ",", "ru\u00b7hig", ",", "mil\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADJD", "$,", "ADJD", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Hadern niemals, nach des Lammes,", "tokens": ["Ha\u00b7dern", "nie\u00b7mals", ",", "nach", "des", "Lam\u00b7mes", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Des Vers\u00f6hners, Musterbilde.", "tokens": ["Des", "Ver\u00b7s\u00f6h\u00b7ners", ",", "Mus\u00b7ter\u00b7bil\u00b7de", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.160": {"line.1": {"text": "Einst im Himmel werden wir", "tokens": ["Einst", "im", "Him\u00b7mel", "wer\u00b7den", "wir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPRART", "NN", "VAFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ganz verkl\u00e4rt zu frommen Englein,", "tokens": ["Ganz", "ver\u00b7kl\u00e4rt", "zu", "from\u00b7men", "En\u00b7glein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "PTKZU", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und wir wandeln dort gottselig,", "tokens": ["Und", "wir", "wan\u00b7deln", "dort", "gott\u00b7se\u00b7lig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In den H\u00e4nden Lilienstenglein.", "tokens": ["In", "den", "H\u00e4n\u00b7den", "Li\u00b7li\u00b7ens\u00b7ten\u00b7glein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.161": {"line.1": {"text": "Statt der groben Kutten tragen", "tokens": ["Statt", "der", "gro\u00b7ben", "Kut\u00b7ten", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wir die reinlichsten Gew\u00e4nder", "tokens": ["Wir", "die", "rein\u00b7lichs\u00b7ten", "Ge\u00b7w\u00e4n\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ART", "ADJA", "NN"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.3": {"text": "Von Muss'lin, Brokat und Seide,", "tokens": ["Von", "Muss'\u00b7lin", ",", "Bro\u00b7kat", "und", "Sei\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "KON", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Goldne Troddeln, bunte B\u00e4nder.", "tokens": ["Gold\u00b7ne", "Trod\u00b7deln", ",", "bun\u00b7te", "B\u00e4n\u00b7der", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.162": {"line.1": {"text": "Keine Glatze mehr! Goldlocken", "tokens": ["Kei\u00b7ne", "Glat\u00b7ze", "mehr", "!", "Gold\u00b7lo\u00b7cken"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PIAT", "NN", "ADV", "$.", "NN"], "meter": "+-+--++-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Flattern dort um unsre K\u00f6pfe;", "tokens": ["Flat\u00b7tern", "dort", "um", "uns\u00b7re", "K\u00f6p\u00b7fe", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Allerliebste Jungfraun flechten", "tokens": ["Al\u00b7ler\u00b7liebs\u00b7te", "Jung\u00b7fraun", "flech\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Uns das Haar in h\u00fcbsche Z\u00f6pfe.", "tokens": ["Uns", "das", "Haar", "in", "h\u00fcb\u00b7sche", "Z\u00f6p\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.163": {"line.1": {"text": "Weinpokale wird es droben", "tokens": ["Wein\u00b7po\u00b7ka\u00b7le", "wird", "es", "dro\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Von viel weiterm Umfang geben,", "tokens": ["Von", "viel", "wei\u00b7term", "Um\u00b7fang", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als die Becher sind hier unten,", "tokens": ["Als", "die", "Be\u00b7cher", "sind", "hier", "un\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VAFIN", "ADV", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Worin sch\u00e4umt der Saft der Reben.", "tokens": ["Wo\u00b7rin", "sch\u00e4umt", "der", "Saft", "der", "Re\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.164": {"line.1": {"text": "Doch im Gegenteil viel enger", "tokens": ["Doch", "im", "Ge\u00b7gen\u00b7teil", "viel", "en\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ADV", "ADJD"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Als ein Weibermund hienieden,", "tokens": ["Als", "ein", "Wei\u00b7ber\u00b7mund", "hien\u00b7ie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wird das Frauenm\u00fcndchen sein,", "tokens": ["Wird", "das", "Frau\u00b7en\u00b7m\u00fcnd\u00b7chen", "sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das dort oben uns beschieden.", "tokens": ["Das", "dort", "o\u00b7ben", "uns", "be\u00b7schie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.165": {"line.1": {"text": "Trinkend, k\u00fcssend, lachend wollen", "tokens": ["Trin\u00b7kend", ",", "k\u00fcs\u00b7send", ",", "la\u00b7chend", "wol\u00b7len"], "token_info": ["word", "punct", "word", "punct", "word", "word"], "pos": ["ADJD", "$,", "ADJD", "$,", "ADJD", "VMFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wir die Ewigkeit verbringen,", "tokens": ["Wir", "die", "E\u00b7wig\u00b7keit", "ver\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und verz\u00fcckt Halleluja,", "tokens": ["Und", "ver\u00b7z\u00fcckt", "Hal\u00b7le\u00b7lu\u00b7ja", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "$,"], "meter": "+-+---+", "measure": "unknown.measure.tri"}, "line.4": {"text": "Kyrie eleison singen.\u00ab", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "sin\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["NE", "NE", "VVINF", "$.", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.166": {"line.1": {"text": "Also schlo\u00df der Christ. Die M\u00f6nchlein", "tokens": ["Al\u00b7so", "schlo\u00df", "der", "Christ", ".", "Die", "M\u00f6nch\u00b7lein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Glaubten schon, Erleuchtung tr\u00e4te", "tokens": ["Glaub\u00b7ten", "schon", ",", "Er\u00b7leuch\u00b7tung", "tr\u00e4\u00b7te"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVFIN", "ADV", "$,", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "In die Herzen, und sie schleppten", "tokens": ["In", "die", "Her\u00b7zen", ",", "und", "sie", "schlepp\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "KON", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Flink herbei das Taufger\u00e4te.", "tokens": ["Flink", "her\u00b7bei", "das", "Tauf\u00b7ge\u00b7r\u00e4\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.167": {"line.1": {"text": "Doch die wasserscheuen Juden", "tokens": ["Doch", "die", "was\u00b7ser\u00b7scheu\u00b7en", "Ju\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sch\u00fctteln sich und grinsen schn\u00f6de.", "tokens": ["Sch\u00fct\u00b7teln", "sich", "und", "grin\u00b7sen", "schn\u00f6\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "KON", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Rabbi Juda, der Navarrer,", "tokens": ["Rab\u00b7bi", "Ju\u00b7da", ",", "der", "Na\u00b7var\u00b7rer", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hub jetzt an die Gegenrede:", "tokens": ["Hub", "jetzt", "an", "die", "Ge\u00b7gen\u00b7re\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.168": {"line.1": {"text": "\u00bbum f\u00fcr deine Saat zu d\u00fcngen", "tokens": ["\u00bb", "um", "f\u00fcr", "dei\u00b7ne", "Saat", "zu", "d\u00fcn\u00b7gen"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KOUI", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Meines Geistes d\u00fcrren Acker,", "tokens": ["Mei\u00b7nes", "Geis\u00b7tes", "d\u00fcr\u00b7ren", "A\u00b7cker", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit Mistkarren voll Schimpfw\u00f6rter", "tokens": ["Mit", "Mist\u00b7kar\u00b7ren", "voll", "Schimpf\u00b7w\u00f6r\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hast du mich beschmissen wacker.", "tokens": ["Hast", "du", "mich", "be\u00b7schmis\u00b7sen", "wa\u00b7cker", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADJD", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.169": {"line.1": {"text": "So folgt jeder der Methode,", "tokens": ["So", "folgt", "je\u00b7der", "der", "Me\u00b7tho\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dran er nun einmal gew\u00f6hnet,", "tokens": ["Dran", "er", "nun", "ein\u00b7mal", "ge\u00b7w\u00f6h\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und anstatt dich drob zu schelten,", "tokens": ["Und", "an\u00b7statt", "dich", "drob", "zu", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sag ich Dank dir, wohlvers\u00f6hnet.", "tokens": ["Sag", "ich", "Dank", "dir", ",", "wohl\u00b7ver\u00b7s\u00f6h\u00b7net", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PPER", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.170": {"line.1": {"text": "Die Dreieinigkeitsdoktrin", "tokens": ["Die", "Drei\u00b7ei\u00b7nig\u00b7keits\u00b7dokt\u00b7rin"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kann f\u00fcr unsre Leut' nicht passen,", "tokens": ["Kann", "f\u00fcr", "uns\u00b7re", "Leut'", "nicht", "pas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PPOSAT", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die mit Regula-de-tri", "tokens": ["Die", "mit", "Re\u00b7gu\u00b7la\u00b7de\u00b7tri"], "token_info": ["word", "word", "word"], "pos": ["ART", "APPR", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sich von Jugend auf befassen.", "tokens": ["Sich", "von", "Ju\u00b7gend", "auf", "be\u00b7fas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "APPR", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.171": {"line.1": {"text": "Da\u00df in deinem Gotte drei,", "tokens": ["Da\u00df", "in", "dei\u00b7nem", "Got\u00b7te", "drei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPOSAT", "NN", "CARD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Drei Personen sind enthalten,", "tokens": ["Drei", "Per\u00b7so\u00b7nen", "sind", "ent\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ist bescheiden noch, sechstausend", "tokens": ["Ist", "be\u00b7schei\u00b7den", "noch", ",", "sech\u00b7stau\u00b7send"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["VAFIN", "ADJD", "ADV", "$,", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "G\u00f6tter gab es bei den Alten.", "tokens": ["G\u00f6t\u00b7ter", "gab", "es", "bei", "den", "Al\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.172": {"line.1": {"text": "Unbekannt ist mir der Gott,", "tokens": ["Un\u00b7be\u00b7kannt", "ist", "mir", "der", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Den ihr Christum pflegt zu nennen;", "tokens": ["Den", "ihr", "Chris\u00b7tum", "pflegt", "zu", "nen\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Seine Jungfer Mutter gleichfalls", "tokens": ["Sei\u00b7ne", "Jung\u00b7fer", "Mut\u00b7ter", "gleich\u00b7falls"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hab ich nicht die Ehr' zu kennen.", "tokens": ["Hab", "ich", "nicht", "die", "Ehr'", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.173": {"line.1": {"text": "Ich bedaure, da\u00df er einst,", "tokens": ["Ich", "be\u00b7dau\u00b7re", ",", "da\u00df", "er", "einst", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vor etwa zw\u00f6lfhundert Jahren,", "tokens": ["Vor", "et\u00b7wa", "zw\u00f6lf\u00b7hun\u00b7dert", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ein'ge Unannehmlichkeiten", "tokens": ["Ein'\u00b7ge", "Un\u00b7an\u00b7nehm\u00b7lich\u00b7kei\u00b7ten"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu Jerusalem erfahren.", "tokens": ["Zu", "Je\u00b7ru\u00b7sa\u00b7lem", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.174": {"line.1": {"text": "Ob die Juden ihn get\u00f6tet,", "tokens": ["Ob", "die", "Ju\u00b7den", "ihn", "ge\u00b7t\u00f6\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Das ist schwer jetzt zu erkunden,", "tokens": ["Das", "ist", "schwer", "jetzt", "zu", "er\u00b7kun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "ADV", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da ja das Corpus delicti", "tokens": ["Da", "ja", "das", "Cor\u00b7pus", "de\u00b7lic\u00b7ti"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NE", "NE"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Schon am dritten Tag verschwunden.", "tokens": ["Schon", "am", "drit\u00b7ten", "Tag", "ver\u00b7schwun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.175": {"line.1": {"text": "Da\u00df er ein Verwandter sei", "tokens": ["Da\u00df", "er", "ein", "Ver\u00b7wand\u00b7ter", "sei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Unsres Gottes, ist nicht minder", "tokens": ["Uns\u00b7res", "Got\u00b7tes", ",", "ist", "nicht", "min\u00b7der"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "VAFIN", "PTKNEG", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zweifelhaft; soviel wir wissen,", "tokens": ["Zwei\u00b7fel\u00b7haft", ";", "so\u00b7viel", "wir", "wis\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hat der letztre keine Kinder.", "tokens": ["Hat", "der", "letz\u00b7tre", "kei\u00b7ne", "Kin\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.176": {"line.1": {"text": "Unser Gott ist nicht gestorben", "tokens": ["Un\u00b7ser", "Gott", "ist", "nicht", "ge\u00b7stor\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als ein armes L\u00e4mmerschw\u00e4nzchen", "tokens": ["Als", "ein", "ar\u00b7mes", "L\u00e4m\u00b7mer\u00b7schw\u00e4nz\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "F\u00fcr die Menschheit, ist kein s\u00fc\u00dfes", "tokens": ["F\u00fcr", "die", "Menschheit", ",", "ist", "kein", "s\u00fc\u00b7\u00dfes"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "VAFIN", "PIAT", "ADJA"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Philantr\u00f6pfchen, Faselh\u00e4nschen.", "tokens": ["Phil\u00b7an\u00b7tr\u00f6pf\u00b7chen", ",", "Fa\u00b7sel\u00b7h\u00e4n\u00b7schen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.177": {"line.1": {"text": "Unser Gott ist nicht die Liebe;", "tokens": ["Un\u00b7ser", "Gott", "ist", "nicht", "die", "Lie\u00b7be", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schn\u00e4beln ist nicht seine Sache,", "tokens": ["Schn\u00e4\u00b7beln", "ist", "nicht", "sei\u00b7ne", "Sa\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PTKNEG", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn er ist ein Donnergott", "tokens": ["Denn", "er", "ist", "ein", "Don\u00b7ner\u00b7gott"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er ist ein Gott der Rache.", "tokens": ["Und", "er", "ist", "ein", "Gott", "der", "Ra\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.178": {"line.1": {"text": "Seines Zornes Blitze treffen", "tokens": ["Sei\u00b7nes", "Zor\u00b7nes", "Blit\u00b7ze", "tref\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unerbittlich jeden S\u00fcnder,", "tokens": ["Un\u00b7er\u00b7bitt\u00b7lich", "je\u00b7den", "S\u00fcn\u00b7der", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und des Vaters Schulden b\u00fc\u00dfen", "tokens": ["Und", "des", "Va\u00b7ters", "Schul\u00b7den", "b\u00fc\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Oft die sp\u00e4ten Enkelkinder.", "tokens": ["Oft", "die", "sp\u00e4\u00b7ten", "En\u00b7kel\u00b7kin\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.179": {"line.1": {"text": "Unser Gott, der ist lebendig,", "tokens": ["Un\u00b7ser", "Gott", ",", "der", "ist", "le\u00b7ben\u00b7dig", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und in seiner Himmelshalle", "tokens": ["Und", "in", "sei\u00b7ner", "Him\u00b7mels\u00b7hal\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Existieret er drauflos", "tokens": ["E\u00b7xis\u00b7tie\u00b7ret", "er", "drauf\u00b7los"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Durch die Ewigkeiten alle.", "tokens": ["Durch", "die", "E\u00b7wig\u00b7kei\u00b7ten", "al\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PIAT", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.180": {"line.1": {"text": "Unser Gott, und der ist auch", "tokens": ["Un\u00b7ser", "Gott", ",", "und", "der", "ist", "auch"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KON", "ART", "VAFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ein gesunder Gott, kein Mythos", "tokens": ["Ein", "ge\u00b7sun\u00b7der", "Gott", ",", "kein", "My\u00b7thos"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PIAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Bleich und d\u00fcnne wie Oblaten", "tokens": ["Bleich", "und", "d\u00fcn\u00b7ne", "wie", "Ob\u00b7la\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "KON", "VVFIN", "KOKOM", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Oder Schatten am Cocytos.", "tokens": ["O\u00b7der", "Schat\u00b7ten", "am", "Co\u00b7cy\u00b7tos", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPRART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.181": {"line.1": {"text": "Unser Gott ist stark. In H\u00e4nden", "tokens": ["Un\u00b7ser", "Gott", "ist", "stark", ".", "In", "H\u00e4n\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$.", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tr\u00e4gt er Sonne, Mond, Gestirne;", "tokens": ["Tr\u00e4gt", "er", "Son\u00b7ne", ",", "Mond", ",", "Ge\u00b7stir\u00b7ne", ";"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Throne brechen, V\u00f6lker schwinden,", "tokens": ["Thro\u00b7ne", "bre\u00b7chen", ",", "V\u00f6l\u00b7ker", "schwin\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn er runzelt seine Stirne.", "tokens": ["Wenn", "er", "run\u00b7zelt", "sei\u00b7ne", "Stir\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.182": {"line.1": {"text": "Und er ist ein gro\u00dfer Gott.", "tokens": ["Und", "er", "ist", "ein", "gro\u00b7\u00dfer", "Gott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "David singt: Ermessen lie\u00dfe", "tokens": ["Da\u00b7vid", "singt", ":", "Er\u00b7mes\u00b7sen", "lie\u00b7\u00dfe"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NE", "VVFIN", "$.", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sich die Gr\u00f6\u00dfe nicht, die Erde", "tokens": ["Sich", "die", "Gr\u00f6\u00b7\u00dfe", "nicht", ",", "die", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PRF", "ART", "NN", "PTKNEG", "$,", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sei der Schemel seiner F\u00fc\u00dfe.", "tokens": ["Sei", "der", "Sche\u00b7mel", "sei\u00b7ner", "F\u00fc\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.183": {"line.1": {"text": "Unser Gott liebt die Musik,", "tokens": ["Un\u00b7ser", "Gott", "liebt", "die", "Mu\u00b7sik", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Saitenspiel und Festges\u00e4nge;", "tokens": ["Sai\u00b7ten\u00b7spiel", "und", "Fest\u00b7ge\u00b7s\u00e4n\u00b7ge", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Doch wie Ferkelgrunzen sind", "tokens": ["Doch", "wie", "Fer\u00b7kel\u00b7grun\u00b7zen", "sind"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PWAV", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihm zuwider Glockenkl\u00e4nge.", "tokens": ["Ihm", "zu\u00b7wi\u00b7der", "Glo\u00b7cken\u00b7kl\u00e4n\u00b7ge", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.184": {"line.1": {"text": "Leviathan hei\u00dft der Fisch,", "tokens": ["Le\u00b7vi\u00b7a\u00b7than", "hei\u00dft", "der", "Fisch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Welcher haust im Meeresgrunde;", "tokens": ["Wel\u00b7cher", "haust", "im", "Mee\u00b7res\u00b7grun\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit ihm spielet Gott der Herr", "tokens": ["Mit", "ihm", "spie\u00b7let", "Gott", "der", "Herr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "VVFIN", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alle Tage eine Stunde \u2013", "tokens": ["Al\u00b7le", "Ta\u00b7ge", "ei\u00b7ne", "Stun\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.185": {"line.1": {"text": "Ausgenommen an dem neunten", "tokens": ["Aus\u00b7ge\u00b7nom\u00b7men", "an", "dem", "neun\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tag des Monats Ab, wo n\u00e4mlich", "tokens": ["Tag", "des", "Mo\u00b7nats", "Ab", ",", "wo", "n\u00e4m\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "ART", "NN", "APPR", "$,", "PWAV", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einge\u00e4schert ward sein Tempel;", "tokens": ["Ein\u00b7ge\u00b7\u00e4\u00b7schert", "ward", "sein", "Tem\u00b7pel", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "An dem Tag ist er zu gr\u00e4mlich.", "tokens": ["An", "dem", "Tag", "ist", "er", "zu", "gr\u00e4m\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "PTKA", "ADJD", "$."], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.186": {"line.1": {"text": "Des Leviathans L\u00e4nge ist", "tokens": ["Des", "Le\u00b7vi\u00b7a\u00b7thans", "L\u00e4n\u00b7ge", "ist"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hundert Meilen, hat Flo\u00dffedern", "tokens": ["Hun\u00b7dert", "Mei\u00b7len", ",", "hat", "Flo\u00df\u00b7fe\u00b7dern"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["CARD", "NN", "$,", "VAFIN", "NN"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "Gro\u00df wie K\u00f6nig Ok von Basan,", "tokens": ["Gro\u00df", "wie", "K\u00f6\u00b7nig", "Ok", "von", "Ba\u00b7san", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "NN", "NE", "APPR", "NE", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und sein Schwanz ist wie ein Zedern.", "tokens": ["Und", "sein", "Schwanz", "ist", "wie", "ein", "Ze\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "KOKOM", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.187": {"line.1": {"text": "Doch sein Fleisch ist delikat,", "tokens": ["Doch", "sein", "Fleisch", "ist", "de\u00b7li\u00b7kat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Delikater als Schildkr\u00f6ten,", "tokens": ["De\u00b7li\u00b7ka\u00b7ter", "als", "Schild\u00b7kr\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KOUS", "NN", "$,"], "meter": "+-+-+++-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Und am Tag der Auferstehung", "tokens": ["Und", "am", "Tag", "der", "Auf\u00b7er\u00b7ste\u00b7hung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wird der Herr zu Tische beten", "tokens": ["Wird", "der", "Herr", "zu", "Ti\u00b7sche", "be\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "APPR", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.188": {"line.1": {"text": "Alle frommen Auserw\u00e4hlten,", "tokens": ["Al\u00b7le", "from\u00b7men", "Au\u00b7ser\u00b7w\u00e4hl\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die Gerechten und die Weisen \u2013", "tokens": ["Die", "Ge\u00b7rech\u00b7ten", "und", "die", "Wei\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unsres Herrgotts Lieblingsfisch", "tokens": ["Uns\u00b7res", "Herr\u00b7gotts", "Lieb\u00b7lings\u00b7fisch"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Werden sie alsdann verspeisen,", "tokens": ["Wer\u00b7den", "sie", "als\u00b7dann", "ver\u00b7spei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.189": {"line.1": {"text": "Teils mit wei\u00dfer Knoblauchbr\u00fche,", "tokens": ["Teils", "mit", "wei\u00b7\u00dfer", "Knob\u00b7lauch\u00b7br\u00fc\u00b7he", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Teils auch braun in Wein gesotten,", "tokens": ["Teils", "auch", "braun", "in", "Wein", "ge\u00b7sot\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "APPR", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit Gew\u00fcrzen und Rosinen,", "tokens": ["Mit", "Ge\u00b7w\u00fcr\u00b7zen", "und", "Ro\u00b7si\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ungef\u00e4hr wie Mateloten.", "tokens": ["Un\u00b7ge\u00b7f\u00e4hr", "wie", "Ma\u00b7te\u00b7lo\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.190": {"line.1": {"text": "In der wei\u00dfen Knoblauchbr\u00fche", "tokens": ["In", "der", "wei\u00b7\u00dfen", "Knob\u00b7lauch\u00b7br\u00fc\u00b7he"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schwimmen kleine Sch\u00e4bchen Rettich \u2013", "tokens": ["Schwim\u00b7men", "klei\u00b7ne", "Sch\u00e4b\u00b7chen", "Ret\u00b7tich", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "NE", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "So bereitet, Frater Jose,", "tokens": ["So", "be\u00b7rei\u00b7tet", ",", "Fra\u00b7ter", "Jo\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "NN", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mundet dir das Fischlein, wett ich!", "tokens": ["Mun\u00b7det", "dir", "das", "Fisc\u00b7hlein", ",", "wett", "ich", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "$,", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.191": {"line.1": {"text": "Auch die braune ist so lecker,", "tokens": ["Auch", "die", "brau\u00b7ne", "ist", "so", "le\u00b7cker", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "N\u00e4mlich die Rosinensauce,", "tokens": ["N\u00e4m\u00b7lich", "die", "Ro\u00b7si\u00b7nen\u00b7sau\u00b7ce", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Sie wird himmlisch wohl behagen", "tokens": ["Sie", "wird", "himm\u00b7lisch", "wohl", "be\u00b7ha\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Deinem B\u00e4uchlein, Frater Jose.", "tokens": ["Dei\u00b7nem", "B\u00e4uch\u00b7lein", ",", "Fra\u00b7ter", "Jo\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.192": {"line.1": {"text": "Was Gott kocht, ist gut gekocht!", "tokens": ["Was", "Gott", "kocht", ",", "ist", "gut", "ge\u00b7kocht", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "ADJD", "VVPP", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "M\u00f6nchlein, nimm jetzt meinen Rat an,", "tokens": ["M\u00f6nch\u00b7lein", ",", "nimm", "jetzt", "mei\u00b7nen", "Rat", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVIMP", "ADV", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Opfre hin die alte Vorhaut", "tokens": ["Opf\u00b7re", "hin", "die", "al\u00b7te", "Vor\u00b7haut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und erquick dich am Leviathan.\u00ab", "tokens": ["Und", "er\u00b7quick", "dich", "am", "Le\u00b7vi\u00b7a\u00b7than", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVIMP", "PPER", "APPRART", "NN", "$.", "$("], "meter": "--+-+-+-+", "measure": "anapaest.init"}}, "stanza.193": {"line.1": {"text": "Also lockend sprach der Rabbi,", "tokens": ["Al\u00b7so", "lo\u00b7ckend", "sprach", "der", "Rab\u00b7bi", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lockend, k\u00f6dernd, heimlich schmunzelnd,", "tokens": ["Lo\u00b7ckend", ",", "k\u00f6\u00b7dernd", ",", "heim\u00b7lich", "schmun\u00b7zelnd", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und die Juden schwangen schon", "tokens": ["Und", "die", "Ju\u00b7den", "schwan\u00b7gen", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihre Messer wonnegrunzelnd,", "tokens": ["Ih\u00b7re", "Mes\u00b7ser", "won\u00b7ne\u00b7grun\u00b7zelnd", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.194": {"line.1": {"text": "Um als Sieger zu skalpieren", "tokens": ["Um", "als", "Sie\u00b7ger", "zu", "skal\u00b7pie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUI", "KOUS", "NN", "PTKZU", "VVINF"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Die verfallenen Vorh\u00e4ute,", "tokens": ["Die", "ver\u00b7fal\u00b7le\u00b7nen", "Vor\u00b7h\u00e4u\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "--+---+-", "measure": "anapaest.init"}, "line.3": {"text": "Wahre spolia opima", "tokens": ["Wah\u00b7re", "spo\u00b7lia", "o\u00b7pi\u00b7ma"], "token_info": ["word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "In dem wunderlichen Streite.", "tokens": ["In", "dem", "wun\u00b7der\u00b7li\u00b7chen", "Strei\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.195": {"line.1": {"text": "Doch die M\u00f6nche hielten fest", "tokens": ["Doch", "die", "M\u00f6n\u00b7che", "hiel\u00b7ten", "fest"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "PTKVZ"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "An dem v\u00e4terlichen Glauben", "tokens": ["An", "dem", "v\u00e4\u00b7ter\u00b7li\u00b7chen", "Glau\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und an ihrer Vorhaut, lie\u00dfen", "tokens": ["Und", "an", "ih\u00b7rer", "Vor\u00b7haut", ",", "lie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["KON", "APPR", "PPOSAT", "NN", "$,", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sich derselben nicht berauben.", "tokens": ["Sich", "der\u00b7sel\u00b7ben", "nicht", "be\u00b7rau\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "PDS", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.196": {"line.1": {"text": "Nach dem Juden sprach aufs neue", "tokens": ["Nach", "dem", "Ju\u00b7den", "sprach", "aufs", "neu\u00b7e"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "APPRART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der katholische Bekehrer;", "tokens": ["Der", "ka\u00b7tho\u00b7li\u00b7sche", "Be\u00b7keh\u00b7rer", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wieder schimpft er, jedes Wort", "tokens": ["Wie\u00b7der", "schimpft", "er", ",", "je\u00b7des", "Wort"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist ein Nachttopf, und kein leerer.", "tokens": ["Ist", "ein", "Nacht\u00b7topf", ",", "und", "kein", "lee\u00b7rer", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$,", "KON", "PIAT", "ADJA", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.197": {"line.1": {"text": "Darauf repliziert der Rabbi", "tokens": ["Da\u00b7rauf", "re\u00b7pli\u00b7ziert", "der", "Rab\u00b7bi"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Mit zur\u00fcckgehaltnem Eifer;", "tokens": ["Mit", "zu\u00b7r\u00fcck\u00b7ge\u00b7halt\u00b7nem", "Ei\u00b7fer", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie sein Herz auch \u00fcberkocht,", "tokens": ["Wie", "sein", "Herz", "auch", "\u00fc\u00b7ber\u00b7kocht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch verschluckt er seinen Geifer.", "tokens": ["Doch", "ver\u00b7schluckt", "er", "sei\u00b7nen", "Gei\u00b7fer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.198": {"line.1": {"text": "Er beruft sich auf die Mischna,", "tokens": ["Er", "be\u00b7ruft", "sich", "auf", "die", "Mischna", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Kommentare und Traktate;", "tokens": ["Kom\u00b7men\u00b7ta\u00b7re", "und", "Trak\u00b7ta\u00b7te", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "Bringt auch aus dem Tausves-Jontof", "tokens": ["Bringt", "auch", "aus", "dem", "Taus\u00b7ve\u00b7sJon\u00b7tof"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Viel beweisende Zitate.", "tokens": ["Viel", "be\u00b7wei\u00b7sen\u00b7de", "Zi\u00b7ta\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.199": {"line.1": {"text": "Aber welche Blasphemie", "tokens": ["A\u00b7ber", "wel\u00b7che", "Blas\u00b7phe\u00b7mie"], "token_info": ["word", "word", "word"], "pos": ["KON", "PWAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mu\u00dft er von dem M\u00f6nche h\u00f6ren!", "tokens": ["Mu\u00dft", "er", "von", "dem", "M\u00f6n\u00b7che", "h\u00f6\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieser sprach: der Tausves-Jontof", "tokens": ["Die\u00b7ser", "sprach", ":", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "$.", "ART", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "M\u00f6ge sich zum Teufel scheren.", "tokens": ["M\u00f6\u00b7ge", "sich", "zum", "Teu\u00b7fel", "sche\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "APPRART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.200": {"line.1": {"text": "\u00bbda h\u00f6rt alles auf, o Gott!\u00ab", "tokens": ["\u00bb", "da", "h\u00f6rt", "al\u00b7les", "auf", ",", "o", "Gott", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PIS", "PTKVZ", "$,", "FM", "NN", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kreischt der Rabbi jetzt entsetzlich;", "tokens": ["Kreischt", "der", "Rab\u00b7bi", "jetzt", "ent\u00b7setz\u00b7lich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und es rei\u00dft ihm die Geduld,", "tokens": ["Und", "es", "rei\u00dft", "ihm", "die", "Ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Rappelk\u00f6pfig wird er pl\u00f6tzlich.", "tokens": ["Rap\u00b7pel\u00b7k\u00f6p\u00b7fig", "wird", "er", "pl\u00f6tz\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.201": {"line.1": {"text": "\u00bbgilt nichts mehr der Tausves-Jontof,", "tokens": ["\u00bb", "gilt", "nichts", "mehr", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PIS", "ADV", "ART", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Was soll gelten? Zeter! Zeter!", "tokens": ["Was", "soll", "gel\u00b7ten", "?", "Ze\u00b7ter", "!", "Ze\u00b7ter", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWS", "VMFIN", "VVINF", "$.", "NN", "$.", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "R\u00e4che, Herr, die Missetat,", "tokens": ["R\u00e4\u00b7che", ",", "Herr", ",", "die", "Mis\u00b7se\u00b7tat", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Strafe, Herr, den \u00dcbelt\u00e4ter!", "tokens": ["Stra\u00b7fe", ",", "Herr", ",", "den", "\u00dc\u00b7belt\u00b7\u00e4\u00b7ter", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.202": {"line.1": {"text": "Denn der Tausves-Jontof, Gott,", "tokens": ["Denn", "der", "Taus\u00b7ve\u00b7sJon\u00b7tof", ",", "Gott", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Das bist du! Und an dem frechen", "tokens": ["Das", "bist", "du", "!", "Und", "an", "dem", "fre\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PPER", "$.", "KON", "APPR", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Tausves-Jontof-Leugner mu\u00dft du", "tokens": ["Taus\u00b7ve\u00b7sJon\u00b7tof\u00b7Leug\u00b7ner", "mu\u00dft", "du"], "token_info": ["word", "word", "word"], "pos": ["NN", "VMFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Deines Namens Ehre r\u00e4chen.", "tokens": ["Dei\u00b7nes", "Na\u00b7mens", "Eh\u00b7re", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.203": {"line.1": {"text": "La\u00df den Abgrund ihn verschlingen,", "tokens": ["La\u00df", "den", "Ab\u00b7grund", "ihn", "ver\u00b7schlin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie des Korah b\u00f6se Rotte,", "tokens": ["Wie", "des", "Ko\u00b7rah", "b\u00f6\u00b7se", "Rot\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die sich wider dich emp\u00f6rt", "tokens": ["Die", "sich", "wi\u00b7der", "dich", "em\u00b7p\u00f6rt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PRF", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Durch Emeute und Komplotte.", "tokens": ["Durch", "E\u00b7meu\u00b7te", "und", "Kom\u00b7plot\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.204": {"line.1": {"text": "Donnre deinen besten Donner!", "tokens": ["Donn\u00b7re", "dei\u00b7nen", "bes\u00b7ten", "Don\u00b7ner", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Strafe, o mein Gott, den Frevel \u2013", "tokens": ["Stra\u00b7fe", ",", "o", "mein", "Gott", ",", "den", "Fre\u00b7vel", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "FM", "PPOSAT", "NN", "$,", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hattest du doch zu Sodoma", "tokens": ["Hat\u00b7test", "du", "doch", "zu", "So\u00b7do\u00b7ma"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und Gomorrha Pech und Schwefel!", "tokens": ["Und", "Go\u00b7morr\u00b7ha", "Pech", "und", "Schwe\u00b7fel", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "KON", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.205": {"line.1": {"text": "Treffe, Herr, die Kapuziner,", "tokens": ["Tref\u00b7fe", ",", "Herr", ",", "die", "Ka\u00b7pu\u00b7zi\u00b7ner", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie du Pharaon getroffen,", "tokens": ["Wie", "du", "Pha\u00b7raon", "ge\u00b7trof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der uns nachgesetzt, als wir", "tokens": ["Der", "uns", "nach\u00b7ge\u00b7setzt", ",", "als", "wir"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PPER", "VVPP", "$,", "KOUS", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wohlbepackt davongeloffen.", "tokens": ["Wohl\u00b7be\u00b7packt", "da\u00b7von\u00b7ge\u00b7lof\u00b7fen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.206": {"line.1": {"text": "Hunderttausend Ritter folgten", "tokens": ["Hun\u00b7dert\u00b7tau\u00b7send", "Rit\u00b7ter", "folg\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["CARD", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Diesem K\u00f6nig von Mizrayim,", "tokens": ["Die\u00b7sem", "K\u00f6\u00b7nig", "von", "Miz\u00b7ra\u00b7yim", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "APPR", "NE", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Stahlbepanzert, blanke Schwerter", "tokens": ["Stahl\u00b7be\u00b7pan\u00b7zert", ",", "blan\u00b7ke", "Schwer\u00b7ter"], "token_info": ["word", "punct", "word", "word"], "pos": ["VVFIN", "$,", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In den schrecklichen Jadayim.", "tokens": ["In", "den", "schreck\u00b7li\u00b7chen", "Ja\u00b7day\u00b7im", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.207": {"line.1": {"text": "Gott! da hast du ausgestreckt", "tokens": ["Gott", "!", "da", "hast", "du", "aus\u00b7ge\u00b7streckt"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "ADV", "VAFIN", "PPER", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Deine Jad, und samt dem Heere", "tokens": ["Dei\u00b7ne", "Jad", ",", "und", "samt", "dem", "Hee\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KON", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ward ertr\u00e4nkt, wie junge Katzen,", "tokens": ["Ward", "er\u00b7tr\u00e4nkt", ",", "wie", "jun\u00b7ge", "Kat\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "PWAV", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Pharao im Roten Meere.", "tokens": ["Pha\u00b7rao", "im", "Ro\u00b7ten", "Mee\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.208": {"line.1": {"text": "Treffe, Herr, die Kapuziner,", "tokens": ["Tref\u00b7fe", ",", "Herr", ",", "die", "Ka\u00b7pu\u00b7zi\u00b7ner", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zeige den infamen Schuften,", "tokens": ["Zei\u00b7ge", "den", "in\u00b7fa\u00b7men", "Schuf\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df die Blitze deines Zorns", "tokens": ["Da\u00df", "die", "Blit\u00b7ze", "dei\u00b7nes", "Zorns"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nicht verrauchten und verpufften.", "tokens": ["Nicht", "ver\u00b7rauch\u00b7ten", "und", "ver\u00b7puff\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "KON", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.209": {"line.1": {"text": "Deines Sieges Ruhm und Preis", "tokens": ["Dei\u00b7nes", "Sie\u00b7ges", "Ruhm", "und", "Preis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Will ich singen dann und sagen,", "tokens": ["Will", "ich", "sin\u00b7gen", "dann", "und", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "VVFIN", "ADV", "KON", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und dabei, wie Mirjam tat,", "tokens": ["Und", "da\u00b7bei", ",", "wie", "Mir\u00b7jam", "tat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "$,", "PWAV", "NE", "VVFIN", "$,"], "meter": "--+-+--", "measure": "anapaest.init"}, "line.4": {"text": "Tanzen und die Pauke schlagen.\u00ab", "tokens": ["Tan\u00b7zen", "und", "die", "Pau\u00b7ke", "schla\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "KON", "ART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.210": {"line.1": {"text": "In die Rede grimmig fiel", "tokens": ["In", "die", "Re\u00b7de", "grim\u00b7mig", "fiel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADJD", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Jetzt der M\u00f6nch dem Zornentflammten:", "tokens": ["Jetzt", "der", "M\u00f6nch", "dem", "Zor\u00b7nent\u00b7flamm\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbmag dich selbst der Herr verderben,", "tokens": ["\u00bb", "mag", "dich", "selbst", "der", "Herr", "ver\u00b7der\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dich Verfluchten und Verdammten!", "tokens": ["Dich", "Ver\u00b7fluch\u00b7ten", "und", "Ver\u00b7damm\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.211": {"line.1": {"text": "Trotzen kann ich deinen Teufeln,", "tokens": ["Trot\u00b7zen", "kann", "ich", "dei\u00b7nen", "Teu\u00b7feln", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deinem schmutz'gen Fliegengotte,", "tokens": ["Dei\u00b7nem", "schmutz'\u00b7gen", "Flie\u00b7gen\u00b7got\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Luzifer und Beelzebube,", "tokens": ["Lu\u00b7zi\u00b7fer", "und", "Beel\u00b7ze\u00b7bu\u00b7be", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Belial und Astarothe.", "tokens": ["Be\u00b7li\u00b7al", "und", "As\u00b7ta\u00b7ro\u00b7the", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.212": {"line.1": {"text": "Trotzen kann ich deinen Geistern,", "tokens": ["Trot\u00b7zen", "kann", "ich", "dei\u00b7nen", "Geis\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deinen dunkeln H\u00f6llenpossen,", "tokens": ["Dei\u00b7nen", "dun\u00b7keln", "H\u00f6l\u00b7len\u00b7pos\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn in mir ist Jesus Christus,", "tokens": ["Denn", "in", "mir", "ist", "Je\u00b7sus", "Chris\u00b7tus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "VAFIN", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Habe seinen Leib genossen.", "tokens": ["Ha\u00b7be", "sei\u00b7nen", "Leib", "ge\u00b7nos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.213": {"line.1": {"text": "Christus ist mein Leibgericht,", "tokens": ["Chris\u00b7tus", "ist", "mein", "Leib\u00b7ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Schmeckt viel besser als Leviathan", "tokens": ["Schmeckt", "viel", "bes\u00b7ser", "als", "Le\u00b7vi\u00b7a\u00b7than"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADJD", "KOKOM", "NE"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Mit der wei\u00dfen Knoblauchsauce,", "tokens": ["Mit", "der", "wei\u00b7\u00dfen", "Knob\u00b7lauch\u00b7sau\u00b7ce", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Die vielleicht gekocht der Satan.", "tokens": ["Die", "viel\u00b7leicht", "ge\u00b7kocht", "der", "Sa\u00b7tan", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVPP", "ART", "NN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.214": {"line.1": {"text": "Ach! anstatt zu disputieren,", "tokens": ["Ach", "!", "an\u00b7statt", "zu", "dis\u00b7pu\u00b7tie\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "KOUI", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Lieber m\u00f6cht ich schmoren, braten", "tokens": ["Lie\u00b7ber", "m\u00f6cht", "ich", "schmo\u00b7ren", ",", "bra\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ADJD", "VMFIN", "PPER", "VVINF", "$,", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf dem w\u00e4rmsten Scheiterhaufen", "tokens": ["Auf", "dem", "w\u00e4rms\u00b7ten", "Schei\u00b7ter\u00b7hau\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dich und deine Kameraden.\u00ab", "tokens": ["Dich", "und", "dei\u00b7ne", "Ka\u00b7me\u00b7ra\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "KON", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.215": {"line.1": {"text": "Also tost in Schimpf und Ernst", "tokens": ["Al\u00b7so", "tost", "in", "Schimpf", "und", "Ernst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Das Turnei f\u00fcr Gott und Glauben,", "tokens": ["Das", "Tur\u00b7nei", "f\u00fcr", "Gott", "und", "Glau\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch die K\u00e4mpen ganz vergeblich", "tokens": ["Doch", "die", "K\u00e4m\u00b7pen", "ganz", "ver\u00b7geb\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Kreischen, schelten, w\u00fcten, schnauben.", "tokens": ["Krei\u00b7schen", ",", "schel\u00b7ten", ",", "w\u00fc\u00b7ten", ",", "schnau\u00b7ben", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.216": {"line.1": {"text": "Schon zw\u00f6lf Stunden w\u00e4hrt der Kampf,", "tokens": ["Schon", "zw\u00f6lf", "Stun\u00b7den", "w\u00e4hrt", "der", "Kampf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Dem kein End' ist abzuschauen;", "tokens": ["Dem", "kein", "End'", "ist", "ab\u00b7zu\u00b7schau\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VAFIN", "VVIZU", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fcde wird das Publikum,", "tokens": ["M\u00fc\u00b7de", "wird", "das", "Pub\u00b7li\u00b7kum", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und es schwitzen stark die Frauen.", "tokens": ["Und", "es", "schwit\u00b7zen", "stark", "die", "Frau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.217": {"line.1": {"text": "Auch der Hof wird ungeduldig,", "tokens": ["Auch", "der", "Hof", "wird", "un\u00b7ge\u00b7dul\u00b7dig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Manche Zofe g\u00e4hnt ein wenig.", "tokens": ["Man\u00b7che", "Zo\u00b7fe", "g\u00e4hnt", "ein", "we\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ART", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Zu der sch\u00f6nen K\u00f6nigin", "tokens": ["Zu", "der", "sch\u00f6\u00b7nen", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wendet fragend sich der K\u00f6nig:", "tokens": ["Wen\u00b7det", "fra\u00b7gend", "sich", "der", "K\u00f6\u00b7nig", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PRF", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.218": {"line.1": {"text": "\u00bbsagt mir, was ist Eure Meinung?", "tokens": ["\u00bb", "sagt", "mir", ",", "was", "ist", "Eu\u00b7re", "Mei\u00b7nung", "?"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PWS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wer hat recht von diesen beiden?", "tokens": ["Wer", "hat", "recht", "von", "die\u00b7sen", "bei\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "APPR", "PDAT", "PIAT", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Wollt Ihr f\u00fcr den Rabbi Euch", "tokens": ["Wollt", "Ihr", "f\u00fcr", "den", "Rab\u00b7bi", "Euch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "ART", "NN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Oder f\u00fcr den M\u00f6nch entscheiden?\u00ab", "tokens": ["O\u00b7der", "f\u00fcr", "den", "M\u00f6nch", "ent\u00b7schei\u00b7den", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.219": {"line.1": {"text": "Do\u00f1a Blanka schaut ihn an,", "tokens": ["Do\u00f1a", "Blan\u00b7ka", "schaut", "ihn", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Und wie sinnend ihre H\u00e4nde", "tokens": ["Und", "wie", "sin\u00b7nend", "ih\u00b7re", "H\u00e4n\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ADJD", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit verschr\u00e4nkten Fingern dr\u00fcckt sie", "tokens": ["Mit", "ver\u00b7schr\u00e4nk\u00b7ten", "Fin\u00b7gern", "dr\u00fcckt", "sie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "An die Stirn und spricht am Ende:", "tokens": ["An", "die", "Stirn", "und", "spricht", "am", "En\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "VVFIN", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.220": {"line.1": {"text": "\u00bbwelcher recht hat, wei\u00df ich nicht \u2013", "tokens": ["\u00bb", "wel\u00b7cher", "recht", "hat", ",", "wei\u00df", "ich", "nicht", "\u2013"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PRELS", "ADV", "VAFIN", "$,", "VVFIN", "PPER", "PTKNEG", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Doch es will mich schier bed\u00fcnken,", "tokens": ["Doch", "es", "will", "mich", "schier", "be\u00b7d\u00fcn\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PRF", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df der Rabbi und der M\u00f6nch,", "tokens": ["Da\u00df", "der", "Rab\u00b7bi", "und", "der", "M\u00f6nch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df sie alle beide stinken.\u00ab", "tokens": ["Da\u00df", "sie", "al\u00b7le", "bei\u00b7de", "stin\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PIAT", "PIS", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}