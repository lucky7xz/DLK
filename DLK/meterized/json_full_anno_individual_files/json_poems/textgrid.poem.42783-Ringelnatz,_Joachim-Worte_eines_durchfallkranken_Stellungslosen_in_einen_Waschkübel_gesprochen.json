{"textgrid.poem.42783": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Worte eines durchfallkranken Stellungslosen in einen Waschk\u00fcbel gesprochen", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Blo\u00df weil ich nicht aus Preu\u00dfen geb\u00fcrtig.", "tokens": ["Blo\u00df", "weil", "ich", "nicht", "aus", "Preu\u00b7\u00dfen", "ge\u00b7b\u00fcr\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PTKNEG", "APPR", "NE", "ADJD", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo hab ich nur den Impfschein verloren?", "tokens": ["Wo", "hab", "ich", "nur", "den", "Impf\u00b7schein", "ver\u00b7lo\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das lange Warten auf den Korridoren,", "tokens": ["Das", "lan\u00b7ge", "War\u00b7ten", "auf", "den", "Kor\u00b7ri\u00b7do\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das ist so un \u2013, so unw\u00fcrdig.", "tokens": ["Das", "ist", "so", "un", "\u2013", ",", "so", "un\u00b7w\u00fcr\u00b7dig", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "FM", "$(", "$,", "ADV", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "W\u00e4ren wenigstens meine Haare geschoren.", "tokens": ["W\u00e4\u00b7ren", "we\u00b7nigs\u00b7tens", "mei\u00b7ne", "Haa\u00b7re", "ge\u00b7scho\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Und den Durchfall habe ich auch.", "tokens": ["Und", "den", "Durch\u00b7fall", "ha\u00b7be", "ich", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Das geht mitten im Gespr\u00e4ch pl\u00f6tzlich eiskalt aus dem Bauch.", "tokens": ["Das", "geht", "mit\u00b7ten", "im", "Ge\u00b7spr\u00e4ch", "pl\u00f6tz\u00b7lich", "eis\u00b7kalt", "aus", "dem", "Bauch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPRART", "NN", "ADJD", "ADJD", "APPR", "ART", "NN", "$."], "meter": "--+-+-+--++--+", "measure": "iambic.hexa.relaxed"}}, "stanza.2": {"line.1": {"text": "Als mich Mi\u00df Hedwin erkannte und rief,", "tokens": ["Als", "mich", "Mi\u00df", "Hed\u00b7win", "er\u00b7kann\u00b7te", "und", "rief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "NE", "VVFIN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Die hab ich vor Jahren, in Genf, einmal \u2013 versetzt.", "tokens": ["Die", "hab", "ich", "vor", "Jah\u00b7ren", ",", "in", "Genf", ",", "ein\u00b7mal", "\u2013", "ver\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "NN", "$,", "APPR", "NE", "$,", "ADV", "$(", "VVPP", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nun sind meine Abs\u00e4tze schief.", "tokens": ["Nun", "sind", "mei\u00b7ne", "Ab\u00b7s\u00e4t\u00b7ze", "schief", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Und sie trug ein Reitkleid und f\u00fctterte K\u00fcken.", "tokens": ["Und", "sie", "trug", "ein", "Reit\u00b7kleid", "und", "f\u00fct\u00b7ter\u00b7te", "K\u00fc\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "KON", "VVFIN", "NN", "$."], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Aber ich darf mich nicht b\u00fccken.", "tokens": ["A\u00b7ber", "ich", "darf", "mich", "nicht", "b\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Denn meine \u2013 ach mein ganzes Herz ist zerfetzt.", "tokens": ["Denn", "mei\u00b7ne", "\u2013", "ach", "mein", "gan\u00b7zes", "Herz", "ist", "zer\u00b7fetzt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "$(", "XY", "PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.3": {"line.1": {"text": "Ob ich gespeist habe?", "tokens": ["Ob", "ich", "ge\u00b7speist", "ha\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.2": {"text": "Ob mir die Hecke gefiele?", "tokens": ["Ob", "mir", "die", "He\u00b7cke", "ge\u00b7fie\u00b7le", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ja ich habe \u2013 gespeist. \u2013 (In Genf!", "tokens": ["Ja", "ich", "ha\u00b7be", "\u2013", "ge\u00b7speist", ".", "\u2013", "(", "In", "Genf", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["PTKANT", "PPER", "VAFIN", "$(", "VVPP", "$.", "$(", "$(", "APPR", "NE", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Und zuletzt, vor drei Tagen, Semmel mit Senf)", "tokens": ["Und", "zu\u00b7letzt", ",", "vor", "drei", "Ta\u00b7gen", ",", "Sem\u00b7mel", "mit", "Senf", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "CARD", "NN", "$,", "NN", "APPR", "NE", "$("], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und mich k\u00f6nnen alle Hecken", "tokens": ["Und", "mich", "k\u00f6n\u00b7nen", "al\u00b7le", "He\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Am Asche \u2013.", "tokens": ["Am", "A\u00b7sche", "\u2013", "."], "token_info": ["word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "$(", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.4": {"line.1": {"text": "Vergessen sei Genf, vergessen die ganze Schweiz!", "tokens": ["Ver\u00b7ges\u00b7sen", "sei", "Genf", ",", "ver\u00b7ges\u00b7sen", "die", "gan\u00b7ze", "Schweiz", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "NE", "$,", "VVFIN", "ART", "ADJA", "NE", "$."], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "D\u00fcrfte ich nur noch einmal in Seifhennersdorf oder Zeitz", "tokens": ["D\u00fcrf\u00b7te", "ich", "nur", "noch", "ein\u00b7mal", "in", "Seif\u00b7hen\u00b7ners\u00b7dorf", "o\u00b7der", "Zeitz"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "ADV", "APPR", "NE", "KON", "NN"], "meter": "+-+-+-+-+--+--+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "Steine klopfen.", "tokens": ["Stei\u00b7ne", "klop\u00b7fen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Ach! \u2013 ich m\u00f6chte jenem verdammten", "tokens": ["Ach", "!", "\u2013", "ich", "m\u00f6ch\u00b7te", "je\u00b7nem", "ver\u00b7damm\u00b7ten"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "$(", "PPER", "VMFIN", "PDAT", "ADJA"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Stellenvermittlungsbeamten", "tokens": ["Stel\u00b7len\u00b7ver\u00b7mitt\u00b7lungs\u00b7be\u00b7am\u00b7ten"], "token_info": ["word"], "pos": ["NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.6": {"text": "Siebzehn Legitimationspapiere meines Gro\u00dfvaters m\u00fctterlicherseits", "tokens": ["Sieb\u00b7zehn", "Le\u00b7gi\u00b7ti\u00b7ma\u00b7ti\u00b7ons\u00b7pa\u00b7pie\u00b7re", "mei\u00b7nes", "Gro\u00df\u00b7va\u00b7ters", "m\u00fct\u00b7ter\u00b7li\u00b7cher\u00b7seits"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.7": {"text": "In den Rachen stopfen!", "tokens": ["In", "den", "Ra\u00b7chen", "stop\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.5": {"line.1": {"text": "Auch hat mich vor\u00fcbergehend durchzuckt:", "tokens": ["Auch", "hat", "mich", "vor\u00b7\u00fc\u00b7ber\u00b7ge\u00b7hend", "durch\u00b7zuckt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich wollte sterben nach einer grellen Raketentat.", "tokens": ["Ich", "woll\u00b7te", "ster\u00b7ben", "nach", "ei\u00b7ner", "grel\u00b7len", "Ra\u00b7ke\u00b7ten\u00b7tat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Ich habe Lysol und einen Drillbohrer verschluckt.", "tokens": ["Ich", "ha\u00b7be", "Ly\u00b7sol", "und", "ei\u00b7nen", "Drill\u00b7boh\u00b7rer", "ver\u00b7schluckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "KON", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ich sandte ein Kuvert an den Hamburger Senat;", "tokens": ["Ich", "sand\u00b7te", "ein", "Ku\u00b7vert", "an", "den", "Ham\u00b7bur\u00b7ger", "Se\u00b7nat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "In das Kuvert hatte ich kr\u00e4ftig gespuckt.", "tokens": ["In", "das", "Ku\u00b7vert", "hat\u00b7te", "ich", "kr\u00e4f\u00b7tig", "ge\u00b7spuckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Aber niemand glaubt an den Dreck.", "tokens": ["A\u00b7ber", "nie\u00b7mand", "glaubt", "an", "den", "Dreck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.7": {"line.1": {"text": "Nun ist meine Seife weg;", "tokens": ["Nun", "ist", "mei\u00b7ne", "Sei\u00b7fe", "weg", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Irgend jemand st\u00f6bert in meinen Taschen. \u2013", "tokens": ["Ir\u00b7gend", "je\u00b7mand", "st\u00f6\u00b7bert", "in", "mei\u00b7nen", "Ta\u00b7schen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "PIS", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}}, "stanza.8": {"line.1": {"text": "Ich kann mir doch nicht", "tokens": ["Ich", "kann", "mir", "doch", "nicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Das Gesicht", "tokens": ["Das", "Ge\u00b7sicht"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Mit einem Bouillonw\u00fcrfel waschen.", "tokens": ["Mit", "ei\u00b7nem", "Bou\u00b7il\u00b7lon\u00b7w\u00fcr\u00b7fel", "wa\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Nun warte ich auf gigantisches Weltgeschehn.", "tokens": ["Nun", "war\u00b7te", "ich", "auf", "gi\u00b7gan\u00b7ti\u00b7sches", "Welt\u00b7ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Wenn's mich \u2013 zusammen mit den andern \u2013 zerfleischt,", "tokens": ["Wenn's", "mich", "\u2013", "zu\u00b7sam\u00b7men", "mit", "den", "an\u00b7dern", "\u2013", "zer\u00b7fleischt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$(", "ADV", "APPR", "ART", "ADJA", "$(", "VVPP", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Wenn das Sterben der anderen, Gl\u00fccklichen mich umkreischt,", "tokens": ["Wenn", "das", "Ster\u00b7ben", "der", "an\u00b7de\u00b7ren", ",", "Gl\u00fcck\u00b7li\u00b7chen", "mich", "um\u00b7kreischt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "ADJA", "$,", "NN", "PPER", "VVFIN", "$,"], "meter": "--+--+--+--+-+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "\u2013 Dann \u2013", "tokens": ["\u2013", "Dann", "\u2013"], "token_info": ["punct", "word", "punct"], "pos": ["$(", "ADV", "$("], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dann will ich mir eine Zigarette drehn!", "tokens": ["Dann", "will", "ich", "mir", "ei\u00b7ne", "Zi\u00b7ga\u00b7ret\u00b7te", "drehn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Blo\u00df weil ich nicht aus Preu\u00dfen geb\u00fcrtig.", "tokens": ["Blo\u00df", "weil", "ich", "nicht", "aus", "Preu\u00b7\u00dfen", "ge\u00b7b\u00fcr\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "PTKNEG", "APPR", "NE", "ADJD", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo hab ich nur den Impfschein verloren?", "tokens": ["Wo", "hab", "ich", "nur", "den", "Impf\u00b7schein", "ver\u00b7lo\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das lange Warten auf den Korridoren,", "tokens": ["Das", "lan\u00b7ge", "War\u00b7ten", "auf", "den", "Kor\u00b7ri\u00b7do\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das ist so un \u2013, so unw\u00fcrdig.", "tokens": ["Das", "ist", "so", "un", "\u2013", ",", "so", "un\u00b7w\u00fcr\u00b7dig", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "FM", "$(", "$,", "ADV", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "W\u00e4ren wenigstens meine Haare geschoren.", "tokens": ["W\u00e4\u00b7ren", "we\u00b7nigs\u00b7tens", "mei\u00b7ne", "Haa\u00b7re", "ge\u00b7scho\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.6": {"text": "Und den Durchfall habe ich auch.", "tokens": ["Und", "den", "Durch\u00b7fall", "ha\u00b7be", "ich", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "ADV", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Das geht mitten im Gespr\u00e4ch pl\u00f6tzlich eiskalt aus dem Bauch.", "tokens": ["Das", "geht", "mit\u00b7ten", "im", "Ge\u00b7spr\u00e4ch", "pl\u00f6tz\u00b7lich", "eis\u00b7kalt", "aus", "dem", "Bauch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPRART", "NN", "ADJD", "ADJD", "APPR", "ART", "NN", "$."], "meter": "--+-+-+--++--+", "measure": "iambic.hexa.relaxed"}}, "stanza.11": {"line.1": {"text": "Als mich Mi\u00df Hedwin erkannte und rief,", "tokens": ["Als", "mich", "Mi\u00df", "Hed\u00b7win", "er\u00b7kann\u00b7te", "und", "rief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "NE", "VVFIN", "KON", "VVFIN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Die hab ich vor Jahren, in Genf, einmal \u2013 versetzt.", "tokens": ["Die", "hab", "ich", "vor", "Jah\u00b7ren", ",", "in", "Genf", ",", "ein\u00b7mal", "\u2013", "ver\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "NN", "$,", "APPR", "NE", "$,", "ADV", "$(", "VVPP", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nun sind meine Abs\u00e4tze schief.", "tokens": ["Nun", "sind", "mei\u00b7ne", "Ab\u00b7s\u00e4t\u00b7ze", "schief", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Und sie trug ein Reitkleid und f\u00fctterte K\u00fcken.", "tokens": ["Und", "sie", "trug", "ein", "Reit\u00b7kleid", "und", "f\u00fct\u00b7ter\u00b7te", "K\u00fc\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN", "KON", "VVFIN", "NN", "$."], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Aber ich darf mich nicht b\u00fccken.", "tokens": ["A\u00b7ber", "ich", "darf", "mich", "nicht", "b\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Denn meine \u2013 ach mein ganzes Herz ist zerfetzt.", "tokens": ["Denn", "mei\u00b7ne", "\u2013", "ach", "mein", "gan\u00b7zes", "Herz", "ist", "zer\u00b7fetzt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "$(", "XY", "PPOSAT", "ADJA", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.12": {"line.1": {"text": "Ob ich gespeist habe?", "tokens": ["Ob", "ich", "ge\u00b7speist", "ha\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.2": {"text": "Ob mir die Hecke gefiele?", "tokens": ["Ob", "mir", "die", "He\u00b7cke", "ge\u00b7fie\u00b7le", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ja ich habe \u2013 gespeist. \u2013 (In Genf!", "tokens": ["Ja", "ich", "ha\u00b7be", "\u2013", "ge\u00b7speist", ".", "\u2013", "(", "In", "Genf", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "punct", "word", "word", "punct"], "pos": ["PTKANT", "PPER", "VAFIN", "$(", "VVPP", "$.", "$(", "$(", "APPR", "NE", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Und zuletzt, vor drei Tagen, Semmel mit Senf)", "tokens": ["Und", "zu\u00b7letzt", ",", "vor", "drei", "Ta\u00b7gen", ",", "Sem\u00b7mel", "mit", "Senf", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "CARD", "NN", "$,", "NN", "APPR", "NE", "$("], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und mich k\u00f6nnen alle Hecken", "tokens": ["Und", "mich", "k\u00f6n\u00b7nen", "al\u00b7le", "He\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Am Asche \u2013.", "tokens": ["Am", "A\u00b7sche", "\u2013", "."], "token_info": ["word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "$(", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.13": {"line.1": {"text": "Vergessen sei Genf, vergessen die ganze Schweiz!", "tokens": ["Ver\u00b7ges\u00b7sen", "sei", "Genf", ",", "ver\u00b7ges\u00b7sen", "die", "gan\u00b7ze", "Schweiz", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "NE", "$,", "VVFIN", "ART", "ADJA", "NE", "$."], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "D\u00fcrfte ich nur noch einmal in Seifhennersdorf oder Zeitz", "tokens": ["D\u00fcrf\u00b7te", "ich", "nur", "noch", "ein\u00b7mal", "in", "Seif\u00b7hen\u00b7ners\u00b7dorf", "o\u00b7der", "Zeitz"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "ADV", "APPR", "NE", "KON", "NN"], "meter": "+-+-+-+-+--+--+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "Steine klopfen.", "tokens": ["Stei\u00b7ne", "klop\u00b7fen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Ach! \u2013 ich m\u00f6chte jenem verdammten", "tokens": ["Ach", "!", "\u2013", "ich", "m\u00f6ch\u00b7te", "je\u00b7nem", "ver\u00b7damm\u00b7ten"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "$(", "PPER", "VMFIN", "PDAT", "ADJA"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Stellenvermittlungsbeamten", "tokens": ["Stel\u00b7len\u00b7ver\u00b7mitt\u00b7lungs\u00b7be\u00b7am\u00b7ten"], "token_info": ["word"], "pos": ["NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.6": {"text": "Siebzehn Legitimationspapiere meines Gro\u00dfvaters m\u00fctterlicherseits", "tokens": ["Sieb\u00b7zehn", "Le\u00b7gi\u00b7ti\u00b7ma\u00b7ti\u00b7ons\u00b7pa\u00b7pie\u00b7re", "mei\u00b7nes", "Gro\u00df\u00b7va\u00b7ters", "m\u00fct\u00b7ter\u00b7li\u00b7cher\u00b7seits"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.7": {"text": "In den Rachen stopfen!", "tokens": ["In", "den", "Ra\u00b7chen", "stop\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.14": {"line.1": {"text": "Auch hat mich vor\u00fcbergehend durchzuckt:", "tokens": ["Auch", "hat", "mich", "vor\u00b7\u00fc\u00b7ber\u00b7ge\u00b7hend", "durch\u00b7zuckt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich wollte sterben nach einer grellen Raketentat.", "tokens": ["Ich", "woll\u00b7te", "ster\u00b7ben", "nach", "ei\u00b7ner", "grel\u00b7len", "Ra\u00b7ke\u00b7ten\u00b7tat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Ich habe Lysol und einen Drillbohrer verschluckt.", "tokens": ["Ich", "ha\u00b7be", "Ly\u00b7sol", "und", "ei\u00b7nen", "Drill\u00b7boh\u00b7rer", "ver\u00b7schluckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "KON", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ich sandte ein Kuvert an den Hamburger Senat;", "tokens": ["Ich", "sand\u00b7te", "ein", "Ku\u00b7vert", "an", "den", "Ham\u00b7bur\u00b7ger", "Se\u00b7nat", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "In das Kuvert hatte ich kr\u00e4ftig gespuckt.", "tokens": ["In", "das", "Ku\u00b7vert", "hat\u00b7te", "ich", "kr\u00e4f\u00b7tig", "ge\u00b7spuckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.15": {"line.1": {"text": "Aber niemand glaubt an den Dreck.", "tokens": ["A\u00b7ber", "nie\u00b7mand", "glaubt", "an", "den", "Dreck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.16": {"line.1": {"text": "Nun ist meine Seife weg;", "tokens": ["Nun", "ist", "mei\u00b7ne", "Sei\u00b7fe", "weg", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Irgend jemand st\u00f6bert in meinen Taschen. \u2013", "tokens": ["Ir\u00b7gend", "je\u00b7mand", "st\u00f6\u00b7bert", "in", "mei\u00b7nen", "Ta\u00b7schen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "PIS", "VVFIN", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}}, "stanza.17": {"line.1": {"text": "Ich kann mir doch nicht", "tokens": ["Ich", "kann", "mir", "doch", "nicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Das Gesicht", "tokens": ["Das", "Ge\u00b7sicht"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Mit einem Bouillonw\u00fcrfel waschen.", "tokens": ["Mit", "ei\u00b7nem", "Bou\u00b7il\u00b7lon\u00b7w\u00fcr\u00b7fel", "wa\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.18": {"line.1": {"text": "Nun warte ich auf gigantisches Weltgeschehn.", "tokens": ["Nun", "war\u00b7te", "ich", "auf", "gi\u00b7gan\u00b7ti\u00b7sches", "Welt\u00b7ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+--+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Wenn's mich \u2013 zusammen mit den andern \u2013 zerfleischt,", "tokens": ["Wenn's", "mich", "\u2013", "zu\u00b7sam\u00b7men", "mit", "den", "an\u00b7dern", "\u2013", "zer\u00b7fleischt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$(", "ADV", "APPR", "ART", "ADJA", "$(", "VVPP", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Wenn das Sterben der anderen, Gl\u00fccklichen mich umkreischt,", "tokens": ["Wenn", "das", "Ster\u00b7ben", "der", "an\u00b7de\u00b7ren", ",", "Gl\u00fcck\u00b7li\u00b7chen", "mich", "um\u00b7kreischt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "ADJA", "$,", "NN", "PPER", "VVFIN", "$,"], "meter": "--+--+--+--+-+", "measure": "anapaest.tetra.plus"}, "line.4": {"text": "\u2013 Dann \u2013", "tokens": ["\u2013", "Dann", "\u2013"], "token_info": ["punct", "word", "punct"], "pos": ["$(", "ADV", "$("], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Dann will ich mir eine Zigarette drehn!", "tokens": ["Dann", "will", "ich", "mir", "ei\u00b7ne", "Zi\u00b7ga\u00b7ret\u00b7te", "drehn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}}}}