{"textgrid.poem.57502": {"metadata": {"author": {"name": "Gottsched, Johann Christoph", "birth": "N.A.", "death": "N.A."}, "title": "1L: Belobte ", "genre": "verse", "period": "N.A.", "pub_year": 1733, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Belobte ", "tokens": ["Be\u00b7lob\u00b7te"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Da\u00df ich mir schwerlich darf die Antwort unterstehn,", "tokens": ["Da\u00df", "ich", "mir", "schwer\u00b7lich", "darf", "die", "Ant\u00b7wort", "un\u00b7ter\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADJD", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Reime flie\u00dfen dir so rein und ungezwungen,", "tokens": ["Die", "Rei\u00b7me", "flie\u00b7\u00dfen", "dir", "so", "rein", "und", "un\u00b7ge\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als sie vor Zeiten kaum der ", "tokens": ["Als", "sie", "vor", "Zei\u00b7ten", "kaum", "der"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADV", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Jedoch ich tadle sie, (du hast es mir erlaubt)", "tokens": ["Je\u00b7doch", "ich", "tad\u00b7le", "sie", ",", "(", "du", "hast", "es", "mir", "er\u00b7laubt", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PPER", "$,", "$(", "PPER", "VAFIN", "PPER", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und habe kaum davon das zehnte Wort geglaubt.", "tokens": ["Und", "ha\u00b7be", "kaum", "da\u00b7von", "das", "zehn\u00b7te", "Wort", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PAV", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was meynst du nun von mir? Mich d\u00fcnkt du wirst mich schelten,", "tokens": ["Was", "meynst", "du", "nun", "von", "mir", "?", "Mich", "d\u00fcnkt", "du", "wirst", "mich", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$.", "PPER", "VVFIN", "PPER", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und l\u00e4ssest mich den Spruch durch deinen Zorn entgelten.", "tokens": ["Und", "l\u00e4s\u00b7sest", "mich", "den", "Spruch", "durch", "dei\u00b7nen", "Zorn", "ent\u00b7gel\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Doch, was kann ich daf\u00fcr? du hast mirs auferlegt:", "tokens": ["Doch", ",", "was", "kann", "ich", "da\u00b7f\u00fcr", "?", "du", "hast", "mirs", "auf\u00b7er\u00b7legt", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWS", "VMFIN", "PPER", "PAV", "$.", "PPER", "VAFIN", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Drum nimm damit vorlieb, wie man zu sagen pflegt;", "tokens": ["Drum", "nimm", "da\u00b7mit", "vor\u00b7lieb", ",", "wie", "man", "zu", "sa\u00b7gen", "pflegt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PAV", "VVFIN", "$,", "PWAV", "PIS", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und zwinge niemand mehr zu Leistung solcher Pflichten,", "tokens": ["Und", "zwin\u00b7ge", "nie\u00b7mand", "mehr", "zu", "Leis\u00b7tung", "sol\u00b7cher", "Pflich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADV", "APPR", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Die so verm\u00f6gend sind, ein Unheil anzurichten.", "tokens": ["Die", "so", "ver\u00b7m\u00f6\u00b7gend", "sind", ",", "ein", "Un\u00b7heil", "an\u00b7zu\u00b7rich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVPP", "VAFIN", "$,", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Allein, ich fahre fort, nachdem ich es gewagt,", "tokens": ["Al\u00b7lein", ",", "ich", "fah\u00b7re", "fort", ",", "nach\u00b7dem", "ich", "es", "ge\u00b7wagt", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "PTKVZ", "$,", "KOUS", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und dir so deutsch und rund die Wahrheit hergesagt:", "tokens": ["Und", "dir", "so", "deutsch", "und", "rund", "die", "Wahr\u00b7heit", "her\u00b7ge\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "ADJD", "KON", "ADJD", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Denn diese ziert allein die Schriften der Poeten,", "tokens": ["Denn", "die\u00b7se", "ziert", "al\u00b7lein", "die", "Schrif\u00b7ten", "der", "Po\u00b7et\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und wo man sie vermi\u00dft, da mu\u00df das Blatt err\u00f6then.", "tokens": ["Und", "wo", "man", "sie", "ver\u00b7mi\u00dft", ",", "da", "mu\u00df", "das", "Blatt", "er\u00b7r\u00f6\u00b7then", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PIS", "PPER", "VVPP", "$,", "ADV", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht so? du giebst mir recht, geschickte Dichterinn!", "tokens": ["Nicht", "so", "?", "du", "giebst", "mir", "recht", ",", "ge\u00b7schick\u00b7te", "Dich\u00b7te\u00b7rinn", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie k\u00f6mmts denn, da\u00df nur ich nicht werth gewesen bin,", "tokens": ["Wie", "k\u00f6mmts", "denn", ",", "da\u00df", "nur", "ich", "nicht", "werth", "ge\u00b7we\u00b7sen", "bin", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ADV", "$,", "KOUS", "ADV", "PPER", "PTKNEG", "ADJD", "VAPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Wahrheit, die dir sonst best\u00e4ndig lieb gewesen,", "tokens": ["Die", "Wahr\u00b7heit", ",", "die", "dir", "sonst", "be\u00b7st\u00e4n\u00b7dig", "lieb", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADJD", "ADJD", "VAPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "In deiner letzten Schrift, nach alter Art, zu lesen?", "tokens": ["In", "dei\u00b7ner", "letz\u00b7ten", "Schrift", ",", "nach", "al\u00b7ter", "Art", ",", "zu", "le\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du lobst mich gar zu sehr, und setzest keinen Reim,", "tokens": ["Du", "lobst", "mich", "gar", "zu", "sehr", ",", "und", "set\u00b7zest", "kei\u00b7nen", "Reim", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKA", "ADV", "$,", "KON", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Den nicht die Schm\u00e4ucheley, durch s\u00fc\u00dfes Honigseim,", "tokens": ["Den", "nicht", "die", "Schm\u00e4u\u00b7che\u00b7ley", ",", "durch", "s\u00fc\u00b7\u00dfes", "Ho\u00b7ni\u00b7gseim", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "ART", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ganz \u00fcberzogen hat. Wie hab ich das verdienet?", "tokens": ["Ganz", "\u00fc\u00b7berz\u00b7o\u00b7gen", "hat", ".", "Wie", "hab", "ich", "das", "ver\u00b7die\u00b7net", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAFIN", "$.", "PWAV", "VAFIN", "PPER", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Hat deine Muse sich nicht gar zu viel erk\u00fchnet;", "tokens": ["Hat", "dei\u00b7ne", "Mu\u00b7se", "sich", "nicht", "gar", "zu", "viel", "er\u00b7k\u00fch\u00b7net", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PRF", "PTKNEG", "ADV", "PTKA", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wenn sie ein Lob besingt, das so ver\u00e4chtlich ist,", "tokens": ["Wenn", "sie", "ein", "Lob", "be\u00b7singt", ",", "das", "so", "ver\u00b7\u00e4cht\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,", "PRELS", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Und wo man \u00fcberall der Wahrheit Spur vermi\u00dft?", "tokens": ["Und", "wo", "man", "\u00fc\u00b7be\u00b7rall", "der", "Wahr\u00b7heit", "Spur", "ver\u00b7mi\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PIS", "ADV", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Doch, da\u00df du wissen magst, was mir im Sinne lieget:", "tokens": ["Doch", ",", "da\u00df", "du", "wis\u00b7sen", "magst", ",", "was", "mir", "im", "Sin\u00b7ne", "lie\u00b7get", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$,", "PWS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "So glaube, da\u00df mich nie ein Schm\u00e4uchelwort vergn\u00fcget.", "tokens": ["So", "glau\u00b7be", ",", "da\u00df", "mich", "nie", "ein", "Schm\u00e4u\u00b7chel\u00b7wort", "ver\u00b7gn\u00fc\u00b7get", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ich weis wohl, was du denkst. Mich d\u00fcnkt, du rufst mir zu.", "tokens": ["Ich", "weis", "wohl", ",", "was", "du", "denkst", ".", "Mich", "d\u00fcnkt", ",", "du", "rufst", "mir", "zu", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "ADV", "$,", "PWS", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Freund! wer lobte wohl bisher so gern als du?", "tokens": ["Mein", "Freund", "!", "wer", "lob\u00b7te", "wohl", "bis\u00b7her", "so", "gern", "als", "du", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "PWS", "VVFIN", "ADV", "ADV", "ADV", "ADV", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ganz recht, ich kenne mich, und will es frey gestehen,", "tokens": ["Ganz", "recht", ",", "ich", "ken\u00b7ne", "mich", ",", "und", "will", "es", "frey", "ge\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PPER", "VVFIN", "PPER", "$,", "KON", "VMFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ich kann mich ebenfalls, wie sonst ein Mensch, vergehen.", "tokens": ["Ich", "kann", "mich", "e\u00b7ben\u00b7falls", ",", "wie", "sonst", "ein", "Mensch", ",", "ver\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "$,", "PWAV", "ADV", "ART", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich habe gern ger\u00fchmt, und stimmte manches Lob", "tokens": ["Ich", "ha\u00b7be", "gern", "ge\u00b7r\u00fchmt", ",", "und", "stimm\u00b7te", "man\u00b7ches", "Lob"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "KON", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit gro\u00dfem Jauchzen an, das den und die erhob.", "tokens": ["Mit", "gro\u00b7\u00dfem", "Jauch\u00b7zen", "an", ",", "das", "den", "und", "die", "er\u00b7hob", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PTKVZ", "$,", "PRELS", "ART", "KON", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Zuweilen wu\u00dft ich gar aus den geringsten Sachen,", "tokens": ["Zu\u00b7wei\u00b7len", "wu\u00dft", "ich", "gar", "aus", "den", "ge\u00b7rings\u00b7ten", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Stax und Mops ver\u00fcbt, ein Wunderding zu machen.", "tokens": ["Die", "Stax", "und", "Mops", "ver\u00b7\u00fcbt", ",", "ein", "Wun\u00b7der\u00b7ding", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ich pries was m\u00e4\u00dfiges als unvergleichlich an;", "tokens": ["Ich", "pries", "was", "m\u00e4\u00b7\u00dfi\u00b7ges", "als", "un\u00b7ver\u00b7gleich\u00b7lich", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PWS", "PIS", "KOKOM", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und ob ich gleich dadurch der Wahrheit weh gethan:", "tokens": ["Und", "ob", "ich", "gleich", "da\u00b7durch", "der", "Wahr\u00b7heit", "weh", "ge\u00b7than", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "PAV", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So war die Absicht doch nicht v\u00f6llig zu verwerfen;", "tokens": ["So", "war", "die", "Ab\u00b7sicht", "doch", "nicht", "v\u00f6l\u00b7lig", "zu", "ver\u00b7wer\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "PTKNEG", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ich suchte durch den Ruhm den Tugendtrieb zu sch\u00e4rfen.", "tokens": ["Ich", "such\u00b7te", "durch", "den", "Ruhm", "den", "Tu\u00b7gend\u00b7trieb", "zu", "sch\u00e4r\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Doch, als die Weisheit mir nach diesem vorgestellt,", "tokens": ["Doch", ",", "als", "die", "Weis\u00b7heit", "mir", "nach", "die\u00b7sem", "vor\u00b7ge\u00b7stellt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "ART", "NN", "PPER", "APPR", "PDAT", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Man f\u00e4nde nichts so sch\u00f6n und trefflich in der Welt,", "tokens": ["Man", "f\u00e4n\u00b7de", "nichts", "so", "sch\u00f6n", "und", "treff\u00b7lich", "in", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "ADV", "ADJD", "KON", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das ohne Tadel w\u00e4r, und keine Flecken h\u00e4tte:", "tokens": ["Das", "oh\u00b7ne", "Ta\u00b7del", "w\u00e4r", ",", "und", "kei\u00b7ne", "Fle\u00b7cken", "h\u00e4t\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "VAFIN", "$,", "KON", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "So seufzt ich oftermals mit jenem um die Wette,", "tokens": ["So", "seufzt", "ich", "of\u00b7ter\u00b7mals", "mit", "je\u00b7nem", "um", "die", "Wet\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der stets mit Traurigkeit der Menschen Schwachheit sah;", "tokens": ["Der", "stets", "mit", "Trau\u00b7rig\u00b7keit", "der", "Men\u00b7schen", "Schwach\u00b7heit", "sah", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NN", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn, nach des P\u00f6bels Wahn, die gr\u00f6\u00dfte That geschah.", "tokens": ["Wenn", ",", "nach", "des", "P\u00f6\u00b7bels", "Wahn", ",", "die", "gr\u00f6\u00df\u00b7te", "That", "ge\u00b7schah", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "APPR", "ART", "NN", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So ist denn, war mein Wort, kein Sterblicher zu loben?", "tokens": ["So", "ist", "denn", ",", "war", "mein", "Wort", ",", "kein", "Sterb\u00b7li\u00b7cher", "zu", "lo\u00b7ben", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "$,", "VAFIN", "PPOSAT", "NN", "$,", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So wird denn all ihr Thun ganz unverdient erhoben?", "tokens": ["So", "wird", "denn", "all", "ihr", "Thun", "ganz", "un\u00b7ver\u00b7di\u00b7ent", "er\u00b7ho\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "PPOSAT", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "So ist denn keine That in allen St\u00fccken rein?", "tokens": ["So", "ist", "denn", "kei\u00b7ne", "That", "in", "al\u00b7len", "St\u00fc\u00b7cken", "rein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Ja, ja! wer loben will, der mu\u00df ein L\u00fcgner seyn.", "tokens": ["Ja", ",", "ja", "!", "wer", "lo\u00b7ben", "will", ",", "der", "mu\u00df", "ein", "L\u00fcg\u00b7ner", "seyn", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADV", "$.", "PWS", "VVINF", "VMFIN", "$,", "ART", "VMFIN", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ich fieng satirisch an, die Thorheit zu besch\u00e4men:", "tokens": ["Ich", "fi\u00b7eng", "sa\u00b7ti\u00b7risch", "an", ",", "die", "Thor\u00b7heit", "zu", "be\u00b7sch\u00e4\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Ich deckte manches auf, das sch\u00f6n von au\u00dfen war,", "tokens": ["Ich", "deck\u00b7te", "man\u00b7ches", "auf", ",", "das", "sch\u00f6n", "von", "au\u00b7\u00dfen", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$,", "PRELS", "ADJD", "APPR", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und stellte manch Gesicht in seiner Bl\u00f6\u00dfe dar:", "tokens": ["Und", "stell\u00b7te", "manch", "Ge\u00b7sicht", "in", "sei\u00b7ner", "Bl\u00f6\u00b7\u00dfe", "dar", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "In Hoffnung, dergestalt, durch die entlarvten Flecken,", "tokens": ["In", "Hoff\u00b7nung", ",", "der\u00b7ge\u00b7stalt", ",", "durch", "die", "ent\u00b7larv\u00b7ten", "Fle\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADV", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die glei\u00dfnerische Welt vom B\u00f6sen abzuschrecken.", "tokens": ["Die", "glei\u00df\u00b7ne\u00b7ri\u00b7sche", "Welt", "vom", "B\u00f6\u00b7sen", "ab\u00b7zu\u00b7schre\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Allein, es war umsonst, die Lauge bi\u00df zu scharf.", "tokens": ["Al\u00b7lein", ",", "es", "war", "um\u00b7sonst", ",", "die", "Lau\u00b7ge", "bi\u00df", "zu", "scharf", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VAFIN", "ADV", "$,", "ART", "NN", "APPR", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es hie\u00df: Wie geht es zu, da\u00df der so l\u00e4stern darf?", "tokens": ["Es", "hie\u00df", ":", "Wie", "geht", "es", "zu", ",", "da\u00df", "der", "so", "l\u00e4s\u00b7tern", "darf", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "ART", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Wer hat ihm immermehr das Strafamt aufgetragen?", "tokens": ["Wer", "hat", "ihm", "im\u00b7mer\u00b7mehr", "das", "Stra\u00b7famt", "auf\u00b7ge\u00b7tra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und wer wird endlich was nach seinen Lehren fragen?", "tokens": ["Und", "wer", "wird", "end\u00b7lich", "was", "nach", "sei\u00b7nen", "Leh\u00b7ren", "fra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADV", "PWS", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So war mein Hoffen aus. Ich merkte, da\u00df die Welt", "tokens": ["So", "war", "mein", "Hof\u00b7fen", "aus", ".", "Ich", "merk\u00b7te", ",", "da\u00df", "die", "Welt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "In allem, was sie thut, nicht viel auf Regeln h\u00e4lt;", "tokens": ["In", "al\u00b7lem", ",", "was", "sie", "thut", ",", "nicht", "viel", "auf", "Re\u00b7geln", "h\u00e4lt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "PPER", "VVFIN", "$,", "PTKNEG", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und lieber blindlings tappt, als recht im Lichte gehet,", "tokens": ["Und", "lie\u00b7ber", "blind\u00b7lings", "tappt", ",", "als", "recht", "im", "Lich\u00b7te", "ge\u00b7het", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "$,", "KOUS", "ADJD", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So nah ihr sichrer Fu\u00df an tausend Fallen stehet.", "tokens": ["So", "nah", "ihr", "sich\u00b7rer", "Fu\u00df", "an", "tau\u00b7send", "Fal\u00b7len", "ste\u00b7het", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "ADJA", "NN", "APPR", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Drum hab ich nach und nach ", "tokens": ["Drum", "hab", "ich", "nach", "und", "nach"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "APPR", "KON", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Doch weil mich ", "tokens": ["Doch", "weil", "mich"], "token_info": ["word", "word", "word"], "pos": ["KON", "KOUS", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Zur Dichtkunst angespornt; so hat mir unter allen,", "tokens": ["Zur", "Dicht\u00b7kunst", "an\u00b7ge\u00b7spornt", ";", "so", "hat", "mir", "un\u00b7ter", "al\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$.", "ADV", "VAFIN", "PPER", "APPR", "PIS", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Die seine Schwestern sind, ", "tokens": ["Die", "sei\u00b7ne", "Schwes\u00b7tern", "sind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Die hat mich, seit der Zeit, mit vielem Ernst gelehrt,", "tokens": ["Die", "hat", "mich", ",", "seit", "der", "Zeit", ",", "mit", "vie\u00b7lem", "Ernst", "ge\u00b7lehrt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "$,", "APPR", "ART", "NN", "$,", "APPR", "PIS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie man der Tugend Werth in klugen Fabeln ehrt;", "tokens": ["Wie", "man", "der", "Tu\u00b7gend", "Werth", "in", "klu\u00b7gen", "Fa\u00b7beln", "ehrt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der Alterth\u00fcmer Nacht in hellen Tag verwandelt,", "tokens": ["Der", "Al\u00b7tert\u00b7h\u00fc\u00b7mer", "Nacht", "in", "hel\u00b7len", "Tag", "ver\u00b7wan\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und in dem Trauerspiel von Sittenlehren handelt;", "tokens": ["Und", "in", "dem", "Trau\u00b7er\u00b7spiel", "von", "Sit\u00b7ten\u00b7leh\u00b7ren", "han\u00b7delt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wie man der Weisheit Macht, der Gro\u00dfmuth St\u00e4rke zeigt,", "tokens": ["Wie", "man", "der", "Weis\u00b7heit", "Macht", ",", "der", "Gro\u00df\u00b7muth", "St\u00e4r\u00b7ke", "zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "NN", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn ein gesetztes Herz kein harter Zufall beugt;", "tokens": ["Wenn", "ein", "ge\u00b7setz\u00b7tes", "Herz", "kein", "har\u00b7ter", "Zu\u00b7fall", "beugt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "PIAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wie hoch die Bosheit w\u00e4chst, wenn ihr die Frevelthaten", "tokens": ["Wie", "hoch", "die", "Bos\u00b7heit", "w\u00e4chst", ",", "wenn", "ihr", "die", "Fre\u00b7vel\u00b7tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Erst wohl von statten gehn, und recht nach Wunsch gerathen;", "tokens": ["Erst", "wohl", "von", "stat\u00b7ten", "gehn", ",", "und", "recht", "nach", "Wunsch", "ge\u00b7ra\u00b7then", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ADJA", "VVINF", "$,", "KON", "ADJD", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und wie des Himmels Zorn mit sich nicht scherzen l\u00e4\u00dft,", "tokens": ["Und", "wie", "des", "Him\u00b7mels", "Zorn", "mit", "sich", "nicht", "scher\u00b7zen", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "NN", "APPR", "PRF", "PTKNEG", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wenn sein gereizter Arm zuletzt der Laster Pest,", "tokens": ["Wenn", "sein", "ge\u00b7reiz\u00b7ter", "Arm", "zu\u00b7letzt", "der", "Las\u00b7ter", "Pest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Mit siegender Gewalt im gr\u00f6\u00dften W\u00fcthen wehret,", "tokens": ["Mit", "sie\u00b7gen\u00b7der", "Ge\u00b7walt", "im", "gr\u00f6\u00df\u00b7ten", "W\u00fct\u00b7hen", "weh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und ihre Raserey auf ihre Scheiteln kehret.", "tokens": ["Und", "ih\u00b7re", "Ra\u00b7se\u00b7rey", "auf", "ih\u00b7re", "Schei\u00b7teln", "keh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die\u00df ist die sichre Bahn, darauf mein Geist sich \u00fcbt,", "tokens": ["Die\u00df", "ist", "die", "sich\u00b7re", "Bahn", ",", "da\u00b7rauf", "mein", "Geist", "sich", "\u00fcbt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "PAV", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wo er nicht schm\u00e4ucheln darf, auch nicht das Tadeln liebt;", "tokens": ["Wo", "er", "nicht", "schm\u00e4u\u00b7cheln", "darf", ",", "auch", "nicht", "das", "Ta\u00b7deln", "liebt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,", "ADV", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Nur blo\u00df die Tugend lobt, nur blo\u00df das Laster sch\u00e4ndet,", "tokens": ["Nur", "blo\u00df", "die", "Tu\u00b7gend", "lobt", ",", "nur", "blo\u00df", "das", "Las\u00b7ter", "sch\u00e4n\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVFIN", "$,", "ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Weil weder Eigennutz, noch Furcht sein Auge blendet.", "tokens": ["Weil", "we\u00b7der", "Ei\u00b7gen\u00b7nutz", ",", "noch", "Furcht", "sein", "Au\u00b7ge", "blen\u00b7det", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KON", "NN", "$,", "ADV", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Doch, wo gerath ich hin? Du kannst hieraus ersehn,", "tokens": ["Doch", ",", "wo", "ge\u00b7rath", "ich", "hin", "?", "Du", "kannst", "hier\u00b7aus", "er\u00b7sehn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWAV", "VVFIN", "PPER", "PTKVZ", "$.", "PPER", "VMFIN", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df deiner Poesie kein Unrecht ist geschehn,", "tokens": ["Da\u00df", "dei\u00b7ner", "Poe\u00b7sie", "kein", "Un\u00b7recht", "ist", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PIAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Belobte Dichterinn; wenn ich dir vorger\u00fccket,", "tokens": ["Be\u00b7lob\u00b7te", "Dich\u00b7te\u00b7rinn", ";", "wenn", "ich", "dir", "vor\u00b7ge\u00b7r\u00fc\u00b7cket", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df sie mein kleines Lob mit Reimen ausgeschm\u00fccket.", "tokens": ["Da\u00df", "sie", "mein", "klei\u00b7nes", "Lob", "mit", "Rei\u00b7men", "aus\u00b7ge\u00b7schm\u00fc\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "ADJA", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Mehr hab ich nicht bemerkt, da\u00df auszusetzen sey.", "tokens": ["Mehr", "hab", "ich", "nicht", "be\u00b7merkt", ",", "da\u00df", "aus\u00b7zu\u00b7set\u00b7zen", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVPP", "$,", "KOUS", "VVIZU", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und l\u00e4\u00dft sich f\u00fcr den Gru\u00df, durch meinen Kiel bedanken;", "tokens": ["Und", "l\u00e4\u00dft", "sich", "f\u00fcr", "den", "Gru\u00df", ",", "durch", "mei\u00b7nen", "Kiel", "be\u00b7dan\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "ART", "NN", "$,", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch z\u00e4hlt der theure Mann sich l\u00e4ngstens zu den Kranken.", "tokens": ["Doch", "z\u00e4hlt", "der", "theu\u00b7re", "Mann", "sich", "l\u00e4ngs\u00b7tens", "zu", "den", "Kran\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PRF", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Augen werden stumpf, es schwindet alle Kraft.", "tokens": ["Die", "Au\u00b7gen", "wer\u00b7den", "stumpf", ",", "es", "schwin\u00b7det", "al\u00b7le", "Kraft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "PPER", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und so f\u00e4llt nach und nach des Geistes Eigenschaft,", "tokens": ["Und", "so", "f\u00e4llt", "nach", "und", "nach", "des", "Geis\u00b7tes", "Ei\u00b7gen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "APPR", "KON", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Davon die halbe Welt bisher die Frucht gelesen:", "tokens": ["Da\u00b7von", "die", "hal\u00b7be", "Welt", "bis\u00b7her", "die", "Frucht", "ge\u00b7le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und kurz, er ist kaum halb, was er bisher gewesen.", "tokens": ["Und", "kurz", ",", "er", "ist", "kaum", "halb", ",", "was", "er", "bis\u00b7her", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,", "PWS", "PPER", "ADV", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Dein ", "tokens": ["Dein"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Verrichte die\u00df von mir an deinen werthen Mann,", "tokens": ["Ver\u00b7rich\u00b7te", "die\u00df", "von", "mir", "an", "dei\u00b7nen", "wert\u00b7hen", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und was dir angeh\u00f6rt. Viel Gl\u00fcck zum neuen Jahre!", "tokens": ["Und", "was", "dir", "an\u00b7ge\u00b7h\u00f6rt", ".", "Viel", "Gl\u00fcck", "zum", "neu\u00b7en", "Jah\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVPP", "$.", "PIAT", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Gott gebe, da\u00df man stets viel Guts von dir erfahre.", "tokens": ["Gott", "ge\u00b7be", ",", "da\u00df", "man", "stets", "viel", "Guts", "von", "dir", "er\u00b7fah\u00b7re", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "KOUS", "PIS", "ADV", "PIAT", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}, "stanza.9": {"line.1": {"text": "Belobte ", "tokens": ["Be\u00b7lob\u00b7te"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Da\u00df ich mir schwerlich darf die Antwort unterstehn,", "tokens": ["Da\u00df", "ich", "mir", "schwer\u00b7lich", "darf", "die", "Ant\u00b7wort", "un\u00b7ter\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADJD", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Reime flie\u00dfen dir so rein und ungezwungen,", "tokens": ["Die", "Rei\u00b7me", "flie\u00b7\u00dfen", "dir", "so", "rein", "und", "un\u00b7ge\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als sie vor Zeiten kaum der ", "tokens": ["Als", "sie", "vor", "Zei\u00b7ten", "kaum", "der"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADV", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Jedoch ich tadle sie, (du hast es mir erlaubt)", "tokens": ["Je\u00b7doch", "ich", "tad\u00b7le", "sie", ",", "(", "du", "hast", "es", "mir", "er\u00b7laubt", ")"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PPER", "$,", "$(", "PPER", "VAFIN", "PPER", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und habe kaum davon das zehnte Wort geglaubt.", "tokens": ["Und", "ha\u00b7be", "kaum", "da\u00b7von", "das", "zehn\u00b7te", "Wort", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PAV", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was meynst du nun von mir? Mich d\u00fcnkt du wirst mich schelten,", "tokens": ["Was", "meynst", "du", "nun", "von", "mir", "?", "Mich", "d\u00fcnkt", "du", "wirst", "mich", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "PPER", "$.", "PPER", "VVFIN", "PPER", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und l\u00e4ssest mich den Spruch durch deinen Zorn entgelten.", "tokens": ["Und", "l\u00e4s\u00b7sest", "mich", "den", "Spruch", "durch", "dei\u00b7nen", "Zorn", "ent\u00b7gel\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Doch, was kann ich daf\u00fcr? du hast mirs auferlegt:", "tokens": ["Doch", ",", "was", "kann", "ich", "da\u00b7f\u00fcr", "?", "du", "hast", "mirs", "auf\u00b7er\u00b7legt", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWS", "VMFIN", "PPER", "PAV", "$.", "PPER", "VAFIN", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Drum nimm damit vorlieb, wie man zu sagen pflegt;", "tokens": ["Drum", "nimm", "da\u00b7mit", "vor\u00b7lieb", ",", "wie", "man", "zu", "sa\u00b7gen", "pflegt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PAV", "VVFIN", "$,", "PWAV", "PIS", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und zwinge niemand mehr zu Leistung solcher Pflichten,", "tokens": ["Und", "zwin\u00b7ge", "nie\u00b7mand", "mehr", "zu", "Leis\u00b7tung", "sol\u00b7cher", "Pflich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADV", "APPR", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Die so verm\u00f6gend sind, ein Unheil anzurichten.", "tokens": ["Die", "so", "ver\u00b7m\u00f6\u00b7gend", "sind", ",", "ein", "Un\u00b7heil", "an\u00b7zu\u00b7rich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVPP", "VAFIN", "$,", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Allein, ich fahre fort, nachdem ich es gewagt,", "tokens": ["Al\u00b7lein", ",", "ich", "fah\u00b7re", "fort", ",", "nach\u00b7dem", "ich", "es", "ge\u00b7wagt", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VVFIN", "PTKVZ", "$,", "KOUS", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und dir so deutsch und rund die Wahrheit hergesagt:", "tokens": ["Und", "dir", "so", "deutsch", "und", "rund", "die", "Wahr\u00b7heit", "her\u00b7ge\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "ADJD", "KON", "ADJD", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Denn diese ziert allein die Schriften der Poeten,", "tokens": ["Denn", "die\u00b7se", "ziert", "al\u00b7lein", "die", "Schrif\u00b7ten", "der", "Po\u00b7et\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und wo man sie vermi\u00dft, da mu\u00df das Blatt err\u00f6then.", "tokens": ["Und", "wo", "man", "sie", "ver\u00b7mi\u00dft", ",", "da", "mu\u00df", "das", "Blatt", "er\u00b7r\u00f6\u00b7then", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PIS", "PPER", "VVPP", "$,", "ADV", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht so? du giebst mir recht, geschickte Dichterinn!", "tokens": ["Nicht", "so", "?", "du", "giebst", "mir", "recht", ",", "ge\u00b7schick\u00b7te", "Dich\u00b7te\u00b7rinn", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "$.", "PPER", "VVFIN", "PPER", "ADJD", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wie k\u00f6mmts denn, da\u00df nur ich nicht werth gewesen bin,", "tokens": ["Wie", "k\u00f6mmts", "denn", ",", "da\u00df", "nur", "ich", "nicht", "werth", "ge\u00b7we\u00b7sen", "bin", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ADV", "$,", "KOUS", "ADV", "PPER", "PTKNEG", "ADJD", "VAPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Wahrheit, die dir sonst best\u00e4ndig lieb gewesen,", "tokens": ["Die", "Wahr\u00b7heit", ",", "die", "dir", "sonst", "be\u00b7st\u00e4n\u00b7dig", "lieb", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADJD", "ADJD", "VAPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "In deiner letzten Schrift, nach alter Art, zu lesen?", "tokens": ["In", "dei\u00b7ner", "letz\u00b7ten", "Schrift", ",", "nach", "al\u00b7ter", "Art", ",", "zu", "le\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Du lobst mich gar zu sehr, und setzest keinen Reim,", "tokens": ["Du", "lobst", "mich", "gar", "zu", "sehr", ",", "und", "set\u00b7zest", "kei\u00b7nen", "Reim", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKA", "ADV", "$,", "KON", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Den nicht die Schm\u00e4ucheley, durch s\u00fc\u00dfes Honigseim,", "tokens": ["Den", "nicht", "die", "Schm\u00e4u\u00b7che\u00b7ley", ",", "durch", "s\u00fc\u00b7\u00dfes", "Ho\u00b7ni\u00b7gseim", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "ART", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ganz \u00fcberzogen hat. Wie hab ich das verdienet?", "tokens": ["Ganz", "\u00fc\u00b7berz\u00b7o\u00b7gen", "hat", ".", "Wie", "hab", "ich", "das", "ver\u00b7die\u00b7net", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAFIN", "$.", "PWAV", "VAFIN", "PPER", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Hat deine Muse sich nicht gar zu viel erk\u00fchnet;", "tokens": ["Hat", "dei\u00b7ne", "Mu\u00b7se", "sich", "nicht", "gar", "zu", "viel", "er\u00b7k\u00fch\u00b7net", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PRF", "PTKNEG", "ADV", "PTKA", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wenn sie ein Lob besingt, das so ver\u00e4chtlich ist,", "tokens": ["Wenn", "sie", "ein", "Lob", "be\u00b7singt", ",", "das", "so", "ver\u00b7\u00e4cht\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,", "PRELS", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Und wo man \u00fcberall der Wahrheit Spur vermi\u00dft?", "tokens": ["Und", "wo", "man", "\u00fc\u00b7be\u00b7rall", "der", "Wahr\u00b7heit", "Spur", "ver\u00b7mi\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PIS", "ADV", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Doch, da\u00df du wissen magst, was mir im Sinne lieget:", "tokens": ["Doch", ",", "da\u00df", "du", "wis\u00b7sen", "magst", ",", "was", "mir", "im", "Sin\u00b7ne", "lie\u00b7get", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "VVINF", "VMFIN", "$,", "PWS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "So glaube, da\u00df mich nie ein Schm\u00e4uchelwort vergn\u00fcget.", "tokens": ["So", "glau\u00b7be", ",", "da\u00df", "mich", "nie", "ein", "Schm\u00e4u\u00b7chel\u00b7wort", "ver\u00b7gn\u00fc\u00b7get", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Ich weis wohl, was du denkst. Mich d\u00fcnkt, du rufst mir zu.", "tokens": ["Ich", "weis", "wohl", ",", "was", "du", "denkst", ".", "Mich", "d\u00fcnkt", ",", "du", "rufst", "mir", "zu", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "ADV", "$,", "PWS", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Freund! wer lobte wohl bisher so gern als du?", "tokens": ["Mein", "Freund", "!", "wer", "lob\u00b7te", "wohl", "bis\u00b7her", "so", "gern", "als", "du", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "PWS", "VVFIN", "ADV", "ADV", "ADV", "ADV", "KOUS", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ganz recht, ich kenne mich, und will es frey gestehen,", "tokens": ["Ganz", "recht", ",", "ich", "ken\u00b7ne", "mich", ",", "und", "will", "es", "frey", "ge\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PPER", "VVFIN", "PPER", "$,", "KON", "VMFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ich kann mich ebenfalls, wie sonst ein Mensch, vergehen.", "tokens": ["Ich", "kann", "mich", "e\u00b7ben\u00b7falls", ",", "wie", "sonst", "ein", "Mensch", ",", "ver\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "$,", "PWAV", "ADV", "ART", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich habe gern ger\u00fchmt, und stimmte manches Lob", "tokens": ["Ich", "ha\u00b7be", "gern", "ge\u00b7r\u00fchmt", ",", "und", "stimm\u00b7te", "man\u00b7ches", "Lob"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,", "KON", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit gro\u00dfem Jauchzen an, das den und die erhob.", "tokens": ["Mit", "gro\u00b7\u00dfem", "Jauch\u00b7zen", "an", ",", "das", "den", "und", "die", "er\u00b7hob", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PTKVZ", "$,", "PRELS", "ART", "KON", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Zuweilen wu\u00dft ich gar aus den geringsten Sachen,", "tokens": ["Zu\u00b7wei\u00b7len", "wu\u00dft", "ich", "gar", "aus", "den", "ge\u00b7rings\u00b7ten", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Stax und Mops ver\u00fcbt, ein Wunderding zu machen.", "tokens": ["Die", "Stax", "und", "Mops", "ver\u00b7\u00fcbt", ",", "ein", "Wun\u00b7der\u00b7ding", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ich pries was m\u00e4\u00dfiges als unvergleichlich an;", "tokens": ["Ich", "pries", "was", "m\u00e4\u00b7\u00dfi\u00b7ges", "als", "un\u00b7ver\u00b7gleich\u00b7lich", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PWS", "PIS", "KOKOM", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und ob ich gleich dadurch der Wahrheit weh gethan:", "tokens": ["Und", "ob", "ich", "gleich", "da\u00b7durch", "der", "Wahr\u00b7heit", "weh", "ge\u00b7than", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "PAV", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So war die Absicht doch nicht v\u00f6llig zu verwerfen;", "tokens": ["So", "war", "die", "Ab\u00b7sicht", "doch", "nicht", "v\u00f6l\u00b7lig", "zu", "ver\u00b7wer\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "PTKNEG", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ich suchte durch den Ruhm den Tugendtrieb zu sch\u00e4rfen.", "tokens": ["Ich", "such\u00b7te", "durch", "den", "Ruhm", "den", "Tu\u00b7gend\u00b7trieb", "zu", "sch\u00e4r\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Doch, als die Weisheit mir nach diesem vorgestellt,", "tokens": ["Doch", ",", "als", "die", "Weis\u00b7heit", "mir", "nach", "die\u00b7sem", "vor\u00b7ge\u00b7stellt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "ART", "NN", "PPER", "APPR", "PDAT", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Man f\u00e4nde nichts so sch\u00f6n und trefflich in der Welt,", "tokens": ["Man", "f\u00e4n\u00b7de", "nichts", "so", "sch\u00f6n", "und", "treff\u00b7lich", "in", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "ADV", "ADJD", "KON", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das ohne Tadel w\u00e4r, und keine Flecken h\u00e4tte:", "tokens": ["Das", "oh\u00b7ne", "Ta\u00b7del", "w\u00e4r", ",", "und", "kei\u00b7ne", "Fle\u00b7cken", "h\u00e4t\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "VAFIN", "$,", "KON", "PIAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "So seufzt ich oftermals mit jenem um die Wette,", "tokens": ["So", "seufzt", "ich", "of\u00b7ter\u00b7mals", "mit", "je\u00b7nem", "um", "die", "Wet\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der stets mit Traurigkeit der Menschen Schwachheit sah;", "tokens": ["Der", "stets", "mit", "Trau\u00b7rig\u00b7keit", "der", "Men\u00b7schen", "Schwach\u00b7heit", "sah", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NN", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn, nach des P\u00f6bels Wahn, die gr\u00f6\u00dfte That geschah.", "tokens": ["Wenn", ",", "nach", "des", "P\u00f6\u00b7bels", "Wahn", ",", "die", "gr\u00f6\u00df\u00b7te", "That", "ge\u00b7schah", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "APPR", "ART", "NN", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So ist denn, war mein Wort, kein Sterblicher zu loben?", "tokens": ["So", "ist", "denn", ",", "war", "mein", "Wort", ",", "kein", "Sterb\u00b7li\u00b7cher", "zu", "lo\u00b7ben", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "$,", "VAFIN", "PPOSAT", "NN", "$,", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So wird denn all ihr Thun ganz unverdient erhoben?", "tokens": ["So", "wird", "denn", "all", "ihr", "Thun", "ganz", "un\u00b7ver\u00b7di\u00b7ent", "er\u00b7ho\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "PPOSAT", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "So ist denn keine That in allen St\u00fccken rein?", "tokens": ["So", "ist", "denn", "kei\u00b7ne", "That", "in", "al\u00b7len", "St\u00fc\u00b7cken", "rein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Ja, ja! wer loben will, der mu\u00df ein L\u00fcgner seyn.", "tokens": ["Ja", ",", "ja", "!", "wer", "lo\u00b7ben", "will", ",", "der", "mu\u00df", "ein", "L\u00fcg\u00b7ner", "seyn", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADV", "$.", "PWS", "VVINF", "VMFIN", "$,", "ART", "VMFIN", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ich fieng satirisch an, die Thorheit zu besch\u00e4men:", "tokens": ["Ich", "fi\u00b7eng", "sa\u00b7ti\u00b7risch", "an", ",", "die", "Thor\u00b7heit", "zu", "be\u00b7sch\u00e4\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Ich deckte manches auf, das sch\u00f6n von au\u00dfen war,", "tokens": ["Ich", "deck\u00b7te", "man\u00b7ches", "auf", ",", "das", "sch\u00f6n", "von", "au\u00b7\u00dfen", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$,", "PRELS", "ADJD", "APPR", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und stellte manch Gesicht in seiner Bl\u00f6\u00dfe dar:", "tokens": ["Und", "stell\u00b7te", "manch", "Ge\u00b7sicht", "in", "sei\u00b7ner", "Bl\u00f6\u00b7\u00dfe", "dar", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "In Hoffnung, dergestalt, durch die entlarvten Flecken,", "tokens": ["In", "Hoff\u00b7nung", ",", "der\u00b7ge\u00b7stalt", ",", "durch", "die", "ent\u00b7larv\u00b7ten", "Fle\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADV", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die glei\u00dfnerische Welt vom B\u00f6sen abzuschrecken.", "tokens": ["Die", "glei\u00df\u00b7ne\u00b7ri\u00b7sche", "Welt", "vom", "B\u00f6\u00b7sen", "ab\u00b7zu\u00b7schre\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Allein, es war umsonst, die Lauge bi\u00df zu scharf.", "tokens": ["Al\u00b7lein", ",", "es", "war", "um\u00b7sonst", ",", "die", "Lau\u00b7ge", "bi\u00df", "zu", "scharf", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VAFIN", "ADV", "$,", "ART", "NN", "APPR", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es hie\u00df: Wie geht es zu, da\u00df der so l\u00e4stern darf?", "tokens": ["Es", "hie\u00df", ":", "Wie", "geht", "es", "zu", ",", "da\u00df", "der", "so", "l\u00e4s\u00b7tern", "darf", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWAV", "VVFIN", "PPER", "PTKVZ", "$,", "KOUS", "ART", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Wer hat ihm immermehr das Strafamt aufgetragen?", "tokens": ["Wer", "hat", "ihm", "im\u00b7mer\u00b7mehr", "das", "Stra\u00b7famt", "auf\u00b7ge\u00b7tra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und wer wird endlich was nach seinen Lehren fragen?", "tokens": ["Und", "wer", "wird", "end\u00b7lich", "was", "nach", "sei\u00b7nen", "Leh\u00b7ren", "fra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VAFIN", "ADV", "PWS", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So war mein Hoffen aus. Ich merkte, da\u00df die Welt", "tokens": ["So", "war", "mein", "Hof\u00b7fen", "aus", ".", "Ich", "merk\u00b7te", ",", "da\u00df", "die", "Welt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "In allem, was sie thut, nicht viel auf Regeln h\u00e4lt;", "tokens": ["In", "al\u00b7lem", ",", "was", "sie", "thut", ",", "nicht", "viel", "auf", "Re\u00b7geln", "h\u00e4lt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "PPER", "VVFIN", "$,", "PTKNEG", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und lieber blindlings tappt, als recht im Lichte gehet,", "tokens": ["Und", "lie\u00b7ber", "blind\u00b7lings", "tappt", ",", "als", "recht", "im", "Lich\u00b7te", "ge\u00b7het", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "$,", "KOUS", "ADJD", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So nah ihr sichrer Fu\u00df an tausend Fallen stehet.", "tokens": ["So", "nah", "ihr", "sich\u00b7rer", "Fu\u00df", "an", "tau\u00b7send", "Fal\u00b7len", "ste\u00b7het", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "ADJA", "NN", "APPR", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Drum hab ich nach und nach ", "tokens": ["Drum", "hab", "ich", "nach", "und", "nach"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "APPR", "KON", "APPR"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Doch weil mich ", "tokens": ["Doch", "weil", "mich"], "token_info": ["word", "word", "word"], "pos": ["KON", "KOUS", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Zur Dichtkunst angespornt; so hat mir unter allen,", "tokens": ["Zur", "Dicht\u00b7kunst", "an\u00b7ge\u00b7spornt", ";", "so", "hat", "mir", "un\u00b7ter", "al\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$.", "ADV", "VAFIN", "PPER", "APPR", "PIS", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Die seine Schwestern sind, ", "tokens": ["Die", "sei\u00b7ne", "Schwes\u00b7tern", "sind", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Die hat mich, seit der Zeit, mit vielem Ernst gelehrt,", "tokens": ["Die", "hat", "mich", ",", "seit", "der", "Zeit", ",", "mit", "vie\u00b7lem", "Ernst", "ge\u00b7lehrt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "$,", "APPR", "ART", "NN", "$,", "APPR", "PIS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie man der Tugend Werth in klugen Fabeln ehrt;", "tokens": ["Wie", "man", "der", "Tu\u00b7gend", "Werth", "in", "klu\u00b7gen", "Fa\u00b7beln", "ehrt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der Alterth\u00fcmer Nacht in hellen Tag verwandelt,", "tokens": ["Der", "Al\u00b7tert\u00b7h\u00fc\u00b7mer", "Nacht", "in", "hel\u00b7len", "Tag", "ver\u00b7wan\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und in dem Trauerspiel von Sittenlehren handelt;", "tokens": ["Und", "in", "dem", "Trau\u00b7er\u00b7spiel", "von", "Sit\u00b7ten\u00b7leh\u00b7ren", "han\u00b7delt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wie man der Weisheit Macht, der Gro\u00dfmuth St\u00e4rke zeigt,", "tokens": ["Wie", "man", "der", "Weis\u00b7heit", "Macht", ",", "der", "Gro\u00df\u00b7muth", "St\u00e4r\u00b7ke", "zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "NN", "$,", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn ein gesetztes Herz kein harter Zufall beugt;", "tokens": ["Wenn", "ein", "ge\u00b7setz\u00b7tes", "Herz", "kein", "har\u00b7ter", "Zu\u00b7fall", "beugt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "PIAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wie hoch die Bosheit w\u00e4chst, wenn ihr die Frevelthaten", "tokens": ["Wie", "hoch", "die", "Bos\u00b7heit", "w\u00e4chst", ",", "wenn", "ihr", "die", "Fre\u00b7vel\u00b7tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Erst wohl von statten gehn, und recht nach Wunsch gerathen;", "tokens": ["Erst", "wohl", "von", "stat\u00b7ten", "gehn", ",", "und", "recht", "nach", "Wunsch", "ge\u00b7ra\u00b7then", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ADJA", "VVINF", "$,", "KON", "ADJD", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und wie des Himmels Zorn mit sich nicht scherzen l\u00e4\u00dft,", "tokens": ["Und", "wie", "des", "Him\u00b7mels", "Zorn", "mit", "sich", "nicht", "scher\u00b7zen", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "NN", "APPR", "PRF", "PTKNEG", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wenn sein gereizter Arm zuletzt der Laster Pest,", "tokens": ["Wenn", "sein", "ge\u00b7reiz\u00b7ter", "Arm", "zu\u00b7letzt", "der", "Las\u00b7ter", "Pest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Mit siegender Gewalt im gr\u00f6\u00dften W\u00fcthen wehret,", "tokens": ["Mit", "sie\u00b7gen\u00b7der", "Ge\u00b7walt", "im", "gr\u00f6\u00df\u00b7ten", "W\u00fct\u00b7hen", "weh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Und ihre Raserey auf ihre Scheiteln kehret.", "tokens": ["Und", "ih\u00b7re", "Ra\u00b7se\u00b7rey", "auf", "ih\u00b7re", "Schei\u00b7teln", "keh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die\u00df ist die sichre Bahn, darauf mein Geist sich \u00fcbt,", "tokens": ["Die\u00df", "ist", "die", "sich\u00b7re", "Bahn", ",", "da\u00b7rauf", "mein", "Geist", "sich", "\u00fcbt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "PAV", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wo er nicht schm\u00e4ucheln darf, auch nicht das Tadeln liebt;", "tokens": ["Wo", "er", "nicht", "schm\u00e4u\u00b7cheln", "darf", ",", "auch", "nicht", "das", "Ta\u00b7deln", "liebt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,", "ADV", "PTKNEG", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Nur blo\u00df die Tugend lobt, nur blo\u00df das Laster sch\u00e4ndet,", "tokens": ["Nur", "blo\u00df", "die", "Tu\u00b7gend", "lobt", ",", "nur", "blo\u00df", "das", "Las\u00b7ter", "sch\u00e4n\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVFIN", "$,", "ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Weil weder Eigennutz, noch Furcht sein Auge blendet.", "tokens": ["Weil", "we\u00b7der", "Ei\u00b7gen\u00b7nutz", ",", "noch", "Furcht", "sein", "Au\u00b7ge", "blen\u00b7det", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KON", "NN", "$,", "ADV", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Doch, wo gerath ich hin? Du kannst hieraus ersehn,", "tokens": ["Doch", ",", "wo", "ge\u00b7rath", "ich", "hin", "?", "Du", "kannst", "hier\u00b7aus", "er\u00b7sehn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWAV", "VVFIN", "PPER", "PTKVZ", "$.", "PPER", "VMFIN", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df deiner Poesie kein Unrecht ist geschehn,", "tokens": ["Da\u00df", "dei\u00b7ner", "Poe\u00b7sie", "kein", "Un\u00b7recht", "ist", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PIAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Belobte Dichterinn; wenn ich dir vorger\u00fccket,", "tokens": ["Be\u00b7lob\u00b7te", "Dich\u00b7te\u00b7rinn", ";", "wenn", "ich", "dir", "vor\u00b7ge\u00b7r\u00fc\u00b7cket", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df sie mein kleines Lob mit Reimen ausgeschm\u00fccket.", "tokens": ["Da\u00df", "sie", "mein", "klei\u00b7nes", "Lob", "mit", "Rei\u00b7men", "aus\u00b7ge\u00b7schm\u00fc\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "ADJA", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Mehr hab ich nicht bemerkt, da\u00df auszusetzen sey.", "tokens": ["Mehr", "hab", "ich", "nicht", "be\u00b7merkt", ",", "da\u00df", "aus\u00b7zu\u00b7set\u00b7zen", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "VVPP", "$,", "KOUS", "VVIZU", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und l\u00e4\u00dft sich f\u00fcr den Gru\u00df, durch meinen Kiel bedanken;", "tokens": ["Und", "l\u00e4\u00dft", "sich", "f\u00fcr", "den", "Gru\u00df", ",", "durch", "mei\u00b7nen", "Kiel", "be\u00b7dan\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "ART", "NN", "$,", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch z\u00e4hlt der theure Mann sich l\u00e4ngstens zu den Kranken.", "tokens": ["Doch", "z\u00e4hlt", "der", "theu\u00b7re", "Mann", "sich", "l\u00e4ngs\u00b7tens", "zu", "den", "Kran\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "PRF", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die Augen werden stumpf, es schwindet alle Kraft.", "tokens": ["Die", "Au\u00b7gen", "wer\u00b7den", "stumpf", ",", "es", "schwin\u00b7det", "al\u00b7le", "Kraft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "PPER", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und so f\u00e4llt nach und nach des Geistes Eigenschaft,", "tokens": ["Und", "so", "f\u00e4llt", "nach", "und", "nach", "des", "Geis\u00b7tes", "Ei\u00b7gen\u00b7schaft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "APPR", "KON", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Davon die halbe Welt bisher die Frucht gelesen:", "tokens": ["Da\u00b7von", "die", "hal\u00b7be", "Welt", "bis\u00b7her", "die", "Frucht", "ge\u00b7le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und kurz, er ist kaum halb, was er bisher gewesen.", "tokens": ["Und", "kurz", ",", "er", "ist", "kaum", "halb", ",", "was", "er", "bis\u00b7her", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$,", "PWS", "PPER", "ADV", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Dein ", "tokens": ["Dein"], "token_info": ["word"], "pos": ["PPOSAT"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "Verrichte die\u00df von mir an deinen werthen Mann,", "tokens": ["Ver\u00b7rich\u00b7te", "die\u00df", "von", "mir", "an", "dei\u00b7nen", "wert\u00b7hen", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "APPR", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und was dir angeh\u00f6rt. Viel Gl\u00fcck zum neuen Jahre!", "tokens": ["Und", "was", "dir", "an\u00b7ge\u00b7h\u00f6rt", ".", "Viel", "Gl\u00fcck", "zum", "neu\u00b7en", "Jah\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVPP", "$.", "PIAT", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Gott gebe, da\u00df man stets viel Guts von dir erfahre.", "tokens": ["Gott", "ge\u00b7be", ",", "da\u00df", "man", "stets", "viel", "Guts", "von", "dir", "er\u00b7fah\u00b7re", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "KOUS", "PIS", "ADV", "PIAT", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}}}}