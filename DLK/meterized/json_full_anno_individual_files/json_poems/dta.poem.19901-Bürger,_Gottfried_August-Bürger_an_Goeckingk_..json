{"dta.poem.19901": {"metadata": {"author": {"name": "B\u00fcrger, Gottfried August", "birth": "N.A.", "death": "N.A."}, "title": "B\u00fcrger an Goeckingk .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1778", "urn": "urn:nbn:de:kobv:b4-20090519672", "language": ["de:0.99"], "booktitle": "B\u00fcrger, Gottfried August: Gedichte. G\u00f6ttingen, 1778."}, "poem": {"stanza.1": {"line.1": {"text": "Nun! Nun! Versch\u00fctt' Er nur nicht gar               ", "tokens": ["Nun", "!", "Nun", "!", "Ver\u00b7sch\u00fctt'", "Er", "nur", "nicht", "gar"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$.", "ADV", "$.", "VVFIN", "PPER", "ADV", "PTKNEG", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Kindlein, samt dem Bade.", "tokens": ["Das", "Kin\u00b7dlein", ",", "samt", "dem", "Ba\u00b7de", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das arme Kindlein das! F\u00fcrwahr!", "tokens": ["Das", "ar\u00b7me", "Kin\u00b7dlein", "das", "!", "F\u00fcr\u00b7wahr", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "$.", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es w\u00e4r\u2019 ja Jammerschade.", "tokens": ["Es", "w\u00e4r'", "ja", "Jam\u00b7mer\u00b7scha\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Denn, sieht Er! troz der Plackerei,", "tokens": ["Denn", ",", "sieht", "Er", "!", "troz", "der", "Pla\u00b7cke\u00b7rei", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$,", "VVFIN", "PPER", "$.", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Beim Zeugen und Geb\u00e4ren,", "tokens": ["Beim", "Zeu\u00b7gen", "und", "Ge\u00b7b\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Mag doch die edle Reimerei", "tokens": ["Mag", "doch", "die", "ed\u00b7le", "Rei\u00b7me\u00b7rei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auch viel Profit bescheeren.", "tokens": ["Auch", "viel", "Pro\u00b7fit", "be\u00b7schee\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Troz Sing und Sang von Cypripor,", "tokens": ["Troz", "Sing", "und", "Sang", "von", "Cyp\u00b7ri\u00b7por", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Apoll, Achill und Hektor,", "tokens": ["A\u00b7poll", ",", "A\u00b7chill", "und", "Hek\u00b7tor", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "KON", "NE", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Bleibt man zwar Amtman, nach wie vor,", "tokens": ["Bleibt", "man", "zwar", "Amt\u00b7man", ",", "nach", "wie", "vor", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "NN", "$,", "APPR", "KOKOM", "APPR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auch \u2014 Herr Kanzlei-Direktor.", "tokens": ["Auch", "Herr", "Kanz\u00b7lei\u00b7Di\u00b7rek\u00b7tor", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADV", "$(", "NN", "NE", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Denn leichter wird Vokation,", "tokens": ["Denn", "leich\u00b7ter", "wird", "Vo\u00b7ka\u00b7ti\u00b7on", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu Pension und Pfr\u00fcnden,", "tokens": ["Zu", "Pen\u00b7si\u00b7on", "und", "Pfr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die kahlste Dissertazion,", "tokens": ["Die", "kahls\u00b7te", "Dis\u00b7ser\u00b7ta\u00b7zi\u00b7on", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als mein Homerus finden.", "tokens": ["Als", "mein", "Ho\u00b7me\u00b7rus", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NE", "VVINF", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.5": {"line.1": {"text": "Auch m\u00e4stet man sich eben nicht", "tokens": ["Auch", "m\u00e4s\u00b7tet", "man", "sich", "e\u00b7ben", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "ADV", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von M\u00e4zenatengnade;", "tokens": ["Von", "M\u00e4\u00b7ze\u00b7na\u00b7teng\u00b7na\u00b7de", ";"], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Tr\u00e4gt ", "tokens": ["Tr\u00e4gt"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Und Schlapperbauch und Wade.", "tokens": ["Und", "Schlap\u00b7per\u00b7bauch", "und", "Wa\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Die Herrn vom Ministerio", "tokens": ["Die", "Herrn", "vom", "Mi\u00b7nis\u00b7te\u00b7rio"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und aus dem edlen Rathe", "tokens": ["Und", "aus", "dem", "ed\u00b7len", "Ra\u00b7the"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Floriren mehr in Jubilo,", "tokens": ["Flo\u00b7ri\u00b7ren", "mehr", "in", "Ju\u00b7bi\u00b7lo", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und prunken bas im Staate.", "tokens": ["Und", "prun\u00b7ken", "bas", "im", "Staa\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Doch neid\u2019 ich nicht das Bonzenheer", "tokens": ["Doch", "neid'", "ich", "nicht", "das", "Bon\u00b7zen\u00b7heer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Um seine dicken K\u00f6pfe;", "tokens": ["Um", "sei\u00b7ne", "di\u00b7cken", "K\u00f6p\u00b7fe", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Denn drin sind viele ja so leer,", "tokens": ["Denn", "drin", "sind", "vie\u00b7le", "ja", "so", "leer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PIS", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie hohle Kirchthurmkn\u00f6pfe.", "tokens": ["Wie", "hoh\u00b7le", "Kirch\u00b7thurm\u00b7kn\u00f6p\u00b7fe", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Nun Spas apart! Und h\u00f6r\u2019 Er an,", "tokens": ["Nun", "Spas", "a\u00b7part", "!", "Und", "h\u00f6r'", "Er", "an", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NE", "$.", "KON", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Fals ihm mein Ernst beliebig.", "tokens": ["Fals", "ihm", "mein", "Ernst", "be\u00b7lie\u00b7big", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ist denn nicht auch f\u00fcr ihren Man", "tokens": ["Ist", "denn", "nicht", "auch", "f\u00fcr", "ih\u00b7ren", "Man"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Poeterei ergiebig?", "tokens": ["Poe\u00b7te\u00b7rei", "er\u00b7gie\u00b7big", "?"], "token_info": ["word", "word", "punct"], "pos": ["NN", "ADJD", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Bedenk\u2019 Er \u2019mal! Wie sch\u00f6n das ist!", "tokens": ["Be\u00b7denk'", "Er", "'mal", "!", "Wie", "sch\u00f6n", "das", "ist", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "$.", "PWAV", "ADJD", "PDS", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verleger, wolgezogen,", "tokens": ["Ver\u00b7le\u00b7ger", ",", "wol\u00b7ge\u00b7zo\u00b7gen", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "PWAV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bezalen oft, zu dieser Frist,", "tokens": ["Be\u00b7za\u00b7len", "oft", ",", "zu", "die\u00b7ser", "Frist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit Louisd\u2019or den Bogen.", "tokens": ["Mit", "Louis\u00b7d'\u00b7or", "den", "Bo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "W\u00e4chst nun im zehnten sauren Jahr", "tokens": ["W\u00e4chst", "nun", "im", "zehn\u00b7ten", "sau\u00b7ren", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPRART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zehn Bogen stark sein B\u00e4ndchen,", "tokens": ["Zehn", "Bo\u00b7gen", "stark", "sein", "B\u00e4nd\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So schnapt Er ja an Trankgeld baar", "tokens": ["So", "schnapt", "Er", "ja", "an", "Trank\u00b7geld", "baar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zehn Blinde, ohne R\u00e4ndchen.", "tokens": ["Zehn", "Blin\u00b7de", ",", "oh\u00b7ne", "R\u00e4nd\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "KOUI", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Und das ist doch kein Kazendrek,", "tokens": ["Und", "das", "ist", "doch", "kein", "Ka\u00b7zen\u00b7drek", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wof\u00fcr man sich kasteiet.", "tokens": ["Wo\u00b7f\u00fcr", "man", "sich", "kas\u00b7tei\u00b7et", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es k\u00f6mt ja kein gebratner Spek", "tokens": ["Es", "k\u00f6mt", "ja", "kein", "ge\u00b7brat\u00b7ner", "Spek"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Umsonst ins Maul geschneiet.", "tokens": ["Um\u00b7sonst", "ins", "Maul", "ge\u00b7schnei\u00b7et", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Herr Ugolino ", "tokens": ["Herr", "U\u00b7go\u00b7li\u00b7no"], "token_info": ["word", "word"], "pos": ["NN", "NE"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Nebst Weib und Kind und G\u00e4sten,", "tokens": ["Nebst", "Weib", "und", "Kind", "und", "G\u00e4s\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Nach altem hergebrachten Brauch,", "tokens": ["Nach", "al\u00b7tem", "her\u00b7ge\u00b7brach\u00b7ten", "Brauch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von unserm Hirn sich m\u00e4sten.", "tokens": ["Von", "un\u00b7serm", "Hirn", "sich", "m\u00e4s\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Steht der gelahrte Fakultist", "tokens": ["Steht", "der", "ge\u00b7lahr\u00b7te", "Fa\u00b7kul\u00b7tist"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dagegen doch viel kahler.", "tokens": ["Da\u00b7ge\u00b7gen", "doch", "viel", "kah\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Dem sezt es kaum, wenn\u2019s k\u00f6stlich ist,", "tokens": ["Dem", "sezt", "es", "kaum", ",", "wenn's", "k\u00f6st\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zwei Gulden oder Thaler.", "tokens": ["Zwei", "Gul\u00b7den", "o\u00b7der", "Tha\u00b7ler", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Drob \u00e4rgern sich nun freilich bas", "tokens": ["Drob", "\u00e4r\u00b7gern", "sich", "nun", "frei\u00b7lich", "bas"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ADV", "ADV", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Herren Fakultisten,", "tokens": ["Die", "Her\u00b7ren", "Fa\u00b7kul\u00b7tis\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und sticheln Ihm ohn\u2019 Unterlas", "tokens": ["Und", "sti\u00b7cheln", "Ihm", "ohn'", "Un\u00b7ter\u00b7las"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Brao auf die Belletristen.", "tokens": ["Brao", "auf", "die", "Bel\u00b7le\u00b7tris\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Manch Herr Professor kriegte schon", "tokens": ["Manch", "Herr", "Pro\u00b7fes\u00b7sor", "krieg\u00b7te", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vor Kummer graue Haare:", "tokens": ["Vor", "Kum\u00b7mer", "grau\u00b7e", "Haa\u00b7re", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df mehr jezt gilt der Agathon,", "tokens": ["Da\u00df", "mehr", "jezt", "gilt", "der", "A\u00b7gat\u00b7hon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Fakult\u00e4tenwaare. \u2014", "tokens": ["Als", "Fa\u00b7kul\u00b7t\u00e4\u00b7ten\u00b7waa\u00b7re", "."], "token_info": ["word", "word", "punct", "punct"], "pos": ["KOUS", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Der Ruhm hat freilich grosse Last,", "tokens": ["Der", "Ruhm", "hat", "frei\u00b7lich", "gros\u00b7se", "Last", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In diesem Jammerleben,", "tokens": ["In", "die\u00b7sem", "Jam\u00b7mer\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wie du davon, zum sprechen, hast", "tokens": ["Wie", "du", "da\u00b7von", ",", "zum", "spre\u00b7chen", ",", "hast"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["PWAV", "PPER", "PAV", "$,", "APPRART", "VVINF", "$,", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Konterfei gegeben.", "tokens": ["Ein", "Kon\u00b7ter\u00b7fei", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Doch nach dem Tode geht\u2019s erst an.", "tokens": ["Doch", "nach", "dem", "To\u00b7de", "geht's", "erst", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn auch bei den Tongusen,", "tokens": ["Denn", "auch", "bei", "den", "Ton\u00b7gu\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "$,"], "meter": "--+-+--", "measure": "anapaest.init"}, "line.3": {"text": "Nach tausend Jahren, ehret man,", "tokens": ["Nach", "tau\u00b7send", "Jah\u00b7ren", ",", "eh\u00b7ret", "man", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$,", "VVFIN", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So Gott wil! unsre Musen.", "tokens": ["So", "Gott", "wil", "!", "uns\u00b7re", "Mu\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "NN", "VMFIN", "$.", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Dort illustrirt man fein aus uns", "tokens": ["Dort", "il\u00b7lust\u00b7rirt", "man", "fein", "aus", "uns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADJD", "APPR", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Antiquit\u00e4tenlisten.", "tokens": ["An\u00b7ti\u00b7qui\u00b7t\u00e4\u00b7ten\u00b7lis\u00b7ten", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+----+-", "measure": "dactylic.init"}, "line.3": {"text": "Uns liest manch hochber\u00fcmter Duns", "tokens": ["Uns", "liest", "manch", "hoch\u00b7be\u00b7r\u00fcm\u00b7ter", "Duns"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Gelahrter Humanisten;", "tokens": ["Ge\u00b7lahr\u00b7ter", "Hu\u00b7ma\u00b7nis\u00b7ten", ";"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Die jezt aus ihrem B\u00fccherschrein", "tokens": ["Die", "jezt", "aus", "ih\u00b7rem", "B\u00fc\u00b7cher\u00b7schrein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ver\u00e4chtlich uns verschieben,", "tokens": ["Ver\u00b7\u00e4cht\u00b7lich", "uns", "ver\u00b7schie\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Weil wir nicht Griechisch und Latein", "tokens": ["Weil", "wir", "nicht", "Grie\u00b7chisch", "und", "La\u00b7tein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "ADJD", "KON", "NN"], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Und nicht Arabisch schrieben.", "tokens": ["Und", "nicht", "A\u00b7ra\u00b7bisch", "schrie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "NN", "VVINF", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.20": {"line.1": {"text": "Dort preist man unsre Opera", "tokens": ["Dort", "preist", "man", "uns\u00b7re", "O\u00b7pe\u00b7ra"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PPOSAT", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Durch Commentationen,", "tokens": ["Durch", "Com\u00b7men\u00b7ta\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Inauguralprogrammata,", "tokens": ["In\u00b7au\u00b7gur\u00b7al\u00b7pro\u00b7gram\u00b7ma\u00b7ta", ","], "token_info": ["word", "punct"], "pos": ["NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und Dissertazionen.", "tokens": ["Und", "Dis\u00b7ser\u00b7ta\u00b7zi\u00b7o\u00b7nen", "."], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Schon h\u00f6r\u2019 ich Kridlermordgeschrei,", "tokens": ["Schon", "h\u00f6r'", "ich", "Krid\u00b7ler\u00b7mord\u00b7ge\u00b7schrei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In meinem stillen Grabe:", "tokens": ["In", "mei\u00b7nem", "stil\u00b7len", "Gra\u00b7be", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wer die Lenore doch wol sey?", "tokens": ["Wer", "die", "Le\u00b7no\u00b7re", "doch", "wol", "sey", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "ADV", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ob sie gelebet habe?", "tokens": ["Ob", "sie", "ge\u00b7le\u00b7bet", "ha\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "Man bringt, bald ", "tokens": ["Man", "bringt", ",", "bald"], "token_info": ["word", "word", "punct", "word"], "pos": ["PIS", "VVFIN", "$,", "ADV"], "meter": "++-", "measure": "unknown.measure.di"}, "line.2": {"text": "Uns winzigklein ", "tokens": ["Uns", "win\u00b7zig\u00b7klein"], "token_info": ["word", "word"], "pos": ["PPER", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Bald, commentirt ", "tokens": ["Bald", ",", "com\u00b7men\u00b7tirt"], "token_info": ["word", "punct", "word"], "pos": ["ADV", "$,", "NE"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.23": {"line.1": {"text": "Wie sch\u00f6n! Wenn Knaben jung und alt,", "tokens": ["Wie", "sch\u00f6n", "!", "Wenn", "Kna\u00b7ben", "jung", "und", "alt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "$.", "KOUS", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In jenen goldnen Tagen,", "tokens": ["In", "je\u00b7nen", "gold\u00b7nen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zur Schul\u2019, in Riemen eingeschnalt,", "tokens": ["Zur", "Schul'", ",", "in", "Rie\u00b7men", "ein\u00b7ge\u00b7schnalt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mich alten Knaster tragen!", "tokens": ["Mich", "al\u00b7ten", "Knas\u00b7ter", "tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.24": {"line.1": {"text": "Aus mir Vokabeln wolgemut", "tokens": ["Aus", "mir", "Vo\u00b7ka\u00b7beln", "wol\u00b7ge\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPER", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und Phrases memoriren,", "tokens": ["Und", "Phra\u00b7ses", "me\u00b7mo\u00b7ri\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "$,"], "meter": "-+---+-", "measure": "dactylic.init"}, "line.3": {"text": "Um mich so recht in Saft und Blut,", "tokens": ["Um", "mich", "so", "recht", "in", "Saft", "und", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "ADV", "ADJD", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vt ajunt, zu vertiren.             ", "tokens": ["Vt", "a\u00b7junt", ",", "zu", "ver\u00b7ti\u00b7ren", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "$,", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Und geht\u2019s nicht mit der Lection,", "tokens": ["Und", "geht's", "nicht", "mit", "der", "Lec\u00b7ti\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und mit dem Exponiren,", "tokens": ["Und", "mit", "dem", "Ex\u00b7po\u00b7ni\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Dann wird\u2019s gar schlecht im Hause stohn. \u2014", "tokens": ["Dann", "wird's", "gar", "schlecht", "im", "Hau\u00b7se", "stohn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADJD", "APPRART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Junker mus kariren. \u2014", "tokens": ["Der", "Jun\u00b7ker", "mus", "ka\u00b7ri\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.26": {"line.1": {"text": "Sieh! was die Reimerei bescheert,", "tokens": ["Sieh", "!", "was", "die", "Rei\u00b7me\u00b7rei", "be\u00b7scheert", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PWS", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die du vermaledeiet!", "tokens": ["Die", "du", "ver\u00b7ma\u00b7le\u00b7de\u00b7i\u00b7et", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "$."], "meter": "+--++-+-", "measure": "dactylic.init"}, "line.3": {"text": "Das ist doch wol der Federn wehrt,", "tokens": ["Das", "ist", "doch", "wol", "der", "Fe\u00b7dern", "wehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die man darum zerk\u00e4uet? \u2014", "tokens": ["Die", "man", "da\u00b7rum", "zer\u00b7k\u00e4u\u00b7et", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PIS", "PAV", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Eins nur verg\u00e4lt mir noch den Ruhm,", "tokens": ["Eins", "nur", "ver\u00b7g\u00e4lt", "mir", "noch", "den", "Ruhm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den ich mir fantasiret:", "tokens": ["Den", "ich", "mir", "fan\u00b7ta\u00b7si\u00b7ret", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn man nur, wie Horatium,", "tokens": ["Wenn", "man", "nur", ",", "wie", "Ho\u00b7ra\u00b7ti\u00b7um", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "$,", "PWAV", "NE", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Mich nicht kombabisiret. \u2014", "tokens": ["Mich", "nicht", "kom\u00b7ba\u00b7bi\u00b7si\u00b7ret", "."], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPER", "PTKNEG", "VVFIN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}