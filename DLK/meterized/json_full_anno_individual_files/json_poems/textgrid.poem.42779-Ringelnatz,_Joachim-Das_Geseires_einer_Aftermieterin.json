{"textgrid.poem.42779": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Das Geseires einer Aftermieterin", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Meine Stellung hatte ich verloren.", "tokens": ["Mei\u00b7ne", "Stel\u00b7lung", "hat\u00b7te", "ich", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weil ich meinem Chef zu h\u00e4\u00dflich bin.", "tokens": ["Weil", "ich", "mei\u00b7nem", "Chef", "zu", "h\u00e4\u00df\u00b7lich", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "PTKA", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und nun habe ich ein M\u00e4dchen geboren,", "tokens": ["Und", "nun", "ha\u00b7be", "ich", "ein", "M\u00e4d\u00b7chen", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wo keinen Vater hat, und kein Kinn.", "tokens": ["Wo", "kei\u00b7nen", "Va\u00b7ter", "hat", ",", "und", "kein", "Kinn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "$,", "KON", "PIAT", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.2": {"line.1": {"text": "Als mein Vormund sich erh\u00e4ngte,", "tokens": ["Als", "mein", "Vor\u00b7mund", "sich", "er\u00b7h\u00e4ng\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Besa\u00df ich noch das Kreppdischingewand,", "tokens": ["Be\u00b7sa\u00df", "ich", "noch", "das", "Kre\u00b7pp\u00b7di\u00b7schin\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Was ich sp\u00e4ter der Anni schenkte.", "tokens": ["Was", "ich", "sp\u00e4\u00b7ter", "der", "An\u00b7ni", "schenk\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "ART", "NE", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Die war Masseuse in Helgoland.", "tokens": ["Die", "war", "Mas\u00b7seu\u00b7se", "in", "Hel\u00b7go\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Aber der bin ich nun b\u00f6se.", "tokens": ["A\u00b7ber", "der", "bin", "ich", "nun", "b\u00f6\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn die lie\u00df mich im Stich.", "tokens": ["Denn", "die", "lie\u00df", "mich", "im", "Stich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und die ist gar keine Masseuse,", "tokens": ["Und", "die", "ist", "gar", "kei\u00b7ne", "Mas\u00b7seu\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sondern geht auf den \u2013", "tokens": ["Son\u00b7dern", "geht", "auf", "den", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "Mir ist nichts nachzusagen.", "tokens": ["Mir", "ist", "nichts", "nach\u00b7zu\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVIZU", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ich habe mit einem Zahnarzt verkehrt.", "tokens": ["Ich", "ha\u00b7be", "mit", "ei\u00b7nem", "Zahn\u00b7arzt", "ver\u00b7kehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der hat mich auf H\u00e4nden getragen.", "tokens": ["Der", "hat", "mich", "auf", "H\u00e4n\u00b7den", "ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Doch ich habe mir selber mein Gl\u00fcck zerst\u00f6rt.", "tokens": ["Doch", "ich", "ha\u00b7be", "mir", "sel\u00b7ber", "mein", "Gl\u00fcck", "zer\u00b7st\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.5": {"line.1": {"text": "Das war im Englischen Garten.", "tokens": ["Das", "war", "im", "Eng\u00b7li\u00b7schen", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da gab mir's der Teufel ein,", "tokens": ["Da", "gab", "mir's", "der", "Teu\u00b7fel", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df ich \u2013 um auf Gustav zu warten \u2013", "tokens": ["Da\u00df", "ich", "\u2013", "um", "auf", "Gus\u00b7tav", "zu", "war\u00b7ten", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$(", "KOUI", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In der Nase bohrte, ich Schwein.", "tokens": ["In", "der", "Na\u00b7se", "bohr\u00b7te", ",", "ich", "Schwein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$,", "PPER", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.6": {"line.1": {"text": "Gustav hat alles gesehn.", "tokens": ["Gus\u00b7tav", "hat", "al\u00b7les", "ge\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PIS", "VVPP", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Er sagte: Das sei kein Benehmen.", "tokens": ["Er", "sag\u00b7te", ":", "Das", "sei", "kein", "Be\u00b7neh\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PDS", "VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was hilft es nun, mich zu sch\u00e4men.", "tokens": ["Was", "hilft", "es", "nun", ",", "mich", "zu", "sch\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ich m\u00f6chte manchmal ins Wasser gehn.", "tokens": ["Ich", "m\u00f6ch\u00b7te", "manch\u00b7mal", "ins", "Was\u00b7ser", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Meine Stellung hatte ich verloren.", "tokens": ["Mei\u00b7ne", "Stel\u00b7lung", "hat\u00b7te", "ich", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weil ich meinem Chef zu h\u00e4\u00dflich bin.", "tokens": ["Weil", "ich", "mei\u00b7nem", "Chef", "zu", "h\u00e4\u00df\u00b7lich", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "PTKA", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und nun habe ich ein M\u00e4dchen geboren,", "tokens": ["Und", "nun", "ha\u00b7be", "ich", "ein", "M\u00e4d\u00b7chen", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wo keinen Vater hat, und kein Kinn.", "tokens": ["Wo", "kei\u00b7nen", "Va\u00b7ter", "hat", ",", "und", "kein", "Kinn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "$,", "KON", "PIAT", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "Als mein Vormund sich erh\u00e4ngte,", "tokens": ["Als", "mein", "Vor\u00b7mund", "sich", "er\u00b7h\u00e4ng\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Besa\u00df ich noch das Kreppdischingewand,", "tokens": ["Be\u00b7sa\u00df", "ich", "noch", "das", "Kre\u00b7pp\u00b7di\u00b7schin\u00b7ge\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Was ich sp\u00e4ter der Anni schenkte.", "tokens": ["Was", "ich", "sp\u00e4\u00b7ter", "der", "An\u00b7ni", "schenk\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "ART", "NE", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Die war Masseuse in Helgoland.", "tokens": ["Die", "war", "Mas\u00b7seu\u00b7se", "in", "Hel\u00b7go\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Aber der bin ich nun b\u00f6se.", "tokens": ["A\u00b7ber", "der", "bin", "ich", "nun", "b\u00f6\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn die lie\u00df mich im Stich.", "tokens": ["Denn", "die", "lie\u00df", "mich", "im", "Stich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und die ist gar keine Masseuse,", "tokens": ["Und", "die", "ist", "gar", "kei\u00b7ne", "Mas\u00b7seu\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sondern geht auf den \u2013", "tokens": ["Son\u00b7dern", "geht", "auf", "den", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.10": {"line.1": {"text": "Mir ist nichts nachzusagen.", "tokens": ["Mir", "ist", "nichts", "nach\u00b7zu\u00b7sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "VVIZU", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ich habe mit einem Zahnarzt verkehrt.", "tokens": ["Ich", "ha\u00b7be", "mit", "ei\u00b7nem", "Zahn\u00b7arzt", "ver\u00b7kehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der hat mich auf H\u00e4nden getragen.", "tokens": ["Der", "hat", "mich", "auf", "H\u00e4n\u00b7den", "ge\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Doch ich habe mir selber mein Gl\u00fcck zerst\u00f6rt.", "tokens": ["Doch", "ich", "ha\u00b7be", "mir", "sel\u00b7ber", "mein", "Gl\u00fcck", "zer\u00b7st\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}}, "stanza.11": {"line.1": {"text": "Das war im Englischen Garten.", "tokens": ["Das", "war", "im", "Eng\u00b7li\u00b7schen", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da gab mir's der Teufel ein,", "tokens": ["Da", "gab", "mir's", "der", "Teu\u00b7fel", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df ich \u2013 um auf Gustav zu warten \u2013", "tokens": ["Da\u00df", "ich", "\u2013", "um", "auf", "Gus\u00b7tav", "zu", "war\u00b7ten", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$(", "KOUI", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In der Nase bohrte, ich Schwein.", "tokens": ["In", "der", "Na\u00b7se", "bohr\u00b7te", ",", "ich", "Schwein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$,", "PPER", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.12": {"line.1": {"text": "Gustav hat alles gesehn.", "tokens": ["Gus\u00b7tav", "hat", "al\u00b7les", "ge\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PIS", "VVPP", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Er sagte: Das sei kein Benehmen.", "tokens": ["Er", "sag\u00b7te", ":", "Das", "sei", "kein", "Be\u00b7neh\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PDS", "VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was hilft es nun, mich zu sch\u00e4men.", "tokens": ["Was", "hilft", "es", "nun", ",", "mich", "zu", "sch\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ich m\u00f6chte manchmal ins Wasser gehn.", "tokens": ["Ich", "m\u00f6ch\u00b7te", "manch\u00b7mal", "ins", "Was\u00b7ser", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}}}}