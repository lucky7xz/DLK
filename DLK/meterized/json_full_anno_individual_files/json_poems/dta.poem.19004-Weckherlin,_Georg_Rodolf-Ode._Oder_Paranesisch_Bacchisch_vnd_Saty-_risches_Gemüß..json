{"dta.poem.19004": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Ode.  \n Oder  \n Paranesisch/ Bacchisch vnd Saty-  \n risches Gem\u00fc\u00df.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1641", "urn": "urn:nbn:de:kobv:b4-200905198111", "language": ["de:0.99"], "booktitle": "Weckherlin, Georg Rodolf: Gaistliche und Weltliche Gedichte. Amsterdam, 1641."}, "poem": {"stanza.1": {"line.1": {"text": "Weil nu der lufft gantz vngestim", "tokens": ["Weil", "nu", "der", "lufft", "gantz", "vn\u00b7ge\u00b7stim"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "APPRART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit schnee vnd regen sich vermischet/", "tokens": ["Mit", "schnee", "vnd", "re\u00b7gen", "sich", "ver\u00b7mi\u00b7schet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "PRF", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd nu der wind mit nichten stum", "tokens": ["Vnd", "nu", "der", "wind", "mit", "nich\u00b7ten", "stum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das erdreich gleichsam saiffend waschet:", "tokens": ["Das", "er\u00b7dreich", "gleich\u00b7sam", "saif\u00b7fend", "wa\u00b7schet", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "ADJD", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So lasset Vns auch/ Liebe Freind", "tokens": ["So", "las\u00b7set", "Vns", "auch", "/", "Lie\u00b7be", "Freind"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$(", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "(was sprachen wir auch jmmer reden)", "tokens": ["(", "was", "spra\u00b7chen", "wir", "auch", "jm\u00b7mer", "re\u00b7den", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "ADV", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Den tisch bed\u00f6cken zu der stund", "tokens": ["Den", "tisch", "be\u00b7d\u00f6\u00b7cken", "zu", "der", "stund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Mit flaschen/ schuncken/ K\u00e4\u00df vnd Fladen.", "tokens": ["Mit", "fla\u00b7schen", "/", "schun\u00b7cken", "/", "K\u00e4\u00df", "vnd", "Fla\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "VVINF", "$(", "VVINF", "$(", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Bring her die gl\u00e4ser/ vnd schenck ein/", "tokens": ["Bring", "her", "die", "gl\u00e4\u00b7ser", "/", "vnd", "schenck", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "$(", "KON", "VVFIN", "ART", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Dan er kaum r\u00fcnnet au\u00df dem loch", "tokens": ["Dan", "er", "kaum", "r\u00fcn\u00b7net", "au\u00df", "dem", "loch"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.4": {"line.1": {"text": "Darumb wer doppeltes gut will", "tokens": ["Da\u00b7rumb", "wer", "dop\u00b7pel\u00b7tes", "gut", "will"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "PWS", "ADV", "ADJD", "VMFIN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Anschawen/ riechen/ schm\u00f6cken/ spiren/", "tokens": ["An\u00b7scha\u00b7wen", "/", "rie\u00b7chen", "/", "schm\u00f6\u00b7cken", "/", "spi\u00b7ren", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "VVINF", "$(", "VVINF", "$(", "VVINF", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Der mu\u00df nu einen becher voll", "tokens": ["Der", "mu\u00df", "nu", "ei\u00b7nen", "be\u00b7cher", "voll"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "ADV", "ART", "ADJA", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Des edlen rebensaffts nicht sparen:", "tokens": ["Des", "ed\u00b7len", "re\u00b7bens\u00b7affts", "nicht", "spa\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So nem ein jeder sein gesch\u00fctz/", "tokens": ["So", "nem", "ein", "je\u00b7der", "sein", "ge\u00b7sch\u00fctz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "PIAT", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Vnd eh wir es zugleich hinrichten/", "tokens": ["Vnd", "eh", "wir", "es", "zu\u00b7gleich", "hin\u00b7rich\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Mu\u00df er mit mir den reichen schatz", "tokens": ["Mu\u00df", "er", "mit", "mir", "den", "rei\u00b7chen", "schatz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "APPR", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Zu loben/ singend nicht verachten.", "tokens": ["Zu", "lo\u00b7ben", "/", "sin\u00b7gend", "nicht", "ver\u00b7ach\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "ADJD", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Dieweil nu dises ein Rhein-wein/", "tokens": ["Die\u00b7weil", "nu", "di\u00b7ses", "ein", "Rhein\u00b7wein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PDAT", "ART", "NN", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Oder dem Rheinwein zuvergleichen;", "tokens": ["O\u00b7der", "dem", "Rhein\u00b7wein", "zu\u00b7ver\u00b7glei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So schenck jhn in den becher ein/", "tokens": ["So", "schenck", "jhn", "in", "den", "be\u00b7cher", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "ART", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Jhn mit gold noch mehr zu bereichen:", "tokens": ["Jhn", "mit", "gold", "noch", "mehr", "zu", "be\u00b7rei\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "NN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Er ist klar/ lieblich/ frisch vnd reich/", "tokens": ["Er", "ist", "klar", "/", "lieb\u00b7lich", "/", "frisch", "vnd", "reich", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$(", "ADJD", "$(", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Darumb mu\u00df er herumb passieren:", "tokens": ["Da\u00b7rumb", "mu\u00df", "er", "he\u00b7rumb", "pas\u00b7sie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Jhr Herren disen bring Ich euch/", "tokens": ["Ihr", "Her\u00b7ren", "di\u00b7sen", "bring", "Ich", "euch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PDS", "VVFIN", "PPER", "PPER", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df keiner m\u00f6g die zeit verlieren.", "tokens": ["Da\u00df", "kei\u00b7ner", "m\u00f6g", "die", "zeit", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ist jener roht wein ein Frantzo\u00df/", "tokens": ["Ist", "je\u00b7ner", "roht", "wein", "ein", "Frant\u00b7zo\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "VVFIN", "ADV", "ART", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So thut er wol zu vns zu kommen:", "tokens": ["So", "thut", "er", "wol", "zu", "vns", "zu", "kom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er l\u00e4chlet wie ein rohte Ro\u00df/", "tokens": ["Er", "l\u00e4ch\u00b7let", "wie", "ein", "roh\u00b7te", "Ro\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd wirt von Vns gern angenommen:", "tokens": ["Vnd", "wirt", "von", "Vns", "gern", "an\u00b7ge\u00b7nom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich h\u00f6r nicht mehr des winds get\u00f6\u00df/", "tokens": ["Ich", "h\u00f6r", "nicht", "mehr", "des", "winds", "ge\u00b7t\u00f6\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sydher wir mit dem wein parlieren/", "tokens": ["Syd\u00b7her", "wir", "mit", "dem", "wein", "par\u00b7lie\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Beuvons. Messieurs, a vos santez,", "tokens": ["Beu\u00b7vons", ".", "Mes\u00b7sie\u00b7urs", ",", "a", "vos", "san\u00b7tez", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NN", "$,", "FM.fr", "FM.fr", "FM.fr", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.8": {"text": "So lasset vns all garaussieren.", "tokens": ["So", "las\u00b7set", "vns", "all", "ga\u00b7raus\u00b7sie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ho! wein her/ den vns das Welschland", "tokens": ["Ho", "!", "wein", "her", "/", "den", "vns", "das", "Wel\u00b7schland"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$.", "NN", "PTKVZ", "$(", "ART", "PPER", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ohn des Bapsts sig vnd segen sendet/", "tokens": ["Ohn", "des", "Bapsts", "sig", "vnd", "se\u00b7gen", "sen\u00b7det", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "KON", "NN", "VVFIN", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ein schalen voll in meiner hand", "tokens": ["Ein", "scha\u00b7len", "voll", "in", "mei\u00b7ner", "hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJD", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Davon/ wirt bald wol angewendet:", "tokens": ["Da\u00b7von", "/", "wirt", "bald", "wol", "an\u00b7ge\u00b7wen\u00b7det", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$(", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Die farb ist angenehm Ich sih/", "tokens": ["Die", "farb", "ist", "an\u00b7ge\u00b7nehm", "Ich", "sih", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Vnd sein geruch thut excellieren:", "tokens": ["Vnd", "sein", "ge\u00b7ruch", "thut", "ex\u00b7cel\u00b7lie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Signori, facciam brindisi,", "tokens": ["Sig\u00b7no\u00b7ri", ",", "fac\u00b7ci\u00b7am", "brin\u00b7di\u00b7si", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "FM", "FM", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Er kan nicht dan euch aggradieren.", "tokens": ["Er", "kan", "nicht", "dan", "euch", "ag\u00b7gra\u00b7die\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ein ander Welschland wei\u00df ich noch/", "tokens": ["Ein", "an\u00b7der", "Wel\u00b7schland", "wei\u00df", "ich", "noch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da man auch zechend fr\u00f6lich lebet", "tokens": ["Da", "man", "auch", "ze\u00b7chend", "fr\u00f6\u00b7lich", "le\u00b7bet"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "ADJD", "VVFIN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Mit Brot vnd K\u00e4\u00df/ vnd ohn den Koch", "tokens": ["Mit", "Brot", "vnd", "K\u00e4\u00df", "/", "vnd", "ohn", "den", "Koch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "$(", "KON", "APPR", "ART", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "(schier Schweitzer gleich) nach ehren strebet:", "tokens": ["(", "schier", "Schweit\u00b7zer", "gleich", ")", "nach", "eh\u00b7ren", "stre\u00b7bet", ":"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "ADV", "$(", "APPR", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Raich her das volle Kr\u00e4u\u00dflein da/", "tokens": ["Raich", "her", "das", "vol\u00b7le", "Kr\u00e4u\u00df\u00b7lein", "da", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APZR", "ART", "ADJA", "NN", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Es gilt den Herren vnd den Frawen:", "tokens": ["Es", "gilt", "den", "Her\u00b7ren", "vnd", "den", "Fra\u00b7wen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "M'y fato chwi (Ho!) miy fa,", "tokens": ["M'y", "fa\u00b7to", "chwi", "(", "Ho", "!", ")", "miy", "fa", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "$(", "NE", "$.", "$(", "FM", "FM", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "So das ist artlich ", "tokens": ["So", "das", "ist", "art\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PDS", "VAFIN", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Ist Engelland schon ohn Weinwachs/", "tokens": ["Ist", "En\u00b7gel\u00b7land", "schon", "ohn", "Wein\u00b7wachs", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADV", "APPR", "NN", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Hat man doch gute wein darinnen/", "tokens": ["Hat", "man", "doch", "gu\u00b7te", "wein", "da\u00b7rin\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "ADJA", "NN", "ADV", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd mancher drincket als ein Sachs/", "tokens": ["Vnd", "man\u00b7cher", "drin\u00b7cket", "als", "ein", "Sachs", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wan er die schlacht gern wolt gewinnen:", "tokens": ["Wan", "er", "die", "schlacht", "gern", "wolt", "ge\u00b7win\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "VVFIN", "ADV", "VMFIN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Drinck mir ein gla\u00df des besten zu/", "tokens": ["Drinck", "mir", "ein", "gla\u00df", "des", "bes\u00b7ten", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "ART", "ADJA", "PTKZU", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit welchem die Jusuln prachtieren:", "tokens": ["Mit", "wel\u00b7chem", "die", "Ju\u00b7suln", "prach\u00b7tie\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Then lett us drink, J'le drink to you,", "tokens": ["Then", "lett", "us", "drink", ",", "J'\u00b7le", "drink", "to", "yo\u00b7u", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "$,", "FM.en", "FM.en", "FM.en", "FM.en", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Kan ein wein disen surpassieren?", "tokens": ["Kan", "ein", "wein", "di\u00b7sen", "sur\u00b7pas\u00b7sie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Die Nider-Teutsche frische Fisch/", "tokens": ["Die", "Ni\u00b7der\u00b7Teut\u00b7sche", "fri\u00b7sche", "Fisch", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die gern lang hinder dem tisch sitzen/", "tokens": ["Die", "gern", "lang", "hin\u00b7der", "dem", "tisch", "sit\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "APPR", "ART", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Lieben den wein der starck vnd frisch/", "tokens": ["Lie\u00b7ben", "den", "wein", "der", "starck", "vnd", "frisch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "ART", "NN", "ART", "NN", "KON", "ADJD", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Vnd zechen gern bi\u00df da\u00df sie schwitzen:", "tokens": ["Vnd", "ze\u00b7chen", "gern", "bi\u00df", "da\u00df", "sie", "schwit\u00b7zen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "KOUS", "PPER", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "So gib auch jhretwegen nu", "tokens": ["So", "gib", "auch", "jhret\u00b7we\u00b7gen", "nu"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "ADV", "PPOSAT", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Den grossen Kelch damit zu zehren:", "tokens": ["Den", "gros\u00b7sen", "Kelch", "da\u00b7mit", "zu", "zeh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PAV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Myn Heeren, ho, ick brengh het u,", "tokens": ["Myn", "Hee\u00b7ren", ",", "ho", ",", "ick", "brengh", "het", "u", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "ITJ", "$,", "PPER", "ADJD", "VAFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So. dises haisset recht laveeren.", "tokens": ["So", ".", "di\u00b7ses", "hais\u00b7set", "recht", "la\u00b7vee\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PDS", "VVFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Seit jhr den Spaniern hie feind/", "tokens": ["Seit", "jhr", "den", "Spa\u00b7ni\u00b7ern", "hie", "feind", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "So langsamb jhrer zugedencken?", "tokens": ["So", "lang\u00b7samb", "jhrer", "zu\u00b7ge\u00b7den\u00b7cken", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Seind sie doch aller L\u00e4nder freind/", "tokens": ["Seind", "sie", "doch", "al\u00b7ler", "L\u00e4n\u00b7der", "freind", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wan sie den wein schon nicht verschencken:", "tokens": ["Wan", "sie", "den", "wein", "schon", "nicht", "ver\u00b7schen\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Gib jhres weins das gl\u00e4\u00dflein da", "tokens": ["Gib", "jhres", "weins", "das", "gl\u00e4\u00df\u00b7lein", "da"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "PPOSAT", "ADJA", "ART", "NN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Damit Ich besser m\u00f6g hablieren/", "tokens": ["Da\u00b7mit", "Ich", "bes\u00b7ser", "m\u00f6g", "hab\u00b7lie\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "A su salud, O, alla va,", "tokens": ["A", "su", "sa\u00b7lud", ",", "O", ",", "al\u00b7la", "va", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "$,", "NE", "$,", "FM.la", "FM.la", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Wer will mag emborraciarsieren.", "tokens": ["Wer", "will", "mag", "em\u00b7bor\u00b7ra\u00b7ci\u00b7ar\u00b7sie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "VMFIN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "In Jrland war ich auch einmahl/", "tokens": ["In", "Jr\u00b7land", "war", "ich", "auch", "ein\u00b7mahl", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VAFIN", "PPER", "ADV", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd sah dort manche ding verwirren/", "tokens": ["Vnd", "sah", "dort", "man\u00b7che", "ding", "ver\u00b7wir\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch wissend wol die rechte wahl", "tokens": ["Doch", "wis\u00b7send", "wol", "die", "rech\u00b7te", "wahl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Lie\u00df ich mich billich nicht verjrren:", "tokens": ["Lie\u00df", "ich", "mich", "bil\u00b7lich", "nicht", "ver\u00b7jr\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Schenck ein ein wenig ", "tokens": ["Schenck", "ein", "ein", "we\u00b7nig"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "ART", "PIS"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "In Jrland vberall geliebet:", "tokens": ["In", "Jr\u00b7land", "vbe\u00b7rall", "ge\u00b7lie\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Sho fed tuorim; den go sugagh,", "tokens": ["Sho", "fed", "tu\u00b7o\u00b7rim", ";", "den", "go", "su\u00b7gagh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$.", "ART", "FM", "FM", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.8": {"text": "So/ dises haisset wol ge\u00fcbet.", "tokens": ["So", "/", "di\u00b7ses", "hais\u00b7set", "wol", "ge\u00b7\u00fc\u00b7bet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PDS", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "H\u00f6r ich nicht Fratzen/ den Dickkopff/", "tokens": ["H\u00f6r", "ich", "nicht", "Frat\u00b7zen", "/", "den", "Dick\u00b7kopff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "PPER", "PTKNEG", "NN", "$(", "ART", "NN", "$("], "meter": "+--+--++", "measure": "dactylic.di.plus"}, "line.2": {"text": "Der/ witzlo\u00df/ jederman will lehren?", "tokens": ["Der", "/", "witz\u00b7lo\u00df", "/", "je\u00b7der\u00b7man", "will", "leh\u00b7ren", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "$(", "VVFIN", "$(", "PIS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd welcher/ ein recht grober Knopff/", "tokens": ["Vnd", "wel\u00b7cher", "/", "ein", "recht", "gro\u00b7ber", "Knopff", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAT", "$(", "ART", "ADJD", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ohn sich selbs niemand sunst will ehren?", "tokens": ["Ohn", "sich", "selbs", "nie\u00b7mand", "sunst", "will", "eh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PRF", "ADV", "PIS", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Es gilt hie sechs/ in einem suff/", "tokens": ["Es", "gilt", "hie", "sechs", "/", "in", "ei\u00b7nem", "suff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "CARD", "$(", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Herr Fratz/ jhr m\u00fcsset das au\u00df sauffen/", "tokens": ["Herr", "Fratz", "/", "jhr", "m\u00fcs\u00b7set", "das", "au\u00df", "sauf\u00b7fen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$(", "PPER", "VMFIN", "PDS", "PTKVZ", "VVFIN", "$("], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Es gilt/ Fratz/ Curly/ Murly/ Buff/", "tokens": ["Es", "gilt", "/", "Fratz", "/", "Cur\u00b7ly", "/", "Mur\u00b7ly", "/", "Buff", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Bi\u00df alle fallen vbern hauffen.", "tokens": ["Bi\u00df", "al\u00b7le", "fal\u00b7len", "vbern", "hauf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "KON", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.14": {"line.1": {"text": "Ich glaub/ jhr liebe ", "tokens": ["Ich", "glaub", "/", "jhr", "lie\u00b7be"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "PPER", "VVFIN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Da\u00df jhr das Latein gar verschworen/", "tokens": ["Da\u00df", "jhr", "das", "La\u00b7tein", "gar", "ver\u00b7schwo\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd auch das Griechisch/ al\u00df ich sih/", "tokens": ["Vnd", "auch", "das", "Grie\u00b7chisch", "/", "al\u00df", "ich", "sih", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$(", "KOUS", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist nu verachtet vnd verloren:", "tokens": ["Ist", "nu", "ver\u00b7ach\u00b7tet", "vnd", "ver\u00b7lo\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVFIN", "KON", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Doch weil ein Christliches R\u00e4uschlein", "tokens": ["Doch", "weil", "ein", "Christ\u00b7li\u00b7ches", "R\u00e4usc\u00b7hlein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Nicht kan (", "tokens": ["Nicht", "kan", "("], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "$("], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "Bring ich euch/ ", "tokens": ["Bring", "ich", "euch", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.8": {"text": "Vnd wolt euch jetzt nicht gern turbieren.", "tokens": ["Vnd", "wolt", "euch", "jetzt", "nicht", "gern", "tur\u00b7bie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Ho! Herr Fratz/ was bedeuten doch", "tokens": ["Ho", "!", "Herr", "Fratz", "/", "was", "be\u00b7deu\u00b7ten", "doch"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "$.", "NN", "NE", "$(", "PWS", "VVFIN", "ADV"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Schmorotzer/ Blacken vnd Bachanten/", "tokens": ["Schmo\u00b7rot\u00b7zer", "/", "Bla\u00b7cken", "vnd", "Ba\u00b7ch\u00b7an\u00b7ten", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die so verhasset von dem Koch/", "tokens": ["Die", "so", "ver\u00b7has\u00b7set", "von", "dem", "Koch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "APPR", "ART", "NE", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Schulf\u00fcchs/ Penalen/ Pedanten?", "tokens": ["Als", "Schul\u00b7f\u00fcchs", "/", "Pe\u00b7na\u00b7len", "/", "Pe\u00b7dan\u00b7ten", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KOUS", "NE", "$(", "NN", "$(", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.5": {"text": "Warumb darff ohn ein Narrenkapp", "tokens": ["Wa\u00b7rumb", "darff", "ohn", "ein", "Nar\u00b7ren\u00b7kapp"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "VMFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Narr halb welsch vn\u0303 halb Teutsch glotzen?", "tokens": ["Ein", "Narr", "halb", "welsch", "v\u00f1", "halb", "Teutsch", "glot\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "ADJD", "ADV", "ADJD", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Warumb doch will ein jeder lapp", "tokens": ["Wa\u00b7rumb", "doch", "will", "ein", "je\u00b7der", "lapp"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "F\u00fcr gut Teutsch ", "tokens": ["F\u00fcr", "gut", "Teutsch"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJD", "NN"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.16": {"line.1": {"text": "Ist es nicht eines bl\u00f6den hirns", "tokens": ["Ist", "es", "nicht", "ei\u00b7nes", "bl\u00f6\u00b7den", "hirns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PTKNEG", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd eines Hasenkopfs merck-zaichen/", "tokens": ["Vnd", "ei\u00b7nes", "Ha\u00b7sen\u00b7kopfs", "mer\u00b7ck\u00b7zai\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der wol wehrt eines langen horns", "tokens": ["Der", "wol", "wehrt", "ei\u00b7nes", "lan\u00b7gen", "horns"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd gar nicht wehrt mit Vns zu zechen?", "tokens": ["Vnd", "gar", "nicht", "wehrt", "mit", "Vns", "zu", "ze\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "VVFIN", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Mit Vns/ die wir dem guten wein", "tokens": ["Mit", "Vns", "/", "die", "wir", "dem", "gu\u00b7ten", "wein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "$(", "PRELS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Allein zu ehren welsch gegecket/", "tokens": ["Al\u00b7lein", "zu", "eh\u00b7ren", "welsch", "ge\u00b7ge\u00b7cket", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VVINF", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Vnd doch mit gr\u00f6sserm flei\u00df vnd wohn", "tokens": ["Vnd", "doch", "mit", "gr\u00f6s\u00b7serm", "flei\u00df", "vnd", "wohn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "In der welt grosses buch gegucket.", "tokens": ["In", "der", "welt", "gros\u00b7ses", "buch", "ge\u00b7gu\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Glick zu du ohn ein G. gesel/", "tokens": ["Glick", "zu", "du", "ohn", "ein", "G.", "ge\u00b7sel", "/"], "token_info": ["word", "word", "word", "word", "word", "abbreviation", "word", "punct"], "pos": ["NN", "APPR", "PPER", "APPR", "ART", "NN", "NE", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat mich der ", "tokens": ["Hat", "mich", "der"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Er glaub mir/ da\u00df dem ", "tokens": ["Er", "glaub", "mir", "/", "da\u00df", "dem"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$(", "KOUS", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Ich auffwarten in wenig stunden:", "tokens": ["Ich", "auff\u00b7war\u00b7ten", "in", "we\u00b7nig", "stun\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "VVINF", "$."], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Dan ", "tokens": ["Dan"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "So hat er sehr gevoyagieret.", "tokens": ["So", "hat", "er", "sehr", "ge\u00b7vo\u00b7ya\u00b7gie\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Der Teufel hol Euch (ohn ein N.", "tokens": ["Der", "Teu\u00b7fel", "hol", "Euch", "(", "ohn", "ein", "N."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "abbreviation"], "pos": ["ART", "NN", "VVFIN", "PPER", "$(", "APPR", "ART", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.8": {"text": "Herr Han\u00df) weil jhr vns all vexieret.", "tokens": ["Herr", "Han\u00df", ")", "weil", "jhr", "vns", "all", "ve\u00b7xie\u00b7ret", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$(", "KOUS", "PPER", "PRF", "PIAT", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Wer Teutsch ist der red auch gut Teutsch/", "tokens": ["Wer", "Teutsch", "ist", "der", "red", "auch", "gut", "Teutsch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "ART", "VVFIN", "ADV", "ADJD", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Wie der Welsch will gut welsch parlieren;", "tokens": ["Wie", "der", "Welsch", "will", "gut", "welsch", "par\u00b7lie\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VMFIN", "ADJD", "ADJD", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Zu fu\u00df geh wer ohn pferd vnd gutsch/", "tokens": ["Zu", "fu\u00df", "geh", "wer", "ohn", "pferd", "vnd", "gutsch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PTKVZ", "VVFIN", "PWS", "APPR", "NN", "KON", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd wer ein Narr/ kan nicht vil lehren:", "tokens": ["Vnd", "wer", "ein", "Narr", "/", "kan", "nicht", "vil", "leh\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "$(", "VMFIN", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So sprechet nu ein Vrthail au\u00df", "tokens": ["So", "spre\u00b7chet", "nu", "ein", "Vrt\u00b7hail", "au\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "(vnd m\u00e4niglich mag es wol h\u00f6ren)", "tokens": ["(", "vnd", "m\u00e4\u00b7nig\u00b7lich", "mag", "es", "wol", "h\u00f6\u00b7ren", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADJD", "VMFIN", "PPER", "ADV", "VVINF", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.7": {"text": "Gleich ist ein halb-Welsch-Teutscher Ha\u00df", "tokens": ["Gleich", "ist", "ein", "halb\u00b7Wel\u00b7schTeut\u00b7scher", "Ha\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Den angestrichnen krancken Huren.", "tokens": ["Den", "an\u00b7ge\u00b7strich\u00b7nen", "kran\u00b7cken", "Hu\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Vnd gleich wie der ein schwein/ gan\u00df/ kalb/", "tokens": ["Vnd", "gleich", "wie", "der", "ein", "schwein", "/", "gan\u00df", "/", "kalb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "ART", "ART", "ADJD", "$(", "ADV", "$(", "ADJD", "$("], "meter": "-----+-+", "measure": "unknown.measure.di"}, "line.2": {"text": "Der gut vnd b\u00f6sen wein vermischet;", "tokens": ["Der", "gut", "vnd", "b\u00f6\u00b7sen", "wein", "ver\u00b7mi\u00b7schet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So dem geh\u00f6rt ein Narren-kolb/", "tokens": ["So", "dem", "ge\u00b7h\u00f6rt", "ein", "Nar\u00b7ren\u00b7kolb", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Teutsch vnd Welsch zusamen waschet:", "tokens": ["Der", "Teutsch", "vnd", "Welsch", "zu\u00b7sa\u00b7men", "wa\u00b7schet", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Sein hirn vnd red seind gelb/ wei\u00df/ schwartz/", "tokens": ["Sein", "hirn", "vnd", "red", "seind", "gelb", "/", "wei\u00df", "/", "schwartz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "VVFIN", "VAFIN", "ADJD", "$(", "VVFIN", "$(", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Gr\u00fcn/ roht/ vnd blaw/ ein schneider k\u00fcssin/", "tokens": ["Gr\u00fcn", "/", "roht", "/", "vnd", "blaw", "/", "ein", "schnei\u00b7der", "k\u00fcs\u00b7sin", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "$(", "KON", "ADJD", "$(", "ART", "ADJA", "NN", "$("], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.7": {"text": "Ein alter schurtz/ ein lamer schertz/", "tokens": ["Ein", "al\u00b7ter", "schurtz", "/", "ein", "la\u00b7mer", "schertz", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Vnd gantz vnw\u00fcrdig mehrer bossen.", "tokens": ["Vnd", "gantz", "vn\u00b7w\u00fcr\u00b7dig", "meh\u00b7rer", "bos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "PIAT", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Kom (schenckend das gla\u00df wider ein)", "tokens": ["Kom", "(", "schen\u00b7ckend", "das", "gla\u00df", "wi\u00b7der", "ein", ")"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "ADJD", "ART", "NN", "APPR", "ART", "$("], "meter": "-----+-+", "measure": "unknown.measure.di"}, "line.2": {"text": "Vns des lusts wider zu begaben:", "tokens": ["Vns", "des", "lusts", "wi\u00b7der", "zu", "be\u00b7ga\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df drinckend/ singend/ redend/ rein/", "tokens": ["Da\u00df", "drin\u00b7ckend", "/", "sin\u00b7gend", "/", "re\u00b7dend", "/", "rein", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KOUS", "ADJD", "$(", "ADJD", "$(", "VVPP", "$(", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir Vns vnd andre auch erlaben:", "tokens": ["Wir", "Vns", "vnd", "and\u00b7re", "auch", "er\u00b7la\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Doch drinck wer will: Ich hab zuvil/", "tokens": ["Doch", "drinck", "wer", "will", ":", "Ich", "hab", "zu\u00b7vil", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PWS", "VMFIN", "$.", "PPER", "VAFIN", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wer will mag drincken/ dantzen/ springen/", "tokens": ["Wer", "will", "mag", "drin\u00b7cken", "/", "dant\u00b7zen", "/", "sprin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWS", "VMFIN", "VMFIN", "VVINF", "$(", "VVINF", "$(", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Frey bleibet jedem alles spihl/", "tokens": ["Frey", "blei\u00b7bet", "je\u00b7dem", "al\u00b7les", "spihl", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIAT", "PIS", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Vnd wer will mag nu mit mir singen:", "tokens": ["Vnd", "wer", "will", "mag", "nu", "mit", "mir", "sin\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VMFIN", "VMFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Frisch auff/ frisch auff/ seit wol zu muht", "tokens": ["Frisch", "auff", "/", "frisch", "auff", "/", "seit", "wol", "zu", "muht"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "APPR", "$(", "ADJD", "APPR", "$(", "APPR", "ADV", "PTKZU", "VVFIN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Herumb das gl\u00e4\u00dflein bald mu\u00df fahren:", "tokens": ["He\u00b7rumb", "das", "gl\u00e4\u00df\u00b7lein", "bald", "mu\u00df", "fah\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "B\u00f6\u00df ist das wetter/ der wein gut/", "tokens": ["B\u00f6\u00df", "ist", "das", "wet\u00b7ter", "/", "der", "wein", "gut", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ART", "NN", "$(", "ART", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd jhrer keines nu zu spahren.", "tokens": ["Vnd", "jhrer", "kei\u00b7nes", "nu", "zu", "spah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "PIS", "ADV", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Der wein sparet zwar die witz", "tokens": ["Der", "wein", "spa\u00b7ret", "zwar", "die", "witz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Mit nichten/", "tokens": ["Mit", "nich\u00b7ten", "/"], "token_info": ["word", "word", "punct"], "pos": ["APPR", "PIS", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Weil er mit zu starcker hitz", "tokens": ["Weil", "er", "mit", "zu", "star\u00b7cker", "hitz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "APPR", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Dr\u00fccknet vnser dichten.", "tokens": ["Dr\u00fcck\u00b7net", "vn\u00b7ser", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.22": {"line.1": {"text": "Ich wai\u00df zwar noch wol wa Ich bin/", "tokens": ["Ich", "wai\u00df", "zwar", "noch", "wol", "wa", "Ich", "bin", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ADV", "VVFIN", "PPER", "VAFIN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Darff aber wol f\u00fcr etlich schw\u00f6ren/", "tokens": ["Darff", "a\u00b7ber", "wol", "f\u00fcr", "et\u00b7lich", "schw\u00f6\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "APPR", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df sie sehr gern jhr hertz vnd sin", "tokens": ["Da\u00df", "sie", "sehr", "gern", "jhr", "hertz", "vnd", "sin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "All tag verbausen vnd verz\u00f6hren:", "tokens": ["All", "tag", "ver\u00b7bau\u00b7sen", "vnd", "ver\u00b7z\u00f6h\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVINF", "KON", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Bleibet jhr verstand ohn wein", "tokens": ["Blei\u00b7bet", "jhr", "ver\u00b7stand", "ohn", "wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Dahinden/", "tokens": ["Da\u00b7hin\u00b7den", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "K\u00f6nden sie/ als st\u00f6ck vnd stein", "tokens": ["K\u00f6n\u00b7den", "sie", "/", "als", "st\u00f6ck", "vnd", "stein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "$(", "KOUS", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Drinckend jhn nicht finden.", "tokens": ["Drin\u00b7ckend", "jhn", "nicht", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.23": {"line.1": {"text": "Sih da/ wie wei\u00df der ", "tokens": ["Sih", "da", "/", "wie", "wei\u00df", "der"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "$(", "PWAV", "VVFIN", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Sich vnder Vns allhie erzaiget/", "tokens": ["Sich", "vn\u00b7der", "Vns", "all\u00b7hie", "er\u00b7zai\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er beissz mir doch auff dise nussz/", "tokens": ["Er", "be\u00b7issz", "mir", "doch", "auff", "di\u00b7se", "nussz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPER", "ADV", "APPR", "PDAT", "NN", "$("], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.4": {"text": "Sprach Fratz mit drincken nicht geschwaiget:", "tokens": ["Sprach", "Fratz", "mit", "drin\u00b7cken", "nicht", "ge\u00b7schwai\u00b7get", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "VVFIN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Vnd ", "tokens": ["Vnd"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Selbs reymen", "tokens": ["Selbs", "rey\u00b7men"], "token_info": ["word", "word"], "pos": ["ADV", "VVINF"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Besser dan jhr/ Ja dan du/", "tokens": ["Bes\u00b7ser", "dan", "jhr", "/", "Ja", "dan", "du", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "PPOSAT", "$(", "PTKANT", "ADV", "PPER", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Vnd das loch verleymen.", "tokens": ["Vnd", "das", "loch", "ver\u00b7ley\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "ADV", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.24": {"line.1": {"text": "Ich hab die l\u00e4nder diser welt", "tokens": ["Ich", "hab", "die", "l\u00e4n\u00b7der", "di\u00b7ser", "welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schon vil Jahr her gedurch-marschieret/", "tokens": ["Schon", "vil", "Jahr", "her", "ge\u00b7durch\u00b7mar\u00b7schie\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "APZR", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd hab auch ", "tokens": ["Vnd", "hab", "auch"], "token_info": ["word", "word", "word"], "pos": ["KON", "VAFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Dan all jhr welsche verspendieret;", "tokens": ["Dan", "all", "jhr", "wel\u00b7sche", "ver\u00b7spen\u00b7die\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "PPOSAT", "NN", "VVFIN", "$."], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Kan ich aber nicht vil welsch", "tokens": ["Kan", "ich", "a\u00b7ber", "nicht", "vil", "welsch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "PTKNEG", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Parlieren/", "tokens": ["Par\u00b7lie\u00b7ren", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "So kan ich doch (gar nicht falsch)", "tokens": ["So", "kan", "ich", "doch", "(", "gar", "nicht", "falsch", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "$(", "ADV", "PTKNEG", "ADJD", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Meinen becher l\u00f6hren.", "tokens": ["Mei\u00b7nen", "be\u00b7cher", "l\u00f6h\u00b7ren", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.25": {"line.1": {"text": "Jhr Herren Ich brauch keine List/", "tokens": ["Ihr", "Her\u00b7ren", "Ich", "brauch", "kei\u00b7ne", "List", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VVFIN", "PIAT", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Ich drinck vnd hab nichts zu bedencken:", "tokens": ["Ich", "drinck", "vnd", "hab", "nichts", "zu", "be\u00b7den\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "KON", "VAFIN", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zu drincken ist allein mein lust/", "tokens": ["Zu", "drin\u00b7cken", "ist", "al\u00b7lein", "mein", "lust", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "ADV", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es gilt/ vnd solt mir keiner dancken:", "tokens": ["Es", "gilt", "/", "vnd", "solt", "mir", "kei\u00b7ner", "dan\u00b7cken", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Will dan ewer keiner mir", "tokens": ["Will", "dan", "e\u00b7wer", "kei\u00b7ner", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ADV", "PPOSAT", "PIS", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Antworten/", "tokens": ["Ant\u00b7wor\u00b7ten", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Sollet jhr auch bi\u00df ich mehr", "tokens": ["Sol\u00b7let", "jhr", "auch", "bi\u00df", "ich", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "KOUS", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Euch hofiere/ warten.", "tokens": ["Euch", "ho\u00b7fie\u00b7re", "/", "war\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.26": {"line.1": {"text": "Wie offt hab ich mit einem wort", "tokens": ["Wie", "offt", "hab", "ich", "mit", "ei\u00b7nem", "wort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verjaget manche dolle Katzen?", "tokens": ["Ver\u00b7ja\u00b7get", "man\u00b7che", "dol\u00b7le", "Kat\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie offt hab ich mit meinem schwert", "tokens": ["Wie", "offt", "hab", "ich", "mit", "mei\u00b7nem", "schwert"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "PPER", "APPR", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zerhacket manchen tollen Kautzen?", "tokens": ["Zer\u00b7ha\u00b7cket", "man\u00b7chen", "tol\u00b7len", "Kaut\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dise faust hat so vil blut", "tokens": ["Di\u00b7se", "faust", "hat", "so", "vil", "blut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "VAFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vergossen/", "tokens": ["Ver\u00b7gos\u00b7sen", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Da\u00df ohn blut kein stein/ baum/ blat/", "tokens": ["Da\u00df", "ohn", "blut", "kein", "stein", "/", "baum", "/", "blat", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "PIAT", "NN", "$(", "ADV", "$(", "VVFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Keine w\u00e4ld/ feld/ gassen.", "tokens": ["Kei\u00b7ne", "w\u00e4ld", "/", "feld", "/", "gas\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PIAT", "NN", "$(", "NN", "$(", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.27": {"line.1": {"text": "So bin ich auch offt auff dem Meer", "tokens": ["So", "bin", "ich", "auch", "offt", "auff", "dem", "Meer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schier in der Sonnen selbs ersoffen:", "tokens": ["Schier", "in", "der", "Son\u00b7nen", "selbs", "er\u00b7sof\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ADV", "VVPP", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Daher ich auch schwartz als ein Mohr", "tokens": ["Da\u00b7her", "ich", "auch", "schwartz", "als", "ein", "Mohr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "ADV", "ADJD", "KOKOM", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Hat mit der Venus offt zuschaffen:", "tokens": ["Hat", "mit", "der", "Ve\u00b7nus", "offt", "zu\u00b7schaf\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Vnd durch manchen haissen schmatz", "tokens": ["Vnd", "durch", "man\u00b7chen", "hais\u00b7sen", "schmatz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Verliebet/", "tokens": ["Ver\u00b7lie\u00b7bet", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Hat der Proserpina schmotz", "tokens": ["Hat", "der", "Pro\u00b7ser\u00b7pi\u00b7na", "schmotz"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NE", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Offt mein hertz erlabet.", "tokens": ["Offt", "mein", "hertz", "er\u00b7la\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.28": {"line.1": {"text": "Was hat sie vnder jhrem B\u00f6ltz/", "tokens": ["Was", "hat", "sie", "vn\u00b7der", "jhrem", "B\u00f6ltz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Da\u00df sie sich lie\u00df so gern auffsch\u00fcrtzen?", "tokens": ["Da\u00df", "sie", "sich", "lie\u00df", "so", "gern", "auff\u00b7sch\u00fcrt\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich wai\u00df nicht was f\u00fcr Plutons boltz/", "tokens": ["Ich", "wai\u00df", "nicht", "was", "f\u00fcr", "Plu\u00b7tons", "boltz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PRELS", "APPR", "NE", "NE", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der pflag gar Teufelisch zu schertzen.", "tokens": ["Der", "pflag", "gar", "Teu\u00b7fe\u00b7lisch", "zu", "schert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ha. Er ist ein arger Fuchs", "tokens": ["Ha", ".", "Er", "ist", "ein", "ar\u00b7ger", "Fuchs"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Ohn zweifel/", "tokens": ["Ohn", "zwei\u00b7fel", "/"], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Er ist alles vbels ", "tokens": ["Er", "ist", "al\u00b7les", "vbels"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIAT", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "Vnd ein rechter Teufel.", "tokens": ["Vnd", "ein", "rech\u00b7ter", "Teu\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.29": {"line.1": {"text": "Er hat zway h\u00f6rner als ein Ochs/", "tokens": ["Er", "hat", "zway", "h\u00f6r\u00b7ner", "als", "ein", "Ochs", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd seine seuftzen seind fewrflammen;", "tokens": ["Vnd", "sei\u00b7ne", "seuft\u00b7zen", "seind", "fewr\u00b7flam\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Dem Dunder gleich ist seine ", "tokens": ["Dem", "Dun\u00b7der", "gleich", "ist", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VAFIN", "PPOSAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Weil er von allen strahlen stammen:", "tokens": ["Weil", "er", "von", "al\u00b7len", "strah\u00b7len", "stam\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Seine augen/ wan es ", "tokens": ["Sei\u00b7ne", "au\u00b7gen", "/", "wan", "es"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$(", "PWAV", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Klar brennen:", "tokens": ["Klar", "bren\u00b7nen", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Ist es tag/ so ist er ", "tokens": ["Ist", "es", "tag", "/", "so", "ist", "er"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NN", "$(", "ADV", "VAFIN", "PPER"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "F\u00fcnsternu\u00df zu nennen.", "tokens": ["F\u00fcns\u00b7ter\u00b7nu\u00df", "zu", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.30": {"line.1": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.2": {"text": "Dabey mein hertz an sie gedencket/", "tokens": ["Da\u00b7bey", "mein", "hertz", "an", "sie", "ge\u00b7den\u00b7cket", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Dieweil zuvor der hipsche ", "tokens": ["Die\u00b7weil", "zu\u00b7vor", "der", "hip\u00b7sche"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Dieselbig jhr au\u00df lieb geschencket:", "tokens": ["Die\u00b7sel\u00b7big", "jhr", "au\u00df", "lieb", "ge\u00b7schen\u00b7cket", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wie Er/ hab ich mit jhr f\u00fcchs", "tokens": ["Wie", "Er", "/", "hab", "ich", "mit", "jhr", "f\u00fcchs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "$(", "VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Gejaget/", "tokens": ["Ge\u00b7ja\u00b7get", "/"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "War es regen oder ", "tokens": ["War", "es", "re\u00b7gen", "o\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADJA", "KON"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Hab ich es gewaget.", "tokens": ["Hab", "ich", "es", "ge\u00b7wa\u00b7get", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.31": {"line.1": {"text": "Gleich wie ein doppelt klare ", "tokens": ["Gleich", "wie", "ein", "dop\u00b7pelt", "kla\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "ADJD", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Die anblick jhrer augen leuchten:", "tokens": ["Die", "an\u00b7blick", "jhrer", "au\u00b7gen", "leuch\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Vor jhrem Man ein T\u00fcrck vnd ", "tokens": ["Vor", "jhrem", "Man", "ein", "T\u00fcrck", "vnd"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "PIS", "ART", "NN", "KON"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Mu\u00df zittern/ stincken vnd bald be\u00fcchten;", "tokens": ["Mu\u00df", "zit\u00b7tern", "/", "stin\u00b7cken", "vnd", "bald", "be\u00fcch\u00b7ten", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$(", "VVINF", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Jhre Magd/ die wie ein Dachs", "tokens": ["Ih\u00b7re", "Magd", "/", "die", "wie", "ein", "Dachs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$(", "ART", "KOKOM", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Sich bucket/", "tokens": ["Sich", "bu\u00b7cket", "/"], "token_info": ["word", "word", "punct"], "pos": ["PRF", "VVFIN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "War auch vrsach/ da\u00df sich ", "tokens": ["War", "auch", "vr\u00b7sach", "/", "da\u00df", "sich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "ADV", "NE", "$(", "KOUS", "PRF"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Zwischen Vns offt drucket.", "tokens": ["Zwi\u00b7schen", "Vns", "offt", "dru\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.32": {"line.1": {"text": "Wer ist begihrig jhres specks/", "tokens": ["Wer", "ist", "be\u00b7gih\u00b7rig", "jhres", "specks", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "PPOSAT", "NN", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Dem will ich bald ein bi\u00dflein schneiden;", "tokens": ["Dem", "will", "ich", "bald", "ein", "bi\u00df\u00b7lein", "schnei\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "ART", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sehr gro\u00df ist jhrer grillen ", "tokens": ["Sehr", "gro\u00df", "ist", "jhrer", "gril\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "PPOSAT", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Die k\u00f6nt ich lieber dan Euch leyden:", "tokens": ["Die", "k\u00f6nt", "ich", "lie\u00b7ber", "dan", "Euch", "ley\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dan ich mag nicht ewers Dr \u2026", "tokens": ["Dan", "ich", "mag", "nicht", "e\u00b7wers", "Dr", "\u2026"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VMFIN", "PTKNEG", "PPOSAT", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vergissen:", "tokens": ["Ver\u00b7gis\u00b7sen", ":"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Drinck da/ drinck/ das ist das ", "tokens": ["Drinck", "da", "/", "drinck", "/", "das", "ist", "das"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$(", "PTKVZ", "$(", "PDS", "VAFIN", "ART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Welcher nicht will essen.", "tokens": ["Wel\u00b7cher", "nicht", "will", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "PTKNEG", "VMFIN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.33": {"line.1": {"text": "F\u00fcr meine witz ist hie kein ", "tokens": ["F\u00fcr", "mei\u00b7ne", "witz", "ist", "hie", "kein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ADV", "PIAT"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "F\u00fcr mein gesicht kein liecht zu sehen;", "tokens": ["F\u00fcr", "mein", "ge\u00b7sicht", "kein", "liecht", "zu", "se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVPP", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr meine hand kein Kelch/ kein ", "tokens": ["F\u00fcr", "mei\u00b7ne", "hand", "kein", "Kelch", "/", "kein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "NN", "PIAT", "NN", "$(", "PIAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "F\u00fcr meine f\u00fc\u00df kein stand zu stehen;", "tokens": ["F\u00fcr", "mei\u00b7ne", "f\u00fc\u00df", "kein", "stand", "zu", "ste\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "PIAT", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ho! wer hat mich bey dem haar", "tokens": ["Ho", "!", "wer", "hat", "mich", "bey", "dem", "haar"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "PWS", "VAFIN", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Gerauffet:", "tokens": ["Ger\u00b7auf\u00b7fet", ":"], "token_info": ["word", "punct"], "pos": ["NE", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Mord/ raub/ raub/ mord/ O gefahr/", "tokens": ["Mord", "/", "raub", "/", "raub", "/", "mord", "/", "O", "ge\u00b7fahr", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "$(", "VVFIN", "$(", "NE", "$(", "NE", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Alles rund vmblauffet.", "tokens": ["Al\u00b7les", "rund", "vm\u00b7blauf\u00b7fet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.34": {"line.1": {"text": "Ach wie kam ich in dieses schiff?", "tokens": ["Ach", "wie", "kam", "ich", "in", "die\u00b7ses", "schiff", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "VVFIN", "PPER", "APPR", "PDAT", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es grauset mir; ich kan nicht schwimmen:", "tokens": ["Es", "grau\u00b7set", "mir", ";", "ich", "kan", "nicht", "schwim\u00b7men", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "PPER", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Hilff/ hilff/ ein sail/ sto\u00df oder griff;", "tokens": ["Hilff", "/", "hilff", "/", "ein", "sail", "/", "sto\u00df", "o\u00b7der", "griff", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "ART", "NN", "$(", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ach weh! nu hab ich auch das grimmen:", "tokens": ["Ach", "weh", "!", "nu", "hab", "ich", "auch", "das", "grim\u00b7men", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "$.", "ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Alles layder! ist vmbsunst/", "tokens": ["Al\u00b7les", "lay\u00b7der", "!", "ist", "vmbsunst", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "PTKVZ", "$.", "VAFIN", "ADV", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Wir sincken.", "tokens": ["Wir", "sin\u00b7cken", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Was? Ja wol in diser brunst", "tokens": ["Was", "?", "Ja", "wol", "in", "di\u00b7ser", "brunst"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "$.", "PTKANT", "ADV", "APPR", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Brennen wir/ vnd stincken.", "tokens": ["Bren\u00b7nen", "wir", "/", "vnd", "stin\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "$(", "KON", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.35": {"line.1": {"text": "Ho! helffet/ raichet das geschirr;", "tokens": ["Ho", "!", "helf\u00b7fet", "/", "rai\u00b7chet", "das", "ge\u00b7schirr", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "$(", "VVFIN", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Es ist vmbsunst/ es ist geschehen.", "tokens": ["Es", "ist", "vmbsunst", "/", "es", "ist", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$(", "PPER", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich bin gantz nassz; Ich bin gantz dirr/", "tokens": ["Ich", "bin", "gantz", "nassz", ";", "Ich", "bin", "gantz", "dirr", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$.", "PPER", "VAFIN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Stum/ lahm/ kan ich nichts h\u00f6ren/ sehen:", "tokens": ["Stum", "/", "lahm", "/", "kan", "ich", "nichts", "h\u00f6\u00b7ren", "/", "se\u00b7hen", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "PTKVZ", "$(", "VMFIN", "PPER", "PIS", "VVINF", "$(", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ach/ die hagelstein/ plitz/ strahl", "tokens": ["Ach", "/", "die", "ha\u00b7gel\u00b7stein", "/", "plitz", "/", "strahl"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["ITJ", "$(", "ART", "NN", "$(", "NE", "$(", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd dunder", "tokens": ["Vnd", "dun\u00b7der"], "token_info": ["word", "word"], "pos": ["KON", "ADJA"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Kammend auff mich auff ein mahl", "tokens": ["Kam\u00b7mend", "auff", "mich", "auff", "ein", "mahl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PPER", "APPR", "ART", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Schlagen mich hinunder.", "tokens": ["Schla\u00b7gen", "mich", "hin\u00b7un\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.36": {"line.1": {"text": "Wa ist mein fu\u00df/ wa meine stirn/", "tokens": ["Wa", "ist", "mein", "fu\u00df", "/", "wa", "mei\u00b7ne", "stirn", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "PTKVZ", "$(", "XY", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Oh/ mein kopff waltzet auff der erden/", "tokens": ["Oh", "/", "mein", "kopff", "walt\u00b7zet", "auff", "der", "er\u00b7den", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["XY", "$(", "PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Halt/ ich verlier sunst all mein hirn/", "tokens": ["Halt", "/", "ich", "ver\u00b7lier", "sunst", "all", "mein", "hirn", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PPER", "ADJD", "ADV", "PIAT", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was wird doch endlich au\u00df mir werden?", "tokens": ["Was", "wird", "doch", "end\u00b7lich", "au\u00df", "mir", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "APPR", "PPER", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ist kein hilff in diser noht", "tokens": ["Ist", "kein", "hilff", "in", "di\u00b7ser", "noht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "APPR", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Zu haben?", "tokens": ["Zu", "ha\u00b7ben", "?"], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VAINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Got erbarm es. Ich bin tod", "tokens": ["Got", "er\u00b7barm", "es", ".", "Ich", "bin", "tod"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "ADJD", "PPER", "$.", "PPER", "VAFIN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Vnd auch schon begraben.", "tokens": ["Vnd", "auch", "schon", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.37": {"line.1": {"text": "Der volle Narr/ der w\u00fcste Fratz/", "tokens": ["Der", "vol\u00b7le", "Narr", "/", "der", "w\u00fcs\u00b7te", "Fratz", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So wol besoffen als geschossen/", "tokens": ["So", "wol", "be\u00b7sof\u00b7fen", "als", "ge\u00b7schos\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "KOKOM", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Hat als ein stinckend nasser Ratz", "tokens": ["Hat", "als", "ein", "stin\u00b7ckend", "nas\u00b7ser", "Ratz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "KOKOM", "ART", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein abenthewer nu beschlossen:", "tokens": ["Sein", "a\u00b7ben\u00b7the\u00b7wer", "nu", "be\u00b7schlos\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Vnd zu gedechtnus seiner that", "tokens": ["Vnd", "zu", "ge\u00b7decht\u00b7nus", "sei\u00b7ner", "that"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "PPOSAT", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Soll Er hie seine Grabschrifft sehen/", "tokens": ["Soll", "Er", "hie", "sei\u00b7ne", "Grab\u00b7schrifft", "se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Wan von dem Rausch der grob Vnflat", "tokens": ["Wan", "von", "dem", "Rausch", "der", "grob", "Vn\u00b7flat"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.8": {"text": "Soll wider-wachend aufferstehen:", "tokens": ["Soll", "wi\u00b7der\u00b7wa\u00b7chend", "auf\u00b7fer\u00b7ste\u00b7hen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Fratz liget vnder diser banck/", "tokens": ["Fratz", "li\u00b7get", "vn\u00b7der", "di\u00b7ser", "banck", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PDAT", "NN", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "An leib vnd seel sehr w\u00fcst besudlet/", "tokens": ["An", "leib", "vnd", "seel", "sehr", "w\u00fcst", "be\u00b7sud\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "VVIMP", "ADV", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der mancherlay gedranck/ gestanck", "tokens": ["Der", "man\u00b7cher\u00b7lay", "ge\u00b7dranck", "/", "ge\u00b7stanck"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "PIAT", "NN", "$(", "XY"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd sprach vermischet vnd verhudlet.", "tokens": ["Vnd", "sprach", "ver\u00b7mi\u00b7schet", "vnd", "ver\u00b7hud\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ach Leser/ w\u00fcnsch/ da\u00df jhm/ dir/ mir/", "tokens": ["Ach", "Le\u00b7ser", "/", "w\u00fcnsch", "/", "da\u00df", "jhm", "/", "dir", "/", "mir", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "NN", "$(", "ADJD", "$(", "KOUS", "PPER", "$(", "PPER", "$(", "PPER", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Got das gedeyhen wolle geben/", "tokens": ["Got", "das", "ge\u00b7dey\u00b7hen", "wol\u00b7le", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "VMFIN", "VVINF", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.7": {"text": "Da\u00df vnser jeder nach gebihr", "tokens": ["Da\u00df", "vn\u00b7ser", "je\u00b7der", "nach", "ge\u00b7bihr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PIS", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "M\u00f6g besser reden/ drincken/ leben!", "tokens": ["M\u00f6g", "bes\u00b7ser", "re\u00b7den", "/", "drin\u00b7cken", "/", "le\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VMFIN", "ADJD", "VVINF", "$(", "VVINF", "$(", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}