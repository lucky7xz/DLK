{"textgrid.poem.42873": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Immer wieder Fasching", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn der Fasching kommt, wird viel verboten.", "tokens": ["Wenn", "der", "Fa\u00b7sching", "kommt", ",", "wird", "viel", "ver\u00b7bo\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Aber manches wird auch andrerseits erlaubt.", "tokens": ["A\u00b7ber", "man\u00b7ches", "wird", "auch", "an\u00b7drer\u00b7seits", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "Dann wird nicht nur Dienstboten,", "tokens": ["Dann", "wird", "nicht", "nur", "Dienst\u00b7bo\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "ADV", "NN", "$,"], "meter": "-+-+++-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Nein, auch F\u00fcrstenh\u00e4usern enstammten", "tokens": ["Nein", ",", "auch", "F\u00fcrs\u00b7ten\u00b7h\u00e4u\u00b7sern", "en\u00b7stamm\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "ADV", "NN", "VVFIN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Damen oder Frauen von Beamten", "tokens": ["Da\u00b7men", "o\u00b7der", "Frau\u00b7en", "von", "Be\u00b7am\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "APPR", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Die Unschuld geraubt.", "tokens": ["Die", "Un\u00b7schuld", "ge\u00b7raubt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.2": {"line.1": {"text": "Jeder l\u00e4\u00dft was springen.", "tokens": ["Je\u00b7der", "l\u00e4\u00dft", "was", "sprin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Viel ist los.", "tokens": ["Viel", "ist", "los", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Und vor allen Dingen", "tokens": ["Und", "vor", "al\u00b7len", "Din\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Beine und Popos.", "tokens": ["Bei\u00b7ne", "und", "Po\u00b7pos", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Wenn sich Masken noch einmal verh\u00fcllen", "tokens": ["Wenn", "sich", "Mas\u00b7ken", "noch", "ein\u00b7mal", "ver\u00b7h\u00fcl\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "NN", "ADV", "ADV", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Mit Phantastik, Seide, Samt und T\u00fcllen,", "tokens": ["Mit", "Phan\u00b7tas\u00b7tik", ",", "Sei\u00b7de", ",", "Samt", "und", "T\u00fcl\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Zeigt sich sehr viel Fleisch und sehr viel Scho\u00df.", "tokens": ["Zeigt", "sich", "sehr", "viel", "Fleisch", "und", "sehr", "viel", "Scho\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "PIAT", "NN", "KON", "ADV", "PIAT", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Da\u00df wir, eh' wir heimw\u00e4rts schwanken,", "tokens": ["Da\u00df", "wir", ",", "eh'", "wir", "heim\u00b7w\u00e4rts", "schwan\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Unsern steifen Hut zerkn\u00fcllen", "tokens": ["Un\u00b7sern", "stei\u00b7fen", "Hut", "zer\u00b7kn\u00fcl\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Im Gedanken:", "tokens": ["Im", "Ge\u00b7dan\u00b7ken", ":"], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "H\u00e4tten wir die H\u00e4lfte blo\u00df!", "tokens": ["H\u00e4t\u00b7ten", "wir", "die", "H\u00e4lf\u00b7te", "blo\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Also brechen wir auf!", "tokens": ["Al\u00b7so", "bre\u00b7chen", "wir", "auf", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Ach nein, bleiben wir noch,", "tokens": ["Ach", "nein", ",", "blei\u00b7ben", "wir", "noch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKANT", "$,", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Bis an ein Loch.", "tokens": ["Bis", "an", "ein", "Loch", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Schlie\u00dflich l\u00f6st sich alles doch", "tokens": ["Schlie\u00df\u00b7lich", "l\u00f6st", "sich", "al\u00b7les", "doch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "PIS", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "In Papier auf.", "tokens": ["In", "Pa\u00b7pier", "auf", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Man vertrollt sich l\u00e4rmlich,", "tokens": ["Man", "ver\u00b7trollt", "sich", "l\u00e4rm\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Wendet sich erb\u00e4rmlich,", "tokens": ["Wen\u00b7det", "sich", "er\u00b7b\u00e4rm\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADJD", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Jedermann ein abgesetzter Held.", "tokens": ["Je\u00b7der\u00b7mann", "ein", "ab\u00b7ge\u00b7setz\u00b7ter", "Held", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "Drau\u00dfen Sturm. Es hetzen", "tokens": ["Drau\u00b7\u00dfen", "Sturm", ".", "Es", "het\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "NN", "$.", "PPER", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "\u00dcber D\u00e4cher kalte Wolkenfetzen", "tokens": ["\u00dc\u00b7ber", "D\u00e4\u00b7cher", "kal\u00b7te", "Wol\u00b7ken\u00b7fet\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Unterm Mond. Wir setzen", "tokens": ["Un\u00b7term", "Mond", ".", "Wir", "set\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$.", "PPER", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Uns ins Auto, fr\u00f6stelnd vor dem letzten Geld.", "tokens": ["Uns", "ins", "Au\u00b7to", ",", "fr\u00f6s\u00b7telnd", "vor", "dem", "letz\u00b7ten", "Geld", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPRART", "NN", "$,", "ADJD", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.7": {"line.1": {"text": "Wenn der Fasching kommt, wird viel verboten.", "tokens": ["Wenn", "der", "Fa\u00b7sching", "kommt", ",", "wird", "viel", "ver\u00b7bo\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Aber manches wird auch andrerseits erlaubt.", "tokens": ["A\u00b7ber", "man\u00b7ches", "wird", "auch", "an\u00b7drer\u00b7seits", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "Dann wird nicht nur Dienstboten,", "tokens": ["Dann", "wird", "nicht", "nur", "Dienst\u00b7bo\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "ADV", "NN", "$,"], "meter": "-+-+++-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Nein, auch F\u00fcrstenh\u00e4usern enstammten", "tokens": ["Nein", ",", "auch", "F\u00fcrs\u00b7ten\u00b7h\u00e4u\u00b7sern", "en\u00b7stamm\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "ADV", "NN", "VVFIN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Damen oder Frauen von Beamten", "tokens": ["Da\u00b7men", "o\u00b7der", "Frau\u00b7en", "von", "Be\u00b7am\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "APPR", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Die Unschuld geraubt.", "tokens": ["Die", "Un\u00b7schuld", "ge\u00b7raubt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.8": {"line.1": {"text": "Jeder l\u00e4\u00dft was springen.", "tokens": ["Je\u00b7der", "l\u00e4\u00dft", "was", "sprin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Viel ist los.", "tokens": ["Viel", "ist", "los", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Und vor allen Dingen", "tokens": ["Und", "vor", "al\u00b7len", "Din\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Beine und Popos.", "tokens": ["Bei\u00b7ne", "und", "Po\u00b7pos", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Wenn sich Masken noch einmal verh\u00fcllen", "tokens": ["Wenn", "sich", "Mas\u00b7ken", "noch", "ein\u00b7mal", "ver\u00b7h\u00fcl\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "NN", "ADV", "ADV", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Mit Phantastik, Seide, Samt und T\u00fcllen,", "tokens": ["Mit", "Phan\u00b7tas\u00b7tik", ",", "Sei\u00b7de", ",", "Samt", "und", "T\u00fcl\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Zeigt sich sehr viel Fleisch und sehr viel Scho\u00df.", "tokens": ["Zeigt", "sich", "sehr", "viel", "Fleisch", "und", "sehr", "viel", "Scho\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "PIAT", "NN", "KON", "ADV", "PIAT", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Da\u00df wir, eh' wir heimw\u00e4rts schwanken,", "tokens": ["Da\u00df", "wir", ",", "eh'", "wir", "heim\u00b7w\u00e4rts", "schwan\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "KOUS", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Unsern steifen Hut zerkn\u00fcllen", "tokens": ["Un\u00b7sern", "stei\u00b7fen", "Hut", "zer\u00b7kn\u00fcl\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Im Gedanken:", "tokens": ["Im", "Ge\u00b7dan\u00b7ken", ":"], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "H\u00e4tten wir die H\u00e4lfte blo\u00df!", "tokens": ["H\u00e4t\u00b7ten", "wir", "die", "H\u00e4lf\u00b7te", "blo\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Also brechen wir auf!", "tokens": ["Al\u00b7so", "bre\u00b7chen", "wir", "auf", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Ach nein, bleiben wir noch,", "tokens": ["Ach", "nein", ",", "blei\u00b7ben", "wir", "noch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKANT", "$,", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Bis an ein Loch.", "tokens": ["Bis", "an", "ein", "Loch", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Schlie\u00dflich l\u00f6st sich alles doch", "tokens": ["Schlie\u00df\u00b7lich", "l\u00f6st", "sich", "al\u00b7les", "doch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "PIS", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "In Papier auf.", "tokens": ["In", "Pa\u00b7pier", "auf", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Man vertrollt sich l\u00e4rmlich,", "tokens": ["Man", "ver\u00b7trollt", "sich", "l\u00e4rm\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Wendet sich erb\u00e4rmlich,", "tokens": ["Wen\u00b7det", "sich", "er\u00b7b\u00e4rm\u00b7lich", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADJD", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Jedermann ein abgesetzter Held.", "tokens": ["Je\u00b7der\u00b7mann", "ein", "ab\u00b7ge\u00b7setz\u00b7ter", "Held", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.12": {"line.1": {"text": "Drau\u00dfen Sturm. Es hetzen", "tokens": ["Drau\u00b7\u00dfen", "Sturm", ".", "Es", "het\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "NN", "$.", "PPER", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "\u00dcber D\u00e4cher kalte Wolkenfetzen", "tokens": ["\u00dc\u00b7ber", "D\u00e4\u00b7cher", "kal\u00b7te", "Wol\u00b7ken\u00b7fet\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Unterm Mond. Wir setzen", "tokens": ["Un\u00b7term", "Mond", ".", "Wir", "set\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "$.", "PPER", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Uns ins Auto, fr\u00f6stelnd vor dem letzten Geld.", "tokens": ["Uns", "ins", "Au\u00b7to", ",", "fr\u00f6s\u00b7telnd", "vor", "dem", "letz\u00b7ten", "Geld", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPRART", "NN", "$,", "ADJD", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}}}}