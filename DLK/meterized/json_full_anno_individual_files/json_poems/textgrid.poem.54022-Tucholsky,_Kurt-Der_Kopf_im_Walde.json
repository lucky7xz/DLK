{"textgrid.poem.54022": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Der Kopf im Walde", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hinter Buckow, etwas westlich vom Alten See,", "tokens": ["Hin\u00b7ter", "Buc\u00b7kow", ",", "et\u00b7was", "west\u00b7lich", "vom", "Al\u00b7ten", "See", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "ADV", "ADJD", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "liege ich, drei\u00dfig Schritte von der Chaussee.", "tokens": ["lie\u00b7ge", "ich", ",", "drei\u00b7\u00dfig", "Schrit\u00b7te", "von", "der", "Chaus\u00b7see", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "CARD", "NN", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Meine Kleider sind schon ganz verfault und welk wie Zunder.", "tokens": ["Mei\u00b7ne", "Klei\u00b7der", "sind", "schon", "ganz", "ver\u00b7fault", "und", "welk", "wie", "Zun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADV", "VVPP", "KON", "ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.4": {"text": "Bei dem hiesigen Boden ist das kein Wunder.", "tokens": ["Bei", "dem", "hie\u00b7si\u00b7gen", "Bo\u00b7den", "ist", "das", "kein", "Wun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VAFIN", "ART", "PIAT", "NN", "$."], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Hier ists moorig.", "tokens": ["Hier", "ists", "moo\u00b7rig", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.6": {"text": "Ich kenne das recht gut.", "tokens": ["Ich", "ken\u00b7ne", "das", "recht", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Ich war doch hier Freiwilliger . . . ich hatte einen S\u00fcdwestafrikaner-Hut,", "tokens": ["Ich", "war", "doch", "hier", "Frei\u00b7wil\u00b7li\u00b7ger", ".", ".", ".", "ich", "hat\u00b7te", "ei\u00b7nen", "S\u00fcd\u00b7west\u00b7afri\u00b7ka\u00b7ner\u00b7Hut", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "NN", "$.", "$.", "$.", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.8": {"text": "und wir hatten Abzeichen und waren national.", "tokens": ["und", "wir", "hat\u00b7ten", "Ab\u00b7zei\u00b7chen", "und", "wa\u00b7ren", "na\u00b7ti\u00b7o\u00b7nal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "KON", "VAFIN", "ADJD", "$."], "meter": "--+-+---+-+--+", "measure": "iambic.penta.chol"}, "line.9": {"text": "Wie kam das doch so auf einmal?", "tokens": ["Wie", "kam", "das", "doch", "so", "auf", "ein\u00b7mal", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PDS", "ADV", "ADV", "APPR", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ja, der L\u00fcbecke hatte aufgebracht, da\u00df ich ein Spitzel w\u00e4re.", "tokens": ["Ja", ",", "der", "L\u00fc\u00b7be\u00b7cke", "hat\u00b7te", "auf\u00b7ge\u00b7bracht", ",", "da\u00df", "ich", "ein", "Spit\u00b7zel", "w\u00e4\u00b7re", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ART", "NN", "VAFIN", "VVPP", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN", "$."], "meter": "+-+--+-+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.2": {"text": "Das ging gegen meine Ehre,", "tokens": ["Das", "ging", "ge\u00b7gen", "mei\u00b7ne", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "und das war von ihm eine gro\u00dfe Gemeinheit.", "tokens": ["und", "das", "war", "von", "ihm", "ei\u00b7ne", "gro\u00b7\u00dfe", "Ge\u00b7mein\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Er war blo\u00df eifers\u00fcchtig auf meine Reinheit.", "tokens": ["Er", "war", "blo\u00df", "ei\u00b7fer\u00b7s\u00fcch\u00b7tig", "auf", "mei\u00b7ne", "Rein\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Denn er machte immer was mit V\u00f6lckner hinter der Scheune.", "tokens": ["Denn", "er", "mach\u00b7te", "im\u00b7mer", "was", "mit", "V\u00f6l\u00b7ck\u00b7ner", "hin\u00b7ter", "der", "Scheu\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PRELS", "APPR", "NN", "APPR", "ART", "NN", "$."], "meter": "--+-+-+-+--+--+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Und eines Sommerabends, so gegen halb neune,", "tokens": ["Und", "ei\u00b7nes", "Som\u00b7mer\u00b7a\u00b7bends", ",", "so", "ge\u00b7gen", "halb", "neu\u00b7ne", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "ADV", "APPR", "ADJD", "ADJA", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "da fa\u00dfte er mich an und wollte mit mir auch einmal.", "tokens": ["da", "fa\u00df\u00b7te", "er", "mich", "an", "und", "woll\u00b7te", "mit", "mir", "auch", "ein\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "PTKVZ", "KON", "VMFIN", "APPR", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.8": {"text": "Aber ich sagte: \u00bbIch melde es dem Korporal \u2013!\u00ab", "tokens": ["A\u00b7ber", "ich", "sag\u00b7te", ":", "\u00bb", "Ich", "mel\u00b7de", "es", "dem", "Kor\u00b7po\u00b7ral", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PPER", "ART", "NN", "$(", "$.", "$("], "meter": "+--+--+-+-+-+", "measure": "dactylic.di.plus"}, "line.9": {"text": "Denn seit zwei Monaten war ich anst\u00e4ndig geworden.", "tokens": ["Denn", "seit", "zwei", "Mo\u00b7na\u00b7ten", "war", "ich", "an\u00b7st\u00e4n\u00b7dig", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "NN", "VAFIN", "PPER", "ADJD", "VAPP", "$."], "meter": "-+--+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "Ich war fast der einzige im ganzen Orden . . .", "tokens": ["Ich", "war", "fast", "der", "ein\u00b7zi\u00b7ge", "im", "gan\u00b7zen", "Or\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "APPRART", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+--+---+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Mir war gleich so komisch . . .", "tokens": ["Mir", "war", "gleich", "so", "ko\u00b7misch", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJD", "$.", "$.", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.12": {"text": "Da! \u2013 Wie sie mich wieder umkreisen:", "tokens": ["Da", "!", "\u2013", "Wie", "sie", "mich", "wie\u00b7der", "um\u00b7krei\u00b7sen", ":"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "$(", "PWAV", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.13": {"text": "die Ameisen! Die Ameisen!", "tokens": ["die", "A\u00b7mei\u00b7sen", "!", "Die", "A\u00b7mei\u00b7sen", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Mir war gleich so komisch . . . Denn L\u00fcbecke wu\u00dfte das von Bern . . .", "tokens": ["Mir", "war", "gleich", "so", "ko\u00b7misch", ".", ".", ".", "Denn", "L\u00fc\u00b7be\u00b7cke", "wu\u00df\u00b7te", "das", "von", "Bern", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJD", "$.", "$.", "$.", "KON", "NN", "VVFIN", "ART", "APPR", "NE", "$.", "$.", "$."], "meter": "-+--+--+--+-+-+", "measure": "amphibrach.tetra.plus"}, "line.2": {"text": "(der hat damals bei Rathenau mitgemacht \u2013 mit Fischer und Kern),", "tokens": ["(", "der", "hat", "da\u00b7mals", "bei", "Ra\u00b7the\u00b7nau", "mit\u00b7ge\u00b7macht", "\u2013", "mit", "Fi\u00b7scher", "und", "Kern", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "VAFIN", "ADV", "APPR", "NE", "VVPP", "$(", "APPR", "NN", "KON", "NN", "$(", "$,"], "meter": "-+---+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "und L\u00fcbecke war furchtbar m\u00e4chtig in unserm Bund.", "tokens": ["und", "L\u00fc\u00b7be\u00b7cke", "war", "furcht\u00b7bar", "m\u00e4ch\u00b7tig", "in", "un\u00b7serm", "Bund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+---+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und was er mal gesagt hatte, das tat er auch, und", "tokens": ["Und", "was", "er", "mal", "ge\u00b7sagt", "hat\u00b7te", ",", "das", "tat", "er", "auch", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "PWS", "PPER", "ADV", "VVPP", "VAFIN", "$,", "PDS", "VVFIN", "PPER", "ADV", "$,", "KON"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "da habe ich beim n\u00e4chsten Appell gefehlt.", "tokens": ["da", "ha\u00b7be", "ich", "beim", "n\u00e4chs\u00b7ten", "Ap\u00b7pell", "ge\u00b7fehlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Und da hat der L\u00fcbecke sicher was Gelegenes erz\u00e4hlt.", "tokens": ["Und", "da", "hat", "der", "L\u00fc\u00b7be\u00b7cke", "si\u00b7cher", "was", "Ge\u00b7le\u00b7ge\u00b7nes", "er\u00b7z\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "ADJD", "PWS", "NN", "VVFIN", "$."], "meter": "-+--+--+-++-+--+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Und Br\u00f6der, unser Kompanief\u00fchrer, war leider nicht hier \u2013", "tokens": ["Und", "Br\u00f6\u00b7der", ",", "un\u00b7ser", "Kom\u00b7pa\u00b7nie\u00b7f\u00fch\u00b7rer", ",", "war", "lei\u00b7der", "nicht", "hier", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PPOSAT", "NN", "$,", "VAFIN", "ADV", "PTKNEG", "ADV", "$("], "meter": "-+-+-+-++--+--+", "measure": "iambelegus"}, "line.8": {"text": "der war n\u00e4mlich fr\u00fcher Offizier \u2013", "tokens": ["der", "war", "n\u00e4m\u00b7lich", "fr\u00fc\u00b7her", "Of\u00b7fi\u00b7zier", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VAFIN", "ADV", "ADJD", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.9": {"text": "der war nicht da. Das war sehr schade.", "tokens": ["der", "war", "nicht", "da", ".", "Das", "war", "sehr", "scha\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "VAFIN", "PTKNEG", "ADV", "$.", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Aber der war in Halle auf Parade.", "tokens": ["A\u00b7ber", "der", "war", "in", "Hal\u00b7le", "auf", "Pa\u00b7ra\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "APPR", "NE", "APPR", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.4": {"line.1": {"text": "Und da haben sie eine \u00dcbung angesetzt im Wald,", "tokens": ["Und", "da", "ha\u00b7ben", "sie", "ei\u00b7ne", "\u00dc\u00b7bung", "an\u00b7ge\u00b7setzt", "im", "Wald", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "APPRART", "NN", "$,"], "meter": "--+--+-+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "damit es nicht auff\u00e4llt, wenn eine Patrone knallt.", "tokens": ["da\u00b7mit", "es", "nicht", "auf\u00b7f\u00e4llt", ",", "wenn", "ei\u00b7ne", "Pat\u00b7ro\u00b7ne", "knallt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVPP", "$,", "KOUS", "ART", "NN", "VVFIN", "$."], "meter": "-+---+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und da waren auf einmal vier da.", "tokens": ["Und", "da", "wa\u00b7ren", "auf", "ein\u00b7mal", "vier", "da", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "APPR", "ADV", "CARD", "ADV", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "L\u00fcbecke nicht. Und sie haben kein Wort gesagt. Und sie kamen ganz nah", "tokens": ["L\u00fc\u00b7be\u00b7cke", "nicht", ".", "Und", "sie", "ha\u00b7ben", "kein", "Wort", "ge\u00b7sagt", ".", "Und", "sie", "ka\u00b7men", "ganz", "nah"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "PTKNEG", "$.", "KON", "PPER", "VAFIN", "PIAT", "NN", "VVPP", "$.", "KON", "PPER", "VVFIN", "ADV", "ADJD"], "meter": "+--+--+--+-+--+--+", "measure": "dactylic.tri.plus"}, "line.5": {"text": "auf mich zu und sahen mich blo\u00df an", "tokens": ["auf", "mich", "zu", "und", "sa\u00b7hen", "mich", "blo\u00df", "an"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "PTKVZ", "KON", "VVFIN", "PPER", "ADV", "APPR"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "und sagten: \u00bbDu bist kein deutscher Mann \u2013!", "tokens": ["und", "sag\u00b7ten", ":", "\u00bb", "Du", "bist", "kein", "deut\u00b7scher", "Mann", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PIAT", "ADJA", "NN", "$(", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Du bist ein Verr\u00e4ter \u2013!\u00ab Und dann kam ein Schlag.", "tokens": ["Du", "bist", "ein", "Ver\u00b7r\u00e4\u00b7ter", "\u2013", "!", "\u00ab", "Und", "dann", "kam", "ein", "Schlag", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "$.", "$(", "KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.8": {"text": "Und einer rief: \u00bbDas wird dein letzter Tag,", "tokens": ["Und", "ei\u00b7ner", "rief", ":", "\u00bb", "Das", "wird", "dein", "letz\u00b7ter", "Tag", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$.", "$(", "PDS", "VAFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "du Hund!\u00ab Und dann waren sie ganz stumm.", "tokens": ["du", "Hund", "!", "\u00ab", "Und", "dann", "wa\u00b7ren", "sie", "ganz", "stumm", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$.", "$(", "KON", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Und ich fiel hin, und sie trampelten noch auf mir herum.", "tokens": ["Und", "ich", "fiel", "hin", ",", "und", "sie", "tram\u00b7pel\u00b7ten", "noch", "auf", "mir", "he\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "$,", "KON", "PPER", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+---+-++--+", "measure": "iambic.hexa.chol"}, "line.11": {"text": "Und dann wei\u00df ich nichts mehr. Doch. Einer hat gerufen: \u00bbWas kann da sein?", "tokens": ["Und", "dann", "wei\u00df", "ich", "nichts", "mehr", ".", "Doch", ".", "Ei\u00b7ner", "hat", "ge\u00b7ru\u00b7fen", ":", "\u00bb", "Was", "kann", "da", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PIS", "ADV", "$.", "KON", "$.", "PIS", "VAFIN", "VVPP", "$.", "$(", "PWS", "VMFIN", "ADV", "VAINF", "$."], "meter": "-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.12": {"text": "Wir fallen ja doch nicht rein!\u00ab", "tokens": ["Wir", "fal\u00b7len", "ja", "doch", "nicht", "rein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "ADJD", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Herrgott, ich bin mein ganzes Leben lang fromm gewesen.", "tokens": ["Herr\u00b7gott", ",", "ich", "bin", "mein", "gan\u00b7zes", "Le\u00b7ben", "lang", "fromm", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "ADJD", "ADJD", "VAPP", "$."], "meter": "+--+-+-+-+--+-", "measure": "hexameter"}, "line.2": {"text": "La\u00df mich doch hier nicht unger\u00e4cht verwesen!", "tokens": ["La\u00df", "mich", "doch", "hier", "nicht", "un\u00b7ge\u00b7r\u00e4cht", "ver\u00b7we\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "PTKNEG", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "La\u00df es doch herauskommen! Sicher steckt der L\u00fcbecke dahinter.", "tokens": ["La\u00df", "es", "doch", "her\u00b7aus\u00b7kom\u00b7men", "!", "Si\u00b7cher", "steckt", "der", "L\u00fc\u00b7be\u00b7cke", "da\u00b7hin\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "VVINF", "$.", "ADV", "VVFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.4": {"text": "Jetzt war schon einmal Sommer, und nun kommt Winter.", "tokens": ["Jetzt", "war", "schon", "ein\u00b7mal", "Som\u00b7mer", ",", "und", "nun", "kommt", "Win\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "NN", "$,", "KON", "ADV", "VVFIN", "NN", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Meine Mutter wei\u00df nicht, wo ich geblieben bin . . .", "tokens": ["Mei\u00b7ne", "Mut\u00b7ter", "wei\u00df", "nicht", ",", "wo", "ich", "ge\u00b7blie\u00b7ben", "bin", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "VVPP", "VAFIN", "$.", "$.", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.6": {"text": "Sie lassen mich sicher suchen, in Amerika oder Tientsin.", "tokens": ["Sie", "las\u00b7sen", "mich", "si\u00b7cher", "su\u00b7chen", ",", "in", "A\u00b7me\u00b7ri\u00b7ka", "o\u00b7der", "Tient\u00b7sin", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVINF", "$,", "APPR", "NE", "KON", "NN", "$."], "meter": "-+--+-+-+-+-+-+-+", "measure": "iambic.octa.plus.relaxed"}}, "stanza.6": {"line.1": {"text": "Lieber Gott, dir kann ichs sagen:", "tokens": ["Lie\u00b7ber", "Gott", ",", "dir", "kann", "ichs", "sa\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wos zu sp\u00e4t ist, wei\u00df ichs jetzt:", "tokens": ["Wos", "zu", "sp\u00e4t", "ist", ",", "wei\u00df", "ichs", "jetzt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKA", "ADJD", "VAFIN", "$,", "VVFIN", "PIS", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Siegreich wolln wir Frankreich schlagen \u2013", "tokens": ["Sieg\u00b7reich", "wolln", "wir", "Fran\u00b7kreich", "schla\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "NE", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "alle haben so gehetzt!", "tokens": ["al\u00b7le", "ha\u00b7ben", "so", "ge\u00b7hetzt", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Das Hakenkreuz, Gott, ich umkrall es!", "tokens": ["Das", "Ha\u00b7ken\u00b7kreuz", ",", "Gott", ",", "ich", "um\u00b7krall", "es", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$,", "PPER", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Lieber Gott, mein Rufen gellt:", "tokens": ["Lie\u00b7ber", "Gott", ",", "mein", "Ru\u00b7fen", "gellt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Deutschland, Deutschland \u00fcber alles!", "tokens": ["Deutschland", ",", "Deutschland", "\u00fc\u00b7ber", "al\u00b7les", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "APPR", "PIS", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "\u00dcber alles in der Welt \u2013!", "tokens": ["\u00dc\u00b7ber", "al\u00b7les", "in", "der", "Welt", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIS", "APPR", "ART", "NN", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Hinter Buckow, etwas westlich vom Alten See,", "tokens": ["Hin\u00b7ter", "Buc\u00b7kow", ",", "et\u00b7was", "west\u00b7lich", "vom", "Al\u00b7ten", "See", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "ADV", "ADJD", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "liege ich, drei\u00dfig Schritte von der Chaussee.", "tokens": ["lie\u00b7ge", "ich", ",", "drei\u00b7\u00dfig", "Schrit\u00b7te", "von", "der", "Chaus\u00b7see", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "CARD", "NN", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Meine Kleider sind schon ganz verfault und welk wie Zunder.", "tokens": ["Mei\u00b7ne", "Klei\u00b7der", "sind", "schon", "ganz", "ver\u00b7fault", "und", "welk", "wie", "Zun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADV", "VVPP", "KON", "ADJD", "KOKOM", "NN", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.4": {"text": "Bei dem hiesigen Boden ist das kein Wunder.", "tokens": ["Bei", "dem", "hie\u00b7si\u00b7gen", "Bo\u00b7den", "ist", "das", "kein", "Wun\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VAFIN", "ART", "PIAT", "NN", "$."], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Hier ists moorig.", "tokens": ["Hier", "ists", "moo\u00b7rig", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.6": {"text": "Ich kenne das recht gut.", "tokens": ["Ich", "ken\u00b7ne", "das", "recht", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Ich war doch hier Freiwilliger . . . ich hatte einen S\u00fcdwestafrikaner-Hut,", "tokens": ["Ich", "war", "doch", "hier", "Frei\u00b7wil\u00b7li\u00b7ger", ".", ".", ".", "ich", "hat\u00b7te", "ei\u00b7nen", "S\u00fcd\u00b7west\u00b7afri\u00b7ka\u00b7ner\u00b7Hut", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "NN", "$.", "$.", "$.", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.8": {"text": "und wir hatten Abzeichen und waren national.", "tokens": ["und", "wir", "hat\u00b7ten", "Ab\u00b7zei\u00b7chen", "und", "wa\u00b7ren", "na\u00b7ti\u00b7o\u00b7nal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "KON", "VAFIN", "ADJD", "$."], "meter": "--+-+---+-+--+", "measure": "iambic.penta.chol"}, "line.9": {"text": "Wie kam das doch so auf einmal?", "tokens": ["Wie", "kam", "das", "doch", "so", "auf", "ein\u00b7mal", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PDS", "ADV", "ADV", "APPR", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ja, der L\u00fcbecke hatte aufgebracht, da\u00df ich ein Spitzel w\u00e4re.", "tokens": ["Ja", ",", "der", "L\u00fc\u00b7be\u00b7cke", "hat\u00b7te", "auf\u00b7ge\u00b7bracht", ",", "da\u00df", "ich", "ein", "Spit\u00b7zel", "w\u00e4\u00b7re", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ART", "NN", "VAFIN", "VVPP", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN", "$."], "meter": "+-+--+-+-+-+-+-+-", "measure": "trochaic.octa.plus.relaxed"}, "line.2": {"text": "Das ging gegen meine Ehre,", "tokens": ["Das", "ging", "ge\u00b7gen", "mei\u00b7ne", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "und das war von ihm eine gro\u00dfe Gemeinheit.", "tokens": ["und", "das", "war", "von", "ihm", "ei\u00b7ne", "gro\u00b7\u00dfe", "Ge\u00b7mein\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Er war blo\u00df eifers\u00fcchtig auf meine Reinheit.", "tokens": ["Er", "war", "blo\u00df", "ei\u00b7fer\u00b7s\u00fcch\u00b7tig", "auf", "mei\u00b7ne", "Rein\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Denn er machte immer was mit V\u00f6lckner hinter der Scheune.", "tokens": ["Denn", "er", "mach\u00b7te", "im\u00b7mer", "was", "mit", "V\u00f6l\u00b7ck\u00b7ner", "hin\u00b7ter", "der", "Scheu\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PRELS", "APPR", "NN", "APPR", "ART", "NN", "$."], "meter": "--+-+-+-+--+--+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Und eines Sommerabends, so gegen halb neune,", "tokens": ["Und", "ei\u00b7nes", "Som\u00b7mer\u00b7a\u00b7bends", ",", "so", "ge\u00b7gen", "halb", "neu\u00b7ne", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "ADV", "APPR", "ADJD", "ADJA", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "da fa\u00dfte er mich an und wollte mit mir auch einmal.", "tokens": ["da", "fa\u00df\u00b7te", "er", "mich", "an", "und", "woll\u00b7te", "mit", "mir", "auch", "ein\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "PTKVZ", "KON", "VMFIN", "APPR", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.8": {"text": "Aber ich sagte: \u00bbIch melde es dem Korporal \u2013!\u00ab", "tokens": ["A\u00b7ber", "ich", "sag\u00b7te", ":", "\u00bb", "Ich", "mel\u00b7de", "es", "dem", "Kor\u00b7po\u00b7ral", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PPER", "ART", "NN", "$(", "$.", "$("], "meter": "+--+--+-+-+-+", "measure": "dactylic.di.plus"}, "line.9": {"text": "Denn seit zwei Monaten war ich anst\u00e4ndig geworden.", "tokens": ["Denn", "seit", "zwei", "Mo\u00b7na\u00b7ten", "war", "ich", "an\u00b7st\u00e4n\u00b7dig", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "NN", "VAFIN", "PPER", "ADJD", "VAPP", "$."], "meter": "-+--+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "Ich war fast der einzige im ganzen Orden . . .", "tokens": ["Ich", "war", "fast", "der", "ein\u00b7zi\u00b7ge", "im", "gan\u00b7zen", "Or\u00b7den", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "APPRART", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+--+---+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Mir war gleich so komisch . . .", "tokens": ["Mir", "war", "gleich", "so", "ko\u00b7misch", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJD", "$.", "$.", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.12": {"text": "Da! \u2013 Wie sie mich wieder umkreisen:", "tokens": ["Da", "!", "\u2013", "Wie", "sie", "mich", "wie\u00b7der", "um\u00b7krei\u00b7sen", ":"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "$(", "PWAV", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.13": {"text": "die Ameisen! Die Ameisen!", "tokens": ["die", "A\u00b7mei\u00b7sen", "!", "Die", "A\u00b7mei\u00b7sen", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Mir war gleich so komisch . . . Denn L\u00fcbecke wu\u00dfte das von Bern . . .", "tokens": ["Mir", "war", "gleich", "so", "ko\u00b7misch", ".", ".", ".", "Denn", "L\u00fc\u00b7be\u00b7cke", "wu\u00df\u00b7te", "das", "von", "Bern", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJD", "$.", "$.", "$.", "KON", "NN", "VVFIN", "ART", "APPR", "NE", "$.", "$.", "$."], "meter": "-+--+--+--+-+-+", "measure": "amphibrach.tetra.plus"}, "line.2": {"text": "(der hat damals bei Rathenau mitgemacht \u2013 mit Fischer und Kern),", "tokens": ["(", "der", "hat", "da\u00b7mals", "bei", "Ra\u00b7the\u00b7nau", "mit\u00b7ge\u00b7macht", "\u2013", "mit", "Fi\u00b7scher", "und", "Kern", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "VAFIN", "ADV", "APPR", "NE", "VVPP", "$(", "APPR", "NN", "KON", "NN", "$(", "$,"], "meter": "-+---+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "und L\u00fcbecke war furchtbar m\u00e4chtig in unserm Bund.", "tokens": ["und", "L\u00fc\u00b7be\u00b7cke", "war", "furcht\u00b7bar", "m\u00e4ch\u00b7tig", "in", "un\u00b7serm", "Bund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADJD", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+---+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Und was er mal gesagt hatte, das tat er auch, und", "tokens": ["Und", "was", "er", "mal", "ge\u00b7sagt", "hat\u00b7te", ",", "das", "tat", "er", "auch", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "PWS", "PPER", "ADV", "VVPP", "VAFIN", "$,", "PDS", "VVFIN", "PPER", "ADV", "$,", "KON"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "da habe ich beim n\u00e4chsten Appell gefehlt.", "tokens": ["da", "ha\u00b7be", "ich", "beim", "n\u00e4chs\u00b7ten", "Ap\u00b7pell", "ge\u00b7fehlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Und da hat der L\u00fcbecke sicher was Gelegenes erz\u00e4hlt.", "tokens": ["Und", "da", "hat", "der", "L\u00fc\u00b7be\u00b7cke", "si\u00b7cher", "was", "Ge\u00b7le\u00b7ge\u00b7nes", "er\u00b7z\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "ADJD", "PWS", "NN", "VVFIN", "$."], "meter": "-+--+--+-++-+--+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Und Br\u00f6der, unser Kompanief\u00fchrer, war leider nicht hier \u2013", "tokens": ["Und", "Br\u00f6\u00b7der", ",", "un\u00b7ser", "Kom\u00b7pa\u00b7nie\u00b7f\u00fch\u00b7rer", ",", "war", "lei\u00b7der", "nicht", "hier", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PPOSAT", "NN", "$,", "VAFIN", "ADV", "PTKNEG", "ADV", "$("], "meter": "-+-+-+-++--+--+", "measure": "iambelegus"}, "line.8": {"text": "der war n\u00e4mlich fr\u00fcher Offizier \u2013", "tokens": ["der", "war", "n\u00e4m\u00b7lich", "fr\u00fc\u00b7her", "Of\u00b7fi\u00b7zier", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VAFIN", "ADV", "ADJD", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.9": {"text": "der war nicht da. Das war sehr schade.", "tokens": ["der", "war", "nicht", "da", ".", "Das", "war", "sehr", "scha\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "VAFIN", "PTKNEG", "ADV", "$.", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Aber der war in Halle auf Parade.", "tokens": ["A\u00b7ber", "der", "war", "in", "Hal\u00b7le", "auf", "Pa\u00b7ra\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "APPR", "NE", "APPR", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.10": {"line.1": {"text": "Und da haben sie eine \u00dcbung angesetzt im Wald,", "tokens": ["Und", "da", "ha\u00b7ben", "sie", "ei\u00b7ne", "\u00dc\u00b7bung", "an\u00b7ge\u00b7setzt", "im", "Wald", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "APPRART", "NN", "$,"], "meter": "--+--+-+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "damit es nicht auff\u00e4llt, wenn eine Patrone knallt.", "tokens": ["da\u00b7mit", "es", "nicht", "auf\u00b7f\u00e4llt", ",", "wenn", "ei\u00b7ne", "Pat\u00b7ro\u00b7ne", "knallt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVPP", "$,", "KOUS", "ART", "NN", "VVFIN", "$."], "meter": "-+---+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und da waren auf einmal vier da.", "tokens": ["Und", "da", "wa\u00b7ren", "auf", "ein\u00b7mal", "vier", "da", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "APPR", "ADV", "CARD", "ADV", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "L\u00fcbecke nicht. Und sie haben kein Wort gesagt. Und sie kamen ganz nah", "tokens": ["L\u00fc\u00b7be\u00b7cke", "nicht", ".", "Und", "sie", "ha\u00b7ben", "kein", "Wort", "ge\u00b7sagt", ".", "Und", "sie", "ka\u00b7men", "ganz", "nah"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "PTKNEG", "$.", "KON", "PPER", "VAFIN", "PIAT", "NN", "VVPP", "$.", "KON", "PPER", "VVFIN", "ADV", "ADJD"], "meter": "+--+--+--+-+--+--+", "measure": "dactylic.tri.plus"}, "line.5": {"text": "auf mich zu und sahen mich blo\u00df an", "tokens": ["auf", "mich", "zu", "und", "sa\u00b7hen", "mich", "blo\u00df", "an"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "PTKVZ", "KON", "VVFIN", "PPER", "ADV", "APPR"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "und sagten: \u00bbDu bist kein deutscher Mann \u2013!", "tokens": ["und", "sag\u00b7ten", ":", "\u00bb", "Du", "bist", "kein", "deut\u00b7scher", "Mann", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PPER", "VAFIN", "PIAT", "ADJA", "NN", "$(", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Du bist ein Verr\u00e4ter \u2013!\u00ab Und dann kam ein Schlag.", "tokens": ["Du", "bist", "ein", "Ver\u00b7r\u00e4\u00b7ter", "\u2013", "!", "\u00ab", "Und", "dann", "kam", "ein", "Schlag", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "$.", "$(", "KON", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.8": {"text": "Und einer rief: \u00bbDas wird dein letzter Tag,", "tokens": ["Und", "ei\u00b7ner", "rief", ":", "\u00bb", "Das", "wird", "dein", "letz\u00b7ter", "Tag", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "$.", "$(", "PDS", "VAFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "du Hund!\u00ab Und dann waren sie ganz stumm.", "tokens": ["du", "Hund", "!", "\u00ab", "Und", "dann", "wa\u00b7ren", "sie", "ganz", "stumm", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "$.", "$(", "KON", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Und ich fiel hin, und sie trampelten noch auf mir herum.", "tokens": ["Und", "ich", "fiel", "hin", ",", "und", "sie", "tram\u00b7pel\u00b7ten", "noch", "auf", "mir", "he\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKVZ", "$,", "KON", "PPER", "VVFIN", "ADV", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+---+-++--+", "measure": "iambic.hexa.chol"}, "line.11": {"text": "Und dann wei\u00df ich nichts mehr. Doch. Einer hat gerufen: \u00bbWas kann da sein?", "tokens": ["Und", "dann", "wei\u00df", "ich", "nichts", "mehr", ".", "Doch", ".", "Ei\u00b7ner", "hat", "ge\u00b7ru\u00b7fen", ":", "\u00bb", "Was", "kann", "da", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PIS", "ADV", "$.", "KON", "$.", "PIS", "VAFIN", "VVPP", "$.", "$(", "PWS", "VMFIN", "ADV", "VAINF", "$."], "meter": "-+-+-+-+-+-+--+-+", "measure": "iambic.octa.plus.relaxed"}, "line.12": {"text": "Wir fallen ja doch nicht rein!\u00ab", "tokens": ["Wir", "fal\u00b7len", "ja", "doch", "nicht", "rein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PTKNEG", "ADJD", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.11": {"line.1": {"text": "Herrgott, ich bin mein ganzes Leben lang fromm gewesen.", "tokens": ["Herr\u00b7gott", ",", "ich", "bin", "mein", "gan\u00b7zes", "Le\u00b7ben", "lang", "fromm", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "ADJD", "ADJD", "VAPP", "$."], "meter": "+--+-+-+-+--+-", "measure": "hexameter"}, "line.2": {"text": "La\u00df mich doch hier nicht unger\u00e4cht verwesen!", "tokens": ["La\u00df", "mich", "doch", "hier", "nicht", "un\u00b7ge\u00b7r\u00e4cht", "ver\u00b7we\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "PTKNEG", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "La\u00df es doch herauskommen! Sicher steckt der L\u00fcbecke dahinter.", "tokens": ["La\u00df", "es", "doch", "her\u00b7aus\u00b7kom\u00b7men", "!", "Si\u00b7cher", "steckt", "der", "L\u00fc\u00b7be\u00b7cke", "da\u00b7hin\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "VVINF", "$.", "ADV", "VVFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.4": {"text": "Jetzt war schon einmal Sommer, und nun kommt Winter.", "tokens": ["Jetzt", "war", "schon", "ein\u00b7mal", "Som\u00b7mer", ",", "und", "nun", "kommt", "Win\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "NN", "$,", "KON", "ADV", "VVFIN", "NN", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Meine Mutter wei\u00df nicht, wo ich geblieben bin . . .", "tokens": ["Mei\u00b7ne", "Mut\u00b7ter", "wei\u00df", "nicht", ",", "wo", "ich", "ge\u00b7blie\u00b7ben", "bin", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "VVPP", "VAFIN", "$.", "$.", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.6": {"text": "Sie lassen mich sicher suchen, in Amerika oder Tientsin.", "tokens": ["Sie", "las\u00b7sen", "mich", "si\u00b7cher", "su\u00b7chen", ",", "in", "A\u00b7me\u00b7ri\u00b7ka", "o\u00b7der", "Tient\u00b7sin", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVINF", "$,", "APPR", "NE", "KON", "NN", "$."], "meter": "-+--+-+-+-+-+-+-+", "measure": "iambic.octa.plus.relaxed"}}, "stanza.12": {"line.1": {"text": "Lieber Gott, dir kann ichs sagen:", "tokens": ["Lie\u00b7ber", "Gott", ",", "dir", "kann", "ichs", "sa\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wos zu sp\u00e4t ist, wei\u00df ichs jetzt:", "tokens": ["Wos", "zu", "sp\u00e4t", "ist", ",", "wei\u00df", "ichs", "jetzt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PTKA", "ADJD", "VAFIN", "$,", "VVFIN", "PIS", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Siegreich wolln wir Frankreich schlagen \u2013", "tokens": ["Sieg\u00b7reich", "wolln", "wir", "Fran\u00b7kreich", "schla\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "NE", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "alle haben so gehetzt!", "tokens": ["al\u00b7le", "ha\u00b7ben", "so", "ge\u00b7hetzt", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Das Hakenkreuz, Gott, ich umkrall es!", "tokens": ["Das", "Ha\u00b7ken\u00b7kreuz", ",", "Gott", ",", "ich", "um\u00b7krall", "es", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "$,", "PPER", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Lieber Gott, mein Rufen gellt:", "tokens": ["Lie\u00b7ber", "Gott", ",", "mein", "Ru\u00b7fen", "gellt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Deutschland, Deutschland \u00fcber alles!", "tokens": ["Deutschland", ",", "Deutschland", "\u00fc\u00b7ber", "al\u00b7les", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "APPR", "PIS", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "\u00dcber alles in der Welt \u2013!", "tokens": ["\u00dc\u00b7ber", "al\u00b7les", "in", "der", "Welt", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PIS", "APPR", "ART", "NN", "$(", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}