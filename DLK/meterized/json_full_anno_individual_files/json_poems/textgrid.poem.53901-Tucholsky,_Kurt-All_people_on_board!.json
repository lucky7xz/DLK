{"textgrid.poem.53901": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "All people on board!", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das ist n\u00e4mlich so in Berlin:", "tokens": ["Das", "ist", "n\u00e4m\u00b7lich", "so", "in", "Ber\u00b7lin", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPR", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Einer ist pl\u00f6tzlich f\u00fcr Biographien.", "tokens": ["Ei\u00b7ner", "ist", "pl\u00f6tz\u00b7lich", "f\u00fcr", "Bio\u00b7gra\u00b7phi\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "APPR", "NN", "$."], "meter": "+--+--+-++", "measure": "dactylic.di.plus"}, "line.3": {"text": "Und aus einem Grunde, grad oder krumm,", "tokens": ["Und", "aus", "ei\u00b7nem", "Grun\u00b7de", ",", "grad", "o\u00b7der", "krumm", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,", "ADV", "KON", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "gef\u00e4llt diese Sache dem Publikum.", "tokens": ["ge\u00b7f\u00e4llt", "die\u00b7se", "Sa\u00b7che", "dem", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Das Publikum mag das Neue gern kaufen . . .", "tokens": ["Das", "Pub\u00b7li\u00b7kum", "mag", "das", "Neu\u00b7e", "gern", "kau\u00b7fen", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "ADJA", "ADV", "VVINF", "$.", "$.", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Nun kommen sie aber alle gelaufen!", "tokens": ["Nun", "kom\u00b7men", "sie", "a\u00b7ber", "al\u00b7le", "ge\u00b7lau\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIS", "VVPP", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Jetzt schie\u00dfen, mit und ohne Komfort,", "tokens": ["Jetzt", "schie\u00b7\u00dfen", ",", "mit", "und", "oh\u00b7ne", "Kom\u00b7fort", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "APPR", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "die Biographien aus dem Boden hervor:", "tokens": ["die", "Bio\u00b7gra\u00b7phi\u00b7en", "aus", "dem", "Bo\u00b7den", "her\u00b7vor", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.3": {"line.1": {"text": "Kaiser Gustav der Heizbare; F\u00fcrstenberg;", "tokens": ["Kai\u00b7ser", "Gus\u00b7tav", "der", "Heiz\u00b7ba\u00b7re", ";", "F\u00fcrs\u00b7ten\u00b7berg", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "ART", "NN", "$.", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "der Herzbesitzer von Heidelberg;", "tokens": ["der", "Herz\u00b7be\u00b7sit\u00b7zer", "von", "Hei\u00b7del\u00b7berg", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Frau Neppach, Einstein und Lindberghs Sohn", "tokens": ["Frau", "Nep\u00b7pach", ",", "Ein\u00b7stein", "und", "Lind\u00b7berghs", "Sohn"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "NE", "$,", "NN", "KON", "NE", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "und vom Landgericht III der Justizrat Cohn \u2013", "tokens": ["und", "vom", "Land\u00b7ge\u00b7richt", "I\u00b7iI", "der", "Jus\u00b7tiz\u00b7rat", "Cohn", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "CARD", "ART", "NN", "NE", "$("], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "sie alle bekommen ihre Biographie", "tokens": ["sie", "al\u00b7le", "be\u00b7kom\u00b7men", "ih\u00b7re", "Bio\u00b7gra\u00b7phie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "PIS", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "(mit Bild auf dem Umschlag) \u2013 jetzt oder nie!", "tokens": ["(", "mit", "Bild", "auf", "dem", "Um\u00b7schlag", ")", "\u2013", "jetzt", "o\u00b7der", "nie", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "APPR", "ART", "NN", "$(", "$(", "ADV", "KON", "ADV", "$."], "meter": "-+---+-+-+", "measure": "dactylic.init"}, "line.7": {"text": "Heute so dick wie ein Lexikon,", "tokens": ["Heu\u00b7te", "so", "dick", "wie", "ein", "Le\u00b7xi\u00b7kon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "und morgen spricht kein Mensch mehr davon.", "tokens": ["und", "mor\u00b7gen", "spricht", "kein", "Mensch", "mehr", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIAT", "NN", "ADV", "PAV", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "Denn morgen ist da ein neues Gl\u00fcck:", "tokens": ["Denn", "mor\u00b7gen", "ist", "da", "ein", "neu\u00b7es", "Gl\u00fcck", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "das englische Grusel- und Geisterst\u00fcck.", "tokens": ["das", "eng\u00b7li\u00b7sche", "Gru\u00b7sel", "und", "Geis\u00b7ter\u00b7st\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "TRUNC", "KON", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.5": {"line.1": {"text": "Da kommen aber in hellen Haufen", "tokens": ["Da", "kom\u00b7men", "a\u00b7ber", "in", "hel\u00b7len", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "die Theaterdirektoren gelaufen!", "tokens": ["die", "The\u00b7a\u00b7ter\u00b7di\u00b7rek\u00b7to\u00b7ren", "ge\u00b7lau\u00b7fen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "--+---+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u203adie Gr\u00e4fin auf der Kirchhofswand\u2039,", "tokens": ["\u203a", "die", "Gr\u00e4\u00b7fin", "auf", "der", "Kirch\u00b7hofs\u00b7wand", "\u2039", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "APPR", "ART", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u203asherlock Piel zwischen Lipp und Kelchesrand\u2039,", "tokens": ["\u203a", "sher\u00b7lock", "Piel", "zwi\u00b7schen", "Lipp", "und", "Kel\u00b7ches\u00b7rand", "\u2039", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "NN", "APPR", "NE", "KON", "NN", "$(", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "\u203adas Bidet im Urwald\u2039 \u2013 oder wie das so hei\u00dft,", "tokens": ["\u203a", "das", "Bi\u00b7det", "im", "Ur\u00b7wald", "\u2039", "\u2013", "o\u00b7der", "wie", "das", "so", "hei\u00dft", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "APPRART", "NN", "$(", "$(", "KON", "PWAV", "PDS", "ADV", "VVFIN", "$,"], "meter": "-+--++-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "und pl\u00f6tzlich hat jedes Theater nen Geist.", "tokens": ["und", "pl\u00f6tz\u00b7lich", "hat", "je\u00b7des", "The\u00b7a\u00b7ter", "nen", "Geist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PIAT", "NN", "ADJA", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.7": {"text": "\u00bbdas kenn Se nich? Das haben Sie noch nicht gesehn \u2013?", "tokens": ["\u00bb", "das", "kenn", "Se", "nich", "?", "Das", "ha\u00b7ben", "Sie", "noch", "nicht", "ge\u00b7sehn", "\u2013", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PDS", "VVFIN", "NE", "PTKNEG", "$.", "PDS", "VAFIN", "PPER", "ADV", "PTKNEG", "VVPP", "$(", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Da m\u00fcssen Sie unbedingt hingehn \u2013!\u00ab", "tokens": ["Da", "m\u00fcs\u00b7sen", "Sie", "un\u00b7be\u00b7dingt", "hin\u00b7gehn", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF", "$(", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Und auf einen, ders nicht gesehn hat, spucken . . .", "tokens": ["Und", "auf", "ei\u00b7nen", ",", "ders", "nicht", "ge\u00b7sehn", "hat", ",", "spu\u00b7cken", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["KON", "APPR", "ART", "$,", "PRELS", "PTKNEG", "VVPP", "VAFIN", "$,", "VVINF", "$.", "$.", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Morgen sind die Achseln ganz m\u00fcde vom Zucken:", "tokens": ["Mor\u00b7gen", "sind", "die", "Ach\u00b7seln", "ganz", "m\u00fc\u00b7de", "vom", "Zu\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ADV", "ADJD", "APPRART", "NN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "\u00bbwenn ich schon Geisterst\u00fccke seh \u2013", "tokens": ["\u00bb", "wenn", "ich", "schon", "Geis\u00b7ter\u00b7st\u00fc\u00b7cke", "seh", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "ADV", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Pass\u00e9!\u00ab", "tokens": ["Pass\u00e9", "!", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["NE", "$.", "$("], "meter": "+", "measure": "single.up"}}, "stanza.7": {"line.1": {"text": "Mal Punktroller und mal Negerplatten;", "tokens": ["Mal", "Punk\u00b7trol\u00b7ler", "und", "mal", "Ne\u00b7ger\u00b7plat\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "KON", "ADV", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "mal Freud und mal Kreuzwortr\u00e4tsel-Debatten;", "tokens": ["mal", "Freud", "und", "mal", "Kreuz\u00b7wort\u00b7r\u00e4t\u00b7sel\u00b7De\u00b7bat\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "ADV", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "mal Tiergeschichten und mal Autorennen;", "tokens": ["mal", "Tier\u00b7ge\u00b7schich\u00b7ten", "und", "mal", "Au\u00b7to\u00b7ren\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "KON", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "mal mu\u00df man den ganzen Brockhaus kennen \u2013", "tokens": ["mal", "mu\u00df", "man", "den", "gan\u00b7zen", "Brock\u00b7haus", "ken\u00b7nen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "(\u00bbFrag mich was!\u00ab \u2013 Sie mir auch.)", "tokens": ["(", "\u00bb", "Frag", "mich", "was", "!", "\u00ab", "\u2013", "Sie", "mir", "auch", ".", ")"], "token_info": ["punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "$(", "VVIMP", "PPER", "PIS", "$.", "$(", "$(", "PPER", "PPER", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Und so haben nun", "tokens": ["Und", "so", "ha\u00b7ben", "nun"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ADV"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "die Berliner immer was zu tun.", "tokens": ["die", "Ber\u00b7li\u00b7ner", "im\u00b7mer", "was", "zu", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Denn so ist das in diesem Falle:", "tokens": ["Denn", "so", "ist", "das", "in", "die\u00b7sem", "Fal\u00b7le", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Was einer macht, das machen sie alle.", "tokens": ["Was", "ei\u00b7ner", "macht", ",", "das", "ma\u00b7chen", "sie", "al\u00b7le", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "PIS", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Macht einer Film mit Neckarstrand,", "tokens": ["Macht", "ei\u00b7ner", "Film", "mit", "Ne\u00b7ckarst\u00b7rand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "dann nehmen das tausend in die Hand.", "tokens": ["dann", "neh\u00b7men", "das", "tau\u00b7send", "in", "die", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "ART", "CARD", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Schreibt einer ein Buch vom Dauerlauf,", "tokens": ["Schreibt", "ei\u00b7ner", "ein", "Buch", "vom", "Dau\u00b7er\u00b7lauf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "dann greifen das hundert Verleger auf.", "tokens": ["dann", "grei\u00b7fen", "das", "hun\u00b7dert", "Ver\u00b7le\u00b7ger", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "CARD", "NN", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Sie begehren immer, die guten Knaben,", "tokens": ["Sie", "be\u00b7geh\u00b7ren", "im\u00b7mer", ",", "die", "gu\u00b7ten", "Kna\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.7": {"text": "des N\u00e4chsten Vieh \u2013", "tokens": ["des", "N\u00e4chs\u00b7ten", "Vieh", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "\u00bbm\u00fcssen wir auch mal haben!\u00ab", "tokens": ["\u00bb", "m\u00fcs\u00b7sen", "wir", "auch", "mal", "ha\u00b7ben", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VMFIN", "PPER", "ADV", "ADV", "VAFIN", "$.", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.9": {"text": "Sie m\u00f6chten niemals die eigenen Sachen.", "tokens": ["Sie", "m\u00f6ch\u00b7ten", "nie\u00b7mals", "die", "ei\u00b7ge\u00b7nen", "Sa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbdas? das m\u00fcssen wir auch mal machen \u2013!\u00ab", "tokens": ["\u00bb", "das", "?", "das", "m\u00fcs\u00b7sen", "wir", "auch", "mal", "ma\u00b7chen", "\u2013", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PDS", "$.", "PDS", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "$(", "$.", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.10": {"line.1": {"text": "Lasset uns dieserhalb nicht weinen.", "tokens": ["Las\u00b7set", "uns", "die\u00b7ser\u00b7halb", "nicht", "wei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PDS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo nichts ist, da borg ich mir einen.", "tokens": ["Wo", "nichts", "ist", ",", "da", "borg", "ich", "mir", "ei\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VAFIN", "$,", "ADV", "VVFIN", "PPER", "PPER", "ART", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Nur ist da eines \u2013 o v\u00f6lkische Schmach! \u2013", "tokens": ["Nur", "ist", "da", "ei\u00b7nes", "\u2013", "o", "v\u00f6l\u00b7ki\u00b7sche", "Schmach", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "$(", "FM", "ADJA", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "komisch:", "tokens": ["ko\u00b7misch", ":"], "token_info": ["word", "punct"], "pos": ["ADJD", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "uns macht keiner nach.", "tokens": ["uns", "macht", "kei\u00b7ner", "nach", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.11": {"line.1": {"text": "Das ist n\u00e4mlich so in Berlin:", "tokens": ["Das", "ist", "n\u00e4m\u00b7lich", "so", "in", "Ber\u00b7lin", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPR", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Einer ist pl\u00f6tzlich f\u00fcr Biographien.", "tokens": ["Ei\u00b7ner", "ist", "pl\u00f6tz\u00b7lich", "f\u00fcr", "Bio\u00b7gra\u00b7phi\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "APPR", "NN", "$."], "meter": "+--+--+-++", "measure": "dactylic.di.plus"}, "line.3": {"text": "Und aus einem Grunde, grad oder krumm,", "tokens": ["Und", "aus", "ei\u00b7nem", "Grun\u00b7de", ",", "grad", "o\u00b7der", "krumm", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,", "ADV", "KON", "ADJD", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "gef\u00e4llt diese Sache dem Publikum.", "tokens": ["ge\u00b7f\u00e4llt", "die\u00b7se", "Sa\u00b7che", "dem", "Pub\u00b7li\u00b7kum", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "NN", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Das Publikum mag das Neue gern kaufen . . .", "tokens": ["Das", "Pub\u00b7li\u00b7kum", "mag", "das", "Neu\u00b7e", "gern", "kau\u00b7fen", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "ADJA", "ADV", "VVINF", "$.", "$.", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Nun kommen sie aber alle gelaufen!", "tokens": ["Nun", "kom\u00b7men", "sie", "a\u00b7ber", "al\u00b7le", "ge\u00b7lau\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIS", "VVPP", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Jetzt schie\u00dfen, mit und ohne Komfort,", "tokens": ["Jetzt", "schie\u00b7\u00dfen", ",", "mit", "und", "oh\u00b7ne", "Kom\u00b7fort", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "APPR", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "die Biographien aus dem Boden hervor:", "tokens": ["die", "Bio\u00b7gra\u00b7phi\u00b7en", "aus", "dem", "Bo\u00b7den", "her\u00b7vor", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.13": {"line.1": {"text": "Kaiser Gustav der Heizbare; F\u00fcrstenberg;", "tokens": ["Kai\u00b7ser", "Gus\u00b7tav", "der", "Heiz\u00b7ba\u00b7re", ";", "F\u00fcrs\u00b7ten\u00b7berg", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "NE", "ART", "NN", "$.", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "der Herzbesitzer von Heidelberg;", "tokens": ["der", "Herz\u00b7be\u00b7sit\u00b7zer", "von", "Hei\u00b7del\u00b7berg", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Frau Neppach, Einstein und Lindberghs Sohn", "tokens": ["Frau", "Nep\u00b7pach", ",", "Ein\u00b7stein", "und", "Lind\u00b7berghs", "Sohn"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "NE", "$,", "NN", "KON", "NE", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "und vom Landgericht III der Justizrat Cohn \u2013", "tokens": ["und", "vom", "Land\u00b7ge\u00b7richt", "I\u00b7iI", "der", "Jus\u00b7tiz\u00b7rat", "Cohn", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "CARD", "ART", "NN", "NE", "$("], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "sie alle bekommen ihre Biographie", "tokens": ["sie", "al\u00b7le", "be\u00b7kom\u00b7men", "ih\u00b7re", "Bio\u00b7gra\u00b7phie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "PIS", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "(mit Bild auf dem Umschlag) \u2013 jetzt oder nie!", "tokens": ["(", "mit", "Bild", "auf", "dem", "Um\u00b7schlag", ")", "\u2013", "jetzt", "o\u00b7der", "nie", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "APPR", "ART", "NN", "$(", "$(", "ADV", "KON", "ADV", "$."], "meter": "-+---+-+-+", "measure": "dactylic.init"}, "line.7": {"text": "Heute so dick wie ein Lexikon,", "tokens": ["Heu\u00b7te", "so", "dick", "wie", "ein", "Le\u00b7xi\u00b7kon", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "und morgen spricht kein Mensch mehr davon.", "tokens": ["und", "mor\u00b7gen", "spricht", "kein", "Mensch", "mehr", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIAT", "NN", "ADV", "PAV", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.14": {"line.1": {"text": "Denn morgen ist da ein neues Gl\u00fcck:", "tokens": ["Denn", "mor\u00b7gen", "ist", "da", "ein", "neu\u00b7es", "Gl\u00fcck", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "das englische Grusel- und Geisterst\u00fcck.", "tokens": ["das", "eng\u00b7li\u00b7sche", "Gru\u00b7sel", "und", "Geis\u00b7ter\u00b7st\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "TRUNC", "KON", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.15": {"line.1": {"text": "Da kommen aber in hellen Haufen", "tokens": ["Da", "kom\u00b7men", "a\u00b7ber", "in", "hel\u00b7len", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "die Theaterdirektoren gelaufen!", "tokens": ["die", "The\u00b7a\u00b7ter\u00b7di\u00b7rek\u00b7to\u00b7ren", "ge\u00b7lau\u00b7fen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "--+---+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u203adie Gr\u00e4fin auf der Kirchhofswand\u2039,", "tokens": ["\u203a", "die", "Gr\u00e4\u00b7fin", "auf", "der", "Kirch\u00b7hofs\u00b7wand", "\u2039", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "APPR", "ART", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u203asherlock Piel zwischen Lipp und Kelchesrand\u2039,", "tokens": ["\u203a", "sher\u00b7lock", "Piel", "zwi\u00b7schen", "Lipp", "und", "Kel\u00b7ches\u00b7rand", "\u2039", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "NN", "APPR", "NE", "KON", "NN", "$(", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "\u203adas Bidet im Urwald\u2039 \u2013 oder wie das so hei\u00dft,", "tokens": ["\u203a", "das", "Bi\u00b7det", "im", "Ur\u00b7wald", "\u2039", "\u2013", "o\u00b7der", "wie", "das", "so", "hei\u00dft", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "APPRART", "NN", "$(", "$(", "KON", "PWAV", "PDS", "ADV", "VVFIN", "$,"], "meter": "-+--++-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "und pl\u00f6tzlich hat jedes Theater nen Geist.", "tokens": ["und", "pl\u00f6tz\u00b7lich", "hat", "je\u00b7des", "The\u00b7a\u00b7ter", "nen", "Geist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PIAT", "NN", "ADJA", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.7": {"text": "\u00bbdas kenn Se nich? Das haben Sie noch nicht gesehn \u2013?", "tokens": ["\u00bb", "das", "kenn", "Se", "nich", "?", "Das", "ha\u00b7ben", "Sie", "noch", "nicht", "ge\u00b7sehn", "\u2013", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PDS", "VVFIN", "NE", "PTKNEG", "$.", "PDS", "VAFIN", "PPER", "ADV", "PTKNEG", "VVPP", "$(", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Da m\u00fcssen Sie unbedingt hingehn \u2013!\u00ab", "tokens": ["Da", "m\u00fcs\u00b7sen", "Sie", "un\u00b7be\u00b7dingt", "hin\u00b7gehn", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF", "$(", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "Und auf einen, ders nicht gesehn hat, spucken . . .", "tokens": ["Und", "auf", "ei\u00b7nen", ",", "ders", "nicht", "ge\u00b7sehn", "hat", ",", "spu\u00b7cken", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["KON", "APPR", "ART", "$,", "PRELS", "PTKNEG", "VVPP", "VAFIN", "$,", "VVINF", "$.", "$.", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Morgen sind die Achseln ganz m\u00fcde vom Zucken:", "tokens": ["Mor\u00b7gen", "sind", "die", "Ach\u00b7seln", "ganz", "m\u00fc\u00b7de", "vom", "Zu\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ADV", "ADJD", "APPRART", "NN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "\u00bbwenn ich schon Geisterst\u00fccke seh \u2013", "tokens": ["\u00bb", "wenn", "ich", "schon", "Geis\u00b7ter\u00b7st\u00fc\u00b7cke", "seh", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "PPER", "ADV", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Pass\u00e9!\u00ab", "tokens": ["Pass\u00e9", "!", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["NE", "$.", "$("], "meter": "+", "measure": "single.up"}}, "stanza.17": {"line.1": {"text": "Mal Punktroller und mal Negerplatten;", "tokens": ["Mal", "Punk\u00b7trol\u00b7ler", "und", "mal", "Ne\u00b7ger\u00b7plat\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "KON", "ADV", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "mal Freud und mal Kreuzwortr\u00e4tsel-Debatten;", "tokens": ["mal", "Freud", "und", "mal", "Kreuz\u00b7wort\u00b7r\u00e4t\u00b7sel\u00b7De\u00b7bat\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "ADV", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "mal Tiergeschichten und mal Autorennen;", "tokens": ["mal", "Tier\u00b7ge\u00b7schich\u00b7ten", "und", "mal", "Au\u00b7to\u00b7ren\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "KON", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "mal mu\u00df man den ganzen Brockhaus kennen \u2013", "tokens": ["mal", "mu\u00df", "man", "den", "gan\u00b7zen", "Brock\u00b7haus", "ken\u00b7nen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "(\u00bbFrag mich was!\u00ab \u2013 Sie mir auch.)", "tokens": ["(", "\u00bb", "Frag", "mich", "was", "!", "\u00ab", "\u2013", "Sie", "mir", "auch", ".", ")"], "token_info": ["punct", "punct", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "$(", "VVIMP", "PPER", "PIS", "$.", "$(", "$(", "PPER", "PPER", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Und so haben nun", "tokens": ["Und", "so", "ha\u00b7ben", "nun"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "ADV"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "die Berliner immer was zu tun.", "tokens": ["die", "Ber\u00b7li\u00b7ner", "im\u00b7mer", "was", "zu", "tun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.18": {"line.1": {"text": "Denn so ist das in diesem Falle:", "tokens": ["Denn", "so", "ist", "das", "in", "die\u00b7sem", "Fal\u00b7le", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Was einer macht, das machen sie alle.", "tokens": ["Was", "ei\u00b7ner", "macht", ",", "das", "ma\u00b7chen", "sie", "al\u00b7le", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "PIS", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Macht einer Film mit Neckarstrand,", "tokens": ["Macht", "ei\u00b7ner", "Film", "mit", "Ne\u00b7ckarst\u00b7rand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "dann nehmen das tausend in die Hand.", "tokens": ["dann", "neh\u00b7men", "das", "tau\u00b7send", "in", "die", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "ART", "CARD", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Schreibt einer ein Buch vom Dauerlauf,", "tokens": ["Schreibt", "ei\u00b7ner", "ein", "Buch", "vom", "Dau\u00b7er\u00b7lauf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "dann greifen das hundert Verleger auf.", "tokens": ["dann", "grei\u00b7fen", "das", "hun\u00b7dert", "Ver\u00b7le\u00b7ger", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "CARD", "NN", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Sie begehren immer, die guten Knaben,", "tokens": ["Sie", "be\u00b7geh\u00b7ren", "im\u00b7mer", ",", "die", "gu\u00b7ten", "Kna\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.7": {"text": "des N\u00e4chsten Vieh \u2013", "tokens": ["des", "N\u00e4chs\u00b7ten", "Vieh", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "\u00bbm\u00fcssen wir auch mal haben!\u00ab", "tokens": ["\u00bb", "m\u00fcs\u00b7sen", "wir", "auch", "mal", "ha\u00b7ben", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VMFIN", "PPER", "ADV", "ADV", "VAFIN", "$.", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.9": {"text": "Sie m\u00f6chten niemals die eigenen Sachen.", "tokens": ["Sie", "m\u00f6ch\u00b7ten", "nie\u00b7mals", "die", "ei\u00b7ge\u00b7nen", "Sa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "\u00bbdas? das m\u00fcssen wir auch mal machen \u2013!\u00ab", "tokens": ["\u00bb", "das", "?", "das", "m\u00fcs\u00b7sen", "wir", "auch", "mal", "ma\u00b7chen", "\u2013", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PDS", "$.", "PDS", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "$(", "$.", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.20": {"line.1": {"text": "Lasset uns dieserhalb nicht weinen.", "tokens": ["Las\u00b7set", "uns", "die\u00b7ser\u00b7halb", "nicht", "wei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PDS", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wo nichts ist, da borg ich mir einen.", "tokens": ["Wo", "nichts", "ist", ",", "da", "borg", "ich", "mir", "ei\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VAFIN", "$,", "ADV", "VVFIN", "PPER", "PPER", "ART", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Nur ist da eines \u2013 o v\u00f6lkische Schmach! \u2013", "tokens": ["Nur", "ist", "da", "ei\u00b7nes", "\u2013", "o", "v\u00f6l\u00b7ki\u00b7sche", "Schmach", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "$(", "FM", "ADJA", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "komisch:", "tokens": ["ko\u00b7misch", ":"], "token_info": ["word", "punct"], "pos": ["ADJD", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "uns macht keiner nach.", "tokens": ["uns", "macht", "kei\u00b7ner", "nach", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}