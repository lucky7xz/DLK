{"textgrid.poem.53313": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "[kommt her, jhr Menschen allesampt]", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Kommt her, jhr Menschen allesampt,", "tokens": ["Kommt", "her", ",", "jhr", "Men\u00b7schen", "al\u00b7le\u00b7sampt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "$,", "PPOSAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die S\u00fcnde, Hell vnd Todt verdampt,", "tokens": ["Die", "S\u00fcn\u00b7de", ",", "Hell", "vnd", "Todt", "ver\u00b7dampt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NE", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "(spricht Christus) kommt mit grossem hauffen", "tokens": ["(", "spricht", "Chris\u00b7tus", ")", "kommt", "mit", "gros\u00b7sem", "hauf\u00b7fen"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "VVFIN", "NE", "$(", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ohn Seumnis her zu mir gelauffen,", "tokens": ["Ohn", "Seum\u00b7nis", "her", "zu", "mir", "ge\u00b7lauf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APZR", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Kompt, trettet f\u00fcr mein Gnaden-Ampt!", "tokens": ["Kompt", ",", "tret\u00b7tet", "f\u00fcr", "mein", "Gna\u00b7den\u00b7Ampt", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Die jhr in Noht vnd Trawrigheit,", "tokens": ["Die", "jhr", "in", "Noht", "vnd", "Traw\u00b7rig\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "M\u00fchseelig vnd beladen seyd,", "tokens": ["M\u00fch\u00b7see\u00b7lig", "vnd", "be\u00b7la\u00b7den", "seyd", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd nirgends h\u00fclffe k\u00f6nnt erblicken,", "tokens": ["Vnd", "nir\u00b7gends", "h\u00fclf\u00b7fe", "k\u00f6nnt", "er\u00b7bli\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zu mir kommt, ich wil euch erquicken", "tokens": ["Zu", "mir", "kommt", ",", "ich", "wil", "euch", "er\u00b7qui\u00b7cken"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "VVFIN", "$,", "PPER", "VMFIN", "PPER", "VVINF"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "In ewrer hochbetr\u00fcbten Zeit.", "tokens": ["In", "ew\u00b7rer", "hoch\u00b7be\u00b7tr\u00fcb\u00b7ten", "Zeit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Seht, da\u00df jhr meines Joches Last", "tokens": ["Seht", ",", "da\u00df", "jhr", "mei\u00b7nes", "Jo\u00b7ches", "Last"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur fein gedultig auff euch fasst,", "tokens": ["Nur", "fein", "ge\u00b7dul\u00b7tig", "auff", "euch", "fasst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJD", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd lernt von mir, ich bin von Hertzen", "tokens": ["Vnd", "lernt", "von", "mir", ",", "ich", "bin", "von", "Hert\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PPER", "$,", "PPER", "VAFIN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sanfft-vnd dem\u00fctig in den Schmertzen,", "tokens": ["Sanf\u00b7ft\u00b7\u00b7vnd", "de\u00b7m\u00fc\u00b7tig", "in", "den", "Schmert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "So findet ewre Seele Rast.", "tokens": ["So", "fin\u00b7det", "ew\u00b7re", "See\u00b7le", "Rast", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Es sey, da\u00df wer zur\u00fccke weicht,", "tokens": ["Es", "sey", ",", "da\u00df", "wer", "zu\u00b7r\u00fc\u00b7cke", "weicht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "KOUS", "PWS", "VVFIN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu bald vor seinem Creutz erbleicht,", "tokens": ["Zu", "bald", "vor", "sei\u00b7nem", "Creutz", "er\u00b7bleicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "APPR", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd h\u00e4lt nur viel von gutten Tagen,", "tokens": ["Vnd", "h\u00e4lt", "nur", "viel", "von", "gut\u00b7ten", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sonst aber ist mein Joch zu tragen", "tokens": ["Sonst", "a\u00b7ber", "ist", "mein", "Joch", "zu", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "PPOSAT", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Gar sanfft, vnd meine Last ist leicht.", "tokens": ["Gar", "sanfft", ",", "vnd", "mei\u00b7ne", "Last", "ist", "leicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KON", "PPOSAT", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}