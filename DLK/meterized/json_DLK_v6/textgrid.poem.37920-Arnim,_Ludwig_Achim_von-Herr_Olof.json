{"textgrid.poem.37920": {"metadata": {"author": {"name": "Arnim, Ludwig Achim von", "birth": "N.A.", "death": "N.A."}, "title": "Herr Olof", "genre": "verse", "period": "N.A.", "pub_year": 1806, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Herr Olof reitet sp\u00e4t und weit,", "tokens": ["Herr", "O\u00b7lof", "rei\u00b7tet", "sp\u00e4t", "und", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VVFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu bieten auf seine Hochzeitleut';", "tokens": ["Zu", "bie\u00b7ten", "auf", "sei\u00b7ne", "Hoch\u00b7zeit\u00b7leut'", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Da tanzen die Elfen auf gr\u00fcnem Land,", "tokens": ["Da", "tan\u00b7zen", "die", "El\u00b7fen", "auf", "gr\u00fc\u00b7nem", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Erl-K\u00f6nigs Tochter ihm reicht die Hand.", "tokens": ["Er\u00b7lK\u00f6\u00b7nigs", "Toch\u00b7ter", "ihm", "reicht", "die", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbwillkommen, Herr Olof, was eilst von hier?", "tokens": ["\u00bb", "will\u00b7kom\u00b7men", ",", "Herr", "O\u00b7lof", ",", "was", "eilst", "von", "hier", "?"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "NN", "NE", "$,", "PWS", "VVFIN", "APPR", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Tritt her in den Reihen und tanz mit mir.\u00ab", "tokens": ["Tritt", "her", "in", "den", "Rei\u00b7hen", "und", "tanz", "mit", "mir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN", "KON", "ADV", "APPR", "PPER", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "\u00bbich darf nicht tanzen, nicht tanzen ich mag,", "tokens": ["\u00bb", "ich", "darf", "nicht", "tan\u00b7zen", ",", "nicht", "tan\u00b7zen", "ich", "mag", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "PTKNEG", "VVFIN", "PPER", "VMFIN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Fr\u00fch Morgen ist mein Hochzeittag.\u00ab", "tokens": ["Fr\u00fch", "Mor\u00b7gen", "ist", "mein", "Hoch\u00b7zeit\u00b7tag", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "NN", "VAFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "\u00bbh\u00f6r an, Herr Olof, tritt tanzen mit mir,", "tokens": ["\u00bb", "h\u00f6r", "an", ",", "Herr", "O\u00b7lof", ",", "tritt", "tan\u00b7zen", "mit", "mir", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "NN", "NE", "$,", "VVFIN", "VVFIN", "APPR", "PPER", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Zwei g\u00fcldene Sporen schenk ich dir,", "tokens": ["Zwei", "g\u00fcl\u00b7de\u00b7ne", "Spo\u00b7ren", "schenk", "ich", "dir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "VVFIN", "PPER", "PPER", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Ein Hemd von Seide so wei\u00df und fein,", "tokens": ["Ein", "Hemd", "von", "Sei\u00b7de", "so", "wei\u00df", "und", "fein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "ADV", "VVFIN", "KON", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Meine Mutter bleichts mit Mondenschein.\u00ab", "tokens": ["Mei\u00b7ne", "Mut\u00b7ter", "bleichts", "mit", "Mon\u00b7den\u00b7schein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "\u00bbich darf nicht tanzen, nicht tanzen ich mag,", "tokens": ["\u00bb", "ich", "darf", "nicht", "tan\u00b7zen", ",", "nicht", "tan\u00b7zen", "ich", "mag", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "PTKNEG", "VVFIN", "PPER", "VMFIN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Fr\u00fch Morgen ist mein Hochzeittag.\u00ab", "tokens": ["Fr\u00fch", "Mor\u00b7gen", "ist", "mein", "Hoch\u00b7zeit\u00b7tag", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "NN", "VAFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "\u00bbh\u00f6r an! Herr Olof tritt tanzen mit mir,", "tokens": ["\u00bb", "h\u00f6r", "an", "!", "Herr", "O\u00b7lof", "tritt", "tan\u00b7zen", "mit", "mir", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$.", "NN", "NE", "VVFIN", "VVFIN", "APPR", "PPER", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Einen Haufen Goldes schenk ich dir.\u00ab", "tokens": ["Ei\u00b7nen", "Hau\u00b7fen", "Gol\u00b7des", "schenk", "ich", "dir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "PPER", "PPER", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "\u00bbeinen Haufen Goldes nehm ich wohl,", "tokens": ["\u00bb", "ei\u00b7nen", "Hau\u00b7fen", "Gol\u00b7des", "nehm", "ich", "wohl", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "NN", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Doch tanzen ich nicht darf noch soll.\u00ab", "tokens": ["Doch", "tan\u00b7zen", "ich", "nicht", "darf", "noch", "soll", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "VMFIN", "ADV", "VMFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "\u00bbund willt, Herr Olof, nicht tanzen mit mir,", "tokens": ["\u00bb", "und", "willt", ",", "Herr", "O\u00b7lof", ",", "nicht", "tan\u00b7zen", "mit", "mir", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VMFIN", "$,", "NN", "NE", "$,", "PTKNEG", "VVINF", "APPR", "PPER", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Soll Seuch und Krankheit folgen dir.\u00ab", "tokens": ["Soll", "Seuch", "und", "Krank\u00b7heit", "fol\u00b7gen", "dir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "KON", "NN", "VVFIN", "PPER", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Sie th\u00e4t einen Schlag ihm auf sein Herz,", "tokens": ["Sie", "th\u00e4t", "ei\u00b7nen", "Schlag", "ihm", "auf", "sein", "Herz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Noch nimmer f\u00fchlt er solchen Schmerz.", "tokens": ["Noch", "nim\u00b7mer", "f\u00fchlt", "er", "sol\u00b7chen", "Schmerz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Sie hob ihn bleichend auf sein Pferd,", "tokens": ["Sie", "hob", "ihn", "blei\u00b7chend", "auf", "sein", "Pferd", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbreit heim nun zu deinem Br\u00e4utlein werth.\u00ab", "tokens": ["\u00bb", "reit", "heim", "nun", "zu", "dei\u00b7nem", "Br\u00e4ut\u00b7lein", "werth", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "PTKVZ", "ADV", "APPR", "PPOSAT", "NN", "ADJD", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "Und als er kam vor Hauses Th\u00fcr,", "tokens": ["Und", "als", "er", "kam", "vor", "Hau\u00b7ses", "Th\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVFIN", "APPR", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Seine Mutter zitternd stand daf\u00fcr.", "tokens": ["Sei\u00b7ne", "Mut\u00b7ter", "zit\u00b7ternd", "stand", "da\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "VVFIN", "PAV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.14": {"line.1": {"text": "\u00bbh\u00f6r an, mein Sohn, sag an mir gleich,", "tokens": ["\u00bb", "h\u00f6r", "an", ",", "mein", "Sohn", ",", "sag", "an", "mir", "gleich", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "PPOSAT", "NN", "$,", "VVFIN", "APPR", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie ist dein Farbe bla\u00df und bleich!\u00ab", "tokens": ["Wie", "ist", "dein", "Far\u00b7be", "bla\u00df", "und", "bleich", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VAFIN", "PPOSAT", "NN", "ADJD", "KON", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "\u00bbund sollt sie nicht seyn bla\u00df und bleich,", "tokens": ["\u00bb", "und", "sollt", "sie", "nicht", "seyn", "bla\u00df", "und", "bleich", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VMFIN", "PPER", "PTKNEG", "VAINF", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich traf in Erlen K\u00f6nigs Reich.\u00ab", "tokens": ["Ich", "traf", "in", "Er\u00b7len", "K\u00f6\u00b7nigs", "Reich", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "\u00bbh\u00f6r an mein Sohn, so lieb und traut,", "tokens": ["\u00bb", "h\u00f6r", "an", "mein", "Sohn", ",", "so", "lieb", "und", "traut", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "APPR", "PPOSAT", "NN", "$,", "ADV", "ADJD", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was soll ich nun sagen deiner Braut?\u00ab", "tokens": ["Was", "soll", "ich", "nun", "sa\u00b7gen", "dei\u00b7ner", "Braut", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.17": {"line.1": {"text": "\u00bbsagt ihr, ich sey im Wald zur Stund,", "tokens": ["\u00bb", "sagt", "ihr", ",", "ich", "sey", "im", "Wald", "zur", "Stund", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PPER", "VAFIN", "APPRART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu proben da mein Pferd und Hund.\u00ab", "tokens": ["Zu", "pro\u00b7ben", "da", "mein", "Pferd", "und", "Hund", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKZU", "VVINF", "ADV", "PPOSAT", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Fr\u00fch Morgen und als es Tag kaum war,", "tokens": ["Fr\u00fch", "Mor\u00b7gen", "und", "als", "es", "Tag", "kaum", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "KOUS", "PPER", "NN", "ADV", "VAFIN", "$,"], "meter": "++--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Da kam die Braut mit der Hochzeitschaar.", "tokens": ["Da", "kam", "die", "Braut", "mit", "der", "Hoch\u00b7zeit\u00b7schaar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.19": {"line.1": {"text": "Sie schenkten Meet, sie schenkten Wein,", "tokens": ["Sie", "schenk\u00b7ten", "Meet", ",", "sie", "schenk\u00b7ten", "Wein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "$,", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbwo ist Herr Olof, der Br\u00e4utgam mein?\u00ab", "tokens": ["\u00bb", "wo", "ist", "Herr", "O\u00b7lof", ",", "der", "Br\u00e4ut\u00b7gam", "mein", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "NN", "NE", "$,", "ART", "NE", "PPOSAT", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.20": {"line.1": {"text": "\u00bbherr Olof, er ritt in den Wald zur Stund,", "tokens": ["\u00bb", "herr", "O\u00b7lof", ",", "er", "ritt", "in", "den", "Wald", "zur", "Stund", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NN", "NE", "$,", "PPER", "VVFIN", "APPR", "ART", "NN", "APPRART", "NN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Er probt allda sein Pferd und Hund.\u00ab", "tokens": ["Er", "probt", "all\u00b7da", "sein", "Pferd", "und", "Hund", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PAV", "PPOSAT", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Die Braut hob auf den Scharlach roth,", "tokens": ["Die", "Braut", "hob", "auf", "den", "Schar\u00b7lach", "roth", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da lag Herr Olof, und er war todt.", "tokens": ["Da", "lag", "Herr", "O\u00b7lof", ",", "und", "er", "war", "todt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "NE", "$,", "KON", "PPER", "VAFIN", "ADJD", "$."], "meter": "-++-+-+-+", "measure": "unknown.measure.penta"}}}}}