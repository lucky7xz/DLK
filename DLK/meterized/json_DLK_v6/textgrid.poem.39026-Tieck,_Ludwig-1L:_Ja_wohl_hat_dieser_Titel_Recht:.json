{"textgrid.poem.39026": {"metadata": {"author": {"name": "Tieck, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ja wohl hat dieser Titel Recht:", "genre": "verse", "period": "N.A.", "pub_year": 1813, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ja wohl hat dieser Titel Recht:", "tokens": ["Ja", "wohl", "hat", "die\u00b7ser", "Ti\u00b7tel", "Recht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "VAFIN", "PDAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie k\u00e4m' es sonst, da\u00df hier an heiliger St\u00e4tte", "tokens": ["Wie", "k\u00e4m'", "es", "sonst", ",", "da\u00df", "hier", "an", "hei\u00b7li\u00b7ger", "St\u00e4t\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Der so oft daheim verschm\u00e4hte Autor", "tokens": ["Der", "so", "oft", "da\u00b7heim", "ver\u00b7schm\u00e4h\u00b7te", "Au\u00b7tor"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Von Menschenha\u00df, dem Kind der Liebe,", "tokens": ["Von", "Men\u00b7schen\u00b7ha\u00df", ",", "dem", "Kind", "der", "Lie\u00b7be", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Und vielen, vielen, vielen langweiligen Thorheiten,", "tokens": ["Und", "vie\u00b7len", ",", "vie\u00b7len", ",", "vie\u00b7len", "lang\u00b7wei\u00b7li\u00b7gen", "Thor\u00b7hei\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "$,", "PIAT", "$,", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-++-+-+-", "measure": "unknown.measure.septa"}, "line.6": {"text": "In diesen Hallen", "tokens": ["In", "die\u00b7sen", "Hal\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PDAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Noch Freunde f\u00e4nde,", "tokens": ["Noch", "Freun\u00b7de", "f\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.8": {"text": "Die Ged\u00e4chtni\u00df und Seele", "tokens": ["Die", "Ge\u00b7d\u00e4cht\u00b7ni\u00df", "und", "See\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.9": {"text": "Mit dem Ballast seiner n\u00fcchternen Sp\u00e4\u00dfe", "tokens": ["Mit", "dem", "Bal\u00b7last", "sei\u00b7ner", "n\u00fcch\u00b7ter\u00b7nen", "Sp\u00e4\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Belasten, um sie herzusagen und abzuspielen?", "tokens": ["Be\u00b7las\u00b7ten", ",", "um", "sie", "her\u00b7zu\u00b7sa\u00b7gen", "und", "ab\u00b7zu\u00b7spie\u00b7len", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUI", "PPER", "VVPP", "KON", "VVIZU", "$."], "meter": "-+--++-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Ja wohl ist der Deutsche deutsch,", "tokens": ["Ja", "wohl", "ist", "der", "Deut\u00b7sche", "deutsch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "VAFIN", "ART", "NN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.12": {"text": "Und wei\u00df sich, wenn er aufrichtig ist,", "tokens": ["Und", "wei\u00df", "sich", ",", "wenn", "er", "auf\u00b7rich\u00b7tig", "ist", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "$,", "KOUS", "PPER", "ADJD", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Nichts besseres als so breites Gew\u00e4sch,", "tokens": ["Nichts", "bes\u00b7se\u00b7res", "als", "so", "brei\u00b7tes", "Ge\u00b7w\u00e4sch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PIS", "KOKOM", "ADV", "ADJA", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.14": {"text": "Ein Lachen ohne Salz, und Tugend im Abgeschmack.", "tokens": ["Ein", "La\u00b7chen", "oh\u00b7ne", "Salz", ",", "und", "Tu\u00b7gend", "im", "Ab\u00b7ge\u00b7schmack", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,", "KON", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.2": {"line.1": {"text": "Und du selber, der du jetzt wieder l\u00e4sterst!", "tokens": ["Und", "du", "sel\u00b7ber", ",", "der", "du", "jetzt", "wie\u00b7der", "l\u00e4s\u00b7terst", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "$,", "PRELS", "PPER", "ADV", "ADV", "ADV", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ist es nicht die Nemesis, die dich erfa\u00dft,", "tokens": ["Ist", "es", "nicht", "die", "Ne\u00b7me\u00b7sis", ",", "die", "dich", "er\u00b7fa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ART", "NN", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "Da\u00df du nun schon bei sechs, bei sieben Proben", "tokens": ["Da\u00df", "du", "nun", "schon", "bei", "sechs", ",", "bei", "sie\u00b7ben", "Pro\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "APPR", "CARD", "$,", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Den Einhelfer machst,", "tokens": ["Den", "Ein\u00b7hel\u00b7fer", "machst", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Und nicht blo\u00df eine, sondern alle Rollen", "tokens": ["Und", "nicht", "blo\u00df", "ei\u00b7ne", ",", "son\u00b7dern", "al\u00b7le", "Rol\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PTKNEG", "ADV", "ART", "$,", "KON", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Wort f\u00fcr Wort zu sagen wei\u00dft?", "tokens": ["Wort", "f\u00fcr", "Wort", "zu", "sa\u00b7gen", "wei\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Konnte f\u00fcr alle Scherze und Kritiken,", "tokens": ["Konn\u00b7te", "f\u00fcr", "al\u00b7le", "Scher\u00b7ze", "und", "Kri\u00b7ti\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PIAT", "NN", "KON", "NN", "$,"], "meter": "+--+-+--+--", "measure": "iambic.tetra.invert"}, "line.8": {"text": "F\u00fcr alles was du gegen den gro\u00dfen Mann gethan,", "tokens": ["F\u00fcr", "al\u00b7les", "was", "du", "ge\u00b7gen", "den", "gro\u00b7\u00dfen", "Mann", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "PWS", "PPER", "APPR", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+------+-+-+", "measure": "dactylic.init"}, "line.9": {"text": "Dir h\u00e4rtere Strafe,", "tokens": ["Dir", "h\u00e4r\u00b7te\u00b7re", "Stra\u00b7fe", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.10": {"text": "Aber auch gerechtere werden?", "tokens": ["A\u00b7ber", "auch", "ge\u00b7rech\u00b7te\u00b7re", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJA", "VAINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Ist es so mit dem Schicksal beschaffen,", "tokens": ["Ist", "es", "so", "mit", "dem", "Schick\u00b7sal", "be\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Wer wei\u00df, was mir dann noch bevorsteht,", "tokens": ["Wer", "wei\u00df", ",", "was", "mir", "dann", "noch", "be\u00b7vor\u00b7steht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "PWS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "In welchen andern Wirrwarr, Mischmasch, Quack- und Qu\u00e4ngelei,", "tokens": ["In", "wel\u00b7chen", "an\u00b7dern", "Wirr\u00b7warr", ",", "Mischmasch", ",", "Quack", "und", "Qu\u00e4n\u00b7ge\u00b7lei", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "ADJA", "NN", "$,", "NN", "$,", "TRUNC", "KON", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Ich noch selber spielen, oder souffliren mu\u00df?", "tokens": ["Ich", "noch", "sel\u00b7ber", "spie\u00b7len", ",", "o\u00b7der", "souff\u00b7li\u00b7ren", "mu\u00df", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "VVINF", "$,", "KON", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.5": {"text": "Doch gn\u00e4dig ist der F\u00fcrst,", "tokens": ["Doch", "gn\u00e4\u00b7dig", "ist", "der", "F\u00fcrst", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Und sch\u00f6n sind die Frauen.", "tokens": ["Und", "sch\u00f6n", "sind", "die", "Frau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "Die reizende Gr\u00e4finn,", "tokens": ["Die", "rei\u00b7zen\u00b7de", "Gr\u00e4\u00b7finn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.8": {"text": "Die die lockere Dirne viel zu anmuthig spielt,", "tokens": ["Die", "die", "lo\u00b7cke\u00b7re", "Dir\u00b7ne", "viel", "zu", "an\u00b7mut\u00b7hig", "spielt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "ADV", "PTKA", "ADJD", "VVFIN", "$,"], "meter": "--+--+-+-+--+", "measure": "anapaest.di.plus"}, "line.9": {"text": "Tritt keinmal \u00e4ngstlich", "tokens": ["Tritt", "kein\u00b7mal", "\u00e4ngst\u00b7lich"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ADV", "ADJD"], "meter": "-+-+-", "measure": "iambic.di"}, "line.10": {"text": "Hinter dem Fl\u00fcgel hervor,", "tokens": ["Hin\u00b7ter", "dem", "Fl\u00fc\u00b7gel", "her\u00b7vor", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.11": {"text": "Da\u00df sie nicht gl\u00e4ubig-katholisch", "tokens": ["Da\u00df", "sie", "nicht", "gl\u00e4u\u00b7big\u00b7ka\u00b7tho\u00b7lisch"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Vor den Anfang der albernen Rede", "tokens": ["Vor", "den", "An\u00b7fang", "der", "al\u00b7ber\u00b7nen", "Re\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.13": {"text": "Ein Kreuz \u00fcber Stirn und Busen z\u00f6ge.", "tokens": ["Ein", "Kreuz", "\u00fc\u00b7ber", "Stirn", "und", "Bu\u00b7sen", "z\u00f6\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "\u2013 Und so heiligst du, Liebliche, mich,", "tokens": ["\u2013", "Und", "so", "hei\u00b7ligst", "du", ",", "Lieb\u00b7li\u00b7che", ",", "mich", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["$(", "KON", "ADV", "VVFIN", "PPER", "$,", "NN", "$,", "PPER", "$,"], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.15": {"text": "Und das frevelhafte Werk,", "tokens": ["Und", "das", "fre\u00b7vel\u00b7haf\u00b7te", "Werk", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.16": {"text": "Und aller Beginnen zugleich.", "tokens": ["Und", "al\u00b7ler", "Be\u00b7gin\u00b7nen", "zu\u00b7gleich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}}}}