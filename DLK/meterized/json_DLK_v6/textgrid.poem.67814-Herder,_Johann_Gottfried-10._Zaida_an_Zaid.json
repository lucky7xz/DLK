{"textgrid.poem.67814": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "10. Zaida an Zaid", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "H\u00f6r, was ich dir melde, Zaid!", "tokens": ["H\u00f6r", ",", "was", "ich", "dir", "mel\u00b7de", ",", "Zaid", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "PWS", "PPER", "PPER", "VVFIN", "$,", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Geh nicht mehr durch meine Strasse,", "tokens": ["Geh", "nicht", "mehr", "durch", "mei\u00b7ne", "Stras\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sprich nicht mehr mit meinen Weibern,", "tokens": ["Sprich", "nicht", "mehr", "mit", "mei\u00b7nen", "Wei\u00b7bern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Noch mit meinen Sklaven sprich mehr!", "tokens": ["Noch", "mit", "mei\u00b7nen", "Skla\u00b7ven", "sprich", "mehr", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "ADV", "ADV", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.2": {"line.1": {"text": "Frage nicht mehr, was ich mache?", "tokens": ["Fra\u00b7ge", "nicht", "mehr", ",", "was", "ich", "ma\u00b7che", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "ADV", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Noch wer komm, mich zu besuchen?", "tokens": ["Noch", "wer", "komm", ",", "mich", "zu", "be\u00b7su\u00b7chen", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "VVFIN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Welche Feste mich erg\u00f6zen?", "tokens": ["Wel\u00b7che", "Fes\u00b7te", "mich", "er\u00b7g\u00f6\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welche Farben mir gefallen?", "tokens": ["Wel\u00b7che", "Far\u00b7ben", "mir", "ge\u00b7fal\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Gnug an der, die deinetwegen", "tokens": ["Gnug", "an", "der", ",", "die", "dei\u00b7net\u00b7we\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPR", "ART", "$,", "PRELS", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jezo meine Wangen f\u00e4rbet!", "tokens": ["Je\u00b7zo", "mei\u00b7ne", "Wan\u00b7gen", "f\u00e4r\u00b7bet", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df ich einen Mohren kannte,", "tokens": ["Da\u00df", "ich", "ei\u00b7nen", "Moh\u00b7ren", "kann\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der so wenig wei\u00df zu leben. \u2013", "tokens": ["Der", "so", "we\u00b7nig", "wei\u00df", "zu", "le\u00b7ben", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADV", "PIS", "VVFIN", "PTKZU", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich gesteh es, du bist tapfer,", "tokens": ["Ich", "ge\u00b7steh", "es", ",", "du", "bist", "tap\u00b7fer", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VAFIN", "ADJD", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Spaltest, trennest, reissest nieder,", "tokens": ["Spal\u00b7test", ",", "tren\u00b7nest", ",", "reis\u00b7sest", "nie\u00b7der", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hast der Christen mehr erleget,", "tokens": ["Hast", "der", "Chris\u00b7ten", "mehr", "er\u00b7le\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Als Blutstropfen in dir fliessen!", "tokens": ["Als", "Bluts\u00b7trop\u00b7fen", "in", "dir", "flies\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}}, "stanza.5": {"line.1": {"text": "Bist ein wackrer sch\u00f6ner Reuter,", "tokens": ["Bist", "ein", "wack\u00b7rer", "sch\u00f6\u00b7ner", "Reu\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "ADJA", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tanzest, singest, spielest lieblich,", "tokens": ["Tan\u00b7zest", ",", "sin\u00b7gest", ",", "spie\u00b7lest", "lieb\u00b7lich", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "VVFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bist so fein, so wohlerzogen,", "tokens": ["Bist", "so", "fein", ",", "so", "woh\u00b7ler\u00b7zo\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie man sich es nur kann denken;", "tokens": ["Wie", "man", "sich", "es", "nur", "kann", "den\u00b7ken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "PPER", "ADV", "VMFIN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Wei\u00df und roth, da\u00df nichts dar\u00fcber!", "tokens": ["Wei\u00df", "und", "roth", ",", "da\u00df", "nichts", "da\u00b7r\u00fc\u00b7ber", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJD", "$,", "KOUS", "PIS", "PAV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Stammest von ber\u00fchmten Ahnen,", "tokens": ["Stam\u00b7mest", "von", "be\u00b7r\u00fchm\u00b7ten", "Ah\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bist die Krone stets im Streite,", "tokens": ["Bist", "die", "Kro\u00b7ne", "stets", "im", "Strei\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bist die Zier in Scherz und Spielen!", "tokens": ["Bist", "die", "Zier", "in", "Scherz", "und", "Spie\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Viel verlier' ich mit dir, Zaid!", "tokens": ["Viel", "ver\u00b7lier'", "ich", "mit", "dir", ",", "Zaid", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "$,", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie ich viel mit dir gewann,", "tokens": ["Wie", "ich", "viel", "mit", "dir", "ge\u00b7wann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und \u2013 w\u00e4rst du nur stumm gebohren,", "tokens": ["Und", "\u2013", "w\u00e4rst", "du", "nur", "stumm", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VAFIN", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "W\u00e4r' es dich zu lieben m\u00f6glich.", "tokens": ["W\u00e4r'", "es", "dich", "zu", "lie\u00b7ben", "m\u00f6g\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "PTKZU", "VVINF", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Aber um des Einen willen,", "tokens": ["A\u00b7ber", "um", "des", "Ei\u00b7nen", "wil\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mu\u00df ich, Zaid, dich verlieren,", "tokens": ["Mu\u00df", "ich", ",", "Zaid", ",", "dich", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "$,", "NN", "$,", "PPER", "VVINF", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Da, Verschwender deiner Seele,", "tokens": ["Da", ",", "Ver\u00b7schwen\u00b7der", "dei\u00b7ner", "See\u00b7le", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Du dir selbst dein Gl\u00fcck ja raubest.", "tokens": ["Du", "dir", "selbst", "dein", "Gl\u00fcck", "ja", "rau\u00b7best", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "ADV", "PPOSAT", "NN", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Denn in Reden dich zu z\u00e4hmen,", "tokens": ["Denn", "in", "Re\u00b7den", "dich", "zu", "z\u00e4h\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Th\u00e4te es ja wahrlich Noth, dir", "tokens": ["Th\u00e4\u00b7te", "es", "ja", "wahr\u00b7lich", "Noth", ",", "dir"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "NN", "$,", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf die Brust ein Schlo\u00df zu sezen,", "tokens": ["Auf", "die", "Brust", "ein", "Schlo\u00df", "zu", "se\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Auf die Lippen einen Kadi.", "tokens": ["Auf", "die", "Lip\u00b7pen", "ei\u00b7nen", "Ka\u00b7di", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NE", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.10": {"line.1": {"text": "Viel verm\u00f6gen bei den Damen", "tokens": ["Viel", "ver\u00b7m\u00f6\u00b7gen", "bei", "den", "Da\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tapfre M\u00e4nner Deinesgleichen;", "tokens": ["Tapf\u00b7re", "M\u00e4n\u00b7ner", "Dei\u00b7nes\u00b7glei\u00b7chen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn sie lieben tapfre M\u00e4nner,", "tokens": ["Denn", "sie", "lie\u00b7ben", "tapf\u00b7re", "M\u00e4n\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die zerstreuen, haun und spalten.", "tokens": ["Die", "zer\u00b7streu\u00b7en", ",", "haun", "und", "spal\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJD", "KON", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Aber kurz und gut, Freund Zaid,", "tokens": ["A\u00b7ber", "kurz", "und", "gut", ",", "Freund", "Zaid", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJD", "KON", "ADJD", "$,", "NN", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn von solchen Gunsterweisen", "tokens": ["Wenn", "von", "sol\u00b7chen", "Guns\u00b7ter\u00b7wei\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Du dir etwa Tafel giebest;", "tokens": ["Du", "dir", "et\u00b7wa", "Ta\u00b7fel", "gie\u00b7best", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "ADV", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Rath ich dir: genie\u00df und schweige!", "tokens": ["Rath", "ich", "dir", ":", "ge\u00b7nie\u00df", "und", "schwei\u00b7ge", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPER", "$.", "VVFIN", "KON", "ADJA", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "K\u00f6stlich wars, was du genossest,", "tokens": ["K\u00f6st\u00b7lich", "wars", ",", "was", "du", "ge\u00b7nos\u00b7sest", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gl\u00fccklich w\u00e4rest du, o Zaid,", "tokens": ["Gl\u00fcck\u00b7lich", "w\u00e4\u00b7rest", "du", ",", "o", "Zaid", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "$,", "FM", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fcstest du, dir zu erhalten,", "tokens": ["W\u00fcs\u00b7test", "du", ",", "dir", "zu", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Was du zu gewinnen wustest.", "tokens": ["Was", "du", "zu", "ge\u00b7win\u00b7nen", "wus\u00b7test", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Aber warest du doch neulich", "tokens": ["A\u00b7ber", "wa\u00b7rest", "du", "doch", "neu\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kaum heraus aus Tarfes Garten,", "tokens": ["Kaum", "he\u00b7raus", "aus", "Tar\u00b7fes", "Gar\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als du ja von deinem Ungl\u00fcck", "tokens": ["Als", "du", "ja", "von", "dei\u00b7nem", "Un\u00b7gl\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und von meinem so beredt warst!", "tokens": ["Und", "von", "mei\u00b7nem", "so", "be\u00b7redt", "warst", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Einem mi\u00dfgeschaffnen Mohren", "tokens": ["Ei\u00b7nem", "mi\u00df\u00b7ge\u00b7schaff\u00b7nen", "Moh\u00b7ren"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zeigtest du, ich wei\u00df es, jene", "tokens": ["Zeig\u00b7test", "du", ",", "ich", "wei\u00df", "es", ",", "je\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER", "$,", "PDS"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Flechte, die von meinen Haaren", "tokens": ["Flech\u00b7te", ",", "die", "von", "mei\u00b7nen", "Haa\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PRELS", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ich dir auf den Turban steckte.", "tokens": ["Ich", "dir", "auf", "den", "Tur\u00b7ban", "steck\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.15": {"line.1": {"text": "Nicht verlang' ich sie zur\u00fccke,", "tokens": ["Nicht", "ver\u00b7lang'", "ich", "sie", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "PPER", "PPER", "PTKVZ", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Noch, da\u00df du das Nichts behaltest,", "tokens": ["Noch", ",", "da\u00df", "du", "das", "Nichts", "be\u00b7hal\u00b7test", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "ART", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber wisse, Mohr! Du hast sie", "tokens": ["A\u00b7ber", "wis\u00b7se", ",", "Mohr", "!", "Du", "hast", "sie"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "$,", "NN", "$.", "PPER", "VAFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Jezt zum Zeichen meiner Ungunst!", "tokens": ["Jezt", "zum", "Zei\u00b7chen", "mei\u00b7ner", "Un\u00b7gunst", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Auch hab' ich es wohl erfahren,", "tokens": ["Auch", "hab'", "ich", "es", "wohl", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Wie du ihn f\u00fcr jene L\u00fcgen,", "tokens": ["Wie", "du", "ihn", "f\u00fcr", "je\u00b7ne", "L\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "L\u00fcgen, die f\u00fcr Wahrheit gelten,", "tokens": ["L\u00fc\u00b7gen", ",", "die", "f\u00fcr", "Wahr\u00b7heit", "gel\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nun herausgefodert habest.", "tokens": ["Nun", "her\u00b7aus\u00b7ge\u00b7fo\u00b7dert", "ha\u00b7best", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Wahrlich, ein so n\u00e4rrisch Ungl\u00fcck", "tokens": ["Wahr\u00b7lich", ",", "ein", "so", "n\u00e4r\u00b7risch", "Un\u00b7gl\u00fcck"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "$,", "ART", "ADV", "ADJD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Macht mich lachen wider Willen,", "tokens": ["Macht", "mich", "la\u00b7chen", "wi\u00b7der", "Wil\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wahrest selbst nicht dein Geheimni\u00df;", "tokens": ["Wah\u00b7rest", "selbst", "nicht", "dein", "Ge\u00b7heim\u00b7ni\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKNEG", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und ein andrer soll es wahren?", "tokens": ["Und", "ein", "an\u00b7drer", "soll", "es", "wah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "VMFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Ich will nichts entschuldigt h\u00f6ren;", "tokens": ["Ich", "will", "nichts", "ent\u00b7schul\u00b7digt", "h\u00f6\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "VVPP", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nochmals will ich dir nur melden,", "tokens": ["Noch\u00b7mals", "will", "ich", "dir", "nur", "mel\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df du jezt zum leztenmale", "tokens": ["Da\u00df", "du", "jezt", "zum", "lez\u00b7ten\u00b7ma\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "APPRART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mich hier siehst, und ich dich spreche.", "tokens": ["Mich", "hier", "siehst", ",", "und", "ich", "dich", "spre\u00b7che", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "$,", "KON", "PPER", "PRF", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.19": {"line.1": {"text": "Also die versch\u00e4mte Mohrin", "tokens": ["Al\u00b7so", "die", "ver\u00b7sch\u00e4m\u00b7te", "Moh\u00b7rin"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "NN"], "meter": "+-+-+---", "measure": "unknown.measure.tri"}, "line.2": {"text": "Sprach zum stolzen Bencerrajen;", "tokens": ["Sprach", "zum", "stol\u00b7zen", "Ben\u00b7cer\u00b7ra\u00b7jen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Sprach noch, da sie weg sich wandte:", "tokens": ["Sprach", "noch", ",", "da", "sie", "weg", "sich", "wand\u00b7te", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "KOUS", "PPER", "ADV", "PRF", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbwers so macht, wird so gelohnet!\u00ab", "tokens": ["\u00bb", "wers", "so", "macht", ",", "wird", "so", "ge\u00b7loh\u00b7net", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "ADV", "VVFIN", "$,", "VAFIN", "ADV", "VVPP", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}