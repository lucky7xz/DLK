{"textgrid.poem.49651": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Wendung", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Feder und Papier und Tinte", "tokens": ["Fe\u00b7der", "und", "Pa\u00b7pier", "und", "Tin\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schufen uns das wohlgesinnte", "tokens": ["Schu\u00b7fen", "uns", "das", "wohl\u00b7ge\u00b7sinn\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Gro\u00dfmachtsfriedensprotokoll,", "tokens": ["Gro\u00df\u00b7machts\u00b7frie\u00b7dens\u00b7pro\u00b7to\u00b7koll", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie es ewig gelten soll.", "tokens": ["Wie", "es", "e\u00b7wig", "gel\u00b7ten", "soll", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJD", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Hier erg\u00e4nzt und dort verbessert,", "tokens": ["Hier", "er\u00b7g\u00e4nzt", "und", "dort", "ver\u00b7bes\u00b7sert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "KON", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hier versch\u00e4rft und dort verw\u00e4ssert,", "tokens": ["Hier", "ver\u00b7sch\u00e4rft", "und", "dort", "ver\u00b7w\u00e4s\u00b7sert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KON", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber endlich hatten wir", "tokens": ["A\u00b7ber", "end\u00b7lich", "hat\u00b7ten", "wir"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Alles sch\u00f6n auf dem Papier.", "tokens": ["Al\u00b7les", "sch\u00f6n", "auf", "dem", "Pa\u00b7pier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Ernst gefa\u00dft trotz der Verluste,", "tokens": ["Ernst", "ge\u00b7fa\u00dft", "trotz", "der", "Ver\u00b7lus\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die der andre tragen mu\u00dfte,", "tokens": ["Die", "der", "and\u00b7re", "tra\u00b7gen", "mu\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "PIS", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sprachen wir ger\u00fchrt dabei,", "tokens": ["Spra\u00b7chen", "wir", "ge\u00b7r\u00fchrt", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVPP", "PAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df es nicht zu \u00e4ndern sei.", "tokens": ["Da\u00df", "es", "nicht", "zu", "\u00e4n\u00b7dern", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Als wir nun mit W\u00fcrde gingen,", "tokens": ["Als", "wir", "nun", "mit", "W\u00fcr\u00b7de", "gin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unser Beileid anzubringen,", "tokens": ["Un\u00b7ser", "Bei\u00b7leid", "an\u00b7zu\u00b7brin\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVIZU", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch benebst dem Aktenst\u00fcck,", "tokens": ["Auch", "be\u00b7nebst", "dem", "Ak\u00b7ten\u00b7st\u00fcck", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wies man es mit Hohn zur\u00fcck.", "tokens": ["Wies", "man", "es", "mit", "Hohn", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Alles war so wohl bemessen,", "tokens": ["Al\u00b7les", "war", "so", "wohl", "be\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und nur eines war vergessen,", "tokens": ["Und", "nur", "ei\u00b7nes", "war", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIS", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df ein Volk, sich selber treu,", "tokens": ["Da\u00df", "ein", "Volk", ",", "sich", "sel\u00b7ber", "treu", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$,", "PRF", "ADV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nicht gehorcht. Und das war neu.", "tokens": ["Nicht", "ge\u00b7horcht", ".", "Und", "das", "war", "neu", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$.", "KON", "PDS", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}