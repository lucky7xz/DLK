{"textgrid.poem.60707": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Tugenden sollten Schwestern sein,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Tugenden sollten Schwestern sein,", "tokens": ["Tu\u00b7gen\u00b7den", "soll\u00b7ten", "Schwes\u00b7tern", "sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "NN", "VAINF", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Gleichwie die Laster Br\u00fcder sind.", "tokens": ["Gleich\u00b7wie", "die", "Las\u00b7ter", "Br\u00fc\u00b7der", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dringt eins von diesen uns ins Herz hinein,", "tokens": ["Dringt", "eins", "von", "die\u00b7sen", "uns", "ins", "Herz", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PDAT", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So folgen alle andern gar geschwind,", "tokens": ["So", "fol\u00b7gen", "al\u00b7le", "an\u00b7dern", "gar", "ge\u00b7schwind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "PIS", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Falls ihnen m\u00f6glich, unter einem Dach zu leben,", "tokens": ["Falls", "ih\u00b7nen", "m\u00f6g\u00b7lich", ",", "un\u00b7ter", "ei\u00b7nem", "Dach", "zu", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "$,", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Indem sie nicht einander hart entgegenstreben.", "tokens": ["In\u00b7dem", "sie", "nicht", "ein\u00b7an\u00b7der", "hart", "ent\u00b7ge\u00b7gen\u00b7stre\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was Tugenden betrifft, so wird man selten finden,", "tokens": ["Was", "Tu\u00b7gen\u00b7den", "be\u00b7tr\u00b7ifft", ",", "so", "wird", "man", "sel\u00b7ten", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "ADV", "VAFIN", "PIS", "ADJD", "VVINF", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Da\u00df alle sich in einem Wesen sch\u00f6n verbinden", "tokens": ["Da\u00df", "al\u00b7le", "sich", "in", "ei\u00b7nem", "We\u00b7sen", "sch\u00f6n", "ver\u00b7bin\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "PRF", "APPR", "ART", "NN", "ADJD", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und treu einander ihre H\u00e4nde geben.", "tokens": ["Und", "treu", "ein\u00b7an\u00b7der", "ih\u00b7re", "H\u00e4n\u00b7de", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Der eine ist voll Mitgef\u00fchl,", "tokens": ["Der", "ei\u00b7ne", "ist", "voll", "Mit\u00b7ge\u00b7f\u00fchl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Doch auch im J\u00e4hzorn ist er gro\u00df,", "tokens": ["Doch", "auch", "im", "J\u00e4h\u00b7zorn", "ist", "er", "gro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Der andre ist zwar klug, doch k\u00fchl.", "tokens": ["Der", "and\u00b7re", "ist", "zwar", "klug", ",", "doch", "k\u00fchl", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "ADV", "ADJD", "$,", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Bei Tieren ist wohl beispiellos", "tokens": ["Bei", "Tie\u00b7ren", "ist", "wohl", "bei\u00b7spiel\u00b7los"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VAFIN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Des Hundes treue Sorge um den Herrn,", "tokens": ["Des", "Hun\u00b7des", "treu\u00b7e", "Sor\u00b7ge", "um", "den", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Doch kann er wie kein andres Tier", "tokens": ["Doch", "kann", "er", "wie", "kein", "and\u00b7res", "Tier"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "PPER", "KOKOM", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Auch t\u00f6richt sein und hei\u00df von Gier.", "tokens": ["Auch", "t\u00f6\u00b7richt", "sein", "und", "hei\u00df", "von", "Gier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "KON", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Beweis: die beiden Hunde, die von fern", "tokens": ["Be\u00b7weis", ":", "die", "bei\u00b7den", "Hun\u00b7de", ",", "die", "von", "fern"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "ART", "PIAT", "NN", "$,", "PRELS", "APPR", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.18": {"text": "Den toten Esel sahn, der auf den Wellen schwamm.", "tokens": ["Den", "to\u00b7ten", "E\u00b7sel", "sahn", ",", "der", "auf", "den", "Wel\u00b7len", "schwamm", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Die Hunde wurden aufmerksam,", "tokens": ["Die", "Hun\u00b7de", "wur\u00b7den", "auf\u00b7merk\u00b7sam", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Doch weiter, weiter trieb der Wind den Leichnam fort.", "tokens": ["Doch", "wei\u00b7ter", ",", "wei\u00b7ter", "trieb", "der", "Wind", "den", "Leich\u00b7nam", "fort", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "\u00bbfreund,\u00ab sprach der eine, \u00bbdeine Augen sind noch besser,", "tokens": ["\u00bb", "freund", ",", "\u00ab", "sprach", "der", "ei\u00b7ne", ",", "\u00bb", "dei\u00b7ne", "Au\u00b7gen", "sind", "noch", "bes\u00b7ser", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$,", "$(", "VVFIN", "ART", "ART", "$,", "$(", "PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Sieh scharf doch \u00fcber das Gew\u00e4sser,", "tokens": ["Sieh", "scharf", "doch", "\u00fc\u00b7ber", "das", "Ge\u00b7w\u00e4s\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist das ein Pferd, ein Ochse dort?\u00ab", "tokens": ["Ist", "das", "ein", "Pferd", ",", "ein", "O\u00b7chse", "dort", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "$,", "ART", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbganz einerlei, welch eine Kreatur,\u00ab", "tokens": ["\u00bb", "ganz", "ei\u00b7ner\u00b7lei", ",", "welch", "ei\u00b7ne", "Kre\u00b7a\u00b7tur", ",", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PIS", "$,", "PWAT", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Entgegnete der andre Fresser,", "tokens": ["Ent\u00b7geg\u00b7ne\u00b7te", "der", "and\u00b7re", "Fres\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "\u00bbist's eine gute Beute nur!", "tokens": ["\u00bb", "ist's", "ei\u00b7ne", "gu\u00b7te", "Beu\u00b7te", "nur", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "ART", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Doch fragt sich's, wie wir sie bekommen k\u00f6nnen.", "tokens": ["Doch", "fragt", "sich's", ",", "wie", "wir", "sie", "be\u00b7kom\u00b7men", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "$,", "PWAV", "PPER", "PPER", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Da uns so weite Wellen von ihr trennen,", "tokens": ["Da", "uns", "so", "wei\u00b7te", "Wel\u00b7len", "von", "ihr", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJA", "NN", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "So hilft uns Schwimmen nicht dazu.", "tokens": ["So", "hilft", "uns", "Schwim\u00b7men", "nicht", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "PTKNEG", "PAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Los, saufen wir das ganze Wasser, ich und du!", "tokens": ["Los", ",", "sau\u00b7fen", "wir", "das", "gan\u00b7ze", "Was\u00b7ser", ",", "ich", "und", "du", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,", "PPER", "KON", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Den hei\u00dfen Kehlen wird es so gelingen,", "tokens": ["Den", "hei\u00b7\u00dfen", "Keh\u00b7len", "wird", "es", "so", "ge\u00b7lin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Den Leichnam bald aufs Trockene zu bringen.", "tokens": ["Den", "Leich\u00b7nam", "bald", "aufs", "Tro\u00b7cke\u00b7ne", "zu", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Dann haben wir f\u00fcr eine Woche Fra\u00df und Ruh.\u00ab", "tokens": ["Dann", "ha\u00b7ben", "wir", "f\u00fcr", "ei\u00b7ne", "Wo\u00b7che", "Fra\u00df", "und", "Ruh", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die Hunde gingen dran, das Wasser zu verschlingen!", "tokens": ["Die", "Hun\u00b7de", "gin\u00b7gen", "dran", ",", "das", "Was\u00b7ser", "zu", "ver\u00b7schlin\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PAV", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Der Atem ging den beiden aus dabei.", "tokens": ["Der", "A\u00b7tem", "ging", "den", "bei\u00b7den", "aus", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "PIAT", "APPR", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Verenden sah man alle zwei.", "tokens": ["Ver\u00b7en\u00b7den", "sah", "man", "al\u00b7le", "zwei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PIAT", "CARD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Mensch treibt's ebenso.", "tokens": ["Der", "Mensch", "treibt's", "e\u00b7ben\u00b7so", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Entflammt ihn eine Sache lichterloh,", "tokens": ["Ent\u00b7flammt", "ihn", "ei\u00b7ne", "Sa\u00b7che", "lich\u00b7ter\u00b7loh", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "So \u00fcbersieht er die Unm\u00f6glichkeit.", "tokens": ["So", "\u00fc\u00b7ber\u00b7sieht", "er", "die", "Un\u00b7m\u00f6g\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wie tobt und rast er nach Verm\u00f6gen oder Ruhm!", "tokens": ["Wie", "tobt", "und", "rast", "er", "nach", "Ver\u00b7m\u00f6\u00b7gen", "o\u00b7der", "Ruhm", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "KON", "VVFIN", "PPER", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wenn ich vergr\u00f6\u00dferte mein Eigentum!", "tokens": ["Wenn", "ich", "ver\u00b7gr\u00f6\u00b7\u00dfer\u00b7te", "mein", "Ei\u00b7gen\u00b7tum", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Wenn ich erstrebte die Allwissenheit,", "tokens": ["Wenn", "ich", "er\u00b7streb\u00b7te", "die", "All\u00b7wis\u00b7sen\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Hebr\u00e4isch lernte, Alchimie, Geschichte!", "tokens": ["Heb\u00b7r\u00e4\u00b7isch", "lern\u00b7te", ",", "Al\u00b7chi\u00b7mie", ",", "Ge\u00b7schich\u00b7te", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "NN", "$,", "NN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.8": {"text": "Wenn ich Dukaten h\u00e4tte, Truhen voll! \u2013", "tokens": ["Wenn", "ich", "Du\u00b7ka\u00b7ten", "h\u00e4t\u00b7te", ",", "Tru\u00b7hen", "voll", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "NN", "VAFIN", "$,", "NN", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "O Mensch, damit dir alles das gelingen soll,", "tokens": ["O", "Mensch", ",", "da\u00b7mit", "dir", "al\u00b7les", "das", "ge\u00b7lin\u00b7gen", "soll", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "KOUS", "PPER", "PIS", "PDS", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Trink Meere aus wie jene Hundewichte.", "tokens": ["Trink", "Mee\u00b7re", "aus", "wie", "je\u00b7ne", "Hun\u00b7de\u00b7wich\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "PTKVZ", "KOKOM", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Den Pl\u00e4nen zu gen\u00fcgen, die ein einziger schmiedet,", "tokens": ["Den", "Pl\u00e4\u00b7nen", "zu", "ge\u00b7n\u00fc\u00b7gen", ",", "die", "ein", "ein\u00b7zi\u00b7ger", "schmie\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "PRELS", "ART", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Vier K\u00f6rper t\u00e4ten not \u2013 und w\u00e4ren doch erm\u00fcdet,", "tokens": ["Vier", "K\u00f6r\u00b7per", "t\u00e4\u00b7ten", "not", "\u2013", "und", "w\u00e4\u00b7ren", "doch", "er\u00b7m\u00fc\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJA", "NN", "$(", "KON", "VAFIN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "So glaub ich, eh die H\u00e4lfte noch vollbracht.", "tokens": ["So", "glaub", "ich", ",", "eh", "die", "H\u00e4lf\u00b7te", "noch", "voll\u00b7bracht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Nicht vier Methusalems bes\u00e4\u00dfen Macht,", "tokens": ["Nicht", "vier", "Me\u00b7thu\u00b7sa\u00b7lems", "be\u00b7s\u00e4\u00b7\u00dfen", "Macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "CARD", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Die Gier zu stillen, die in ", "tokens": ["Die", "Gier", "zu", "stil\u00b7len", ",", "die", "in"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "PRELS", "APPR"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}}}}