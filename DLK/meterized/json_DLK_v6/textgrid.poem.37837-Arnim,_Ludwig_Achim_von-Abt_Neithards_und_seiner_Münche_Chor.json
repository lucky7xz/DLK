{"textgrid.poem.37837": {"metadata": {"author": {"name": "Arnim, Ludwig Achim von", "birth": "N.A.", "death": "N.A."}, "title": "Abt Neithards und seiner M\u00fcnche Chor", "genre": "verse", "period": "N.A.", "pub_year": 1806, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich will mich aber freuen gegen diesen Mayen,", "tokens": ["Ich", "will", "mich", "a\u00b7ber", "freu\u00b7en", "ge\u00b7gen", "die\u00b7sen", "Ma\u00b7yen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "VVFIN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.2": {"text": "Der mir gar \u00fcppiglichen Muth soll verleihen,", "tokens": ["Der", "mir", "gar", "\u00fcp\u00b7pig\u00b7li\u00b7chen", "Muth", "soll", "ver\u00b7lei\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADJA", "NN", "VMFIN", "VVINF", "$,"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Das sey eim Bauer und seinen Gesellen leide.", "tokens": ["Das", "sey", "eim", "Bau\u00b7er", "und", "sei\u00b7nen", "Ge\u00b7sel\u00b7len", "lei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Ich habe der Lieben gedient also lange,", "tokens": ["Ich", "ha\u00b7be", "der", "Lie\u00b7ben", "ge\u00b7dient", "al\u00b7so", "lan\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "ADV", "ADV", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Oft und viel mit meinem neuen Gesange,", "tokens": ["Oft", "und", "viel", "mit", "mei\u00b7nem", "neu\u00b7en", "Ge\u00b7san\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KON", "ADV", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Die gelben Bl\u00fcmelein bracht ich ihr von der Heyde.", "tokens": ["Die", "gel\u00b7ben", "Bl\u00fc\u00b7me\u00b7lein", "bracht", "ich", "ihr", "von", "der", "Hey\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-++-+--+-", "measure": "iambic.hexa.relaxed"}}, "stanza.3": {"line.1": {"text": "Die trug sie gar h\u00fcbschlich zu dem Tanze,", "tokens": ["Die", "trug", "sie", "gar", "h\u00fcb\u00b7schlich", "zu", "dem", "Tan\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Alle meine Hoffnung mu\u00dft mir werden ganze,", "tokens": ["Al\u00b7le", "mei\u00b7ne", "Hoff\u00b7nung", "mu\u00dft", "mir", "wer\u00b7den", "gan\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "VMFIN", "PPER", "VAFIN", "ADJA", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Da ich sie sah die s\u00e4uberliche Magd.", "tokens": ["Da", "ich", "sie", "sah", "die", "s\u00e4u\u00b7ber\u00b7li\u00b7che", "Magd", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Ich kam zu der Lieben schon gegessen,", "tokens": ["Ich", "kam", "zu", "der", "Lie\u00b7ben", "schon", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "ADV", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wohl vier und zwanzig Bauern, die hatten sich vermessen,", "tokens": ["Wohl", "vier", "und", "zwan\u00b7zig", "Bau\u00b7ern", ",", "die", "hat\u00b7ten", "sich", "ver\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "KON", "CARD", "NN", "$,", "PRELS", "VAFIN", "PRF", "VVPP", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Von ihne da ward sch\u00e4mlich ich verjagt.", "tokens": ["Von", "ih\u00b7ne", "da", "ward", "sch\u00e4m\u00b7lich", "ich", "ver\u00b7jagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "VAFIN", "ADJD", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "In einer weiten Stube mit Gedr\u00e4nge,", "tokens": ["In", "ei\u00b7ner", "wei\u00b7ten", "Stu\u00b7be", "mit", "Ge\u00b7dr\u00e4n\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die weite Stube ward mir viel zu enge,", "tokens": ["Die", "wei\u00b7te", "Stu\u00b7be", "ward", "mir", "viel", "zu", "en\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "ADV", "APPR", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und meines Lebens h\u00e4tte ich n\u00e4chst versagt.", "tokens": ["Und", "mei\u00b7nes", "Le\u00b7bens", "h\u00e4t\u00b7te", "ich", "n\u00e4chst", "ver\u00b7sagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Aller meiner Noth konnt ich nicht bedenken.", "tokens": ["Al\u00b7ler", "mei\u00b7ner", "Noth", "konnt", "ich", "nicht", "be\u00b7den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Um und um hin lief ich an den B\u00e4nken,", "tokens": ["Um", "und", "um", "hin", "lief", "ich", "an", "den", "B\u00e4n\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "KON", "APPR", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Bis da\u00df ich doch die recht Th\u00fcr erschreite.", "tokens": ["Bis", "da\u00df", "ich", "doch", "die", "recht", "Th\u00fcr", "er\u00b7schrei\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PPER", "ADV", "ART", "ADJD", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "Meines Unfalls Rath h\u00e4tt ich bald vergessen,", "tokens": ["Mei\u00b7nes", "Un\u00b7falls", "Rath", "h\u00e4tt", "ich", "bald", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Meine weiten Spr\u00fcng die waren ungemessen,", "tokens": ["Mei\u00b7ne", "wei\u00b7ten", "Spr\u00fcng", "die", "wa\u00b7ren", "un\u00b7ge\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ART", "VAFIN", "ADJD", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Die ich vor den alten Gauchen hin schreite.", "tokens": ["Die", "ich", "vor", "den", "al\u00b7ten", "Gau\u00b7chen", "hin", "schrei\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ART", "ADJA", "NN", "ADV", "VVFIN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Dahin gen Wien, da eilt ich also balde,", "tokens": ["Da\u00b7hin", "gen", "Wi\u00b7en", ",", "da", "eilt", "ich", "al\u00b7so", "bal\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "APPR", "NE", "$,", "ADV", "VVFIN", "PPER", "ADV", "ADV", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "H\u00e4tt ich einen Laden Tuchs mit Gewalte,", "tokens": ["H\u00e4tt", "ich", "ei\u00b7nen", "La\u00b7den", "Tuchs", "mit", "Ge\u00b7wal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Bey hundert Ellen, darum zahlt ich gut.", "tokens": ["Bey", "hun\u00b7dert", "El\u00b7len", ",", "da\u00b7rum", "zahlt", "ich", "gut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$,", "PAV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Und zehn Ellen mehr, darum wollt ichs nicht lassen,", "tokens": ["Und", "zehn", "El\u00b7len", "mehr", ",", "da\u00b7rum", "wollt", "ichs", "nicht", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "NN", "ADV", "$,", "PAV", "VMFIN", "PIS", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Darum so wollt ich \u00fcppiglichen stossen", "tokens": ["Da\u00b7rum", "so", "wollt", "ich", "\u00fcp\u00b7pig\u00b7li\u00b7chen", "stos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VMFIN", "PPER", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Die vier und zwanzig Bauren hochgemuthe.", "tokens": ["Die", "vier", "und", "zwan\u00b7zig", "Bau\u00b7ren", "hoch\u00b7ge\u00b7mu\u00b7the", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "KON", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Und h\u00e4tt ich einen Schneider mit zweien Knechten,", "tokens": ["Und", "h\u00e4tt", "ich", "ei\u00b7nen", "Schnei\u00b7der", "mit", "zwei\u00b7en", "Knech\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ART", "NE", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Die mir schnitten die Kleider also gerechte,", "tokens": ["Die", "mir", "schnit\u00b7ten", "die", "Klei\u00b7der", "al\u00b7so", "ge\u00b7rech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Vier und zwanzig Kutten mu\u00dften sie tragen.", "tokens": ["Vier", "und", "zwan\u00b7zig", "Kut\u00b7ten", "mu\u00df\u00b7ten", "sie", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "KON", "CARD", "NN", "VMFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.11": {"line.1": {"text": "Die eine kurz, die andere wohl gel\u00e4nget,", "tokens": ["Die", "ei\u00b7ne", "kurz", ",", "die", "an\u00b7de\u00b7re", "wohl", "ge\u00b7l\u00e4n\u00b7get", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJD", "$,", "PRELS", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Als Gott ihnen ihr Gew\u00e4chs nun hat verh\u00e4nget,", "tokens": ["Als", "Gott", "ih\u00b7nen", "ihr", "Ge\u00b7w\u00e4chs", "nun", "hat", "ver\u00b7h\u00e4n\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "PPOSAT", "NN", "ADV", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Und oben weit gefalten um den Kragen,", "tokens": ["Und", "o\u00b7ben", "weit", "ge\u00b7fal\u00b7ten", "um", "den", "Kra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Die f\u00fcnf und zwanzigst Kutten will ich selber tragen,", "tokens": ["Die", "f\u00fcnf", "und", "zwan\u00b7zigst", "Kut\u00b7ten", "will", "ich", "sel\u00b7ber", "tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "KON", "VVFIN", "NN", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df man f\u00fcr den Abt mich m\u00fcsse ansagen,", "tokens": ["Da\u00df", "man", "f\u00fcr", "den", "Abt", "mich", "m\u00fcs\u00b7se", "an\u00b7sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "ART", "NN", "PPER", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wann ich in dem Land mit ihnen umfahre.", "tokens": ["Wann", "ich", "in", "dem", "Land", "mit", "ih\u00b7nen", "um\u00b7fah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ART", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "Und h\u00e4tt ich einen Scherer also gute,", "tokens": ["Und", "h\u00e4tt", "ich", "ei\u00b7nen", "Sche\u00b7rer", "al\u00b7so", "gu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der mir die Bauern bescheret die Bauern hochgemuthe,", "tokens": ["Der", "mir", "die", "Bau\u00b7ern", "be\u00b7sche\u00b7ret", "die", "Bau\u00b7ern", "hoch\u00b7ge\u00b7mu\u00b7the", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "VVFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Ich wollt ihnen scheeren die alten Bauern-Haare.", "tokens": ["Ich", "wollt", "ih\u00b7nen", "schee\u00b7ren", "die", "al\u00b7ten", "Bau\u00b7ern\u00b7Haa\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.14": {"line.1": {"text": "Noch so mu\u00df ich haben viererley Dinge,", "tokens": ["Noch", "so", "mu\u00df", "ich", "ha\u00b7ben", "vie\u00b7rer\u00b7ley", "Din\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "VAFIN", "PIAT", "NN", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Oben eine Platte und darum einen Ringe,", "tokens": ["O\u00b7ben", "ei\u00b7ne", "Plat\u00b7te", "und", "da\u00b7rum", "ei\u00b7nen", "Rin\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "KON", "PAV", "ART", "NN", "$,"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Gleichwie ein M\u00f6nch auf Erden soll seyn.", "tokens": ["Gleich\u00b7wie", "ein", "M\u00f6nch", "auf", "Er\u00b7den", "soll", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.15": {"line.1": {"text": "Noch so hab ich der Abentheuer nicht gare,", "tokens": ["Noch", "so", "hab", "ich", "der", "A\u00b7bent\u00b7heu\u00b7er", "nicht", "ga\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "+-+--++-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Er hie\u00df ihm bringen ein Osterwein so klare,", "tokens": ["Er", "hie\u00df", "ihm", "brin\u00b7gen", "ein", "Os\u00b7ter\u00b7wein", "so", "kla\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "ART", "NN", "ADV", "ADJA", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und ein Schlaftrinken go\u00df er ihnen darein.", "tokens": ["Und", "ein", "Schlaf\u00b7trin\u00b7ken", "go\u00df", "er", "ih\u00b7nen", "da\u00b7rein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "PPER", "PAV", "$."], "meter": "--++-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.16": {"line.1": {"text": "Also war das Abentheuer bereitet,", "tokens": ["Al\u00b7so", "war", "das", "A\u00b7bent\u00b7heu\u00b7er", "be\u00b7rei\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Und auf einem Karren schnelle geleitet,", "tokens": ["Und", "auf", "ei\u00b7nem", "Kar\u00b7ren", "schnel\u00b7le", "ge\u00b7lei\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "VVPP", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wohl zu dem gr\u00fcnen Anger hin.", "tokens": ["Wohl", "zu", "dem", "gr\u00fc\u00b7nen", "An\u00b7ger", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Zum gr\u00fcnen Anger unter der sch\u00f6nen Linden,", "tokens": ["Zum", "gr\u00fc\u00b7nen", "An\u00b7ger", "un\u00b7ter", "der", "sch\u00f6\u00b7nen", "Lin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ART", "ADJA", "NE", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da lie\u00dfen sich die Bauren allsammt finden,", "tokens": ["Da", "lie\u00b7\u00dfen", "sich", "die", "Bau\u00b7ren", "all\u00b7sammt", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ihrer vier und zwanzig, das war ihr Ungewinn.", "tokens": ["Ih\u00b7rer", "vier", "und", "zwan\u00b7zig", ",", "das", "war", "ihr", "Un\u00b7ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "CARD", "KON", "CARD", "$,", "PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}}, "stanza.18": {"line.1": {"text": "Der erste der sprach, wollt ihr den Neithard sehen,", "tokens": ["Der", "ers\u00b7te", "der", "sprach", ",", "wollt", "ihr", "den", "Neit\u00b7hard", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ART", "VVFIN", "$,", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Der ander sprach, ja m\u00fcst ihm Leid geschehen,", "tokens": ["Der", "an\u00b7der", "sprach", ",", "ja", "m\u00fcst", "ihm", "Leid", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "$,", "ADV", "VMFIN", "PPER", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und meld sein nicht, es mu\u00df an sein Leben gahn.", "tokens": ["Und", "meld", "sein", "nicht", ",", "es", "mu\u00df", "an", "sein", "Le\u00b7ben", "gahn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "PTKNEG", "$,", "PPER", "VMFIN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+----+-+-+", "measure": "dactylic.init"}}, "stanza.19": {"line.1": {"text": "Er zog die Gugel von der Platten gare,", "tokens": ["Er", "zog", "die", "Gu\u00b7gel", "von", "der", "Plat\u00b7ten", "ga\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der dritt sprach, es ist ein M\u00f6nch f\u00fcrwahre,", "tokens": ["Der", "dritt", "sprach", ",", "es", "ist", "ein", "M\u00f6nch", "f\u00fcr\u00b7wah\u00b7re", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "$,", "PPER", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Und ist in unserm Land ein fremder Mann.", "tokens": ["Und", "ist", "in", "un\u00b7serm", "Land", "ein", "frem\u00b7der", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPOSAT", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.20": {"line.1": {"text": "Er zuckt die Gugel gar nieder auf den R\u00fccken,", "tokens": ["Er", "zuckt", "die", "Gu\u00b7gel", "gar", "nie\u00b7der", "auf", "den", "R\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADV", "PTKVZ", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Er trat zu den Bauren gar voll T\u00fccken,", "tokens": ["Er", "trat", "zu", "den", "Bau\u00b7ren", "gar", "voll", "T\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ADV", "ADJD", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wie bald trat Engelmayer zu ihm dar.", "tokens": ["Wie", "bald", "trat", "En\u00b7gel\u00b7may\u00b7er", "zu", "ihm", "dar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "NE", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.21": {"line.1": {"text": "Er sprach: \u00bbGr\u00fc\u00df euch Gott Kinder, wollt ihr trinken?", "tokens": ["Er", "sprach", ":", "\u00bb", "Gr\u00fc\u00df", "euch", "Gott", "Kin\u00b7der", ",", "wollt", "ihr", "trin\u00b7ken", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VVIMP", "PPER", "NN", "NN", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Guten Osterwein will ich euch schenken.\u00ab", "tokens": ["Gu\u00b7ten", "Os\u00b7ter\u00b7wein", "will", "ich", "euch", "schen\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "PPER", "VVINF", "$.", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Da bot er ihnen das Schlaftr\u00e4nklein dar.", "tokens": ["Da", "bot", "er", "ih\u00b7nen", "das", "Schlaf\u00b7tr\u00e4nk\u00b7lein", "dar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Sie trunken alle den Oesterwein gar vaste,", "tokens": ["Sie", "trun\u00b7ken", "al\u00b7le", "den", "O\u00b7es\u00b7ter\u00b7wein", "gar", "vas\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ART", "NN", "ADV", "ADJA", "$,"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Je l\u00e4nger, je mehr, so mehret sich ihr Laster,", "tokens": ["Je", "l\u00e4n\u00b7ger", ",", "je", "mehr", ",", "so", "meh\u00b7ret", "sich", "ihr", "Las\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADV", "$,", "ADV", "VVFIN", "PRF", "PPOSAT", "NN", "$,"], "meter": "-+----+-+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Sie lagen alle vor tod an einer Schaar.", "tokens": ["Sie", "la\u00b7gen", "al\u00b7le", "vor", "tod", "an", "ei\u00b7ner", "Schaar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.23": {"line.1": {"text": "Die Messer und die Schwerdt begunnt er ihnen rauffen,", "tokens": ["Die", "Mes\u00b7ser", "und", "die", "Schwerdt", "be\u00b7gunnt", "er", "ih\u00b7nen", "rauf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "VVFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die dicken Stecken mit den gro\u00dfen Knauffen,", "tokens": ["Die", "di\u00b7cken", "Ste\u00b7cken", "mit", "den", "gro\u00b7\u00dfen", "Knauf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "G\u00fcrtel und Taschen nahm er von ihnen gar.", "tokens": ["G\u00fcr\u00b7tel", "und", "Ta\u00b7schen", "nahm", "er", "von", "ih\u00b7nen", "gar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "PPER", "APPR", "PPER", "ADV", "$."], "meter": "+--+-+--+-+", "measure": "iambic.penta.invert"}}, "stanza.24": {"line.1": {"text": "Also wurden ihrer vier und zwanzig beschoren,", "tokens": ["Al\u00b7so", "wur\u00b7den", "ih\u00b7rer", "vier", "und", "zwan\u00b7zig", "be\u00b7scho\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "CARD", "KON", "CARD", "VVINF", "$,"], "meter": "+-+-+-+-+--+-", "measure": "hexameter"}, "line.2": {"text": "Rock und Mantel h\u00e4ttens all verlohren,", "tokens": ["Rock", "und", "Man\u00b7tel", "h\u00e4t\u00b7tens", "all", "ver\u00b7loh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "PIAT", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Vier und zwanzig Kutten stie\u00df er ihnen an.", "tokens": ["Vier", "und", "zwan\u00b7zig", "Kut\u00b7ten", "stie\u00df", "er", "ih\u00b7nen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "KON", "CARD", "NN", "VVFIN", "PPER", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.25": {"line.1": {"text": "Sie lagen bis an den vierten Tag ohne Sinnen,", "tokens": ["Sie", "la\u00b7gen", "bis", "an", "den", "vier\u00b7ten", "Tag", "oh\u00b7ne", "Sin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "-+----+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Allererst da wurden sie's wohl innen,", "tokens": ["Al\u00b7le\u00b7rerst", "da", "wur\u00b7den", "sie's", "wohl", "in\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "ADV", "ADV", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Und h\u00f6rt, wie einer sprach der alten Knaben.", "tokens": ["Und", "h\u00f6rt", ",", "wie", "ei\u00b7ner", "sprach", "der", "al\u00b7ten", "Kna\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PWAV", "PIS", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.26": {"line.1": {"text": "Der greift da mit der Hand wohl auf das Haare:", "tokens": ["Der", "greift", "da", "mit", "der", "Hand", "wohl", "auf", "das", "Haa\u00b7re", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "ART", "NN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbnun freut euch alle, ich bin ein M\u00f6nch f\u00fcrwahre,", "tokens": ["\u00bb", "nun", "freut", "euch", "al\u00b7le", ",", "ich", "bin", "ein", "M\u00f6nch", "f\u00fcr\u00b7wah\u00b7re", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "PIS", "$,", "PPER", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und will uns Morgen eine Fr\u00fchme\u00df haben.\u00ab", "tokens": ["Und", "will", "uns", "Mor\u00b7gen", "ei\u00b7ne", "Fr\u00fch\u00b7me\u00df", "ha\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VMFIN", "PPER", "NN", "ART", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.27": {"line.1": {"text": "Der andere sprach: \u00bbSo sing uns das Amte,", "tokens": ["Der", "an\u00b7de\u00b7re", "sprach", ":", "\u00bb", "So", "sing", "uns", "das", "Am\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das helfen wir dir Bruder allesammte,", "tokens": ["Das", "hel\u00b7fen", "wir", "dir", "Bru\u00b7der", "al\u00b7le\u00b7samm\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Als wir vor und nach dem Pfluge gethan haben.\u00ab", "tokens": ["Als", "wir", "vor", "und", "nach", "dem", "Pflu\u00b7ge", "ge\u00b7than", "ha\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PTKVZ", "KON", "APPR", "ART", "NN", "VVPP", "VAINF", "$.", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.28": {"line.1": {"text": "Der Neithard kam wohl zu den Bauren getreten:", "tokens": ["Der", "Neit\u00b7hard", "kam", "wohl", "zu", "den", "Bau\u00b7ren", "ge\u00b7tre\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "\u00bbihr liebe Kind wer hat euch her gebeten,", "tokens": ["\u00bb", "ihr", "lie\u00b7be", "Kind", "wer", "hat", "euch", "her", "ge\u00b7be\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "ADJA", "NN", "PWS", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df ihr so liegt in Gottes Ordnung hie.\u00ab", "tokens": ["Da\u00df", "ihr", "so", "liegt", "in", "Got\u00b7tes", "Ord\u00b7nung", "hie", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "APPR", "NN", "NN", "ADV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.29": {"line.1": {"text": "\u00bbnun lieber Herr, das hat uns Gott erschaffen,", "tokens": ["\u00bb", "nun", "lie\u00b7ber", "Herr", ",", "das", "hat", "uns", "Gott", "er\u00b7schaf\u00b7fen", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADV", "NN", "$,", "PDS", "VAFIN", "PPER", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wir sind all worden hie zu Pfaffen,", "tokens": ["Wir", "sind", "all", "wor\u00b7den", "hie", "zu", "Pfaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "VAPP", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und sind dazu gar wenig doch gelehrt.\u00ab", "tokens": ["Und", "sind", "da\u00b7zu", "gar", "we\u00b7nig", "doch", "ge\u00b7lehrt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "PAV", "ADV", "PIS", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.30": {"line.1": {"text": "\u00bbihr lieben Kind, zum Lernen seyd ihr junge,", "tokens": ["\u00bb", "ihr", "lie\u00b7ben", "Kind", ",", "zum", "Ler\u00b7nen", "seyd", "ihr", "jun\u00b7ge", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "ADJA", "NN", "$,", "APPRART", "NN", "VAFIN", "PPOSAT", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "In meinem Mund trag ich eine gelehrte Zunge,", "tokens": ["In", "mei\u00b7nem", "Mund", "trag", "ich", "ei\u00b7ne", "ge\u00b7lehr\u00b7te", "Zun\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und gute Lehre geb ich euch nun hie.\u00ab", "tokens": ["Und", "gu\u00b7te", "Leh\u00b7re", "geb", "ich", "euch", "nun", "hie", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJA", "NN", "VVFIN", "PPER", "PPER", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.31": {"line.1": {"text": "Mit guten Worten bracht er's auf die Stra\u00dfe,", "tokens": ["Mit", "gu\u00b7ten", "Wor\u00b7ten", "bracht", "er's", "auf", "die", "Stra\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dahin gen Wien, so sie Gott immer hasse,", "tokens": ["Da\u00b7hin", "gen", "Wi\u00b7en", ",", "so", "sie", "Gott", "im\u00b7mer", "has\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "APPR", "NE", "$,", "ADV", "PPER", "NN", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Wohl auf die Br\u00fccke vor des Herzogs Thor.", "tokens": ["Wohl", "auf", "die", "Br\u00fc\u00b7cke", "vor", "des", "Her\u00b7zogs", "Thor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.32": {"line.1": {"text": "Er stellt sie vor das Thor wohl auf die Br\u00fccken,", "tokens": ["Er", "stellt", "sie", "vor", "das", "Thor", "wohl", "auf", "die", "Br\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Er kehrt ihnen die Gel\u00e4nder wohl an den R\u00fccken:", "tokens": ["Er", "kehrt", "ih\u00b7nen", "die", "Ge\u00b7l\u00e4n\u00b7der", "wohl", "an", "den", "R\u00fc\u00b7cken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+----+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbnun lieben Br\u00fcder wartet mein hiervor.", "tokens": ["\u00bb", "nun", "lie\u00b7ben", "Br\u00fc\u00b7der", "war\u00b7tet", "mein", "hier\u00b7vor", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADJA", "NN", "VVFIN", "PPOSAT", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.33": {"line.1": {"text": "So will ich gehen zu Herzog Otten grade,", "tokens": ["So", "will", "ich", "ge\u00b7hen", "zu", "Her\u00b7zog", "Ot\u00b7ten", "gra\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "VVFIN", "APPR", "NE", "NE", "ADV", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da\u00df er uns bald mit einer Zell berathe,", "tokens": ["Da\u00df", "er", "uns", "bald", "mit", "ei\u00b7ner", "Zell", "be\u00b7ra\u00b7the", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Darin wollen wir singen grob und klar.", "tokens": ["Da\u00b7rin", "wol\u00b7len", "wir", "sin\u00b7gen", "grob", "und", "klar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}}, "stanza.34": {"line.1": {"text": "Lieber Herzog Otto, ich bin ein Priester worden,", "tokens": ["Lie\u00b7ber", "Her\u00b7zog", "Ot\u00b7to", ",", "ich", "bin", "ein", "Pries\u00b7ter", "wor\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NE", "NE", "$,", "PPER", "VAFIN", "ART", "NN", "VAPP", "$,"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Und habe mir gestiftet selbst einen neuen Orden,", "tokens": ["Und", "ha\u00b7be", "mir", "ge\u00b7stif\u00b7tet", "selbst", "ei\u00b7nen", "neu\u00b7en", "Or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "VVPP", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Drau\u00dfen stehn meine Br\u00fcder all in einer Schaar.", "tokens": ["Drau\u00b7\u00dfen", "stehn", "mei\u00b7ne", "Br\u00fc\u00b7der", "all", "in", "ei\u00b7ner", "Schaar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "PIAT", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.35": {"line.1": {"text": "Nun lieber Herr verleiht ein Zell mir balde,", "tokens": ["Nun", "lie\u00b7ber", "Herr", "ver\u00b7leiht", "ein", "Zell", "mir", "bal\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "VVFIN", "ART", "NN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df man mich f\u00fcr einen rechten Abten halte.\u00ab", "tokens": ["Da\u00df", "man", "mich", "f\u00fcr", "ei\u00b7nen", "rech\u00b7ten", "Ab\u00b7ten", "hal\u00b7te", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PIS", "PRF", "APPR", "ART", "ADJA", "NN", "VVFIN", "$.", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Herr Otto sprach: \u00bbIch hab ein leeren Tempel stahn.", "tokens": ["Herr", "Ot\u00b7to", "sprach", ":", "\u00bb", "Ich", "hab", "ein", "lee\u00b7ren", "Tem\u00b7pel", "stahn", "."], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.36": {"line.1": {"text": "Wohl auf drey S\u00e4ulen ist er weidentlich geschicket,", "tokens": ["Wohl", "auf", "drey", "S\u00e4u\u00b7len", "ist", "er", "wei\u00b7dent\u00b7lich", "ge\u00b7schi\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "CARD", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein offen M\u00fcnster, daraus man weite blicket,", "tokens": ["Ein", "of\u00b7fen", "M\u00fcns\u00b7ter", ",", "da\u00b7raus", "man", "wei\u00b7te", "bli\u00b7cket", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "$,", "PAV", "PIS", "ADJA", "VVFIN", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Darauf mu\u00df Engelmayer sein Amte han.\u00ab", "tokens": ["Da\u00b7rauf", "mu\u00df", "En\u00b7gel\u00b7may\u00b7er", "sein", "Am\u00b7te", "han", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PAV", "VMFIN", "NE", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.37": {"line.1": {"text": "\u00bbach lieber Herr, dort hats kein rechten Schalle,", "tokens": ["\u00bb", "ach", "lie\u00b7ber", "Herr", ",", "dort", "hats", "kein", "rech\u00b7ten", "Schal\u00b7le", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "ADV", "NN", "$,", "ADV", "VAFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Den Br\u00fcdern m\u00f6chte wohl die Stimme fallen,", "tokens": ["Den", "Br\u00fc\u00b7dern", "m\u00f6ch\u00b7te", "wohl", "die", "Stim\u00b7me", "fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und w\u00fcrd dem Abten selbst der Gugelhals zu enge.\u00ab", "tokens": ["Und", "w\u00fcrd", "dem", "Ab\u00b7ten", "selbst", "der", "Gu\u00b7gel\u00b7hals", "zu", "en\u00b7ge", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "ADV", "ART", "NN", "APPR", "ADJA", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.38": {"line.1": {"text": "\u00bbso wei\u00df ich noch ein Chor f\u00fcr deine Knaben,", "tokens": ["\u00bb", "so", "wei\u00df", "ich", "noch", "ein", "Chor", "f\u00fcr", "dei\u00b7ne", "Kna\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da mag ein jeder leicht sein Nothdurft haben,", "tokens": ["Da", "mag", "ein", "je\u00b7der", "leicht", "sein", "Noth\u00b7durft", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "PIS", "ADJD", "PPOSAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und durch die Brillen schauen auf die L\u00e4nge.\u00ab", "tokens": ["Und", "durch", "die", "Bril\u00b7len", "schau\u00b7en", "auf", "die", "L\u00e4n\u00b7ge", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.39": {"line.1": {"text": "Nun hob sich an ein Singen gar ungleiche,", "tokens": ["Nun", "hob", "sich", "an", "ein", "Sin\u00b7gen", "gar", "un\u00b7glei\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "APPR", "ART", "NN", "ADV", "ADJA", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mit gro\u00dfen Scheitern begannen sie sich streichen,", "tokens": ["Mit", "gro\u00b7\u00dfen", "Schei\u00b7tern", "be\u00b7gan\u00b7nen", "sie", "sich", "strei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Herr Otto sprach: \u00bbWir stehen recht sicher weit davon.\u00ab", "tokens": ["Herr", "Ot\u00b7to", "sprach", ":", "\u00bb", "Wir", "ste\u00b7hen", "recht", "si\u00b7cher", "weit", "da\u00b7von", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "NE", "VVFIN", "$.", "$(", "PPER", "VVFIN", "ADV", "ADJD", "ADJD", "PAV", "$.", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.40": {"line.1": {"text": "Der erste sang von Ochsen und von Rindern,", "tokens": ["Der", "ers\u00b7te", "sang", "von", "Och\u00b7sen", "und", "von", "Rin\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der andere sprach und sang von Menschen und von Kindern,", "tokens": ["Der", "an\u00b7de\u00b7re", "sprach", "und", "sang", "von", "Men\u00b7schen", "und", "von", "Kin\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "KON", "VVFIN", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Die machen zu Haus an seines Vaters Thor.", "tokens": ["Die", "ma\u00b7chen", "zu", "Haus", "an", "sei\u00b7nes", "Va\u00b7ters", "Thor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "NN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.41": {"line.1": {"text": "Der dritt der sang: \u00bbNun fahr ich aus dem Lande,", "tokens": ["Der", "dritt", "der", "sang", ":", "\u00bb", "Nun", "fahr", "ich", "aus", "dem", "Lan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "VVFIN", "$.", "$(", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dieses Lasters hab ich immer Schande,", "tokens": ["Die\u00b7ses", "Las\u00b7ters", "hab", "ich", "im\u00b7mer", "Schan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VAFIN", "PPER", "ADV", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Es werden sein die Freunde mein gewahr.\u00ab", "tokens": ["Es", "wer\u00b7den", "sein", "die", "Freun\u00b7de", "mein", "ge\u00b7wahr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ART", "NN", "PPOSAT", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.42": {"line.1": {"text": "Die andern Herrn, genannt die Br\u00fcder Otte,", "tokens": ["Die", "an\u00b7dern", "Herrn", ",", "ge\u00b7nannt", "die", "Br\u00fc\u00b7der", "Ot\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVPP", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Deren einer sang: \u00bbH\u00e4tt ich ein Topf voll Schotten", "tokens": ["De\u00b7ren", "ei\u00b7ner", "sang", ":", "\u00bb", "H\u00e4tt", "ich", "ein", "Topf", "voll", "Schot\u00b7ten"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "VVFIN", "$.", "$(", "VVFIN", "PPER", "ART", "NN", "ADJD", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Von meiner Mutter, ich fr\u00e4\u00df ihn alle gar.\u00ab", "tokens": ["Von", "mei\u00b7ner", "Mut\u00b7ter", ",", "ich", "fr\u00e4\u00df", "ihn", "al\u00b7le", "gar", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "PPER", "VVFIN", "PPER", "PIS", "ADV", "$.", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.43": {"line.1": {"text": "Der Engelmayer sang und zerrt sein Kutten oben:", "tokens": ["Der", "En\u00b7gel\u00b7may\u00b7er", "sang", "und", "zerrt", "sein", "Kut\u00b7ten", "o\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KON", "VVFIN", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "\u00bbder Neithard hat mich in ein Sack geschoben,", "tokens": ["\u00bb", "der", "Neit\u00b7hard", "hat", "mich", "in", "ein", "Sack", "ge\u00b7scho\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NE", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "De\u00df hab ich Schand und Laster immerdar.\u00ab", "tokens": ["De\u00df", "hab", "ich", "Schand", "und", "Las\u00b7ter", "im\u00b7mer\u00b7dar", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "PPER", "NN", "KON", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.44": {"line.1": {"text": "Sie wurden Zornes voll ohn Fressen und ohn Saufen,", "tokens": ["Sie", "wur\u00b7den", "Zor\u00b7nes", "voll", "ohn", "Fres\u00b7sen", "und", "ohn", "Sau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "ADJD", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Begunnten sich einander aus b\u00f6sem Muth zu raufen,", "tokens": ["Be\u00b7gunn\u00b7ten", "sich", "ein\u00b7an\u00b7der", "aus", "b\u00f6\u00b7sem", "Muth", "zu", "rau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PRF", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Und waren doch geschoren ohne Haar.", "tokens": ["Und", "wa\u00b7ren", "doch", "ge\u00b7scho\u00b7ren", "oh\u00b7ne", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "VVPP", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.45": {"line.1": {"text": "Der Herzog sprach: \u00bbNun fertig' sie von hinnen,", "tokens": ["Der", "Her\u00b7zog", "sprach", ":", "\u00bb", "Nun", "fer\u00b7tig'", "sie", "von", "hin\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "ADV", "VVFIN", "PPER", "APPR", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "All mein Hofgesind mu\u00df schier entrinnen,", "tokens": ["All", "mein", "Hof\u00b7ge\u00b7sind", "mu\u00df", "schier", "ent\u00b7rin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "VMFIN", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Es sind gar ungef\u00fcge M\u00f6nch f\u00fcrwahr.\u00ab", "tokens": ["Es", "sind", "gar", "un\u00b7ge\u00b7f\u00fc\u00b7ge", "M\u00f6nch", "f\u00fcr\u00b7wahr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJA", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.46": {"line.1": {"text": "Da rief Herr Neithard vom Fenster nieder:", "tokens": ["Da", "rief", "Herr", "Neit\u00b7hard", "vom", "Fens\u00b7ter", "nie\u00b7der", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "NE", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbverk\u00fcndets aller Welt ihr frommen Br\u00fcder,", "tokens": ["\u00bb", "ver\u00b7k\u00fcn\u00b7dets", "al\u00b7ler", "Welt", "ihr", "from\u00b7men", "Br\u00fc\u00b7der", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "PIAT", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und la\u00dft euch nicht wachsen lauter graue Haar.\u00ab", "tokens": ["Und", "la\u00dft", "euch", "nicht", "wach\u00b7sen", "lau\u00b7ter", "grau\u00b7e", "Haar", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVIMP", "PPER", "PTKNEG", "VVFIN", "PIAT", "ADJA", "NN", "$.", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.47": {"line.1": {"text": "Mit Murren zogen sie wie eine Wetterwolken,", "tokens": ["Mit", "Mur\u00b7ren", "zo\u00b7gen", "sie", "wie", "ei\u00b7ne", "Wet\u00b7ter\u00b7wol\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihre vierbeinicht Schwestern standen ungemolken,", "tokens": ["Ih\u00b7re", "vier\u00b7bei\u00b7nicht", "Schwes\u00b7tern", "stan\u00b7den", "un\u00b7ge\u00b7mol\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "CARD", "NN", "VVFIN", "ADJD", "$,"], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Ohn Urlaubnehmen ward Fluchen nicht gespart.", "tokens": ["Ohn", "Ur\u00b7laub\u00b7neh\u00b7men", "ward", "Flu\u00b7chen", "nicht", "ge\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "NN", "PTKNEG", "VVPP", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.48": {"line.1": {"text": "Sie huben sich zum Thor hinaus zu traben,", "tokens": ["Sie", "hu\u00b7ben", "sich", "zum", "Thor", "hin\u00b7aus", "zu", "tra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPRART", "NN", "APZR", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die alten dummen steifen Ackerknaben,", "tokens": ["Die", "al\u00b7ten", "dum\u00b7men", "stei\u00b7fen", "A\u00b7cker\u00b7kna\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Tanzten in ihren langen Kutten", "tokens": ["Tanz\u00b7ten", "in", "ih\u00b7ren", "lan\u00b7gen", "Kut\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Wie Winzer in den Butten,", "tokens": ["Wie", "Win\u00b7zer", "in", "den", "But\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Darnach warens Bauren hinten nach wie vor.", "tokens": ["Dar\u00b7nach", "wa\u00b7rens", "Bau\u00b7ren", "hin\u00b7ten", "nach", "wie", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJA", "NN", "ADV", "APPR", "KOKOM", "APPR", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}}}}