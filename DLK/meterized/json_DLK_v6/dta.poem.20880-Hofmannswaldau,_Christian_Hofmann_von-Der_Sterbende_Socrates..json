{"dta.poem.20880": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "Der  \n Sterbende  \n  Socrates.", "genre": "Lyrik; Prosa; Drama", "period": "N.A.", "pub_year": "1679", "urn": "urn:nbn:de:kobv:b4-20289-1", "language": ["de:0.99"], "booktitle": "Hofmann von Hofmannswaldau, Christian: Deutsche Ubersetzungen und Gedichte. Breslau, 1679."}, "poem": {"stanza.1": {"line.1": {"text": "Das ungemeine Ma\u00df der Leiber", "tokens": ["Das", "un\u00b7ge\u00b7mei\u00b7ne", "Ma\u00df", "der", "Lei\u00b7ber"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist bey den Menschen ungemein/", "tokens": ["Ist", "bey", "den", "Men\u00b7schen", "un\u00b7ge\u00b7mein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es melden der Geschichten Schreiber/", "tokens": ["Es", "mel\u00b7den", "der", "Ge\u00b7schich\u00b7ten", "Schrei\u00b7ber", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df wenig Zwerg und Riesen seyn.", "tokens": ["Da\u00df", "we\u00b7nig", "Zwerg", "und", "Rie\u00b7sen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "KON", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Kein Bruder kan sich Castorn gleichen/", "tokens": ["Kein", "Bru\u00b7der", "kan", "sich", "Cas\u00b7torn", "glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PRF", "NE", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wer d\u00e5mpfft doch Adons Sch\u00f6nheit Licht?", "tokens": ["Wer", "d\u00e5mpfft", "doch", "A\u00b7dons", "Sch\u00f6n\u00b7heit", "Licht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "NE", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Silen darff keinem S\u00e4uffer weichen/", "tokens": ["Si\u00b7len", "darff", "kei\u00b7nem", "S\u00e4uf\u00b7fer", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PIAT", "NN", "VVINF", "$("], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "Und Nestor keinem Weisen nicht.", "tokens": ["Und", "Nes\u00b7tor", "kei\u00b7nem", "Wei\u00b7sen", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PIAT", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Kein Hund hat Cerbers Krafft und St\u00e4rcke/", "tokens": ["Kein", "Hund", "hat", "Cer\u00b7bers", "Krafft", "und", "St\u00e4r\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "NE", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Kein Flu\u00df hat Acherontens Art/", "tokens": ["Kein", "Flu\u00df", "hat", "A\u00b7cher\u00b7on\u00b7tens", "Art", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "NE", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Kein Weib verricht Magerens Wercke/", "tokens": ["Kein", "Weib", "ver\u00b7richt", "Ma\u00b7ge\u00b7rens", "Wer\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "NE", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Kein Schiffer Charons bleiche Fahrt.", "tokens": ["Kein", "Schif\u00b7fer", "Cha\u00b7rons", "blei\u00b7che", "Fahrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Was trotzt Amintens sch\u00f6ne Wangen/", "tokens": ["Was", "trotzt", "A\u00b7min\u00b7tens", "sch\u00f6\u00b7ne", "Wan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NE", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Was trifft der Sonnen Klarheit zu/", "tokens": ["Was", "trifft", "der", "Son\u00b7nen", "Klar\u00b7heit", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "NN", "PTKZU", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Was f\u00fchrt der Rosen-gleiches Prangen?", "tokens": ["Was", "f\u00fchrt", "der", "Ro\u00b7sen\u00b7glei\u00b7ches", "Pran\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Was ist so s\u00fc\u00df als Schlaff und Ruh/", "tokens": ["Was", "ist", "so", "s\u00fc\u00df", "als", "Schlaff", "und", "Ruh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADJD", "KOKOM", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Was gleichet sich des Donners Knallen/", "tokens": ["Was", "glei\u00b7chet", "sich", "des", "Don\u00b7ners", "Knal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PRF", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Wo ist ein Berg wie Pelion?", "tokens": ["Wo", "ist", "ein", "Berg", "wie", "Pe\u00b7li\u00b7on", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "KOKOM", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Und von den starcken Thieren allen/", "tokens": ["Und", "von", "den", "star\u00b7cken", "Thie\u00b7ren", "al\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "PIAT", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "F\u00fchrt Leuenkrafft den Prei\u00df davon.", "tokens": ["F\u00fchrt", "Leu\u00b7en\u00b7krafft", "den", "Prei\u00df", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Sehr gro\u00df Gel\u00fcck und gute Tage/", "tokens": ["Sehr", "gro\u00df", "Ge\u00b7l\u00fcck", "und", "gu\u00b7te", "Ta\u00b7ge", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Wie auch sehr grosse Noth und Pein/", "tokens": ["Wie", "auch", "sehr", "gros\u00b7se", "Noth", "und", "Pein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "ADJA", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Die werden nach der Weisen Sage", "tokens": ["Die", "wer\u00b7den", "nach", "der", "Wei\u00b7sen", "Sa\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Stets ungemein und seltsam seyn.", "tokens": ["Stets", "un\u00b7ge\u00b7mein", "und", "selt\u00b7sam", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KON", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}