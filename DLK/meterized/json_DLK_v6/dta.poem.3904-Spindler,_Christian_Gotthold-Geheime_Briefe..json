{"dta.poem.3904": {"metadata": {"author": {"name": "Spindler, Christian Gotthold", "birth": "N.A.", "death": "N.A."}, "title": "Geheime Briefe.", "genre": "Lyrik, Drama", "period": "N.A.", "pub_year": "1745", "urn": "urn:nbn:de:kobv:b4-20581-9", "language": ["de:0.99"], "booktitle": "Spindler, Christian Gotthold: Unschuldige Jugend-Fr\u00fcchte. Leipzig, 1745."}, "poem": {"stanza.1": {"line.1": {"text": "Entbieten unsern Gru\u00df und Gnade denen allen,", "tokens": ["Ent\u00b7bie\u00b7ten", "un\u00b7sern", "Gru\u00df", "und", "Gna\u00b7de", "de\u00b7nen", "al\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "KON", "NN", "PRELS", "PIS", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In F\u00fcrst- und Grafen-Stand, Pr\u00e4laten und Va-", "tokens": ["In", "F\u00fcr\u00b7st", "und", "Gra\u00b7fen\u00b7Stand", ",", "Pr\u00e4\u00b7la\u00b7ten", "und", "Va"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "TRUNC", "KON", "NN", "$,", "NN", "KON", "TRUNC"], "meter": "--+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "sallen,", "tokens": ["sal\u00b7len", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Wes Standes sie auch sind, wie auch der Ritter-", "tokens": ["Wes", "Stan\u00b7des", "sie", "auch", "sind", ",", "wie", "auch", "der", "Rit\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "PPER", "ADV", "VAFIN", "$,", "PWAV", "ADV", "ART", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "schafft,", "tokens": ["schafft", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Auch wer uns fernerweit mit Pflicht u. Lehn verhafft;", "tokens": ["Auch", "wer", "uns", "fer\u00b7ner\u00b7weit", "mit", "Pflicht", "u.", "Lehn", "ver\u00b7hafft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation", "word", "word", "punct"], "pos": ["ADV", "PWS", "PPER", "ADV", "APPR", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.7": {"text": "Und f\u00fcgen m\u00e4nniglich durch di\u00df ", "tokens": ["Und", "f\u00fc\u00b7gen", "m\u00e4n\u00b7nig\u00b7lich", "durch", "di\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "PDS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df itzt viel Jrrungen im Lieben eingerissen,", "tokens": ["Da\u00df", "itzt", "viel", "Jr\u00b7run\u00b7gen", "im", "Lie\u00b7ben", "ein\u00b7ge\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "NN", "APPRART", "ADJA", "VVPP", "$,"], "meter": "-+-++--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Wor\u00fcber sonderlich das Man\u0303svolck sich beschwert,", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "son\u00b7der\u00b7lich", "das", "Ma\u00f1s\u00b7volck", "sich", "be\u00b7schwert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ART", "NN", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Weil manches M\u00e4gdgen offt zu viele M\u00fch begehrt,", "tokens": ["Weil", "man\u00b7ches", "M\u00e4gd\u00b7gen", "offt", "zu", "vie\u00b7le", "M\u00fch", "be\u00b7gehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ADV", "APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Auch manch verliebter Tropff sich gar zu hoch ver-", "tokens": ["Auch", "manch", "ver\u00b7lieb\u00b7ter", "Tropff", "sich", "gar", "zu", "hoch", "ver"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PIAT", "ADJA", "NN", "PRF", "ADV", "PTKA", "ADJD", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "steigt,", "tokens": ["steigt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.13": {"text": "Und vor ein M\u00e4gdgen sich als eine G\u00f6ttin beugt.", "tokens": ["Und", "vor", "ein", "M\u00e4gd\u00b7gen", "sich", "als", "ei\u00b7ne", "G\u00f6t\u00b7tin", "beugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "PRF", "KOUS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Dadurch wird das Geschlecht der M\u00e4nner schlecht", "tokens": ["Da\u00b7durch", "wird", "das", "Ge\u00b7schlecht", "der", "M\u00e4n\u00b7ner", "schlecht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "ART", "NN", "ART", "NN", "VVFIN"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.15": {"text": "geacht,", "tokens": ["ge\u00b7acht", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.16": {"text": "Ja wohl zum ", "tokens": ["Ja", "wohl", "zum"], "token_info": ["word", "word", "word"], "pos": ["PTKANT", "ADV", "APPRART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.17": {"text": "Dieweil wir aber nicht die Unart leiden wollen,", "tokens": ["Die\u00b7weil", "wir", "a\u00b7ber", "nicht", "die", "Un\u00b7art", "lei\u00b7den", "wol\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PTKNEG", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Da\u00df die, so Freyen gehn, sich selbst verschleudern", "tokens": ["Da\u00df", "die", ",", "so", "Frey\u00b7en", "gehn", ",", "sich", "selbst", "ver\u00b7schleu\u00b7dern"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "ART", "$,", "ADV", "NN", "VVINF", "$,", "PRF", "ADV", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "sollen;", "tokens": ["sol\u00b7len", ";"], "token_info": ["word", "punct"], "pos": ["VMFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.20": {"text": "Als haben wir die ", "tokens": ["Als", "ha\u00b7ben", "wir", "die"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "VAFIN", "PPER", "ART"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.21": {"text": "In unserm gantzen Reich von neuen ausgeschickt.", "tokens": ["In", "un\u00b7serm", "gant\u00b7zen", "Reich", "von", "neu\u00b7en", "aus\u00b7ge\u00b7schickt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "APPR", "ADJA", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Befehlen euch demnach derselben nachzuleben,", "tokens": ["Be\u00b7feh\u00b7len", "euch", "dem\u00b7nach", "der\u00b7sel\u00b7ben", "nach\u00b7zu\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PAV", "PDAT", "VVIZU", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Und wer hierinnen wird mit Willen widerstreben,", "tokens": ["Und", "wer", "hie\u00b7rin\u00b7nen", "wird", "mit", "Wil\u00b7len", "wi\u00b7der\u00b7stre\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PAV", "VAFIN", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Dem wird nach Unterscheid der Sachen Wichtigkeit", "tokens": ["Dem", "wird", "nach", "Un\u00b7ter\u00b7scheid", "der", "Sa\u00b7chen", "Wich\u00b7tig\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Ohn Ansehn der Person, Bestraffung angedeut.", "tokens": ["Ohn", "An\u00b7sehn", "der", "Per\u00b7son", ",", "Be\u00b7straf\u00b7fung", "an\u00b7ge\u00b7deut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "$,", "NN", "VVPP", "$."], "meter": "-+--++-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "Das alles habet ihr getreulich zu erf\u00fcllen,", "tokens": ["Das", "al\u00b7les", "ha\u00b7bet", "ihr", "ge\u00b7treu\u00b7lich", "zu", "er\u00b7f\u00fcl\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "PPER", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Und so geschicht dadurch nach unsern ernsten Willen.", "tokens": ["Und", "so", "ge\u00b7schicht", "da\u00b7durch", "nach", "un\u00b7sern", "erns\u00b7ten", "Wil\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "PAV", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Gegeben Liebenthal, in unsrer Residentz,", "tokens": ["Ge\u00b7ge\u00b7ben", "Lie\u00b7bent\u00b7hal", ",", "in", "uns\u00b7rer", "Re\u00b7si\u00b7dentz", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Nebst Cantzeley-", "tokens": ["Nebst", "Cant\u00b7ze\u00b7ley"], "token_info": ["word", "word"], "pos": ["APPR", "TRUNC"], "meter": "-+-+", "measure": "iambic.di"}, "line.30": {"text": "Cupido Petersquenz.", "tokens": ["Cu\u00b7pi\u00b7do", "Pe\u00b7ters\u00b7quenz", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "+--+--", "measure": "dactylic.di.plus"}}}}}