{"textgrid.poem.26386": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[aber zu Haus Frau K\u00f6nigin]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Aber zu Haus Frau K\u00f6nigin", "tokens": ["A\u00b7ber", "zu", "Haus", "Frau", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "NN", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sa\u00df wie ein Geist im Zimmer drin.", "tokens": ["Sa\u00df", "wie", "ein", "Geist", "im", "Zim\u00b7mer", "drin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sie sprach: \u00bbGlaub nur, da\u00df l\u00e4ngst ich's wei\u00df,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Glaub", "nur", ",", "da\u00df", "l\u00e4ngst", "ich's", "wei\u00df", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "ADV", "$,", "KOUS", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und darum schwitz' ich Todesschwei\u00df.", "tokens": ["Und", "da\u00b7rum", "schwitz'", "ich", "To\u00b7des\u00b7schwei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Du bist mir heut untreu gewesen,", "tokens": ["Du", "bist", "mir", "heut", "un\u00b7treu", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ADJD", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und davon werd' ich nie genesen.", "tokens": ["Und", "da\u00b7von", "werd'", "ich", "nie", "ge\u00b7ne\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich konnte es durch W\u00e4nde sehn,", "tokens": ["Ich", "konn\u00b7te", "es", "durch", "W\u00e4n\u00b7de", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Du tatst gestohlne Wege gehn.", "tokens": ["Du", "tatst", "ge\u00b7stohl\u00b7ne", "We\u00b7ge", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Da sieh, ich hab' von Folterqual", "tokens": ["Da", "sieh", ",", "ich", "hab'", "von", "Fol\u00b7ter\u00b7qual"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "PPER", "VAFIN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An jeder Hand ein N\u00e4gelmal.", "tokens": ["An", "je\u00b7der", "Hand", "ein", "N\u00e4\u00b7gel\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Als ich das Dunkel taghell fand,", "tokens": ["Als", "ich", "das", "Dun\u00b7kel", "ta\u00b7ghell", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dr\u00fcckt' ich die N\u00e4gel in die Hand.\u00ab", "tokens": ["Dr\u00fcckt'", "ich", "die", "N\u00e4\u00b7gel", "in", "die", "Hand", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Und kugelnd, wie die Eier rollen,", "tokens": ["Und", "ku\u00b7gelnd", ",", "wie", "die", "Ei\u00b7er", "rol\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "PWAV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sind uns die Tr\u00e4nen schnell entquollen.", "tokens": ["Sind", "uns", "die", "Tr\u00e4\u00b7nen", "schnell", "ent\u00b7quol\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Wir lie\u00dfen ihnen flotten Lauf,", "tokens": ["Wir", "lie\u00b7\u00dfen", "ih\u00b7nen", "flot\u00b7ten", "Lauf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eins hob sie dem andern auf.", "tokens": ["Und", "eins", "hob", "sie", "dem", "an\u00b7dern", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PPER", "ART", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Noch liefen vier gesalzte Fl\u00fcsse,", "tokens": ["Noch", "lie\u00b7fen", "vier", "ge\u00b7salz\u00b7te", "Fl\u00fcs\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "CARD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da fanden wir schon alte K\u00fcsse,", "tokens": ["Da", "fan\u00b7den", "wir", "schon", "al\u00b7te", "K\u00fcs\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Tanzten wie die geheilten Lahmen,", "tokens": ["Tanz\u00b7ten", "wie", "die", "ge\u00b7heil\u00b7ten", "Lah\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Die sich vom Herz die Kr\u00fccken nahmen.", "tokens": ["Die", "sich", "vom", "Herz", "die", "Kr\u00fc\u00b7cken", "nah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPRART", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Ich sank zum Ku\u00df auf ihre Hand", "tokens": ["Ich", "sank", "zum", "Ku\u00df", "auf", "ih\u00b7re", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und hab' das N\u00e4gelmal erkannt.", "tokens": ["Und", "hab'", "das", "N\u00e4\u00b7gel\u00b7mal", "er\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Ist es, als wenn die Steine klagen,", "tokens": ["Ist", "es", ",", "als", "wenn", "die", "Stei\u00b7ne", "kla\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "$,", "KOKOM", "KOUS", "ART", "NN", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Dann traut man sich nichts mehr zu sagen.", "tokens": ["Dann", "traut", "man", "sich", "nichts", "mehr", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "PIS", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Tat wie geschlachtet tief err\u00f6ten,", "tokens": ["Tat", "wie", "ge\u00b7schlach\u00b7tet", "tief", "er\u00b7r\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "VVPP", "ADJD", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "W\u00fcnschend, man m\u00f6g' mich schleunigst t\u00f6ten.", "tokens": ["W\u00fcn\u00b7schend", ",", "man", "m\u00f6g'", "mich", "schleu\u00b7nigst", "t\u00f6\u00b7ten", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PIS", "VMFIN", "PPER", "VVFIN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.14": {"line.1": {"text": "Pl\u00f6tzlich sah sie die leere Hand,", "tokens": ["Pl\u00f6tz\u00b7lich", "sah", "sie", "die", "lee\u00b7re", "Hand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Den Finger mit dem wei\u00dfen Rand,", "tokens": ["Den", "Fin\u00b7ger", "mit", "dem", "wei\u00b7\u00dfen", "Rand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Dort, wo mein Ring vorher gesessen,", "tokens": ["Dort", ",", "wo", "mein", "Ring", "vor\u00b7her", "ge\u00b7ses\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie sprach: \u00bbHat sich der Ring vergessen?\u00ab", "tokens": ["Sie", "sprach", ":", "\u00bb", "Hat", "sich", "der", "Ring", "ver\u00b7ges\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "VAFIN", "PRF", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Erkl\u00e4ren tat ich nur ein Wort, \u2013", "tokens": ["Er\u00b7kl\u00e4\u00b7ren", "tat", "ich", "nur", "ein", "Wort", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da flog ihr Ring vom Finger fort.", "tokens": ["Da", "flog", "ihr", "Ring", "vom", "Fin\u00b7ger", "fort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Sie sprang und stampfte auf das Gold,", "tokens": ["Sie", "sprang", "und", "stampf\u00b7te", "auf", "das", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als wenn sie sich zertreten wollt'.", "tokens": ["Als", "wenn", "sie", "sich", "zer\u00b7tre\u00b7ten", "wollt'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Sie flog zur Luft mit hundert Armen,", "tokens": ["Sie", "flog", "zur", "Luft", "mit", "hun\u00b7dert", "Ar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rauste ihr Haar wild zum Erbarmen,", "tokens": ["Raus\u00b7te", "ihr", "Haar", "wild", "zum", "Er\u00b7bar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "ADJD", "APPRART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.19": {"line.1": {"text": "Und es erschien mir voll Entsetzen,", "tokens": ["Und", "es", "er\u00b7schien", "mir", "voll", "Ent\u00b7set\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als riss' sie sich in kleine Fetzen.", "tokens": ["Als", "riss'", "sie", "sich", "in", "klei\u00b7ne", "Fet\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PPER", "PRF", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Ich rief: \u00bbAch, alles ist vorbei,", "tokens": ["Ich", "rief", ":", "\u00bb", "Ach", ",", "al\u00b7les", "ist", "vor\u00b7bei", ","], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "ITJ", "$,", "PIS", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geh du nur selbst mir nicht entzwei!", "tokens": ["Geh", "du", "nur", "selbst", "mir", "nicht", "ent\u00b7zwei", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "PPER", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Dein Schmerz tat meine S\u00fcnde richten,", "tokens": ["Dein", "Schmerz", "tat", "mei\u00b7ne", "S\u00fcn\u00b7de", "rich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich werd' auf Fortsetzung verzichten.", "tokens": ["Ich", "werd'", "auf", "Fort\u00b7set\u00b7zung", "ver\u00b7zich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.22": {"line.1": {"text": "Die ganze Welt sei jetzt vergessen,", "tokens": ["Die", "gan\u00b7ze", "Welt", "sei", "jetzt", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und treu wird stets zu Haus gesessen.", "tokens": ["Und", "treu", "wird", "stets", "zu", "Haus", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Ich habe mich so lang gewehrt,", "tokens": ["Ich", "ha\u00b7be", "mich", "so", "lang", "ge\u00b7wehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sehnsucht jedoch um nichts sich schert.", "tokens": ["Sehn\u00b7sucht", "je\u00b7doch", "um", "nichts", "sich", "schert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PIS", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Das Mohrle liebte ich ganz stumm,", "tokens": ["Das", "Mohr\u00b7le", "lieb\u00b7te", "ich", "ganz", "stumm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich lieb' euch beid', das bringt mich um.\u00ab", "tokens": ["Ich", "lieb'", "euch", "beid'", ",", "das", "bringt", "mich", "um", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "$,", "PDS", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Da nahte sie sich auf den Zehen,", "tokens": ["Da", "nah\u00b7te", "sie", "sich", "auf", "den", "Ze\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie L\u00f6winnen auf W\u00fcsten gehen.", "tokens": ["Wie", "L\u00f6\u00b7win\u00b7nen", "auf", "W\u00fcs\u00b7ten", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPR", "NN", "VVINF", "$."], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.26": {"line.1": {"text": "Sie sprach: \u00bbUnd das nennst du verzichten?", "tokens": ["Sie", "sprach", ":", "\u00bb", "Und", "das", "nennst", "du", "ver\u00b7zich\u00b7ten", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "KON", "PDS", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Gott m\u00f6ge dich und sie vernichten!\u00ab", "tokens": ["Gott", "m\u00f6\u00b7ge", "dich", "und", "sie", "ver\u00b7nich\u00b7ten", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VMFIN", "PPER", "KON", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Ich rief: \u00bbGott straf' mich auf dem Sitz,", "tokens": ["Ich", "rief", ":", "\u00bb", "Gott", "straf'", "mich", "auf", "dem", "Sitz", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tu Unrecht ich, Gott send' den Blitz!\u00ab", "tokens": ["Tu", "Un\u00b7recht", "ich", ",", "Gott", "send'", "den", "Blitz", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "NN", "PPER", "$,", "NN", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "\u00bbach nein,\u00ab rief pl\u00f6tzlich da mein Weib", "tokens": ["\u00bb", "ach", "nein", ",", "\u00ab", "rief", "pl\u00f6tz\u00b7lich", "da", "mein", "Weib"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "XY", "PTKANT", "$,", "$(", "VVFIN", "ADJD", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und warf sich \u00fcber meinen Leib,", "tokens": ["Und", "warf", "sich", "\u00fc\u00b7ber", "mei\u00b7nen", "Leib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "\u00bbkommt dir der Blitz, sterb' ich mit dir,", "tokens": ["\u00bb", "kommt", "dir", "der", "Blitz", ",", "sterb'", "ich", "mit", "dir", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "ART", "NN", "$,", "VVFIN", "PPER", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich bleibe nicht alleine hier.", "tokens": ["Ich", "blei\u00b7be", "nicht", "al\u00b7lei\u00b7ne", "hier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Ich wei\u00df, dein Herz ist immer rein,", "tokens": ["Ich", "wei\u00df", ",", "dein", "Herz", "ist", "im\u00b7mer", "rein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00df anders nur als andre sein.", "tokens": ["Mu\u00df", "an\u00b7ders", "nur", "als", "and\u00b7re", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "KOUS", "PIS", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Was schlecht ist, wenns ein andrer tut,", "tokens": ["Was", "schlecht", "ist", ",", "wenns", "ein", "an\u00b7drer", "tut", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$,", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn du es tust, dann ist es gut.\u00ab", "tokens": ["Wenn", "du", "es", "tust", ",", "dann", "ist", "es", "gut", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVFIN", "$,", "ADV", "VAFIN", "PPER", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Die Tr\u00e4nen liefen um uns rund,", "tokens": ["Die", "Tr\u00e4\u00b7nen", "lie\u00b7fen", "um", "uns", "rund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir weinten eine lange Stund',", "tokens": ["Wir", "wein\u00b7ten", "ei\u00b7ne", "lan\u00b7ge", "Stund'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Und endlich sahn wir beide ein,", "tokens": ["Und", "end\u00b7lich", "sahn", "wir", "bei\u00b7de", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie einer ist, so mu\u00df er sein.", "tokens": ["Wie", "ei\u00b7ner", "ist", ",", "so", "mu\u00df", "er", "sein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VAFIN", "$,", "ADV", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}