{"textgrid.poem.42883": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Was die Irre sprach", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir armen Schizophrenen!", "tokens": ["Wir", "ar\u00b7men", "Schi\u00b7zop\u00b7hre\u00b7nen", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Wir sind nur ein Begriff.", "tokens": ["Wir", "sind", "nur", "ein", "Be\u00b7griff", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wir lassen uns endlos dehnen.", "tokens": ["Wir", "las\u00b7sen", "uns", "end\u00b7los", "deh\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Aber es war ein englisches Schiff.", "tokens": ["A\u00b7ber", "es", "war", "ein", "eng\u00b7li\u00b7sches", "Schiff", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "Ich wei\u00df, Sie m\u00f6chten was fragen;", "tokens": ["Ich", "wei\u00df", ",", "Sie", "m\u00f6ch\u00b7ten", "was", "fra\u00b7gen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Seien sie ruhig ganz streng zu mir.", "tokens": ["Sei\u00b7en", "sie", "ru\u00b7hig", "ganz", "streng", "zu", "mir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "ADJD", "APPR", "PPER", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Sie sind nur gl\u00fccklich, und ein Tier \u2013", "tokens": ["Sie", "sind", "nur", "gl\u00fcck\u00b7lich", ",", "und", "ein", "Tier", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "KON", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df man treten und schlagen.", "tokens": ["Mu\u00df", "man", "tre\u00b7ten", "und", "schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "VVINF", "KON", "VVINF", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.3": {"line.1": {"text": "Die Blicke sind selbstverst\u00e4ndlich", "tokens": ["Die", "Bli\u00b7cke", "sind", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADJD"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Bei Kapit\u00e4nen Befehle.", "tokens": ["Bei", "Ka\u00b7pi\u00b7t\u00e4\u00b7nen", "Be\u00b7feh\u00b7le", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich habe auch Eure Seele,", "tokens": ["Ich", "ha\u00b7be", "auch", "Eu\u00b7re", "See\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Aber \u2013 die Schwester l\u00fcgt. Sie l\u00fcgt sch\u00e4ndlich.", "tokens": ["A\u00b7ber", "\u2013", "die", "Schwes\u00b7ter", "l\u00fcgt", ".", "Sie", "l\u00fcgt", "sch\u00e4nd\u00b7lich", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$(", "ART", "NN", "VVFIN", "$.", "PPER", "VVFIN", "ADJD", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.4": {"line.1": {"text": "Vielleicht ist Hingeben Schande.", "tokens": ["Viel\u00b7leicht", "ist", "Hin\u00b7ge\u00b7ben", "Schan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kein Tier wei\u00df, was es redlich tut.", "tokens": ["Kein", "Tier", "wei\u00df", ",", "was", "es", "red\u00b7lich", "tut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$,", "PWS", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So wahr er tausend Meter vom Lande \u2013", "tokens": ["So", "wahr", "er", "tau\u00b7send", "Me\u00b7ter", "vom", "Lan\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "CARD", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Amen \u2013 im Wasser ruht.", "tokens": ["A\u00b7men", "\u2013", "im", "Was\u00b7ser", "ruht", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "APPRART", "NN", "VVFIN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.5": {"line.1": {"text": "Nein danke! Ich bin nicht m\u00fcde.", "tokens": ["Nein", "dan\u00b7ke", "!", "Ich", "bin", "nicht", "m\u00fc\u00b7de", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VVFIN", "$.", "PPER", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Oder spreche ich Ihnen zu viel? \u2013", "tokens": ["O\u00b7der", "spre\u00b7che", "ich", "Ih\u00b7nen", "zu", "viel", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPER", "PTKA", "PIS", "$.", "$("], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Die Quintessenz der G\u00fcte", "tokens": ["Die", "Quin\u00b7tes\u00b7senz", "der", "G\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Liegt schlie\u00dflich nicht im Peitschenstiel.", "tokens": ["Liegt", "schlie\u00df\u00b7lich", "nicht", "im", "Peit\u00b7schen\u00b7stiel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKNEG", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Er hebt oder senkt die Bl\u00fcte. \u2013", "tokens": ["Er", "hebt", "o\u00b7der", "senkt", "die", "Bl\u00fc\u00b7te", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Nun aber genug im grausamen Spiel.", "tokens": ["Nun", "a\u00b7ber", "ge\u00b7nug", "im", "grau\u00b7sa\u00b7men", "Spiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Sie haben doch recht! Ich bin m\u00fcde.", "tokens": ["Sie", "ha\u00b7ben", "doch", "recht", "!", "Ich", "bin", "m\u00fc\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$.", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Living or dead \u2013 Mir riecht sich das gleich.", "tokens": ["Li\u00b7ving", "or", "de\u00b7ad", "\u2013", "Mir", "riecht", "sich", "das", "gleich", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$(", "PPER", "VVFIN", "PRF", "PDS", "ADV", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Aber w\u00e4ren Sie englisch ersoffen,", "tokens": ["A\u00b7ber", "w\u00e4\u00b7ren", "Sie", "eng\u00b7lisch", "er\u00b7sof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Sie k\u00e4men vielleicht auch ins Himmelreich. \u2013", "tokens": ["Sie", "k\u00e4\u00b7men", "viel\u00b7leicht", "auch", "ins", "Him\u00b7mel\u00b7reich", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Amen. \u2013 Wir wollen es hoffen. \u2013", "tokens": ["A\u00b7men", ".", "\u2013", "Wir", "wol\u00b7len", "es", "hof\u00b7fen", ".", "\u2013"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "$(", "PPER", "VMFIN", "PPER", "VVINF", "$.", "$("], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.5": {"text": "Jetzt ist er zum ersten Male weich.", "tokens": ["Jetzt", "ist", "er", "zum", "ers\u00b7ten", "Ma\u00b7le", "weich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Sehen sie nur: Wie der Oberarzt schaut!", "tokens": ["Se\u00b7hen", "sie", "nur", ":", "Wie", "der", "O\u00b7be\u00b7rarzt", "schaut", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$.", "PWAV", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Er soll viel strenger zu mir sein.", "tokens": ["Er", "soll", "viel", "stren\u00b7ger", "zu", "mir", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "APPR", "PPER", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich bin doch allein. Weil ich ein Schwein", "tokens": ["Ich", "bin", "doch", "al\u00b7lein", ".", "Weil", "ich", "ein", "Schwein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$.", "KOUS", "PPER", "ART", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Bin. Ich bin eine Seemannsbraut", "tokens": ["Bin", ".", "Ich", "bin", "ei\u00b7ne", "See\u00b7manns\u00b7braut"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Tausend Meter vom Lande. \u2013", "tokens": ["Tau\u00b7send", "Me\u00b7ter", "vom", "Lan\u00b7de", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["CARD", "NN", "APPRART", "NN", "$.", "$("], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.6": {"text": "Die Schwester h\u00e4lt das f\u00fcr Schande.", "tokens": ["Die", "Schwes\u00b7ter", "h\u00e4lt", "das", "f\u00fcr", "Schan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PDS", "APPR", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Ihr schmutziges Volk! Euer Captain ist fort. \u2013", "tokens": ["Ihr", "schmut\u00b7zi\u00b7ges", "Volk", "!", "Eu\u00b7er", "Cap\u00b7ta\u00b7in", "ist", "fort", ".", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$.", "PPOSAT", "NN", "VAFIN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nie wieder die Stiefel lecken mu\u00df.", "tokens": ["Nie", "wie\u00b7der", "die", "Stie\u00b7fel", "le\u00b7cken", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ja, f\u00fchrt mich hinaus! Wir treffen uns dort. \u2013", "tokens": ["Ja", ",", "f\u00fchrt", "mich", "hin\u00b7aus", "!", "Wir", "tref\u00b7fen", "uns", "dort", ".", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKANT", "$,", "VVFIN", "PPER", "PTKVZ", "$.", "PPER", "VVFIN", "PPER", "ADV", "$.", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wo Anfang ist, da ist auch ein Schlu\u00df.", "tokens": ["Wo", "An\u00b7fang", "ist", ",", "da", "ist", "auch", "ein", "Schlu\u00df", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "VAFIN", "$,", "ADV", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Weil Ihr uns um unser freieres Sehnen", "tokens": ["Weil", "Ihr", "uns", "um", "un\u00b7ser", "frei\u00b7e\u00b7res", "Seh\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Beneidet. \u2013 Hier fragt sich: Wer f\u00fchrt das Wort?", "tokens": ["Be\u00b7nei\u00b7det", ".", "\u2013", "Hier", "fragt", "sich", ":", "Wer", "f\u00fchrt", "das", "Wort", "?"], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "$(", "ADV", "VVFIN", "PRF", "$.", "PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Ihr armen Schizophrenen.", "tokens": ["Ihr", "ar\u00b7men", "Schi\u00b7zop\u00b7hre\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}