{"dta.poem.9164": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "V.  \n Kleine Leute sind so gut als die Grossen.", "genre": "Lyrik; Drama; Prosa", "period": "N.A.", "pub_year": "1701", "urn": "urn:nbn:de:kobv:b4-25043-0", "language": ["de:0.99"], "booktitle": "Weise, Christian: \u00dcberfl\u00fc\u00dfige Gedancken Der gr\u00fcnenden jugend. Leipzig, 1701."}, "poem": {"stanza.1": {"line.1": {"text": "Jhr leute wolt ihr meiner lachen", "tokens": ["Ihr", "leu\u00b7te", "wolt", "ihr", "mei\u00b7ner", "la\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VMFIN", "PPER", "PPOSAT", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich ein bi\u00dfgen kleine bin/", "tokens": ["Da\u00df", "ich", "ein", "bi\u00df\u00b7gen", "klei\u00b7ne", "bin", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "ADJA", "VAFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So wil ich euch zuschanden machen/", "tokens": ["So", "wil", "ich", "euch", "zu\u00b7schan\u00b7den", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich dencke lachet immer hin/", "tokens": ["Ich", "den\u00b7cke", "la\u00b7chet", "im\u00b7mer", "hin", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVFIN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie offte sieht ein kleines hau\u00df", "tokens": ["Wie", "off\u00b7te", "sieht", "ein", "klei\u00b7nes", "hau\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Viel sch\u00f6ner als ein grosses aus.", "tokens": ["Viel", "sch\u00f6\u00b7ner", "als", "ein", "gros\u00b7ses", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ART", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "2. Die kleinen zobeln kommen h\u00f6her", "tokens": ["Die", "klei\u00b7nen", "zo\u00b7beln", "kom\u00b7men", "h\u00f6\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als eine grosse beeren-haut/", "tokens": ["Als", "ei\u00b7ne", "gros\u00b7se", "bee\u00b7ren\u00b7haut", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein mandel-n\u00fc\u00dfgen isst man eher", "tokens": ["Ein", "man\u00b7del\u00b7n\u00fc\u00df\u00b7gen", "isst", "man", "e\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PIS", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als eine sch\u00fcssel sauer kraut/", "tokens": ["Als", "ei\u00b7ne", "sch\u00fcs\u00b7sel", "sau\u00b7er", "kraut", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADJD", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und weil die kleinen lerchen gehn", "tokens": ["Und", "weil", "die", "klei\u00b7nen", "ler\u00b7chen", "gehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So l\u00e4st man wohl das rindfleisch stehn.", "tokens": ["So", "l\u00e4st", "man", "wohl", "das", "rind\u00b7fleisch", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "ART", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "3. Ein kleines mode-h\u00fctgen stutzet", "tokens": ["Ein", "klei\u00b7nes", "mo\u00b7de\u00b7h\u00fct\u00b7gen", "stut\u00b7zet"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mehr als ein babilonscher thurm/", "tokens": ["Mehr", "als", "ein", "ba\u00b7bi\u00b7lon\u00b7scher", "thurm", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "KOKOM", "ART", "ADJA", "NN", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Ein kleines seidenw\u00fcrmgen nutzet", "tokens": ["Ein", "klei\u00b7nes", "sei\u00b7den\u00b7w\u00fcrm\u00b7gen", "nut\u00b7zet"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mehr als ein grosser regen wurm/", "tokens": ["Mehr", "als", "ein", "gros\u00b7ser", "re\u00b7gen", "wurm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "KOKOM", "ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der beste sammt hat kleiner ma\u00df", "tokens": ["Der", "bes\u00b7te", "sammt", "hat", "klei\u00b7ner", "ma\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "VVFIN", "VAFIN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Als wohl der grobe cannefa\u00df.", "tokens": ["Als", "wohl", "der", "gro\u00b7be", "can\u00b7ne\u00b7fa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "4. Manch b\u00fcbgen hat zwar kurtze bein/", "tokens": ["Manch", "b\u00fcb\u00b7gen", "hat", "zwar", "kurt\u00b7ze", "bein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "VAFIN", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und macht doch einen guten tantz/", "tokens": ["Und", "macht", "doch", "ei\u00b7nen", "gu\u00b7ten", "tantz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Manch f\u00fcchsgen ist von ansehn kleine", "tokens": ["Manch", "f\u00fcchs\u00b7gen", "ist", "von", "an\u00b7sehn", "klei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "ADJA", "VAFIN", "APPR", "CARD", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hat doch einen grossen schwantz/", "tokens": ["Und", "hat", "doch", "ei\u00b7nen", "gros\u00b7sen", "schwantz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Den kleinsten hunden h\u00e4nget man", "tokens": ["Den", "kleins\u00b7ten", "hun\u00b7den", "h\u00e4n\u00b7get", "man"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PIS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die allergr\u00f6sten kl\u00f6ppel an.", "tokens": ["Die", "al\u00b7ler\u00b7gr\u00f6s\u00b7ten", "kl\u00f6p\u00b7pel", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "5. Und dieses l\u00e4ugnet warlich keiner/", "tokens": ["Und", "die\u00b7ses", "l\u00e4ug\u00b7net", "war\u00b7lich", "kei\u00b7ner", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ADV", "PIS", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein resolvierter kerl ist jo", "tokens": ["Ein", "re\u00b7sol\u00b7vier\u00b7ter", "kerl", "ist", "jo"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "NE"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "In kleinen duodez viel feiner", "tokens": ["In", "klei\u00b7nen", "duo\u00b7dez", "viel", "fei\u00b7ner"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "PIAT", "ADJA"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Als so ein narr in folio.", "tokens": ["Als", "so", "ein", "narr", "in", "fo\u00b7lio", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJD", "APPR", "NE", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.5": {"text": "Ein kleiner hat ein loses maul/", "tokens": ["Ein", "klei\u00b7ner", "hat", "ein", "lo\u00b7ses", "maul", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hingegen sind die grossen faul.", "tokens": ["Hin\u00b7ge\u00b7gen", "sind", "die", "gros\u00b7sen", "faul", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.6": {"line.1": {"text": "6. Wie war es in dem paradiesse/", "tokens": ["Wie", "war", "es", "in", "dem", "pa\u00b7ra\u00b7dies\u00b7se", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da Adam zu der Eva kam/", "tokens": ["Da", "A\u00b7dam", "zu", "der", "E\u00b7va", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sie als seine zuckers\u00fcsse", "tokens": ["Und", "sie", "als", "sei\u00b7ne", "zu\u00b7cker\u00b7s\u00fcs\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vergn\u00fcgung in die armen nahm/", "tokens": ["Ver\u00b7gn\u00fc\u00b7gung", "in", "die", "ar\u00b7men", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und als das h\u00f6chst-verliebte paar", "tokens": ["Und", "als", "das", "h\u00f6chst\u00b7ver\u00b7lieb\u00b7te", "paar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}}, "stanza.7": {"line.1": {"text": "7. Nach diesem ist es ja geschehen/", "tokens": ["Nach", "die\u00b7sem", "ist", "es", "ja", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VAFIN", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sich ein weibgen ohngefehr", "tokens": ["Da\u00df", "sich", "ein", "weib\u00b7gen", "ohn\u00b7ge\u00b7fehr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "An einen ochsen hat versehen/", "tokens": ["An", "ei\u00b7nen", "och\u00b7sen", "hat", "ver\u00b7se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da kommen nun die grossen her:", "tokens": ["Da", "kom\u00b7men", "nun", "die", "gros\u00b7sen", "her", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Drum bleibt die art auch unverr\u00fcckt/", "tokens": ["Drum", "bleibt", "die", "art", "auch", "un\u00b7ver\u00b7r\u00fcckt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und sie sind etwas ungeschickt.", "tokens": ["Und", "sie", "sind", "et\u00b7was", "un\u00b7ge\u00b7schickt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "8. Ich wil gar gerne kleine bleiben/", "tokens": ["Ich", "wil", "gar", "ger\u00b7ne", "klei\u00b7ne", "blei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein ander kerle mag sich nun", "tokens": ["Ein", "an\u00b7der", "ker\u00b7le", "mag", "sich", "nun"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "VVFIN", "VMFIN", "PRF", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "In Goliats regiester schreiben/", "tokens": ["In", "Go\u00b7li\u00b7ats", "re\u00b7gies\u00b7ter", "schrei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So wil ich doch nicht furchtsam thun/", "tokens": ["So", "wil", "ich", "doch", "nicht", "furcht\u00b7sam", "thun", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der kleinste zwerg ist gleich so gut", "tokens": ["Der", "kleins\u00b7te", "zwerg", "ist", "gleich", "so", "gut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Als auch der gr\u00f6ste \u2012 \u2012 \u2012 \u2012 \u2012", "tokens": ["Als", "auch", "der", "gr\u00f6s\u00b7te", "\u2012", "\u2012", "\u2012", "\u2012", "\u2012"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA", "$(", "$(", "$(", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}