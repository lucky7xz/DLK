{"textgrid.poem.42954": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Sch\u00f6ne Fraun und Katzen pflegen", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sch\u00f6ne Fraun und Katzen pflegen", "tokens": ["Sch\u00f6\u00b7ne", "Fraun", "und", "Kat\u00b7zen", "pfle\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "KON", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "H\u00e4ufig Freundschaft, wenn sie gleich sind,", "tokens": ["H\u00e4u\u00b7fig", "Freund\u00b7schaft", ",", "wenn", "sie", "gleich", "sind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "KOUS", "PPER", "ADV", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Weil sie weich sind", "tokens": ["Weil", "sie", "weich", "sind"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Und mit Grazie sich bewegen.", "tokens": ["Und", "mit", "Gra\u00b7zie", "sich", "be\u00b7we\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "PRF", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Weil sie leise sich verstehen,", "tokens": ["Weil", "sie", "lei\u00b7se", "sich", "ver\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Weil sie selber leise gehen,", "tokens": ["Weil", "sie", "sel\u00b7ber", "lei\u00b7se", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Alles Plumpe oder Laute", "tokens": ["Al\u00b7les", "Plum\u00b7pe", "o\u00b7der", "Lau\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Fliehen und als wohlgebaute", "tokens": ["Flie\u00b7hen", "und", "als", "wohl\u00b7ge\u00b7bau\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "KOUS", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Wesen stets ein sch\u00f6nes Bild sind.", "tokens": ["We\u00b7sen", "stets", "ein", "sch\u00f6\u00b7nes", "Bild", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Unter sich sind sie Vertraute,", "tokens": ["Un\u00b7ter", "sich", "sind", "sie", "Ver\u00b7trau\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "VAFIN", "PPER", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Sie, die sonst unz\u00e4hmbar wild sind.", "tokens": ["Sie", ",", "die", "sonst", "un\u00b7z\u00e4hm\u00b7bar", "wild", "sind", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "ADV", "ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Fell wie Samt und Haar wie Seide.", "tokens": ["Fell", "wie", "Samt", "und", "Haar", "wie", "Sei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "NN", "KON", "NN", "KOKOM", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Allverw\u00f6hnt. \u2013 Man meint, da\u00df beide", "tokens": ["All\u00b7ver\u00b7w\u00f6hnt", ".", "\u2013", "Man", "meint", ",", "da\u00df", "bei\u00b7de"], "token_info": ["word", "punct", "punct", "word", "word", "punct", "word", "word"], "pos": ["VVPP", "$.", "$(", "PIS", "VVFIN", "$,", "KOUS", "PIS"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sich nach nichts, als danach sehnen,", "tokens": ["Sich", "nach", "nichts", ",", "als", "da\u00b7nach", "seh\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "PIS", "$,", "KOUS", "PAV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sich auf Sofas sch\u00f6n zu dehnen.", "tokens": ["Sich", "auf", "So\u00b7fas", "sch\u00f6n", "zu", "deh\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Sch\u00f6ne Fraun mit sch\u00f6nen Katzen,", "tokens": ["Sch\u00f6\u00b7ne", "Fraun", "mit", "sch\u00f6\u00b7nen", "Kat\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wem von ihnen man dann schmeichelt,", "tokens": ["Wem", "von", "ih\u00b7nen", "man", "dann", "schmei\u00b7chelt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PPER", "PIS", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wen von ihnen man gar streichelt,", "tokens": ["Wen", "von", "ih\u00b7nen", "man", "gar", "strei\u00b7chelt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PPER", "PIS", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Stets riskiert man, da\u00df sie kratzen.", "tokens": ["Stets", "ris\u00b7kiert", "man", ",", "da\u00df", "sie", "krat\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$,", "KOUS", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Denn sie haben meistens Mucken,", "tokens": ["Denn", "sie", "ha\u00b7ben", "meis\u00b7tens", "Mu\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADV", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die zuletzt uns andre jucken.", "tokens": ["Die", "zu\u00b7letzt", "uns", "and\u00b7re", "ju\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PPER", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wei\u00df man recht, ob sie im Hellen", "tokens": ["Wei\u00df", "man", "recht", ",", "ob", "sie", "im", "Hel\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "ADJD", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Echt sind oder sich verstellen?", "tokens": ["Echt", "sind", "o\u00b7der", "sich", "ver\u00b7stel\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "KON", "PRF", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Wei\u00df man, wenn sie tief sich ducken,", "tokens": ["Wei\u00df", "man", ",", "wenn", "sie", "tief", "sich", "du\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "KOUS", "PPER", "ADJD", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ob das nicht zum Sprung geschieht?", "tokens": ["Ob", "das", "nicht", "zum", "Sprung", "ge\u00b7schieht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "PTKNEG", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Aber abends, nachts, im Dunkeln,", "tokens": ["A\u00b7ber", "a\u00b7bends", ",", "nachts", ",", "im", "Dun\u00b7keln", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "$,", "APPRART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Wenn dann ihre Augen funkeln,", "tokens": ["Wenn", "dann", "ih\u00b7re", "Au\u00b7gen", "fun\u00b7keln", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.9": {"text": "Wei\u00df man alles oder flieht", "tokens": ["Wei\u00df", "man", "al\u00b7les", "o\u00b7der", "flieht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PIS", "KON", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.10": {"text": "Vor den Funken, die sie stieben.", "tokens": ["Vor", "den", "Fun\u00b7ken", ",", "die", "sie", "stie\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Doch man soll nicht Fraun, die ihre", "tokens": ["Doch", "man", "soll", "nicht", "Fraun", ",", "die", "ih\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "PIS", "VMFIN", "PTKNEG", "NE", "$,", "PRELS", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sch\u00f6nen Katzen wirklich lieben,", "tokens": ["Sch\u00f6\u00b7nen", "Kat\u00b7zen", "wirk\u00b7lich", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Menschen \u00fcberhaupt, die Tiere", "tokens": ["Men\u00b7schen", "\u00fc\u00b7ber\u00b7haupt", ",", "die", "Tie\u00b7re"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "ADV", "$,", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Lieben, dieserhalb verdammen.", "tokens": ["Lie\u00b7ben", ",", "die\u00b7ser\u00b7halb", "ver\u00b7dam\u00b7men", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADJA", "$,", "PAV", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Sind Verliebte auch wie Flammen,", "tokens": ["Sind", "Ver\u00b7lieb\u00b7te", "auch", "wie", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "KOKOM", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu- und ineinander passend,", "tokens": ["Zu", "und", "in\u00b7ein\u00b7an\u00b7der", "pas\u00b7send", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Alles Fremde aber hassend.", "tokens": ["Al\u00b7les", "Frem\u00b7de", "a\u00b7ber", "has\u00b7send", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Ob sie anders oder so sind,", "tokens": ["Ob", "sie", "an\u00b7ders", "o\u00b7der", "so", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "KON", "ADV", "VAFIN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ob sie m\u00e4nnlich, feminin sind,", "tokens": ["Ob", "sie", "m\u00e4nn\u00b7lich", ",", "fe\u00b7mi\u00b7nin", "sind", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "$,", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob sie traurig oder froh sind,", "tokens": ["Ob", "sie", "trau\u00b7rig", "o\u00b7der", "froh", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "KON", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Aus Madrid oder Berlin sind,", "tokens": ["Aus", "Mad\u00b7rid", "o\u00b7der", "Ber\u00b7lin", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "VAFIN", "$,"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.5": {"text": "Ob sie schwarz, ob gelb, ob grau, \u2013", "tokens": ["Ob", "sie", "schwarz", ",", "ob", "gelb", ",", "ob", "grau", ",", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADJD", "$,", "KOUS", "ADJD", "$,", "KOUS", "ADJD", "$,", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Auch wer weder Katz noch Frau", "tokens": ["Auch", "wer", "we\u00b7der", "Katz", "noch", "Frau"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PWS", "KON", "NN", "ADV", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Sch\u00e4tzt, wird Katzen gern mit Frauen,", "tokens": ["Sch\u00e4tzt", ",", "wird", "Kat\u00b7zen", "gern", "mit", "Frau\u00b7en", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VAFIN", "NN", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn sie beide sch\u00f6n sind, schauen.", "tokens": ["Wenn", "sie", "bei\u00b7de", "sch\u00f6n", "sind", ",", "schau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "ADJD", "VAFIN", "$,", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Doch begegnen Ringelnatzen", "tokens": ["Doch", "be\u00b7geg\u00b7nen", "Rin\u00b7gel\u00b7nat\u00b7zen"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "H\u00e4\u00dflich alte Fraun mit Katzen,", "tokens": ["H\u00e4\u00df\u00b7lich", "al\u00b7te", "Fraun", "mit", "Kat\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Geht er schnell drei Schritt zur\u00fcck.", "tokens": ["Geht", "er", "schnell", "drei", "Schritt", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "CARD", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Denn er sagt: Das bringt kein Gl\u00fcck.", "tokens": ["Denn", "er", "sagt", ":", "Das", "bringt", "kein", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$.", "PDS", "VVFIN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}