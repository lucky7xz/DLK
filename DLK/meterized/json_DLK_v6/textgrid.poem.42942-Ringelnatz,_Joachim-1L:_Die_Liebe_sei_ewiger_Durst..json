{"textgrid.poem.42942": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die Liebe sei ewiger Durst.", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die Liebe sei ewiger Durst.", "tokens": ["Die", "Lie\u00b7be", "sei", "e\u00b7wi\u00b7ger", "Durst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Darauf m\u00fc\u00dfte die Freundschaft bedacht sein.", "tokens": ["Da\u00b7rauf", "m\u00fc\u00df\u00b7te", "die", "Freund\u00b7schaft", "be\u00b7dacht", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+---+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und, etwa wie Leberwurst,", "tokens": ["Und", ",", "et\u00b7wa", "wie", "Le\u00b7ber\u00b7wurst", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$,", "ADV", "KOKOM", "NN", "$,"], "meter": "----+-+", "measure": "unknown.measure.di"}, "line.4": {"text": "Immer neu anders gemacht sein.", "tokens": ["Im\u00b7mer", "neu", "an\u00b7ders", "ge\u00b7macht", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "VVPP", "VAINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Damit man's nicht \u00fcberkriegt.", "tokens": ["Da\u00b7mit", "man's", "nicht", "\u00fc\u00b7ber\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "PTKNEG", "VVPP", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wer einmal den Kanal", "tokens": ["Wer", "ein\u00b7mal", "den", "Ka\u00b7nal"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "ADV", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00dcberfliegt,", "tokens": ["\u00dc\u00b7berf\u00b7liegt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Merkt: Der ist so und so breit.", "tokens": ["Merkt", ":", "Der", "ist", "so", "und", "so", "breit", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "$.", "PDS", "VAFIN", "ADV", "KON", "ADV", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Und das \u00e4ndert sich kaum", "tokens": ["Und", "das", "\u00e4n\u00b7dert", "sich", "kaum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PDS", "VVFIN", "PRF", "ADV"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "In menschlein-absehbarer Zeit.", "tokens": ["In", "menschlein\u00b7ab\u00b7seh\u00b7ba\u00b7rer", "Zeit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Wohl aber kann man dies Zwischenraum", "tokens": ["Wohl", "a\u00b7ber", "kann", "man", "dies", "Zwi\u00b7schen\u00b7raum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VMFIN", "PIS", "PDS", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Schneller oder k\u00fcrzer durchqueren.", "tokens": ["Schnel\u00b7ler", "o\u00b7der", "k\u00fcr\u00b7zer", "durch\u00b7que\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.9": {"text": "Wie? Das mu\u00df die Freundschaft uns lehren.", "tokens": ["Wie", "?", "Das", "mu\u00df", "die", "Freund\u00b7schaft", "uns", "leh\u00b7ren", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PDS", "VMFIN", "ART", "NN", "PPER", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.3": {"line.1": {"text": "Ach, man sollte diesen allerh\u00f6chsten Schaft,", "tokens": ["Ach", ",", "man", "soll\u00b7te", "die\u00b7sen", "al\u00b7ler\u00b7h\u00f6chs\u00b7ten", "Schaft", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PIS", "VMFIN", "PDAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Immer wieder einmal j\u00fcnglingshaft", "tokens": ["Im\u00b7mer", "wie\u00b7der", "ein\u00b7mal", "j\u00fcng\u00b7lings\u00b7haft"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADV", "ADJD"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "\u00dcberschwenglich begie\u00dfen.", "tokens": ["\u00dc\u00b7bersc\u00b7hweng\u00b7lich", "be\u00b7gie\u00b7\u00dfen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Eh' uns jener ausgeschlachtete Knochenmann dahinrafft.", "tokens": ["Eh'", "uns", "je\u00b7ner", "aus\u00b7ge\u00b7schlach\u00b7te\u00b7te", "Kno\u00b7chen\u00b7mann", "da\u00b7hin\u00b7rafft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDAT", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-+--+--+-+", "measure": "trochaic.septa.relaxed"}}}}}