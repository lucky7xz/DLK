{"textgrid.poem.57957": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "1L: Es war im zwanzigsten Jahrhundert,", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es war im zwanzigsten Jahrhundert,", "tokens": ["Es", "war", "im", "zwan\u00b7zigs\u00b7ten", "Jahr\u00b7hun\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Als man vom Heimatschutz geschw\u00f6gt,", "tokens": ["Als", "man", "vom", "Hei\u00b7mat\u00b7schutz", "ge\u00b7schw\u00f6gt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da hat man unsre Eilenriede", "tokens": ["Da", "hat", "man", "uns\u00b7re", "Ei\u00b7len\u00b7rie\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIS", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In aller Ruhe abges\u00e4gt.", "tokens": ["In", "al\u00b7ler", "Ru\u00b7he", "ab\u00b7ge\u00b7s\u00e4gt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Doch, da\u00df man sehr f\u00fcr Heimatschutz war,", "tokens": ["Doch", ",", "da\u00df", "man", "sehr", "f\u00fcr", "Hei\u00b7mat\u00b7schutz", "war", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PIS", "ADV", "APPR", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das kann man heute hier noch sehn,", "tokens": ["Das", "kann", "man", "heu\u00b7te", "hier", "noch", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PIS", "ADV", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Von jeder Sorte B\u00e4ume lie\u00df man", "tokens": ["Von", "je\u00b7der", "Sor\u00b7te", "B\u00e4u\u00b7me", "lie\u00df", "man"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "NN", "VVFIN", "PIS"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zum mindesten zwei St\u00fccke stehn.", "tokens": ["Zum", "min\u00b7des\u00b7ten", "zwei", "St\u00fc\u00b7cke", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "CARD", "NN", "VVINF", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.3": {"line.1": {"text": "Und als Naturdenkmal gen\u00fcgt das", "tokens": ["Und", "als", "Na\u00b7tur\u00b7denk\u00b7mal", "ge\u00b7n\u00fcgt", "das"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "NN", "VVFIN", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ja schlie\u00dflich auch, denn Baum ist Baum,", "tokens": ["Ja", "schlie\u00df\u00b7lich", "auch", ",", "denn", "Baum", "ist", "Baum", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "ADV", "$,", "KON", "NE", "VAFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ob zwei, ob tausend von der Sorte", "tokens": ["Ob", "zwei", ",", "ob", "tau\u00b7send", "von", "der", "Sor\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "CARD", "$,", "KOUS", "CARD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da stehn, das interessiert doch kaum.", "tokens": ["Da", "stehn", ",", "das", "in\u00b7ter\u00b7es\u00b7siert", "doch", "kaum", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PDS", "VVFIN", "ADV", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Denn von der List bis hin nach D\u00f6hren,", "tokens": ["Denn", "von", "der", "List", "bis", "hin", "nach", "D\u00f6h\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sah man auf der ganzen Tour,", "tokens": ["Da", "sah", "man", "auf", "der", "gan\u00b7zen", "Tour", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bis es zum Halse einem 'raushing,", "tokens": ["Bis", "es", "zum", "Hal\u00b7se", "ei\u00b7nem", "'raus\u00b7hing", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "ART", "NE", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Fast nichts als solche B\u00e4ume nur.", "tokens": ["Fast", "nichts", "als", "sol\u00b7che", "B\u00e4u\u00b7me", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "KOKOM", "PIAT", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}