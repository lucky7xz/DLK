{"textgrid.poem.53728": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Immer raus mit der Mutter . . . !", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Verdumpft, verengt, verpennt, blockiert,", "tokens": ["Ver\u00b7dumpft", ",", "ver\u00b7engt", ",", "ver\u00b7pennt", ",", "blo\u00b7ckiert", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "so geht das seit zehn Jahren.", "tokens": ["so", "geht", "das", "seit", "zehn", "Jah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "APPR", "CARD", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wie sind die Deutschen dezimiert,", "tokens": ["Wie", "sind", "die", "Deut\u00b7schen", "de\u00b7zi\u00b7miert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die einst von Goethe waren!", "tokens": ["die", "einst", "von", "Goe\u00b7the", "wa\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NE", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ein Mittel gibts \u2013 und das ist rar.", "tokens": ["Ein", "Mit\u00b7tel", "gibts", "\u2013", "und", "das", "ist", "rar", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "KON", "PDS", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das Mittel das ist dies:", "tokens": ["Das", "Mit\u00b7tel", "das", "ist", "dies", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PDS", "VAFIN", "PDS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Mensch, ein Mal auf dem Buhlewar!", "tokens": ["Mensch", ",", "ein", "Mal", "auf", "dem", "Buh\u00b7le\u00b7war", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "Mensch, ein Mal in Paris!", "tokens": ["Mensch", ",", "ein", "Mal", "in", "Pa\u00b7ris", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "NE", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.2": {"line.1": {"text": "Als Ludendorff einst L\u00fcttich nahm", "tokens": ["Als", "Lu\u00b7den\u00b7dorff", "einst", "L\u00fct\u00b7tich", "nahm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "ADV", "ADJD", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und nachher nicht mehr rausfand \u2013", "tokens": ["und", "nach\u00b7her", "nicht", "mehr", "raus\u00b7fand", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADV", "VVFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Welch Tag f\u00fcr ihn! Der Brave kam", "tokens": ["Welch", "Tag", "f\u00fcr", "ihn", "!", "Der", "Bra\u00b7ve", "kam"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIAT", "NN", "APPR", "PPER", "$.", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "zum ersten Mal ins Ausland.", "tokens": ["zum", "ers\u00b7ten", "Mal", "ins", "Aus\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Man denk ihn sich mit Schnurrbarthaar,", "tokens": ["Man", "denk", "ihn", "sich", "mit", "Schnurr\u00b7bart\u00b7haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PRF", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "mit Orden, Helm und Spie\u00df,", "tokens": ["mit", "Or\u00b7den", ",", "Helm", "und", "Spie\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Mensch, ein Mal auf dem Buhlewar!", "tokens": ["Mensch", ",", "ein", "Mal", "auf", "dem", "Buh\u00b7le\u00b7war", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "Mensch, ein Mal in Paris!", "tokens": ["Mensch", ",", "ein", "Mal", "in", "Pa\u00b7ris", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "NE", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.3": {"line.1": {"text": "Hannover-S\u00fcd und Franken-Nord.", "tokens": ["Han\u00b7no\u00b7ver\u00b7S\u00fcd", "und", "Fran\u00b7ken\u00b7Nord", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Horizont wird kleiner.", "tokens": ["Der", "Ho\u00b7ri\u00b7zont", "wird", "klei\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von Hause kommen wenige fort", "tokens": ["Von", "Hau\u00b7se", "kom\u00b7men", "we\u00b7ni\u00b7ge", "fort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PIS", "PTKVZ"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "und in die Welt fast keiner.", "tokens": ["und", "in", "die", "Welt", "fast", "kei\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADV", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ich w\u00fcnsch der Angestelltenschar", "tokens": ["Ich", "w\u00fcnsch", "der", "An\u00b7ge\u00b7stell\u00b7ten\u00b7schar"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "statt brandenburger Kies:", "tokens": ["statt", "bran\u00b7den\u00b7bur\u00b7ger", "Kies", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "nur ein Mal auf dem Buhlewar!", "tokens": ["nur", "ein", "Mal", "auf", "dem", "Buh\u00b7le\u00b7war", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.8": {"text": "nur ein Mal in Paris!", "tokens": ["nur", "ein", "Mal", "in", "Pa\u00b7ris", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "NE", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.4": {"line.1": {"text": "Da drau\u00dfen k\u00fcmmert sich kein Bein", "tokens": ["Da", "drau\u00b7\u00dfen", "k\u00fcm\u00b7mert", "sich", "kein", "Bein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "um eure Fahrdienstleiter.", "tokens": ["um", "eu\u00b7re", "Fahr\u00b7dienst\u00b7lei\u00b7ter", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ihr k\u00f6nnt Hep-Hep und Hurra schrein:", "tokens": ["Ihr", "k\u00f6nnt", "Hep\u00b7Hep", "und", "Hur\u00b7ra", "schrein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die Welt geht ruhig weiter.", "tokens": ["die", "Welt", "geht", "ru\u00b7hig", "wei\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die V\u00f6lker leben. Freude lacht.", "tokens": ["Die", "V\u00f6l\u00b7ker", "le\u00b7ben", ".", "Freu\u00b7de", "lacht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wir stehn in letzter Reihe.", "tokens": ["Wir", "stehn", "in", "letz\u00b7ter", "Rei\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Was sich bei uns so mausig macht,", "tokens": ["Was", "sich", "bei", "uns", "so", "mau\u00b7sig", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "PPER", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "das sollte mal ins Freie!", "tokens": ["das", "soll\u00b7te", "mal", "ins", "Frei\u00b7e", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Den Richtern, Bonzen, ja, sogar", "tokens": ["Den", "Rich\u00b7tern", ",", "Bon\u00b7zen", ",", "ja", ",", "so\u00b7gar"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "NN", "$,", "PTKANT", "$,", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Herrn Hitler w\u00fcnsch ich dies:", "tokens": ["Herrn", "Hit\u00b7ler", "w\u00fcnsch", "ich", "dies", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "PPER", "PDS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Mensch, ein Mal auf dem Buhlewar!", "tokens": ["Mensch", ",", "ein", "Mal", "auf", "dem", "Buh\u00b7le\u00b7war", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.12": {"text": "Mensch, ein Mal nach Paris \u2013!", "tokens": ["Mensch", ",", "ein", "Mal", "nach", "Pa\u00b7ris", "\u2013", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$,", "ART", "NN", "APPR", "NE", "$(", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}}}}