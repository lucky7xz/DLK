{"textgrid.poem.53897": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Geheimnis", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "J\u00fcngst betraf mich ein Japaner,", "tokens": ["J\u00fcngst", "be\u00b7traf", "mich", "ein", "Ja\u00b7pa\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "und in des Gespr\u00e4ches Wellen,", "tokens": ["und", "in", "des", "Ge\u00b7spr\u00e4\u00b7ches", "Wel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "als wir von Matrosen sprachen,", "tokens": ["als", "wir", "von", "Mat\u00b7ro\u00b7sen", "spra\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NE", "VVFIN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "lie\u00df er ein klein W\u00f6rtlein fallen:", "tokens": ["lie\u00df", "er", "ein", "klein", "W\u00f6rt\u00b7lein", "fal\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJD", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "\u203askibi\u2039.", "tokens": ["\u203a", "ski\u00b7bi", "\u2039", "."], "token_info": ["punct", "word", "punct", "punct"], "pos": ["FM", "FM", "FM", "$."], "meter": "OO", "measure": "unknown.measure.zero"}}, "stanza.2": {"line.1": {"text": "\u00bbwas bedeutet das, Geehrter?\u00ab", "tokens": ["\u00bb", "was", "be\u00b7deu\u00b7tet", "das", ",", "Ge\u00b7ehr\u00b7ter", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PDS", "$,", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "fragt ich leicht und glatt und h\u00f6flich.", "tokens": ["fragt", "ich", "leicht", "und", "glatt", "und", "h\u00f6f\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "KON", "ADJD", "KON", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbnie noch h\u00f6rt ich diese Silben:", "tokens": ["\u00bb", "nie", "noch", "h\u00f6rt", "ich", "die\u00b7se", "Sil\u00b7ben", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADV", "VVFIN", "PPER", "PDAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Skibi \u2013?", "tokens": ["Ski\u00b7bi", "\u2013", "?"], "token_info": ["word", "punct", "punct"], "pos": ["NE", "$(", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.3": {"line.1": {"text": "Ists ein Laster? Ein Gesellschafts\u2013", "tokens": ["Ists", "ein", "Las\u00b7ter", "?", "Ein", "Ge\u00b7sell\u00b7schafts", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$.", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "spiel? Kann man es konsumieren?", "tokens": ["spiel", "?", "Kann", "man", "es", "kon\u00b7su\u00b7mie\u00b7ren", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "VMFIN", "PIS", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Tun Matrosen es? Mit wem wohl?", "tokens": ["Tun", "Mat\u00b7ro\u00b7sen", "es", "?", "Mit", "wem", "wohl", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PPER", "$.", "APPR", "PIS", "ADV", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Skibi \u2013?\u00ab", "tokens": ["Ski\u00b7bi", "\u2013", "?", "\u00ab"], "token_info": ["word", "punct", "punct", "punct"], "pos": ["NE", "$(", "$.", "$("], "meter": "+-", "measure": "trochaic.single"}}, "stanza.4": {"line.1": {"text": "Der Japaner nickte h\u00f6flich,", "tokens": ["Der", "Ja\u00b7pa\u00b7ner", "nick\u00b7te", "h\u00f6f\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "l\u00e4chelte und schwieg. Und seitdem", "tokens": ["l\u00e4\u00b7chel\u00b7te", "und", "schwieg", ".", "Und", "seit\u00b7dem"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "KON", "VVFIN", "$.", "KON", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "hockt auf mir der Skibi-Wahnsinn.", "tokens": ["hockt", "auf", "mir", "der", "Ski\u00b7bi\u00b7Wahn\u00b7sinn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Skibi! zwitschern alle Spatzen.", "tokens": ["Ski\u00b7bi", "!", "zwit\u00b7schern", "al\u00b7le", "Spat\u00b7zen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Skibi-skibi! gellt die Hupe.", "tokens": ["Ski\u00b7bi\u00b7ski\u00b7bi", "!", "gellt", "die", "Hu\u00b7pe", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Und die Stadtbahn-Wagenachsen", "tokens": ["Und", "die", "Stadt\u00b7bahn\u00b7Wa\u00b7ge\u00b7nach\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "rattern: Skibi-skibi-skibi . . . !", "tokens": ["rat\u00b7tern", ":", "Ski\u00b7bi\u00b7ski\u00b7bi\u00b7ski\u00b7bi", ".", ".", ".", "!"], "token_info": ["word", "punct", "word", "punct", "punct", "punct", "punct"], "pos": ["VVFIN", "$.", "NN", "$.", "$.", "$.", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Skibi! piept die Bodenmaus.", "tokens": ["Ski\u00b7bi", "!", "piept", "die", "Bo\u00b7den\u00b7maus", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und so sieht die Sonne aus:", "tokens": ["Und", "so", "sieht", "die", "Son\u00b7ne", "aus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Traurig krauche ich durchs Leben.", "tokens": ["Trau\u00b7rig", "krau\u00b7che", "ich", "durchs", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Kann mir niemand Rettung geben?", "tokens": ["Kann", "mir", "nie\u00b7mand", "Ret\u00b7tung", "ge\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PIS", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf, nach Japan la\u00dft mich fahren,", "tokens": ["Auf", ",", "nach", "Ja\u00b7pan", "la\u00dft", "mich", "fah\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "APPR", "NE", "VVFIN", "PPER", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "seekrank, hei\u00df, mit M\u00f6wenscharen,", "tokens": ["see\u00b7krank", ",", "hei\u00df", ",", "mit", "M\u00f6\u00b7wen\u00b7scha\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADJD", "$,", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "wochenlang in Schiffsbewegung,", "tokens": ["wo\u00b7chen\u00b7lang", "in", "Schiffs\u00b7be\u00b7we\u00b7gung", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ii. Klasse (mit Verpflegung) \u2013", "tokens": ["I\u00b7i", ".", "Klas\u00b7se", "(", "mit", "Ver\u00b7pfle\u00b7gung", ")", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$.", "NN", "$(", "APPR", "NN", "$(", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Und ich seh nicht Pal\u00e4stina,", "tokens": ["Und", "ich", "seh", "nicht", "Pa\u00b7l\u00e4s\u00b7ti\u00b7na", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKNEG", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Indien nicht an und China \u2013", "tokens": ["In\u00b7di\u00b7en", "nicht", "an", "und", "Chi\u00b7na", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "PTKVZ", "KON", "NE", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.9": {"text": "Bombay nicht und nicht Kalkutta,", "tokens": ["Bom\u00b7bay", "nicht", "und", "nicht", "Kal\u00b7kut\u00b7ta", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "KON", "PTKNEG", "NE", "$,"], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "in Port Said die Kuppelmutter . . .", "tokens": ["in", "Port", "Said", "die", "Kup\u00b7pel\u00b7mut\u00b7ter", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "NE", "NE", "ART", "NN", "$.", "$.", "$."], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.11": {"text": "Ungegessen, ungeschlafen,", "tokens": ["Un\u00b7ge\u00b7ges\u00b7sen", ",", "un\u00b7ge\u00b7schla\u00b7fen", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.12": {"text": "fahr ich.", "tokens": ["fahr", "ich", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "PPER", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.7": {"line.1": {"text": "Auf dem Quai im Japan-Hafen", "tokens": ["Auf", "dem", "Qua\u00b7i", "im", "Ja\u00b7pan\u00b7Ha\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPRART", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "spring ich auf den ersten besten,", "tokens": ["spring", "ich", "auf", "den", "ers\u00b7ten", "bes\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ADJA", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "halt ihn an am Knopf der Westen \u2013", "tokens": ["halt", "ihn", "an", "am", "Knopf", "der", "Wes\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "APPRART", "NN", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "schreiend frag ich:", "tokens": ["schrei\u00b7end", "frag", "ich", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "\u00bbwas ist Skibi \u2013?\u00ab", "tokens": ["\u00bb", "was", "ist", "Ski\u00b7bi", "\u2013", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "NN", "$(", "$.", "$("], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.8": {"line.1": {"text": "Der Japaner, kalten Blutes, spricht:", "tokens": ["Der", "Ja\u00b7pa\u00b7ner", ",", "kal\u00b7ten", "Blu\u00b7tes", ",", "spricht", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "ADJA", "NN", "$,", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbdas fragt man nicht. Man tut es.", "tokens": ["\u00bb", "das", "fragt", "man", "nicht", ".", "Man", "tut", "es", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VVFIN", "PIS", "PTKNEG", "$.", "PIS", "VVFIN", "PPER", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Skibi-skibi-skibi-skibi \u2013!\u00ab", "tokens": ["Ski\u00b7bi\u00b7ski\u00b7bi\u00b7ski\u00b7bi\u00b7ski\u00b7bi", "\u2013", "!", "\u00ab"], "token_info": ["word", "punct", "punct", "punct"], "pos": ["NE", "$(", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "In die Heimat f\u00e4hrt ein Greis.", "tokens": ["In", "die", "Hei\u00b7mat", "f\u00e4hrt", "ein", "Greis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Stumm. Zerbrochen. Haar schlohwei\u00df.", "tokens": ["Stumm", ".", "Zer\u00b7bro\u00b7chen", ".", "Haar", "schloh\u00b7wei\u00df", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$.", "NN", "$.", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Geht ins Kloster als Trappist,", "tokens": ["Geht", "ins", "Klos\u00b7ter", "als", "Trap\u00b7pist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "KOUS", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "weil er nicht wei\u00df, was Skibi ist.", "tokens": ["weil", "er", "nicht", "wei\u00df", ",", "was", "Ski\u00b7bi", "ist", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVFIN", "$,", "PWS", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}