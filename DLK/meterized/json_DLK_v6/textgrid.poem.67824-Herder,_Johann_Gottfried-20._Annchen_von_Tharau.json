{"textgrid.poem.67824": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "20. Annchen von Tharau", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Annchen von Tharau ist, die mir gef\u00e4llt;", "tokens": ["Ann\u00b7chen", "von", "Tha\u00b7rau", "ist", ",", "die", "mir", "ge\u00b7f\u00e4llt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "VAFIN", "$,", "PRELS", "PPER", "VVPP", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Sie ist mein Leben, mein Gut und mein Geld.", "tokens": ["Sie", "ist", "mein", "Le\u00b7ben", ",", "mein", "Gut", "und", "mein", "Geld", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.2": {"line.1": {"text": "Annchen von Tharau hat wieder ihr Herz", "tokens": ["Ann\u00b7chen", "von", "Tha\u00b7rau", "hat", "wie\u00b7der", "ihr", "Herz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "APPR", "NN", "VAFIN", "ADV", "PPOSAT", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Auf mich gerichtet in Lieb' und in Schmerz.", "tokens": ["Auf", "mich", "ge\u00b7rich\u00b7tet", "in", "Lieb'", "und", "in", "Schmerz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVPP", "APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.3": {"line.1": {"text": "Annchen von Tharau, mein Reichthum, mein Gut,", "tokens": ["Ann\u00b7chen", "von", "Tha\u00b7rau", ",", "mein", "Reicht\u00b7hum", ",", "mein", "Gut", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Du meine Seele, mein Fleisch und mein Blut!", "tokens": ["Du", "mei\u00b7ne", "See\u00b7le", ",", "mein", "Fleisch", "und", "mein", "Blut", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.4": {"line.1": {"text": "K\u00e4m' alles Wetter gleich auf uns zu schlahn,", "tokens": ["K\u00e4m'", "al\u00b7les", "Wet\u00b7ter", "gleich", "auf", "uns", "zu", "schlahn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "ADV", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Wir sind gesinnet bei einander zu stahn.", "tokens": ["Wir", "sind", "ge\u00b7sin\u00b7net", "bei", "ein\u00b7an\u00b7der", "zu", "stahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPR", "PRF", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.5": {"line.1": {"text": "Krankheit, Verfolgung, Betr\u00fcbni\u00df und Pein", "tokens": ["Krank\u00b7heit", ",", "Ver\u00b7fol\u00b7gung", ",", "Be\u00b7tr\u00fcb\u00b7ni\u00df", "und", "Pein"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Soll unsrer Liebe Verknotigung seyn.", "tokens": ["Soll", "uns\u00b7rer", "Lie\u00b7be", "Ver\u00b7kno\u00b7ti\u00b7gung", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPOSAT", "NN", "NN", "VAINF", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.6": {"line.1": {"text": "Recht als ein Palmenbaum \u00fcber sich steigt,", "tokens": ["Recht", "als", "ein", "Pal\u00b7men\u00b7baum", "\u00fc\u00b7ber", "sich", "steigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "APPR", "PRF", "VVFIN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Je mehr ihn Hagel und Regen anficht;", "tokens": ["Je", "mehr", "ihn", "Ha\u00b7gel", "und", "Re\u00b7gen", "an\u00b7ficht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "NN", "KON", "NN", "VVFIN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.7": {"line.1": {"text": "So wird die Lieb' in uns m\u00e4chtig und gro\u00df", "tokens": ["So", "wird", "die", "Lieb'", "in", "uns", "m\u00e4ch\u00b7tig", "und", "gro\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "PPER", "ADJD", "KON", "ADJD"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch Kreuz, durch Leiden, durch allerlei Noth.", "tokens": ["Durch", "Kreuz", ",", "durch", "Lei\u00b7den", ",", "durch", "al\u00b7ler\u00b7lei", "Noth", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "APPR", "NN", "$,", "APPR", "PIAT", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.8": {"line.1": {"text": "W\u00fcrdest du gleich einmal von mir getrennt, L", "tokens": ["W\u00fcr\u00b7dest", "du", "gleich", "ein\u00b7mal", "von", "mir", "ge\u00b7trennt", ",", "L"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "APPR", "PPER", "VVPP", "$,", "NE"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "ebtest, da wo man die Sonne kaum kennt;", "tokens": ["eb\u00b7test", ",", "da", "wo", "man", "die", "Son\u00b7ne", "kaum", "kennt", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "PWAV", "PIS", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.9": {"line.1": {"text": "Ich will dir folgen durch W\u00e4lder, durch Meer,", "tokens": ["Ich", "will", "dir", "fol\u00b7gen", "durch", "W\u00e4l\u00b7der", ",", "durch", "Meer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "APPR", "NN", "$,", "APPR", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Durch Eis, durch Eisen, durch feindliches Heer.", "tokens": ["Durch", "Eis", ",", "durch", "Ei\u00b7sen", ",", "durch", "feind\u00b7li\u00b7ches", "Heer", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "APPR", "NN", "$,", "APPR", "ADJA", "NN", "$."], "meter": "++-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Annchen von Tharau, mein Licht, meine Sonn,", "tokens": ["Ann\u00b7chen", "von", "Tha\u00b7rau", ",", "mein", "Licht", ",", "mei\u00b7ne", "Sonn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Mein Leben schlie\u00df' ich um deines herum.", "tokens": ["Mein", "Le\u00b7ben", "schlie\u00df'", "ich", "um", "dei\u00b7nes", "he\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "APPR", "PPOSAT", "PTKVZ", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Was ich gebiete, wird von dir gethan,", "tokens": ["Was", "ich", "ge\u00b7bie\u00b7te", ",", "wird", "von", "dir", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "VAFIN", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Was ich verbiete, das l\u00e4st du mir stahn.", "tokens": ["Was", "ich", "ver\u00b7bie\u00b7te", ",", "das", "l\u00e4st", "du", "mir", "stahn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "PDS", "VVFIN", "PPER", "PPER", "VVFIN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Was hat die Liebe doch f\u00fcr ein Bestand,", "tokens": ["Was", "hat", "die", "Lie\u00b7be", "doch", "f\u00fcr", "ein", "Be\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wo nicht Ein Herz ist, Ein Mund, Eine Hand?", "tokens": ["Wo", "nicht", "Ein", "Herz", "ist", ",", "Ein", "Mund", ",", "Ei\u00b7ne", "Hand", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "ART", "NN", "VAFIN", "$,", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "Wo man sich peiniget, zanket und schl\u00e4gt,", "tokens": ["Wo", "man", "sich", "pei\u00b7ni\u00b7get", ",", "zan\u00b7ket", "und", "schl\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PRF", "VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Und gleich den Hunden und Kazen betr\u00e4gt?", "tokens": ["Und", "gleich", "den", "Hun\u00b7den", "und", "Ka\u00b7zen", "be\u00b7tr\u00e4gt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Annchen von Tharau, das woll'n wir nicht thun;", "tokens": ["Ann\u00b7chen", "von", "Tha\u00b7rau", ",", "das", "woll'n", "wir", "nicht", "thun", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "NN", "$,", "PDS", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.2": {"text": "Du bist mein T\u00e4ubchen, mein Sch\u00e4fchen, mein Huhn.", "tokens": ["Du", "bist", "mein", "T\u00e4ub\u00b7chen", ",", "mein", "Sch\u00e4f\u00b7chen", ",", "mein", "Huhn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.15": {"line.1": {"text": "Was ich begehre, ist lieb dir und gut;", "tokens": ["Was", "ich", "be\u00b7geh\u00b7re", ",", "ist", "lieb", "dir", "und", "gut", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "VAFIN", "ADJD", "PPER", "KON", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ich la\u00df den Rock dir, du l\u00e4\u00dft mir den Hut!", "tokens": ["Ich", "la\u00df", "den", "Rock", "dir", ",", "du", "l\u00e4\u00dft", "mir", "den", "Hut", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PPER", "$,", "PPER", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "Dies ist uns Annchen die s\u00fcsseste Ruh,", "tokens": ["Dies", "ist", "uns", "Ann\u00b7chen", "die", "s\u00fcs\u00b7ses\u00b7te", "Ruh", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ein Leib und Seele wird aus Ich und Du.", "tokens": ["Ein", "Leib", "und", "See\u00b7le", "wird", "aus", "Ich", "und", "Du", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VAFIN", "APPR", "PPER", "KON", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Dies macht das Leben zum himmlischen Reich,", "tokens": ["Dies", "macht", "das", "Le\u00b7ben", "zum", "himm\u00b7li\u00b7schen", "Reich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Durch Zanken wird es der H\u00f6lle gleich.", "tokens": ["Durch", "Zan\u00b7ken", "wird", "es", "der", "H\u00f6l\u00b7le", "gleich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ART", "NN", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}}}}