{"dta.poem.20459": {"metadata": {"author": {"name": "Hofmann von Hofmannswaldau, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Verliebte Arien.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1695", "urn": "urn:nbn:de:kobv:b4-200905197751", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Og\u00f6ttin/ der ich voller pflicht", "tokens": ["O\u00b7g\u00f6t\u00b7tin", "/", "der", "ich", "vol\u00b7ler", "pflicht"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$(", "PRELS", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein erstes opffer angericht/", "tokens": ["Mein", "ers\u00b7tes", "opf\u00b7fer", "an\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Verachte nicht die letzten flammen/", "tokens": ["Ver\u00b7ach\u00b7te", "nicht", "die", "letz\u00b7ten", "flam\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "ART", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dencke noch an das altar/", "tokens": ["Und", "den\u00b7cke", "noch", "an", "das", "al\u00b7tar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Darauff mein kindisch rauch-werck war/", "tokens": ["Dar\u00b7auff", "mein", "kin\u00b7disch", "rauch\u00b7\u00b7werck", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "NE", "VAFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So dich und mich verband zusammen.", "tokens": ["So", "dich", "und", "mich", "ver\u00b7band", "zu\u00b7sam\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "KON", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich wei\u00df wohl/ da\u00df die schn\u00f6de zeit/", "tokens": ["Ich", "wei\u00df", "wohl", "/", "da\u00df", "die", "schn\u00f6\u00b7de", "zeit", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$(", "KOUS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und meine grosse niedrigkeit", "tokens": ["Und", "mei\u00b7ne", "gros\u00b7se", "nied\u00b7rig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dein ohre von mir weggerissen/", "tokens": ["Dein", "oh\u00b7re", "von", "mir", "weg\u00b7ge\u00b7ris\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und da\u00df kein zeugni\u00df meiner pflicht/", "tokens": ["Und", "da\u00df", "kein", "zeug\u00b7ni\u00df", "mei\u00b7ner", "pflicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIAT", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So hand und seele zugericht/", "tokens": ["So", "hand", "und", "see\u00b7le", "zu\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "VVFIN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Recht w\u00fcrdig ist/ dich zu begr\u00fcssen.", "tokens": ["Recht", "w\u00fcr\u00b7dig", "ist", "/", "dich", "zu", "be\u00b7gr\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "VAFIN", "$(", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Doch aber/ wilstu g\u00f6ttin seyn?", "tokens": ["Doch", "a\u00b7ber", "/", "wils\u00b7tu", "g\u00f6t\u00b7tin", "seyn", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "VMFIN", "NE", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So mu\u00df auch deiner strahlen-schein", "tokens": ["So", "mu\u00df", "auch", "dei\u00b7ner", "strah\u00b7len\u00b7schein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein kleines opffer nicht verh\u00f6hnen.", "tokens": ["Ein", "klei\u00b7nes", "opf\u00b7fer", "nicht", "ver\u00b7h\u00f6h\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der himmel liebt barmhertzigkeit/", "tokens": ["Der", "him\u00b7mel", "liebt", "barm\u00b7hert\u00b7zig\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und alle g\u00f6tter sind erfreut/", "tokens": ["Und", "al\u00b7le", "g\u00f6t\u00b7ter", "sind", "er\u00b7freut", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VAFIN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wenn unsre h\u00e4nde sie vers\u00f6hnen.", "tokens": ["Wenn", "uns\u00b7re", "h\u00e4n\u00b7de", "sie", "ver\u00b7s\u00f6h\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Drum thu auch deinen himmel auff/", "tokens": ["Drum", "thu", "auch", "dei\u00b7nen", "him\u00b7mel", "auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ADV", "PPOSAT", "NN", "APPR", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und la\u00df der tauben seiten lauff", "tokens": ["Und", "la\u00df", "der", "tau\u00b7ben", "sei\u00b7ten", "lauff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mich und mein opffer nicht verzehren!", "tokens": ["Mich", "und", "mein", "opf\u00b7fer", "nicht", "ver\u00b7zeh\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "PPOSAT", "NN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die d\u00fcrfftigkeit hemmt meine hand/", "tokens": ["Die", "d\u00fcr\u00b7ff\u00b7tig\u00b7keit", "hemmt", "mei\u00b7ne", "hand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und ist dir doch zuvor bekandt/", "tokens": ["Und", "ist", "dir", "doch", "zu\u00b7vor", "be\u00b7kandt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Was dir mein armuth kan gew\u00e4hren.", "tokens": ["Was", "dir", "mein", "ar\u00b7muth", "kan", "ge\u00b7w\u00e4h\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPOSAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ist gleich rauch-opffer/ brand und heerdt", "tokens": ["Ist", "gleich", "rauch\u00b7opf\u00b7fer", "/", "brand", "und", "heerdt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ADV", "NE", "$(", "VVFIN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht deiner himmel-sch\u00f6nheit werth/", "tokens": ["Nicht", "dei\u00b7ner", "him\u00b7mel\u00b7sch\u00f6n\u00b7heit", "werth", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PPOSAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So wird dich das doch nicht beflecken;", "tokens": ["So", "wird", "dich", "das", "doch", "nicht", "be\u00b7fle\u00b7cken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PDS", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und bistu g\u00f6ttin/ so da liebt/", "tokens": ["Und", "bis\u00b7tu", "g\u00f6t\u00b7tin", "/", "so", "da", "liebt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "$(", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da man ihr himmels-ehre giebt?", "tokens": ["Da", "man", "ihr", "him\u00b7mels\u00b7eh\u00b7re", "giebt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So la\u00df mich deinen nectar schmecken.", "tokens": ["So", "la\u00df", "mich", "dei\u00b7nen", "nec\u00b7tar", "schme\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "So dich mein feuer lencken kan/", "tokens": ["So", "dich", "mein", "feu\u00b7er", "len\u00b7cken", "kan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PPOSAT", "NN", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So schaue dessen funcken an/", "tokens": ["So", "schau\u00b7e", "des\u00b7sen", "fun\u00b7cken", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PDS", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und la\u00df mich nicht so schm\u00e4hlig sterben;", "tokens": ["Und", "la\u00df", "mich", "nicht", "so", "schm\u00e4h\u00b7lig", "ster\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "PTKNEG", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch/ soll es ja gestorben seyn/", "tokens": ["Doch", "/", "soll", "es", "ja", "ge\u00b7stor\u00b7ben", "seyn", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VMFIN", "PPER", "ADV", "VVPP", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So la\u00df mein leben samt der pein", "tokens": ["So", "la\u00df", "mein", "le\u00b7ben", "samt", "der", "pein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "PPOSAT", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Durch deiner augen-glut verderben.", "tokens": ["Durch", "dei\u00b7ner", "au\u00b7gen\u00b7glut", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Es komme leben oder tod/", "tokens": ["Es", "kom\u00b7me", "le\u00b7ben", "o\u00b7der", "tod", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVINF", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es komme wohlfahrt oder noth/", "tokens": ["Es", "kom\u00b7me", "wohl\u00b7fahrt", "o\u00b7der", "noth", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich nehm es an mit tausend k\u00fcssen/", "tokens": ["Ich", "nehm", "es", "an", "mit", "tau\u00b7send", "k\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "APPR", "CARD", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dein urtheil st\u00e4rcket meinen muth/", "tokens": ["Dein", "ur\u00b7theil", "st\u00e4r\u00b7cket", "mei\u00b7nen", "muth", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ich bin bereit/ mein theures blut", "tokens": ["Ich", "bin", "be\u00b7reit", "/", "mein", "theu\u00b7res", "blut"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "$(", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Vor deinen f\u00fcssen zu vergiessen.", "tokens": ["Vor", "dei\u00b7nen", "f\u00fcs\u00b7sen", "zu", "ver\u00b7gies\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "VVINF", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}