{"textgrid.poem.24290": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "Sowas!", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Heute nacht erschien ich mir", "tokens": ["Heu\u00b7te", "nacht", "er\u00b7schien", "ich", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "VVFIN", "PPER", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Als ein sonderbares Tier.", "tokens": ["Als", "ein", "son\u00b7der\u00b7ba\u00b7res", "Tier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Hatte Krallen", "tokens": ["Hat\u00b7te", "Kral\u00b7len"], "token_info": ["word", "word"], "pos": ["VAFIN", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Aus Korallen,", "tokens": ["Aus", "Ko\u00b7ral\u00b7len", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Hatte H\u00f6rner wie ein Stier.", "tokens": ["Hat\u00b7te", "H\u00f6r\u00b7ner", "wie", "ein", "Stier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KOKOM", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Doch sie waren, ich gab wohl acht,", "tokens": ["Doch", "sie", "wa\u00b7ren", ",", "ich", "gab", "wohl", "acht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "$,", "PPER", "VVFIN", "ADV", "CARD", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.7": {"text": "Waren aus Muschelkalk gemacht.", "tokens": ["Wa\u00b7ren", "aus", "Mu\u00b7schel\u00b7kalk", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "VVPP", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "An ihren Spitzen sa\u00dfen,", "tokens": ["An", "ih\u00b7ren", "Spit\u00b7zen", "sa\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Geschliffen aus Topasen,", "tokens": ["Ge\u00b7schlif\u00b7fen", "aus", "To\u00b7pa\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zwei Augen, die waren wie wolkichte Nacht.", "tokens": ["Zwei", "Au\u00b7gen", ",", "die", "wa\u00b7ren", "wie", "wol\u00b7kich\u00b7te", "Nacht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PRELS", "VAFIN", "KOKOM", "ADJA", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.3": {"line.1": {"text": "Wo sonst die Augen sitzen,", "tokens": ["Wo", "sonst", "die", "Au\u00b7gen", "sit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Sah ich aus schmalen Ritzen", "tokens": ["Sah", "ich", "aus", "schma\u00b7len", "Rit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein gr\u00fcnes Funkeln gehn;", "tokens": ["Ein", "gr\u00fc\u00b7nes", "Fun\u00b7keln", "gehn", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Da glaubte ich mein Leben", "tokens": ["Da", "glaub\u00b7te", "ich", "mein", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Verfunkeln, verschweben,", "tokens": ["Ver\u00b7fun\u00b7keln", ",", "ver\u00b7schwe\u00b7ben", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Verblitzen zu sehn.", "tokens": ["Ver\u00b7blit\u00b7zen", "zu", "sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.4": {"line.1": {"text": "Ich hatte Lippen keine;", "tokens": ["Ich", "hat\u00b7te", "Lip\u00b7pen", "kei\u00b7ne", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "PIAT", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Es lagen zwei Steine", "tokens": ["Es", "la\u00b7gen", "zwei", "Stei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "CARD", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Malmend \u00fcbereinander her,", "tokens": ["Mal\u00b7mend", "\u00fc\u00b7be\u00b7re\u00b7in\u00b7an\u00b7der", "her", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Die waren ganz glatt gerieben;", "tokens": ["Die", "wa\u00b7ren", "ganz", "glatt", "ge\u00b7rie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Es stand darauf geschrieben", "tokens": ["Es", "stand", "da\u00b7rauf", "ge\u00b7schrie\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PAV", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "All meine Schuld und S\u00fcnde; \u2013 gottlob, ich wei\u00df es nicht mehr.", "tokens": ["All", "mei\u00b7ne", "Schuld", "und", "S\u00fcn\u00b7de", ";", "\u2013", "gott\u00b7lob", ",", "ich", "wei\u00df", "es", "nicht", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "KON", "NN", "$.", "$(", "ADV", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}}, "stanza.5": {"line.1": {"text": "Es hatte das Tier ein Fell, das war", "tokens": ["Es", "hat\u00b7te", "das", "Tier", "ein", "Fell", ",", "das", "war"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "ART", "NN", "$,", "PDS", "VAFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aus gr\u00fcnem Grase, nicht aus Haar,", "tokens": ["Aus", "gr\u00fc\u00b7nem", "Gra\u00b7se", ",", "nicht", "aus", "Haar", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und war gebl\u00fcmt, \u2013 h\u00f6chst wunderbar:", "tokens": ["Und", "war", "ge\u00b7bl\u00fcmt", ",", "\u2013", "h\u00f6chst", "wun\u00b7der\u00b7bar", ":"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVPP", "$,", "$(", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Lauter Herbstzeitlosen.", "tokens": ["Lau\u00b7ter", "Herbst\u00b7zeit\u00b7lo\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Statt eines Schweifes schwang es einen gro\u00dfen", "tokens": ["Statt", "ei\u00b7nes", "Schwei\u00b7fes", "schwang", "es", "ei\u00b7nen", "gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "VVFIN", "PPER", "ART", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Fleischigen, dicken Lilienstengel", "tokens": ["Flei\u00b7schi\u00b7gen", ",", "di\u00b7cken", "Li\u00b7li\u00b7ens\u00b7ten\u00b7gel"], "token_info": ["word", "punct", "word", "word"], "pos": ["NN", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Mit einer quittegelben Lilie dran.", "tokens": ["Mit", "ei\u00b7ner", "quit\u00b7te\u00b7gel\u00b7ben", "Li\u00b7lie", "dran", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Das Ding bewegte sich gleich einem Glockenschwengel", "tokens": ["Das", "Ding", "be\u00b7weg\u00b7te", "sich", "gleich", "ei\u00b7nem", "Glo\u00b7cken\u00b7schwen\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und sah sich eigentlich l\u00e4cherlich an.", "tokens": ["Und", "sah", "sich", "ei\u00b7gent\u00b7lich", "l\u00e4\u00b7cher\u00b7lich", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Ich kam \u00fcberhaupt bald auf den Verdacht,", "tokens": ["Ich", "kam", "\u00fc\u00b7ber\u00b7haupt", "bald", "auf", "den", "Ver\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hier wird sich \u00fcber dich lustig gemacht.", "tokens": ["Hier", "wird", "sich", "\u00fc\u00b7ber", "dich", "lus\u00b7tig", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "APPR", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Auch fragt ich mich: Wie?", "tokens": ["Auch", "fragt", "ich", "mich", ":", "Wie", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "$.", "PWAV", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Ich w\u00e4re das Vieh?", "tokens": ["Ich", "w\u00e4\u00b7re", "das", "Vieh", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Das Vieh w\u00e4re ich?", "tokens": ["Das", "Vieh", "w\u00e4\u00b7re", "ich", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Ich bin doch wei\u00df Gott nicht so wunderlich.", "tokens": ["Ich", "bin", "doch", "wei\u00df", "Gott", "nicht", "so", "wun\u00b7der\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN", "NN", "PTKNEG", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Indes eben das", "tokens": ["In\u00b7des", "e\u00b7ben", "das"], "token_info": ["word", "word", "word"], "pos": ["NN", "ADV", "ART"], "meter": "+-+--", "measure": "unknown.measure.di"}, "line.2": {"text": "War so \u00fcber alle Ma\u00dfen kra\u00df:", "tokens": ["War", "so", "\u00fc\u00b7ber", "al\u00b7le", "Ma\u00b7\u00dfen", "kra\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Es tanzte das Monstrum", "tokens": ["Es", "tanz\u00b7te", "das", "Monst\u00b7rum"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Nicht blo\u00df so umsonst rum", "tokens": ["Nicht", "blo\u00df", "so", "um\u00b7sonst", "rum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADV", "ADV", "ADV", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Zu meinem Pl\u00e4sier.", "tokens": ["Zu", "mei\u00b7nem", "Pl\u00e4\u00b7sier", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "Nein ... ich ... war ... das .. Tier.", "tokens": ["Nein", "...", "ich", "...", "war", "...", "das", "..", "Tier", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PTKANT", "$(", "PPER", "$(", "VAFIN", "$(", "ART", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Es l\u00e4\u00dft sich nicht sagen, wieso, \u2013 es war", "tokens": ["Es", "l\u00e4\u00dft", "sich", "nicht", "sa\u00b7gen", ",", "wie\u00b7so", ",", "\u2013", "es", "war"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "PTKNEG", "VVINF", "$,", "PWAV", "$,", "$(", "PPER", "VAFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.8": {"text": "Mir einfach klar.", "tokens": ["Mir", "ein\u00b7fach", "klar", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Also gut! dacht ich mir:", "tokens": ["Al\u00b7so", "gut", "!", "dacht", "ich", "mir", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "VVFIN", "PPER", "PPER", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "Wir sind eins: ich und \u2013 das;", "tokens": ["Wir", "sind", "eins", ":", "ich", "und", "\u2013", "das", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "$.", "PPER", "KON", "$(", "PDS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Aber ich w\u00fc\u00dfte nun wenigstens gerne: was,", "tokens": ["A\u00b7ber", "ich", "w\u00fc\u00df\u00b7te", "nun", "we\u00b7nigs\u00b7tens", "ger\u00b7ne", ":", "was", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADV", "ADV", "$.", "PWS", "$,"], "meter": "+--+-+++-+-+", "measure": "iambic.septa.invert"}, "line.4": {"text": "Bitte, was soll das bedeuten?", "tokens": ["Bit\u00b7te", ",", "was", "soll", "das", "be\u00b7deu\u00b7ten", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWS", "VMFIN", "PDS", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.10": {"line.1": {"text": "Wie ich so dachte, h\u00f6rt ich ein L\u00e4uten,", "tokens": ["Wie", "ich", "so", "dach\u00b7te", ",", "h\u00f6rt", "ich", "ein", "L\u00e4u\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVFIN", "$,", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Oder vielmehr ein Gl\u00f6ckeln: gingging.", "tokens": ["O\u00b7der", "viel\u00b7mehr", "ein", "Gl\u00f6\u00b7ckeln", ":", "ging\u00b7ging", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$.", "VVFIN", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Es war die gelbe Lilie, das komische Ding,", "tokens": ["Es", "war", "die", "gel\u00b7be", "Li\u00b7lie", ",", "das", "ko\u00b7mi\u00b7sche", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Das auf seine Weise zu reden anfing:", "tokens": ["Das", "auf", "sei\u00b7ne", "Wei\u00b7se", "zu", "re\u00b7den", "an\u00b7fing", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+--+--+---", "measure": "amphibrach.tri.plus"}}, "stanza.11": {"line.1": {"text": "Gingging, gingging, nein so was, nein:", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", ",", "nein", "so", "was", ",", "nein", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "PTKANT", "ADV", "PIS", "$,", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt sieht der Kerl sich selber nicht ein!", "tokens": ["Jetzt", "sieht", "der", "Kerl", "sich", "sel\u00b7ber", "nicht", "ein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PRF", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.12": {"line.1": {"text": "Gingging, gingging, welch ein Kamel!", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", ",", "welch", "ein", "Ka\u00b7mel", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "PWAT", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Er kennt nicht seine eigene Seel.", "tokens": ["Er", "kennt", "nicht", "sei\u00b7ne", "ei\u00b7ge\u00b7ne", "Seel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.13": {"line.1": {"text": "Gingging, gingging, wie dumm, wie dumm!", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", ",", "wie", "dumm", ",", "wie", "dumm", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "PWAV", "ADJD", "$,", "PWAV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er fragt: wieso, weshalb, warum?", "tokens": ["Er", "fragt", ":", "wie\u00b7so", ",", "we\u00b7shalb", ",", "wa\u00b7rum", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "ADV", "$,", "PWAV", "$,", "PWAV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.14": {"line.1": {"text": "Gingging, gingging, man glaubt es kaum:", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", ",", "man", "glaubt", "es", "kaum", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es denkt das Schaf sogar im Traum.", "tokens": ["Es", "denkt", "das", "Schaf", "so\u00b7gar", "im", "Traum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Gingging, gingging, ich habe es dick:", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", ",", "ich", "ha\u00b7be", "es", "dick", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "PPER", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Der Kerl \u00fcbt an sich selbst Kritik!", "tokens": ["Der", "Kerl", "\u00fcbt", "an", "sich", "selbst", "Kri\u00b7tik", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PRF", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Gingging, gingging ... Da gabs einen Krach:", "tokens": ["Ging\u00b7ging", ",", "ging\u00b7ging", "...", "Da", "gabs", "ei\u00b7nen", "Krach", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$(", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Weg war das Monstrum, und ich war wach.", "tokens": ["Weg", "war", "das", "Monst\u00b7rum", ",", "und", "ich", "war", "wach", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "$,", "KON", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.17": {"line.1": {"text": "Wie seltsam kam \u2013 der Traum? Ach nein:", "tokens": ["Wie", "selt\u00b7sam", "kam", "\u2013", "der", "Traum", "?", "Ach", "nein", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "$(", "ART", "NN", "$.", "NN", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie seltsam kam der Tag mir vor!", "tokens": ["Wie", "selt\u00b7sam", "kam", "der", "Tag", "mir", "vor", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "ART", "NN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das da im Bett: \u2013 ich soll das sein?", "tokens": ["Das", "da", "im", "Bett", ":", "\u2013", "ich", "soll", "das", "sein", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "APPRART", "NN", "$.", "$(", "PPER", "VMFIN", "ART", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mein ganzes Ich das? \u2013: Kinderein! \u2013:", "tokens": ["Mein", "gan\u00b7zes", "Ich", "das", "?", "\u2013", ":", "Kin\u00b7der\u00b7ein", "!", "\u2013", ":"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "word", "punct", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "PPER", "PDS", "$.", "$(", "$.", "NN", "$.", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Teil, der sich ins Licht verlor.", "tokens": ["Ein", "Teil", ",", "der", "sich", "ins", "Licht", "ver\u00b7lor", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PRF", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Glied von mir:", "tokens": ["Ein", "Glied", "von", "mir", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPER", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Nichts weiter bin ich hier.", "tokens": ["Nichts", "wei\u00b7ter", "bin", "ich", "hier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "VAFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "In der heiligen Allnacht, im unendlichen Raum", "tokens": ["In", "der", "hei\u00b7li\u00b7gen", "All\u00b7nacht", ",", "im", "un\u00b7end\u00b7li\u00b7chen", "Raum"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "APPRART", "ADJA", "NN"], "meter": "--+--++--+--+", "measure": "anapaest.di.plus"}, "line.9": {"text": "Streck ich mich, dehn ich mich tausendgestaltig,", "tokens": ["Streck", "ich", "mich", ",", "dehn", "ich", "mich", "tau\u00b7send\u00b7ge\u00b7stal\u00b7tig", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PRF", "$,", "VVIMP", "PPER", "PRF", "ADJD", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.10": {"text": "Bin Pflanze, Luft, Stein, Wasser, Tier:", "tokens": ["Bin", "Pflan\u00b7ze", ",", "Luft", ",", "Stein", ",", "Was\u00b7ser", ",", "Tier", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Leben in allen Formen, l\u00e4cherlich und gewaltig.", "tokens": ["Le\u00b7ben", "in", "al\u00b7len", "For\u00b7men", ",", "l\u00e4\u00b7cher\u00b7lich", "und", "ge\u00b7wal\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "$,", "ADJD", "KON", "ADJD", "$."], "meter": "+--+-+-+-+--+-", "measure": "hexameter"}}, "stanza.18": {"line.1": {"text": "Gingging, es denkt das Schaf sogar im Traum.", "tokens": ["Ging\u00b7ging", ",", "es", "denkt", "das", "Schaf", "so\u00b7gar", "im", "Traum", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PPER", "VVFIN", "ART", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}