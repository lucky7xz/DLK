{"dta.poem.2979": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "25.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1838", "urn": "urn:nbn:de:kobv:b4-200905195108", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Nichts Greuelvollres ist berichtet im Berichte", "tokens": ["Nichts", "Greu\u00b7el\u00b7voll\u00b7res", "ist", "be\u00b7rich\u00b7tet", "im", "Be\u00b7rich\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "NE", "VAFIN", "VVPP", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der zwar von Greueln ganz erf\u00fcllten Weltgeschichte,", "tokens": ["Der", "zwar", "von", "Greu\u00b7eln", "ganz", "er\u00b7f\u00fcll\u00b7ten", "Welt\u00b7ge\u00b7schich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "NN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Als wenn ein fremdes Volk, an Glauben fremd' und Sitt',", "tokens": ["Als", "wenn", "ein", "frem\u00b7des", "Volk", ",", "an", "Glau\u00b7ben", "fremd'", "und", "Sitt'", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "ADJA", "NN", "$,", "APPR", "NN", "PTKVZ", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Eroberisch ein unbekanntes Land betritt.", "tokens": ["Er\u00b7o\u00b7be\u00b7risch", "ein", "un\u00b7be\u00b7kann\u00b7tes", "Land", "be\u00b7tritt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Der Sieger, sei er auch von Hausaus mild und g\u00fctig,", "tokens": ["Der", "Sie\u00b7ger", ",", "sei", "er", "auch", "von", "Haus\u00b7aus", "mild", "und", "g\u00fc\u00b7tig", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "VAFIN", "PPER", "ADV", "APPR", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch die Besiegten w\u00fcrgt er schonungslos kaltbl\u00fctig.", "tokens": ["Doch", "die", "Be\u00b7sieg\u00b7ten", "w\u00fcrgt", "er", "scho\u00b7nungs\u00b7los", "kalt\u00b7bl\u00fc\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+--++-", "measure": "iambic.hexa.relaxed"}}, "stanza.4": {"line.1": {"text": "Warum? es machet wild ihn ein wildfremd Gefild,", "tokens": ["Wa\u00b7rum", "?", "es", "ma\u00b7chet", "wild", "ihn", "ein", "wild\u00b7fremd", "Ge\u00b7fild", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$.", "PPER", "VVFIN", "ADJD", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und nicht als seins erkennt er andrer Menschheit Bild.", "tokens": ["Und", "nicht", "als", "seins", "er\u00b7kennt", "er", "an\u00b7drer", "Menschheit", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "KOUS", "PPER", "VVFIN", "PPER", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.5": {"line.1": {"text": "In fremdgekleideten, fremdblickend fremdgef\u00e4rbten,", "tokens": ["In", "fremd\u00b7ge\u00b7klei\u00b7de\u00b7ten", ",", "fremd\u00b7bli\u00b7ckend", "fremd\u00b7ge\u00b7f\u00e4rb\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "$,", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Fremdredenden vernimmt er nichts vom Angeerbten.", "tokens": ["Fremd\u00b7re\u00b7den\u00b7den", "ver\u00b7nimmt", "er", "nichts", "vom", "An\u00b7ge\u00b7erb\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PIS", "APPRART", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}, "stanza.6": {"line.1": {"text": "Nicht die Bewegung f\u00fchlt er seiner Eingeweide,", "tokens": ["Nicht", "die", "Be\u00b7we\u00b7gung", "f\u00fchlt", "er", "sei\u00b7ner", "Ein\u00b7ge\u00b7wei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die jeder Bruder f\u00fchlt bei seines Bruders Leide.", "tokens": ["Die", "je\u00b7der", "Bru\u00b7der", "f\u00fchlt", "bei", "sei\u00b7nes", "Bru\u00b7ders", "Lei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Gottes Gepr\u00e4ge mit dem Stempel der Natur,", "tokens": ["Got\u00b7tes", "Ge\u00b7pr\u00e4\u00b7ge", "mit", "dem", "Stem\u00b7pel", "der", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "In seiner Schrift und Form h\u00e4lt er f\u00fcr echt sie nur.", "tokens": ["In", "sei\u00b7ner", "Schrift", "und", "Form", "h\u00e4lt", "er", "f\u00fcr", "echt", "sie", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "NN", "VVFIN", "PPER", "APPR", "ADJD", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Und fragt er sich, ob sie sein Sch\u00f6pfer auch erschaffen,", "tokens": ["Und", "fragt", "er", "sich", ",", "ob", "sie", "sein", "Sch\u00f6p\u00b7fer", "auch", "er\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PRF", "$,", "KOUS", "PPER", "PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gibt ers nur zu im Grimm und sich zum Spott als Affen.", "tokens": ["Gibt", "ers", "nur", "zu", "im", "Grimm", "und", "sich", "zum", "Spott", "als", "Af\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "APPR", "APPRART", "NE", "KON", "PRF", "APPRART", "NN", "KOUS", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Wie Tiger nicht und Wolf bei Rehes Mord und Lamms", "tokens": ["Wie", "Ti\u00b7ger", "nicht", "und", "Wolf", "bei", "Re\u00b7hes", "Mord", "und", "Lamms"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "PTKNEG", "KON", "NE", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gewissensbisse f\u00fchlt, weil sie sind andern Stamms.", "tokens": ["Ge\u00b7wis\u00b7sens\u00b7bis\u00b7se", "f\u00fchlt", ",", "weil", "sie", "sind", "an\u00b7dern", "Stamms", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "KOUS", "PPER", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Wie seit Jahrhunderten Mohammedaner hetzten", "tokens": ["Wie", "seit", "Jahr\u00b7hun\u00b7der\u00b7ten", "Mo\u00b7ham\u00b7me\u00b7da\u00b7ner", "hetz\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "NN", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Harmlose Indier, die kaum sich widersetzten.", "tokens": ["Harm\u00b7lo\u00b7se", "In\u00b7dier", ",", "die", "kaum", "sich", "wi\u00b7der\u00b7setz\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "ADV", "PRF", "VVFIN", "$."], "meter": "+---+-+-+-+-", "measure": "dactylic.init"}}, "stanza.11": {"line.1": {"text": "Die, wann sie erst im Kampf die M\u00e4nner \u00fcbermannten,", "tokens": ["Die", ",", "wann", "sie", "erst", "im", "Kampf", "die", "M\u00e4n\u00b7ner", "\u00fc\u00b7berm\u00b7ann\u00b7ten", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PPER", "ADV", "APPRART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wehrlose St\u00e4dte drauf und Tempel niederbrannten;", "tokens": ["Wehr\u00b7lo\u00b7se", "St\u00e4d\u00b7te", "drauf", "und", "Tem\u00b7pel", "nie\u00b7der\u00b7brann\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "KON", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}, "stanza.12": {"line.1": {"text": "Und wo ein H\u00e4uflein sich entzog durch scheue Flucht,", "tokens": ["Und", "wo", "ein", "H\u00e4uf\u00b7lein", "sich", "ent\u00b7zog", "durch", "scheu\u00b7e", "Flucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "PRF", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Auch diesem Wilde gab nicht Freistatt Wald und Schlucht:", "tokens": ["Auch", "die\u00b7sem", "Wil\u00b7de", "gab", "nicht", "Freis\u00b7tatt", "Wald", "und", "Schlucht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "NN", "VVFIN", "PTKNEG", "NN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Gehalten ward auf sie ein ordentliches Jagen,", "tokens": ["Ge\u00b7hal\u00b7ten", "ward", "auf", "sie", "ein", "or\u00b7dent\u00b7li\u00b7ches", "Ja\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Erlegtes Menschenwild gez\u00e4hlt mit Wohlbehagen.", "tokens": ["Er\u00b7leg\u00b7tes", "Men\u00b7schen\u00b7wild", "ge\u00b7z\u00e4hlt", "mit", "Wohl\u00b7be\u00b7ha\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVPP", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Wer hat der wilden Jagd gesetzet Ziel und Fristen?", "tokens": ["Wer", "hat", "der", "wil\u00b7den", "Jagd", "ge\u00b7set\u00b7zet", "Ziel", "und", "Fris\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "VVPP", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gesegnet seien, die zuletzt es thaten, Christen;", "tokens": ["Ge\u00b7seg\u00b7net", "sei\u00b7en", ",", "die", "zu\u00b7letzt", "es", "tha\u00b7ten", ",", "Chris\u00b7ten", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVPP", "VAFIN", "$,", "PRELS", "ADV", "PPER", "VVFIN", "$,", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Zuletzt es thaten, als sie besser sich besonnen,", "tokens": ["Zu\u00b7letzt", "es", "tha\u00b7ten", ",", "als", "sie", "bes\u00b7ser", "sich", "be\u00b7son\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "$,", "KOUS", "PPER", "ADJD", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nachdem sie besser nicht, und schlechter fast begonnen.", "tokens": ["Nach\u00b7dem", "sie", "bes\u00b7ser", "nicht", ",", "und", "schlech\u00b7ter", "fast", "be\u00b7gon\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "PTKNEG", "$,", "KON", "ADJD", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Gesegnet seien sie, nicht weil sie Christen sind,", "tokens": ["Ge\u00b7seg\u00b7net", "sei\u00b7en", "sie", ",", "nicht", "weil", "sie", "Chris\u00b7ten", "sind", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPER", "$,", "PTKNEG", "KOUS", "PPER", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch Menschen, weniger f\u00fcr fremde Menschheit blind.", "tokens": ["Doch", "Men\u00b7schen", ",", "we\u00b7ni\u00b7ger", "f\u00fcr", "frem\u00b7de", "Menschheit", "blind", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "ADV", "APPR", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Gesegnet aber sei, die langsam langsam schreitet,", "tokens": ["Ge\u00b7seg\u00b7net", "a\u00b7ber", "sei", ",", "die", "lang\u00b7sam", "lang\u00b7sam", "schrei\u00b7tet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VAFIN", "$,", "PRELS", "ADJD", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bildung, doch durch die Welt sich weiter weiter breitet.", "tokens": ["Bil\u00b7dung", ",", "doch", "durch", "die", "Welt", "sich", "wei\u00b7ter", "wei\u00b7ter", "brei\u00b7tet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "APPR", "ART", "NN", "PRF", "ADV", "ADV", "VVFIN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}, "stanza.18": {"line.1": {"text": "Die Bildung, die dazu will alle Sprachen lernen,", "tokens": ["Die", "Bil\u00b7dung", ",", "die", "da\u00b7zu", "will", "al\u00b7le", "Spra\u00b7chen", "ler\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PAV", "VMFIN", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und V\u00f6lkersitte sehn in allen L\u00e4nderfernen,", "tokens": ["Und", "V\u00f6l\u00b7ker\u00b7sit\u00b7te", "sehn", "in", "al\u00b7len", "L\u00e4n\u00b7der\u00b7fer\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Damit die Menschheit einst, von einem Band umschlungen,", "tokens": ["Da\u00b7mit", "die", "Menschheit", "einst", ",", "von", "ei\u00b7nem", "Band", "um\u00b7schlun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "$,", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "In allen Farben sich erkenn' und allen Zungen.", "tokens": ["In", "al\u00b7len", "Far\u00b7ben", "sich", "er\u00b7kenn'", "und", "al\u00b7len", "Zun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PRF", "VVFIN", "KON", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}