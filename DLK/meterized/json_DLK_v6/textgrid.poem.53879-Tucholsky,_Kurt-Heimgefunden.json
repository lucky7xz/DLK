{"textgrid.poem.53879": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Heimgefunden", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Na, Gottseidank! Nun sind wir ja soweit,", "tokens": ["Na", ",", "Gott\u00b7sei\u00b7dank", "!", "Nun", "sind", "wir", "ja", "so\u00b7weit", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "NN", "$.", "ADV", "VAFIN", "PPER", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "da\u00df jeder: Hoch die Republike! schreit \u2013", "tokens": ["da\u00df", "je\u00b7der", ":", "Hoch", "die", "Re\u00b7pub\u00b7li\u00b7ke", "!", "schreit", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PIS", "$.", "ADJD", "ART", "NN", "$.", "ADJD", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "es war auch h\u00f6chste Zeit.", "tokens": ["es", "war", "auch", "h\u00f6chs\u00b7te", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Ja, fr\u00fcher! Ohne da\u00df du es verlangst,", "tokens": ["Ja", ",", "fr\u00fc\u00b7her", "!", "Oh\u00b7ne", "da\u00df", "du", "es", "ver\u00b7langst", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADJD", "$.", "APPR", "KOUS", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "da hatten alle vor dem Dingrichs Angst \u2013", "tokens": ["da", "hat\u00b7ten", "al\u00b7le", "vor", "dem", "Ding\u00b7richs", "Angst", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "man kann nie wissen, wie? Sie schlichen dumm", "tokens": ["man", "kann", "nie", "wis\u00b7sen", ",", "wie", "?", "Sie", "schli\u00b7chen", "dumm"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VMFIN", "ADV", "VVINF", "$,", "PWAV", "$.", "PPER", "VVFIN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "mi\u00dftrauisch um den neuen Balg herum.", "tokens": ["mi\u00df\u00b7trau\u00b7isch", "um", "den", "neu\u00b7en", "Balg", "he\u00b7rum", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Und fa\u00dften leise tappend auch mal hin . . .", "tokens": ["Und", "fa\u00df\u00b7ten", "lei\u00b7se", "tap\u00b7pend", "auch", "mal", "hin", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "VVFIN", "ADJD", "ADJD", "ADV", "ADV", "PTKVZ", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Bei\u00dft sie? Bei\u00dft sie nicht? Bei\u00dft sie? Bei\u00dft sie nicht? . . .", "tokens": ["Bei\u00dft", "sie", "?", "Bei\u00dft", "sie", "nicht", "?", "Bei\u00dft", "sie", "?", "Bei\u00dft", "sie", "nicht", "?", ".", ".", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["VVFIN", "PPER", "$.", "VVFIN", "PPER", "PTKNEG", "$.", "VVFIN", "PPER", "$.", "VVFIN", "PPER", "PTKNEG", "$.", "$.", "$.", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Der armen Republik war nicht danach zu Sinn.", "tokens": ["Der", "ar\u00b7men", "Re\u00b7pub\u00b7lik", "war", "nicht", "da\u00b7nach", "zu", "Sinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PTKNEG", "PAV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die bi\u00df nicht, als der Wilhelm kniff;", "tokens": ["Die", "bi\u00df", "nicht", ",", "als", "der", "Wil\u00b7helm", "kniff", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PTKNEG", "$,", "KOUS", "ART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die bi\u00df nicht, als der Kapp was pfiff;", "tokens": ["die", "bi\u00df", "nicht", ",", "als", "der", "Kapp", "was", "pfiff", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PTKNEG", "$,", "KOUS", "ART", "NN", "PWS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die bi\u00df nicht, als Erzberger fiel,", "tokens": ["die", "bi\u00df", "nicht", ",", "als", "Erz\u00b7ber\u00b7ger", "fiel", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PTKNEG", "$,", "KOUS", "NN", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "als Rathenau fiel,", "tokens": ["als", "Ra\u00b7the\u00b7nau", "fiel", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "und als Haase fiel \u2013", "tokens": ["und", "als", "Haa\u00b7se", "fiel", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NN", "VVFIN", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "die bi\u00df keine Reichswehrkompanie \u2013", "tokens": ["die", "bi\u00df", "kei\u00b7ne", "Reichs\u00b7wehr\u00b7kom\u00b7pa\u00b7nie", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.8": {"text": "die bi\u00df nie.", "tokens": ["die", "bi\u00df", "nie", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "APPR", "ADV", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Hat sich, im Gegenteil, sch\u00f6n gewandelt.", "tokens": ["Hat", "sich", ",", "im", "Ge\u00b7gen\u00b7teil", ",", "sch\u00f6n", "ge\u00b7wan\u00b7delt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "$,", "APPRART", "NN", "$,", "ADJD", "VVPP", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hat nachgelassen und kuhgehandelt,", "tokens": ["Hat", "nach\u00b7ge\u00b7las\u00b7sen", "und", "kuh\u00b7ge\u00b7han\u00b7delt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "KON", "VVFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "mal lag sie unten, mal lagen die andern oben,", "tokens": ["mal", "lag", "sie", "un\u00b7ten", ",", "mal", "la\u00b7gen", "die", "an\u00b7dern", "o\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "ADV", "VVFIN", "ART", "ADJA", "ADV", "$,"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und heute darf jeder das Dingrichs loben.", "tokens": ["und", "heu\u00b7te", "darf", "je\u00b7der", "das", "Ding\u00b7richs", "lo\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PIS", "ART", "NN", "VVINF", "$."], "meter": "-+-++--+-+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Kaiserreich? Republik? Welches von beiden?", "tokens": ["Kai\u00b7ser\u00b7reich", "?", "Re\u00b7pub\u00b7lik", "?", "Wel\u00b7ches", "von", "bei\u00b7den", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "NN", "$.", "PWS", "APPR", "PIAT", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.6": {"text": "Sie sind kaum noch zu unterscheiden.", "tokens": ["Sie", "sind", "kaum", "noch", "zu", "un\u00b7ter\u00b7schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Und nun kommen in hellen Haufen", "tokens": ["Und", "nun", "kom\u00b7men", "in", "hel\u00b7len", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "APPR", "ADJA", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "alle, alle angelaufen.", "tokens": ["al\u00b7le", ",", "al\u00b7le", "an\u00b7ge\u00b7lau\u00b7fen", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["PIS", "$,", "PIS", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die schlimmsten, \u00e4ltesten Reaktion\u00e4re,", "tokens": ["Die", "schlimms\u00b7ten", ",", "\u00e4l\u00b7tes\u00b7ten", "Re\u00b7ak\u00b7ti\u00b7o\u00b7n\u00e4\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "NN", "$,"], "meter": "-+-+---+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Past\u00f6re, Generale \u2013 ganze Heere \u2013 \u2013", "tokens": ["Pas\u00b7t\u00f6\u00b7re", ",", "Ge\u00b7ne\u00b7ra\u00b7le", "\u2013", "gan\u00b7ze", "Hee\u00b7re", "\u2013", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "$,", "NN", "$(", "ADJA", "NN", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "ist das ein Konjunkturisten-Rennen!", "tokens": ["ist", "das", "ein", "Kon\u00b7junk\u00b7tu\u00b7ris\u00b7ten\u00b7Ren\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Man darf sich, ob Sies glauben oder nicht,", "tokens": ["Man", "darf", "sich", ",", "ob", "Sies", "glau\u00b7ben", "o\u00b7der", "nicht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PRF", "$,", "KOUS", "PIS", "VVINF", "KON", "PTKNEG", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ruhig zur Republik bekennen.", "tokens": ["ru\u00b7hig", "zur", "Re\u00b7pub\u00b7lik", "be\u00b7ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die ist nicht aus Eisen \u2013 die ist aus Holz.", "tokens": ["Die", "ist", "nicht", "aus", "Ei\u00b7sen", "\u2013", "die", "ist", "aus", "Holz", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKNEG", "APPR", "NN", "$(", "PDS", "VAFIN", "APPR", "NN", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "Und die Republikaner sind noch so stolz \u2013!", "tokens": ["Und", "die", "Re\u00b7pub\u00b7li\u00b7ka\u00b7ner", "sind", "noch", "so", "stolz", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "ADV", "ADV", "ADJD", "$(", "$."], "meter": "---+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Gut ausgestopft und richtig gemischt.", "tokens": ["Gut", "aus\u00b7ge\u00b7stopft", "und", "rich\u00b7tig", "ge\u00b7mischt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Gehn Sie ruhig ran.", "tokens": ["Gehn", "Sie", "ru\u00b7hig", "ran", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Die tut Ihnen nischt.", "tokens": ["Die", "tut", "Ih\u00b7nen", "nischt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}