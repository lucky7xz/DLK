{"textgrid.poem.42504": {"metadata": {"author": {"name": "Liliencron, Detlev von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Aus Langerweile, im fremden Ort,", "genre": "verse", "period": "N.A.", "pub_year": 1876, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Aus Langerweile, im fremden Ort,", "tokens": ["Aus", "Lan\u00b7ger\u00b7wei\u00b7le", ",", "im", "frem\u00b7den", "Ort", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ging ich \u00fcber den Kirchhof fort,", "tokens": ["Ging", "ich", "\u00fc\u00b7ber", "den", "Kirch\u00b7hof", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Sah mir ein Kreuzchen an, einen Stein,", "tokens": ["Sah", "mir", "ein", "Kreuzc\u00b7hen", "an", ",", "ei\u00b7nen", "Stein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKVZ", "$,", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Manch seltsam Spr\u00fcchlein von Sterben und Sein,", "tokens": ["Manch", "selt\u00b7sam", "Spr\u00fcch\u00b7lein", "von", "Ster\u00b7ben", "und", "Sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "NN", "APPR", "NN", "KON", "PPOSAT", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.5": {"text": "Und lie\u00df mir zufl\u00fcstern von den Zypressen,", "tokens": ["Und", "lie\u00df", "mir", "zu\u00b7fl\u00fcs\u00b7tern", "von", "den", "Zyp\u00b7res\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-+--", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Da\u00df hier Alles l\u00e4ngst, l\u00e4ngst vergessen.", "tokens": ["Da\u00df", "hier", "Al\u00b7les", "l\u00e4ngst", ",", "l\u00e4ngst", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIS", "ADV", "$,", "ADV", "VVPP", "$."], "meter": "-++--+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Emiliens Grab \u2013 da blieb ich stehn,", "tokens": ["E\u00b7mi\u00b7li\u00b7ens", "Grab", "\u2013", "da", "blieb", "ich", "stehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$(", "ADV", "VVFIN", "PPER", "VVINF", "$,"], "meter": "----+-+-+", "measure": "unknown.measure.tri"}, "line.8": {"text": "War nichts andres drauf zu sehn,", "tokens": ["War", "nichts", "and\u00b7res", "drauf", "zu", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PIS", "PAV", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.9": {"text": "Weder Bibelwort, Zeit, noch Familienname,", "tokens": ["We\u00b7der", "Bi\u00b7bel\u00b7wort", ",", "Zeit", ",", "noch", "Fa\u00b7mi\u00b7li\u00b7en\u00b7na\u00b7me", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NN", "$,", "ADV", "NN", "$,"], "meter": "+-+--+---+-+-", "measure": "trochaic.penta.relaxed"}, "line.10": {"text": "Nur einzig stand drauf, wie eine Brosame:", "tokens": ["Nur", "ein\u00b7zig", "stand", "drauf", ",", "wie", "ei\u00b7ne", "Bro\u00b7sa\u00b7me", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PTKVZ", "$,", "PWAV", "ART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.11": {"text": "Emiliens Grab.", "tokens": ["E\u00b7mi\u00b7li\u00b7ens", "Grab", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.12": {"text": "Das fiel mir auf und ging mir ins Blut;", "tokens": ["Das", "fiel", "mir", "auf", "und", "ging", "mir", "ins", "Blut", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.13": {"text": "Mein Gott, wer war sie, die hier ruht?", "tokens": ["Mein", "Gott", ",", "wer", "war", "sie", ",", "die", "hier", "ruht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VAFIN", "PPER", "$,", "PRELS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Das Gras, die Fr\u00fchlingsblumen, die Bienen,", "tokens": ["Das", "Gras", ",", "die", "Fr\u00fch\u00b7lings\u00b7blu\u00b7men", ",", "die", "Bie\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "War Alles so froh von der Sonne beschienen.", "tokens": ["War", "Al\u00b7les", "so", "froh", "von", "der", "Son\u00b7ne", "be\u00b7schie\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "ADJD", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.16": {"text": "Doch hatte niemand den Platz gepflegt;", "tokens": ["Doch", "hat\u00b7te", "nie\u00b7mand", "den", "Platz", "ge\u00b7pflegt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.17": {"text": "Alles wucherte, ungehegt.", "tokens": ["Al\u00b7les", "wu\u00b7cher\u00b7te", ",", "un\u00b7ge\u00b7hegt", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "ADJD", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.18": {"text": "Nichts konnte auf dem Grabe prunken,", "tokens": ["Nichts", "konn\u00b7te", "auf", "dem", "Gra\u00b7be", "prun\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Selbst die Einfassung morschte versunken.", "tokens": ["Selbst", "die", "Ein\u00b7fas\u00b7sung", "morschte", "ver\u00b7sun\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich ging meiner Wege am Friedhofsrand,", "tokens": ["Ich", "ging", "mei\u00b7ner", "We\u00b7ge", "am", "Fried\u00b7hofs\u00b7rand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPRART", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Als ich endlich ein steinalt M\u00fctterchen fand.", "tokens": ["Als", "ich", "end\u00b7lich", "ein", "stei\u00b7nalt", "M\u00fct\u00b7ter\u00b7chen", "fand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "ADJD", "NN", "VVFIN", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}, "line.3": {"text": "\u00bbwas ist denn das dort mit der Emilie?", "tokens": ["\u00bb", "was", "ist", "denn", "das", "dort", "mit", "der", "E\u00b7mi\u00b7lie", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ADV", "ART", "ADV", "APPR", "ART", "NE", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Der Nachname fehlt ja; wie hie\u00df die Familie?\u00ab", "tokens": ["Der", "Nach\u00b7na\u00b7me", "fehlt", "ja", ";", "wie", "hie\u00df", "die", "Fa\u00b7mi\u00b7lie", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "$.", "PWAV", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ja, Herr, das ist wer wei\u00df wie viel Jahre;", "tokens": ["Ja", ",", "Herr", ",", "das", "ist", "wer", "wei\u00df", "wie", "viel", "Jah\u00b7re", ";"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$,", "PDS", "VAFIN", "PWS", "VVFIN", "KOKOM", "PIAT", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ich stand an ihrer Totenbahre.", "tokens": ["Ich", "stand", "an", "ih\u00b7rer", "To\u00b7ten\u00b7bah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "War ein jung Ding, einfacher Leute Kind,", "tokens": ["War", "ein", "jung", "Ding", ",", "ein\u00b7fa\u00b7cher", "Leu\u00b7te", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJD", "NN", "$,", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Doch wie sie dann alle leichtgl\u00e4ubig sind:", "tokens": ["Doch", "wie", "sie", "dann", "al\u00b7le", "leicht\u00b7gl\u00e4u\u00b7big", "sind", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "PIS", "ADJD", "VAFIN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Kam ein fremder Mann angegangen,", "tokens": ["Kam", "ein", "frem\u00b7der", "Mann", "an\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "--+-++-+-", "measure": "anapaest.init"}, "line.10": {"text": "Hat sie in seine Netze gefangen,", "tokens": ["Hat", "sie", "in", "sei\u00b7ne", "Net\u00b7ze", "ge\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Versprach ihr, sie auf sein Schlo\u00df zu bringen,", "tokens": ["Ver\u00b7sprach", "ihr", ",", "sie", "auf", "sein", "Schlo\u00df", "zu", "brin\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PPER", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Er sei reich und k\u00f6nn ihr Alles erschwingen.", "tokens": ["Er", "sei", "reich", "und", "k\u00f6nn", "ihr", "Al\u00b7les", "er\u00b7schwin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "VMFIN", "PPER", "PIS", "VVPP", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Und hat sie geheiratet. Dann zogen sie fort,", "tokens": ["Und", "hat", "sie", "ge\u00b7hei\u00b7ra\u00b7tet", ".", "Dann", "zo\u00b7gen", "sie", "fort", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "VVPP", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.14": {"text": "Fern weg an den Rhein; da ist sie verdorrt.", "tokens": ["Fern", "weg", "an", "den", "Rhein", ";", "da", "ist", "sie", "ver\u00b7dorrt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "ART", "NE", "$.", "ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.15": {"text": "War Alles Schwindel, war Alles erlogen,", "tokens": ["War", "Al\u00b7les", "Schwin\u00b7del", ",", "war", "Al\u00b7les", "er\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "$,", "VAFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-++--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Er hat sie in seinen Schmutz gezogen.", "tokens": ["Er", "hat", "sie", "in", "sei\u00b7nen", "Schmutz", "ge\u00b7zo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.17": {"text": "Hat sie verlassen. Und sie kam wieder", "tokens": ["Hat", "sie", "ver\u00b7las\u00b7sen", ".", "Und", "sie", "kam", "wie\u00b7der"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "VVINF", "$.", "KON", "PPER", "VVFIN", "ADV"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Und brach am Haus ihrer Mutter nieder,", "tokens": ["Und", "brach", "am", "Haus", "ih\u00b7rer", "Mut\u00b7ter", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.19": {"text": "Ist schnell gestorben aus Elend und Gram,", "tokens": ["Ist", "schnell", "ge\u00b7stor\u00b7ben", "aus", "E\u00b7lend", "und", "Gram", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.20": {"text": "Konnte nicht l\u00e4nger ertragen die Scham.", "tokens": ["Konn\u00b7te", "nicht", "l\u00e4n\u00b7ger", "er\u00b7tra\u00b7gen", "die", "Scham", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "ADJD", "VVFIN", "ART", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.21": {"text": "Die Mutter, von Ha\u00df und Wut ganz besessen,", "tokens": ["Die", "Mut\u00b7ter", ",", "von", "Ha\u00df", "und", "Wut", "ganz", "be\u00b7ses\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "NN", "KON", "NN", "ADV", "VVPP", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.22": {"text": "Wollt ihres Eidams Namen vergessen,", "tokens": ["Wollt", "ih\u00b7res", "Ei\u00b7dams", "Na\u00b7men", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPOSAT", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.23": {"text": "Hat ein Kreuz ihr gesetzt, als sich das begab,", "tokens": ["Hat", "ein", "Kreuz", "ihr", "ge\u00b7setzt", ",", "als", "sich", "das", "be\u00b7gab", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "VVPP", "$,", "KOUS", "PRF", "PDS", "VVFIN", "$,"], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}, "line.24": {"text": "Steht weiter nichts drauf als:", "tokens": ["Steht", "wei\u00b7ter", "nichts", "drauf", "als", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIS", "PAV", "KOUS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.25": {"text": "Emiliens Grab.", "tokens": ["E\u00b7mi\u00b7li\u00b7ens", "Grab", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}}}}