{"textgrid.poem.53801": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Psychoanalyse", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Drei Irre gingen in den Garten", "tokens": ["Drei", "Ir\u00b7re", "gin\u00b7gen", "in", "den", "Gar\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und wollten auf die Antwort warten.", "tokens": ["und", "woll\u00b7ten", "auf", "die", "Ant\u00b7wort", "war\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der erste Irre sprach:", "tokens": ["Der", "ers\u00b7te", "Ir\u00b7re", "sprach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbo Freud!", "tokens": ["\u00bb", "o", "Freud", "!"], "token_info": ["punct", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.3": {"text": "Hat dich noch niemals nicht gereut,", "tokens": ["Hat", "dich", "noch", "nie\u00b7mals", "nicht", "ge\u00b7reut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "da\u00df du Sch\u00fcler hast? Und was f\u00fcr welche \u2013?", "tokens": ["da\u00df", "du", "Sch\u00fc\u00b7ler", "hast", "?", "Und", "was", "f\u00fcr", "wel\u00b7che", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "NN", "VAFIN", "$.", "KON", "PWS", "APPR", "PRELS", "$(", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "Sie gehen an keinem vor\u00fcber, die Kelche.", "tokens": ["Sie", "ge\u00b7hen", "an", "kei\u00b7nem", "vor\u00b7\u00fc\u00b7ber", ",", "die", "Kel\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "PTKVZ", "$,", "ART", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.6": {"text": "Ich kenne ja wirklich allerhand", "tokens": ["Ich", "ken\u00b7ne", "ja", "wirk\u00b7lich", "al\u00b7ler\u00b7hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "PIAT"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "als Mitglied vom Deutschen Reichsirrenverband \u2013", "tokens": ["als", "Mit\u00b7glied", "vom", "Deut\u00b7schen", "Reichs\u00b7ir\u00b7ren\u00b7ver\u00b7band", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "--+-+-++--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "aber die alten Doktoren sind mir beinah lieber", "tokens": ["a\u00b7ber", "die", "al\u00b7ten", "Dok\u00b7to\u00b7ren", "sind", "mir", "bei\u00b7nah", "lie\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "NN", "VAFIN", "PPER", "ADV", "ADV"], "meter": "+--+--+-+-+-+-", "measure": "elegiambus"}, "line.9": {"text": "als das Getue dieser", "tokens": ["als", "das", "Ge\u00b7tue", "die\u00b7ser"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PDAT"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.10": {"text": "Ja.\u00ab", "tokens": ["Ja", ".", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["PTKANT", "$.", "$("], "meter": "+", "measure": "single.up"}}, "stanza.3": {"line.1": {"text": "Der zweite Irre sprach:", "tokens": ["Der", "zwei\u00b7te", "Ir\u00b7re", "sprach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbschmecks.", "tokens": ["\u00bb", "schmecks", "."], "token_info": ["punct", "word", "punct"], "pos": ["$(", "ADJD", "$."], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Ich habe hinten einen Komplex.", "tokens": ["Ich", "ha\u00b7be", "hin\u00b7ten", "ei\u00b7nen", "Kom\u00b7plex", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Den hab ich nicht richtig abreagiert,", "tokens": ["Den", "hab", "ich", "nicht", "rich\u00b7tig", "ab\u00b7re\u00b7a\u00b7giert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "PTKNEG", "ADJD", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "jetzt ist mir die Unterhose fixiert.", "tokens": ["jetzt", "ist", "mir", "die", "Un\u00b7ter\u00b7ho\u00b7se", "fi\u00b7xiert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Und ich versp\u00fcre mit gro\u00dfer Beklemmung", "tokens": ["Und", "ich", "ver\u00b7sp\u00fc\u00b7re", "mit", "gro\u00b7\u00dfer", "Be\u00b7klem\u00b7mung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "rechts eine Hemmung und links eine Hemmung.", "tokens": ["rechts", "ei\u00b7ne", "Hem\u00b7mung", "und", "links", "ei\u00b7ne", "Hem\u00b7mung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "KON", "ADV", "ART", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Vorn h\u00e4ngt meine \u00e4ltere Schwester", "tokens": ["Vorn", "h\u00e4ngt", "mei\u00b7ne", "\u00e4l\u00b7te\u00b7re", "Schwes\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.9": {"text": "und in der Mitte bin ich ziemlich gesund.", "tokens": ["und", "in", "der", "Mit\u00b7te", "bin", "ich", "ziem\u00b7lich", "ge\u00b7sund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "Ja.\u00ab", "tokens": ["Ja", ".", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["PTKANT", "$.", "$("], "meter": "+", "measure": "single.up"}}, "stanza.4": {"line.1": {"text": "Der dritte Irre sprach:", "tokens": ["Der", "drit\u00b7te", "Ir\u00b7re", "sprach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbwenn", "tokens": ["\u00bb", "wenn"], "token_info": ["punct", "word"], "pos": ["$(", "KOUS"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "heut einer mal mu\u00df, dann sagt ers nicht, denn", "tokens": ["heut", "ei\u00b7ner", "mal", "mu\u00df", ",", "dann", "sagt", "ers", "nicht", ",", "denn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "ART", "ADV", "VMFIN", "$,", "ADV", "VVFIN", "PIS", "PTKNEG", "$,", "KON"], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "er umwickelt sich mit d\u00fcstern Neurosen,", "tokens": ["er", "um\u00b7wi\u00b7ckelt", "sich", "mit", "d\u00fcs\u00b7tern", "Neu\u00b7ro\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ADJA", "NN", "$,"], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "mit Analfunktionen und Stumpfdiagnosen \u2013\u00ab", "tokens": ["mit", "A\u00b7nal\u00b7funk\u00b7ti\u00b7o\u00b7nen", "und", "Stumpf\u00b7di\u00b7ag\u00b7no\u00b7sen", "\u2013", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$(", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "(\u00bbHa! \u2013 Stumpf!\u00ab riefen die beiden andern Irren,", "tokens": ["(", "\u00bb", "Ha", "!", "\u2013", "Stumpf", "!", "\u00ab", "rie\u00b7fen", "die", "bei\u00b7den", "an\u00b7dern", "Ir\u00b7ren", ","], "token_info": ["punct", "punct", "word", "punct", "punct", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "$(", "ITJ", "$.", "$(", "NN", "$.", "$(", "VVFIN", "ART", "PIAT", "ADJA", "NN", "$,"], "meter": "-+---+-+-+-", "measure": "dactylic.init"}, "line.7": {"text": "konnten den dritten aber nicht verwirren.", "tokens": ["konn\u00b7ten", "den", "drit\u00b7ten", "a\u00b7ber", "nicht", "ver\u00b7wir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADJA", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.8": {"text": "Der fuhr fort:)", "tokens": ["Der", "fuhr", "fort", ":)"], "token_info": ["word", "word", "word", "emoticon"], "pos": ["PDS", "VVFIN", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "\u00bbvorlust, Nachlust und n\u00e4chtliches Zaudern \u2013", "tokens": ["\u00bb", "vor\u00b7lust", ",", "Nach\u00b7lust", "und", "n\u00e4cht\u00b7li\u00b7ches", "Zau\u00b7dern", "\u2013"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "NN", "KON", "ADJA", "NN", "$("], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.10": {"text": "es macht so viel Spa\u00df, dar\u00fcber zu plaudern!", "tokens": ["es", "macht", "so", "viel", "Spa\u00df", ",", "da\u00b7r\u00fc\u00b7ber", "zu", "plau\u00b7dern", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN", "$,", "PAV", "PTKZU", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Die Fachdebatte \u2013 welch ein Genu\u00df! \u2013", "tokens": ["Die", "Fach\u00b7de\u00b7bat\u00b7te", "\u2013", "welch", "ein", "Ge\u00b7nu\u00df", "!", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "$(", "PWAT", "ART", "NN", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.12": {"text": "ist beinah so sch\u00f6n wie ein", "tokens": ["ist", "bei\u00b7nah", "so", "sch\u00f6n", "wie", "ein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "KOKOM", "ART"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.13": {"text": "Ja.\u00ab", "tokens": ["Ja", ".", "\u00ab"], "token_info": ["word", "punct", "punct"], "pos": ["PTKANT", "$.", "$("], "meter": "+", "measure": "single.up"}}, "stanza.5": {"line.1": {"text": "Die drei Irren sangen nun im Verein:", "tokens": ["Die", "drei", "Ir\u00b7ren", "san\u00b7gen", "nun", "im", "Ver\u00b7ein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "--+-+-++-+", "measure": "anapaest.init"}, "line.2": {"text": "\u00bbwir wollen keine Freudisten sein!", "tokens": ["\u00bb", "wir", "wol\u00b7len", "kei\u00b7ne", "Freu\u00b7dis\u00b7ten", "sein", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die jungen Leute, die davon kohlen,", "tokens": ["Die", "jun\u00b7gen", "Leu\u00b7te", ",", "die", "da\u00b7von", "koh\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "PAV", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "denen sollte man kr\u00e4ftig das Fell versohlen.", "tokens": ["de\u00b7nen", "soll\u00b7te", "man", "kr\u00e4f\u00b7tig", "das", "Fell", "ver\u00b7soh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PIS", "ADJD", "ART", "NN", "VVINF", "$."], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.5": {"text": "Erreichen sie jemals das Genie?", "tokens": ["Er\u00b7rei\u00b7chen", "sie", "je\u00b7mals", "das", "Ge\u00b7nie", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "O na nie \u2013!", "tokens": ["O", "na", "nie", "\u2013", "!"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ITJ", "ITJ", "ADV", "$(", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Jeder J\u00fcngling von etwas guten Manieren", "tokens": ["Je\u00b7der", "J\u00fcng\u00b7ling", "von", "et\u00b7was", "gu\u00b7ten", "Ma\u00b7nie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "APPR", "PIAT", "ADJA", "NN"], "meter": "+-+----+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "geht heute mal Muttern deflorieren.", "tokens": ["geht", "heu\u00b7te", "mal", "Mut\u00b7tern", "de\u00b7flo\u00b7rie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "NN", "VVINF", "$."], "meter": "++--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Jede Frau, die in die Epoche pa\u00dft,", "tokens": ["Je\u00b7de", "Frau", ",", "die", "in", "die", "E\u00b7po\u00b7che", "pa\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "hat schon mal ihren Vater geha\u00dft.", "tokens": ["hat", "schon", "mal", "ih\u00b7ren", "Va\u00b7ter", "ge\u00b7ha\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Und die ganze Geschichte stammt aus Wien,", "tokens": ["Und", "die", "gan\u00b7ze", "Ge\u00b7schich\u00b7te", "stammt", "aus", "Wi\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN", "APPR", "NE", "$,"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.6": {"text": "und darum ist sie besonders schien \u2013!", "tokens": ["und", "da\u00b7rum", "ist", "sie", "be\u00b7son\u00b7ders", "schien", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PAV", "VAFIN", "PPER", "ADV", "VVFIN", "$(", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "Wir drei Irre sehen, wie Liebespaare", "tokens": ["Wir", "drei", "Ir\u00b7re", "se\u00b7hen", ",", "wie", "Lie\u00b7be\u00b7spaa\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "CARD", "NN", "VVINF", "$,", "PWAV", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "sich gegenseitig die sch\u00f6nsten Haare", "tokens": ["sich", "ge\u00b7gen\u00b7sei\u00b7tig", "die", "sch\u00f6ns\u00b7ten", "Haa\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "ADJD", "ART", "ADJA", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "spalten \u2013 und rufen jetzt rund und nett:", "tokens": ["spal\u00b7ten", "\u2013", "und", "ru\u00b7fen", "jetzt", "rund", "und", "nett", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "KON", "VVFIN", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Rein ins Bett oder raus aus dem Bett!", "tokens": ["Rein", "ins", "Bett", "o\u00b7der", "raus", "aus", "dem", "Bett", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "KON", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Keine Tischkante ohne Symbol und kein Loch . . .", "tokens": ["Kei\u00b7ne", "Tischkan\u00b7te", "oh\u00b7ne", "Sym\u00b7bol", "und", "kein", "Loch", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PIAT", "NN", "APPR", "NN", "KON", "PIAT", "NN", "$.", "$.", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Wie lange noch \u2013? Wie lange noch \u2013?\u00ab", "tokens": ["Wie", "lan\u00b7ge", "noch", "\u2013", "?", "Wie", "lan\u00b7ge", "noch", "\u2013", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWAV", "ADV", "ADV", "$(", "$.", "PWAV", "ADV", "ADV", "$(", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Drei Irre standen in dem Garten", "tokens": ["Drei", "Ir\u00b7re", "stan\u00b7den", "in", "dem", "Gar\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und t\u00e4ten auf die Antwort warten.", "tokens": ["und", "t\u00e4\u00b7ten", "auf", "die", "Ant\u00b7wort", "war\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}