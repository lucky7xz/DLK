{"textgrid.poem.44532": {"metadata": {"author": {"name": "Grillparzer, Franz", "birth": "N.A.", "death": "N.A."}, "title": "An die \u00dcberdeutschen", "genre": "verse", "period": "N.A.", "pub_year": 1843, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Macht nur nicht so ernste Gesichter,", "tokens": ["Macht", "nur", "nicht", "so", "erns\u00b7te", "Ge\u00b7sich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PTKNEG", "ADV", "ADJA", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Am End ist ja viel doch nur Spa\u00df,", "tokens": ["Am", "End", "ist", "ja", "viel", "doch", "nur", "Spa\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ADV", "ADV", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr seid nicht Geschworne noch Richter,", "tokens": ["Ihr", "seid", "nicht", "Ge\u00b7schwor\u00b7ne", "noch", "Rich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "NN", "ADV", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und w\u00e4rs auch, was hindert uns das?", "tokens": ["Und", "w\u00e4rs", "auch", ",", "was", "hin\u00b7dert", "uns", "das", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "$,", "PWS", "VVFIN", "PPER", "PDS", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.2": {"line.1": {"text": "Seht nur eure Nachbarn, die Franken,", "tokens": ["Seht", "nur", "eu\u00b7re", "Nach\u00b7barn", ",", "die", "Fran\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Den Briten, das wandelnde Fa\u00df,", "tokens": ["Den", "Bri\u00b7ten", ",", "das", "wan\u00b7deln\u00b7de", "Fa\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Sie richten und streiten und zanken,", "tokens": ["Sie", "rich\u00b7ten", "und", "strei\u00b7ten", "und", "zan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "KON", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Drauf heben sie lustig das Glas.", "tokens": ["Drauf", "he\u00b7ben", "sie", "lus\u00b7tig", "das", "Glas", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADJD", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.3": {"line.1": {"text": "Wir wissen, ihr seid Philosophen,", "tokens": ["Wir", "wis\u00b7sen", ",", "ihr", "seid", "Phi\u00b7lo\u00b7so\u00b7phen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sucht Wahrheit, als g\u00e4lts Blindekuh,", "tokens": ["Sucht", "Wahr\u00b7heit", ",", "als", "g\u00e4lts", "Blin\u00b7de\u00b7kuh", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "KOUS", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch fragen wir, was ihr getroffen,", "tokens": ["Doch", "fra\u00b7gen", "wir", ",", "was", "ihr", "ge\u00b7trof\u00b7fen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "PWS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nimmt kaum die Bewunderung zu.", "tokens": ["Nimmt", "kaum", "die", "Be\u00b7wun\u00b7de\u00b7rung", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.4": {"line.1": {"text": "Des Jenseits Ma\u00df w\u00e4r die Hierzeit,", "tokens": ["Des", "Jen\u00b7seits", "Ma\u00df", "w\u00e4r", "die", "Hier\u00b7zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Euch selber macht ihr zum Gott,", "tokens": ["Euch", "sel\u00b7ber", "macht", "ihr", "zum", "Gott", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Doch ist er nicht kl\u00fcger, als ihr seid,", "tokens": ["Doch", "ist", "er", "nicht", "kl\u00fc\u00b7ger", ",", "als", "ihr", "seid", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PTKNEG", "ADJD", "$,", "KOUS", "PPER", "VAFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "D\u00fcnkt uns der Allweise nur Spott.", "tokens": ["D\u00fcnkt", "uns", "der", "All\u00b7wei\u00b7se", "nur", "Spott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.5": {"line.1": {"text": "Auch habt ihr die Fremden geschlagen;", "tokens": ["Auch", "habt", "ihr", "die", "Frem\u00b7den", "ge\u00b7schla\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Das taten wohl andre vor euch:", "tokens": ["Das", "ta\u00b7ten", "wohl", "and\u00b7re", "vor", "euch", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PIS", "APPR", "PPER", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Der Franke in st\u00fcrmischen Tagen,", "tokens": ["Der", "Fran\u00b7ke", "in", "st\u00fcr\u00b7mi\u00b7schen", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Der Spanier \u2013 Wen nenn ich nur gleich?", "tokens": ["Der", "Spa\u00b7nier", "\u2013", "Wen", "nenn", "ich", "nur", "gleich", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PWS", "VVFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Es staken da manche dahinter,", "tokens": ["Es", "sta\u00b7ken", "da", "man\u00b7che", "da\u00b7hin\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "PAV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Manch Helfer stand Mann da f\u00fcr Mann.", "tokens": ["Manch", "Hel\u00b7fer", "stand", "Mann", "da", "f\u00fcr", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "NN", "ADV", "APPR", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Der hitzigste war wohl der Winter,", "tokens": ["Der", "hit\u00b7zigs\u00b7te", "war", "wohl", "der", "Win\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Der schlug, als noch voll der Tyrann.", "tokens": ["Der", "schlug", ",", "als", "noch", "voll", "der", "Ty\u00b7rann", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "KOUS", "ADV", "ADJD", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.7": {"line.1": {"text": "Euch schm\u00fcckt ein deutsches Bewu\u00dftsein,", "tokens": ["Euch", "schm\u00fcckt", "ein", "deut\u00b7sches", "Be\u00b7wu\u00df\u00b7tsein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als eins, nicht f\u00e4ltig nur ein-,", "tokens": ["Als", "eins", ",", "nicht", "f\u00e4l\u00b7tig", "nur", "ein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "$,", "PTKNEG", "ADJD", "ADV", "TRUNC", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Wie sollt auch nicht einig die Brust sein,", "tokens": ["Wie", "sollt", "auch", "nicht", "ei\u00b7nig", "die", "Brust", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "ADV", "PTKNEG", "ADJD", "ART", "NN", "VAINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Da eins der Zoll im Verein?", "tokens": ["Da", "eins", "der", "Zoll", "im", "Ver\u00b7ein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.8": {"line.1": {"text": "Nur, streitet ihr noch um den Glauben,", "tokens": ["Nur", ",", "strei\u00b7tet", "ihr", "noch", "um", "den", "Glau\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fehlt zu Treu und Glauben die Treu,", "tokens": ["Fehlt", "zu", "Treu", "und", "Glau\u00b7ben", "die", "Treu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "KON", "NN", "ART", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Auch wi\u00dft ihr, h\u00e4lt mancher nur Tauben,", "tokens": ["Auch", "wi\u00dft", "ihr", ",", "h\u00e4lt", "man\u00b7cher", "nur", "Tau\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "VVFIN", "PIS", "ADV", "NN", "$,"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.4": {"text": "Um andre zu fangen dabei.", "tokens": ["Um", "and\u00b7re", "zu", "fan\u00b7gen", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PIS", "PTKZU", "VVFIN", "PAV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.9": {"line.1": {"text": "Auch seid ihr frei. \u2013 Nicht in Worten,", "tokens": ["Auch", "seid", "ihr", "frei", ".", "\u2013", "Nicht", "in", "Wor\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKVZ", "$.", "$(", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Geschriebne bewacht die Zensur,", "tokens": ["Ge\u00b7schrieb\u00b7ne", "be\u00b7wacht", "die", "Zen\u00b7sur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "In Taten? Noch minder, als dorten,", "tokens": ["In", "Ta\u00b7ten", "?", "Noch", "min\u00b7der", ",", "als", "dor\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "$.", "ADV", "ADV", "$,", "KOUS", "ADV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Wie treff ich die Sache doch nur?", "tokens": ["Wie", "treff", "ich", "die", "Sa\u00b7che", "doch", "nur", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.10": {"line.1": {"text": "Nun denn: ihr seid frei mit dem Maule.", "tokens": ["Nun", "denn", ":", "ihr", "seid", "frei", "mit", "dem", "Mau\u00b7le", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$.", "PPER", "VAFIN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Nun hab ich den rechten Pfiff;", "tokens": ["Nun", "hab", "ich", "den", "rech\u00b7ten", "Pfiff", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wir sitzen auf Hegelschem Gaule,", "tokens": ["Wir", "sit\u00b7zen", "auf", "He\u00b7gel\u00b7schem", "Gau\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Ihr seid denn frei: im Begriff.", "tokens": ["Ihr", "seid", "denn", "frei", ":", "im", "Be\u00b7griff", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKVZ", "$.", "APPRART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.11": {"line.1": {"text": "Und da der Begriff auch das Wahre,", "tokens": ["Und", "da", "der", "Be\u00b7griff", "auch", "das", "Wah\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "ADV", "ART", "ADJA", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Seid frei ihr in Wirklichkeit.", "tokens": ["Seid", "frei", "ihr", "in", "Wirk\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "PPER", "APPR", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Man spart so Taten und Jahre,", "tokens": ["Man", "spart", "so", "Ta\u00b7ten", "und", "Jah\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ist frei au\u00dfer Raum und Zeit.", "tokens": ["Ist", "frei", "au\u00b7\u00dfer", "Raum", "und", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Und so nun mitten im Rechten,", "tokens": ["Und", "so", "nun", "mit\u00b7ten", "im", "Rech\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ziemt alles euch gro\u00df und neu,", "tokens": ["Ziemt", "al\u00b7les", "euch", "gro\u00df", "und", "neu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "La\u00dft Schiller und Goethe den Knechten,", "tokens": ["La\u00dft", "Schil\u00b7ler", "und", "Goe\u00b7the", "den", "Knech\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NE", "KON", "NE", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "F\u00fcr euch sind Dichter, die frei.", "tokens": ["F\u00fcr", "euch", "sind", "Dich\u00b7ter", ",", "die", "frei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "NN", "$,", "PRELS", "ADJD", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.13": {"line.1": {"text": "Sie machen Krieg den Tyrannen", "tokens": ["Sie", "ma\u00b7chen", "Krieg", "den", "Ty\u00b7ran\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und rufen Erhebung euch zu;", "tokens": ["Und", "ru\u00b7fen", "Er\u00b7he\u00b7bung", "euch", "zu", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr leert einstimmig die Kannen,", "tokens": ["Ihr", "leert", "ein\u00b7stim\u00b7mig", "die", "Kan\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und legt um halb eilf euch zur Ruh.", "tokens": ["Und", "legt", "um", "halb", "eilf", "euch", "zur", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJD", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Statt l\u00e4nger mit Griechen zu prahlen", "tokens": ["Statt", "l\u00e4n\u00b7ger", "mit", "Grie\u00b7chen", "zu", "prah\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "APPR", "NN", "PTKZU", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Und anderm veralteten Schnack,", "tokens": ["Und", "an\u00b7derm", "ver\u00b7al\u00b7te\u00b7ten", "Schnack", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADJA", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Von Goethen entstammt und Vandalen,", "tokens": ["Von", "Goe\u00b7then", "ent\u00b7stammt", "und", "Van\u00b7da\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "KON", "NN", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sei euch auch der V\u00e4ter Geschmack.", "tokens": ["Sei", "euch", "auch", "der", "V\u00e4\u00b7ter", "Ge\u00b7schmack", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "NN", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.15": {"line.1": {"text": "Die Nibe- und Amelungen,", "tokens": ["Die", "Ni\u00b7be", "und", "A\u00b7me\u00b7lun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und Gunther, Gudrun, oder was?", "tokens": ["Und", "Gun\u00b7ther", ",", "Gud\u00b7run", ",", "o\u00b7der", "was", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "NE", "$,", "KON", "PWS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ists auch etwas knarrend gesungen,", "tokens": ["Ists", "auch", "et\u00b7was", "knar\u00b7rend", "ge\u00b7sun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Ein Deutscher! und fr\u00e4gt noch um das?", "tokens": ["Ein", "Deut\u00b7scher", "!", "und", "fr\u00e4gt", "noch", "um", "das", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "KON", "VVFIN", "ADV", "APPR", "PDS", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.16": {"line.1": {"text": "So viel f\u00fcr die Form. Um die Sache", "tokens": ["So", "viel", "f\u00fcr", "die", "Form", ".", "Um", "die", "Sa\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$.", "KOUI", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Braucht ihr zu suchen nicht weit,", "tokens": ["Braucht", "ihr", "zu", "su\u00b7chen", "nicht", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKZU", "VVINF", "PTKNEG", "ADJD", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Der Stoff eurer holprichten Mache", "tokens": ["Der", "Stoff", "eu\u00b7rer", "hol\u00b7prich\u00b7ten", "Ma\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Sei eben die Wirklichkeit.", "tokens": ["Sei", "e\u00b7ben", "die", "Wirk\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.17": {"line.1": {"text": "Die Helden, die Ruhm sich erworben,", "tokens": ["Die", "Hel\u00b7den", ",", "die", "Ruhm", "sich", "er\u00b7wor\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "PRF", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Nur gestern in eurer N\u00e4h,", "tokens": ["Nur", "ge\u00b7stern", "in", "eu\u00b7rer", "N\u00e4h", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die f\u00fcr die Freiheit gestorben,", "tokens": ["Die", "f\u00fcr", "die", "Frei\u00b7heit", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Hei\u00dft das in effigie.", "tokens": ["Hei\u00dft", "das", "in", "ef\u00b7fi\u00b7gie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Was sonst noch des Fortschritts B\u00fcrgschaft:", "tokens": ["Was", "sonst", "noch", "des", "Fort\u00b7schritts", "B\u00fcrg\u00b7schaft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "ART", "NN", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Zolleinung und Eisenbahn,", "tokens": ["Zol\u00b7lei\u00b7nung", "und", "Ei\u00b7sen\u00b7bahn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zweikammern-, Dreifelder-Wirtschaft,", "tokens": ["Zwei\u00b7kam\u00b7mern", ",", "Drei\u00b7fel\u00b7der\u00b7Wirt\u00b7schaft", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["TRUNC", "$,", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Beut sich zum Besingen euch an.", "tokens": ["Beut", "sich", "zum", "Be\u00b7sin\u00b7gen", "euch", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPRART", "NN", "PPER", "PTKVZ", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.19": {"line.1": {"text": "Das Dasein in all seiner Bl\u00f6\u00dfe,", "tokens": ["Das", "Da\u00b7sein", "in", "all", "sei\u00b7ner", "Bl\u00f6\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "PPOSAT", "NN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Was sonst als Prosa sich gab;", "tokens": ["Was", "sonst", "als", "Pro\u00b7sa", "sich", "gab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "KOUS", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Klatscht dichtend die eigene Gr\u00f6\u00dfe", "tokens": ["Klatscht", "dich\u00b7tend", "die", "ei\u00b7ge\u00b7ne", "Gr\u00f6\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "ART", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Auf graues L\u00f6schpapier ab.", "tokens": ["Auf", "grau\u00b7es", "L\u00f6schpa\u00b7pier", "ab", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Und so, vermengend die Richtung,", "tokens": ["Und", "so", ",", "ver\u00b7men\u00b7gend", "die", "Rich\u00b7tung", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "VVPP", "ART", "NN", "$,"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.2": {"text": "Sei, alles in eines gepackt,", "tokens": ["Sei", ",", "al\u00b7les", "in", "ei\u00b7nes", "ge\u00b7packt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PIS", "APPR", "ART", "VVPP", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Ein Daguerrotyp eure Dichtung,", "tokens": ["Ein", "Da\u00b7gu\u00b7er\u00b7ro\u00b7typ", "eu\u00b7re", "Dich\u00b7tung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "So \u00e4hnlich, als abgeschmackt.", "tokens": ["So", "\u00e4hn\u00b7lich", ",", "als", "ab\u00b7ge\u00b7schmackt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "VVPP", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}}}}