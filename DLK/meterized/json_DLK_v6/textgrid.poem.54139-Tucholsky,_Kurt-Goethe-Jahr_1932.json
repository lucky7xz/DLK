{"textgrid.poem.54139": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Goethe-Jahr 1932", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "N\u00e4chstes Jahr, da werden wir was erleben!", "tokens": ["N\u00e4chs\u00b7tes", "Jahr", ",", "da", "wer\u00b7den", "wir", "was", "er\u00b7le\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADV", "VAFIN", "PPER", "PIS", "VVINF", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.2": {"text": "So im M\u00e4rz, April und Mai:", "tokens": ["So", "im", "M\u00e4rz", ",", "Ap\u00b7ril", "und", "Mai", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "Goethe hundert Jahre tot! Das wird was geben!", "tokens": ["Goe\u00b7the", "hun\u00b7dert", "Jah\u00b7re", "tot", "!", "Das", "wird", "was", "ge\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "CARD", "NN", "ADJD", "$.", "PDS", "VAFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "W\u00e4r es schon vorbei \u2013!", "tokens": ["W\u00e4r", "es", "schon", "vor\u00b7bei", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Richtig, Joethe!", "tokens": ["Rich\u00b7tig", ",", "Joe\u00b7the", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "NE", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.2": {"text": "Hundert Philologen w\u00e4lzen", "tokens": ["Hun\u00b7dert", "Phi\u00b7lo\u00b7lo\u00b7gen", "w\u00e4l\u00b7zen"], "token_info": ["word", "word", "word"], "pos": ["CARD", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Briefe, Werke, Bilder im Archiv.", "tokens": ["Brie\u00b7fe", ",", "Wer\u00b7ke", ",", "Bil\u00b7der", "im", "Ar\u00b7chiv", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-++-", "measure": "unknown.measure.penta"}, "line.4": {"text": "Und schon seh ich Wolfgang Goetzen stelzen", "tokens": ["Und", "schon", "seh", "ich", "Wolf\u00b7gang", "Goet\u00b7zen", "stel\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "NE", "NE", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "durch die F\u00f6lljet\u00f6ner lang und tief.", "tokens": ["durch", "die", "F\u00f6ll\u00b7je\u00b7t\u00f6\u00b7ner", "lang", "und", "tief", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "KON", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Richtig, Joethe!", "tokens": ["Rich\u00b7tig", ",", "Joe\u00b7the", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "NE", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.2": {"text": "Spitzen der Beh\u00f6rden", "tokens": ["Spit\u00b7zen", "der", "Be\u00b7h\u00f6r\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["NN", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "weihen \u00f6lig quasselnd etwas ein.", "tokens": ["wei\u00b7hen", "\u00f6\u00b7lig", "quas\u00b7selnd", "et\u00b7was", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VVPP", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Und die Spitzen der Beh\u00f6rden w\u00f6rden", "tokens": ["Und", "die", "Spit\u00b7zen", "der", "Be\u00b7h\u00f6r\u00b7den", "w\u00f6r\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ART", "NN", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "alle voll von Faust-Zitaten sein \u2013", "tokens": ["al\u00b7le", "voll", "von", "Faust\u00b7Zi\u00b7ta\u00b7ten", "sein", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "APPR", "NN", "VAINF", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "richtig, Joethe!", "tokens": ["rich\u00b7tig", ",", "Joe\u00b7the", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$,", "NE", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Und es wimmelt von Bez\u00fcglichkeiten:", "tokens": ["Und", "es", "wim\u00b7melt", "von", "Be\u00b7z\u00fcg\u00b7lich\u00b7kei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbgoethe und . . . \u00ab so t\u00f6nt es immerzu.", "tokens": ["\u00bb", "goe\u00b7the", "und", ".", ".", ".", "\u00ab", "so", "t\u00f6nt", "es", "im\u00b7mer\u00b7zu", "."], "token_info": ["punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "KON", "$.", "$.", "$.", "$(", "ADV", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Auf den bunten Marken mu\u00df er schreiten,", "tokens": ["Auf", "den", "bun\u00b7ten", "Mar\u00b7ken", "mu\u00df", "er", "schrei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VMFIN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "und dann sagen alle zu ihm Du!", "tokens": ["und", "dann", "sa\u00b7gen", "al\u00b7le", "zu", "ihm", "Du", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "APPR", "PPER", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "B\u00f6te, Kr\u00f6te, N\u00f6te, R\u00f6te, Fl\u00f6te . . .", "tokens": ["B\u00f6\u00b7te", ",", "Kr\u00f6\u00b7te", ",", "N\u00f6\u00b7te", ",", "R\u00f6\u00b7te", ",", "Fl\u00f6\u00b7te", ".", ".", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,", "NN", "$,", "NN", "$.", "$.", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "wochenlang reimt alles sich auf Goethe.", "tokens": ["wo\u00b7chen\u00b7lang", "reimt", "al\u00b7les", "sich", "auf", "Goe\u00b7the", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PIS", "PRF", "APPR", "NE", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Dann verstummen Prosa und Sonett.", "tokens": ["Dann", "ver\u00b7stum\u00b7men", "Pro\u00b7sa", "und", "So\u00b7nett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "KON", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Von den deutschen Angestellten-Massen", "tokens": ["Von", "den", "deut\u00b7schen", "An\u00b7ge\u00b7stell\u00b7ten\u00b7Mas\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "hat man keinen weniger entlassen.", "tokens": ["hat", "man", "kei\u00b7nen", "we\u00b7ni\u00b7ger", "ent\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PIAT", "PIS", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Klassiker sind nur f\u00fcrs B\u00fccherbrett.", "tokens": ["Klas\u00b7si\u00b7ker", "sind", "nur", "f\u00fcrs", "B\u00fc\u00b7cher\u00b7brett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "APPRART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "N\u00e4chstes Jahr, da kannst du was erleben!", "tokens": ["N\u00e4chs\u00b7tes", "Jahr", ",", "da", "kannst", "du", "was", "er\u00b7le\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADV", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "So im M\u00e4rz, April und Mai . . .", "tokens": ["So", "im", "M\u00e4rz", ",", "Ap\u00b7ril", "und", "Mai", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "NN", "KON", "NN", "$.", "$.", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "Lieben Freunde, das wird etwas geben!", "tokens": ["Lie\u00b7ben", "Freun\u00b7de", ",", "das", "wird", "et\u00b7was", "ge\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PDS", "VAFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "W\u00e4r es schon vorbei \u2013!", "tokens": ["W\u00e4r", "es", "schon", "vor\u00b7bei", "\u2013", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}}}}