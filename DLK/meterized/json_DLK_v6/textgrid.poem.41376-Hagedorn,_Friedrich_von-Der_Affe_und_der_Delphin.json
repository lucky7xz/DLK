{"textgrid.poem.41376": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Der Affe und der Delphin", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Den Mutterwitz bringt jeder auf die Welt;", "tokens": ["Den", "Mut\u00b7ter\u00b7witz", "bringt", "je\u00b7der", "auf", "die", "Welt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Der Schulwitz wird durch B\u00fccher uns gegeben;", "tokens": ["Der", "Schul\u00b7witz", "wird", "durch", "B\u00fc\u00b7cher", "uns", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Der eitle Mensch, dem Schein und Wahn gef\u00e4llt,", "tokens": ["Der", "eit\u00b7le", "Mensch", ",", "dem", "Schein", "und", "Wahn", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sucht \u00fcberdies dem dritten nachzustreben.", "tokens": ["Sucht", "\u00fc\u00b7ber\u00b7dies", "dem", "drit\u00b7ten", "nach\u00b7zu\u00b7stre\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "VVIZU", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Das ist der Witz, den man, galant zu leben,", "tokens": ["Das", "ist", "der", "Witz", ",", "den", "man", ",", "ga\u00b7lant", "zu", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$,", "PRELS", "PIS", "$,", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Auf Reisen sucht, nur in der Fremd' erh\u00e4lt,", "tokens": ["Auf", "Rei\u00b7sen", "sucht", ",", "nur", "in", "der", "Fremd'", "er\u00b7h\u00e4lt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,", "ADV", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wo, ehe man den letztern aufgesp\u00fcret,", "tokens": ["Wo", ",", "e\u00b7he", "man", "den", "letz\u00b7tern", "auf\u00b7ge\u00b7sp\u00fc\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "KOUS", "PIS", "ART", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Manch' Mutterkind die ersten oft verlieret.", "tokens": ["Man\u00b7ch'", "Mut\u00b7ter\u00b7kind", "die", "ers\u00b7ten", "oft", "ver\u00b7lie\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "ADJA", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.2": {"line.1": {"text": "Und dennoch ist's ein Ruhm, (ich leiste die Gew\u00e4hr)", "tokens": ["Und", "den\u00b7noch", "ist's", "ein", "Ruhm", ",", "(", "ich", "leis\u00b7te", "die", "Ge\u00b7w\u00e4hr", ")"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ART", "NN", "$,", "$(", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mit Vorwitz, Gold und Stolz sich auf den Weg zu machen.", "tokens": ["Mit", "Vor\u00b7witz", ",", "Gold", "und", "Stolz", "sich", "auf", "den", "Weg", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "PRF", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man holt von St\u00e4dten, Leuten, Sachen", "tokens": ["Man", "holt", "von", "St\u00e4d\u00b7ten", ",", "Leu\u00b7ten", ",", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["PIS", "VVFIN", "APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zum wenigsten die Namen her.", "tokens": ["Zum", "we\u00b7nigs\u00b7ten", "die", "Na\u00b7men", "her", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "+----+-+", "measure": "dactylic.init"}, "line.5": {"text": "Ist dieses nicht genug? wer darf noch mehr verlangen?", "tokens": ["Ist", "die\u00b7ses", "nicht", "ge\u00b7nug", "?", "wer", "darf", "noch", "mehr", "ver\u00b7lan\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "ADV", "$.", "PWS", "VMFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wer alles wissen will, der gehe selbst dahin,", "tokens": ["Wer", "al\u00b7les", "wis\u00b7sen", "will", ",", "der", "ge\u00b7he", "selbst", "da\u00b7hin", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVINF", "VMFIN", "$,", "PRELS", "VVFIN", "ADV", "PAV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wo ich bereits gewesen bin;", "tokens": ["Wo", "ich", "be\u00b7reits", "ge\u00b7we\u00b7sen", "bin", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VAPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da kann er Unterricht empfangen.", "tokens": ["Da", "kann", "er", "Un\u00b7ter\u00b7richt", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ganz recht! du bist schon hier: dir droht nicht die Gefahr,", "tokens": ["Ganz", "recht", "!", "du", "bist", "schon", "hier", ":", "dir", "droht", "nicht", "die", "Ge\u00b7fahr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "PPER", "VAFIN", "ADV", "ADV", "$.", "PPER", "VVFIN", "PTKNEG", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die jenem Affen t\u00f6dtlich war.", "tokens": ["Die", "je\u00b7nem", "Af\u00b7fen", "t\u00f6dt\u00b7lich", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der ging zu Schiffe, von Athen", "tokens": ["Der", "ging", "zu", "Schif\u00b7fe", ",", "von", "A\u00b7then"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "NN", "$,", "APPR", "NE"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nach Laced\u00e4mon hin zu reisen,", "tokens": ["Nach", "La\u00b7ce\u00b7d\u00e4\u00b7mon", "hin", "zu", "rei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Den Sch\u00f6nen dort, die ihn noch nicht gesehn,", "tokens": ["Den", "Sch\u00f6\u00b7nen", "dort", ",", "die", "ihn", "noch", "nicht", "ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "PRELS", "PPER", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sein liebliches Gesicht zu weisen.", "tokens": ["Sein", "lieb\u00b7li\u00b7ches", "Ge\u00b7sicht", "zu", "wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Die Fahrt fing gl\u00fccklich an, bei hellem Sonnenschein.", "tokens": ["Die", "Fahrt", "fing", "gl\u00fcck\u00b7lich", "an", ",", "bei", "hel\u00b7lem", "Son\u00b7nen\u00b7schein", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Luft flo\u00df, wie das Meer, gelind und spiegelrein.", "tokens": ["Die", "Luft", "flo\u00df", ",", "wie", "das", "Meer", ",", "ge\u00b7lind", "und", "spie\u00b7gel\u00b7rein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PWAV", "ART", "NN", "$,", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Drum singt der Steuermann, den noch kein Unfall st\u00f6ret,", "tokens": ["Drum", "singt", "der", "Steu\u00b7er\u00b7mann", ",", "den", "noch", "kein", "Un\u00b7fall", "st\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "$,", "PRELS", "ADV", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und lenkt das Schiff mit Lust; man jauchzet \u00fcberall.", "tokens": ["Und", "lenkt", "das", "Schiff", "mit", "Lust", ";", "man", "jauch\u00b7zet", "\u00fc\u00b7be\u00b7rall", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "NN", "$.", "PIS", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die allgemeine Ruh', der \u00f6ftre Freudenschall", "tokens": ["Die", "all\u00b7ge\u00b7mei\u00b7ne", "Ruh'", ",", "der", "\u00f6ft\u00b7re", "Freu\u00b7den\u00b7schall"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Reizt meinen Passagier, der bald den Scherz vermehret,", "tokens": ["Reizt", "mei\u00b7nen", "Pas\u00b7sa\u00b7gier", ",", "der", "bald", "den", "Scherz", "ver\u00b7meh\u00b7ret", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "$,", "PRELS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Z\u00e4hne bleckt, erz\u00e4hlt, wo er herumgeschweift,", "tokens": ["Die", "Z\u00e4h\u00b7ne", "bleckt", ",", "er\u00b7z\u00e4hlt", ",", "wo", "er", "her\u00b7um\u00b7ge\u00b7schweift", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+---+-+", "measure": "unknown.measure.penta"}, "line.8": {"text": "Und es beim Zeus beschw\u00f6rt, ein Liedchen h\u00fcpfend pfeift,", "tokens": ["Und", "es", "beim", "Zeus", "be\u00b7schw\u00f6rt", ",", "ein", "Lied\u00b7chen", "h\u00fcp\u00b7fend", "pfeift", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "VVPP", "$,", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Das er beim Chier Wein von Phrynis selbst geh\u00f6ret.", "tokens": ["Das", "er", "beim", "Chier", "Wein", "von", "Phry\u00b7nis", "selbst", "ge\u00b7h\u00f6\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "APPRART", "NN", "NN", "APPR", "NE", "ADV", "VVFIN", "$."], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}}, "stanza.6": {"line.1": {"text": "Der Wind verbleibt geneigt. Man sieht zur rechten Hand,", "tokens": ["Der", "Wind", "ver\u00b7bleibt", "ge\u00b7neigt", ".", "Man", "sieht", "zur", "rech\u00b7ten", "Hand", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVPP", "$.", "PIS", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In einem fernen Blau, Trez\u00f6ns ber\u00fchmten Strand,", "tokens": ["In", "ei\u00b7nem", "fer\u00b7nen", "Blau", ",", "Tre\u00b7z\u00f6ns", "be\u00b7r\u00fchm\u00b7ten", "Strand", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und Argo's breiten Busen liegen.", "tokens": ["Und", "Ar\u00b7go's", "brei\u00b7ten", "Bu\u00b7sen", "lie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der Thetis weibischen und schnellen Unbestand", "tokens": ["Der", "The\u00b7tis", "wei\u00b7bi\u00b7schen", "und", "schnel\u00b7len", "Un\u00b7be\u00b7stand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVINF", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Scheint Eurus webend einzuwiegen.", "tokens": ["Scheint", "Eu\u00b7rus", "we\u00b7bend", "ein\u00b7zu\u00b7wie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Bald aber schw\u00e4rzet sich die heitre Himmelsluft;", "tokens": ["Bald", "a\u00b7ber", "schw\u00e4r\u00b7zet", "sich", "die", "heit\u00b7re", "Him\u00b7mel\u00b7sluft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es rei\u00dft sich Boreas aus seiner tiefsten Kluft", "tokens": ["Es", "rei\u00dft", "sich", "Bo\u00b7re\u00b7as", "aus", "sei\u00b7ner", "tiefs\u00b7ten", "Kluft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "NE", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In Wirbeln brausend los, und th\u00fcrmt auf Wellen Wellen.", "tokens": ["In", "Wir\u00b7beln", "brau\u00b7send", "los", ",", "und", "th\u00fcrmt", "auf", "Wel\u00b7len", "Wel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "PTKVZ", "$,", "KON", "VVFIN", "APPR", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das Schiffvolk sieht erstaunt die wilden Fluten schwellen,", "tokens": ["Das", "Schiff\u00b7volk", "sieht", "er\u00b7staunt", "die", "wil\u00b7den", "Flu\u00b7ten", "schwel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und zieht die Segel ein: doch fehlt ihm Zeit und Licht.", "tokens": ["Und", "zieht", "die", "Se\u00b7gel", "ein", ":", "doch", "fehlt", "ihm", "Zeit", "und", "Licht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$.", "ADV", "VVFIN", "PPER", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Sturm verfolgt das Schiff: es krachet, splittert, bricht.", "tokens": ["Der", "Sturm", "ver\u00b7folgt", "das", "Schiff", ":", "es", "kra\u00b7chet", ",", "split\u00b7tert", ",", "bricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$.", "PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "So wird die Hoffnung bald betrogen!", "tokens": ["So", "wird", "die", "Hoff\u00b7nung", "bald", "be\u00b7tro\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die in erw\u00fcnschter Sicherheit", "tokens": ["Die", "in", "er\u00b7w\u00fcnschter", "Si\u00b7cher\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Der guten Reise sich erfreut,", "tokens": ["Der", "gu\u00b7ten", "Rei\u00b7se", "sich", "er\u00b7freut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sind jetzt ein Spiel emp\u00f6rter Wogen.", "tokens": ["Sind", "jetzt", "ein", "Spiel", "em\u00b7p\u00f6r\u00b7ter", "Wo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Ein jeder ringt mit Furcht und Wellen,", "tokens": ["Ein", "je\u00b7der", "ringt", "mit", "Furcht", "und", "Wel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und jedem sinket Hand und Muth.", "tokens": ["Und", "je\u00b7dem", "sin\u00b7ket", "Hand", "und", "Muth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch pl\u00f6tzlich legt sich Wind und Flut;", "tokens": ["Doch", "pl\u00f6tz\u00b7lich", "legt", "sich", "Wind", "und", "Flut", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PRF", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Luft f\u00e4ngt an sich aufzuhellen.", "tokens": ["Die", "Luft", "f\u00e4ngt", "an", "sich", "auf\u00b7zu\u00b7hel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PRF", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Als nun die Stille zugenommen,", "tokens": ["Als", "nun", "die", "Stil\u00b7le", "zu\u00b7ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da k\u00f6mmt, vielleicht von ungef\u00e4hr,", "tokens": ["Da", "k\u00f6mmt", ",", "viel\u00b7leicht", "von", "un\u00b7ge\u00b7f\u00e4hr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "ADV", "APPR", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein spielendes Delphinenheer,", "tokens": ["Ein", "spie\u00b7len\u00b7des", "Del\u00b7phi\u00b7nen\u00b7heer", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu aller Trost, herbeigeschwommen.", "tokens": ["Zu", "al\u00b7ler", "Trost", ",", "her\u00b7bei\u00b7ge\u00b7schwom\u00b7men", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Dies Thier pflegt Menschen gern zu dienen.", "tokens": ["Dies", "Thier", "pflegt", "Men\u00b7schen", "gern", "zu", "die\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VVFIN", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Selbst Plinius erz\u00e4hlt es so.", "tokens": ["Selbst", "Pli\u00b7nius", "er\u00b7z\u00e4hlt", "es", "so", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "An welchem Ort? ich wei\u00df nicht wo;", "tokens": ["An", "wel\u00b7chem", "Ort", "?", "ich", "wei\u00df", "nicht", "wo", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "$.", "PPER", "VVFIN", "PTKNEG", "PWAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In dem Capitel von Delphinen.", "tokens": ["In", "dem", "Ca\u00b7pi\u00b7tel", "von", "Del\u00b7phi\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "+-++-+-+-", "measure": "unknown.measure.penta"}}, "stanza.12": {"line.1": {"text": "Der Affe naht sich mit Entz\u00fccken.", "tokens": ["Der", "Af\u00b7fe", "naht", "sich", "mit", "Ent\u00b7z\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da nimmt ein solcher Menschenfreund,", "tokens": ["Da", "nimmt", "ein", "sol\u00b7cher", "Men\u00b7schen\u00b7freund", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dem er ein Mensch, wie andre, scheint,", "tokens": ["Dem", "er", "ein", "Mensch", ",", "wie", "and\u00b7re", ",", "scheint", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "$,", "PWAV", "PIS", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ihn unverz\u00fcglich auf den R\u00fccken.", "tokens": ["Ihn", "un\u00b7ver\u00b7z\u00fcg\u00b7lich", "auf", "den", "R\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Er freuet sich der stolzen B\u00fcrde.", "tokens": ["Er", "freu\u00b7et", "sich", "der", "stol\u00b7zen", "B\u00fcr\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sein Reiter ziert sich auch so sch\u00f6n,", "tokens": ["Sein", "Rei\u00b7ter", "ziert", "sich", "auch", "so", "sch\u00f6n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PRF", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df, wer ihn nicht zu scharf besehn,", "tokens": ["Da\u00df", ",", "wer", "ihn", "nicht", "zu", "scharf", "be\u00b7sehn", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PWS", "PPER", "PTKNEG", "PTKA", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ihn f\u00fcr Arion halten w\u00fcrde.", "tokens": ["Ihn", "f\u00fcr", "A\u00b7rion", "hal\u00b7ten", "w\u00fcr\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "NN", "VVINF", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Der junge Herr wird fortgetragen,", "tokens": ["Der", "jun\u00b7ge", "Herr", "wird", "fort\u00b7ge\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bis endlich sein Erretter ruht,", "tokens": ["Bis", "end\u00b7lich", "sein", "Er\u00b7ret\u00b7ter", "ruht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und h\u00f6flich diese Frage thut,", "tokens": ["Und", "h\u00f6f\u00b7lich", "die\u00b7se", "Fra\u00b7ge", "thut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PDAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie ihn der Sturm hieher verschlagen.", "tokens": ["Wie", "ihn", "der", "Sturm", "hie\u00b7her", "ver\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Sie sind ja von Athen gekommen? ...", "tokens": ["Sie", "sind", "ja", "von", "A\u00b7then", "ge\u00b7kom\u00b7men", "?", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NE", "VVPP", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Ja freilich komm' ich von Athen.", "tokens": ["Ja", "frei\u00b7lich", "komm'", "ich", "von", "A\u00b7then", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Hat Er noch nichts von mir vernommen?", "tokens": ["Hat", "Er", "noch", "nichts", "von", "mir", "ver\u00b7nom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIS", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Hat Ihnen diese Stadt gefallen?", "tokens": ["Hat", "Ih\u00b7nen", "die\u00b7se", "Stadt", "ge\u00b7fal\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PDAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er fragt? wem steht Athen nicht an?", "tokens": ["Er", "fragt", "?", "wem", "steht", "A\u00b7then", "nicht", "an", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PWS", "VVFIN", "NE", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Mein Vetter, der ber\u00fchmte Mann,", "tokens": ["Mein", "Vet\u00b7ter", ",", "der", "be\u00b7r\u00fchm\u00b7te", "Mann", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist Archon dort, und gilt bei allen.", "tokens": ["Ist", "Ar\u00b7chon", "dort", ",", "und", "gilt", "bei", "al\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "$,", "KON", "VVFIN", "APPR", "PIAT", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Um meine Rettung fr\u00f6hlich sein!", "tokens": ["Um", "mei\u00b7ne", "Ret\u00b7tung", "fr\u00f6h\u00b7lich", "sein", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie wird sich mein ", "tokens": ["Wie", "wird", "sich", "mein"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "PRF", "PPOSAT"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.18": {"line.1": {"text": "So ist auch (doch kaum braucht's der Frage)", "tokens": ["So", "ist", "auch", "(", "doch", "kaum", "braucht's", "der", "Fra\u00b7ge", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "$(", "ADV", "ADV", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Pir\u00e4us Ihnen wohl bekannt? ...", "tokens": ["Pi\u00b7r\u00e4us", "Ih\u00b7nen", "wohl", "be\u00b7kannt", "?", "..."], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "PPER", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "O der? Pir\u00e4us hat Verstand;", "tokens": ["O", "der", "?", "Pi\u00b7r\u00e4us", "hat", "Ver\u00b7stand", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ART", "$.", "NE", "VAFIN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wir sahen uns fast alle Tage.", "tokens": ["Wir", "sa\u00b7hen", "uns", "fast", "al\u00b7le", "Ta\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Das hie\u00df nun recht die Klugheit zeigen!", "tokens": ["Das", "hie\u00df", "nun", "recht", "die", "Klug\u00b7heit", "zei\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADJD", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Meister hat das Schlo\u00df erdacht,", "tokens": ["Kein", "Meis\u00b7ter", "hat", "das", "Schlo\u00df", "er\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das rohe M\u00e4uler sprachlos macht.", "tokens": ["Das", "ro\u00b7he", "M\u00e4u\u00b7ler", "sprach\u00b7los", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "O w\u00fc\u00dften Affen doch zu schweigen!", "tokens": ["O", "w\u00fc\u00df\u00b7ten", "Af\u00b7fen", "doch", "zu", "schwei\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Er wird erkannt, und mu\u00df ertrinken.", "tokens": ["Er", "wird", "er\u00b7kannt", ",", "und", "mu\u00df", "er\u00b7trin\u00b7ken", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$,", "KON", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man wirft ihn in das Meer, und spricht:", "tokens": ["Man", "wirft", "ihn", "in", "das", "Meer", ",", "und", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "$,", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Delphinen retten Affen nicht;", "tokens": ["Del\u00b7phi\u00b7nen", "ret\u00b7ten", "Af\u00b7fen", "nicht", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fort; du magst schwimmen, oder sinken!", "tokens": ["Fort", ";", "du", "magst", "schwim\u00b7men", ",", "o\u00b7der", "sin\u00b7ken", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$.", "PPER", "VMFIN", "VVINF", "$,", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}