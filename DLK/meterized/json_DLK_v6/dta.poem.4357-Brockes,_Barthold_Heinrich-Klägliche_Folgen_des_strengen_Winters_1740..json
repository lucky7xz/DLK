{"dta.poem.4357": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Kl\u00e4gliche Folgen des strengen  \n Winters 1740.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Wie ist doch der betr\u00fcbte Winter von einer gar zu langen", "tokens": ["Wie", "ist", "doch", "der", "be\u00b7tr\u00fcb\u00b7te", "Win\u00b7ter", "von", "ei\u00b7ner", "gar", "zu", "lan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "ADV", "ART", "ADJA", "NN", "APPR", "ART", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "Dauer,", "tokens": ["Dau\u00b7er", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Kein Laub, kein Bl\u00e4ttgen zeiget sich, kein Kraut, kein Gr\u00e4s-", "tokens": ["Kein", "Laub", ",", "kein", "Bl\u00e4tt\u00b7gen", "zei\u00b7get", "sich", ",", "kein", "Kraut", ",", "kein", "Gr\u00e4s"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "VVFIN", "PRF", "$,", "PIAT", "NN", "$,", "PIAT", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "gen bricht herf\u00fcr!", "tokens": ["gen", "bricht", "her\u00b7f\u00fcr", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Wo ist das sch\u00f6ne gr\u00fcne Feld? Wo der bebl\u00fchmten G\u00e4rten", "tokens": ["Wo", "ist", "das", "sch\u00f6\u00b7ne", "gr\u00fc\u00b7ne", "Feld", "?", "Wo", "der", "be\u00b7bl\u00fchm\u00b7ten", "G\u00e4r\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "ART", "ADJA", "ADJA", "NN", "$.", "PWAV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.6": {"text": "Zier?", "tokens": ["Zier", "?"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Bleibt unsre Mutter heuer denn in unver\u00e4nderlicher", "tokens": ["Bleibt", "uns\u00b7re", "Mut\u00b7ter", "heu\u00b7er", "denn", "in", "un\u00b7ver\u00b7\u00e4n\u00b7der\u00b7li\u00b7cher"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "ADV", "ADV", "APPR", "ADJA"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.8": {"text": "Trauer?", "tokens": ["Trau\u00b7er", "?"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.9": {"text": "Ver\u00e4ndert sich denn die Natur? F\u00fchrt uns der Winter", "tokens": ["Ver\u00b7\u00e4n\u00b7dert", "sich", "denn", "die", "Na\u00b7tur", "?", "F\u00fchrt", "uns", "der", "Win\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "ART", "NN", "$.", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "nicht zum Lenzen?", "tokens": ["nicht", "zum", "Len\u00b7zen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "APPRART", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "K\u00f6mmt keine W\u00e4rme nach dem Frost? Der sonst bebl\u00fchmte", "tokens": ["K\u00f6mmt", "kei\u00b7ne", "W\u00e4r\u00b7me", "nach", "dem", "Frost", "?", "Der", "sonst", "be\u00b7bl\u00fchm\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PIAT", "NN", "APPR", "ART", "NN", "$.", "ART", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Monat May", "tokens": ["Mo\u00b7nat", "May"], "token_info": ["word", "word"], "pos": ["NN", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.13": {"text": "Hat keine Bluhmen, kennt kein Gr\u00fcn, und ist dennoch schon", "tokens": ["Hat", "kei\u00b7ne", "Bluh\u00b7men", ",", "kennt", "kein", "Gr\u00fcn", ",", "und", "ist", "den\u00b7noch", "schon"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "$,", "VVFIN", "PIAT", "NN", "$,", "KON", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.14": {"text": "halb vorbey.", "tokens": ["halb", "vor\u00b7bey", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.15": {"text": "Des alten Grases welker Wust deckt, wie ein schmutzigs", "tokens": ["Des", "al\u00b7ten", "Gra\u00b7ses", "wel\u00b7ker", "Wust", "deckt", ",", "wie", "ein", "schmut\u00b7zigs"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PWAT", "NN", "VVFIN", "$,", "PWAV", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "blasses Heu,", "tokens": ["blas\u00b7ses", "Heu", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.17": {"text": "Die sonst so lieblichen Gefilde, wo Bluhmen, Klee und", "tokens": ["Die", "sonst", "so", "lieb\u00b7li\u00b7chen", "Ge\u00b7fil\u00b7de", ",", "wo", "Bluh\u00b7men", ",", "Klee", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADV", "ADV", "ADJA", "NN", "$,", "PWAV", "NN", "$,", "NN", "KON"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "Kr\u00e4uter gl\u00e4nzen.", "tokens": ["Kr\u00e4u\u00b7ter", "gl\u00e4n\u00b7zen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Ist gleich das Wasser aufgethaut, so scheint die Luft an-", "tokens": ["Ist", "gleich", "das", "Was\u00b7ser", "auf\u00b7ge\u00b7thaut", ",", "so", "scheint", "die", "Luft", "an"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "NN", "VVPP", "$,", "ADV", "VVFIN", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "noch gefroren,", "tokens": ["noch", "ge\u00b7fro\u00b7ren", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Sie f\u00fcllt ein Heer von kalten Theilen, das uns mit stren-", "tokens": ["Sie", "f\u00fcllt", "ein", "Heer", "von", "kal\u00b7ten", "Thei\u00b7len", ",", "das", "uns", "mit", "stren"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$,", "PRELS", "PPER", "APPR", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "ger Sch\u00e4rfe pre\u00dft,", "tokens": ["ger", "Sch\u00e4r\u00b7fe", "pre\u00dft", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Und fast kein Kraut aus harter Erde, kein Blatt aus seiner", "tokens": ["Und", "fast", "kein", "Kraut", "aus", "har\u00b7ter", "Er\u00b7de", ",", "kein", "Blatt", "aus", "sei\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PIAT", "NN", "APPR", "ADJA", "NN", "$,", "PIAT", "NN", "APPR", "PPOSAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Knospe l\u00e4\u00dft.", "tokens": ["Knos\u00b7pe", "l\u00e4\u00dft", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Ein lauer Regen spr\u00fcet nirgend, fr\u00fch wird f\u00fcr uns", "tokens": ["Ein", "lau\u00b7er", "Re\u00b7gen", "spr\u00fcet", "nir\u00b7gend", ",", "fr\u00fch", "wird", "f\u00fcr", "uns"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "$,", "ADJD", "VAFIN", "APPR", "PPER"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "kein Thau gebohren,", "tokens": ["kein", "Thau", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Das Vieh bl\u00f6ckt in dem leeren Stall, es bl\u00f6cket auf den", "tokens": ["Das", "Vieh", "bl\u00f6ckt", "in", "dem", "lee\u00b7ren", "Stall", ",", "es", "bl\u00f6\u00b7cket", "auf", "den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,", "PPER", "VVFIN", "APPR", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "\u00f6den Weiden,", "tokens": ["\u00f6\u00b7den", "Wei\u00b7den", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Dort ist kein Futter, hier kein Gras, die ganz enthaarte", "tokens": ["Dort", "ist", "kein", "Fut\u00b7ter", ",", "hier", "kein", "Gras", ",", "die", "ganz", "ent\u00b7haar\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "$,", "ADV", "PIAT", "NN", "$,", "PRELS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "nackte Haut,", "tokens": ["nack\u00b7te", "Haut", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "So kaum die d\u00fcrren Knochen deckt, wird ohne Gram nicht", "tokens": ["So", "kaum", "die", "d\u00fcr\u00b7ren", "Kno\u00b7chen", "deckt", ",", "wird", "oh\u00b7ne", "Gram", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,", "VAFIN", "APPR", "NN", "PTKNEG"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "angeschaut.", "tokens": ["an\u00b7ge\u00b7schaut", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Jhr Br\u00fcllen bricht des Landmanns Herz, er leidet selbst bey", "tokens": ["Ihr", "Br\u00fcl\u00b7len", "bricht", "des", "Land\u00b7manns", "Herz", ",", "er", "lei\u00b7det", "selbst", "bey"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "NN", "$,", "PPER", "VVFIN", "ADV", "APPR"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.8": {"text": "ihrem Leiden;", "tokens": ["ih\u00b7rem", "Lei\u00b7den", ";"], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "Sieht etwan einst ein Spierchen Gras, das niedrig steht am", "tokens": ["Sieht", "et\u00b7wan", "einst", "ein", "Spier\u00b7chen", "Gras", ",", "das", "nied\u00b7rig", "steht", "am"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ADV", "ART", "NN", "NN", "$,", "PRELS", "ADJD", "VVFIN", "APPRART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wasser-Graben,", "tokens": ["Was\u00b7ser\u00b7Gra\u00b7ben", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "So st\u00fcrzet es oft in das Wasser. Man rettet dieses arme", "tokens": ["So", "st\u00fcr\u00b7zet", "es", "oft", "in", "das", "Was\u00b7ser", ".", "Man", "ret\u00b7tet", "die\u00b7ses", "ar\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$.", "PIS", "VVFIN", "PDAT", "ADJA"], "meter": "-+--+--+--+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.12": {"text": "Vieh", "tokens": ["Vieh"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.13": {"text": "Zuweilen nicht, und es ertrinkt; zuweilen, doch mit vieler", "tokens": ["Zu\u00b7wei\u00b7len", "nicht", ",", "und", "es", "er\u00b7trinkt", ";", "zu\u00b7wei\u00b7len", ",", "doch", "mit", "vie\u00b7ler"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "PTKNEG", "$,", "KON", "PPER", "VVFIN", "$.", "ADV", "$,", "ADV", "APPR", "PIAT"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.14": {"text": "M\u00fch,", "tokens": ["M\u00fch", ","], "token_info": ["word", "punct"], "pos": ["NE", "$,"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Indem sie selber sich zu helfen, f\u00fcr Hunger, keine Kr\u00e4fte", "tokens": ["In\u00b7dem", "sie", "sel\u00b7ber", "sich", "zu", "hel\u00b7fen", ",", "f\u00fcr", "Hun\u00b7ger", ",", "kei\u00b7ne", "Kr\u00e4f\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "PRF", "PTKZU", "VVINF", "$,", "APPR", "NN", "$,", "PIAT", "NN"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.16": {"text": "haben.", "tokens": ["ha\u00b7ben", "."], "token_info": ["word", "punct"], "pos": ["VAFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.17": {"text": "So geht es Ochsen, K\u00fchen, K\u00e4lbern, den Schaafen gleich-", "tokens": ["So", "geht", "es", "Och\u00b7sen", ",", "K\u00fc\u00b7hen", ",", "K\u00e4l\u00b7bern", ",", "den", "Schaa\u00b7fen", "gleich"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$,", "NN", "$,", "NN", "$,", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "falls, nebst den Pferden,", "tokens": ["falls", ",", "nebst", "den", "Pfer\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "APPR", "ART", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.19": {"text": "Sie schleppen kaum ihr eigne Last. Nun soll annoch gepfl\u00fc-", "tokens": ["Sie", "schlep\u00b7pen", "kaum", "ihr", "eig\u00b7ne", "Last", ".", "Nun", "soll", "an\u00b7noch", "ge\u00b7pfl\u00fc"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$.", "ADV", "VMFIN", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.20": {"text": "get werden.", "tokens": ["get", "wer\u00b7den", "."], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VAINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.21": {"text": "Ja, wo das Ungl\u00fcck sich nicht \u00e4ndert, wie soll im Herbste", "tokens": ["Ja", ",", "wo", "das", "Un\u00b7gl\u00fcck", "sich", "nicht", "\u00e4n\u00b7dert", ",", "wie", "soll", "im", "Herbs\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "PWAV", "ART", "NN", "PRF", "PTKNEG", "VVFIN", "$,", "PWAV", "VMFIN", "APPRART", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "doch das Feld,", "tokens": ["doch", "das", "Feld", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.23": {"text": "Wenn sie noch weiter sterben sollten, zur n\u00f6ht\u2019gen Winter-", "tokens": ["Wenn", "sie", "noch", "wei\u00b7ter", "ster\u00b7ben", "soll\u00b7ten", ",", "zur", "n\u00f6ht'\u00b7gen", "Win\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVINF", "VMFIN", "$,", "APPRART", "ADJA", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.24": {"text": "Saat bestellt,", "tokens": ["Saat", "be\u00b7stellt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.25": {"text": "Gepfl\u00fcget und geeget werden? Woher soll Milch und", "tokens": ["Ge\u00b7pfl\u00fc\u00b7get", "und", "gee\u00b7get", "wer\u00b7den", "?", "Wo\u00b7her", "soll", "Milch", "und"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "KON", "VVPP", "VAINF", "$.", "PWAV", "VMFIN", "NN", "KON"], "meter": "-+--+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.26": {"text": "Butter kommen?", "tokens": ["But\u00b7ter", "kom\u00b7men", "?"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.27": {"text": "Woher, zu unsrer Nahrung, Fleisch? Ach, wird denn nun", "tokens": ["Wo\u00b7her", ",", "zu", "uns\u00b7rer", "Nah\u00b7rung", ",", "Fleisch", "?", "Ach", ",", "wird", "denn", "nun"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "$,", "APPR", "PPOSAT", "NN", "$,", "NN", "$.", "ITJ", "$,", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "nicht wahrgenommen,", "tokens": ["nicht", "wahr\u00b7ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.29": {"text": "Was f\u00fcr ein Schatz im Grase steckt, worauf wir niemahls", "tokens": ["Was", "f\u00fcr", "ein", "Schatz", "im", "Gra\u00b7se", "steckt", ",", "wo\u00b7rauf", "wir", "nie\u00b7mahls"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "APPR", "ART", "NN", "APPRART", "NN", "VVFIN", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "fast gedacht,", "tokens": ["fast", "ge\u00b7dacht", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.31": {"text": "Weil es, so lang man denkt und wei\u00df, das Land von selbst", "tokens": ["Weil", "es", ",", "so", "lang", "man", "denkt", "und", "wei\u00df", ",", "das", "Land", "von", "selbst"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "ADV", "ADJD", "PIS", "VVFIN", "KON", "VVFIN", "$,", "ART", "NN", "APPR", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "hervorgebracht.", "tokens": ["her\u00b7vor\u00b7ge\u00b7bracht", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Ach, welch ein g\u00f6ttliches Geschenke hat in der Erden", "tokens": ["Ach", ",", "welch", "ein", "g\u00f6tt\u00b7li\u00b7ches", "Ge\u00b7schen\u00b7ke", "hat", "in", "der", "Er\u00b7den"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "PWAT", "ART", "ADJA", "NN", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Schmuck gesteckt,", "tokens": ["Schmuck", "ge\u00b7steckt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Das man, wie alles Gute, leider! nunmehr, blo\u00df im Verlust,", "tokens": ["Das", "man", ",", "wie", "al\u00b7les", "Gu\u00b7te", ",", "lei\u00b7der", "!", "nun\u00b7mehr", ",", "blo\u00df", "im", "Ver\u00b7lust", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "$,", "PWAV", "PIS", "NN", "$,", "ADV", "$.", "ADV", "$,", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.4": {"text": "entdeckt!", "tokens": ["ent\u00b7deckt", "!"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "O GOtt! aus Dessen Lieb\u2019 und Macht allein das Laub", "tokens": ["O", "Gott", "!", "aus", "Des\u00b7sen", "Lieb'", "und", "Macht", "al\u00b7lein", "das", "Laub"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "$.", "APPR", "PDAT", "NN", "KON", "NN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "und Gras entstehet,", "tokens": ["und", "Gras", "ent\u00b7ste\u00b7het", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Wir haben durch des Undanks Laster, durch unsre Uner-", "tokens": ["Wir", "ha\u00b7ben", "durch", "des", "Un\u00b7danks", "Las\u00b7ter", ",", "durch", "uns\u00b7re", "Un\u00b7er"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "NN", "$,", "APPR", "PPOSAT", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "kenntlichkeit,", "tokens": ["kennt\u00b7lich\u00b7keit", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "Die Strafe mehr als wohl verdient, da\u00df es uns geht, wie es", "tokens": ["Die", "Stra\u00b7fe", "mehr", "als", "wohl", "ver\u00b7dient", ",", "da\u00df", "es", "uns", "geht", ",", "wie", "es"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "PIAT", "KOKOM", "ADV", "VVPP", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,", "PWAV", "PPER"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.10": {"text": "uns gehet.", "tokens": ["uns", "ge\u00b7het", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.11": {"text": "Wir f\u00fchlen itzt von unsrer S\u00fcnden die str\u00e4fliche Beschaf-", "tokens": ["Wir", "f\u00fch\u00b7len", "itzt", "von", "uns\u00b7rer", "S\u00fcn\u00b7den", "die", "str\u00e4f\u00b7li\u00b7che", "Be\u00b7schaf"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "ART", "ADJA", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.12": {"text": "fenheit.", "tokens": ["fen\u00b7heit", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "--", "measure": "unknown.measure.zero"}, "line.13": {"text": "Wir haben nicht des Grases Wehrt erwogen, nicht, wie es", "tokens": ["Wir", "ha\u00b7ben", "nicht", "des", "Gra\u00b7ses", "Wehrt", "er\u00b7wo\u00b7gen", ",", "nicht", ",", "wie", "es"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ART", "ADJA", "NN", "VVPP", "$,", "PTKNEG", "$,", "PWAV", "PPER"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "so sch\u00f6n,", "tokens": ["so", "sch\u00f6n", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "ADJD", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.15": {"text": "Bey seinem Nutz, zu unsrer Lust, anbey geschm\u00fcckt, nicht", "tokens": ["Bey", "sei\u00b7nem", "Nutz", ",", "zu", "uns\u00b7rer", "Lust", ",", "an\u00b7bey", "ge\u00b7schm\u00fcckt", ",", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["APPR", "PPOSAT", "NN", "$,", "APPR", "PPOSAT", "NN", "$,", "ADV", "VVPP", "$,", "PTKNEG"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "angesehn.", "tokens": ["an\u00b7ge\u00b7sehn", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.17": {"text": "Steckt gleich der Thier\u2019 und Menschen Nahrung fast in", "tokens": ["Steckt", "gleich", "der", "Thier'", "und", "Men\u00b7schen", "Nah\u00b7rung", "fast", "in"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "NN", "KON", "NN", "NN", "ADV", "APPR"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.18": {"text": "dem Grase nur allein;", "tokens": ["dem", "Gra\u00b7se", "nur", "al\u00b7lein", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.19": {"text": "Wir haben es fast nie bedacht, wie konnten wir ger\u00fchret", "tokens": ["Wir", "ha\u00b7ben", "es", "fast", "nie", "be\u00b7dacht", ",", "wie", "konn\u00b7ten", "wir", "ge\u00b7r\u00fch\u00b7ret"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$,", "PWAV", "VMFIN", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.20": {"text": "seyn?", "tokens": ["seyn", "?"], "token_info": ["word", "punct"], "pos": ["VAINF", "$."], "meter": "+", "measure": "single.up"}, "line.21": {"text": "Wie konnten wir dem Sch\u00f6pfer danken, da wir doch offen-", "tokens": ["Wie", "konn\u00b7ten", "wir", "dem", "Sch\u00f6p\u00b7fer", "dan\u00b7ken", ",", "da", "wir", "doch", "of\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,", "KOUS", "PPER", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "bar itzt sehn,", "tokens": ["bar", "itzt", "sehn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "VVINF", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.23": {"text": "Da\u00df, wenn allein das Gras uns fehlte, die lebendige Welt", "tokens": ["Da\u00df", ",", "wenn", "al\u00b7lein", "das", "Gras", "uns", "fehl\u00b7te", ",", "die", "le\u00b7ben\u00b7di\u00b7ge", "Welt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "$,", "KOUS", "ADV", "ART", "NN", "PPER", "VVFIN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.24": {"text": "vergehn,", "tokens": ["ver\u00b7gehn", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.25": {"text": "So Thier\u2019, als Menschen sterben m\u00fcsten? Nun aber, da", "tokens": ["So", "Thier'", ",", "als", "Men\u00b7schen", "ster\u00b7ben", "m\u00fcs\u00b7ten", "?", "Nun", "a\u00b7ber", ",", "da"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ADV", "NN", "$,", "KOUS", "NN", "VVINF", "VMFIN", "$.", "ADV", "ADV", "$,", "KOUS"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "wir leider finden,", "tokens": ["wir", "lei\u00b7der", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.27": {"text": "Was aller Welt daran gelegen, und da es uns nunmehro", "tokens": ["Was", "al\u00b7ler", "Welt", "da\u00b7ran", "ge\u00b7le\u00b7gen", ",", "und", "da", "es", "uns", "nun\u00b7meh\u00b7ro"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "PIAT", "NN", "PAV", "VVPP", "$,", "KON", "KOUS", "PPER", "PPER", "ADV"], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}, "line.28": {"text": "fehlt,", "tokens": ["fehlt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.29": {"text": "Mit Thr\u00e4nen in den Augen f\u00fchlen, wie heftig der Verlust", "tokens": ["Mit", "Thr\u00e4\u00b7nen", "in", "den", "Au\u00b7gen", "f\u00fch\u00b7len", ",", "wie", "hef\u00b7tig", "der", "Ver\u00b7lust"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "VVINF", "$,", "PWAV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.30": {"text": "uns qu\u00e4lt,", "tokens": ["uns", "qu\u00e4lt", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.31": {"text": "Und wir nunmehr die bittern Folgen von diesem Mangel", "tokens": ["Und", "wir", "nun\u00b7mehr", "die", "bit\u00b7tern", "Fol\u00b7gen", "von", "die\u00b7sem", "Man\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "ADV", "ART", "ADJA", "NN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.32": {"text": "kaum ergr\u00fcnden;", "tokens": ["kaum", "er\u00b7gr\u00fcn\u00b7den", ";"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "So f\u00e4llt mir Moses Beyspiel ein, da er sich f\u00fcr sein Volk", "tokens": ["So", "f\u00e4llt", "mir", "Mo\u00b7ses", "Bey\u00b7spiel", "ein", ",", "da", "er", "sich", "f\u00fcr", "sein", "Volk"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADJA", "NN", "PTKVZ", "$,", "KOUS", "PPER", "PRF", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "erk\u00fchnte,", "tokens": ["er\u00b7k\u00fchn\u00b7te", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Und gegen Dich, HErr Zebaoth! sich Deines Namens selbst", "tokens": ["Und", "ge\u00b7gen", "Dich", ",", "Herr", "Ze\u00b7bao\u00b7th", "!", "sich", "Dei\u00b7nes", "Na\u00b7mens", "selbst"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PPER", "$,", "NN", "NE", "$.", "PRF", "PPOSAT", "NN", "ADV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.4": {"text": "bediente.", "tokens": ["be\u00b7dien\u00b7te", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "Soll, sprach er, denn der Heiden Schaar von Deines Na-", "tokens": ["Soll", ",", "sprach", "er", ",", "denn", "der", "Hei\u00b7den", "Schaar", "von", "Dei\u00b7nes", "Na"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "$,", "VVFIN", "PPER", "$,", "KON", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "mens Ehre sagen:", "tokens": ["mens", "Eh\u00b7re", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Du habest Jsrael erl\u00f6s\u2019st, sie in der W\u00fcsten zu erschlagen?", "tokens": ["Du", "ha\u00b7best", "Js\u00b7rael", "er\u00b7l\u00f6s'st", ",", "sie", "in", "der", "W\u00fcs\u00b7ten", "zu", "er\u00b7schla\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "VVFIN", "$,", "PPER", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "Auf gleiche Weise bin ich k\u00fchn, und stelle, grosser Sch\u00f6pfer!", "tokens": ["Auf", "glei\u00b7che", "Wei\u00b7se", "bin", "ich", "k\u00fchn", ",", "und", "stel\u00b7le", ",", "gros\u00b7ser", "Sch\u00f6p\u00b7fer", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "PPER", "ADJD", "$,", "KON", "VVFIN", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.9": {"text": "Dir", "tokens": ["Dir"], "token_info": ["word"], "pos": ["PPER"], "meter": "+", "measure": "single.up"}, "line.10": {"text": "Die Ehre Deines grossen Namens, in tiefster Unterwerfung,", "tokens": ["Die", "Eh\u00b7re", "Dei\u00b7nes", "gros\u00b7sen", "Na\u00b7mens", ",", "in", "tiefs\u00b7ter", "Un\u00b7ter\u00b7wer\u00b7fung", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.11": {"text": "f\u00fcr.", "tokens": ["f\u00fcr", "."], "token_info": ["word", "punct"], "pos": ["APPR", "$."], "meter": "-", "measure": "single.down"}, "line.12": {"text": "Soll, denk ich, denn ein Atheist (da Du ja aller Welt ver-", "tokens": ["Soll", ",", "denk", "ich", ",", "denn", "ein", "A\u00b7theist", "(", "da", "Du", "ja", "al\u00b7ler", "Welt", "ver"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "$,", "VVFIN", "PPER", "$,", "KON", "ART", "NN", "$(", "KOUS", "PPER", "ADV", "PIAT", "NN", "TRUNC"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.13": {"text": "sprochen:", "tokens": ["spro\u00b7chen", ":"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.14": {"text": "So lang\u2019 als Erd\u2019 und Himmel steht, h\u00f6rt in dem fest", "tokens": ["So", "lang'", "als", "Erd'", "und", "Him\u00b7mel", "steht", ",", "h\u00f6rt", "in", "dem", "fest"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "KOUS", "NN", "KON", "NN", "VVFIN", "$,", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "gestellten Lauf", "tokens": ["ge\u00b7stell\u00b7ten", "Lauf"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.16": {"text": "Der Fr\u00fchling, Sommer, Herbst und Winter, in ihrer", "tokens": ["Der", "Fr\u00fch\u00b7ling", ",", "Som\u00b7mer", ",", "Herbst", "und", "Win\u00b7ter", ",", "in", "ih\u00b7rer"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,", "APPR", "PPOSAT"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.17": {"text": "Ordnung, nimmer auf.)", "tokens": ["Ord\u00b7nung", ",", "nim\u00b7mer", "auf", ".", ")"], "token_info": ["word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "$,", "ADV", "PTKVZ", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.18": {"text": "Wohl ungeahndet sagen k\u00f6nnen: Es w\u00e4r Dein grosses", "tokens": ["Wohl", "un\u00b7ge\u00b7ahn\u00b7det", "sa\u00b7gen", "k\u00f6n\u00b7nen", ":", "Es", "w\u00e4r", "Dein", "gros\u00b7ses"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVINF", "VMINF", "$.", "PPER", "VAFIN", "PPOSAT", "ADJA"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.19": {"text": "Wort gebrochen?", "tokens": ["Wort", "ge\u00b7bro\u00b7chen", "?"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.20": {"text": "Ich flehe Dich nicht um ein Volk, in aller V\u00f6lker Namen an,", "tokens": ["Ich", "fle\u00b7he", "Dich", "nicht", "um", "ein", "Volk", ",", "in", "al\u00b7ler", "V\u00f6l\u00b7ker", "Na\u00b7men", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "APPR", "ART", "NN", "$,", "APPR", "PIAT", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.21": {"text": "Wovon kein einzigs ohne Gras, wie wir gesehn, bestehen", "tokens": ["Wo\u00b7von", "kein", "ein\u00b7zigs", "oh\u00b7ne", "Gras", ",", "wie", "wir", "ge\u00b7sehn", ",", "be\u00b7ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PWAV", "PIAT", "PIS", "APPR", "NN", "$,", "PWAV", "PPER", "VVPP", "$,", "VVFIN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.22": {"text": "kann.", "tokens": ["kann", "."], "token_info": ["word", "punct"], "pos": ["VMFIN", "$."], "meter": "+", "measure": "single.up"}, "line.23": {"text": "Ach la\u00df, HErr, Deiner Gnaden Schein, in lauer W\u00e4rme,", "tokens": ["Ach", "la\u00df", ",", "Herr", ",", "Dei\u00b7ner", "Gna\u00b7den", "Schein", ",", "in", "lau\u00b7er", "W\u00e4r\u00b7me", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "VVFIN", "$,", "NN", "$,", "PPOSAT", "NN", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "wiederkehren!", "tokens": ["wie\u00b7der\u00b7keh\u00b7ren", "!"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.25": {"text": "Schenk uns aufs neue Laub und Gras, damit sich Thier\u2019 und", "tokens": ["Schenk", "uns", "aufs", "neu\u00b7e", "Laub", "und", "Gras", ",", "da\u00b7mit", "sich", "Thier'", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "PPER", "APPRART", "ADJA", "NN", "KON", "NN", "$,", "KOUS", "PRF", "NN", "KON"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Menschen n\u00e4hren,", "tokens": ["Men\u00b7schen", "n\u00e4h\u00b7ren", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.27": {"text": "Und la\u00df, im frohen Danken, uns daf\u00fcr mehr, als zuvor,", "tokens": ["Und", "la\u00df", ",", "im", "fro\u00b7hen", "Dan\u00b7ken", ",", "uns", "da\u00b7f\u00fcr", "mehr", ",", "als", "zu\u00b7vor", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVIMP", "$,", "APPRART", "ADJA", "NN", "$,", "PPER", "PAV", "ADV", "$,", "KOUS", "ADV", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.28": {"text": "Dich ehren!Siehe die sehr sp\u00e4te Heu-Erndte.", "tokens": ["Dich", "eh\u00b7ren", "!", "Sie\u00b7he", "die", "sehr", "sp\u00e4\u00b7te", "Heu\u00b7Ernd\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$.", "VVIMP", "ART", "ADV", "ADJA", "NN", "$."], "meter": "-+-+---+--+-", "measure": "iambic.tetra.relaxed"}}}}}