{"textgrid.poem.53544": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Eisner", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Da war ein Mann, der noch an Ideale glaubte", "tokens": ["Da", "war", "ein", "Mann", ",", "der", "noch", "an", "I\u00b7dea\u00b7le", "glaub\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "NN", "VVFIN"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "und tatkr\u00e4ftig war.", "tokens": ["und", "tat\u00b7kr\u00e4f\u00b7tig", "war", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "In Deutschland ist das t\u00f6dlich. Denn wir haben", "tokens": ["In", "Deutschland", "ist", "das", "t\u00f6d\u00b7lich", ".", "Denn", "wir", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NE", "VAFIN", "ART", "ADJD", "$.", "KON", "PPER", "VAFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "entweder rohe Kraft, die wir mi\u00dfbrauchen,", "tokens": ["ent\u00b7we\u00b7der", "ro\u00b7he", "Kraft", ",", "die", "wir", "mi\u00df\u00b7brau\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$,", "PRELS", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "die Gattung nennt man Patrioten \u2013 oder aber", "tokens": ["die", "Gat\u00b7tung", "nennt", "man", "Pat\u00b7ri\u00b7o\u00b7ten", "\u2013", "o\u00b7der", "a\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PIS", "NN", "$(", "KON", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "wir haben feine Sinne und ein zart Gewissen", "tokens": ["wir", "ha\u00b7ben", "fei\u00b7ne", "Sin\u00b7ne", "und", "ein", "zart", "Ge\u00b7wis\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJA", "NN", "KON", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "und richten gar nichts aus. Der aber, tatenfroh befl\u00fcgelt,", "tokens": ["und", "rich\u00b7ten", "gar", "nichts", "aus", ".", "Der", "a\u00b7ber", ",", "ta\u00b7ten\u00b7froh", "be\u00b7fl\u00fc\u00b7gelt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIS", "PTKVZ", "$.", "ART", "ADV", "$,", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.8": {"text": "hieb fest dazwischen \u2013 und daneben, freilich!", "tokens": ["hieb", "fest", "da\u00b7zwi\u00b7schen", "\u2013", "und", "da\u00b7ne\u00b7ben", ",", "frei\u00b7lich", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADJD", "PAV", "$(", "KON", "PAV", "$,", "ADV", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Jedoch er hieb, da\u00df faule Sp\u00e4ne flogen.", "tokens": ["Je\u00b7doch", "er", "hieb", ",", "da\u00df", "fau\u00b7le", "Sp\u00e4\u00b7ne", "flo\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "$,", "KOUS", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Welch eine Wohltat war das, zu erleben,", "tokens": ["Welch", "ei\u00b7ne", "Wohl\u00b7tat", "war", "das", ",", "zu", "er\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "ART", "NN", "VAFIN", "PDS", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "da\u00df einer \u00fcberhaupt den Degen zog,", "tokens": ["da\u00df", "ei\u00b7ner", "\u00fc\u00b7ber\u00b7haupt", "den", "De\u00b7gen", "zog", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "ein Tapferer war und doch kein General.", "tokens": ["ein", "Tap\u00b7fe\u00b7rer", "war", "und", "doch", "kein", "Ge\u00b7ne\u00b7ral", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "KON", "ADV", "PIAT", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Ein L\u00fcmmel, irgendeiner von den Schwarz-Wei\u00df-Roten", "tokens": ["Ein", "L\u00fcm\u00b7mel", ",", "ir\u00b7gend\u00b7ei\u00b7ner", "von", "den", "Schwa\u00b7rz\u00b7Wei\u00df\u00b7Ro\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "(der letzte Zulukaffer steht uns andern n\u00e4her),", "tokens": ["(", "der", "letz\u00b7te", "Zu\u00b7lu\u00b7kaf\u00b7fer", "steht", "uns", "an\u00b7dern", "n\u00e4\u00b7her", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "ADJA", "NN", "VVFIN", "PPER", "PIS", "ADJD", "$(", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "scho\u00df ihn von hinten \u00fcbern Haufen.", "tokens": ["scho\u00df", "ihn", "von", "hin\u00b7ten", "\u00fc\u00b7bern", "Hau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADV", "APPRART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Kurt Eisner starb \u2013 und lebt in unser aller Herzen!", "tokens": ["Kurt", "Eis\u00b7ner", "starb", "\u2013", "und", "lebt", "in", "un\u00b7ser", "al\u00b7ler", "Her\u00b7zen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "$(", "KON", "VVFIN", "APPR", "PPOSAT", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Was aber Trauer bitter macht und schmerzlicher den Schmerz,", "tokens": ["Was", "a\u00b7ber", "Trau\u00b7er", "bit\u00b7ter", "macht", "und", "schmerz\u00b7li\u00b7cher", "den", "Schmerz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "NN", "ADJD", "VVFIN", "KON", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "was \u00fcber einer Gruft die F\u00e4uste ballen l\u00e4\u00dft,", "tokens": ["was", "\u00fc\u00b7ber", "ei\u00b7ner", "Gruft", "die", "F\u00e4us\u00b7te", "bal\u00b7len", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "ist dies:", "tokens": ["ist", "dies", ":"], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "PDS", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Die B\u00fcrger nicken.", "tokens": ["Die", "B\u00fcr\u00b7ger", "ni\u00b7cken", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Es starb Jaur\u00e8s, Karl Liebknecht, Luxemburg,", "tokens": ["Es", "starb", "Jaur\u00e8s", ",", "Karl", "Lieb\u00b7knecht", ",", "Lu\u00b7xem\u00b7burg", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "$,", "NE", "NN", "$,", "NE", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "Kurt Eisner \u2013.", "tokens": ["Kurt", "Eis\u00b7ner", "\u2013", "."], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NN", "$(", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Wir wissen wohl, wie jener gro\u00df war, dieser kleiner \u2013", "tokens": ["Wir", "wis\u00b7sen", "wohl", ",", "wie", "je\u00b7ner", "gro\u00df", "war", ",", "die\u00b7ser", "klei\u00b7ner", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PWAV", "PDAT", "ADJD", "VAFIN", "$,", "PDAT", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "wer feilscht hier um Formate! Eine Reinheit", "tokens": ["wer", "feilscht", "hier", "um", "For\u00b7ma\u00b7te", "!", "Ei\u00b7ne", "Rein\u00b7heit"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VVFIN", "ADV", "APPR", "NE", "$.", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "ging von den vieren aus,", "tokens": ["ging", "von", "den", "vie\u00b7ren", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "die leuchtete auf ihren Stirnen und den H\u00e4nden.", "tokens": ["die", "leuch\u00b7te\u00b7te", "auf", "ih\u00b7ren", "Stir\u00b7nen", "und", "den", "H\u00e4n\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "APPR", "PPOSAT", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und ihre Stimme sprach: Ihr sollt nicht leiden!", "tokens": ["Und", "ih\u00b7re", "Stim\u00b7me", "sprach", ":", "Ihr", "sollt", "nicht", "lei\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "$.", "PPER", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Vier Sch\u00fcsse und vier S\u00e4rge und vier Gr\u00e4ber.", "tokens": ["Vier", "Sch\u00fcs\u00b7se", "und", "vier", "S\u00e4r\u00b7ge", "und", "vier", "Gr\u00e4\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "CARD", "NN", "KON", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Wir strecken unsre Arme in die Runde", "tokens": ["Wir", "stre\u00b7cken", "uns\u00b7re", "Ar\u00b7me", "in", "die", "Run\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "und klagen: \u00bbWelt! schl\u00e4gst du noch immer an die Kreuze", "tokens": ["und", "kla\u00b7gen", ":", "\u00bb", "Welt", "!", "schl\u00e4gst", "du", "noch", "im\u00b7mer", "an", "die", "Kreu\u00b7ze"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVINF", "$.", "$(", "NN", "$.", "VVFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "die, die dich lieben?\u00ab", "tokens": ["die", ",", "die", "dich", "lie\u00b7ben", "?", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["ART", "$,", "PRELS", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.16": {"text": "Und die B\u00fcrger nicken.", "tokens": ["Und", "die", "B\u00fcr\u00b7ger", "ni\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.17": {"text": "Behaglich nicken sie, zufrieden, da\u00df sie leben,", "tokens": ["Be\u00b7hag\u00b7lich", "ni\u00b7cken", "sie", ",", "zu\u00b7frie\u00b7den", ",", "da\u00df", "sie", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "$,", "ADJD", "$,", "KOUS", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "und froh, die St\u00f6renfriede los zu sein,", "tokens": ["und", "froh", ",", "die", "St\u00f6\u00b7ren\u00b7frie\u00b7de", "los", "zu", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "ART", "NN", "PTKVZ", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "die St\u00f6renfriede ihrer Kontokasse.", "tokens": ["die", "St\u00f6\u00b7ren\u00b7frie\u00b7de", "ih\u00b7rer", "Kon\u00b7to\u00b7kas\u00b7se", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.20": {"text": "Wo braust Emp\u00f6rung auf? Wo lodern Flammen,", "tokens": ["Wo", "braust", "Em\u00b7p\u00f6\u00b7rung", "auf", "?", "Wo", "lo\u00b7dern", "Flam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "NN", "PTKVZ", "$.", "PWAV", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.21": {"text": "die Unrat zehren, und sie heilsam brennen?", "tokens": ["die", "Un\u00b7rat", "zeh\u00b7ren", ",", "und", "sie", "heil\u00b7sam", "bren\u00b7nen", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.22": {"text": "Die B\u00fcrger nicken. Schlecht verhohlne Freude.", "tokens": ["Die", "B\u00fcr\u00b7ger", "ni\u00b7cken", ".", "Schlecht", "ver\u00b7hohl\u00b7ne", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.23": {"text": "Sie wollen Ordnung \u2013 das hei\u00dft: Unterordnung.", "tokens": ["Sie", "wol\u00b7len", "Ord\u00b7nung", "\u2013", "das", "hei\u00dft", ":", "Un\u00b7ter\u00b7ord\u00b7nung", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "$(", "PDS", "VVFIN", "$.", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.24": {"text": "Sie wollen Ruhe \u2013 das hei\u00dft: Kirchhofsstille.", "tokens": ["Sie", "wol\u00b7len", "Ru\u00b7he", "\u2013", "das", "hei\u00dft", ":", "Kirch\u00b7hofs\u00b7stil\u00b7le", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "$(", "PDS", "VVFIN", "$.", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Sie wollen Brot \u2013 das karge Brot der andern.", "tokens": ["Sie", "wol\u00b7len", "Brot", "\u2013", "das", "kar\u00b7ge", "Brot", "der", "an\u00b7dern", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "$(", "ART", "ADJA", "NN", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und satt und schleimig \u2013 fett und vollgesogen", "tokens": ["Und", "satt", "und", "schlei\u00b7mig", "\u2013", "fett", "und", "voll\u00b7ge\u00b7so\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "KON", "ADJD", "$(", "ADJD", "KON", "ADJA"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "hockt \u00fcber diesem Lande eine Spinne:", "tokens": ["hockt", "\u00fc\u00b7ber", "die\u00b7sem", "Lan\u00b7de", "ei\u00b7ne", "Spin\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PDAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "gel\u00e4hmtes Leid, gel\u00e4hmte deutsche Seelen.", "tokens": ["ge\u00b7l\u00e4hm\u00b7tes", "Leid", ",", "ge\u00b7l\u00e4hm\u00b7te", "deut\u00b7sche", "See\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Und doch: nach allem, was bergab gegangen,", "tokens": ["Und", "doch", ":", "nach", "al\u00b7lem", ",", "was", "berg\u00b7ab", "ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$.", "APPR", "PIS", "$,", "PWS", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "nach dem, was uns entt\u00e4uscht und auch betrogen,", "tokens": ["nach", "dem", ",", "was", "uns", "ent\u00b7t\u00e4uscht", "und", "auch", "be\u00b7tro\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PPER", "VVPP", "KON", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "nach Kompromi\u00df und braven Leisetretern \u2013 \u2013", "tokens": ["nach", "Kom\u00b7pro\u00b7mi\u00df", "und", "bra\u00b7ven", "Lei\u00b7se\u00b7tre\u00b7tern", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "wir wissen ihre Werke, da\u00df sie weder kalt noch warm", "tokens": ["wir", "wis\u00b7sen", "ih\u00b7re", "Wer\u00b7ke", ",", "da\u00df", "sie", "we\u00b7der", "kalt", "noch", "warm"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$,", "KOUS", "PPER", "KON", "ADJD", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.5": {"text": "gewesen sind. Ach, w\u00e4rt ihr kalt! Ach, w\u00e4rt ihr warm!", "tokens": ["ge\u00b7we\u00b7sen", "sind", ".", "Ach", ",", "w\u00e4rt", "ihr", "kalt", "!", "Ach", ",", "w\u00e4rt", "ihr", "warm", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAPP", "VAFIN", "$.", "ITJ", "$,", "VAFIN", "PPER", "ADJD", "$.", "ITJ", "$,", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Doch sie sind lau \u2013", "tokens": ["Doch", "sie", "sind", "lau", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADJD", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Und dennoch, dennoch:", "tokens": ["Und", "den\u00b7noch", ",", "den\u00b7noch", ":"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "$,", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "Wir glauben weiter unter grauem Himmel!", "tokens": ["Wir", "glau\u00b7ben", "wei\u00b7ter", "un\u00b7ter", "grau\u00b7em", "Him\u00b7mel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Wir warten deiner unter grauem Himmel!", "tokens": ["Wir", "war\u00b7ten", "dei\u00b7ner", "un\u00b7ter", "grau\u00b7em", "Him\u00b7mel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Wir wissen, da\u00df du kommst \u2013", "tokens": ["Wir", "wis\u00b7sen", ",", "da\u00df", "du", "kommst", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Du sollst nicht r\u00e4chen.", "tokens": ["Du", "sollst", "nicht", "r\u00e4\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.12": {"text": "Doch du sollst flammen, sch\u00fcren, leuchten, brennen.", "tokens": ["Doch", "du", "sollst", "flam\u00b7men", ",", "sch\u00fc\u00b7ren", ",", "leuch\u00b7ten", ",", "bren\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Luft! Gib uns Luft, darin wir atmen k\u00f6nnen!", "tokens": ["Luft", "!", "Gib", "uns", "Luft", ",", "da\u00b7rin", "wir", "at\u00b7men", "k\u00f6n\u00b7nen", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVIMP", "PPER", "NN", "$,", "PAV", "PPER", "VVINF", "VMINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.14": {"text": "W\u00fchl unsre Seelen auf, pfl\u00fcg um die Herzen", "tokens": ["W\u00fchl", "uns\u00b7re", "See\u00b7len", "auf", ",", "pfl\u00fcg", "um", "die", "Her\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$,", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "und l\u00f6se uns von unserm deutschen Elend", "tokens": ["und", "l\u00f6\u00b7se", "uns", "von", "un\u00b7serm", "deut\u00b7schen", "E\u00b7lend"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "und nimm von uns das niederste der Leiden.", "tokens": ["und", "nimm", "von", "uns", "das", "nie\u00b7ders\u00b7te", "der", "Lei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "APPR", "PPER", "ART", "ADJA", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.17": {"text": "Die beiden mach gesund vor allen Dingen:", "tokens": ["Die", "bei\u00b7den", "mach", "ge\u00b7sund", "vor", "al\u00b7len", "Din\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "VVFIN", "ADJD", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.18": {"text": "gel\u00e4hmtes Land und die gel\u00e4hmten Schwingen!", "tokens": ["ge\u00b7l\u00e4hm\u00b7tes", "Land", "und", "die", "ge\u00b7l\u00e4hm\u00b7ten", "Schwin\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "KON", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}