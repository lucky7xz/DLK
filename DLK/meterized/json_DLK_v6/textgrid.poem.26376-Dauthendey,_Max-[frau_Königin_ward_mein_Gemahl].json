{"textgrid.poem.26376": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[frau K\u00f6nigin ward mein Gemahl]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Frau K\u00f6nigin ward mein Gemahl", "tokens": ["Frau", "K\u00f6\u00b7ni\u00b7gin", "ward", "mein", "Ge\u00b7mahl"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NN", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auf einer Insel im Kanal.", "tokens": ["Auf", "ei\u00b7ner", "In\u00b7sel", "im", "Ka\u00b7nal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-----+", "measure": "dactylic.init"}}, "stanza.2": {"line.1": {"text": "In einem Kirchlein, klein und bieder,", "tokens": ["In", "ei\u00b7nem", "Kirch\u00b7lein", ",", "klein", "und", "bie\u00b7der", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Knieten wir am Altare nieder,", "tokens": ["Knie\u00b7ten", "wir", "am", "Al\u00b7ta\u00b7re", "nie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Und niemand hat gelacht, geweint,", "tokens": ["Und", "nie\u00b7mand", "hat", "ge\u00b7lacht", ",", "ge\u00b7weint", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als uns der Priester still geeint.", "tokens": ["Als", "uns", "der", "Pries\u00b7ter", "still", "ge\u00b7eint", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Doch als wir aus der Kirch' hinaus,", "tokens": ["Doch", "als", "wir", "aus", "der", "Kirch'", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "ART", "NN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sahn beide wir erstaunter aus.", "tokens": ["Sahn", "bei\u00b7de", "wir", "er\u00b7staun\u00b7ter", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Den Ehring ungewohnt ich fand,", "tokens": ["Den", "Eh\u00b7ring", "un\u00b7ge\u00b7wohnt", "ich", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und er ging leicht mir von der Hand.", "tokens": ["Und", "er", "ging", "leicht", "mir", "von", "der", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADJD", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Denn stets, wenn ich nach Hause ging,", "tokens": ["Denn", "stets", ",", "wenn", "ich", "nach", "Hau\u00b7se", "ging", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Legte ich ab Hut, Stock und Ring.", "tokens": ["Leg\u00b7te", "ich", "ab", "Hut", ",", "Stock", "und", "Ring", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "+--+++-+", "measure": "dactylic.init"}}, "stanza.7": {"line.1": {"text": "Gar l\u00e4stig scheint der Au\u00dfenzwang,", "tokens": ["Gar", "l\u00e4s\u00b7tig", "scheint", "der", "Au\u00b7\u00dfen\u00b7zwang", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat man so vielen Innendrang.", "tokens": ["Hat", "man", "so", "vie\u00b7len", "In\u00b7nen\u00b7drang", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Als Gast bei unserm Hochzeitsschmaus", "tokens": ["Als", "Gast", "bei", "un\u00b7serm", "Hoch\u00b7zeits\u00b7schmaus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "War nur ein wei\u00dfer Rosenstrau\u00df.", "tokens": ["War", "nur", "ein", "wei\u00b7\u00dfer", "Ro\u00b7sen\u00b7strau\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wir sa\u00dfen leis wie im Versteck", "tokens": ["Wir", "sa\u00b7\u00dfen", "leis", "wie", "im", "Ver\u00b7steck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KOKOM", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit unserm Gl\u00fcck in einer Eck.", "tokens": ["Mit", "un\u00b7serm", "Gl\u00fcck", "in", "ei\u00b7ner", "Eck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Sch\u00f6n kann erst recht die Hochzeit sein,", "tokens": ["Sch\u00f6n", "kann", "erst", "recht", "die", "Hoch\u00b7zeit", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "ADV", "ADV", "ART", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind Braut und Br\u00e4utigam allein.", "tokens": ["Sind", "Braut", "und", "Br\u00e4u\u00b7ti\u00b7gam", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NE", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Doch was die Lieb' erst wirklich macht,", "tokens": ["Doch", "was", "die", "Lieb'", "erst", "wirk\u00b7lich", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das ist das Fest der Hochzeitsnacht.", "tokens": ["Das", "ist", "das", "Fest", "der", "Hoch\u00b7zeits\u00b7nacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Man ahnte sich ja vorderhand", "tokens": ["Man", "ahn\u00b7te", "sich", "ja", "vor\u00b7der\u00b7hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PRF", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur immer durch die Kleiderwand,", "tokens": ["Nur", "im\u00b7mer", "durch", "die", "Klei\u00b7der\u00b7wand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und man wird dann sich erst zu eigen,", "tokens": ["Und", "man", "wird", "dann", "sich", "erst", "zu", "ei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "ADV", "PRF", "ADV", "PTKA", "ADJD", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Darf man dem Kleiderschrank entsteigen.", "tokens": ["Darf", "man", "dem", "Klei\u00b7der\u00b7schrank", "ent\u00b7stei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Wir stammen sicher nicht vom Affen,", "tokens": ["Wir", "stam\u00b7men", "si\u00b7cher", "nicht", "vom", "Af\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKNEG", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu herrlich ist der Mensch geschaffen.", "tokens": ["Zu", "herr\u00b7lich", "ist", "der", "Mensch", "ge\u00b7schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Das g\u00f6ttlich zarte Ebenma\u00df", "tokens": ["Das", "g\u00f6tt\u00b7lich", "zar\u00b7te", "E\u00b7ben\u00b7ma\u00df"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Affe ganz bei sich verga\u00df.", "tokens": ["Der", "Af\u00b7fe", "ganz", "bei", "sich", "ver\u00b7ga\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Wir Menschen d\u00fcrfen sagen laut,", "tokens": ["Wir", "Men\u00b7schen", "d\u00fcr\u00b7fen", "sa\u00b7gen", "laut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VMFIN", "VVINF", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir haben edel uns gebaut.", "tokens": ["Wir", "ha\u00b7ben", "e\u00b7del", "uns", "ge\u00b7baut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Doch was beim ersten Ku\u00df gesagt,", "tokens": ["Doch", "was", "beim", "ers\u00b7ten", "Ku\u00df", "ge\u00b7sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPRART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sei auch zur Hochzeitsnacht geklagt.", "tokens": ["Sei", "auch", "zur", "Hoch\u00b7zeits\u00b7nacht", "ge\u00b7klagt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Sie ist nicht so, wie man sie denkt,", "tokens": ["Sie", "ist", "nicht", "so", ",", "wie", "man", "sie", "denkt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADV", "$,", "PWAV", "PIS", "PPER", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Viel sch\u00f6nere die Zukunft schenkt.", "tokens": ["Viel", "sch\u00f6\u00b7ne\u00b7re", "die", "Zu\u00b7kunft", "schenkt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Denn ist man keusch, f\u00fchlt man ein Trennen,", "tokens": ["Denn", "ist", "man", "keusch", ",", "f\u00fchlt", "man", "ein", "Tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ADJD", "$,", "VVFIN", "PIS", "ART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Man tut sich kleiderlos nicht kennen,", "tokens": ["Man", "tut", "sich", "klei\u00b7der\u00b7los", "nicht", "ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Der Leib f\u00fchlt sich noch unverwandt,", "tokens": ["Der", "Leib", "f\u00fchlt", "sich", "noch", "un\u00b7ver\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur das Gesicht bleibt uns bekannt.", "tokens": ["Nur", "das", "Ge\u00b7sicht", "bleibt", "uns", "be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Doch selig s\u00fc\u00df wird das Erschrecken,", "tokens": ["Doch", "se\u00b7lig", "s\u00fc\u00df", "wird", "das", "Er\u00b7schre\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJD", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tut man allm\u00e4hlich sich entdecken.", "tokens": ["Tut", "man", "all\u00b7m\u00e4h\u00b7lich", "sich", "ent\u00b7de\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "ADJD", "PRF", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.22": {"line.1": {"text": "Der K\u00f6rper in so fremder Weise", "tokens": ["Der", "K\u00f6r\u00b7per", "in", "so", "frem\u00b7der", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "D\u00fcnkt ohne Kleider uns so leise,", "tokens": ["D\u00fcnkt", "oh\u00b7ne", "Klei\u00b7der", "uns", "so", "lei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Fast unsichtbar wirkt man als nackt,", "tokens": ["Fast", "un\u00b7sicht\u00b7bar", "wirkt", "man", "als", "nackt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PIS", "KOKOM", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Bis uns das Blut am Herzen packt.", "tokens": ["Bis", "uns", "das", "Blut", "am", "Her\u00b7zen", "packt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Das Blut, der alte G\u00f6tterwein,", "tokens": ["Das", "Blut", ",", "der", "al\u00b7te", "G\u00f6t\u00b7ter\u00b7wein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit K\u00fcssen schenkt man ihn sich ein,", "tokens": ["Mit", "K\u00fcs\u00b7sen", "schenkt", "man", "ihn", "sich", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "PPER", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Der ganze Mensch verbrennt davon", "tokens": ["Der", "gan\u00b7ze", "Mensch", "ver\u00b7brennt", "da\u00b7von"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und steigt zur vierten Dimension.", "tokens": ["Und", "steigt", "zur", "vier\u00b7ten", "Di\u00b7men\u00b7si\u00b7on", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.26": {"line.1": {"text": "Der Tod, sagt man, beschlie\u00dft das Leben,", "tokens": ["Der", "Tod", ",", "sagt", "man", ",", "be\u00b7schlie\u00dft", "das", "Le\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "VVFIN", "PIS", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und dann soll's noch was Be\u00dfres geben.", "tokens": ["Und", "dann", "soll's", "noch", "was", "Be\u00df\u00b7res", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "ADV", "PWS", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Doch wenn sich lebt ein Weib, ein Mann,", "tokens": ["Doch", "wenn", "sich", "lebt", "ein", "Weib", ",", "ein", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "KOUS", "PRF", "VVFIN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man sich nichts Be\u00dfres w\u00fcnschen kann.", "tokens": ["Man", "sich", "nichts", "Be\u00df\u00b7res", "w\u00fcn\u00b7schen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PRF", "PIS", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Das Bett, das ist das Himmelreich,", "tokens": ["Das", "Bett", ",", "das", "ist", "das", "Him\u00b7mel\u00b7reich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PDS", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dort sind wir Gott und Mensch zugleich,", "tokens": ["Dort", "sind", "wir", "Gott", "und", "Mensch", "zu\u00b7gleich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "KON", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Dort liegt des Weltalls Schwergewicht,", "tokens": ["Dort", "liegt", "des", "Welt\u00b7alls", "Schwer\u00b7ge\u00b7wicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mehr Gl\u00fcck als Liebe gibt es nicht.", "tokens": ["Mehr", "Gl\u00fcck", "als", "Lie\u00b7be", "gibt", "es", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KOUS", "NN", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Von meiner Nacht ist noch bekannt:", "tokens": ["Von", "mei\u00b7ner", "Nacht", "ist", "noch", "be\u00b7kannt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Viel Volk ist laut umhergerannt,", "tokens": ["Viel", "Volk", "ist", "laut", "um\u00b7her\u00b7ge\u00b7rannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Die Fenster klirrten von den Wagen,", "tokens": ["Die", "Fens\u00b7ter", "klirr\u00b7ten", "von", "den", "Wa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich h\u00f6rte schreien, h\u00f6rte fragen,", "tokens": ["Ich", "h\u00f6r\u00b7te", "schrei\u00b7en", ",", "h\u00f6r\u00b7te", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVFIN", "$,", "VVFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Am Fenster zuckte rot ein Tanz,", "tokens": ["Am", "Fens\u00b7ter", "zuck\u00b7te", "rot", "ein", "Tanz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Himmel flog ein Feuerkranz.", "tokens": ["Zum", "Him\u00b7mel", "flog", "ein", "Feu\u00b7er\u00b7kranz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Gleich Hochzeitsfackeln in der Stadt", "tokens": ["Gleich", "Hoch\u00b7zeits\u00b7fa\u00b7ckeln", "in", "der", "Stadt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Feuer hell gew\u00fctet hat.", "tokens": ["Ein", "Feu\u00b7er", "hell", "ge\u00b7w\u00fc\u00b7tet", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Deshalb der L\u00e4rm in allen Gassen,", "tokens": ["Des\u00b7halb", "der", "L\u00e4rm", "in", "al\u00b7len", "Gas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Feuer schien heut nacht zu prassen.", "tokens": ["Das", "Feu\u00b7er", "schien", "heut", "nacht", "zu", "pras\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "Ich hielt es hei\u00df in meinem Arm,", "tokens": ["Ich", "hielt", "es", "hei\u00df", "in", "mei\u00b7nem", "Arm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und eine Stadt ward davon warm.", "tokens": ["Und", "ei\u00b7ne", "Stadt", "ward", "da\u00b7von", "warm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PAV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}