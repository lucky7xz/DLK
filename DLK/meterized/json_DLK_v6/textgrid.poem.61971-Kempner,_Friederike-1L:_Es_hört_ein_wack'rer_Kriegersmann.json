{"textgrid.poem.61971": {"metadata": {"author": {"name": "Kempner, Friederike", "birth": "N.A.", "death": "N.A."}, "title": "1L: Es h\u00f6rt ein wack'rer Kriegersmann", "genre": "verse", "period": "N.A.", "pub_year": 1868, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es h\u00f6rt ein wack'rer Kriegersmann", "tokens": ["Es", "h\u00f6rt", "ein", "wack'\u00b7rer", "Krie\u00b7gers\u00b7mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich dies Geschichtchen einmal an,", "tokens": ["Sich", "dies", "Ge\u00b7schicht\u00b7chen", "ein\u00b7mal", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PDS", "NN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dem Tod konnt' er ins Antlitz sehen,", "tokens": ["Dem", "Tod", "konnt'", "er", "ins", "Ant\u00b7litz", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch jetzt im Aug' ihm Tr\u00e4nen steh'n.", "tokens": ["Doch", "jetzt", "im", "Aug'", "ihm", "Tr\u00e4\u00b7nen", "steh'", "n."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["KON", "ADV", "APPRART", "NN", "PPER", "NN", "VVFIN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein Leichenhaus, ein Leichenhaus,", "tokens": ["Ein", "Lei\u00b7chen\u00b7haus", ",", "ein", "Lei\u00b7chen\u00b7haus", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ruft er aus vollem Halse aus,", "tokens": ["Ruft", "er", "aus", "vol\u00b7lem", "Hal\u00b7se", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir wollen nicht auf blo\u00dfen Schein", "tokens": ["Wir", "wol\u00b7len", "nicht", "auf", "blo\u00b7\u00dfen", "Schein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PTKNEG", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beseitigt und begraben sein!", "tokens": ["Be\u00b7sei\u00b7tigt", "und", "be\u00b7gra\u00b7ben", "sein", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wir wollen, alle Wetter auch,", "tokens": ["Wir", "wol\u00b7len", ",", "al\u00b7le", "Wet\u00b7ter", "auch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$,", "PIAT", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht halten an dem dummen Brauch,", "tokens": ["Nicht", "hal\u00b7ten", "an", "dem", "dum\u00b7men", "Brauch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df man mit uns zu Grabe rennt,", "tokens": ["Da\u00df", "man", "mit", "uns", "zu", "Gra\u00b7be", "rennt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als wenn man's nicht erwarten k\u00f6nnt'!", "tokens": ["Als", "wenn", "man's", "nicht", "er\u00b7war\u00b7ten", "k\u00f6nnt'", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PIS", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "F\u00fcrs Denkmal haben Gelder wir,", "tokens": ["F\u00fcrs", "Denk\u00b7mal", "ha\u00b7ben", "Gel\u00b7der", "wir", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "NN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und um Lebend'ge handelt's hier!", "tokens": ["Und", "um", "Le\u00b7ben\u00b7d'\u00b7ge", "han\u00b7delt's", "hier", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VAFIN", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Man s\u00fchnt wohl solche Grausamkeit", "tokens": ["Man", "s\u00fchnt", "wohl", "sol\u00b7che", "Grau\u00b7sam\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nicht mehr in aller Ewigkeit.", "tokens": ["Nicht", "mehr", "in", "al\u00b7ler", "E\u00b7wig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "F\u00fcr T\u00e4nzer giebt es Raum und Zeit \u2013", "tokens": ["F\u00fcr", "T\u00e4n\u00b7zer", "giebt", "es", "Raum", "und", "Zeit", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "O, tiefbet\u00f6rte Menschlichkeit!", "tokens": ["O", ",", "tief\u00b7be\u00b7t\u00f6r\u00b7te", "Menschlich\u00b7keit", "!"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So lang' nicht Leichenh\u00e4user sind,", "tokens": ["So", "lang'", "nicht", "Lei\u00b7chen\u00b7h\u00e4u\u00b7ser", "sind", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKNEG", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Seid Alle Ihr so schlecht als blind! \u2013", "tokens": ["Seid", "Al\u00b7le", "Ihr", "so", "schlecht", "als", "blind", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAIMP", "PIS", "PPER", "ADV", "ADJD", "KOKOM", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}