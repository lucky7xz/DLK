{"textgrid.poem.54105": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Eine kleine Geburt", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich lebte mit Frau Sobernheimer;", "tokens": ["Ich", "leb\u00b7te", "mit", "Frau", "So\u00b7bern\u00b7hei\u00b7mer", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "sie war so lieb, sie war so nett.", "tokens": ["sie", "war", "so", "lieb", ",", "sie", "war", "so", "nett", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir wuschen uns im selben Eimer,", "tokens": ["Wir", "wu\u00b7schen", "uns", "im", "sel\u00b7ben", "Ei\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wir schliefen in demselben Bett.", "tokens": ["wir", "schlie\u00b7fen", "in", "dem\u00b7sel\u00b7ben", "Bett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So trieben wir es manches Jahr . . .", "tokens": ["So", "trie\u00b7ben", "wir", "es", "man\u00b7ches", "Jahr", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PIAT", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Bis sie den Knaben mir gebar.", "tokens": ["Bis", "sie", "den", "Kna\u00b7ben", "mir", "ge\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Doch dieser Knabe war kein Knabe.", "tokens": ["Doch", "die\u00b7ser", "Kna\u00b7be", "war", "kein", "Kna\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "NN", "VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir hatten in der dunklen Nacht", "tokens": ["Wir", "hat\u00b7ten", "in", "der", "dunk\u00b7len", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "als Zeitvertreib und Liebesgabe", "tokens": ["als", "Zeit\u00b7ver\u00b7treib", "und", "Lie\u00b7bes\u00b7ga\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "uns dieses Wesen ausgedacht.", "tokens": ["uns", "die\u00b7ses", "We\u00b7sen", "aus\u00b7ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PDAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Frau S. war jeden Kindes bar.", "tokens": ["Frau", "S.", "war", "je\u00b7den", "Kin\u00b7des", "bar", "."], "token_info": ["word", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Der Knabe, der hie\u00df Waldemar.", "tokens": ["Der", "Kna\u00b7be", ",", "der", "hie\u00df", "Wal\u00b7de\u00b7mar", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VVFIN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Und war so klug! \u2013 Nach f\u00fcnfzehn Tagen,", "tokens": ["Und", "war", "so", "klug", "!", "\u2013", "Nach", "f\u00fcnf\u00b7zehn", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "$.", "$(", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "gelebt im Kinderparadies,", "tokens": ["ge\u00b7lebt", "im", "Kin\u00b7der\u00b7pa\u00b7ra\u00b7dies", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVPP", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "da konnte er schon Scheibe sagen,", "tokens": ["da", "konn\u00b7te", "er", "schon", "Schei\u00b7be", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "bis man ihm solches leicht verwies.", "tokens": ["bis", "man", "ihm", "sol\u00b7ches", "leicht", "ver\u00b7wies", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "PIAT", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Er setzte sich aufs Tintenfa\u00df", "tokens": ["Er", "setz\u00b7te", "sich", "aufs", "Tin\u00b7ten\u00b7fa\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "und machte meinen Schreibtisch na\u00df.", "tokens": ["und", "mach\u00b7te", "mei\u00b7nen", "Schreib\u00b7tisch", "na\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Er wuchs heran, der Eltern Freude,", "tokens": ["Er", "wuchs", "he\u00b7ran", ",", "der", "El\u00b7tern", "Freu\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "ein braves, aufgewecktes Kind.", "tokens": ["ein", "bra\u00b7ves", ",", "auf\u00b7ge\u00b7weck\u00b7tes", "Kind", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir merkten an ihm alle beude,", "tokens": ["Wir", "merk\u00b7ten", "an", "ihm", "al\u00b7le", "beu\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wie s\u00fc\u00df der Liebe Fr\u00fcchte sind.", "tokens": ["wie", "s\u00fc\u00df", "der", "Lie\u00b7be", "Fr\u00fcch\u00b7te", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ART", "NN", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da fragte Mutti ganz real:", "tokens": ["Da", "frag\u00b7te", "Mut\u00b7ti", "ganz", "re\u00b7al", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "\u00bbwas wird der Junge denn nun mal \u2013?\u00ab", "tokens": ["\u00bb", "was", "wird", "der", "Jun\u00b7ge", "denn", "nun", "mal", "\u2013", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "NN", "KON", "ADV", "ADV", "$(", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Hebamme? General? Direktor?", "tokens": ["He\u00b7bam\u00b7me", "?", "Ge\u00b7ne\u00b7ral", "?", "Di\u00b7rek\u00b7tor", "?"], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "NN", "$."], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.2": {"text": "Bootlegger? Hirt? Ein Schiffsbarbier?", "tokens": ["Boot\u00b7leg\u00b7ger", "?", "Hirt", "?", "Ein", "Schiffs\u00b7bar\u00b7bier", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "$.", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Verlorner M\u00e4dchenheim-Inspektor?", "tokens": ["Ver\u00b7lor\u00b7ner", "M\u00e4d\u00b7chen\u00b7heim\u00b7In\u00b7spek\u00b7tor", "?"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Biographist? Gerichtsvollziehr?", "tokens": ["Bio\u00b7gra\u00b7phist", "?", "Ge\u00b7richts\u00b7voll\u00b7ziehr", "?"], "token_info": ["word", "punct", "word", "punct"], "pos": ["VAFIN", "$.", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ein Freudenm\u00e4nnchen? Jubilar \u2013?", "tokens": ["Ein", "Freu\u00b7den\u00b7m\u00e4nn\u00b7chen", "?", "Ju\u00b7bi\u00b7lar", "\u2013", "?"], "token_info": ["word", "word", "punct", "word", "punct", "punct"], "pos": ["ART", "NN", "$.", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Uneinig war das Elternpaar.", "tokens": ["Un\u00b7ei\u00b7nig", "war", "das", "El\u00b7tern\u00b7paar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ein Krach stieg auf, bis zu den Sternen!", "tokens": ["Ein", "Krach", "stieg", "auf", ",", "bis", "zu", "den", "Ster\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$,", "KOUS", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Frau S., die krisch. Die T\u00fcre knallt.", "tokens": ["Frau", "S.", ",", "die", "krisch", ".", "Die", "T\u00fc\u00b7re", "knallt", "."], "token_info": ["word", "abbreviation", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PRELS", "ADJD", "$.", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie wollt ihn lassen Bildung lernen,", "tokens": ["Sie", "wollt", "ihn", "las\u00b7sen", "Bil\u00b7dung", "ler\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "ich aber war f\u00fcr Staatsanwalt.", "tokens": ["ich", "a\u00b7ber", "war", "f\u00fcr", "Staats\u00b7an\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein Kompromi\u00df nahm sie nicht an:", "tokens": ["Ein", "Kom\u00b7pro\u00b7mi\u00df", "nahm", "sie", "nicht", "an", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "im Kino, als Bed\u00fcrfnismann.", "tokens": ["im", "Ki\u00b7no", ",", "als", "Be\u00b7d\u00fcrf\u00b7nis\u00b7mann", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "KOUS", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Der L\u00fcmmel gr\u00f6lte in der K\u00fcche", "tokens": ["Der", "L\u00fcm\u00b7mel", "gr\u00f6l\u00b7te", "in", "der", "K\u00fc\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und fand den Krach ganz wunderbar.", "tokens": ["und", "fand", "den", "Krach", "ganz", "wun\u00b7der\u00b7bar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So ging die Liebe in die Br\u00fcche \u2013", "tokens": ["So", "ging", "die", "Lie\u00b7be", "in", "die", "Br\u00fc\u00b7che", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und alles wegen Waldemar?", "tokens": ["und", "al\u00b7les", "we\u00b7gen", "Wal\u00b7de\u00b7mar", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da sprach ich fest: \u00bbMein trautes Gl\u00fcck!", "tokens": ["Da", "sprach", "ich", "fest", ":", "\u00bb", "Mein", "trau\u00b7tes", "Gl\u00fcck", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$.", "$(", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wir geben dieses J\u00f6r zur\u00fcck!\u00ab", "tokens": ["Wir", "ge\u00b7ben", "die\u00b7ses", "J\u00f6r", "zu\u00b7r\u00fcck", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Gemacht.", "tokens": ["Ge\u00b7macht", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Nun ist Frau Sobernheimer", "tokens": ["Nun", "ist", "Frau", "So\u00b7bern\u00b7hei\u00b7mer"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "NN", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "wie ehedem so lieb und nett.", "tokens": ["wie", "e\u00b7he\u00b7dem", "so", "lieb", "und", "nett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir waschen uns im selben Eimer,", "tokens": ["Wir", "wa\u00b7schen", "uns", "im", "sel\u00b7ben", "Ei\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "wir schlafen in demselben Bett.", "tokens": ["wir", "schla\u00b7fen", "in", "dem\u00b7sel\u00b7ben", "Bett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und denken nur noch hier und dar", "tokens": ["Und", "den\u00b7ken", "nur", "noch", "hier", "und", "dar"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "ADV", "ADV", "KON", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "mal an den seligen Waldemar.", "tokens": ["mal", "an", "den", "se\u00b7li\u00b7gen", "Wal\u00b7de\u00b7mar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}}}}