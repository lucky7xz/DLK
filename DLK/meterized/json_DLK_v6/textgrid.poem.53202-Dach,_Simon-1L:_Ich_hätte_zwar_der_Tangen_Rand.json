{"textgrid.poem.53202": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ich h\u00e4tte zwar der Tangen Rand", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich h\u00e4tte zwar der Tangen Rand", "tokens": ["Ich", "h\u00e4t\u00b7te", "zwar", "der", "Tan\u00b7gen", "Rand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Noch gern einmahl gegr\u00fcsset,", "tokens": ["Noch", "gern", "ein\u00b7mahl", "ge\u00b7gr\u00fcs\u00b7set", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Gern dich, mein liebes Vaterland,", "tokens": ["Gern", "dich", ",", "mein", "lie\u00b7bes", "Va\u00b7ter\u00b7land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu guter letzt gek\u00fcsset,", "tokens": ["Zu", "gu\u00b7ter", "letzt", "ge\u00b7k\u00fcs\u00b7set", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Eh mich der Tod h\u00e4tt auffgeleckt,", "tokens": ["Eh", "mich", "der", "Tod", "h\u00e4tt", "auff\u00b7ge\u00b7leckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der mich verfolgt ohn Ende,", "tokens": ["Der", "mich", "ver\u00b7folgt", "ohn", "En\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vnd stets nach mir h\u00e4lt au\u00dfgestreckt", "tokens": ["Vnd", "stets", "nach", "mir", "h\u00e4lt", "au\u00df\u00b7ge\u00b7streckt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "PPER", "VVFIN", "VVPP"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Die abgefleischten H\u00e4nde.", "tokens": ["Die", "ab\u00b7ge\u00b7fleischten", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.3": {"line.1": {"text": "Ich h\u00e4tt' auff den Fall nicht allein", "tokens": ["Ich", "h\u00e4tt'", "auff", "den", "Fall", "nicht", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "PTKNEG", "ADV"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Mich auffgemacht, die Sch\u00f6ne,", "tokens": ["Mich", "auff\u00b7ge\u00b7macht", ",", "die", "Sch\u00f6\u00b7ne", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Mein liebstes Hertz w\u00fcrd umb mich seyn,", "tokens": ["Mein", "liebs\u00b7tes", "Hertz", "w\u00fcrd", "umb", "mich", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "APPR", "PPER", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sampt einem meiner S\u00f6hne.", "tokens": ["Sampt", "ei\u00b7nem", "mei\u00b7ner", "S\u00f6h\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Wornach die meinen mich gefragt,", "tokens": ["Wor\u00b7nach", "die", "mei\u00b7nen", "mich", "ge\u00b7fragt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was l\u00e4ngst die Zeit verlohren,", "tokens": ["Was", "l\u00e4ngst", "die", "Zeit", "ver\u00b7loh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da- h\u00e4tt ich -von Bescheid gesagt:", "tokens": ["Da", "h\u00e4tt", "ich", "Be\u00b7scheid", "ge\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["TRUNC", "VAFIN", "PPER", "NE", "NN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die\u00df Hau\u00df hat mich gebohren,", "tokens": ["Die\u00df", "Hau\u00df", "hat", "mich", "ge\u00b7boh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Seht, diesen Weg bin offtmals ich", "tokens": ["Seht", ",", "die\u00b7sen", "Weg", "bin", "offt\u00b7mals", "ich"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PDAT", "NN", "VAFIN", "ADV", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Schlo\u00df hinauffgegangen,", "tokens": ["Das", "Schlo\u00df", "hin\u00b7auff\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Woselbst mein frommer Vater mich", "tokens": ["Wo\u00b7selbst", "mein", "from\u00b7mer", "Va\u00b7ter", "mich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit aller Lieb empfangen,", "tokens": ["Mit", "al\u00b7ler", "Lieb", "emp\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Mich auff dem Wall umbher gef\u00fchrt,", "tokens": ["Mich", "auff", "dem", "Wall", "um\u00b7bher", "ge\u00b7f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dort (sprach er) schaw doch, lieber,", "tokens": ["Dort", "(", "sprach", "er", ")", "schaw", "doch", ",", "lie\u00b7ber", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$(", "VVFIN", "PPER", "$(", "VVFIN", "ADV", "$,", "ADV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ward vormals keine See gesp\u00fcrt,", "tokens": ["Ward", "vor\u00b7mals", "kei\u00b7ne", "See", "ge\u00b7sp\u00fcrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Sandberg gieng dar\u00fcber,", "tokens": ["Der", "Sand\u00b7berg", "gieng", "da\u00b7r\u00fc\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PAV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Jetzt kanst du sie und Segel sehn", "tokens": ["Jetzt", "kanst", "du", "sie", "und", "Se\u00b7gel", "sehn"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "KON", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In jhren W\u00e4llen fahren,", "tokens": ["In", "jhren", "W\u00e4l\u00b7len", "fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Die\u00df ist bey meiner Zeit geschehn", "tokens": ["Die\u00df", "ist", "bey", "mei\u00b7ner", "Zeit", "ge\u00b7schehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nur inner dreysig Jahren;", "tokens": ["Nur", "in\u00b7ner", "drey\u00b7sig", "Jah\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "CARD", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Vnd so ist aller Ding ein Ziel.", "tokens": ["Vnd", "so", "ist", "al\u00b7ler", "Ding", "ein", "Ziel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PIAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hier (h\u00e4tt' ich mehr gesprochen)", "tokens": ["Hier", "(", "h\u00e4tt'", "ich", "mehr", "ge\u00b7spro\u00b7chen", ")"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "VAFIN", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ward j\u00e4hrlich umb das Fa\u00dfnacht-Spiel", "tokens": ["Ward", "j\u00e4hr\u00b7lich", "umb", "das", "Fa\u00df\u00b7nacht\u00b7Spiel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Geritten und gestochen.", "tokens": ["Ge\u00b7rit\u00b7ten", "und", "ge\u00b7sto\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Viel G\u00e4rten sind zu jener Zeit", "tokens": ["Viel", "G\u00e4r\u00b7ten", "sind", "zu", "je\u00b7ner", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VAFIN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hie, d\u00fcncket mich, gewesen,", "tokens": ["Hie", ",", "d\u00fcn\u00b7cket", "mich", ",", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "VVFIN", "PPER", "$,", "VAPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Mars hat die\u00df alles f\u00fcr den Streit", "tokens": ["Mars", "hat", "die\u00df", "al\u00b7les", "f\u00fcr", "den", "Streit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PDS", "PIS", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ihm nun zum Wall erlesen.", "tokens": ["Ihm", "nun", "zum", "Wall", "er\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Wie dort auch, wo die Pfarr-Geb\u00e4w", "tokens": ["Wie", "dort", "auch", ",", "wo", "die", "Pfarr\u00b7Ge\u00b7b\u00e4w"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "ADV", "ADV", "$,", "PWAV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd Schule damals stunden,", "tokens": ["Vnd", "Schu\u00b7le", "da\u00b7mals", "stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "ADJA", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Jetzt, seht jhr, wird nur W\u00fcsteney", "tokens": ["Jetzt", ",", "seht", "jhr", ",", "wird", "nur", "W\u00fcs\u00b7te\u00b7ney"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "VVFIN", "PPER", "$,", "VAFIN", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd Erde da gefunden.", "tokens": ["Vnd", "Er\u00b7de", "da", "ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Die Meinen wohnten letzlich dort,", "tokens": ["Die", "Mei\u00b7nen", "wohn\u00b7ten", "letz\u00b7lich", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie hat es sich verkehret!", "tokens": ["Wie", "hat", "es", "sich", "ver\u00b7keh\u00b7ret", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das Feuer, seh' ich, hat den Ort", "tokens": ["Das", "Feu\u00b7er", ",", "seh'", "ich", ",", "hat", "den", "Ort"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "VVFIN", "PPER", "$,", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bi\u00df auff den Grund verheret.", "tokens": ["Bi\u00df", "auff", "den", "Grund", "ver\u00b7he\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Was Kurtzweil brachte der Ort mir", "tokens": ["Was", "Kurt\u00b7zweil", "brach\u00b7te", "der", "Ort", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "NN", "VVFIN", "ART", "NN", "PPER"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Vnd meines gleichen Knaben!", "tokens": ["Vnd", "mei\u00b7nes", "glei\u00b7chen", "Kna\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die mich gebohren hat liegt hier,", "tokens": ["Die", "mich", "ge\u00b7boh\u00b7ren", "hat", "liegt", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVPP", "VAFIN", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mein Vater dort begraben.", "tokens": ["Mein", "Va\u00b7ter", "dort", "be\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Die\u00df und dergleichen w\u00fcrde seyn", "tokens": ["Die\u00df", "und", "derg\u00b7lei\u00b7chen", "w\u00fcr\u00b7de", "seyn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "KON", "PIS", "VAFIN", "PPOSAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Daselbst mein Zeit-Vertreiben,", "tokens": ["Da\u00b7selbst", "mein", "Zeit\u00b7Ver\u00b7trei\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ach mein Verh\u00e4ngni\u00df saget: nein!", "tokens": ["Ach", "mein", "Ver\u00b7h\u00e4ng\u00b7ni\u00df", "sa\u00b7get", ":", "nein", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ITJ", "PPOSAT", "NN", "VVFIN", "$.", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich mu\u00df es lassen bleiben.", "tokens": ["Ich", "mu\u00df", "es", "las\u00b7sen", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVINF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Was ladet jhr doch, Herr Pretor,", "tokens": ["Was", "la\u00b7det", "jhr", "doch", ",", "Herr", "Pre\u00b7tor", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "$,", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mich ein zu ewrer Freude?", "tokens": ["Mich", "ein", "zu", "ew\u00b7rer", "Freu\u00b7de", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Kr\u00e4fft' hiezu hatt' ich zuvor,", "tokens": ["Die", "Kr\u00e4fft'", "hie\u00b7zu", "hatt'", "ich", "zu\u00b7vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VAFIN", "PPER", "ADV", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Die ich nun ewig meide.", "tokens": ["Die", "ich", "nun", "e\u00b7wig", "mei\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Habt Danck, empfindet Gn\u00fcg und Ruh", "tokens": ["Habt", "Danck", ",", "emp\u00b7fin\u00b7det", "Gn\u00fcg", "und", "Ruh"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "NN", "$,", "VVFIN", "NE", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An dieser sch\u00f6nen Liebe,", "tokens": ["An", "die\u00b7ser", "sch\u00f6\u00b7nen", "Lie\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Kein Leid komm' jhren Freuden zu,", "tokens": ["Kein", "Leid", "komm'", "jhren", "Freu\u00b7den", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-++-+-+", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Vnd mach' jhr Wetter tr\u00fcbe.", "tokens": ["Vnd", "mach'", "jhr", "Wet\u00b7ter", "tr\u00fc\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Erfahrt umb jede Jahres-Zeit,", "tokens": ["Er\u00b7fahrt", "umb", "je\u00b7de", "Jah\u00b7res\u00b7Zeit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df sie sich fleissig bawe,", "tokens": ["Da\u00df", "sie", "sich", "fleis\u00b7sig", "ba\u00b7we", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVFIN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Vnd nebenst guter Fruchtbarkeit", "tokens": ["Vnd", "ne\u00b7benst", "gu\u00b7ter", "Frucht\u00b7bar\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auch grosse G\u00fcter schawe.", "tokens": ["Auch", "gros\u00b7se", "G\u00fc\u00b7ter", "scha\u00b7we", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.17": {"line.1": {"text": "Ich stelle nunmehr Lust und Welt", "tokens": ["Ich", "stel\u00b7le", "nun\u00b7mehr", "Lust", "und", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Fern ausser meinem Hertzen,", "tokens": ["Fern", "aus\u00b7ser", "mei\u00b7nem", "Hert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So bald es meinem Gott gef\u00e4lt,", "tokens": ["So", "bald", "es", "mei\u00b7nem", "Gott", "ge\u00b7f\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df ich Ihm folg ohn Schmertzen.", "tokens": ["Da\u00df", "ich", "Ihm", "folg", "ohn", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Ich bin auff andre Lust bedacht,", "tokens": ["Ich", "bin", "auff", "and\u00b7re", "Lust", "be\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Gott wird dort mir geben.", "tokens": ["Die", "Gott", "wird", "dort", "mir", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Du wehrte M\u00fcmmel, gute Nacht,", "tokens": ["Du", "wehr\u00b7te", "M\u00fcm\u00b7mel", ",", "gu\u00b7te", "Nacht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Du m\u00fcssest gl\u00fcckhafft leben.", "tokens": ["Du", "m\u00fcs\u00b7sest", "gl\u00fcck\u00b7hafft", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Kein Vnmuht, kein Verlust, kein Leid", "tokens": ["Kein", "Vn\u00b7muht", ",", "kein", "Ver\u00b7lust", ",", "kein", "Leid"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "$,", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Geb' Vrsach dir zu trawren,", "tokens": ["Geb'", "Vr\u00b7sach", "dir", "zu", "traw\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Empfinde Fried und gute Zeit", "tokens": ["Emp\u00b7fin\u00b7de", "Fried", "und", "gu\u00b7te", "Zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Stets inner deinen Mawren.", "tokens": ["Stets", "in\u00b7ner", "dei\u00b7nen", "Maw\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Gehabt euch wol, jhr Berg und Thal,", "tokens": ["Ge\u00b7habt", "euch", "wol", ",", "jhr", "Berg", "und", "Thal", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stein, Brunnen, Pusch und Awen,", "tokens": ["Stein", ",", "Brun\u00b7nen", ",", "Pusch", "und", "A\u00b7wen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wo ich geschertzt so manches mal,", "tokens": ["Wo", "ich", "ge\u00b7schertzt", "so", "man\u00b7ches", "mal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVPP", "ADV", "PIS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich werd' euch nicht mehr schawen.", "tokens": ["Ich", "werd'", "euch", "nicht", "mehr", "scha\u00b7wen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.21": {"line.1": {"text": "Wie auch jhr Freund, Herr Rodemann,", "tokens": ["Wie", "auch", "jhr", "Freund", ",", "Herr", "Ro\u00b7de\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "NN", "$,", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Herr Friedrichsen imgleichen,", "tokens": ["Herr", "Fried\u00b7rich\u00b7sen", "im\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "NE", "ADV", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Lebt wol, kein Vngl\u00fcck komm euch an,", "tokens": ["Lebt", "wol", ",", "kein", "Vn\u00b7gl\u00fcck", "komm", "euch", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "PIAT", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kein Leid m\u00fcss' euch bestreichen!", "tokens": ["Kein", "Leid", "m\u00fcss'", "euch", "be\u00b7strei\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "K\u00f6mpt euch zu Ohren ohngefehr,", "tokens": ["K\u00f6mpt", "euch", "zu", "Oh\u00b7ren", "ohn\u00b7ge\u00b7fehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich sey nun hingenommen,", "tokens": ["Ich", "sey", "nun", "hin\u00b7ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So lasst aus ewren Hertzen her", "tokens": ["So", "lasst", "aus", "ew\u00b7ren", "Hert\u00b7zen", "her"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "PPOSAT", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nur einen Seufftzer kommen.", "tokens": ["Nur", "ei\u00b7nen", "Seufft\u00b7zer", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.23": {"line.1": {"text": "Was wichtigers begehr' ich nicht,", "tokens": ["Was", "wich\u00b7ti\u00b7gers", "be\u00b7gehr'", "ich", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Wehrt ist zu geringe,", "tokens": ["Mein", "Wehrt", "ist", "zu", "ge\u00b7rin\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "APPR", "ADJA", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Es w\u00e4re, da\u00df ich die Geticht'", "tokens": ["Es", "w\u00e4\u00b7re", ",", "da\u00df", "ich", "die", "Ge\u00b7ticht'"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "KOUS", "PPER", "ART", "NN"], "meter": "-+-++--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Erst her in Preussen bringe,", "tokens": ["Erst", "her", "in", "Preus\u00b7sen", "brin\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.24": {"line.1": {"text": "Ich erst den deutschen Helicon", "tokens": ["Ich", "erst", "den", "deut\u00b7schen", "He\u00b7li\u00b7con"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nach K\u00f6nigsberg versetzet,", "tokens": ["Nach", "K\u00f6\u00b7nigs\u00b7berg", "ver\u00b7set\u00b7zet", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ob dessen Danck ist oder Lohn,", "tokens": ["Ob", "des\u00b7sen", "Danck", "ist", "o\u00b7der", "Lohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRELAT", "NN", "VAFIN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir wird es gleich gesch\u00e4tzet.", "tokens": ["Mir", "wird", "es", "gleich", "ge\u00b7sch\u00e4t\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Gnug, wo mein Reim das Gl\u00fcck nur hat", "tokens": ["Gnug", ",", "wo", "mein", "Reim", "das", "Gl\u00fcck", "nur", "hat"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PWAV", "PPOSAT", "NN", "ART", "NN", "ADV", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd wird nach mir gelesen,", "tokens": ["Vnd", "wird", "nach", "mir", "ge\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df dennoch meine Vater-Stad", "tokens": ["Da\u00df", "den\u00b7noch", "mei\u00b7ne", "Va\u00b7ter\u00b7Stad"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die M\u00fcmmel ist gewesen.", "tokens": ["Die", "M\u00fcm\u00b7mel", "ist", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VAPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}