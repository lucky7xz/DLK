{"textgrid.poem.57064": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Butterbrotpapier im Wald,", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Butterbrotpapier im Wald,", "tokens": ["Ein", "But\u00b7ter\u00b7brot\u00b7pa\u00b7pier", "im", "Wald", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da es beschneit wird, f\u00fchlt sich kalt ...", "tokens": ["da", "es", "be\u00b7schneit", "wird", ",", "f\u00fchlt", "sich", "kalt", "..."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$,", "VVFIN", "PRF", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "In seiner Angst, wiewohl es nie", "tokens": ["In", "sei\u00b7ner", "Angst", ",", "wie\u00b7wohl", "es", "nie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KOUS", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "an Denken vorher irgendwie", "tokens": ["an", "Den\u00b7ken", "vor\u00b7her", "ir\u00b7gend\u00b7wie"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "gedacht, nat\u00fcrlich, als ein Ding", "tokens": ["ge\u00b7dacht", ",", "na\u00b7t\u00fcr\u00b7lich", ",", "als", "ein", "Ding"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVPP", "$,", "ADV", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "aus Lumpen usw., fing,", "tokens": ["aus", "Lum\u00b7pen", "us\u00b7w.", ",", "fing", ","], "token_info": ["word", "word", "abbreviation", "punct", "word", "punct"], "pos": ["APPR", "NN", "NE", "$,", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "aus Angst, so sagte ich, fing an", "tokens": ["aus", "Angst", ",", "so", "sag\u00b7te", "ich", ",", "fing", "an"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$,", "ADV", "VVFIN", "PPER", "$,", "VVFIN", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zu denken, fing, hob an, begann", "tokens": ["zu", "den\u00b7ken", ",", "fing", ",", "hob", "an", ",", "be\u00b7gann"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word"], "pos": ["PTKZU", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "PTKVZ", "$,", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "zu denken, denkt euch, was das hei\u00dft,", "tokens": ["zu", "den\u00b7ken", ",", "denkt", "euch", ",", "was", "das", "hei\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "VVFIN", "PPER", "$,", "PWS", "PDS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bekam (aus Angst, so sagt' ich) \u2013 Geist,", "tokens": ["be\u00b7kam", "(", "aus", "Angst", ",", "so", "sagt'", "ich", ")", "\u2013", "Geist", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["VVFIN", "$(", "APPR", "NN", "$,", "ADV", "VVFIN", "PPER", "$(", "$(", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "und zwar, versteht sich, nicht blo\u00df so", "tokens": ["und", "zwar", ",", "ver\u00b7steht", "sich", ",", "nicht", "blo\u00df", "so"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "VVFIN", "PRF", "$,", "PTKNEG", "ADV", "ADV"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "vom Himmel droben irgendwo,", "tokens": ["vom", "Him\u00b7mel", "dro\u00b7ben", "ir\u00b7gend\u00b7wo", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "vielmehr infolge einer ganz", "tokens": ["viel\u00b7mehr", "in\u00b7fol\u00b7ge", "ei\u00b7ner", "ganz"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "exakt entstandnen Hirnsubstanz \u2013", "tokens": ["ex\u00b7akt", "ent\u00b7stand\u00b7nen", "Hirn\u00b7subs\u00b7tanz", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$("], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.8": {"line.1": {"text": "die aus Holz, Eiwei\u00df, Mehl und Schmer,", "tokens": ["die", "aus", "Holz", ",", "Ei\u00b7wei\u00df", ",", "Mehl", "und", "Schmer", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "(durch Angst), mit \u00dcberspringung der", "tokens": ["(", "durch", "Angst", ")", ",", "mit", "\u00dc\u00b7bers\u00b7prin\u00b7gung", "der"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["$(", "APPR", "NN", "$(", "$,", "APPR", "NN", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "sonst \u00fcblichen Weltalter, an", "tokens": ["sonst", "\u00fcb\u00b7li\u00b7chen", "Welt\u00b7al\u00b7ter", ",", "an"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ADV", "ADJA", "NN", "$,", "APPR"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "ihm Boden und Gef\u00e4\u00df gewann \u2013", "tokens": ["ihm", "Bo\u00b7den", "und", "Ge\u00b7f\u00e4\u00df", "ge\u00b7wann", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "[(mit \u00dcberspringung) in und an", "tokens": ["(", "mit", "\u00dc\u00b7bers\u00b7prin\u00b7gung", ")", "in", "und", "an"], "token_info": ["punct", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "$(", "APPR", "NN", "$(", "APPR", "KON", "APPR"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "ihm Boden und Gef\u00e4\u00df gewann.]", "tokens": ["ihm", "Bo\u00b7den", "und", "Ge\u00b7f\u00e4\u00df", "ge\u00b7wann", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "NN", "KON", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Mithilfe dieser Hilfe nun", "tokens": ["Mi\u00b7thil\u00b7fe", "die\u00b7ser", "Hil\u00b7fe", "nun"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PDAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "entschlo\u00df sich das Papier zum Tun,", "tokens": ["ent\u00b7schlo\u00df", "sich", "das", "Pa\u00b7pier", "zum", "Tun", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "zum Leben, zum \u2013 gleichviel, es fing", "tokens": ["zum", "Le\u00b7ben", ",", "zum", "\u2013", "gleich\u00b7viel", ",", "es", "fing"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["APPRART", "NN", "$,", "APPRART", "$(", "VVFIN", "$,", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "zu gehn an \u2013 wie ein Schmetterling ...", "tokens": ["zu", "gehn", "an", "\u2013", "wie", "ein", "Schmet\u00b7ter\u00b7ling", "..."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "$(", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "zu kriechen erst, zu fliegen drauf,", "tokens": ["zu", "krie\u00b7chen", "erst", ",", "zu", "flie\u00b7gen", "drauf", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVFIN", "ADV", "$,", "PTKZU", "VVINF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "bis \u00fcbers Unterholz hinauf,", "tokens": ["bis", "\u00fc\u00b7bers", "Un\u00b7ter\u00b7holz", "hin\u00b7auf", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "dann \u00fcber die Chaussee und quer", "tokens": ["dann", "\u00fc\u00b7ber", "die", "Chaus\u00b7see", "und", "quer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "KON", "NN"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "und kreuz und links und hin und her \u2013", "tokens": ["und", "kreuz", "und", "links", "und", "hin", "und", "her", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "ADV", "KON", "PTKVZ", "KON", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "wie eben solch ein Tier zur Welt", "tokens": ["wie", "e\u00b7ben", "solch", "ein", "Tier", "zur", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "PIAT", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(je nach dem Wind) (und sonst) sich stellt.", "tokens": ["(", "je", "nach", "dem", "Wind", ")", "(", "und", "sonst", ")", "sich", "stellt", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "$(", "$(", "KON", "ADV", "$(", "PRF", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.16": {"line.1": {"text": "Doch, Freunde! werdet bleich gleich mir! \u2013:", "tokens": ["Doch", ",", "Freun\u00b7de", "!", "wer\u00b7det", "bleich", "gleich", "mir", "!", "\u2013", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "$,", "NN", "$.", "VAFIN", "ADJD", "ADV", "PPER", "$.", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Vogel, dick und ganz voll Gier,", "tokens": ["Ein", "Vo\u00b7gel", ",", "dick", "und", "ganz", "voll", "Gier", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "$,", "ADJD", "KON", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "erblickt's (wir sind im Januar ...) \u2013", "tokens": ["er\u00b7blickt's", "(", "wir", "sind", "im", "Ja\u00b7nu\u00b7ar", "...", ")", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NE", "$(", "PPER", "VAFIN", "APPRART", "NN", "$(", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und schickt sich an, mit Haut und Haar \u2013", "tokens": ["und", "schickt", "sich", "an", ",", "mit", "Haut", "und", "Haar", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "$,", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "und schickt sich an, mit Haar und Haut \u2013", "tokens": ["und", "schickt", "sich", "an", ",", "mit", "Haar", "und", "Haut", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "$,", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(wer mag da endigen!) (mir graut) \u2013", "tokens": ["(", "wer", "mag", "da", "en\u00b7di\u00b7gen", "!", ")", "(", "mir", "graut", ")", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VMFIN", "ADV", "VVINF", "$.", "$(", "$(", "PPER", "VVFIN", "$(", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.19": {"line.1": {"text": "(bedenkt, was alles n\u00f6tig war!) \u2013", "tokens": ["(", "be\u00b7denkt", ",", "was", "al\u00b7les", "n\u00f6\u00b7tig", "war", "!", ")", "\u2013"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "VVFIN", "$,", "PRELS", "PIS", "ADJD", "VAFIN", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und schickt sich an, mit Haut und Haar \u2013 \u2013", "tokens": ["und", "schickt", "sich", "an", ",", "mit", "Haut", "und", "Haar", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKVZ", "$,", "APPR", "NN", "KON", "NN", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "ein Butterbrotpapier im Wald", "tokens": ["ein", "But\u00b7ter\u00b7brot\u00b7pa\u00b7pier", "im", "Wald"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "gewinnt \u2013 aus Angst \u2013 Naturgestalt ...", "tokens": ["ge\u00b7winnt", "\u2013", "aus", "Angst", "\u2013", "Na\u00b7tur\u00b7ge\u00b7stalt", "..."], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$(", "APPR", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Genug!! Der wilde Specht verschluckt", "tokens": ["Ge\u00b7nug", "!!", "Der", "wil\u00b7de", "Specht", "ver\u00b7schluckt"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "$.", "ART", "ADJA", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "das unersetzliche Produkt ...", "tokens": ["das", "un\u00b7er\u00b7setz\u00b7li\u00b7che", "Pro\u00b7dukt", "..."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}