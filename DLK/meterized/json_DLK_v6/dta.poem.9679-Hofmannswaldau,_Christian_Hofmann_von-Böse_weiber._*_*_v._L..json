{"dta.poem.9679": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "B\u00f6se weiber.  \n * * v. L.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1697", "urn": "urn:nbn:de:kobv:b4-200905199377", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Nichts ist auf erden \u00fcber b\u00f6se weiber/", "tokens": ["Nichts", "ist", "auf", "er\u00b7den", "\u00fc\u00b7ber", "b\u00f6\u00b7se", "wei\u00b7ber", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "APPR", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie fressen ihren m\u00e4nnern aus das hertz;", "tokens": ["Sie", "fres\u00b7sen", "ih\u00b7ren", "m\u00e4n\u00b7nern", "aus", "das", "hertz", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Sie machen matt die sonsten frische leiber/", "tokens": ["Sie", "ma\u00b7chen", "matt", "die", "sons\u00b7ten", "fri\u00b7sche", "lei\u00b7ber", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "ART", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Geb\u00e4hren nichts/ als unruh/ sorgen/ schmertz/", "tokens": ["Ge\u00b7b\u00e4h\u00b7ren", "nichts", "/", "als", "un\u00b7ruh", "/", "sor\u00b7gen", "/", "schmertz", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "PIS", "$(", "KOUS", "NN", "$(", "NN", "$(", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sie saugen aus das marck aus allen beinen/", "tokens": ["Sie", "sau\u00b7gen", "aus", "das", "marck", "aus", "al\u00b7len", "bei\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "APPR", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Und machen sie durch zancken m\u00fcd und faul/", "tokens": ["Und", "ma\u00b7chen", "sie", "durch", "zan\u00b7cken", "m\u00fcd", "und", "faul", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "VVINF", "VMFIN", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Da\u00df sie gleich ausgedorrtem holtze scheinen/", "tokens": ["Da\u00df", "sie", "gleich", "aus\u00b7ge\u00b7dorr\u00b7tem", "holt\u00b7ze", "schei\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und wie ein d\u00fcrr und abgeschlagner gaul.", "tokens": ["Und", "wie", "ein", "d\u00fcrr", "und", "ab\u00b7ge\u00b7schlag\u00b7ner", "gaul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "ADJD", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Man h\u00f6rt sie wie die b\u00f6sen hunde bellen/", "tokens": ["Man", "h\u00f6rt", "sie", "wie", "die", "b\u00f6\u00b7sen", "hun\u00b7de", "bel\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "KOKOM", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Wenn sie einmal von zorn und grimm entbrandt;", "tokens": ["Wenn", "sie", "ein\u00b7mal", "von", "zorn", "und", "grimm", "ent\u00b7brandt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NN", "KON", "ADJD", "VVPP", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.11": {"text": "Kein gutes wort kan sie zu frieden stellen/", "tokens": ["Kein", "gu\u00b7tes", "wort", "kan", "sie", "zu", "frie\u00b7den", "stel\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Sie wollen allzeit haben oberhand.", "tokens": ["Sie", "wol\u00b7len", "all\u00b7zeit", "ha\u00b7ben", "o\u00b7ber\u00b7hand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VAFIN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Der mann mu\u00df donner/ hagel/ h\u00f6ren st\u00fcndlich/", "tokens": ["Der", "mann", "mu\u00df", "don\u00b7ner", "/", "ha\u00b7gel", "/", "h\u00f6\u00b7ren", "st\u00fcnd\u00b7lich", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "NN", "$(", "NE", "$(", "VVFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "Es heist: du bettelhund/ bey tag und nacht;", "tokens": ["Es", "heist", ":", "du", "bet\u00b7tel\u00b7hund", "/", "bey", "tag", "und", "nacht", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VVFIN", "$(", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Zu allen schl\u00e4gen sind sie unempfindlich/", "tokens": ["Zu", "al\u00b7len", "schl\u00e4\u00b7gen", "sind", "sie", "un\u00b7emp\u00b7find\u00b7lich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "VAFIN", "PPER", "ADJD", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Und werden \u00e4rger nur dadurch gemacht.", "tokens": ["Und", "wer\u00b7den", "\u00e4r\u00b7ger", "nur", "da\u00b7durch", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "ADV", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}