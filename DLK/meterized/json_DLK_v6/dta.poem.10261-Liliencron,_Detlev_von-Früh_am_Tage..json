{"dta.poem.10261": {"metadata": {"author": {"name": "Liliencron, Detlev von", "birth": "N.A.", "death": "N.A."}, "title": "Fr\u00fch am Tage.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1883", "urn": "urn:nbn:de:kobv:b4-200905197184", "language": ["de:0.99"], "booktitle": "Liliencron, Detlev von: Adjutantenritte und andere Gedichte. Leipzig, [1883]."}, "poem": {"stanza.1": {"line.1": {"text": "In der Fensterluken schmalen Ritzen             ", "tokens": ["In", "der", "Fens\u00b7ter\u00b7lu\u00b7ken", "schma\u00b7len", "Rit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Klemmt der Morgen sich die Fingerspitzen.", "tokens": ["Klemmt", "der", "Mor\u00b7gen", "sich", "die", "Fin\u00b7ger\u00b7spit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "PRF", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Kann von meinem M\u00e4dchen mich nicht trennen,", "tokens": ["Kann", "von", "mei\u00b7nem", "M\u00e4d\u00b7chen", "mich", "nicht", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PPOSAT", "NN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Mu\u00df mit tausend Schmeichelnamen sie benennen.", "tokens": ["Mu\u00df", "mit", "tau\u00b7send", "Schmei\u00b7chel\u00b7na\u00b7men", "sie", "be\u00b7nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "CARD", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.2": {"line.1": {"text": "Dr\u00e4ngt die liebe Kleine nach der Th\u00fcre,", "tokens": ["Dr\u00e4ngt", "die", "lie\u00b7be", "Klei\u00b7ne", "nach", "der", "Th\u00fc\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Halt\u2019 ich sie durch tausend Liebesschw\u00fcre.", "tokens": ["Halt'", "ich", "sie", "durch", "tau\u00b7send", "Lie\u00b7bes\u00b7schw\u00fc\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPER", "APPR", "CARD", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Mu\u00df ich leider endlich selber treiben,", "tokens": ["Mu\u00df", "ich", "lei\u00b7der", "end\u00b7lich", "sel\u00b7ber", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "F\u00e4llt sie, wortlos, um den Hals mir, m\u00f6chte bleiben.", "tokens": ["F\u00e4llt", "sie", ",", "wort\u00b7los", ",", "um", "den", "Hals", "mir", ",", "m\u00f6ch\u00b7te", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PWAV", "$,", "KOUI", "ART", "NN", "PPER", "$,", "VMFIN", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.3": {"line.1": {"text": "Liebster, so, nun la\u00df mich, la\u00df mich gehen,", "tokens": ["Liebs\u00b7ter", ",", "so", ",", "nun", "la\u00df", "mich", ",", "la\u00df", "mich", "ge\u00b7hen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "$,", "ADV", "VVIMP", "PPER", "$,", "VVIMP", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Doch im Gehen bleibt sie z\u00f6gernd stehen,", "tokens": ["Doch", "im", "Ge\u00b7hen", "bleibt", "sie", "z\u00f6\u00b7gernd", "ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Noch ein letztes Horchen, letzte Winke,", "tokens": ["Noch", "ein", "letz\u00b7tes", "Hor\u00b7chen", ",", "letz\u00b7te", "Win\u00b7ke", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und dann fa\u00dft und dr\u00fcckt sie leise, leis die Klinke.", "tokens": ["Und", "dann", "fa\u00dft", "und", "dr\u00fcckt", "sie", "lei\u00b7se", ",", "leis", "die", "Klin\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "KON", "VVFIN", "PPER", "ADJD", "$,", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.4": {"line.1": {"text": "Schuh\u2019 aus, schleicht sie, da\u00df sie Keiner sp\u00fcre,", "tokens": ["Schuh'", "aus", ",", "schleicht", "sie", ",", "da\u00df", "sie", "Kei\u00b7ner", "sp\u00fc\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$,", "VVFIN", "PPER", "$,", "KOUS", "PPER", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Und ich schlie\u00dfe sachte, sacht die Th\u00fcre,", "tokens": ["Und", "ich", "schlie\u00b7\u00dfe", "sach\u00b7te", ",", "sacht", "die", "Th\u00fc\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "VVFIN", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "\u00d6ffne leise, leise dann die Luken,", "tokens": ["\u00d6ff\u00b7ne", "lei\u00b7se", ",", "lei\u00b7se", "dann", "die", "Lu\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "In die frische, sch\u00f6ne Morgenwelt zu gucken.", "tokens": ["In", "die", "fri\u00b7sche", ",", "sch\u00f6\u00b7ne", "Mor\u00b7gen\u00b7welt", "zu", "gu\u00b7cken", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "$,", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}}}}