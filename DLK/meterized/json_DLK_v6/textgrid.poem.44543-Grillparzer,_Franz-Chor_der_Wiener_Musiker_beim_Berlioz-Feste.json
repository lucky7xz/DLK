{"textgrid.poem.44543": {"metadata": {"author": {"name": "Grillparzer, Franz", "birth": "N.A.", "death": "N.A."}, "title": "Chor der Wiener Musiker beim Berlioz-Feste", "genre": "verse", "period": "N.A.", "pub_year": 1845, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Genossen, macht ein ernst Gesicht,", "tokens": ["Ge\u00b7nos\u00b7sen", ",", "macht", "ein", "ernst", "Ge\u00b7sicht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es geht um unsre Ehre,", "tokens": ["Es", "geht", "um", "uns\u00b7re", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und k\u00f6nnen wir das Leichte nicht,", "tokens": ["Und", "k\u00f6n\u00b7nen", "wir", "das", "Leich\u00b7te", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Versuchen wir das Schwere.", "tokens": ["Ver\u00b7su\u00b7chen", "wir", "das", "Schwe\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Setzt hoch und h\u00f6her euch das Ziel,", "tokens": ["Setzt", "hoch", "und", "h\u00f6\u00b7her", "euch", "das", "Ziel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verspottet alle Schranken;", "tokens": ["Ver\u00b7spot\u00b7tet", "al\u00b7le", "Schran\u00b7ken", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Von fern gesehn, erspart man viel,", "tokens": ["Von", "fern", "ge\u00b7sehn", ",", "er\u00b7spart", "man", "viel", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VVPP", "$,", "VVFIN", "PIS", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vor allem die Gedanken.", "tokens": ["Vor", "al\u00b7lem", "die", "Ge\u00b7dan\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Und fehlt uns etwa das Talent,", "tokens": ["Und", "fehlt", "uns", "et\u00b7wa", "das", "Ta\u00b7lent", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Genie lacht der Gemeinheit,", "tokens": ["Ge\u00b7nie", "lacht", "der", "Ge\u00b7mein\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Drum, Nullen, schart soviel ihr k\u00f6nnt,", "tokens": ["Drum", ",", "Nul\u00b7len", ",", "schart", "so\u00b7viel", "ihr", "k\u00f6nnt", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "NN", "$,", "VVFIN", "PIS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Euch um die fremde Einheit.", "tokens": ["Euch", "um", "die", "frem\u00b7de", "Ein\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Der Haydn ist doch gar zu alt,", "tokens": ["Der", "Haydn", "ist", "doch", "gar", "zu", "alt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "PTKA", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Was soll uns solch Gewinsel?", "tokens": ["Was", "soll", "uns", "solch", "Ge\u00b7win\u00b7sel", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir malen auch, wie er gemalt,", "tokens": ["Wir", "ma\u00b7len", "auch", ",", "wie", "er", "ge\u00b7malt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PWAV", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nur mit dem groben Pinsel.", "tokens": ["Nur", "mit", "dem", "gro\u00b7ben", "Pin\u00b7sel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.5": {"line.1": {"text": "Und h\u00e4lt sie Mozart noch behext,", "tokens": ["Und", "h\u00e4lt", "sie", "Mo\u00b7zart", "noch", "be\u00b7hext", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "NE", "ADV", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Sein Reich soll bald verschwinden.", "tokens": ["Sein", "Reich", "soll", "bald", "ver\u00b7schwin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wir ", "tokens": ["Wir"], "token_info": ["word"], "pos": ["PPER"], "meter": "-", "measure": "single.down"}, "line.4": {"text": "Bei ihm wars blo\u00df Empfinden.", "tokens": ["Bei", "ihm", "wars", "blo\u00df", "Emp\u00b7fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "ADV", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.6": {"line.1": {"text": "Beethoven erst hob sich vom Staub,", "tokens": ["Beet\u00b7ho\u00b7ven", "erst", "hob", "sich", "vom", "Staub", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Drum sei er unser Lehrer;", "tokens": ["Drum", "sei", "er", "un\u00b7ser", "Leh\u00b7rer", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Hei\u00dft das: von da an, wo er taub,", "tokens": ["Hei\u00dft", "das", ":", "von", "da", "an", ",", "wo", "er", "taub", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "$.", "APPR", "ADV", "PTKVZ", "$,", "PWAV", "PPER", "ADJD", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "So w\u00fcnschen wir die H\u00f6rer.", "tokens": ["So", "w\u00fcn\u00b7schen", "wir", "die", "H\u00f6\u00b7rer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Und wo ein Gro\u00dfes, wo ein Kleins,", "tokens": ["Und", "wo", "ein", "Gro\u00b7\u00dfes", ",", "wo", "ein", "Kleins", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "$,", "PWAV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir schildern es in T\u00f6nen:", "tokens": ["Wir", "schil\u00b7dern", "es", "in", "T\u00f6\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Fibel und das Einmaleins,", "tokens": ["Die", "Fi\u00b7bel", "und", "das", "Ein\u00b7mal\u00b7eins", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum Henker mit dem Sch\u00f6nen!", "tokens": ["Zum", "Hen\u00b7ker", "mit", "dem", "Sch\u00f6\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Nehmt noch das Feldgeschrei zuletzt", "tokens": ["Nehmt", "noch", "das", "Feld\u00b7ge\u00b7schrei", "zu\u00b7letzt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von Macbeths Zauberschwestern.", "tokens": ["Von", "Mac\u00b7beths", "Zau\u00b7ber\u00b7schwes\u00b7tern", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Das foul is fair hei\u00dft \u00fcbersetzt:", "tokens": ["Das", "foul", "is", "fair", "hei\u00dft", "\u00fc\u00b7bers\u00b7etzt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "FM", "FM", "FM", "VVFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Lobhudeln \u2013 und verl\u00e4stern.", "tokens": ["Lob\u00b7hu\u00b7deln", "\u2013", "und", "ver\u00b7l\u00e4s\u00b7tern", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}