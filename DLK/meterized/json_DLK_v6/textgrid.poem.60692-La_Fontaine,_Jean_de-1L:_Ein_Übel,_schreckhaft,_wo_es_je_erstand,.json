{"textgrid.poem.60692": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein \u00dcbel, schreckhaft, wo es je erstand,", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein \u00dcbel, schreckhaft, wo es je erstand,", "tokens": ["Ein", "\u00dc\u00b7bel", ",", "schreck\u00b7haft", ",", "wo", "es", "je", "er\u00b7stand", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADJD", "$,", "PWAV", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein \u00dcbel, das des Himmels Zorn erfand,", "tokens": ["Ein", "\u00dc\u00b7bel", ",", "das", "des", "Him\u00b7mels", "Zorn", "er\u00b7fand", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Der Erde \u00dcbeltun zu r\u00e4chen,", "tokens": ["Der", "Er\u00b7de", "\u00dc\u00b7bel\u00b7tun", "zu", "r\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Pest (kaum wag ich's auszusprechen),", "tokens": ["Die", "Pest", "(", "kaum", "wag", "ich's", "aus\u00b7zu\u00b7spre\u00b7chen", ")", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "$(", "ADV", "VVFIN", "PIS", "VVIZU", "$(", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Sie, die den Acheron so schnell bereichern kann,", "tokens": ["Sie", ",", "die", "den", "A\u00b7che\u00b7ron", "so", "schnell", "be\u00b7rei\u00b7chern", "kann", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "ART", "NN", "ADV", "ADJD", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Fiel kriegerisch die Tiere an.", "tokens": ["Fiel", "krie\u00b7ge\u00b7risch", "die", "Tie\u00b7re", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Nicht alle starben, aber alle wurden krank:", "tokens": ["Nicht", "al\u00b7le", "star\u00b7ben", ",", "a\u00b7ber", "al\u00b7le", "wur\u00b7den", "krank", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIS", "VVFIN", "$,", "ADV", "PIS", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Nicht einer, der noch sorgte, da\u00df er a\u00df und trank.", "tokens": ["Nicht", "ei\u00b7ner", ",", "der", "noch", "sorg\u00b7te", ",", "da\u00df", "er", "a\u00df", "und", "trank", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "$,", "PRELS", "ADV", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kein Mahl erregte ihre Gier,", "tokens": ["Kein", "Mahl", "er\u00b7reg\u00b7te", "ih\u00b7re", "Gier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Nicht Wolf noch Fuchs umsp\u00e4hten mehr", "tokens": ["Nicht", "Wolf", "noch", "Fuchs", "um\u00b7sp\u00e4h\u00b7ten", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PTKNEG", "NE", "ADV", "NE", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Das unschuldvolle Beutetier.", "tokens": ["Das", "un\u00b7schuld\u00b7vol\u00b7le", "Beu\u00b7te\u00b7tier", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Die Turteltauben flogen unstet hin und her,", "tokens": ["Die", "Tur\u00b7tel\u00b7tau\u00b7ben", "flo\u00b7gen", "un\u00b7stet", "hin", "und", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "KON", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Sie suchten keine Liebe, keine Freude mehr.", "tokens": ["Sie", "such\u00b7ten", "kei\u00b7ne", "Lie\u00b7be", ",", "kei\u00b7ne", "Freu\u00b7de", "mehr", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "PIAT", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Der Leu hielt Rat und sprach: \u00bbIch glaub es zu erfassen:", "tokens": ["Der", "Leu", "hielt", "Rat", "und", "sprach", ":", "\u00bb", "Ich", "glaub", "es", "zu", "er\u00b7fas\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "KON", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ob unsrer S\u00fcnden gro\u00df und schwer", "tokens": ["Ob", "uns\u00b7rer", "S\u00fcn\u00b7den", "gro\u00df", "und", "schwer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat diese Pein der Himmel zugelassen.", "tokens": ["Hat", "die\u00b7se", "Pein", "der", "Him\u00b7mel", "zu\u00b7ge\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Drum s\u00e4ume unser gr\u00f6\u00dfter S\u00fcnder nicht,", "tokens": ["Drum", "s\u00e4u\u00b7me", "un\u00b7ser", "gr\u00f6\u00df\u00b7ter", "S\u00fcn\u00b7der", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPOSAT", "ADJA", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Dem Zorn des Himmels sich als S\u00fchne anzutragen.", "tokens": ["Dem", "Zorn", "des", "Him\u00b7mels", "sich", "als", "S\u00fch\u00b7ne", "an\u00b7zu\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "PRF", "KOUS", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Vielleicht da\u00df dies die H\u00e4rte unsrer Strafe bricht.", "tokens": ["Viel\u00b7leicht", "da\u00df", "dies", "die", "H\u00e4r\u00b7te", "uns\u00b7rer", "Stra\u00b7fe", "bricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PDS", "ART", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ihr wi\u00dft ja, da\u00df man oft in \u00e4hnlich schlimmen Lagen", "tokens": ["Ihr", "wi\u00dft", "ja", ",", "da\u00df", "man", "oft", "in", "\u00e4hn\u00b7lich", "schlim\u00b7men", "La\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "$,", "KOUS", "PIS", "ADV", "APPR", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Solche Ergebenheitsbeweise brachte.", "tokens": ["Sol\u00b7che", "Er\u00b7ge\u00b7ben\u00b7heits\u00b7be\u00b7wei\u00b7se", "brach\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.9": {"text": "Bekennt euch also. Jeder hier betrachte", "tokens": ["Be\u00b7kennt", "euch", "al\u00b7so", ".", "Je\u00b7der", "hier", "be\u00b7trach\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$.", "PIS", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Ganz ohne Nachsicht sein Gewissen.", "tokens": ["Ganz", "oh\u00b7ne", "Nach\u00b7sicht", "sein", "Ge\u00b7wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Was mich betrifft: fiel mich der Hunger an,", "tokens": ["Was", "mich", "be\u00b7tr\u00b7ifft", ":", "fiel", "mich", "der", "Hun\u00b7ger", "an", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$.", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "So hab ich oft ein Schaf zerrissen,", "tokens": ["So", "hab", "ich", "oft", "ein", "Schaf", "zer\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Das nicht das kleinste mir zuleid getan;", "tokens": ["Das", "nicht", "das", "kleins\u00b7te", "mir", "zu\u00b7leid", "ge\u00b7tan", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PTKNEG", "ART", "ADJA", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Und hie und da geschah's,", "tokens": ["Und", "hie", "und", "da", "ge\u00b7scha\u00b7h's", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KON", "ADV", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.15": {"text": "Da\u00df ich sogar den Hirten a\u00df.", "tokens": ["Da\u00df", "ich", "so\u00b7gar", "den", "Hir\u00b7ten", "a\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Doch scheint mir\u2019s richtig, da\u00df erst jedermann ermi\u00dft,", "tokens": ["Doch", "scheint", "mir's", "rich\u00b7tig", ",", "da\u00df", "erst", "je\u00b7der\u00b7mann", "er\u00b7mi\u00dft", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "ADJD", "$,", "KOUS", "ADV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Ob nicht noch schwerer wiegt sein eignes S\u00fcndenma\u00df,", "tokens": ["Ob", "nicht", "noch", "schwe\u00b7rer", "wiegt", "sein", "eig\u00b7nes", "S\u00fcn\u00b7den\u00b7ma\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKNEG", "ADV", "ADJD", "VVFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Da es gerecht ist, da\u00df der gr\u00f6\u00dfte S\u00fcnder stirbt.\u00ab", "tokens": ["Da", "es", "ge\u00b7recht", "ist", ",", "da\u00df", "der", "gr\u00f6\u00df\u00b7te", "S\u00fcn\u00b7der", "stirbt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$,", "KOUS", "ART", "ADJA", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "\u00bboh, unser guter K\u00f6nig,\u00ab rief der Fuchs darauf,", "tokens": ["\u00bb", "oh", ",", "un\u00b7ser", "gu\u00b7ter", "K\u00f6\u00b7nig", ",", "\u00ab", "rief", "der", "Fuchs", "da\u00b7rauf", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "$,", "PPOSAT", "ADJA", "NN", "$,", "$(", "VVFIN", "ART", "NE", "PAV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "\u00bbder hier wie immer unser aller Lob erwirbt!", "tokens": ["\u00bb", "der", "hier", "wie", "im\u00b7mer", "un\u00b7ser", "al\u00b7ler", "Lob", "er\u00b7wirbt", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "ADV", "KOKOM", "ADV", "PPOSAT", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Doch, Herr, Ihr fra\u00dft nur Schafe, dummen P\u00f6bel auf \u2013", "tokens": ["Doch", ",", "Herr", ",", "Ihr", "fra\u00dft", "nur", "Scha\u00b7fe", ",", "dum\u00b7men", "P\u00f6\u00b7bel", "auf", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$,", "NN", "$,", "PPER", "VVFIN", "ADV", "NN", "$,", "ADJA", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Wie k\u00f6nnt Ihr denken, da\u00df dies eine S\u00fcnde w\u00e4re?", "tokens": ["Wie", "k\u00f6nnt", "Ihr", "den\u00b7ken", ",", "da\u00df", "dies", "ei\u00b7ne", "S\u00fcn\u00b7de", "w\u00e4\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "VVINF", "$,", "KOUS", "PDS", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Nein, nein! Indem Ihr sie zu Eurem Mahl ergrifft,", "tokens": ["Nein", ",", "nein", "!", "In\u00b7dem", "Ihr", "sie", "zu", "Eu\u00b7rem", "Mahl", "er\u00b7grifft", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PTKANT", "$.", "KOUS", "PPER", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Erwiest Ihr ihnen hohe Ehre.", "tokens": ["Er\u00b7wiest", "Ihr", "ih\u00b7nen", "ho\u00b7he", "Eh\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Und was den Hirten anbetrifft,", "tokens": ["Und", "was", "den", "Hir\u00b7ten", "an\u00b7be\u00b7tr\u00b7ifft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.26": {"text": "Der war gewi\u00df des Todes wert,", "tokens": ["Der", "war", "ge\u00b7wi\u00df", "des", "To\u00b7des", "wert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.27": {"text": "Da er zu jenen Leuten ja geh\u00f6rt,", "tokens": ["Da", "er", "zu", "je\u00b7nen", "Leu\u00b7ten", "ja", "ge\u00b7h\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PDAT", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.28": {"text": "Die sich in ihrem Hochmut oft so weit verga\u00dfen,", "tokens": ["Die", "sich", "in", "ih\u00b7rem", "Hoch\u00b7mut", "oft", "so", "weit", "ver\u00b7ga\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "PPOSAT", "NN", "ADV", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Sich \u00fcber uns, die Tiere, Rechte anzuma\u00dfen.\u00ab", "tokens": ["Sich", "\u00fc\u00b7ber", "uns", ",", "die", "Tie\u00b7re", ",", "Rech\u00b7te", "an\u00b7zu\u00b7ma\u00b7\u00dfen", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PRF", "APPR", "PPER", "$,", "ART", "NN", "$,", "NN", "VVIZU", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "So sprach der Fuchs. Die Schmeichler pflichteten ihm bei.", "tokens": ["So", "sprach", "der", "Fuchs", ".", "Die", "Schmeich\u00b7ler", "pflich\u00b7te\u00b7ten", "ihm", "bei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NE", "$.", "ART", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Man wagte auch dem Tiger, B\u00e4r und andern Gro\u00dfen", "tokens": ["Man", "wag\u00b7te", "auch", "dem", "Ti\u00b7ger", ",", "B\u00e4r", "und", "an\u00b7dern", "Gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "ART", "NN", "$,", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Nicht nachzuweisen, da\u00df ihr Rauben s\u00fcndhaft sei.", "tokens": ["Nicht", "nach\u00b7zu\u00b7wei\u00b7sen", ",", "da\u00df", "ihr", "Rau\u00b7ben", "s\u00fcnd\u00b7haft", "sei", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "$,", "KOUS", "PPOSAT", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Man scheute sich, die starken Frevler zu erbosen,", "tokens": ["Man", "scheu\u00b7te", "sich", ",", "die", "star\u00b7ken", "Frev\u00b7ler", "zu", "er\u00b7bo\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "$,", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Sie alle bis zum Hofhund hie\u00df man heilige Leute.", "tokens": ["Sie", "al\u00b7le", "bis", "zum", "Hof\u00b7hund", "hie\u00df", "man", "hei\u00b7li\u00b7ge", "Leu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "ADV", "APPRART", "NN", "VVFIN", "PIS", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.35": {"text": "Nun kam der Esel an die Reih:", "tokens": ["Nun", "kam", "der", "E\u00b7sel", "an", "die", "Reih", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.36": {"text": "\u00bbich kam an einer Klosterwiese einst vorbei.", "tokens": ["\u00bb", "ich", "kam", "an", "ei\u00b7ner", "Klos\u00b7ter\u00b7wie\u00b7se", "einst", "vor\u00b7bei", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "APPR", "ART", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "Die zarten Gr\u00e4ser, die Gelegenheit zur Beute,", "tokens": ["Die", "zar\u00b7ten", "Gr\u00e4\u00b7ser", ",", "die", "Ge\u00b7le\u00b7gen\u00b7heit", "zur", "Beu\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "Der Hunger und, ich glaube, irgendwelcher Teufel", "tokens": ["Der", "Hun\u00b7ger", "und", ",", "ich", "glau\u00b7be", ",", "ir\u00b7gend\u00b7wel\u00b7cher", "Teu\u00b7fel"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "KON", "$,", "PPER", "VVFIN", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Verf\u00fchrten mich, den gr\u00fcnen Rasen", "tokens": ["Ver\u00b7f\u00fchr\u00b7ten", "mich", ",", "den", "gr\u00fc\u00b7nen", "Ra\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.40": {"text": "Auf Zungenbreite abzugrasen.", "tokens": ["Auf", "Zun\u00b7gen\u00b7brei\u00b7te", "ab\u00b7zu\u00b7gra\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.41": {"text": "Da tat ich unrecht \u2013 ohne Zweifel.\u00ab", "tokens": ["Da", "tat", "ich", "un\u00b7recht", "\u2013", "oh\u00b7ne", "Zwei\u00b7fel", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$(", "APPR", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.42": {"text": "\u00bbo Schmach!\u00ab schrie man den Esel an.", "tokens": ["\u00bb", "o", "Schmach", "!", "\u00ab", "schrie", "man", "den", "E\u00b7sel", "an", "."], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "NN", "$.", "$(", "VVFIN", "PIS", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.43": {"text": "Und ein geri\u00dfner Wolf bewies mit vielen Phrasen,", "tokens": ["Und", "ein", "ge\u00b7ri\u00df\u00b7ner", "Wolf", "be\u00b7wies", "mit", "vie\u00b7len", "Phra\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NE", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Die Pest sei darum da, um diese Tat zu r\u00e4chen,", "tokens": ["Die", "Pest", "sei", "da\u00b7rum", "da", ",", "um", "die\u00b7se", "Tat", "zu", "r\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PAV", "PTKVZ", "$,", "KOUI", "PDAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Das r\u00e4udige Eselsvieh allein sei schuld daran.", "tokens": ["Das", "r\u00e4u\u00b7di\u00b7ge", "E\u00b7sels\u00b7vieh", "al\u00b7lein", "sei", "schuld", "da\u00b7ran", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VAFIN", "ADJD", "PAV", "$."], "meter": "-+-+-+---+--+", "measure": "iambic.penta.chol"}, "line.46": {"text": "Welch ein emp\u00f6rendes Verbrechen:", "tokens": ["Welch", "ein", "em\u00b7p\u00f6\u00b7ren\u00b7des", "Ver\u00b7bre\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.47": {"text": "Der andern Leute Gras zu fressen!", "tokens": ["Der", "an\u00b7dern", "Leu\u00b7te", "Gras", "zu", "fres\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.48": {"text": "Der Tod nur s\u00fchnte solche Tat des B\u00f6sewichts.", "tokens": ["Der", "Tod", "nur", "s\u00fchn\u00b7te", "sol\u00b7che", "Tat", "des", "B\u00f6\u00b7se\u00b7wichts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "PIAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Das zeigte man ihm gar geschwind.", "tokens": ["Das", "zeig\u00b7te", "man", "ihm", "gar", "ge\u00b7schwind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ganz je nachdem, wie m\u00e4chtig oder schwach wir sind,", "tokens": ["Ganz", "je", "nach\u00b7dem", ",", "wie", "m\u00e4ch\u00b7tig", "o\u00b7der", "schwach", "wir", "sind", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "KOUS", "$,", "PWAV", "ADJD", "KON", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Macht wei\u00df uns oder schwarz das Urteil des Gerichts.", "tokens": ["Macht", "wei\u00df", "uns", "o\u00b7der", "schwarz", "das", "Ur\u00b7teil", "des", "Ge\u00b7richts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "KON", "ADJD", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}}}}