{"textgrid.poem.46026": {"metadata": {"author": {"name": "Brentano, Clemens", "birth": "N.A.", "death": "N.A."}, "title": "[ich bin aus fremdem Land gekommen]", "genre": "verse", "period": "N.A.", "pub_year": 1818, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich bin aus fremdem Land gekommen", "tokens": ["Ich", "bin", "aus", "frem\u00b7dem", "Land", "ge\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein fremder, armer, kranker Mann", "tokens": ["Ein", "frem\u00b7der", ",", "ar\u00b7mer", ",", "kran\u00b7ker", "Mann"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "$,", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Du hast mich liebvoll aufgenommen", "tokens": ["Du", "hast", "mich", "lieb\u00b7voll", "auf\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie Jesus es und Jesu Freundin kann.", "tokens": ["Wie", "Je\u00b7sus", "es", "und", "Je\u00b7su", "Freun\u00b7din", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "PPER", "KON", "NE", "NN", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Was du gehabt, hast du geteilet,", "tokens": ["Was", "du", "ge\u00b7habt", ",", "hast", "du", "ge\u00b7tei\u00b7let", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAPP", "$,", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dein Brot, jed Wort aus Gottes Mund,", "tokens": ["Dein", "Brot", ",", "jed", "Wort", "aus", "Got\u00b7tes", "Mund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PIAT", "NN", "APPR", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Du hast geliebet und geheilet,", "tokens": ["Du", "hast", "ge\u00b7lie\u00b7bet", "und", "ge\u00b7hei\u00b7let", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hast geschlossen mir den neuen Bund.", "tokens": ["Und", "hast", "ge\u00b7schlos\u00b7sen", "mir", "den", "neu\u00b7en", "Bund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVPP", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Du l\u00e4\u00dft mich fremden Mann nicht scheiden", "tokens": ["Du", "l\u00e4\u00dft", "mich", "frem\u00b7den", "Mann", "nicht", "schei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADJA", "NN", "PTKNEG", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Du hast ihm auch den Weg gezeigt,", "tokens": ["Du", "hast", "ihm", "auch", "den", "Weg", "ge\u00b7zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den Weg der \u00fcber Lieb' und Leiden", "tokens": ["Den", "Weg", "der", "\u00fc\u00b7ber", "Lieb'", "und", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zum Kreuz, und bis zur Siegeskrone steigt.", "tokens": ["Zum", "Kreuz", ",", "und", "bis", "zur", "Sie\u00b7ges\u00b7kro\u00b7ne", "steigt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "KON", "APPR", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Ich durft' dir all mein Heimweh klagen,", "tokens": ["Ich", "durft'", "dir", "all", "mein", "Heim\u00b7weh", "kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PIAT", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und was mich in der Fremde h\u00e4lt,", "tokens": ["Und", "was", "mich", "in", "der", "Frem\u00b7de", "h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Du halfst die Last mir hinzutragen,", "tokens": ["Du", "halfst", "die", "Last", "mir", "hin\u00b7zu\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Zum Lamme, das da tr\u00e4gt die Schuld der Welt.", "tokens": ["Zum", "Lam\u00b7me", ",", "das", "da", "tr\u00e4gt", "die", "Schuld", "der", "Welt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PRELS", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Und da\u00df ich nicht besch\u00e4met werde,", "tokens": ["Und", "da\u00df", "ich", "nicht", "be\u00b7sch\u00e4\u00b7met", "wer\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hast du auch deine Last bekannt,", "tokens": ["Hast", "du", "auch", "dei\u00b7ne", "Last", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sa\u00dft bei mir an der dunklen Erde", "tokens": ["Sa\u00dft", "bei", "mir", "an", "der", "dunk\u00b7len", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Von der der liebe Heiland auferstand.", "tokens": ["Von", "der", "der", "lie\u00b7be", "Hei\u00b7land", "auf\u00b7er\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Wir haben uns wohl weinen sehen,", "tokens": ["Wir", "ha\u00b7ben", "uns", "wohl", "wei\u00b7nen", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und haben uns auch angelacht,", "tokens": ["Und", "ha\u00b7ben", "uns", "auch", "an\u00b7ge\u00b7lacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wollen still den Kreuzweg gehen", "tokens": ["Und", "wol\u00b7len", "still", "den", "Kreuz\u00b7weg", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADJD", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bis wir einst sagen, Herr es ist vollbracht.", "tokens": ["Bis", "wir", "einst", "sa\u00b7gen", ",", "Herr", "es", "ist", "voll\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVINF", "$,", "NN", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Du wie du liebend mich gef\u00fchret,", "tokens": ["Du", "wie", "du", "lie\u00b7bend", "mich", "ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PWAV", "PPER", "ADJD", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sprachst du gar ein freundlich Wort", "tokens": ["Da", "sprachst", "du", "gar", "ein", "freund\u00b7lich", "Wort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das hat mich durch und durch ger\u00fchret,", "tokens": ["Das", "hat", "mich", "durch", "und", "durch", "ge\u00b7r\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PRF", "APPR", "KON", "APPR", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und soll mich r\u00fchren immer fort und fort.", "tokens": ["Und", "soll", "mich", "r\u00fch\u00b7ren", "im\u00b7mer", "fort", "und", "fort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "VVFIN", "ADV", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Du sprachst, da sind wir ja vereinet", "tokens": ["Du", "sprachst", ",", "da", "sind", "wir", "ja", "ver\u00b7ei\u00b7net"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "ADV", "VAFIN", "PPER", "ADV", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich, du und sie, ich kenn' sie gut,", "tokens": ["Ich", ",", "du", "und", "sie", ",", "ich", "kenn'", "sie", "gut", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PPER", "KON", "PPER", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich wei\u00df, wie innig sie es meinet", "tokens": ["Ich", "wei\u00df", ",", "wie", "in\u00b7nig", "sie", "es", "mei\u00b7net"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "ADJD", "PPER", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wie sie glaubend hofft auf Jesu Blut.", "tokens": ["Und", "wie", "sie", "glau\u00b7bend", "hofft", "auf", "Je\u00b7su", "Blut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADJD", "VVFIN", "APPR", "NE", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Und auch die Tr\u00f6sterin der S\u00fcnder,", "tokens": ["Und", "auch", "die", "Tr\u00f6s\u00b7te\u00b7rin", "der", "S\u00fcn\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Mutter, die das Kindlein trug", "tokens": ["Die", "Mut\u00b7ter", ",", "die", "das", "Kin\u00b7dlein", "trug"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das zu uns sprach, seid wie die Kinder,", "tokens": ["Das", "zu", "uns", "sprach", ",", "seid", "wie", "die", "Kin\u00b7der", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "PPER", "VVFIN", "$,", "VAFIN", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "War da zur Seligkeit uns nah genug.", "tokens": ["War", "da", "zur", "Se\u00b7lig\u00b7keit", "uns", "nah", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "PPER", "ADJD", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Zusammen sind wir auch gegangen", "tokens": ["Zu\u00b7sam\u00b7men", "sind", "wir", "auch", "ge\u00b7gan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vereinet zu Sankt Klemens' Grab \u2013", "tokens": ["Ver\u00b7ei\u00b7net", "zu", "Sankt", "Kle\u00b7mens'", "Grab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "PTKZU", "VVFIN", "NE", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der deinem liebenden Verlangen", "tokens": ["Der", "dei\u00b7nem", "lie\u00b7ben\u00b7den", "Ver\u00b7lan\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr mich ein heil'ges liebes Kleinod gab.", "tokens": ["F\u00fcr", "mich", "ein", "heil'\u00b7ges", "lie\u00b7bes", "Klei\u00b7nod", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Auch an den \u00d6lberg durft' ich gehen", "tokens": ["Auch", "an", "den", "\u00d6l\u00b7berg", "durft'", "ich", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "VMFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit dir in seines Sohns Person.", "tokens": ["Mit", "dir", "in", "sei\u00b7nes", "Sohns", "Per\u00b7son", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "PPOSAT", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zum lieben Vater aufzuflehen,", "tokens": ["Zum", "lie\u00b7ben", "Va\u00b7ter", "auf\u00b7zu\u00b7fle\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der nichts versagt dem eingebornen Sohn.", "tokens": ["Der", "nichts", "ver\u00b7sagt", "dem", "ein\u00b7ge\u00b7bor\u00b7nen", "Sohn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Und zu der Kerker Jammerh\u00f6hlen,", "tokens": ["Und", "zu", "der", "Ker\u00b7ker", "Jam\u00b7mer\u00b7h\u00f6h\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat deine Liebe mich gef\u00fchrt,", "tokens": ["Hat", "dei\u00b7ne", "Lie\u00b7be", "mich", "ge\u00b7f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch dich hat mich der armen Seelen", "tokens": ["Durch", "dich", "hat", "mich", "der", "ar\u00b7men", "See\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "VAFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Betr\u00fcbter h\u00fclfsbed\u00fcrft'ger Stand ger\u00fchrt.", "tokens": ["Be\u00b7tr\u00fcb\u00b7ter", "h\u00fclfs\u00b7be\u00b7d\u00fcrft'\u00b7ger", "Stand", "ge\u00b7r\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "Und hin zum heil'gen Kirchenleibe", "tokens": ["Und", "hin", "zum", "heil'\u00b7gen", "Kir\u00b7chen\u00b7lei\u00b7be"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hin zu der Heil'gen Freudenchor,", "tokens": ["Hin", "zu", "der", "Heil'\u00b7gen", "Freu\u00b7denc\u00b7hor", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Hobst du, da\u00df sie einst Bl\u00fcten treibe", "tokens": ["Hobst", "du", ",", "da\u00df", "sie", "einst", "Bl\u00fc\u00b7ten", "trei\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPER", "ADV", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Des armen S\u00fcnders d\u00fcrre Hand empor.", "tokens": ["Des", "ar\u00b7men", "S\u00fcn\u00b7ders", "d\u00fcr\u00b7re", "Hand", "em\u00b7por", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Auch durch die W\u00fcste durft' ich ziehen,", "tokens": ["Auch", "durch", "die", "W\u00fcs\u00b7te", "durft'", "ich", "zie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Durft' schreien nach ersehnter Frucht,", "tokens": ["Durft'", "schrei\u00b7en", "nach", "er\u00b7sehn\u00b7ter", "Frucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wo wir die Schwester sahen fliehen,", "tokens": ["Wo", "wir", "die", "Schwes\u00b7ter", "sa\u00b7hen", "flie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die wir bis jetzt vergebens aufgesucht.", "tokens": ["Die", "wir", "bis", "jetzt", "ver\u00b7ge\u00b7bens", "auf\u00b7ge\u00b7sucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.15": {"line.1": {"text": "Was haben alles wir gesehen,", "tokens": ["Was", "ha\u00b7ben", "al\u00b7les", "wir", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PIS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was haben alles wir geliebt,", "tokens": ["Was", "ha\u00b7ben", "al\u00b7les", "wir", "ge\u00b7liebt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PIS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und m\u00fcssen auf der Erde stehen", "tokens": ["Und", "m\u00fcs\u00b7sen", "auf", "der", "Er\u00b7de", "ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Dorn und Blumen auf die Gr\u00e4ber giebt.", "tokens": ["Die", "Dorn", "und", "Blu\u00b7men", "auf", "die", "Gr\u00e4\u00b7ber", "giebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.16": {"line.1": {"text": "Doch wollen wir die Dornen w\u00e4hlen,", "tokens": ["Doch", "wol\u00b7len", "wir", "die", "Dor\u00b7nen", "w\u00e4h\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Dornen, die der Heiland trug,", "tokens": ["Die", "Dor\u00b7nen", ",", "die", "der", "Hei\u00b7land", "trug", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wollen nicht die Tr\u00e4nen z\u00e4hlen,", "tokens": ["Und", "wol\u00b7len", "nicht", "die", "Tr\u00e4\u00b7nen", "z\u00e4h\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PTKNEG", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Um unsre Schuld sind deren nie genug.", "tokens": ["Um", "uns\u00b7re", "Schuld", "sind", "de\u00b7ren", "nie", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "NN", "VAFIN", "PDS", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.17": {"line.1": {"text": "Und nie genug um seine Leiden,", "tokens": ["Und", "nie", "ge\u00b7nug", "um", "sei\u00b7ne", "Lei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und nie genug um unsre Schuld,", "tokens": ["Und", "nie", "ge\u00b7nug", "um", "uns\u00b7re", "Schuld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wenn wir von einander scheiden,", "tokens": ["Und", "wenn", "wir", "von", "ein\u00b7an\u00b7der", "schei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So gebe Jesus mir die g\u00f6ttliche Geduld.", "tokens": ["So", "ge\u00b7be", "Je\u00b7sus", "mir", "die", "g\u00f6tt\u00b7li\u00b7che", "Ge\u00b7duld", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Geduld die heute wir verehren", "tokens": ["Ge\u00b7duld", "die", "heu\u00b7te", "wir", "ver\u00b7eh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ART", "ADV", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In dir du heil'ge Martyrin!", "tokens": ["In", "dir", "du", "heil'\u00b7ge", "Mar\u00b7ty\u00b7rin", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sankt Katharina, wir begehren", "tokens": ["Sankt", "Ka\u00b7tha\u00b7ri\u00b7na", ",", "wir", "be\u00b7geh\u00b7ren"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VVFIN", "NE", "$,", "PPER", "VVINF"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "F\u00fchr' uns zu deinem, unserm Heiland hin.", "tokens": ["F\u00fchr'", "uns", "zu", "dei\u00b7nem", ",", "un\u00b7serm", "Hei\u00b7land", "hin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPR", "PPOSAT", "$,", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}