{"textgrid.poem.52785": {"metadata": {"author": {"name": "Eichrodt, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Mit Scheffelma\u00df", "genre": "verse", "period": "N.A.", "pub_year": 1859, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Einst scho\u00df ich im siebenten Himmel", "tokens": ["Einst", "scho\u00df", "ich", "im", "sie\u00b7ben\u00b7ten", "Him\u00b7mel"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Krampfhaft als ", "tokens": ["Krampf\u00b7haft", "als"], "token_info": ["word", "word"], "pos": ["NN", "KOKOM"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Aus dem Engeren in das Weitre,", "tokens": ["Aus", "dem", "En\u00b7ge\u00b7ren", "in", "das", "Weit\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und hatte glanzvollen Humor.", "tokens": ["Und", "hat\u00b7te", "glanz\u00b7vol\u00b7len", "Hu\u00b7mor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Ein chaotischer Erdbahnschnupfen", "tokens": ["Ein", "chao\u00b7ti\u00b7scher", "Erd\u00b7bahn\u00b7schnup\u00b7fen"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nieste mich feurig herab,", "tokens": ["Nies\u00b7te", "mich", "feu\u00b7rig", "her\u00b7ab", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ADV", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Ich kam bedeutend herunter,", "tokens": ["Ich", "kam", "be\u00b7deu\u00b7tend", "her\u00b7un\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wie des Rodensteiners Knapp.", "tokens": ["Wie", "des", "Ro\u00b7dens\u00b7tei\u00b7ners", "Knapp", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Nun, gefallen aus allen Himmeln,", "tokens": ["Nun", ",", "ge\u00b7fal\u00b7len", "aus", "al\u00b7len", "Him\u00b7meln", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VVPP", "APPR", "PIAT", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Tr\u00f6stet mich Selbsteinkehr:", "tokens": ["Tr\u00f6s\u00b7tet", "mich", "Selbs\u00b7tein\u00b7kehr", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Als ich h\u00f6heren Schwindel getrieben,", "tokens": ["Als", "ich", "h\u00f6\u00b7he\u00b7ren", "Schwin\u00b7del", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "War ich dochschon nur h\u00f6chstephemer.", "tokens": ["War", "ich", "doch\u00b7schon", "nur", "h\u00f6chs\u00b7te\u00b7phe\u00b7mer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Die chlorberauscht\u00fcberm\u00fcth'ge", "tokens": ["Die", "chlor\u00b7be\u00b7rauscht\u00b7\u00fcber\u00b7m\u00fcth'\u00b7ge"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Menschheit hatte l\u00e4ngst mich durchschaut,", "tokens": ["Menschheit", "hat\u00b7te", "l\u00e4ngst", "mich", "durch\u00b7schaut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Das Mannweib Spektralanalyse", "tokens": ["Das", "Mann\u00b7weib", "Spekt\u00b7ra\u00b7la\u00b7na\u00b7ly\u00b7se"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Entri\u00df mir die L\u00f6wenhaut.", "tokens": ["Ent\u00b7ri\u00df", "mir", "die", "L\u00f6\u00b7wen\u00b7haut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Obschon ich in Wahrheit weit ", "tokens": ["Ob\u00b7schon", "ich", "in", "Wahr\u00b7heit", "weit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist doch nichts Besondres an mir,", "tokens": ["Ist", "doch", "nichts", "Be\u00b7sond\u00b7res", "an", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "PIS", "APPR", "PPER", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich bin erkannt \u2013 als Nickel,", "tokens": ["Ich", "bin", "er\u00b7kannt", "\u2013", "als", "Ni\u00b7ckel", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$(", "KOUS", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und war's, schon im Zeichen des Stier.", "tokens": ["Und", "wa\u00b7r's", ",", "schon", "im", "Zei\u00b7chen", "des", "Stier", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "$,", "ADV", "APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.6": {"line.1": {"text": "Wegen sehr \u00bbzwecklosen Umherziehn's\u00ab", "tokens": ["We\u00b7gen", "sehr", "\u00bb", "zweck\u00b7lo\u00b7sen", "Um\u00b7her\u00b7ziehn's", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADV", "$(", "ADJA", "NN", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Vielleicht gar \u00bbunwissend wo\u00ab", "tokens": ["Viel\u00b7leicht", "gar", "\u00bb", "un\u00b7wis\u00b7send", "wo", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "$(", "KON", "PWAV", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Kam ich auf den Schub, so spr\u00e4che", "tokens": ["Kam", "ich", "auf", "den", "Schub", ",", "so", "spr\u00e4\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["NE", "PPER", "APPR", "ART", "NN", "$,", "ADV", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Gern die Wissenschaft im B\u00fcreau.", "tokens": ["Gern", "die", "Wis\u00b7sen\u00b7schaft", "im", "B\u00fc\u00b7re\u00b7au", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPRART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "Doch, ein ", "tokens": ["Doch", ",", "ein"], "token_info": ["word", "punct", "word"], "pos": ["KON", "$,", "ART"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Dem Anaxagoras, bum!", "tokens": ["Dem", "A\u00b7na\u00b7xa\u00b7go\u00b7ras", ",", "bum", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "XY", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Das freut mich noch heut; es machte", "tokens": ["Das", "freut", "mich", "noch", "heut", ";", "es", "mach\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "$.", "PPER", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Da\u00df die Philosophie kehrte um.", "tokens": ["Da\u00df", "die", "Phi\u00b7lo\u00b7so\u00b7phie", "kehr\u00b7te", "um", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "+-++-++-+", "measure": "unknown.measure.hexa"}}, "stanza.8": {"line.1": {"text": "Zwar es platzten Meinesgleichen", "tokens": ["Zwar", "es", "platz\u00b7ten", "Mei\u00b7nes\u00b7glei\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PPER", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nicht blos zur Kulturwelt herein,", "tokens": ["Nicht", "blos", "zur", "Kul\u00b7tur\u00b7welt", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nicht ganz hoffnungslos \u2013 ein Stein.", "tokens": ["Nicht", "ganz", "hoff\u00b7nungs\u00b7los", "\u2013", "ein", "Stein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ADJD", "$(", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Sie begriff, was mir irdischen Werth lieh,", "tokens": ["Sie", "be\u00b7griff", ",", "was", "mir", "ir\u00b7di\u00b7schen", "Werth", "lieh", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Zumal meine seltsamen hex-", "tokens": ["Zu\u00b7mal", "mei\u00b7ne", "selt\u00b7sa\u00b7men", "he\u00b7x"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "TRUNC"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "agonalen Konkavit\u00e4ten,", "tokens": ["a\u00b7go\u00b7na\u00b7len", "Kon\u00b7ka\u00b7vi\u00b7t\u00e4\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Davon die Kehrwand konvex.", "tokens": ["Da\u00b7von", "die", "Kehr\u00b7wand", "kon\u00b7vex", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "VMFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Und neueste Forschung erquickt mich:", "tokens": ["Und", "neu\u00b7es\u00b7te", "For\u00b7schung", "er\u00b7quickt", "mich", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVFIN", "PPER", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "War Weltk\u00f6rpertrumm, ", "tokens": ["War", "Welt\u00b7k\u00f6r\u00b7per\u00b7trumm", ","], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Planetarische Stoffanh\u00e4ufung \u2013", "tokens": ["Pla\u00b7ne\u00b7ta\u00b7ri\u00b7sche", "Stoff\u00b7an\u00b7h\u00e4u\u00b7fung", "\u2013"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$("], "meter": "+----+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Kein bloses Mondexkret.", "tokens": ["Kein", "blo\u00b7ses", "Mon\u00b7dex\u00b7kret", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Ich fiel \u2013 zwischen G\u00f6rz und Gradiska", "tokens": ["Ich", "fiel", "\u2013", "zwi\u00b7schen", "G\u00f6rz", "und", "Gra\u00b7dis\u00b7ka"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "APPR", "NE", "KON", "NE"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In's sediment\u00e4re Friaul.", "tokens": ["In's", "se\u00b7di\u00b7men\u00b7t\u00e4\u00b7re", "Fri\u00b7aul", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Zu Venedig im rothen Ochsen", "tokens": ["Zu", "Ve\u00b7ne\u00b7dig", "im", "ro\u00b7then", "Och\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}