{"textgrid.poem.42987": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Eine Zuschauerin im Flughafen", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbnie wieder wird's Menschen geben,", "tokens": ["\u00bb", "nie", "wie\u00b7der", "wird's", "Men\u00b7schen", "ge\u00b7ben", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADV", "VAFIN", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die so viel erleben,", "tokens": ["Die", "so", "viel", "er\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Wie wir, in unsrer gigantischen Zeit!", "tokens": ["Wie", "wir", ",", "in", "uns\u00b7rer", "gi\u00b7gan\u00b7ti\u00b7schen", "Zeit", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "$,", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Weltkrieg und die ihm folgenden Leiden \u2013", "tokens": ["Der", "Welt\u00b7krieg", "und", "die", "ihm", "fol\u00b7gen\u00b7den", "Lei\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "PPER", "ADJA", "NN", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Wird keiner auch uns darum beneiden \u2013", "tokens": ["Wird", "kei\u00b7ner", "auch", "uns", "da\u00b7rum", "be\u00b7nei\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "PPER", "PAV", "VVINF", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Haben doch alles, was in der Welt", "tokens": ["Ha\u00b7ben", "doch", "al\u00b7les", ",", "was", "in", "der", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "PIS", "$,", "PRELS", "APPR", "ART", "NN"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.7": {"text": "Fr\u00fcher geschah, in den Schatten gestellt.", "tokens": ["Fr\u00fc\u00b7her", "ge\u00b7schah", ",", "in", "den", "Schat\u00b7ten", "ge\u00b7stellt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.8": {"text": "O unsre Zeit! Und speziell unser Land!\u00ab", "tokens": ["O", "uns\u00b7re", "Zeit", "!", "Und", "spe\u00b7zi\u00b7ell", "un\u00b7ser", "Land", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "PPOSAT", "NN", "$.", "KON", "ADJD", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.2": {"line.1": {"text": "Der Platzleiter b\u00fcckte sich, hob galant", "tokens": ["Der", "Platz\u00b7lei\u00b7ter", "b\u00fcck\u00b7te", "sich", ",", "hob", "ga\u00b7lant"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "$,", "VVFIN", "NE"], "meter": "-++-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ein Buch auf, gab's mit der linken Hand", "tokens": ["Ein", "Buch", "auf", ",", "gab's", "mit", "der", "lin\u00b7ken", "Hand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKVZ", "$,", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Der Dame zur\u00fcck, nicht mit der rechten.", "tokens": ["Der", "Da\u00b7me", "zu\u00b7r\u00fcck", ",", "nicht", "mit", "der", "rech\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKVZ", "$,", "PTKNEG", "APPR", "ART", "ADJA", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "(er war im Kriege in Luftgefechten", "tokens": ["(", "er", "war", "im", "Krie\u00b7ge", "in", "Luft\u00b7ge\u00b7fech\u00b7ten"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VAFIN", "APPRART", "NN", "APPR", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Dreimal abgeschossen und r\u00fchmlichst bekannt.)", "tokens": ["Drei\u00b7mal", "ab\u00b7ge\u00b7schos\u00b7sen", "und", "r\u00fchm\u00b7lichst", "be\u00b7kannt", ".", ")"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVPP", "KON", "VVFIN", "PTKVZ", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbdanke. \u2013 Ach, wie der Gedanke erhebt:", "tokens": ["\u00bb", "dan\u00b7ke", ".", "\u2013", "Ach", ",", "wie", "der", "Ge\u00b7dan\u00b7ke", "er\u00b7hebt", ":"], "token_info": ["punct", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$.", "$(", "ITJ", "$,", "PWAV", "ART", "NN", "VVFIN", "$."], "meter": "+-+---+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Nie wird \u2013 nie hat eine Generation", "tokens": ["Nie", "wird", "\u2013", "nie", "hat", "ei\u00b7ne", "Ge\u00b7ne\u00b7ra\u00b7ti\u00b7on"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "$(", "ADV", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "Soviel Erfindungen neu erlebt.", "tokens": ["So\u00b7viel", "Er\u00b7fin\u00b7dun\u00b7gen", "neu", "er\u00b7lebt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.4": {"text": "Denken Sie nur an Edison,", "tokens": ["Den\u00b7ken", "Sie", "nur", "an", "E\u00b7di\u00b7son", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "An Fahrrad, Auto und Grammophon,", "tokens": ["An", "Fahr\u00b7rad", ",", "Au\u00b7to", "und", "Gram\u00b7mo\u00b7phon", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "An Kino, Radio, R\u00f6ntgenstrahlen,", "tokens": ["An", "Ki\u00b7no", ",", "Ra\u00b7dio", ",", "R\u00f6nt\u00b7gen\u00b7strah\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "$,", "NE", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Schon Trambahn, Rohrpost und Salvarsan.", "tokens": ["Schon", "Tram\u00b7bahn", ",", "Rohr\u00b7post", "und", "Sal\u00b7var\u00b7san", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "KON", "NE", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "All das hat unsere Zeit getan!", "tokens": ["All", "das", "hat", "un\u00b7se\u00b7re", "Zeit", "ge\u00b7tan", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PDS", "VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.9": {"text": "Und was noch folgt, ist kaum auszumalen.", "tokens": ["Und", "was", "noch", "folgt", ",", "ist", "kaum", "aus\u00b7zu\u00b7ma\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "VVFIN", "$,", "VAFIN", "ADV", "VVIZU", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Wir schreiten weiter von Siegen zu Siegen.", "tokens": ["Wir", "schrei\u00b7ten", "wei\u00b7ter", "von", "Sie\u00b7gen", "zu", "Sie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Nicht Fortschritt mehr, sondern Fortflug. Wir fliegen", "tokens": ["Nicht", "Fort\u00b7schritt", "mehr", ",", "son\u00b7dern", "Fort\u00b7flug", ".", "Wir", "flie\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PTKNEG", "NN", "ADV", "$,", "KON", "NN", "$.", "PPER", "VVFIN"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Empor. Wir werden zu h\u00f6heren Fernen", "tokens": ["Em\u00b7por", ".", "Wir", "wer\u00b7den", "zu", "h\u00f6\u00b7he\u00b7ren", "Fer\u00b7nen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$.", "PPER", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Schweben, zum Mars und zu s\u00e4mtlichen Sternen.", "tokens": ["Schwe\u00b7ben", ",", "zum", "Mars", "und", "zu", "s\u00e4mt\u00b7li\u00b7chen", "Ster\u00b7nen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "APPRART", "NN", "KON", "APPR", "ADJA", "NN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.14": {"text": "Wir werden vielleicht", "tokens": ["Wir", "wer\u00b7den", "viel\u00b7leicht"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.15": {"text": "Die aller\u00e4u\u00dferste Peripherie", "tokens": ["Die", "al\u00b7ler\u00b7\u00e4u\u00b7\u00dfers\u00b7te", "Pe\u00b7ri\u00b7phe\u00b7rie"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Des Weltalls erreichen. \u2013 \u2013", "tokens": ["Des", "Welt\u00b7alls", "er\u00b7rei\u00b7chen", ".", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "$(", "$("], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.17": {"text": "Ich danke Ihnen, das haben Sie", "tokens": ["Ich", "dan\u00b7ke", "Ih\u00b7nen", ",", "das", "ha\u00b7ben", "Sie"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PDS", "VAFIN", "PPER"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.18": {"text": "Und Ihresgleichen", "tokens": ["Und", "Ih\u00b7res\u00b7glei\u00b7chen"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.19": {"text": "Durch Ihr Genie und durch Mut erreicht.\u00ab", "tokens": ["Durch", "Ihr", "Ge\u00b7nie", "und", "durch", "Mut", "er\u00b7reicht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "NN", "KON", "APPR", "NN", "VVPP", "$.", "$("], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.4": {"line.1": {"text": "Die Dame schwieg, und sie f\u00e4chelte", "tokens": ["Die", "Da\u00b7me", "schwieg", ",", "und", "sie", "f\u00e4\u00b7chel\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "KON", "PPER", "VVFIN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mit ihren Armen, als wollte sie fliegen.", "tokens": ["Mit", "ih\u00b7ren", "Ar\u00b7men", ",", "als", "woll\u00b7te", "sie", "flie\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KOUS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Der Flugplatzleiter l\u00e4chelte.", "tokens": ["Der", "Flug\u00b7platz\u00b7lei\u00b7ter", "l\u00e4\u00b7chel\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbbin oft nach der Sonne zu aufgestiegen\u00ab,", "tokens": ["\u00bb", "bin", "oft", "nach", "der", "Son\u00b7ne", "zu", "auf\u00b7ge\u00b7stie\u00b7gen", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "ADV", "APPR", "ART", "NN", "PTKZU", "VVINF", "$(", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "So sagte er heiter,", "tokens": ["So", "sag\u00b7te", "er", "hei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "\u00bbdoch zog sie sich immer um jedes St\u00fcck", "tokens": ["\u00bb", "doch", "zog", "sie", "sich", "im\u00b7mer", "um", "je\u00b7des", "St\u00fcck"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "VVFIN", "PPER", "PRF", "ADV", "APPR", "PIAT", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Meiner erstrebten Ann\u00e4herung weiter", "tokens": ["Mei\u00b7ner", "er\u00b7streb\u00b7ten", "An\u00b7n\u00e4\u00b7he\u00b7rung", "wei\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "ADV"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.6": {"text": "Und h\u00f6her zum alten Abstand zur\u00fcck.\u00ab", "tokens": ["Und", "h\u00f6\u00b7her", "zum", "al\u00b7ten", "Ab\u00b7stand", "zu\u00b7r\u00fcck", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJD", "APPRART", "ADJA", "NN", "PTKVZ", "$.", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}}}}