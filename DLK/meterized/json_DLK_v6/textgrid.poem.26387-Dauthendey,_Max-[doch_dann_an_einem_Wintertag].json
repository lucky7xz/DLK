{"textgrid.poem.26387": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[doch dann an einem Wintertag]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Doch dann an einem Wintertag", "tokens": ["Doch", "dann", "an", "ei\u00b7nem", "Win\u00b7ter\u00b7tag"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6nigin sprach: \u00bbIch nicht mehr mag.", "tokens": ["K\u00f6\u00b7ni\u00b7gin", "sprach", ":", "\u00bb", "Ich", "nicht", "mehr", "mag."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "abbreviation"], "pos": ["NN", "VVFIN", "$.", "$(", "PPER", "PTKNEG", "ADV", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Frau Holle wohnt jetzt wei\u00df im Land,", "tokens": ["Frau", "Hol\u00b7le", "wohnt", "jetzt", "wei\u00df", "im", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VVFIN", "ADV", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und sch\u00f6n ich stets ihr Bettuch fand.", "tokens": ["Und", "sch\u00f6n", "ich", "stets", "ihr", "Bet\u00b7tuch", "fand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PPER", "ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Will auf dem Land mir Ruhe holen,", "tokens": ["Will", "auf", "dem", "Land", "mir", "Ru\u00b7he", "ho\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "PPER", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Ruhe, die man mir gestohlen.", "tokens": ["Die", "Ru\u00b7he", ",", "die", "man", "mir", "ge\u00b7stoh\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Wir sind ja beide dick voll Sorgen,", "tokens": ["Wir", "sind", "ja", "bei\u00b7de", "dick", "voll", "Sor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "ADJD", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich reise noch an diesem Morgen.", "tokens": ["Ich", "rei\u00b7se", "noch", "an", "die\u00b7sem", "Mor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Drei Tag' geb' ich dir zu bedenken,", "tokens": ["Drei", "Tag'", "geb'", "ich", "dir", "zu", "be\u00b7den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "PPER", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich kann mich l\u00e4nger nicht verrenken.", "tokens": ["Ich", "kann", "mich", "l\u00e4n\u00b7ger", "nicht", "ver\u00b7ren\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADJD", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und soll ich dich nur halb stets haben,", "tokens": ["Und", "soll", "ich", "dich", "nur", "halb", "stets", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PRF", "ADV", "ADJD", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Werf' ich mich lieber vor die Raben.\u00ab", "tokens": ["Wer\u00b7f'", "ich", "mich", "lie\u00b7ber", "vor", "die", "Ra\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "PPER", "PRF", "ADV", "APPR", "ART", "NN", "$.", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Kaum ging K\u00f6nigin aus dem Haus,", "tokens": ["Kaum", "ging", "K\u00f6\u00b7ni\u00b7gin", "aus", "dem", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Da wollt' ich selber auch hinaus.", "tokens": ["Da", "wollt'", "ich", "sel\u00b7ber", "auch", "hin\u00b7aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Aus Angst vor dem Entscheidungsruck", "tokens": ["Aus", "Angst", "vor", "dem", "Ent\u00b7schei\u00b7dungs\u00b7ruck"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Trieb ich am hellen Tage Spuk.", "tokens": ["Trieb", "ich", "am", "hel\u00b7len", "Ta\u00b7ge", "Spuk", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "APPRART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Mir schien, an jedem Droschkenstand", "tokens": ["Mir", "schien", ",", "an", "je\u00b7dem", "Droschken\u00b7stand"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "APPR", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Gingen die G\u00e4ul' aus Rand und Band,", "tokens": ["Gin\u00b7gen", "die", "G\u00e4ul'", "aus", "Rand", "und", "Band", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "F\u00fchlte, wenn ich vor\u00fcberkam,", "tokens": ["F\u00fchl\u00b7te", ",", "wenn", "ich", "vor\u00b7\u00fc\u00b7ber\u00b7kam", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Selbst Droschkeng\u00e4ule sind dir gram.", "tokens": ["Selbst", "Droschken\u00b7g\u00e4u\u00b7le", "sind", "dir", "gram", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VAFIN", "PPER", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Der Gaul, der auch ein edles Pferd,", "tokens": ["Der", "Gaul", ",", "der", "auch", "ein", "ed\u00b7les", "Pferd", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denkt: Du bist keine Droschke wert.", "tokens": ["Denkt", ":", "Du", "bist", "kei\u00b7ne", "Droschke", "wert", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "PPER", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.12": {"line.1": {"text": "Wie man in alter Zeit schon frug,", "tokens": ["Wie", "man", "in", "al\u00b7ter", "Zeit", "schon", "frug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "APPR", "ADJA", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Befragte ich der V\u00f6gel Flug", "tokens": ["Be\u00b7frag\u00b7te", "ich", "der", "V\u00f6\u00b7gel", "Flug"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Und sah durch meine Fensterscheiben", "tokens": ["Und", "sah", "durch", "mei\u00b7ne", "Fens\u00b7ter\u00b7schei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "V\u00f6gel im Flug weissagend schreiben,", "tokens": ["V\u00f6\u00b7gel", "im", "Flug", "weis\u00b7sa\u00b7gend", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "ADJD", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.14": {"line.1": {"text": "Doch Tauben, Spatzen, Kirchenraben", "tokens": ["Doch", "Tau\u00b7ben", ",", "Spat\u00b7zen", ",", "Kir\u00b7chen\u00b7ra\u00b7ben"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["KON", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verschiedene Weissagung gaben.", "tokens": ["Ver\u00b7schie\u00b7de\u00b7ne", "Weis\u00b7sa\u00b7gung", "ga\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.15": {"line.1": {"text": "Wahrsagt so viel das Vogelreich,", "tokens": ["Wahr\u00b7sagt", "so", "viel", "das", "Vo\u00b7gel\u00b7reich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Meint man zuletzt, 's ist alles gleich.", "tokens": ["Meint", "man", "zu\u00b7letzt", ",", "'s", "ist", "al\u00b7les", "gleich", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "$,", "PPER", "VAFIN", "PIS", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.16": {"line.1": {"text": "Es dr\u00e4ngte mich hinaus aufs Land,", "tokens": ["Es", "dr\u00e4ng\u00b7te", "mich", "hin\u00b7aus", "aufs", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APZR", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dort, wo mein Puppenh\u00e4uschen stand.", "tokens": ["Dort", ",", "wo", "mein", "Pup\u00b7pen\u00b7h\u00e4usc\u00b7hen", "stand", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Dort tat ich durch die R\u00e4ume steigen,", "tokens": ["Dort", "tat", "ich", "durch", "die", "R\u00e4u\u00b7me", "stei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Einsamkeit war auch hier mein eigen.", "tokens": ["Ein\u00b7sam\u00b7keit", "war", "auch", "hier", "mein", "ei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "ADV", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.18": {"line.1": {"text": "In Stuhl und Bett fehlt was hinein,", "tokens": ["In", "Stuhl", "und", "Bett", "fehlt", "was", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVFIN", "PIS", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Fehlende soll weiblich sein.", "tokens": ["Das", "Feh\u00b7len\u00b7de", "soll", "weib\u00b7lich", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Ich tat best\u00fcrzt die Augen senken,", "tokens": ["Ich", "tat", "be\u00b7st\u00fcrzt", "die", "Au\u00b7gen", "sen\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn man erschrickt auch bei dem Denken.", "tokens": ["Denn", "man", "er\u00b7schrickt", "auch", "bei", "dem", "Den\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Am n\u00e4chsten Tage kam ich wieder,", "tokens": ["Am", "n\u00e4chs\u00b7ten", "Ta\u00b7ge", "kam", "ich", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gezupfter Schnee flog wie Gefieder.", "tokens": ["Ge\u00b7zupf\u00b7ter", "Schnee", "flog", "wie", "Ge\u00b7fie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Ich trat vor meine Ahnen hin,", "tokens": ["Ich", "trat", "vor", "mei\u00b7ne", "Ah\u00b7nen", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Trotzdem ich l\u00e4ngst vollj\u00e4hrig bin.", "tokens": ["Trotz\u00b7dem", "ich", "l\u00e4ngst", "voll\u00b7j\u00e4h\u00b7rig", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Sie sitzen an der Wand in Rahmen,", "tokens": ["Sie", "sit\u00b7zen", "an", "der", "Wand", "in", "Rah\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie Menschen, die schon h\u00f6her kamen.", "tokens": ["Wie", "Men\u00b7schen", ",", "die", "schon", "h\u00f6\u00b7her", "ka\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "PRELS", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Doch da sie auch aus dieser Welt", "tokens": ["Doch", "da", "sie", "auch", "aus", "die\u00b7ser", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nur durchs Totsein hochgestellt,", "tokens": ["Und", "nur", "durchs", "Tot\u00b7sein", "hoch\u00b7ge\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Fragte ich: \u00bbSagt mir, liebe V\u00e4ter,", "tokens": ["Frag\u00b7te", "ich", ":", "\u00bb", "Sagt", "mir", ",", "lie\u00b7be", "V\u00e4\u00b7ter", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$.", "$(", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Nennt ihr mich einen Misset\u00e4ter,", "tokens": ["Nennt", "ihr", "mich", "ei\u00b7nen", "Mis\u00b7se\u00b7t\u00e4\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Wenn ich mein Liebesleid abk\u00fcrze", "tokens": ["Wenn", "ich", "mein", "Lie\u00b7be\u00b7sleid", "ab\u00b7k\u00fcr\u00b7ze"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und jemand in die Arme st\u00fcrze?\u00ab", "tokens": ["Und", "je\u00b7mand", "in", "die", "Ar\u00b7me", "st\u00fcr\u00b7ze", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PIS", "APPR", "ART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Sie sahen unbestimmt mich an,", "tokens": ["Sie", "sa\u00b7hen", "un\u00b7be\u00b7stimmt", "mich", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was man sich ja auch denken kann.", "tokens": ["Was", "man", "sich", "ja", "auch", "den\u00b7ken", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "PRF", "ADV", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Ich sprach: \u00bbSie hat schon zugesagt,", "tokens": ["Ich", "sprach", ":", "\u00bb", "Sie", "hat", "schon", "zu\u00b7ge\u00b7sagt", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hab' proforma nur gefragt.", "tokens": ["Ich", "hab'", "pro\u00b7for\u00b7ma", "nur", "ge\u00b7fragt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NE", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Das Mohrle kommt zur Nacht zu mir,", "tokens": ["Das", "Mohr\u00b7le", "kommt", "zur", "Nacht", "zu", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und morgen fr\u00fch ist sie noch hier.", "tokens": ["Und", "mor\u00b7gen", "fr\u00fch", "ist", "sie", "noch", "hier", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VAFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Verzeiht, da\u00df zuviel ich mich freue,", "tokens": ["Ver\u00b7zeiht", ",", "da\u00df", "zu\u00b7viel", "ich", "mich", "freu\u00b7e", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PIS", "PPER", "PRF", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Und sp\u00e4ter kommt auch keine Reue.", "tokens": ["Und", "sp\u00e4\u00b7ter", "kommt", "auch", "kei\u00b7ne", "Reu\u00b7e", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Ich mu\u00df es endlich klar bekommen,", "tokens": ["Ich", "mu\u00df", "es", "end\u00b7lich", "klar", "be\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat sich mein Herz zwei Fraun genommen.\u00ab", "tokens": ["Hat", "sich", "mein", "Herz", "zwei", "Fraun", "ge\u00b7nom\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PRF", "PPOSAT", "NN", "CARD", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Die Ahnen blieben m\u00e4uschenstill.", "tokens": ["Die", "Ah\u00b7nen", "blie\u00b7ben", "m\u00e4u\u00b7schen\u00b7still", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was bei Ahnen nichts hei\u00dfen will.", "tokens": ["Was", "bei", "Ah\u00b7nen", "nichts", "hei\u00b7\u00dfen", "will", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "PIS", "VVINF", "VMFIN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.32": {"line.1": {"text": "\u00bbihr habt also gar nichts dagegen,", "tokens": ["\u00bb", "ihr", "habt", "al\u00b7so", "gar", "nichts", "da\u00b7ge\u00b7gen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ADV", "ADV", "PIS", "PAV", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "So nehm' ich sie mit eurem Segen.", "tokens": ["So", "nehm'", "ich", "sie", "mit", "eu\u00b7rem", "Se\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Ist's schlecht, so konnt' man mich ja mahnen.", "tokens": ["Ist's", "schlecht", ",", "so", "konnt'", "man", "mich", "ja", "mah\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "$,", "ADV", "VMFIN", "PIS", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Weshalb h\u00e4lt man denn sonst auf Ahnen?\u00ab", "tokens": ["We\u00b7shalb", "h\u00e4lt", "man", "denn", "sonst", "auf", "Ah\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}}}}