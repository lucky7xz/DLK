{"dta.poem.19734": {"metadata": {"author": {"name": "Brentano, Clemens", "birth": "N.A.", "death": "N.A."}, "title": "Kinderey .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1808", "urn": "urn:nbn:de:kobv:b4-20090519168", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Als sich der Hahn th\u00e4t kr\u00e4hen,               ", "tokens": ["Als", "sich", "der", "Hahn", "th\u00e4t", "kr\u00e4\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "VVFIN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Da war es noch lange nicht Tag,", "tokens": ["Da", "war", "es", "noch", "lan\u00b7ge", "nicht", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "PTKNEG", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Da gingen die jungen Geseellchen", "tokens": ["Da", "gin\u00b7gen", "die", "jun\u00b7gen", "Ge\u00b7seell\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Spazieren die ganze Nacht.", "tokens": ["Spa\u00b7zie\u00b7ren", "die", "gan\u00b7ze", "Nacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Und als sie lange gegangen,", "tokens": ["Und", "als", "sie", "lan\u00b7ge", "ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da wollten sie gerne herein:", "tokens": ["Da", "woll\u00b7ten", "sie", "ger\u00b7ne", "her\u00b7ein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.3": {"line.1": {"text": "Er. Steh auf, steh auf Feinsliebchen,               ", "tokens": ["Er", ".", "Steh", "auf", ",", "steh", "auf", "Feins\u00b7lieb\u00b7chen", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "VVFIN", "PTKVZ", "$,", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Steh auf und la\u00df mich ein.", "tokens": ["Steh", "auf", "und", "la\u00df", "mich", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "KON", "VVIMP", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Sie. Ich steh noch nicht auf f\u00fcrwahr,               ", "tokens": ["Sie", ".", "Ich", "steh", "noch", "nicht", "auf", "f\u00fcr\u00b7wahr", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VVFIN", "ADV", "PTKNEG", "APPR", "ADV", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ich la\u00df dich f\u00fcrwahr nicht herein,", "tokens": ["Ich", "la\u00df", "dich", "f\u00fcr\u00b7wahr", "nicht", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich kenne dich ja an der Sprache,", "tokens": ["Ich", "ken\u00b7ne", "dich", "ja", "an", "der", "Spra\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df du es mein Sch\u00e4tzchen nicht seyst.", "tokens": ["Da\u00df", "du", "es", "mein", "Sch\u00e4tz\u00b7chen", "nicht", "seyst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PPOSAT", "NN", "PTKNEG", "VAFIN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.5": {"line.1": {"text": "Er. Kennst du es mich an der Sprache,               ", "tokens": ["Er", ".", "Kennst", "du", "es", "mich", "an", "der", "Spra\u00b7che", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "VVFIN", "PPER", "PPER", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich es dein Sch\u00e4tzchen nicht sey,", "tokens": ["Da\u00df", "ich", "es", "dein", "Sch\u00e4tz\u00b7chen", "nicht", "sey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PPOSAT", "NN", "PTKNEG", "VAFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "So stecke du an nur dein Kerzchen,", "tokens": ["So", "ste\u00b7cke", "du", "an", "nur", "dein", "Kerz\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dann siehest du, wer ich bin.", "tokens": ["Dann", "sie\u00b7hest", "du", ",", "wer", "ich", "bin", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PWS", "PPER", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Sie. Kein F\u00fcnkchen mehr in der Asche ist,               ", "tokens": ["Sie", ".", "Kein", "F\u00fcnk\u00b7chen", "mehr", "in", "der", "A\u00b7sche", "ist", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PIAT", "NN", "ADV", "APPR", "ART", "NN", "VAFIN", "$,"], "meter": "+-+-++-+-+", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Mein Kerzchen ist l\u00e4ngst ausgebrannt,", "tokens": ["Mein", "Kerz\u00b7chen", "ist", "l\u00e4ngst", "aus\u00b7ge\u00b7brannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Adi, Adi mein Engelssch\u00e4tzchen,", "tokens": ["A\u00b7di", ",", "A\u00b7di", "mein", "En\u00b7gels\u00b7sch\u00e4tz\u00b7chen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "PPOSAT", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Jezt reis' ich nach Engelland.", "tokens": ["Jezt", "reis'", "ich", "nach", "En\u00b7gel\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Er. Nach Engelland will ich dich fahren,               ", "tokens": ["Er", ".", "Nach", "En\u00b7gel\u00b7land", "will", "ich", "dich", "fah\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "APPR", "NE", "VMFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Ich bin ein Schiffmann gut,", "tokens": ["Ich", "bin", "ein", "Schiff\u00b7mann", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Du bist in deinen Jahren", "tokens": ["Du", "bist", "in", "dei\u00b7nen", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Noch immer kindisch genug.", "tokens": ["Noch", "im\u00b7mer", "kin\u00b7disch", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "ADV", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}}}}