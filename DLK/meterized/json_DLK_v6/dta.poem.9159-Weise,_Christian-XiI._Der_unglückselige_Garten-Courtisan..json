{"dta.poem.9159": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "XiI.  \n Der ungl\u00fcckselige Garten-Courtisan.", "genre": "Lyrik; Drama; Prosa", "period": "N.A.", "pub_year": "1701", "urn": "urn:nbn:de:kobv:b4-25043-0", "language": ["de:0.99"], "booktitle": "Weise, Christian: \u00dcberfl\u00fc\u00dfige Gedancken Der gr\u00fcnenden jugend. Leipzig, 1701."}, "poem": {"stanza.1": {"line.1": {"text": "Er.", "tokens": ["Er", "."], "token_info": ["word", "punct"], "pos": ["PPER", "$."], "meter": "-", "measure": "single.down"}, "line.2": {"text": "Mein kind so treff ich sie", "tokens": ["Mein", "kind", "so", "treff", "ich", "sie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN", "PPER", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "In diesem garten an?", "tokens": ["In", "die\u00b7sem", "gar\u00b7ten", "an", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Sie. Ach da\u00df er doch die m\u00fch", "tokens": ["Sie", ".", "Ach", "da\u00df", "er", "doch", "die", "m\u00fch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ITJ", "KOUS", "PPER", "ADV", "ART", "ADJD"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.5": {"text": "Um mich nicht lassen kan.", "tokens": ["Um", "mich", "nicht", "las\u00b7sen", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPER", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Er. Ich habe sie gesucht/", "tokens": ["Er", ".", "Ich", "ha\u00b7be", "sie", "ge\u00b7sucht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VAFIN", "PPER", "VVPP", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und habe sie gefunden.", "tokens": ["Und", "ha\u00b7be", "sie", "ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Dadurch ist mir die frucht", "tokens": ["Sie", ".", "Da\u00b7durch", "ist", "mir", "die", "frucht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PAV", "VAFIN", "PPER", "ART", "NN"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Der einsamkeit verschwunden.", "tokens": ["Der", "ein\u00b7sam\u00b7keit", "ver\u00b7schwun\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Er. Sie g\u00f6nne mir die lust", "tokens": ["Er", ".", "Sie", "g\u00f6n\u00b7ne", "mir", "die", "lust"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPER", "VVFIN", "PPER", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und lasse mich hinein.", "tokens": ["Und", "las\u00b7se", "mich", "hin\u00b7ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Dem Herren ists bewust", "tokens": ["Sie", ".", "Dem", "Her\u00b7ren", "ists", "be\u00b7wust"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ART", "NN", "NE", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ich mu\u00df alleine seyn.", "tokens": ["Ich", "mu\u00df", "al\u00b7lei\u00b7ne", "seyn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Er. Mein kind wie ist sie doch", "tokens": ["Er", ".", "Mein", "kind", "wie", "ist", "sie", "doch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPOSAT", "NN", "KOKOM", "VAFIN", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So sauer und vermessen.", "tokens": ["So", "sau\u00b7er", "und", "ver\u00b7mes\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Mein Herr ich habe noch", "tokens": ["Sie", ".", "Mein", "Herr", "ich", "ha\u00b7be", "noch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPOSAT", "NN", "PPER", "VAFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Kein honig-b\u00e4mmgen gessen.", "tokens": ["Kein", "ho\u00b7nig\u00b7b\u00e4mm\u00b7gen", "ges\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Er. Beh\u00e4lt sie ihren sinn", "tokens": ["Er", ".", "Be\u00b7h\u00e4lt", "sie", "ih\u00b7ren", "sinn"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie sie allzeit gethan.", "tokens": ["Wie", "sie", "all\u00b7zeit", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. So geh er immer hin/", "tokens": ["Sie", ".", "So", "geh", "er", "im\u00b7mer", "hin", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "ADV", "VVFIN", "PPER", "ADV", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn ers nicht leiden kan.", "tokens": ["Wenn", "ers", "nicht", "lei\u00b7den", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Er. Ach was vor stiche gibt", "tokens": ["Er", ".", "Ach", "was", "vor", "sti\u00b7che", "gibt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ITJ", "PWS", "APPR", "ADJA", "VVFIN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sie meinem frommen hertzen.", "tokens": ["Sie", "mei\u00b7nem", "from\u00b7men", "hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. In warheit es beliebt", "tokens": ["Sie", ".", "In", "war\u00b7heit", "es", "be\u00b7liebt"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "APPR", "PWAV", "PPER", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Demselben so zu schertzen.", "tokens": ["Dem\u00b7sel\u00b7ben", "so", "zu", "schert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Er. Ich stelle mich ja gar", "tokens": ["Er", ".", "Ich", "stel\u00b7le", "mich", "ja", "gar"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPER", "VVFIN", "PPER", "ADV", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu ihren diensten ein.", "tokens": ["Zu", "ih\u00b7ren", "diens\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Der diener wird f\u00fcrwahr", "tokens": ["Sie", ".", "Der", "die\u00b7ner", "wird", "f\u00fcr\u00b7wahr"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ART", "NN", "VAFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Vor mich zu k\u00f6stlich seyn.", "tokens": ["Vor", "mich", "zu", "k\u00f6st\u00b7lich", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKA", "ADJD", "VAINF", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.8": {"line.1": {"text": "Er. Ich w\u00fcnsch in ihrer pflicht", "tokens": ["Er", ".", "Ich", "w\u00fcnsch", "in", "ih\u00b7rer", "pflicht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPER", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu leben und zu sterben.", "tokens": ["Zu", "le\u00b7ben", "und", "zu", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Ach nein/ ich la\u00df ihn nicht", "tokens": ["Sie", ".", "Ach", "nein", "/", "ich", "la\u00df", "ihn", "nicht"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "NN", "PTKANT", "$(", "PPER", "VVFIN", "PPER", "PTKNEG"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "So liederlich verderben.", "tokens": ["So", "lie\u00b7der\u00b7lich", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Er. Wenn dieses m\u00f6glich w\u00e4r", "tokens": ["Er", ".", "Wenn", "die\u00b7ses", "m\u00f6g\u00b7lich", "w\u00e4r"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "KOUS", "PDAT", "ADJD", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So d\u00f6rfft ich zu ihr hin.", "tokens": ["So", "d\u00f6rfft", "ich", "zu", "ihr", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Er h\u00f6re wieder her/", "tokens": ["Sie", ".", "Er", "h\u00f6\u00b7re", "wie\u00b7der", "her", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VVFIN", "ADV", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wann ich nicht haussen bin.", "tokens": ["Wann", "ich", "nicht", "haus\u00b7sen", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PTKNEG", "VVPP", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Er. Ein sch\u00f6nes m\u00e4dgen soll", "tokens": ["Er", ".", "Ein", "sch\u00f6\u00b7nes", "m\u00e4d\u00b7gen", "soll"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ART", "ADJA", "VVINF", "VMFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So strenge nicht verfahren.", "tokens": ["So", "stren\u00b7ge", "nicht", "ver\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Den athen m\u00f6cht er wol", "tokens": ["Sie", ".", "Den", "at\u00b7hen", "m\u00f6cht", "er", "wol"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ART", "VVINF", "VMFIN", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu einer suppe sparen.", "tokens": ["Zu", "ei\u00b7ner", "sup\u00b7pe", "spa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Er. So darff ich nicht zu ihr", "tokens": ["Er", ".", "So", "darff", "ich", "nicht", "zu", "ihr"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ADV", "VMFIN", "PPER", "PTKNEG", "APPR", "PPOSAT"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mein kind was mach ich nun.", "tokens": ["Mein", "kind", "was", "mach", "ich", "nun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PWS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Botz tausend kan er mir", "tokens": ["Sie", ".", "Botz", "tau\u00b7send", "kan", "er", "mir"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "NN", "CARD", "VMFIN", "PPER", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sonst keinen possen thun.", "tokens": ["Sonst", "kei\u00b7nen", "pos\u00b7sen", "thun", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Er. Ein sch\u00f6nes angesicht", "tokens": ["Er", ".", "Ein", "sch\u00f6\u00b7nes", "an\u00b7ge\u00b7sicht"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PPER", "$.", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Werd ich f\u00fcrwahr nicht hassen.", "tokens": ["Werd", "ich", "f\u00fcr\u00b7wahr", "nicht", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Mein blut! er kan doch nicht", "tokens": ["Sie", ".", "Mein", "blut", "!", "er", "kan", "doch", "nicht"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPOSAT", "NN", "$.", "PPER", "VMFIN", "ADV", "PTKNEG"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Die complementen lassen.", "tokens": ["Die", "com\u00b7ple\u00b7men\u00b7ten", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+---+-", "measure": "dactylic.init"}}, "stanza.13": {"line.1": {"text": "Er. Sie ist ja roth und wei\u00df", "tokens": ["Er", ".", "Sie", "ist", "ja", "roth", "und", "wei\u00df"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPER", "VAFIN", "ADV", "ADJD", "KON", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit rosen umgestreut.", "tokens": ["Mit", "ro\u00b7sen", "um\u00b7ge\u00b7streut", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Gewi\u00df er hat den prei\u00df", "tokens": ["Sie", ".", "Ge\u00b7wi\u00df", "er", "hat", "den", "prei\u00df"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PTKANT", "PPER", "VAFIN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Der h\u00f6chsten h\u00f6ffligkeit.", "tokens": ["Der", "h\u00f6chs\u00b7ten", "h\u00f6ff\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Er. Doch f\u00fchl ich meine qual", "tokens": ["Er", ".", "Doch", "f\u00fchl", "ich", "mei\u00b7ne", "qual"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "KON", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und w\u00fcnsche kaum zuleben.", "tokens": ["Und", "w\u00fcn\u00b7sche", "kaum", "zu\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Er sag es noch einmahl/", "tokens": ["Sie", ".", "Er", "sag", "es", "noch", "ein\u00b7mahl", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VVFIN", "PPER", "ADV", "ADV", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das war recht wohl gegeben.", "tokens": ["Das", "war", "recht", "wohl", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Er. Ich sag es noch einmahl/", "tokens": ["Er", ".", "Ich", "sag", "es", "noch", "ein\u00b7mahl", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VVFIN", "PPER", "ADV", "ADV", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie ist sie nun gesinnt?", "tokens": ["Wie", "ist", "sie", "nun", "ge\u00b7sinnt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. F\u00fcrwar das steht gar kahl/", "tokens": ["Sie", ".", "F\u00fcr\u00b7war", "das", "steht", "gar", "kahl", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "NN", "PDS", "VVFIN", "ADV", "ADJD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist er ein pfaffen-kind?", "tokens": ["Ist", "er", "ein", "pfaf\u00b7fen\u00b7kind", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Er. Es ist ihr steter brauch/", "tokens": ["Er", ".", "Es", "ist", "ihr", "ste\u00b7ter", "brauch", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Sie treibet ihr geh\u00f6ne.", "tokens": ["Sie", "trei\u00b7bet", "ihr", "ge\u00b7h\u00f6\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Hat seine mutter auch", "tokens": ["Sie", ".", "Hat", "sei\u00b7ne", "mut\u00b7ter", "auch"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "VAFIN", "PPOSAT", "NN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Mehr solche kluge s\u00f6hne?", "tokens": ["Mehr", "sol\u00b7che", "klu\u00b7ge", "s\u00f6h\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Er. Was hilffts/ ich ehre sie/", "tokens": ["Er", ".", "Was", "hilffts", "/", "ich", "eh\u00b7re", "sie", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PWS", "VVFIN", "$(", "PPER", "VVFIN", "PPER", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und wann sie eisern w\u00e4r.", "tokens": ["Und", "wann", "sie", "ei\u00b7sern", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Drum war mir heute fr\u00fch", "tokens": ["Sie", ".", "Drum", "war", "mir", "heu\u00b7te", "fr\u00fch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PAV", "VAFIN", "PPER", "ADV", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Das hertze noch so schwer.", "tokens": ["Das", "hert\u00b7ze", "noch", "so", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Er. Ich lauff und bin doch matt/", "tokens": ["Er", ".", "Ich", "lauff", "und", "bin", "doch", "matt", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "NN", "KON", "VAFIN", "ADV", "ADJD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ich brenn/ und will mich w\u00e4rmen.", "tokens": ["Ich", "brenn", "/", "und", "will", "mich", "w\u00e4r\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "VMFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Wer keine bienen hat", "tokens": ["Sie", ".", "Wer", "kei\u00b7ne", "bie\u00b7nen", "hat"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PWS", "PIAT", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Mu\u00df freylich selber schw\u00e4rmen.", "tokens": ["Mu\u00df", "frey\u00b7lich", "sel\u00b7ber", "schw\u00e4r\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Er. Nun meine seele soll", "tokens": ["Er", ".", "Nun", "mei\u00b7ne", "see\u00b7le", "soll"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ADV", "PPOSAT", "NN", "VMFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "In ihrer seele ruhn.", "tokens": ["In", "ih\u00b7rer", "see\u00b7le", "ruhn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Mein herr es wirds ihm wol", "tokens": ["Sie", ".", "Mein", "herr", "es", "wirds", "ihm", "wol"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$.", "PPOSAT", "NN", "PPER", "VAFIN", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ein ander h\u00f6ltzgen thun.", "tokens": ["Ein", "an\u00b7der", "h\u00f6ltz\u00b7gen", "thun", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVINF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Er. F\u00fcrwar ich bleibe da/", "tokens": ["Er", ".", "F\u00fcr\u00b7war", "ich", "blei\u00b7be", "da", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "NN", "PPER", "VVFIN", "ADV", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df ich sie nicht verliere.", "tokens": ["Da\u00df", "ich", "sie", "nicht", "ver\u00b7lie\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Ach er gewene ja", "tokens": ["Sie", ".", "Ach", "er", "ge\u00b7we\u00b7ne", "ja"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ITJ", "PPER", "ADJA", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Das maul zu anderm biere.", "tokens": ["Das", "maul", "zu", "an\u00b7derm", "bie\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Er. Jedoch sie lasse mich", "tokens": ["Er", ".", "Je\u00b7doch", "sie", "las\u00b7se", "mich"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ADV", "PPER", "VVFIN", "PPER"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Nicht vor der th\u00fcre stehn.", "tokens": ["Nicht", "vor", "der", "th\u00fc\u00b7re", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Mein was bem\u00fcht er sich/", "tokens": ["Sie", ".", "Mein", "was", "be\u00b7m\u00fcht", "er", "sich", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPOSAT", "PWS", "VVFIN", "PPER", "PRF", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ich mu\u00df nach hause gehn.", "tokens": ["Ich", "mu\u00df", "nach", "hau\u00b7se", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "Er. Nun so behalte sie", "tokens": ["Er", ".", "Nun", "so", "be\u00b7hal\u00b7te", "sie"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$.", "ADV", "ADV", "VVFIN", "PPER"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Auch ihre stoltze weise.", "tokens": ["Auch", "ih\u00b7re", "stolt\u00b7ze", "wei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie. Er schone seiner knie/", "tokens": ["Sie", ".", "Er", "scho\u00b7ne", "sei\u00b7ner", "knie", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$.", "PPER", "VVFIN", "PPOSAT", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Viel gl\u00fccks auff seine reise.", "tokens": ["Viel", "gl\u00fccks", "auff", "sei\u00b7ne", "rei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}