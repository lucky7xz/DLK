{"textgrid.poem.47904": {"metadata": {"author": {"name": "Klaj, Johann", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wann eine Brunst entbrennt und reisset au\u00df den D\u00e4chern/", "genre": "verse", "period": "N.A.", "pub_year": 1636, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wann eine Brunst entbrennt und reisset au\u00df den D\u00e4chern/", "tokens": ["Wann", "ei\u00b7ne", "Brunst", "ent\u00b7brennt", "und", "reis\u00b7set", "au\u00df", "den", "D\u00e4\u00b7chern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "tr\u00e4gt man und wirffet au\u00df/ l\u00e4ufft gar au\u00df den Gem\u00e4chern/", "tokens": ["tr\u00e4gt", "man", "und", "wirf\u00b7fet", "au\u00df", "/", "l\u00e4ufft", "gar", "au\u00df", "den", "Ge\u00b7m\u00e4\u00b7chern", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "KON", "VVFIN", "PTKVZ", "$(", "VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "wann jetzt die Flamme flammt; es lodert Dampff und Tufft/", "tokens": ["wann", "jetzt", "die", "Flam\u00b7me", "flammt", ";", "es", "lo\u00b7dert", "Dampff", "und", "Tufft", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "VVFIN", "$.", "PPER", "VVFIN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "die Sternen sehen zu und flammen in der Lufft;", "tokens": ["die", "Ster\u00b7nen", "se\u00b7hen", "zu", "und", "flam\u00b7men", "in", "der", "Lufft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "PTKZU", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "das Feuer feyret nicht/ ist gleich die Stadt zu Bette/", "tokens": ["das", "Feu\u00b7er", "fey\u00b7ret", "nicht", "/", "ist", "gleich", "die", "Stadt", "zu", "Bet\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "$(", "VAFIN", "ADV", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "so t\u00f6nt der Glockenschlag/ es rufft die Gluttrompette/", "tokens": ["so", "t\u00f6nt", "der", "Glo\u00b7cken\u00b7schlag", "/", "es", "rufft", "die", "Glut\u00b7trom\u00b7pet\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$(", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "die Paucken paucken Lerm/ es pochet Haus an Haus/", "tokens": ["die", "Pau\u00b7cken", "pau\u00b7cken", "Lerm", "/", "es", "po\u00b7chet", "Haus", "an", "Haus", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$(", "PPER", "VVFIN", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "die B\u00fcrgerschafft steht auf und h\u00e4nget Feuer au\u00df:", "tokens": ["die", "B\u00fcr\u00b7ger\u00b7schafft", "steht", "auf", "und", "h\u00e4n\u00b7get", "Feu\u00b7er", "au\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "KON", "VVFIN", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Die T\u00fcrner stecken au\u00df die rote Feuerfahne/", "tokens": ["Die", "T\u00fcr\u00b7ner", "ste\u00b7cken", "au\u00df", "die", "ro\u00b7te", "Feu\u00b7er\u00b7fah\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "der Capitan ergreifft die blancke Partisane/", "tokens": ["der", "Ca\u00b7pi\u00b7tan", "er\u00b7greifft", "die", "blan\u00b7cke", "Par\u00b7ti\u00b7sa\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "die sch\u00f6ne Reuterey in sch\u00f6ner Ordnung steht/", "tokens": ["die", "sch\u00f6\u00b7ne", "Reu\u00b7te\u00b7rey", "in", "sch\u00f6\u00b7ner", "Ord\u00b7nung", "steht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "ein jeder r\u00fcstet sich/ auf seinen Platz hingeht.", "tokens": ["ein", "je\u00b7der", "r\u00fcs\u00b7tet", "sich", "/", "auf", "sei\u00b7nen", "Platz", "hin\u00b7geht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PRF", "$(", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Man f\u00e4het an mit Macht das Feuer zu bestreiten/", "tokens": ["Man", "f\u00e4\u00b7het", "an", "mit", "Macht", "das", "Feu\u00b7er", "zu", "be\u00b7strei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "APPR", "NN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "f\u00fchrt Wasserk\u00fcnste zu/ man spr\u00fctzet aller Seiten/", "tokens": ["f\u00fchrt", "Was\u00b7ser\u00b7k\u00fcns\u00b7te", "zu", "/", "man", "spr\u00fct\u00b7zet", "al\u00b7ler", "Sei\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PTKZU", "$(", "PIS", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "bringt Leitern/ Hacken dar/ steigt/ klettert auf das Dach/", "tokens": ["bringt", "Lei\u00b7tern", "/", "Ha\u00b7cken", "dar", "/", "steigt", "/", "klet\u00b7tert", "auf", "das", "Dach", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$(", "NN", "PTKVZ", "$(", "VVFIN", "$(", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "reisst Nebenh\u00e4user ein/ zerg\u00e4ntzet Dach und Fach.", "tokens": ["reisst", "Ne\u00b7ben\u00b7h\u00e4u\u00b7ser", "ein", "/", "zer\u00b7g\u00e4nt\u00b7zet", "Dach", "und", "Fach", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "$(", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Ach Glut/ die hat gegl\u00fct; die Sonne/ die viel heller", "tokens": ["Ach", "Glut", "/", "die", "hat", "ge\u00b7gl\u00fct", ";", "die", "Son\u00b7ne", "/", "die", "viel", "hel\u00b7ler"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "NN", "$(", "PDS", "VAFIN", "VVPP", "$.", "ART", "NN", "$(", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "als sonst der Sternen Chor/ gleicht einem Feuerteller/", "tokens": ["als", "sonst", "der", "Ster\u00b7nen", "Chor", "/", "gleicht", "ei\u00b7nem", "Feu\u00b7er\u00b7tel\u00b7ler", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "NN", "$(", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "wirfft Feuerkugeln au\u00df/ die Teutschland aufgezehrt/", "tokens": ["wirfft", "Feu\u00b7er\u00b7ku\u00b7geln", "au\u00df", "/", "die", "Teutschland", "auf\u00b7ge\u00b7zehrt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PTKVZ", "$(", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "da\u00df es mit Besemen zusammen wird gekehrt.", "tokens": ["da\u00df", "es", "mit", "Be\u00b7se\u00b7men", "zu\u00b7sam\u00b7men", "wird", "ge\u00b7kehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "VVPP", "VAFIN", "VVPP", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.21": {"text": "Laufft zu/ lescht/ lescht; \u00fcmsonst/ mit seinen S\u00fcnden k\u00e4mpfen", "tokens": ["Laufft", "zu", "/", "lescht", "/", "lescht", ";", "\u00fcm\u00b7sonst", "/", "mit", "sei\u00b7nen", "S\u00fcn\u00b7den", "k\u00e4mp\u00b7fen"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "PTKZU", "$(", "VVFIN", "$(", "VVFIN", "$.", "ADV", "$(", "APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "und herrschen \u00fcber sie; kan S\u00fcndenfeuer d\u00e4mpfen", "tokens": ["und", "herr\u00b7schen", "\u00fc\u00b7ber", "sie", ";", "kan", "S\u00fcn\u00b7den\u00b7feu\u00b7er", "d\u00e4mp\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PPER", "$.", "VMFIN", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "kein Fischbach leschet so die schwefelgelbe Glut/", "tokens": ["kein", "Fischbach", "le\u00b7schet", "so", "die", "schwe\u00b7fel\u00b7gel\u00b7be", "Glut", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.24": {"text": "als wann ein Regen f\u00e4llt von Bu\u00dfbethrenter Flut.", "tokens": ["als", "wann", "ein", "Re\u00b7gen", "f\u00e4llt", "von", "Bu\u00df\u00b7be\u00b7th\u00b7ren\u00b7ter", "Flut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "VVFIN", "APPR", "NN", "NN", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.25": {"text": "Ach Teutschland steht/ beweint mit Schaden ihren Schaden/", "tokens": ["Ach", "Teutschland", "steht", "/", "be\u00b7weint", "mit", "Scha\u00b7den", "ih\u00b7ren", "Scha\u00b7den", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "VVFIN", "$(", "VVFIN", "APPR", "NN", "PPOSAT", "NN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.26": {"text": "als wolte sie sich gantz im Threnenbade baden.", "tokens": ["als", "wol\u00b7te", "sie", "sich", "gantz", "im", "Thre\u00b7nen\u00b7ba\u00b7de", "ba\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VMFIN", "PPER", "PRF", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "O Himmelsk\u00e4iser hilf/ hilf/ hilf/ hilf Erdengott/", "tokens": ["O", "Him\u00b7mel\u00b7sk\u00e4i\u00b7ser", "hilf", "/", "hilf", "/", "hilf", "/", "hilf", "Er\u00b7den\u00b7gott", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$(", "XY", "$(", "XY", "$(", "VVIMP", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "errette mich au\u00df Noht/ Tod/ Kot/ au\u00df Angst und Spott.", "tokens": ["er\u00b7ret\u00b7te", "mich", "au\u00df", "Noht", "/", "Tod", "/", "Kot", "/", "au\u00df", "Angst", "und", "Spott", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$(", "NN", "$(", "NE", "$(", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Gott/ du bester Kriegzerbrecher/", "tokens": ["Gott", "/", "du", "bes\u00b7ter", "Krieg\u00b7zer\u00b7bre\u00b7cher", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PPER", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "mache Fried/ Fried/ es ist Zeit:", "tokens": ["ma\u00b7che", "Fried", "/", "Fried", "/", "es", "ist", "Zeit", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$(", "NN", "$(", "PPER", "VAFIN", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Mein Reich wird ja st\u00fcndlich schw\u00e4cher", "tokens": ["Mein", "Reich", "wird", "ja", "st\u00fcnd\u00b7lich", "schw\u00e4\u00b7cher"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "ADJA"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "durch den L\u00e4nderfresserstreit.", "tokens": ["durch", "den", "L\u00e4n\u00b7der\u00b7fres\u00b7ser\u00b7streit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ach ich bin de\u00df Krieges m\u00fcde/", "tokens": ["Ach", "ich", "bin", "de\u00df", "Krie\u00b7ges", "m\u00fc\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PPER", "VAFIN", "ART", "NN", "ADJD", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Friedenmacher mache Friede.", "tokens": ["Frie\u00b7den\u00b7ma\u00b7cher", "ma\u00b7che", "Frie\u00b7de", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Ihr/ ihr hohen Potentaten/", "tokens": ["Ihr", "/", "ihr", "ho\u00b7hen", "Po\u00b7ten\u00b7ta\u00b7ten", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$(", "PPOSAT", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Haubt und Glieder/ Gro\u00df und Klein/", "tokens": ["Haubt", "und", "Glie\u00b7der", "/", "Gro\u00df", "und", "Klein", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$(", "NE", "KON", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "helfet doch zum Friede rahten/", "tokens": ["hel\u00b7fet", "doch", "zum", "Frie\u00b7de", "rah\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPRART", "NN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "lasset doch die Titel seyn.", "tokens": ["las\u00b7set", "doch", "die", "Ti\u00b7tel", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Lasset alles ungerochen/", "tokens": ["Las\u00b7set", "al\u00b7les", "un\u00b7ge\u00b7ro\u00b7chen", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADJD", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "was man hat bisher verbrochen.", "tokens": ["was", "man", "hat", "bis\u00b7her", "ver\u00b7bro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VAFIN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich bitt euch \u00fcm Gottes willen/", "tokens": ["Ich", "bitt", "euch", "\u00fcm", "Got\u00b7tes", "wil\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPRART", "NN", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "stecket doch die Degen ein/", "tokens": ["ste\u00b7cket", "doch", "die", "De\u00b7gen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "ART", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "da\u00df sich Meer und L\u00e4nder stillen/", "tokens": ["da\u00df", "sich", "Meer", "und", "L\u00e4n\u00b7der", "stil\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "NN", "KON", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "da\u00df die Strassen werden rein.", "tokens": ["da\u00df", "die", "Stras\u00b7sen", "wer\u00b7den", "rein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "da\u00df man kan zu Wasser handeln/", "tokens": ["da\u00df", "man", "kan", "zu", "Was\u00b7ser", "han\u00b7deln", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "APPR", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "sicher auf die Messen wandeln.", "tokens": ["si\u00b7cher", "auf", "die", "Mes\u00b7sen", "wan\u00b7deln", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Seit man Kriegen hat getrieben/", "tokens": ["Seit", "man", "Krie\u00b7gen", "hat", "ge\u00b7trie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Land und Stand geblasen an/", "tokens": ["Land", "und", "Stand", "ge\u00b7bla\u00b7sen", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVPP", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "sind nur auf der Wahlstatt blieben", "tokens": ["sind", "nur", "auf", "der", "Wahl\u00b7statt", "blie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "ART", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "hundert tausend tausend Mann.", "tokens": ["hun\u00b7dert", "tau\u00b7send", "tau\u00b7send", "Mann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "CARD", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was f\u00fcr Mord hat man erfahren", "tokens": ["Was", "f\u00fcr", "Mord", "hat", "man", "er\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "APPR", "NN", "VAFIN", "PIS", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "in den dreymal zehen Jahren?", "tokens": ["in", "den", "drey\u00b7mal", "ze\u00b7hen", "Jah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADV", "CARD", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Wieviel tausend sind verloren", "tokens": ["Wie\u00b7viel", "tau\u00b7send", "sind", "ver\u00b7lo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVPP", "VAFIN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "hier zu Wasser/ dar zu Land/", "tokens": ["hier", "zu", "Was\u00b7ser", "/", "dar", "zu", "Land", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$(", "PTKVZ", "APPR", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wieviel tausend sind erfroren/", "tokens": ["wie\u00b7viel", "tau\u00b7send", "sind", "er\u00b7fro\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "VAFIN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "wieviel tausend sind verbrannt/", "tokens": ["wie\u00b7viel", "tau\u00b7send", "sind", "ver\u00b7brannt", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "wieviel tausend sind geblieben/", "tokens": ["wie\u00b7viel", "tau\u00b7send", "sind", "ge\u00b7blie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "die der Hunger aufgerieben?", "tokens": ["die", "der", "Hun\u00b7ger", "auf\u00b7ge\u00b7rie\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Was ist sonst im Feld gestorben", "tokens": ["Was", "ist", "sonst", "im", "Feld", "ge\u00b7stor\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "APPRART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "von der Peste/ von der Ruhr/", "tokens": ["von", "der", "Pes\u00b7te", "/", "von", "der", "Ruhr", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$(", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "wieviel tausend sind verdorben/", "tokens": ["wie\u00b7viel", "tau\u00b7send", "sind", "ver\u00b7dor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVPP", "VAFIN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "ohne Labsal/ ohne Cur/", "tokens": ["oh\u00b7ne", "Lab\u00b7sal", "/", "oh\u00b7ne", "Cur", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "APPR", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "wieviel haben auf der Strassen", "tokens": ["wie\u00b7viel", "ha\u00b7ben", "auf", "der", "Stras\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "VAFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "m\u00fcssen Leib und Leben lassen?", "tokens": ["m\u00fcs\u00b7sen", "Leib", "und", "Le\u00b7ben", "las\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "KON", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "W\u00e4isen/ Witwen/ Jammerl\u00e4chtzen", "tokens": ["W\u00e4i\u00b7sen", "/", "Wit\u00b7wen", "/", "Jam\u00b7mer\u00b7l\u00e4cht\u00b7zen"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["NN", "$(", "NN", "$(", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "ohne Vatter/ ohne Mann/", "tokens": ["oh\u00b7ne", "Vat\u00b7ter", "/", "oh\u00b7ne", "Mann", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "APPR", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Witwen wie die Tauben \u00e4chtzen/", "tokens": ["Wit\u00b7wen", "wie", "die", "Tau\u00b7ben", "\u00e4cht\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "ART", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "W\u00e4isen Fremden unterthan/", "tokens": ["W\u00e4i\u00b7sen", "Frem\u00b7den", "un\u00b7ter\u00b7than", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "NN", "VVPP", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "essen Threnenbrod mit Wimmern", "tokens": ["es\u00b7sen", "Thre\u00b7nen\u00b7brod", "mit", "Wim\u00b7mern"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "und sich fast zu Tode k\u00fcmmern.", "tokens": ["und", "sich", "fast", "zu", "To\u00b7de", "k\u00fcm\u00b7mern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Die/ so zu Gottesburg die Hertzensschmertzen schicket/", "tokens": ["Die", "/", "so", "zu", "Got\u00b7tes\u00b7burg", "die", "Hert\u00b7zens\u00b7schmert\u00b7zen", "schi\u00b7cket", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "ADV", "APPR", "NE", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "in Wolckenheller Lufft ein neu Gesicht erblicket/", "tokens": ["in", "Wol\u00b7cken\u00b7hel\u00b7ler", "Lufft", "ein", "neu", "Ge\u00b7sicht", "er\u00b7bli\u00b7cket", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "ART", "ADJD", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "das Englische Gedritt sich hin und wieder schwingt/", "tokens": ["das", "Eng\u00b7li\u00b7sche", "Ge\u00b7dritt", "sich", "hin", "und", "wie\u00b7der", "schwingt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "PTKVZ", "KON", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "so Teutschland gute Post vom Krieges Abzug bringt:", "tokens": ["so", "Teutschland", "gu\u00b7te", "Post", "vom", "Krie\u00b7ges", "Ab\u00b7zug", "bringt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "ADJA", "NN", "APPRART", "NN", "NN", "VVFIN", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}}}}}