{"textgrid.poem.46174": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Drunkenheit", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Kont ihr mich dan sunst gar nichts fragen,", "tokens": ["Kont", "ihr", "mich", "dan", "sunst", "gar", "nichts", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "ADV", "ADV", "ADV", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "ihr herren, meine gute freind,", "tokens": ["ihr", "her\u00b7ren", ",", "mei\u00b7ne", "gu\u00b7te", "freind", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "dan was ich euch k\u00f6nd neues sagen,", "tokens": ["dan", "was", "ich", "euch", "k\u00f6nd", "neu\u00b7es", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "PPER", "PPER", "VMFIN", "ADJA", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "wie stark und wa jetzund der feind?", "tokens": ["wie", "stark", "und", "wa", "je\u00b7tzund", "der", "feind", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "KON", "XY", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "ich bit, doch wollet mir verzeihen,", "tokens": ["ich", "bit", ",", "doch", "wol\u00b7let", "mir", "ver\u00b7zei\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "$,", "ADV", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "mit fragen nicht zu fahren fort,", "tokens": ["mit", "fra\u00b7gen", "nicht", "zu", "fah\u00b7ren", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PTKNEG", "PTKZU", "VVINF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "dan sunsten will ich euch verleihen", "tokens": ["dan", "suns\u00b7ten", "will", "ich", "euch", "ver\u00b7lei\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVINF", "VMFIN", "PPER", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "kein einig wort.", "tokens": ["kein", "ei\u00b7nig", "wort", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Ich red nicht gern von schm\u00e4hen, tr\u00e4uen,", "tokens": ["Ich", "red", "nicht", "gern", "von", "schm\u00e4\u00b7hen", ",", "tr\u00e4u\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "ADJA", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "von krieg, bronst, raub, ungl\u00fcck und not,", "tokens": ["von", "krieg", ",", "bronst", ",", "raub", ",", "un\u00b7gl\u00fcck", "und", "not", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "VVFIN", "$,", "VVFIN", "$,", "ADJD", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "sondern allein, uns zu erfreuen,", "tokens": ["son\u00b7dern", "al\u00b7lein", ",", "uns", "zu", "er\u00b7freu\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "von gutem wildbret, wein und brot.", "tokens": ["von", "gu\u00b7tem", "wild\u00b7bret", ",", "wein", "und", "brot", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "PTKVZ", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "den man der wein mit lieb entz\u00fcndet", "tokens": ["den", "man", "der", "wein", "mit", "lieb", "ent\u00b7z\u00fcn\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "ART", "NN", "APPR", "ADJD", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "und das brot st\u00e4rket ihm den leib,", "tokens": ["und", "das", "brot", "st\u00e4r\u00b7ket", "ihm", "den", "leib", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "da\u00df er das wildbret besser findet", "tokens": ["da\u00df", "er", "das", "wild\u00b7bret", "bes\u00b7ser", "fin\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PDS", "VVFIN", "ADJD", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "bei seinem weib.", "tokens": ["bei", "sei\u00b7nem", "weib", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "So lang zu reden, lesen, h\u00f6ren,", "tokens": ["So", "lang", "zu", "re\u00b7den", ",", "le\u00b7sen", ",", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVINF", "$,", "VVINF", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "und mit dem haupt, hut, kn\u00fc, fu\u00df, hand", "tokens": ["und", "mit", "dem", "haupt", ",", "hut", ",", "kn\u00fc", ",", "fu\u00df", ",", "hand"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["KON", "APPR", "ART", "NN", "$,", "VVFIN", "$,", "VVFIN", "$,", "PTKVZ", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "gesandten, herren, k\u00f6nig ehren,", "tokens": ["ge\u00b7sand\u00b7ten", ",", "her\u00b7ren", ",", "k\u00f6\u00b7nig", "eh\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVIZU", "$,", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "so lang zu sprachen an der wand,", "tokens": ["so", "lang", "zu", "spra\u00b7chen", "an", "der", "wand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVFIN", "APPR", "ART", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "so lang zu schreiben und zu reden", "tokens": ["so", "lang", "zu", "schrei\u00b7ben", "und", "zu", "re\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "von Gabor, Tilly, Wallenstein,", "tokens": ["von", "Ga\u00b7bor", ",", "Til\u00b7ly", ",", "Wal\u00b7len\u00b7stein", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "$,", "NE", "$,", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "von Frankreich, Welschland, Denmark, Schweden", "tokens": ["von", "Fran\u00b7kreich", ",", "Wel\u00b7schland", ",", "Den\u00b7mark", ",", "Schwe\u00b7den"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NE", "$,", "NN", "$,", "NE", "$,", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "ist eine pein.", "tokens": ["ist", "ei\u00b7ne", "pein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Darum fort, fort mit solchem trauren,", "tokens": ["Da\u00b7rum", "fort", ",", "fort", "mit", "sol\u00b7chem", "trau\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PTKVZ", "$,", "PTKVZ", "APPR", "PIAT", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df man alsbald bedeck den tisch,", "tokens": ["da\u00df", "man", "als\u00b7bald", "be\u00b7deck", "den", "tisch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "und keiner la\u00df die m\u00fch sich dauren,", "tokens": ["und", "kei\u00b7ner", "la\u00df", "die", "m\u00fch", "sich", "dau\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ART", "ADJD", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "wan wein, brot, fleisch und alles frisch;", "tokens": ["wan", "wein", ",", "brot", ",", "fleisch", "und", "al\u00b7les", "frisch", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKVZ", "$,", "VVFIN", "$,", "ADJD", "KON", "PIS", "ADJD", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.5": {"text": "der erst bei tisch soll der erst drinken,", "tokens": ["der", "erst", "bei", "tisch", "soll", "der", "erst", "drin\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJD", "VMFIN", "ART", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "so, herren, wie behend? wolan!", "tokens": ["so", ",", "her\u00b7ren", ",", "wie", "be\u00b7hend", "?", "wo\u00b7lan", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "VVIZU", "$,", "PWAV", "VVPP", "$.", "ADV", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.7": {"text": "schenk voll! die frau thut dir nicht winken.", "tokens": ["schenk", "voll", "!", "die", "frau", "thut", "dir", "nicht", "win\u00b7ken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "$.", "ART", "NN", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "nu fang ich an.", "tokens": ["nu", "fang", "ich", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Ho! Toman, Lamy, Sering, Rumler,", "tokens": ["Ho", "!", "To\u00b7man", ",", "La\u00b7my", ",", "Se\u00b7ring", ",", "Rum\u00b7ler", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$.", "NE", "$,", "NE", "$,", "NE", "$,", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "es gilt euch! dieser mu\u00df herum!", "tokens": ["es", "gilt", "euch", "!", "die\u00b7ser", "mu\u00df", "he\u00b7rum", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "PDS", "VMFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ich wei\u00df, ihr seid all gute tumler", "tokens": ["ich", "wei\u00df", ",", "ihr", "seid", "all", "gu\u00b7te", "tum\u00b7ler"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und liebet nicht was quad und krum,", "tokens": ["und", "lie\u00b7bet", "nicht", "was", "quad", "und", "krum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "FM", "FM", "KON", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "dan nur das, so man kaum kan manglen,", "tokens": ["dan", "nur", "das", ",", "so", "man", "kaum", "kan", "mang\u00b7len", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PDS", "$,", "ADV", "PIS", "ADV", "VMFIN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.6": {"text": "die weiber wissen auch wol was,", "tokens": ["die", "wei\u00b7ber", "wis\u00b7sen", "auch", "wol", "was", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "gedenkend alsbald an das anglen.", "tokens": ["ge\u00b7den\u00b7kend", "als\u00b7bald", "an", "das", "ang\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "aus ist mein glas.", "tokens": ["aus", "ist", "mein", "glas", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "VAFIN", "PPOSAT", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Nim weg von meinem ohr die feder,", "tokens": ["Nim", "weg", "von", "mei\u00b7nem", "ohr", "die", "fe\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "PPOSAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "gib mir daf\u00fcr ein messer her;", "tokens": ["gib", "mir", "da\u00b7f\u00fcr", "ein", "mes\u00b7ser", "her", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PAV", "ART", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ho, Schweizer, kotz kreuz, zeuch von leder", "tokens": ["ho", ",", "Schwei\u00b7zer", ",", "kotz", "kreuz", ",", "zeuch", "von", "le\u00b7der"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ITJ", "$,", "NN", "$,", "APPR", "NN", "$,", "VVIMP", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und Schweizer gleich streb nu nach ehr!", "tokens": ["und", "Schwei\u00b7zer", "gleich", "streb", "nu", "nach", "ehr", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "wolan, ihr dapfere soldaten", "tokens": ["wo\u00b7lan", ",", "ihr", "dap\u00b7fe\u00b7re", "sol\u00b7da\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "mit unverzagtem frischen mut", "tokens": ["mit", "un\u00b7ver\u00b7zag\u00b7tem", "fri\u00b7schen", "mut"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "waget zu neu und freien thaten", "tokens": ["wa\u00b7get", "zu", "neu", "und", "frei\u00b7en", "tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKA", "ADJD", "KON", "ADJA", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "nu fleisch und blut.", "tokens": ["nu", "fleisch", "und", "blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "Feind haben wir gnug zu bestreiten", "tokens": ["Feind", "ha\u00b7ben", "wir", "gnug", "zu", "be\u00b7strei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "in dem vortrab und dem nachtrab;", "tokens": ["in", "dem", "vor\u00b7trab", "und", "dem", "nach\u00b7trab", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "nu greifet an auf allen seiten", "tokens": ["nu", "grei\u00b7fet", "an", "auf", "al\u00b7len", "sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und schneidet k\u00f6pf und schenkel ab,", "tokens": ["und", "schnei\u00b7det", "k\u00f6pf", "und", "schen\u00b7kel", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "KON", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "indem sich streich, schnit, bi\u00df vermischen,", "tokens": ["in\u00b7dem", "sich", "streich", ",", "schnit", ",", "bi\u00df", "ver\u00b7mi\u00b7schen", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ADJD", "$,", "VVFIN", "$,", "KOUS", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "und der nachtrab mag hitzig sein,", "tokens": ["und", "der", "nach\u00b7trab", "mag", "hit\u00b7zig", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADV", "VMFIN", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "so ruf ich stets, euch zu erfrischen:", "tokens": ["so", "ruf", "ich", "stets", ",", "euch", "zu", "er\u00b7fri\u00b7schen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "ho! schenk uns ein!", "tokens": ["ho", "!", "schenk", "uns", "ein", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Sih, wie mit brechen, schneiden, bei\u00dfen", "tokens": ["Sih", ",", "wie", "mit", "bre\u00b7chen", ",", "schnei\u00b7den", ",", "bei\u00b7\u00dfen"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "PWAV", "APPR", "VVINF", "$,", "VVFIN", "$,", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "dem lieben feind wir machen graus!", "tokens": ["dem", "lie\u00b7ben", "feind", "wir", "ma\u00b7chen", "graus", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "la\u00df mich das spanf\u00e4rlin zerrei\u00dfen,", "tokens": ["la\u00df", "mich", "das", "span\u00b7f\u00e4r\u00b7lin", "zer\u00b7rei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "stich dem kalbskopf die augen aus.", "tokens": ["stich", "dem", "kalbs\u00b7kopf", "die", "au\u00b7gen", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "+-++-+-+", "measure": "unknown.measure.penta"}, "line.5": {"text": "so, so, wirf damit an die frauen,", "tokens": ["so", ",", "so", ",", "wirf", "da\u00b7mit", "an", "die", "frau\u00b7en", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "$,", "VVFIN", "PAV", "APPR", "ART", "NN", "$,"], "meter": "--+----+-", "measure": "anapaest.init"}, "line.6": {"text": "die, wan sie schon so s\u00fc\u00df und mild,", "tokens": ["die", ",", "wan", "sie", "schon", "so", "s\u00fc\u00df", "und", "mild", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PPER", "ADV", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "doch k\u00f6nden hauen und auch klauen.", "tokens": ["doch", "k\u00f6n\u00b7den", "hau\u00b7en", "und", "auch", "klau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "VVINF", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "es gilt! es gilt!", "tokens": ["es", "gilt", "!", "es", "gilt", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Wan die soldaten vor Roschellen,", "tokens": ["Wan", "die", "sol\u00b7da\u00b7ten", "vor", "Ro\u00b7schel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "wan die soldaten vor Stralsund", "tokens": ["wan", "die", "sol\u00b7da\u00b7ten", "vor", "Stral\u00b7sund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "APPR", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "die mauren k\u00f6nten so wol f\u00e4llen,", "tokens": ["die", "mau\u00b7ren", "k\u00f6n\u00b7ten", "so", "wol", "f\u00e4l\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "als herzhaft wir zu dieser stund", "tokens": ["als", "herz\u00b7haft", "wir", "zu", "die\u00b7ser", "stund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "PPER", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "nu st\u00fcrmen wollen die pasteien,", "tokens": ["nu", "st\u00fcr\u00b7men", "wol\u00b7len", "die", "pas\u00b7tei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VMFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "ich sag: die stark wildbret pastet,", "tokens": ["ich", "sag", ":", "die", "stark", "wild\u00b7bret", "pas\u00b7tet", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "ART", "ADJD", "VVFIN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "so w\u00fcrden sie nicht lang mehr freien", "tokens": ["so", "w\u00fcr\u00b7den", "sie", "nicht", "lang", "mehr", "frei\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADJD", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "die beede st\u00e4t.", "tokens": ["die", "bee\u00b7de", "st\u00e4t", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.10": {"line.1": {"text": "Frisch auf, wer ist der beste treffer?", "tokens": ["Frisch", "auf", ",", "wer", "ist", "der", "bes\u00b7te", "tref\u00b7fer", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$,", "PWS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "ha ha! frisch her! ho, ich bin wund!", "tokens": ["ha", "ha", "!", "frisch", "her", "!", "ho", ",", "ich", "bin", "wund", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "ITJ", "$.", "ADJD", "PTKVZ", "$.", "XY", "$,", "PPER", "VAFIN", "ADJD", "$."], "meter": "----+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "das pulver ist von salz und pfeffer!", "tokens": ["das", "pul\u00b7ver", "ist", "von", "salz", "und", "pfef\u00b7fer", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "APPR", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "ho! die brunst ist in meinem mund!", "tokens": ["ho", "!", "die", "brunst", "ist", "in", "mei\u00b7nem", "mund", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "ART", "NN", "VAFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.5": {"text": "doch sih, es hat euch auch getroffen;", "tokens": ["doch", "sih", ",", "es", "hat", "euch", "auch", "ge\u00b7trof\u00b7fen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PPER", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "zu l\u00f6schen, mu\u00df es nicht mehr sein", "tokens": ["zu", "l\u00f6\u00b7schen", ",", "mu\u00df", "es", "nicht", "mehr", "sein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$,", "VMFIN", "PPER", "PTKNEG", "ADV", "VAINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "gedrunken, sondern stark gesoffen.", "tokens": ["ge\u00b7drun\u00b7ken", ",", "son\u00b7dern", "stark", "ge\u00b7sof\u00b7fen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "so schenk nur ein!", "tokens": ["so", "schenk", "nur", "ein", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Durch diesen becher seind wir siger!", "tokens": ["Durch", "die\u00b7sen", "be\u00b7cher", "seind", "wir", "si\u00b7ger", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "so sauf herum knap, munder, doll!", "tokens": ["so", "sauf", "he\u00b7rum", "knap", ",", "mun\u00b7der", ",", "doll", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADV", "APZR", "VVFIN", "$,", "ADJD", "$,", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "drink aus! es gilt der alten schwiger!", "tokens": ["drink", "aus", "!", "es", "gilt", "der", "al\u00b7ten", "schwi\u00b7ger", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$.", "PPER", "VVFIN", "ART", "ADJA", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "ich bin schon mehr dan halb, gar, voll.", "tokens": ["ich", "bin", "schon", "mehr", "dan", "halb", ",", "gar", ",", "voll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$,", "ADV", "$,", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "darum so la\u00df den k\u00e4s herbringen.", "tokens": ["da\u00b7rum", "so", "la\u00df", "den", "k\u00e4s", "her\u00b7brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVIMP", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "kom k\u00fc\u00df! so k\u00fc\u00df mich artlich! so!", "tokens": ["kom", "k\u00fc\u00df", "!", "so", "k\u00fc\u00df", "mich", "art\u00b7lich", "!", "so", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADJD", "$.", "ADV", "VVFIN", "PPER", "ADJD", "$.", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "la\u00df uns ein lied zusamen singen!", "tokens": ["la\u00df", "uns", "ein", "lied", "zu\u00b7sa\u00b7men", "sin\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVINF", "VVINF", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "hem hoscha ho!", "tokens": ["hem", "hosc\u00b7ha", "ho", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Die Schw\u00e4blein, die so gar gern schw\u00e4tzen,", "tokens": ["Die", "Schw\u00e4b\u00b7lein", ",", "die", "so", "gar", "gern", "schw\u00e4t\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "in Th\u00fcringen, dem dollen land,", "tokens": ["in", "Th\u00fc\u00b7rin\u00b7gen", ",", "dem", "dol\u00b7len", "land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "fr\u00e4\u00dfen ein rad f\u00fcr eine bretzen", "tokens": ["fr\u00e4\u00b7\u00dfen", "ein", "rad", "f\u00fcr", "ei\u00b7ne", "bret\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "APPR", "ART", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "mit einem k\u00e4s aus Schweizerland.", "tokens": ["mit", "ei\u00b7nem", "k\u00e4s", "aus", "Schwei\u00b7zer\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "in unsrer h\u00fcbschen frauen namen", "tokens": ["in", "uns\u00b7rer", "h\u00fcb\u00b7schen", "frau\u00b7en", "na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Schwab, Schweizer, Th\u00fcringer, Franzos,", "tokens": ["Schwab", ",", "Schwei\u00b7zer", ",", "Th\u00fc\u00b7rin\u00b7ger", ",", "Fran\u00b7zos", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,", "NE", "$,"], "meter": "-+-+--++", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "so singet fr\u00f6lich nu zusamen:", "tokens": ["so", "sin\u00b7get", "fr\u00f6\u00b7lich", "nu", "zu\u00b7sa\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJD", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "kom k\u00fc\u00df mich, ros!", "tokens": ["kom", "k\u00fc\u00df", "mich", ",", "ros", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "VVFIN", "PPER", "$,", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.13": {"line.1": {"text": "O da\u00df die Schweizer mit den l\u00e4tzen,", "tokens": ["O", "da\u00df", "die", "Schwei\u00b7zer", "mit", "den", "l\u00e4t\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOUS", "ART", "NN", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die Schwaben mit dem leberlein,", "tokens": ["die", "Schwa\u00b7ben", "mit", "dem", "le\u00b7berl\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "die Welschen mit den frischen metzen,", "tokens": ["die", "Wel\u00b7schen", "mit", "den", "fri\u00b7schen", "met\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "die Th\u00fcringer mit bier und wein", "tokens": ["die", "Th\u00fc\u00b7rin\u00b7ger", "mit", "bier", "und", "wein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE", "KON", "NN"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.5": {"text": "in ihrer h\u00fcbschen frauen namen", "tokens": ["in", "ih\u00b7rer", "h\u00fcb\u00b7schen", "frau\u00b7en", "na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "ein jeder fr\u00f6lich, frisch herum", "tokens": ["ein", "je\u00b7der", "fr\u00f6\u00b7lich", ",", "frisch", "he\u00b7rum"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PIAT", "ADJD", "$,", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "sing, spring und drink, und allzusamen.", "tokens": ["sing", ",", "spring", "und", "drink", ",", "und", "all\u00b7zu\u00b7sa\u00b7men", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "KON", "ADJD", "$,", "KON", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "k\u00fc\u00df mich widrum!", "tokens": ["k\u00fc\u00df", "mich", "wid\u00b7rum", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.14": {"line.1": {"text": "Nu schenk uns ein den gro\u00dfen becher,", "tokens": ["Nu", "schenk", "uns", "ein", "den", "gro\u00b7\u00dfen", "be\u00b7cher", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "schenk voll! so! so! ihr liebe freind,", "tokens": ["schenk", "voll", "!", "so", "!", "so", "!", "ihr", "lie\u00b7be", "freind", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "$.", "ADV", "$.", "ADV", "$.", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ein jeder guter zecher, stecher", "tokens": ["ein", "je\u00b7der", "gu\u00b7ter", "ze\u00b7cher", ",", "ste\u00b7cher"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "PIAT", "ADJA", "ADJA", "$,", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "so oft, als vil buchstaben seind", "tokens": ["so", "oft", ",", "als", "vil", "buch\u00b7sta\u00b7ben", "seind"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "KOUS", "PIS", "VVINF", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "in seines lieben stechblats namen,", "tokens": ["in", "sei\u00b7nes", "lie\u00b7ben", "stech\u00b7blats", "na\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "hie disen ganz abdrinken soll;", "tokens": ["hie", "di\u00b7sen", "ganz", "ab\u00b7drin\u00b7ken", "soll", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "ich neunmal, rechnet ihr zusamen.", "tokens": ["ich", "neun\u00b7mal", ",", "rech\u00b7net", "ihr", "zu\u00b7sa\u00b7men", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "es gilt ganz voll.", "tokens": ["es", "gilt", "ganz", "voll", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.15": {"line.1": {"text": "Wol! hat ein jeder abgedrunken?", "tokens": ["Wol", "!", "hat", "ein", "je\u00b7der", "ab\u00b7ge\u00b7drun\u00b7ken", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "VAFIN", "ART", "PIS", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "drei, f\u00fcnf, sechs, siben, zehenmal?", "tokens": ["drei", ",", "f\u00fcnf", ",", "sechs", ",", "si\u00b7ben", ",", "ze\u00b7hen\u00b7mal", "?"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["CARD", "$,", "CARD", "$,", "CARD", "$,", "VVFIN", "$,", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ist dises k\u00e4s, fisch oder schunken?", "tokens": ["ist", "di\u00b7ses", "k\u00e4s", ",", "fisch", "o\u00b7der", "schun\u00b7ken", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "VVFIN", "$,", "ADJD", "KON", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "ist dises pferd grau oder fahl?", "tokens": ["ist", "di\u00b7ses", "pferd", "grau", "o\u00b7der", "fahl", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADJD", "KON", "VVFIN", "$."], "meter": "+---+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "darauf ich schwitz? gib her die flaschen!", "tokens": ["da\u00b7rauf", "ich", "schwitz", "?", "gib", "her", "die", "fla\u00b7schen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADJD", "$.", "VVIMP", "ADV", "ART", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "es gilt herr Grey, herr Gro, Gro, Groll!", "tokens": ["es", "gilt", "herr", "Grey", ",", "herr", "Gro", ",", "Gro", ",", "Groll", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "NE", "$,", "NN", "NE", "$,", "NE", "$,", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "so dise w\u00e4sch wird wol gewaschen!", "tokens": ["so", "di\u00b7se", "w\u00e4sch", "wird", "wol", "ge\u00b7wa\u00b7schen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJD", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "seid ihr all doll?", "tokens": ["seid", "ihr", "all", "doll", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "ADJD", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.16": {"line.1": {"text": "Ho! seind das reuter oder mucken?", "tokens": ["Ho", "!", "seind", "das", "reu\u00b7ter", "o\u00b7der", "mu\u00b7cken", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VAFIN", "ART", "ADJA", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "buff, buff! es ist ein hafenk\u00e4s", "tokens": ["buff", ",", "buff", "!", "es", "ist", "ein", "ha\u00b7fen\u00b7k\u00e4s"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKVZ", "$,", "NE", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "zu zucken, schmucken, schlucken, drucken.", "tokens": ["zu", "zu\u00b7cken", ",", "schmu\u00b7cken", ",", "schlu\u00b7cken", ",", "dru\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "warum ist doch der A. das gs\u00e4\u00df?", "tokens": ["wa\u00b7rum", "ist", "doch", "der", "A.", "das", "gs\u00e4\u00df", "?"], "token_info": ["word", "word", "word", "word", "abbreviation", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ADV", "ART", "APPRART", "PDS", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "pfui dich! k\u00fc\u00df mich! thust du da schmecken?", "tokens": ["pfui", "dich", "!", "k\u00fc\u00df", "mich", "!", "thust", "du", "da", "schme\u00b7cken", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PPER", "$.", "VVFIN", "PPER", "$.", "VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "wer zornig ist, der ist ein lump!", "tokens": ["wer", "zor\u00b7nig", "ist", ",", "der", "ist", "ein", "lump", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$,", "PRELS", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "hei ho! das ding die z\u00e4hn thut blecken.", "tokens": ["hei", "ho", "!", "das", "ding", "die", "z\u00e4hn", "thut", "ble\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ITJ", "$.", "ART", "NN", "ART", "NN", "VVFIN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.8": {"text": "bump bidi bump.", "tokens": ["bump", "bi\u00b7di", "bump", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.17": {"line.1": {"text": "Ha! duck den kopf! schei\u00df, bei\u00df, meerwunder.", "tokens": ["Ha", "!", "duck", "den", "kopf", "!", "schei\u00df", ",", "bei\u00df", ",", "meer\u00b7wun\u00b7der", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ITJ", "$.", "VVIMP", "ART", "NN", "$.", "VVFIN", "$,", "VVFIN", "$,", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "nu brauset, sauset laut das meer.", "tokens": ["nu", "brau\u00b7set", ",", "sau\u00b7set", "laut", "das", "meer", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ein regen, hagel, blitz und dunder.", "tokens": ["ein", "re\u00b7gen", ",", "ha\u00b7gel", ",", "blitz", "und", "dun\u00b7der", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "NE", "$,", "VVIMP", "KON", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "hei, von heuschrecken ein kriegsheer!", "tokens": ["hei", ",", "von", "heu\u00b7schre\u00b7cken", "ein", "kriegs\u00b7heer", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKVZ", "$,", "APPR", "ADV", "ART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "ho! schlag den elefanten nider.", "tokens": ["ho", "!", "schlag", "den", "e\u00b7lef\u00b7an\u00b7ten", "ni\u00b7der", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "ART", "ADJA", "PTKVZ", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.6": {"text": "es ist ein stork! ha, nein, ein laus.", "tokens": ["es", "ist", "ein", "stork", "!", "ha", ",", "nein", ",", "ein", "laus", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJD", "$.", "ITJ", "$,", "PTKANT", "$,", "ART", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "gl\u00fcck zu! gut nacht! kom, k\u00fc\u00df mich wider.", "tokens": ["gl\u00fcck", "zu", "!", "gut", "nacht", "!", "kom", ",", "k\u00fc\u00df", "mich", "wi\u00b7der", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "$.", "ADJD", "NN", "$.", "VVFIN", "$,", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.8": {"text": "das liecht ist aus.", "tokens": ["das", "liecht", "ist", "aus", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.18": {"line.1": {"text": "Alsdan vergessend mehr zu drinken", "tokens": ["Als\u00b7dan", "ver\u00b7ges\u00b7send", "mehr", "zu", "drin\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVPP", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "sah man die vier, wie fromme schaf,", "tokens": ["sah", "man", "die", "vier", ",", "wie", "from\u00b7me", "schaf", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "CARD", "$,", "PWAV", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "zu grund und auf die b\u00e4nk hinsinken,", "tokens": ["zu", "grund", "und", "auf", "die", "b\u00e4nk", "hin\u00b7sin\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "beschlie\u00dfend ihre freud mit schlaf.", "tokens": ["be\u00b7schlie\u00b7\u00dfend", "ih\u00b7re", "freud", "mit", "schlaf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPOSAT", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "und indem sie die zeit vertriben,", "tokens": ["und", "in\u00b7dem", "sie", "die", "zeit", "ver\u00b7tri\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "hat diesen seiner freinden chor", "tokens": ["hat", "die\u00b7sen", "sei\u00b7ner", "frein\u00b7den", "chor"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PDAT", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "alsbald auf dise weis beschriben", "tokens": ["als\u00b7bald", "auf", "di\u00b7se", "weis", "be\u00b7schri\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PDAT", "PTKVZ", "VVINF"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "ihr Filodor.", "tokens": ["ihr", "Fi\u00b7lo\u00b7dor", "."], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}