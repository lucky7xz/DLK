{"dta.poem.2355": {"metadata": {"author": {"name": "Neumark, Georg", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1652", "urn": "urn:nbn:de:kobv:b4-20428-0", "language": ["de:0.99"], "booktitle": "Neumark, Georg: Poetisch- und Musikalisches Lustw\u00e4ldchen. Hamburg, 1652."}, "poem": {"stanza.1": {"line.1": {"text": "Ich habe Mich mein Gott mit Mir besprochen/", "tokens": ["Ich", "ha\u00b7be", "Mich", "mein", "Gott", "mit", "Mir", "be\u00b7spro\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PPOSAT", "NN", "APPR", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df Ich der Welt ihr Gut-sein oder Pochen", "tokens": ["Da\u00df", "Ich", "der", "Welt", "ihr", "Gut\u00b7sein", "o\u00b7der", "Po\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ins k\u00fcnfftige/ Dich liebend/ fromm und still/", "tokens": ["Ins", "k\u00fcnff\u00b7ti\u00b7ge", "/", "Dich", "lie\u00b7bend", "/", "fromm", "und", "still", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$(", "PPER", "ADJD", "$(", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Nicht achten wil.", "tokens": ["Nicht", "ach\u00b7ten", "wil", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Weg schn\u00f6de Welt mit allen deinen Dingen/", "tokens": ["Weg", "schn\u00f6\u00b7de", "Welt", "mit", "al\u00b7len", "dei\u00b7nen", "Din\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "APPR", "PIAT", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wodurch Du offt den Menschen pflegst zu bringen", "tokens": ["Wo\u00b7durch", "Du", "offt", "den", "Men\u00b7schen", "pflegst", "zu", "brin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "ART", "NN", "VVFIN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "In gro\u00dfes Leid/ ja offtmals in den Tod", "tokens": ["In", "gro\u00b7\u00dfes", "Leid", "/", "ja", "offt\u00b7mals", "in", "den", "Tod"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$(", "ADV", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und Seelennoht.", "tokens": ["Und", "See\u00b7len\u00b7noht", "."], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Mein sag/ was sind doch alle deine G\u00fcter?", "tokens": ["Mein", "sag", "/", "was", "sind", "doch", "al\u00b7le", "dei\u00b7ne", "G\u00fc\u00b7ter", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PWS", "VAFIN", "ADV", "PIS", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nichts anders ja als Netze der Gem\u00fchter;", "tokens": ["Nichts", "an\u00b7ders", "ja", "als", "Net\u00b7ze", "der", "Ge\u00b7m\u00fch\u00b7ter", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ADV", "KOUS", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Was ist dein Geld/ dein Gold und Heuser auch?", "tokens": ["Was", "ist", "dein", "Geld", "/", "dein", "Gold", "und", "Heu\u00b7ser", "auch", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPOSAT", "NN", "$(", "PPOSAT", "NN", "KON", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Nur Wind und Rauch.", "tokens": ["Nur", "Wind", "und", "Rauch", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "So bald der Zorn de\u00df HErrn dar\u00fcber wehet", "tokens": ["So", "bald", "der", "Zorn", "de\u00df", "Herrn", "da\u00b7r\u00fc\u00b7ber", "we\u00b7het"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "NN", "ART", "NN", "PAV", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo bleibt es denn? So bald ein Krieg entstehet/", "tokens": ["Wo", "bleibt", "es", "denn", "?", "So", "bald", "ein", "Krieg", "ent\u00b7ste\u00b7het", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "$.", "ADV", "ADV", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So bald ein Feur da\u00dfelbe nur erhascht/", "tokens": ["So", "bald", "ein", "Feur", "da\u00b7\u00dfel\u00b7be", "nur", "er\u00b7hascht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "PDAT", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So ists verascht.", "tokens": ["So", "ists", "ver\u00b7ascht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "War Hiob nicht ein Mann von vielen Geldern?", "tokens": ["War", "Hiob", "nicht", "ein", "Mann", "von", "vie\u00b7len", "Gel\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "PTKNEG", "ART", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Von gro\u00dfere Gut\u2019 und reichbegabten Feldern?", "tokens": ["Von", "gro\u00b7\u00dfe\u00b7re", "Gut'", "und", "reich\u00b7be\u00b7gab\u00b7ten", "Fel\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Jhm war kein Mensch/ und war Er noch so reich/", "tokens": ["Jhm", "war", "kein", "Mensch", "/", "und", "war", "Er", "noch", "so", "reich", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$(", "KON", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "An G\u00fctern gleich.", "tokens": ["An", "G\u00fc\u00b7tern", "gleich", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "In einem Huj verderben seine Kinder/", "tokens": ["In", "ei\u00b7nem", "Huj", "ver\u00b7der\u00b7ben", "sei\u00b7ne", "Kin\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Es gehet weg sein Haub/ Hof/ Vieh/ und Rinder/", "tokens": ["Es", "ge\u00b7het", "weg", "sein", "Haub", "/", "Hof", "/", "Vieh", "/", "und", "Rin\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$(", "NE", "$(", "NN", "$(", "KON", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Er wird gar bald/ eh Er es d\u00e4nken kan/", "tokens": ["Er", "wird", "gar", "bald", "/", "eh", "Er", "es", "d\u00e4n\u00b7ken", "kan", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$(", "KOUS", "PPER", "PPER", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein armer Mann.", "tokens": ["Ein", "ar\u00b7mer", "Mann", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "Der Dioni\u00df sa\u00df in den h\u00f6chsten Ehren", "tokens": ["Der", "Dio\u00b7ni\u00df", "sa\u00df", "in", "den", "h\u00f6chs\u00b7ten", "Eh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu Syraku\u00df\u2019/ und muste Kinder lehren/", "tokens": ["Zu", "Sy\u00b7ra\u00b7ku\u00df'", "/", "und", "mus\u00b7te", "Kin\u00b7der", "leh\u00b7ren", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "KON", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Dort zu Korinth/ hernach aus gro\u00dfer Noht", "tokens": ["Dort", "zu", "Ko\u00b7rinth", "/", "her\u00b7nach", "aus", "gro\u00b7\u00dfer", "Noht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NE", "$(", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ums liebe Brodt.", "tokens": ["Ums", "lie\u00b7be", "Brodt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUI", "ADJA", "NN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.8": {"line.1": {"text": "Du Prahler h\u00f6r/ was hilfft dich doch dein Stutzen?", "tokens": ["Du", "Prah\u00b7ler", "h\u00f6r", "/", "was", "hilfft", "dich", "doch", "dein", "Stut\u00b7zen", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVFIN", "$(", "PWS", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Was hilfft Dich doch dein Gottvergesnes Putzen?", "tokens": ["Was", "hilfft", "Dich", "doch", "dein", "Gott\u00b7ver\u00b7ges\u00b7nes", "Put\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Du tritst herein/ und d\u00e4nkest nicht einmal", "tokens": ["Du", "tritst", "her\u00b7ein", "/", "und", "d\u00e4n\u00b7kest", "nicht", "ein\u00b7mal"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKVZ", "$(", "KON", "VVFIN", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "An jene Quahl.", "tokens": ["An", "je\u00b7ne", "Quahl", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Du lebest wol. Was aber d\u00e4nkt in dessen", "tokens": ["Du", "le\u00b7best", "wol", ".", "Was", "a\u00b7ber", "d\u00e4nkt", "in", "des\u00b7sen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "$.", "PWS", "ADV", "VVFIN", "APPR", "PRELAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dein arme Seel\u2019? als welcher Du vergessen/", "tokens": ["Dein", "ar\u00b7me", "Seel'", "?", "als", "wel\u00b7cher", "Du", "ver\u00b7ges\u00b7sen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$.", "KOUS", "PRELS", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Du eitler Mensch? Sie klagt ihr Ungemach", "tokens": ["Du", "eit\u00b7ler", "Mensch", "?", "Sie", "klagt", "ihr", "Un\u00b7ge\u00b7mach"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mit Weh und Ach.", "tokens": ["Mit", "Weh", "und", "Ach", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.10": {"line.1": {"text": "Sie ist bem\u00fcht Dein b\u00f6ses Thun zu stillen/", "tokens": ["Sie", "ist", "be\u00b7m\u00fcht", "Dein", "b\u00f6\u00b7ses", "Thun", "zu", "stil\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVFIN", "PPOSAT", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie h\u00e4lt Dir vor de\u00df frommen Gottes Willen/", "tokens": ["Sie", "h\u00e4lt", "Dir", "vor", "de\u00df", "from\u00b7men", "Got\u00b7tes", "Wil\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Dir aber ists und deiner frechen Rott\u2019", "tokens": ["Dir", "a\u00b7ber", "ists", "und", "dei\u00b7ner", "fre\u00b7chen", "Rott'"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VAFIN", "KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein lautrer Spott.", "tokens": ["Ein", "laut\u00b7rer", "Spott", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Sie klagt Dich an mit bittren hei\u00dfen Z\u00e4hren/", "tokens": ["Sie", "klagt", "Dich", "an", "mit", "bit\u00b7tren", "hei\u00b7\u00dfen", "Z\u00e4h\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "APPR", "ADJA", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sie \u00e4chtzt und seuftzt/ und wil sich gern erwehren/", "tokens": ["Sie", "\u00e4chtzt", "und", "seuftzt", "/", "und", "wil", "sich", "gern", "er\u00b7weh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "$(", "KON", "VMFIN", "PRF", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Zugleich mit Dir zugehn in jenes Leid/", "tokens": ["Zu\u00b7gleich", "mit", "Dir", "zu\u00b7gehn", "in", "je\u00b7nes", "Leid", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "VVPP", "APPR", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Der Ewigkeit.", "tokens": ["Der", "E\u00b7wig\u00b7keit", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.12": {"line.1": {"text": "Bedenke Mensch/ was sind doch Herreng\u00fcnste?", "tokens": ["Be\u00b7den\u00b7ke", "Mensch", "/", "was", "sind", "doch", "Her\u00b7ren\u00b7g\u00fcns\u00b7te", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "PWS", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ja anders nichts/ als Rauch und leere D\u00fcnste/", "tokens": ["Ja", "an\u00b7ders", "nichts", "/", "als", "Rauch", "und", "lee\u00b7re", "D\u00fcns\u00b7te", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "ADV", "PIS", "$(", "KOUS", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So diesen gleich/ so bald Sie fast entstehn/", "tokens": ["So", "die\u00b7sen", "gleich", "/", "so", "bald", "Sie", "fast", "ent\u00b7stehn", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADV", "$(", "ADV", "ADV", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Auch bald vergehn.", "tokens": ["Auch", "bald", "ver\u00b7gehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.13": {"line.1": {"text": "Stundt\u2019 Haman dort nicht auf der Ehrenspitzen?", "tokens": ["Stundt'", "Ha\u00b7man", "dort", "nicht", "auf", "der", "Eh\u00b7ren\u00b7spit\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "PTKNEG", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Must\u2019 Er nicht stets bey Ahasverus sitzen?", "tokens": ["Must'", "Er", "nicht", "stets", "bey", "A\u00b7has\u00b7ve\u00b7rus", "sit\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKNEG", "ADV", "APPR", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wie gieng es ihm? Eh es ein Mensche d\u00e4nkt", "tokens": ["Wie", "gieng", "es", "ihm", "?", "Eh", "es", "ein", "Men\u00b7sche", "d\u00e4nkt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "PPER", "$.", "KOUS", "PPER", "ART", "NN", "VVFIN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Wird Er erhenkt.", "tokens": ["Wird", "Er", "er\u00b7henkt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.14": {"line.1": {"text": "Wer war doch wol dem gro\u00dfen Griechen lieber", "tokens": ["Wer", "war", "doch", "wol", "dem", "gro\u00b7\u00dfen", "Grie\u00b7chen", "lie\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "ART", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Als Klitus du? An Gunst war keiner dr\u00fcber.", "tokens": ["Als", "Kli\u00b7tus", "du", "?", "An", "Gunst", "war", "kei\u00b7ner", "dr\u00fc\u00b7ber", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "$.", "APPR", "NN", "VAFIN", "PIS", "PAV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Dennoch hat Er/ nach dem Er sich ergretzt/", "tokens": ["Den\u00b7noch", "hat", "Er", "/", "nach", "dem", "Er", "sich", "er\u00b7gretzt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$(", "APPR", "PRELS", "PPER", "PRF", "VVPP", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dich hingemetzt.", "tokens": ["Dich", "hin\u00b7ge\u00b7metzt", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.15": {"line.1": {"text": "Weg Wollust/ weg mit deinen leichten Sinnen!", "tokens": ["Weg", "Wol\u00b7lust", "/", "weg", "mit", "dei\u00b7nen", "leich\u00b7ten", "Sin\u00b7nen", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$(", "ADV", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Weg Uppigkeit mit deinem Schandbeginn en!", "tokens": ["Weg", "Up\u00b7pig\u00b7keit", "mit", "dei\u00b7nem", "Schand\u00b7be\u00b7ginn", "en", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wie mancher Mensch ist doch von Dir betr\u00fcbt/", "tokens": ["Wie", "man\u00b7cher", "Mensch", "ist", "doch", "von", "Dir", "be\u00b7tr\u00fcbt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "ADV", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Der Dich beliebt.", "tokens": ["Der", "Dich", "be\u00b7liebt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.16": {"line.1": {"text": "Ist Faon nicht in geiler Lust gestorben?", "tokens": ["Ist", "Faon", "nicht", "in", "gei\u00b7ler", "Lust", "ge\u00b7stor\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "PTKNEG", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Was hat doch sonst dich Tigellin verdorben/", "tokens": ["Was", "hat", "doch", "sonst", "dich", "Ti\u00b7gel\u00b7lin", "ver\u00b7dor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "PPER", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Als Prasserey/ und dein beliebter Wust/", "tokens": ["Als", "Pras\u00b7se\u00b7rey", "/", "und", "dein", "be\u00b7lieb\u00b7ter", "Wust", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "$(", "KON", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die Liebeslust.", "tokens": ["Die", "Lie\u00b7bes\u00b7lust", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.17": {"line.1": {"text": "Die Sch\u00f6nheit ist ein Lokkaas vieler S\u00fcnden/", "tokens": ["Die", "Sch\u00f6n\u00b7heit", "ist", "ein", "Lok\u00b7kaas", "vie\u00b7ler", "S\u00fcn\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ein Ungl\u00fckssee den man kaum kan ergr\u00fcnden.", "tokens": ["Ein", "Un\u00b7gl\u00fcks\u00b7see", "den", "man", "kaum", "kan", "er\u00b7gr\u00fcn\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "PIS", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wie manches Leid/ wenn man es recht erwegt/", "tokens": ["Wie", "man\u00b7ches", "Leid", "/", "wenn", "man", "es", "recht", "er\u00b7wegt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "$(", "KOUS", "PIS", "PPER", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Hat Sie erregt.", "tokens": ["Hat", "Sie", "er\u00b7regt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.18": {"line.1": {"text": "Wie manches Leid hat Sie doch angerichtet?", "tokens": ["Wie", "man\u00b7ches", "Leid", "hat", "Sie", "doch", "an\u00b7ge\u00b7rich\u00b7tet", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wie mancher Mensch ist doch durch sie vernichtet?", "tokens": ["Wie", "man\u00b7cher", "Mensch", "ist", "doch", "durch", "sie", "ver\u00b7nich\u00b7tet", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "ADV", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Schonheit hat so manches Land verheert/", "tokens": ["Die", "Schon\u00b7heit", "hat", "so", "man\u00b7ches", "Land", "ver\u00b7heert", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und gantz zerst\u00f6hrt.", "tokens": ["Und", "gantz", "zer\u00b7st\u00f6hrt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.19": {"line.1": {"text": "Wenn Bathseba mit ihren zarten Wangen", "tokens": ["Wenn", "Bath\u00b7se\u00b7ba", "mit", "ih\u00b7ren", "zar\u00b7ten", "Wan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "NE", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und Leibesziehr den David nicht gefangen/", "tokens": ["Und", "Lei\u00b7bes\u00b7ziehr", "den", "Da\u00b7vid", "nicht", "ge\u00b7fan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "NE", "PTKNEG", "ADJD", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "So hett\u2019 Er nicht fast Gottes Gunst verschertzt/", "tokens": ["So", "hett'", "Er", "nicht", "fast", "Got\u00b7tes", "Gunst", "ver\u00b7schertzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADV", "NN", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Welchs Jhn geschmertzt.", "tokens": ["Welchs", "Jhn", "ge\u00b7schmertzt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.20": {"line.1": {"text": "Wenn jener Held Antonius die Sinnen", "tokens": ["Wenn", "je\u00b7ner", "Held", "An\u00b7to\u00b7ni\u00b7us", "die", "Sin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "NN", "NE", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nicht gantz gericht zur sch\u00f6nen Aegyptinnen/", "tokens": ["Nicht", "gantz", "ge\u00b7richt", "zur", "sch\u00f6\u00b7nen", "A\u00b7e\u00b7gyp\u00b7tin\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ADJD", "APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "So hett\u2019 Er ihm sein Leben nicht verk\u00fcrtzt/", "tokens": ["So", "hett'", "Er", "ihm", "sein", "Le\u00b7ben", "nicht", "ver\u00b7k\u00fcrtzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "PPOSAT", "NN", "PTKNEG", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und sich gest\u00fcrtzt.", "tokens": ["Und", "sich", "ge\u00b7st\u00fcrtzt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PRF", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.21": {"line.1": {"text": "Da\u00df Briechenland zehn Jahre lang bekrieget/", "tokens": ["Da\u00df", "Brie\u00b7chen\u00b7land", "zehn", "Jah\u00b7re", "lang", "be\u00b7krie\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "CARD", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df Troja dort so gantz ver\u00f6det lieget/", "tokens": ["Da\u00df", "Tro\u00b7ja", "dort", "so", "gantz", "ver\u00b7\u00f6\u00b7det", "lie\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADV", "ADV", "ADV", "VVPP", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "War nur allein Helene Schuld daran/", "tokens": ["War", "nur", "al\u00b7lein", "He\u00b7le\u00b7ne", "Schuld", "da\u00b7ran", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "NE", "NN", "PAV", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Die hats gethan.", "tokens": ["Die", "hats", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.22": {"line.1": {"text": "Kurtz/ was der Mensch vor Hoch und Treflich sch\u00e4tzet/", "tokens": ["Kurtz", "/", "was", "der", "Mensch", "vor", "Hoch", "und", "Tref\u00b7lich", "sch\u00e4t\u00b7zet", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$(", "PWS", "ART", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Womit Er sich nach Hertzenslust ergetzet/", "tokens": ["Wo\u00b7mit", "Er", "sich", "nach", "Hert\u00b7zens\u00b7lust", "er\u00b7get\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ist alles doch in dieser Lebenszeit/", "tokens": ["Ist", "al\u00b7les", "doch", "in", "die\u00b7ser", "Le\u00b7bens\u00b7zeit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "ADV", "APPR", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Nur Eitelkeit.", "tokens": ["Nur", "Ei\u00b7tel\u00b7keit", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.23": {"line.1": {"text": "Was hilfft es Jhn wenn Er einmal gestorben/", "tokens": ["Was", "hilfft", "es", "Jhn", "wenn", "Er", "ein\u00b7mal", "ge\u00b7stor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PPER", "KOUS", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Jmfall sein Leib durch einen Tod verdorben/", "tokens": ["Jm\u00b7fall", "sein", "Leib", "durch", "ei\u00b7nen", "Tod", "ver\u00b7dor\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "APPR", "ART", "NN", "VVPP", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Der Madensakk? Es wird ein Grab und Stein", "tokens": ["Der", "Ma\u00b7den\u00b7sakk", "?", "Es", "wird", "ein", "Grab", "und", "Stein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$.", "PPER", "VAFIN", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sein Reichthum seyn.", "tokens": ["Sein", "Reicht\u00b7hum", "seyn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.24": {"line.1": {"text": "Hat Er  sich denn in Tugend nicht ge\u00fcbet/", "tokens": ["Hat", "Er", "sich", "denn", "in", "Tu\u00b7gend", "nicht", "ge\u00b7\u00fc\u00b7bet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADV", "APPR", "NN", "PTKNEG", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und Gottes Wort von Hertzen hochbeliebet/", "tokens": ["Und", "Got\u00b7tes", "Wort", "von", "Hert\u00b7zen", "hoch\u00b7be\u00b7lie\u00b7bet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ach Weh! Ach Weh! wie wird es Jhm doch gehn", "tokens": ["Ach", "Weh", "!", "Ach", "Weh", "!", "wie", "wird", "es", "Jhm", "doch", "gehn"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "NN", "$.", "ITJ", "NN", "$.", "PWAV", "VAFIN", "PPER", "PPER", "ADV", "VVINF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Jm Aufferstehn.", "tokens": ["Jm", "Auf\u00b7fer\u00b7stehn", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.25": {"line.1": {"text": "Der Himmelsf\u00fcrst/ der Richter aller Seelen/", "tokens": ["Der", "Him\u00b7mels\u00b7f\u00fcrst", "/", "der", "Rich\u00b7ter", "al\u00b7ler", "See\u00b7len", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "NN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wird Jhn mit Ernst mang die Verdampten z\u00e4hlen/", "tokens": ["Wird", "Jhn", "mit", "Ernst", "mang", "die", "Ver\u00b7damp\u00b7ten", "z\u00e4h\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NE", "VVFIN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Da wird Er stehn zu seiner linken Hand", "tokens": ["Da", "wird", "Er", "stehn", "zu", "sei\u00b7ner", "lin\u00b7ken", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mit Spott und Schand.", "tokens": ["Mit", "Spott", "und", "Schand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.26": {"line.1": {"text": "Der Teufel selbst wird seyn sein Mitgeselle/", "tokens": ["Der", "Teu\u00b7fel", "selbst", "wird", "seyn", "sein", "Mit\u00b7ge\u00b7sel\u00b7le", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "PPOSAT", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das hellsche Feur wird seyn an Goldes Stelle/", "tokens": ["Das", "hell\u00b7sche", "Feur", "wird", "seyn", "an", "Gol\u00b7des", "Stel\u00b7le", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPOSAT", "APPR", "NN", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ein greulich Loch/ und schwefelichter Pfuhl", "tokens": ["Ein", "greu\u00b7lich", "Loch", "/", "und", "schwe\u00b7fe\u00b7lich\u00b7ter", "Pfuhl"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "$(", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wird seyn sein Stuhl.", "tokens": ["Wird", "seyn", "sein", "Stuhl", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VAINF", "PPOSAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.27": {"line.1": {"text": "Da mu\u00df Er denn die unerh\u00f6rten Plagen/", "tokens": ["Da", "mu\u00df", "Er", "denn", "die", "un\u00b7er\u00b7h\u00f6r\u00b7ten", "Pla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mit Ach und Weh in Ewigkeit ertragen/", "tokens": ["Mit", "Ach", "und", "Weh", "in", "E\u00b7wig\u00b7keit", "er\u00b7tra\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "In Ewigkeit wird keine Rettung seyn", "tokens": ["In", "E\u00b7wig\u00b7keit", "wird", "kei\u00b7ne", "Ret\u00b7tung", "seyn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VAFIN", "PIAT", "NN", "VAINF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "von dieser Pein.", "tokens": ["von", "die\u00b7ser", "Pein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.28": {"line.1": {"text": "Dr\u00fcm lenk/ Ach Gott/ Ach lenke meine Sinnen/", "tokens": ["Dr\u00fcm", "lenk", "/", "Ach", "Gott", "/", "Ach", "len\u00b7ke", "mei\u00b7ne", "Sin\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "ITJ", "NN", "$(", "ITJ", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da\u00df Sie sonst nichts als Dich nur lieb gewinnen/", "tokens": ["Da\u00df", "Sie", "sonst", "nichts", "als", "Dich", "nur", "lieb", "ge\u00b7win\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PIS", "KOKOM", "PPER", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Gieb Deinen Geist/ Der mich auf rechter Bahn/", "tokens": ["Gieb", "Dei\u00b7nen", "Geist", "/", "Der", "mich", "auf", "rech\u00b7ter", "Bahn", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "NN", "$(", "ART", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Weis\u2019 Himmel-an.", "tokens": ["Weis'", "Him\u00b7mel\u00b7an", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NE", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.29": {"line.1": {"text": "Der \u00fcber Mich mit seinen Gaben walte/", "tokens": ["Der", "\u00fc\u00b7ber", "Mich", "mit", "sei\u00b7nen", "Ga\u00b7ben", "wal\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der mein Gem\u00fcht\u2019 in deiner Lieb\u2019 erhalte/", "tokens": ["Der", "mein", "Ge\u00b7m\u00fcht'", "in", "dei\u00b7ner", "Lieb'", "er\u00b7hal\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Der meinen Sinn zur Gottesf\u00fcrchtigkeit/", "tokens": ["Der", "mei\u00b7nen", "Sinn", "zur", "Got\u00b7tes\u00b7f\u00fcrch\u00b7tig\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Lenk\u2019 allezeit.", "tokens": ["Lenk'", "al\u00b7le\u00b7zeit", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.30": {"line.1": {"text": "Ach Gott/ schl\u00e4gt mich ein Fehler etwa nieder/", "tokens": ["Ach", "Gott", "/", "schl\u00e4gt", "mich", "ein", "Feh\u00b7ler", "et\u00b7wa", "nie\u00b7der", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "$(", "VVFIN", "PPER", "ART", "NN", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "So heb Mich auf und tr\u00f6ste Mich doch wieder/", "tokens": ["So", "heb", "Mich", "auf", "und", "tr\u00f6s\u00b7te", "Mich", "doch", "wie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "PPER", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Reitzt mich die S\u00fcnd- und bringt mich in ihr Joch/", "tokens": ["Reitzt", "mich", "die", "S\u00fcn\u00b7d", "und", "bringt", "mich", "in", "ihr", "Joch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "TRUNC", "KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$("], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "So schone doch.", "tokens": ["So", "scho\u00b7ne", "doch", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.31": {"line.1": {"text": "La\u00df dein Gericht nicht \u00fcber mich ergehen/", "tokens": ["La\u00df", "dein", "Ge\u00b7richt", "nicht", "\u00fc\u00b7ber", "mich", "er\u00b7ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "NN", "PTKNEG", "APPR", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wer kan doch HErr/ wer kan doch vor Dir stehen?", "tokens": ["Wer", "kan", "doch", "Herr", "/", "wer", "kan", "doch", "vor", "Dir", "ste\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "NN", "$(", "PWS", "VMFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Kein Mensche nicht. La\u00df deiner Gnaden Schein", "tokens": ["Kein", "Men\u00b7sche", "nicht", ".", "La\u00df", "dei\u00b7ner", "Gna\u00b7den", "Schein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "PTKNEG", "$.", "VVIMP", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mein Beystand seyn.", "tokens": ["Mein", "Beys\u00b7tand", "seyn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.32": {"line.1": {"text": "Lenk so mein Hertz in meinem gantzen Leben/", "tokens": ["Lenk", "so", "mein", "Hertz", "in", "mei\u00b7nem", "gant\u00b7zen", "Le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PPOSAT", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Da\u00df es nichts woll\u2019 als Dir nur seyn ergeben/", "tokens": ["Da\u00df", "es", "nichts", "woll'", "als", "Dir", "nur", "seyn", "er\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "VMFIN", "KOUS", "PPER", "ADV", "PPOSAT", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und wenn ich denn vollendet meinen Lauff/", "tokens": ["Und", "wenn", "ich", "denn", "voll\u00b7en\u00b7det", "mei\u00b7nen", "Lauff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "VVPP", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So nim mich auf.", "tokens": ["So", "nim", "mich", "auf", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}