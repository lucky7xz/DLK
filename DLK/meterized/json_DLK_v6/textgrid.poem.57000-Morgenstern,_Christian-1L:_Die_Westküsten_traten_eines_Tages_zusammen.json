{"textgrid.poem.57000": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Die Westk\u00fcsten traten eines Tages zusammen", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die Westk\u00fcsten traten eines Tages zusammen", "tokens": ["Die", "West\u00b7k\u00fcs\u00b7ten", "tra\u00b7ten", "ei\u00b7nes", "Ta\u00b7ges", "zu\u00b7sam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "PTKVZ"], "meter": "-++-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "und erkl\u00e4rten, sie seien keine Westk\u00fcsten,", "tokens": ["und", "er\u00b7kl\u00e4r\u00b7ten", ",", "sie", "sei\u00b7en", "kei\u00b7ne", "West\u00b7k\u00fcs\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PPER", "VAFIN", "PIAT", "NN", "$,"], "meter": "--+--+-+-++-", "measure": "anapaest.di.plus"}, "line.3": {"text": "weder Ostk\u00fcsten noch Westk\u00fcsten \u2013", "tokens": ["we\u00b7der", "Ost\u00b7k\u00fcs\u00b7ten", "noch", "West\u00b7k\u00fcs\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "NN", "$("], "meter": "+-++--++-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "\u00bbda\u00df sie nicht w\u00fc\u00dften!\u00ab", "tokens": ["\u00bb", "da\u00df", "sie", "nicht", "w\u00fc\u00df\u00b7ten", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KOUS", "PPER", "PTKNEG", "VVFIN", "$.", "$("], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Sie wollten wieder ihre Freiheit haben", "tokens": ["Sie", "woll\u00b7ten", "wie\u00b7der", "ih\u00b7re", "Frei\u00b7heit", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "PPOSAT", "NN", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und f\u00fcr immer das Joch des Namens absch\u00fctteln,", "tokens": ["und", "f\u00fcr", "im\u00b7mer", "das", "Joch", "des", "Na\u00b7mens", "ab\u00b7sch\u00fct\u00b7teln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADV", "ART", "NN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.3": {"text": "womit eine Horde von Menschenb\u00fctteln", "tokens": ["wo\u00b7mit", "ei\u00b7ne", "Hor\u00b7de", "von", "Men\u00b7schen\u00b7b\u00fct\u00b7teln"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "APPR", "NN"], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.4": {"text": "sich angema\u00dft habe, sie zu begaben.", "tokens": ["sich", "an\u00b7ge\u00b7ma\u00dft", "ha\u00b7be", ",", "sie", "zu", "be\u00b7ga\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PRF", "VVPP", "VAFIN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Doch wie sich befreien, wie sich erretten", "tokens": ["Doch", "wie", "sich", "be\u00b7frei\u00b7en", ",", "wie", "sich", "er\u00b7ret\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PWAV", "PRF", "VVINF", "$,", "PWAV", "PRF", "VVINF"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "aus diesen widerw\u00e4rtigen Ketten?", "tokens": ["aus", "die\u00b7sen", "wi\u00b7der\u00b7w\u00e4r\u00b7ti\u00b7gen", "Ket\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ihr Westk\u00fcsten, fing eine an zu spotten,", "tokens": ["Ihr", "West\u00b7k\u00fcs\u00b7ten", ",", "fing", "ei\u00b7ne", "an", "zu", "spot\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "ART", "APPR", "PTKZU", "VVINF", "$,"], "meter": "-++--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "gedenkt ihr den Menschen etwan auszurotten?", "tokens": ["ge\u00b7denkt", "ihr", "den", "Men\u00b7schen", "et\u00b7wan", "aus\u00b7zu\u00b7rot\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADV", "VVIZU", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Und wenn schon! rief eine andre schrill.", "tokens": ["Und", "wenn", "schon", "!", "rief", "ei\u00b7ne", "and\u00b7re", "schrill", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "$.", "VVFIN", "ART", "ADJA", "ADJD", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Wenn ich seine Magd nicht mehr hei\u00dfen will? \u2013", "tokens": ["Wenn", "ich", "sei\u00b7ne", "Magd", "nicht", "mehr", "hei\u00b7\u00dfen", "will", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "PTKNEG", "ADV", "VVINF", "VMFIN", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Dann blieben aber immer noch die Atlanten \u2013", "tokens": ["Dann", "blie\u00b7ben", "a\u00b7ber", "im\u00b7mer", "noch", "die", "At\u00b7lan\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ADV", "ART", "NN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "meinte eine von den asiatischen Tanten.", "tokens": ["mein\u00b7te", "ei\u00b7ne", "von", "den", "a\u00b7sia\u00b7ti\u00b7schen", "Tan\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.5": {"line.1": {"text": "Schlie\u00dflich, wie immer in solchen F\u00e4llen,", "tokens": ["Schlie\u00df\u00b7lich", ",", "wie", "im\u00b7mer", "in", "sol\u00b7chen", "F\u00e4l\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWAV", "ADV", "APPR", "PIAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "tat man eine Resolution aufstellen.", "tokens": ["tat", "man", "ei\u00b7ne", "Re\u00b7so\u00b7lu\u00b7ti\u00b7on", "auf\u00b7stel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "F\u00fcnfhundert Tintenfische wurden aufgetrieben,", "tokens": ["F\u00fcnf\u00b7hun\u00b7dert", "Tin\u00b7ten\u00b7fi\u00b7sche", "wur\u00b7den", "auf\u00b7ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "und mit ihnen wurde folgendes geschrieben:", "tokens": ["und", "mit", "ih\u00b7nen", "wur\u00b7de", "fol\u00b7gen\u00b7des", "ge\u00b7schrie\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "VAFIN", "PIS", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.6": {"line.1": {"text": "Wir Westk\u00fcsten erkl\u00e4ren hiermit einstimmig,", "tokens": ["Wir", "West\u00b7k\u00fcs\u00b7ten", "er\u00b7kl\u00e4\u00b7ren", "hier\u00b7mit", "ein\u00b7stim\u00b7mig", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVFIN", "PAV", "ADJD", "$,"], "meter": "-++--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "da\u00df es uns nicht gibt, und zeichnen hochachtungsvoll:", "tokens": ["da\u00df", "es", "uns", "nicht", "gibt", ",", "und", "zeich\u00b7nen", "hoch\u00b7ach\u00b7tungs\u00b7voll", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "VVFIN", "$,", "KON", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+-+--+", "measure": "iambic.hexa.chol"}, "line.3": {"text": "Die vereinigten Westk\u00fcsten der Erde. \u2013", "tokens": ["Die", "ver\u00b7ei\u00b7nig\u00b7ten", "West\u00b7k\u00fcs\u00b7ten", "der", "Er\u00b7de", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "$.", "$("], "meter": "--+--++--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Und nun wollte man, da\u00df dies verbreitet werde.", "tokens": ["Und", "nun", "woll\u00b7te", "man", ",", "da\u00df", "dies", "ver\u00b7brei\u00b7tet", "wer\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PIS", "$,", "KOUS", "PDS", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.7": {"line.1": {"text": "Sie riefen den Walfisch, doch er tat's nicht achten;", "tokens": ["Sie", "rie\u00b7fen", "den", "Wal\u00b7fisch", ",", "doch", "er", "tat's", "nicht", "ach\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "KON", "PPER", "VVFIN", "PTKNEG", "VVINF", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "sie riefen die M\u00f6wen, doch die M\u00f6wen lachten;", "tokens": ["sie", "rie\u00b7fen", "die", "M\u00f6\u00b7wen", ",", "doch", "die", "M\u00f6\u00b7wen", "lach\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "sie riefen die Wolke, doch die Wolke vernahm nicht;", "tokens": ["sie", "rie\u00b7fen", "die", "Wol\u00b7ke", ",", "doch", "die", "Wol\u00b7ke", "ver\u00b7nahm", "nicht", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ADV", "ART", "NN", "VVFIN", "PTKNEG", "$."], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "sie riefen ich wei\u00df nicht was, doch ich wei\u00df nicht was kam nicht.", "tokens": ["sie", "rie\u00b7fen", "ich", "wei\u00df", "nicht", "was", ",", "doch", "ich", "wei\u00df", "nicht", "was", "kam", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVFIN", "PTKNEG", "PIS", "$,", "KON", "PPER", "VVFIN", "PTKNEG", "PWS", "VVFIN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.8": {"line.1": {"text": "Ja, wieso denn, wieso? schrie die K\u00fcste von Ecuador:", "tokens": ["Ja", ",", "wie\u00b7so", "denn", ",", "wie\u00b7so", "?", "schrie", "die", "K\u00fcs\u00b7te", "von", "E\u00b7cua\u00b7dor", ":"], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADV", "ADV", "$,", "PWAV", "$.", "VVFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "-+-+--+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "W\u00e4rst du etwa kein Walfisch, du grober Tor?", "tokens": ["W\u00e4rst", "du", "et\u00b7wa", "kein", "Wal\u00b7fisch", ",", "du", "gro\u00b7ber", "Tor", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIAT", "NN", "$,", "PPER", "ADJA", "NN", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Sehr richtig, sagte der Walfisch mit vollkommener Ruh:", "tokens": ["Sehr", "rich\u00b7tig", ",", "sag\u00b7te", "der", "Wal\u00b7fisch", "mit", "voll\u00b7kom\u00b7me\u00b7ner", "Ruh", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Dein Denken, liebe K\u00fcste, dein Denken macht mich erst dazu.", "tokens": ["Dein", "Den\u00b7ken", ",", "lie\u00b7be", "K\u00fcs\u00b7te", ",", "dein", "Den\u00b7ken", "macht", "mich", "erst", "da\u00b7zu", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "ADJA", "NN", "$,", "PPOSAT", "NN", "VVFIN", "PPER", "ADV", "PAV", "$."], "meter": "-+-+-+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.9": {"line.1": {"text": "Da war's den K\u00fcsten, als s\u00e4h'n sie sich im Spiegel;", "tokens": ["Da", "wa\u00b7r's", "den", "K\u00fcs\u00b7ten", ",", "als", "s\u00e4h'n", "sie", "sich", "im", "Spie\u00b7gel", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "$,", "KOUS", "VVFIN", "PPER", "PRF", "APPRART", "NN", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "ganz seltsam erschien ihnen pl\u00f6tzlich ihr Gewiegel.", "tokens": ["ganz", "selt\u00b7sam", "er\u00b7schien", "ih\u00b7nen", "pl\u00f6tz\u00b7lich", "ihr", "Ge\u00b7wie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+-+++-+-+-+-", "measure": "unknown.measure.septa"}, "line.3": {"text": "Still schwammen sie heim, eine jede nach ihrem Land.", "tokens": ["Still", "schwam\u00b7men", "sie", "heim", ",", "ei\u00b7ne", "je\u00b7de", "nach", "ih\u00b7rem", "Land", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "PIAT", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+--+--+-+", "measure": "amphibrach.tetra.plus"}, "line.4": {"text": "Und die Resolution, die blieb unversandt.", "tokens": ["Und", "die", "Re\u00b7so\u00b7lu\u00b7ti\u00b7on", ",", "die", "blieb", "un\u00b7ver\u00b7sandt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "VVFIN", "ADJD", "$."], "meter": "--+-+--+-+-+", "measure": "iambic.penta.relaxed"}}}}}