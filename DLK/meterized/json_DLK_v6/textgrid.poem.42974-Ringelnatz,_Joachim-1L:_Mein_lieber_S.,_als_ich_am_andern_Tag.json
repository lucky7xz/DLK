{"textgrid.poem.42974": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Mein lieber S., als ich am andern Tag", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein lieber S., als ich am andern Tag", "tokens": ["Mein", "lie\u00b7ber", "S.", ",", "als", "ich", "am", "an\u00b7dern", "Tag"], "token_info": ["word", "word", "abbreviation", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "KOUS", "PPER", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Erwachte, wu\u00dfte ich nicht mehr Genaues.", "tokens": ["Er\u00b7wach\u00b7te", ",", "wu\u00df\u00b7te", "ich", "nicht", "mehr", "Ge\u00b7nau\u00b7es", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PPER", "PTKNEG", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ich hab ein rotes Auge, Ruth ein blaues.", "tokens": ["Ich", "hab", "ein", "ro\u00b7tes", "Au\u00b7ge", ",", "Ruth", "ein", "blau\u00b7es", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$,", "NN", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wie sich das zugetragen haben mag!!", "tokens": ["Wie", "sich", "das", "zu\u00b7ge\u00b7tra\u00b7gen", "ha\u00b7ben", "mag", "!!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "PDS", "VVPP", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "In meinem Anzug klebt ein Pfund Spinat.", "tokens": ["In", "mei\u00b7nem", "An\u00b7zug", "klebt", "ein", "Pfund", "Spi\u00b7nat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wie kam das nur? Ich wei\u00df nur noch, da\u00df Deine", "tokens": ["Wie", "kam", "das", "nur", "?", "Ich", "wei\u00df", "nur", "noch", ",", "da\u00df", "Dei\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "VVFIN", "PDS", "ADV", "$.", "PPER", "VVFIN", "ADV", "ADV", "$,", "KOUS", "PPOSAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Frau oder Oskars in den Spiegel trat.", "tokens": ["Frau", "o\u00b7der", "Os\u00b7kars", "in", "den", "Spie\u00b7gel", "trat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Doch wer go\u00df Hermann Suppe auf die Beine?", "tokens": ["Doch", "wer", "go\u00df", "Her\u00b7mann", "Sup\u00b7pe", "auf", "die", "Bei\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "NE", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Ich gebe zu, da\u00df ich den Anla\u00df gab.", "tokens": ["Ich", "ge\u00b7be", "zu", ",", "da\u00df", "ich", "den", "An\u00b7la\u00df", "gab", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ich war besoffen wie noch nie seit Wochen.", "tokens": ["Ich", "war", "be\u00b7sof\u00b7fen", "wie", "noch", "nie", "seit", "Wo\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "KOKOM", "ADV", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Verzeiht mir, was ich ge-, zer- und verbrochen", "tokens": ["Ver\u00b7zeiht", "mir", ",", "was", "ich", "ge", ",", "zer", "und", "ver\u00b7bro\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "PWS", "PPER", "TRUNC", "$,", "TRUNC", "KON", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und da\u00df ich Fips mit Wachs betr\u00e4ufelt hab.", "tokens": ["Und", "da\u00df", "ich", "Fips", "mit", "Wachs", "be\u00b7tr\u00e4u\u00b7felt", "hab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "NN", "APPR", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Nun sind wir alle pl\u00f6tzlich j\u00e4h entzweit", "tokens": ["Nun", "sind", "wir", "al\u00b7le", "pl\u00f6tz\u00b7lich", "j\u00e4h", "ent\u00b7zweit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PIS", "ADJD", "ADJD", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und waren Freunde, die nie be\u00dfre finden.", "tokens": ["Und", "wa\u00b7ren", "Freun\u00b7de", ",", "die", "nie", "be\u00df\u00b7re", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "NN", "$,", "PRELS", "ADV", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Man sollte bei solch reicher Festlichkeit", "tokens": ["Man", "soll\u00b7te", "bei", "solch", "rei\u00b7cher", "Fest\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Lieber mehr essen und sich \u00fcberwinden.", "tokens": ["Lie\u00b7ber", "mehr", "es\u00b7sen", "und", "sich", "\u00fc\u00b7berw\u00b7in\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "VVINF", "KON", "PRF", "VVINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.5": {"line.1": {"text": "Wie war die Bowle gut und der Fasan!", "tokens": ["Wie", "war", "die", "Bow\u00b7le", "gut", "und", "der", "Fa\u00b7san", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "ADJD", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Vorbei. \u2013 Am liebsten w\u00fcrd ich mich erh\u00e4ngen. \u2013", "tokens": ["Vor\u00b7bei", ".", "\u2013", "Am", "liebs\u00b7ten", "w\u00fcrd", "ich", "mich", "er\u00b7h\u00e4n\u00b7gen", ".", "\u2013"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "$.", "$(", "PTKA", "ADJD", "VAFIN", "PPER", "PRF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Verdammt nicht ganz den, der das Porzellan", "tokens": ["Ver\u00b7dammt", "nicht", "ganz", "den", ",", "der", "das", "Por\u00b7zel\u00b7lan"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PTKNEG", "ADV", "ART", "$,", "PRELS", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Euch gern ersetzen will. Ohne sich aufzudr\u00e4ngen.", "tokens": ["Euch", "gern", "er\u00b7set\u00b7zen", "will", ".", "Oh\u00b7ne", "sich", "auf\u00b7zu\u00b7dr\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVINF", "VMFIN", "$.", "APPR", "PRF", "VVIZU", "$."], "meter": "---+--+--+-+-", "measure": "iambic.tetra.relaxed"}}}}}