{"textgrid.poem.37323": {"metadata": {"author": {"name": "Busch, Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "1L: Von Fruchtomletts da mag berichten", "genre": "verse", "period": "N.A.", "pub_year": 1870, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Von Fruchtomletts da mag berichten", "tokens": ["Von", "Fruch\u00b7tom\u00b7letts", "da", "mag", "be\u00b7rich\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "ADV", "VMFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Dichter aus den h\u00f6hern Schichten.", "tokens": ["Ein", "Dich\u00b7ter", "aus", "den", "h\u00f6\u00b7hern", "Schich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wir aber, ohne Neid nach oben,", "tokens": ["Wir", "a\u00b7ber", ",", "oh\u00b7ne", "Neid", "nach", "o\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$,", "KOUI", "NN", "APPR", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit b\u00fcrgerlicher Zunge loben", "tokens": ["Mit", "b\u00fcr\u00b7ger\u00b7li\u00b7cher", "Zun\u00b7ge", "lo\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Uns Pfannekuchen und Salat.", "tokens": ["Uns", "Pfan\u00b7ne\u00b7ku\u00b7chen", "und", "Sa\u00b7lat", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wie unsre Liese delikat", "tokens": ["Wie", "uns\u00b7re", "Lie\u00b7se", "de\u00b7li\u00b7kat"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So etwas backt und zubereitet,", "tokens": ["So", "et\u00b7was", "backt", "und", "zu\u00b7be\u00b7rei\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sei hier in Worten angedeutet.", "tokens": ["Sei", "hier", "in", "Wor\u00b7ten", "an\u00b7ge\u00b7deu\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Drei Eier, frisch und ohne Fehl,", "tokens": ["Drei", "Ei\u00b7er", ",", "frisch", "und", "oh\u00b7ne", "Fehl", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "ADJD", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und Milch und einen L\u00f6ffel Mehl,", "tokens": ["Und", "Milch", "und", "ei\u00b7nen", "L\u00f6f\u00b7fel", "Mehl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die quirlt sie flei\u00dfig durcheinand", "tokens": ["Die", "quirlt", "sie", "flei\u00b7\u00dfig", "durch\u00b7ei\u00b7nand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPER", "ADJD", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu einem innigen Verband.", "tokens": ["Zu", "ei\u00b7nem", "in\u00b7ni\u00b7gen", "Ver\u00b7band", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.5": {"line.1": {"text": "Sodann, wenn Tr\u00e4nen auch ein \u00dcbel,", "tokens": ["So\u00b7dann", ",", "wenn", "Tr\u00e4\u00b7nen", "auch", "ein", "\u00dc\u00b7bel", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "NN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zerst\u00fcckelt sie und mengt die Zwiebel", "tokens": ["Zer\u00b7st\u00fc\u00b7ckelt", "sie", "und", "mengt", "die", "Zwie\u00b7bel"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mit \u00d6l und Salz zu einer Br\u00fche,", "tokens": ["Mit", "\u00d6l", "und", "Salz", "zu", "ei\u00b7ner", "Br\u00fc\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df der Salat sie an sich ziehe.", "tokens": ["Da\u00df", "der", "Sa\u00b7lat", "sie", "an", "sich", "zie\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "APPR", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Um diesen ferner herzustellen,", "tokens": ["Um", "die\u00b7sen", "fer\u00b7ner", "her\u00b7zu\u00b7stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PDAT", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat sie Kartoffeln abzupellen.", "tokens": ["Hat", "sie", "Kar\u00b7tof\u00b7feln", "ab\u00b7zu\u00b7pel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da hei\u00dft es, fix die Finger brauchen,", "tokens": ["Da", "hei\u00dft", "es", ",", "fix", "die", "Fin\u00b7ger", "brau\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "ADJD", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Den Mund zu spitzen und zu hauchen,", "tokens": ["Den", "Mund", "zu", "spit\u00b7zen", "und", "zu", "hau\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Denn hei\u00df geschnitten nur allein", "tokens": ["Denn", "hei\u00df", "ge\u00b7schnit\u00b7ten", "nur", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kann der Salat geschmeidig sein.", "tokens": ["Kann", "der", "Sa\u00b7lat", "ge\u00b7schmei\u00b7dig", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Hierauf so geht es wieder heiter", "tokens": ["Hier\u00b7auf", "so", "geht", "es", "wie\u00b7der", "hei\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit unserm Pfannekuchen weiter.", "tokens": ["Mit", "un\u00b7serm", "Pfan\u00b7ne\u00b7ku\u00b7chen", "wei\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Nachdem das Feuer leicht gesch\u00fcrt,", "tokens": ["Nach\u00b7dem", "das", "Feu\u00b7er", "leicht", "ge\u00b7sch\u00fcrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Pfanne sorgsam auspoliert,", "tokens": ["Die", "Pfan\u00b7ne", "sorg\u00b7sam", "aus\u00b7po\u00b7liert", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der W\u00fcrfelspeck hineingesch\u00fcttelt,", "tokens": ["Der", "W\u00fcr\u00b7fel\u00b7speck", "hin\u00b7ein\u00b7ge\u00b7sch\u00fct\u00b7telt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So da\u00df es lustig br\u00e4t und brittelt,", "tokens": ["So", "da\u00df", "es", "lus\u00b7tig", "br\u00e4t", "und", "brit\u00b7telt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADJD", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Pisch, kommt dar\u00fcber mit Gezisch", "tokens": ["Pisch", ",", "kommt", "da\u00b7r\u00fc\u00b7ber", "mit", "Ge\u00b7zisch"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$,", "VVFIN", "PAV", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das ersterw\u00e4hnte Kunstgemisch.", "tokens": ["Das", "ers\u00b7ter\u00b7w\u00e4hn\u00b7te", "Kunst\u00b7ge\u00b7misch", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Nun zeigt besonders und apart", "tokens": ["Nun", "zeigt", "be\u00b7son\u00b7ders", "und", "a\u00b7part"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "KON", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich Lieschens Geistesgegenwart,", "tokens": ["Sich", "Lie\u00b7schens", "Geis\u00b7tes\u00b7ge\u00b7gen\u00b7wart", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "NE", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn nur zu bald, wie allbekannt,", "tokens": ["Denn", "nur", "zu", "bald", ",", "wie", "all\u00b7be\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADV", "$,", "PWAV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist solch ein Kuchen angebrannt.", "tokens": ["Ist", "solch", "ein", "Ku\u00b7chen", "an\u00b7ge\u00b7brannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie prickelt ihn, sie stockert ihn,", "tokens": ["Sie", "pri\u00b7ckelt", "ihn", ",", "sie", "sto\u00b7ckert", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sie r\u00fcttelt, sch\u00fcttelt, lockert ihn", "tokens": ["Sie", "r\u00fct\u00b7telt", ",", "sch\u00fct\u00b7telt", ",", "lo\u00b7ckert", "ihn"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "$,", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und l\u00fcftet ihn, bis augenscheinlich", "tokens": ["Und", "l\u00fcf\u00b7tet", "ihn", ",", "bis", "au\u00b7gen\u00b7schein\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "$,", "KOUS", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Unterseite eben br\u00e4unlich,", "tokens": ["Die", "Un\u00b7ter\u00b7sei\u00b7te", "e\u00b7ben", "br\u00e4un\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die umgekehrt geschickt und prompt", "tokens": ["Die", "um\u00b7ge\u00b7kehrt", "ge\u00b7schickt", "und", "prompt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "VVPP", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Jetzt ihrerseits nach oben kommt.", "tokens": ["Jetzt", "ih\u00b7rer\u00b7seits", "nach", "o\u00b7ben", "kommt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Geduld, es w\u00e4hrt nur noch ein bissel,", "tokens": ["Ge\u00b7duld", ",", "es", "w\u00e4hrt", "nur", "noch", "ein", "bis\u00b7sel", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VVFIN", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dann liegt der Kuchen auf der Sch\u00fcssel.", "tokens": ["Dann", "liegt", "der", "Ku\u00b7chen", "auf", "der", "Sch\u00fcs\u00b7sel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Doch sp\u00e4terhin die Einverleibung,", "tokens": ["Doch", "sp\u00e4\u00b7ter\u00b7hin", "die", "Ein\u00b7ver\u00b7lei\u00b7bung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie die zu Mund und Herzen spricht,", "tokens": ["Wie", "die", "zu", "Mund", "und", "Her\u00b7zen", "spricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das spottet jeglicher Beschreibung,", "tokens": ["Das", "spot\u00b7tet", "jeg\u00b7li\u00b7cher", "Be\u00b7schrei\u00b7bung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und darum endet das Gedicht.", "tokens": ["Und", "da\u00b7rum", "en\u00b7det", "das", "Ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}