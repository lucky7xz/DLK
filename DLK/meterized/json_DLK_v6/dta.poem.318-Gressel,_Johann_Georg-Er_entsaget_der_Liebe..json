{"dta.poem.318": {"metadata": {"author": {"name": "Gressel, Johann Georg", "birth": "N.A.", "death": "N.A."}, "title": "Er entsaget der Liebe.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1716", "urn": "urn:nbn:de:kobv:b4-200905199041", "language": ["de:0.99"], "booktitle": "Celander [i. e. Gressel, Johann Georg]: Verliebte-Galante/ Sinn-Vermischte und Grab-Gedichte. Hamburg u. a., 1716."}, "poem": {"stanza.1": {"line.1": {"text": "Was soll ich im Lieben hoffen", "tokens": ["Was", "soll", "ich", "im", "Lie\u00b7ben", "hof\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "PPER", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Freude/ oder Angst und Pein?", "tokens": ["Freu\u00b7de", "/", "o\u00b7der", "Angst", "und", "Pein", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist das weisse Ziel getroffen", "tokens": ["Ist", "das", "weis\u00b7se", "Ziel", "ge\u00b7trof\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Oder wirds das schwartze seyn?", "tokens": ["O\u00b7der", "wirds", "das", "schwart\u00b7ze", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PDS", "VVFIN", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Scheinet mir das Licht des Lebens", "tokens": ["Schei\u00b7net", "mir", "das", "Licht", "des", "Le\u00b7bens"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Oder hoffe ich vergebens.", "tokens": ["O\u00b7der", "hof\u00b7fe", "ich", "ver\u00b7ge\u00b7bens", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Echo. Vergebens.", "tokens": ["E\u00b7cho", ".", "Ver\u00b7ge\u00b7bens", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.2": {"line.1": {"text": "Echo wilt du mit mir schertzen;", "tokens": ["E\u00b7cho", "wilt", "du", "mit", "mir", "schert\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "APPR", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hast du deine Lust daran/", "tokens": ["Hast", "du", "dei\u00b7ne", "Lust", "da\u00b7ran", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPOSAT", "NN", "PAV", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df mein Hertz vor Angst und Schniertzen", "tokens": ["Da\u00df", "mein", "Hertz", "vor", "Angst", "und", "Schniert\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Niemahls freudig werden kan?", "tokens": ["Nie\u00b7mahls", "freu\u00b7dig", "wer\u00b7den", "kan", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wilt du da\u00df es mir soll gehen", "tokens": ["Wilt", "du", "da\u00df", "es", "mir", "soll", "ge\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "KOUS", "PPER", "PPER", "VMFIN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Wie es ist mit dir geschehen?", "tokens": ["Wie", "es", "ist", "mit", "dir", "ge\u00b7sche\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VAFIN", "APPR", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Echo. Geschehen.", "tokens": ["E\u00b7cho", ".", "Ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.3": {"line.1": {"text": "Nymphe bin ich schon verlohren", "tokens": ["Nym\u00b7phe", "bin", "ich", "schon", "ver\u00b7loh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "K\u00fc\u00dft ein ander ", "tokens": ["K\u00fc\u00dft", "ein", "an\u00b7der"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ART", "ADJD"], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Gelte ich bey ihr nicht viel?", "tokens": ["Gel\u00b7te", "ich", "bey", "ihr", "nicht", "viel", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPER", "PTKNEG", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.5": {"text": "Bringet mir das s\u00fcsse Lieben", "tokens": ["Brin\u00b7get", "mir", "das", "s\u00fcs\u00b7se", "Lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Vor Vergn\u00fcgen nur Betr\u00fcben?", "tokens": ["Vor", "Ver\u00b7gn\u00fc\u00b7gen", "nur", "Be\u00b7tr\u00fc\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Echo. Betr\u00fcben.", "tokens": ["E\u00b7cho", ".", "Be\u00b7tr\u00fc\u00b7ben", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "Ha/ nun mag ich nicht mehr lieben", "tokens": ["Ha", "/", "nun", "mag", "ich", "nicht", "mehr", "lie\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "ADV", "VMFIN", "PPER", "PTKNEG", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Amor weg aus meiner Brust.", "tokens": ["A\u00b7mor", "weg", "aus", "mei\u00b7ner", "Brust", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Bringt das Lieben nur Betr\u00fcben", "tokens": ["Bringt", "das", "Lie\u00b7ben", "nur", "Be\u00b7tr\u00fc\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "ADV", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Was versprichst du denn vor Lust?", "tokens": ["Was", "ver\u00b7sprichst", "du", "denn", "vor", "Lust", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Freyheit wird von mir verehret", "tokens": ["Frey\u00b7heit", "wird", "von", "mir", "ver\u00b7eh\u00b7ret"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Bis mein Lauff zum Grabe kehret", "tokens": ["Bis", "mein", "Lauff", "zum", "Gra\u00b7be", "keh\u00b7ret"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "APPRART", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Echo. Bekehret.", "tokens": ["E\u00b7cho", ".", "Be\u00b7keh\u00b7ret", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}}}}