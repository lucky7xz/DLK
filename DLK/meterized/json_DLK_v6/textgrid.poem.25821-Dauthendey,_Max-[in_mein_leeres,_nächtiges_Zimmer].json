{"textgrid.poem.25821": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[in mein leeres, n\u00e4chtiges Zimmer]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In mein leeres, n\u00e4chtiges Zimmer", "tokens": ["In", "mein", "lee\u00b7res", ",", "n\u00e4ch\u00b7ti\u00b7ges", "Zim\u00b7mer"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "$,", "ADJA", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Flogen oft V\u00f6gel lichthell herein,", "tokens": ["Flo\u00b7gen", "oft", "V\u00f6\u00b7gel", "licht\u00b7hell", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "NN", "ADJD", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Es war Lachen fr\u00f6hlicher Menschen", "tokens": ["Es", "war", "La\u00b7chen", "fr\u00f6h\u00b7li\u00b7cher", "Men\u00b7schen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "NN", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Unten aus Nacht und Laternenschein.", "tokens": ["Un\u00b7ten", "aus", "Nacht", "und", "La\u00b7ter\u00b7nen\u00b7schein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "+--+----+", "measure": "dactylic.di.plus"}, "line.5": {"text": "In mancher kargen, hungernden Stunde", "tokens": ["In", "man\u00b7cher", "kar\u00b7gen", ",", "hun\u00b7gern\u00b7den", "Stun\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Hab' ich von diesem Lachen gezehrt,", "tokens": ["Hab'", "ich", "von", "die\u00b7sem", "La\u00b7chen", "ge\u00b7zehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "PDAT", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Und f\u00fcr den Bruchteil einer Sekunde", "tokens": ["Und", "f\u00fcr", "den", "Bruch\u00b7teil", "ei\u00b7ner", "Se\u00b7kun\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Wurden die lachenden Menschen mein.", "tokens": ["Wur\u00b7den", "die", "la\u00b7chen\u00b7den", "Men\u00b7schen", "mein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "PPOSAT", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.9": {"text": "Doch im Erwachen mu\u00dft ich mich hassen,", "tokens": ["Doch", "im", "Er\u00b7wa\u00b7chen", "mu\u00dft", "ich", "mich", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VMFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Wie der Entthronte sich hassen mag.", "tokens": ["Wie", "der", "Ent\u00b7thron\u00b7te", "sich", "has\u00b7sen", "mag."], "token_info": ["word", "word", "word", "word", "word", "abbreviation"], "pos": ["PWAV", "ART", "NN", "PRF", "VVFIN", "NE"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.11": {"text": "Statt Leben zu prassen bis zum Ermatten,", "tokens": ["Statt", "Le\u00b7ben", "zu", "pras\u00b7sen", "bis", "zum", "Er\u00b7mat\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "PTKZU", "VVINF", "KON", "APPRART", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Sa\u00df ich bei Schatten, fra\u00df Schatten.", "tokens": ["Sa\u00df", "ich", "bei", "Schat\u00b7ten", ",", "fra\u00df", "Schat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$,", "VVFIN", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}