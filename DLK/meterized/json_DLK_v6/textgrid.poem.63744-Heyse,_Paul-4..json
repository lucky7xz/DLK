{"textgrid.poem.63744": {"metadata": {"author": {"name": "Heyse, Paul", "birth": "N.A.", "death": "N.A."}, "title": "4.", "genre": "verse", "period": "N.A.", "pub_year": 1872, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das Stirnhaar leicht mit Puder angegraut,", "tokens": ["Das", "Stirn\u00b7haar", "leicht", "mit", "Pu\u00b7der", "an\u00b7ge\u00b7graut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Den Schopf gekr\u00f6nt mit falscher Flechtenmasse,", "tokens": ["Den", "Schopf", "ge\u00b7kr\u00f6nt", "mit", "fal\u00b7scher", "Flech\u00b7ten\u00b7mas\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ihr F\u00e4hnchen lang nachschleifend auf der Gasse,", "tokens": ["Ihr", "F\u00e4hn\u00b7chen", "lang", "nach\u00b7schlei\u00b7fend", "auf", "der", "Gas\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Bachstelzenhaft, mit zwitschernd hellem Laut;", "tokens": ["Bach\u00b7stel\u00b7zen\u00b7haft", ",", "mit", "zwit\u00b7schernd", "hel\u00b7lem", "Laut", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Zu jedem Mannsbild, das her\u00fcberschaut,", "tokens": ["Zu", "je\u00b7dem", "Manns\u00b7bild", ",", "das", "her\u00b7\u00fc\u00b7bersc\u00b7haut", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "PDS", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Hin\u00e4ugelnd, ob ein Netz sich werfen lasse,", "tokens": ["Hin\u00b7\u00e4u\u00b7gelnd", ",", "ob", "ein", "Netz", "sich", "wer\u00b7fen", "las\u00b7se", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "KOUS", "ART", "NN", "PRF", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Nicht sch\u00f6n, doch zierlich, von gemischter Rasse,", "tokens": ["Nicht", "sch\u00f6n", ",", "doch", "zier\u00b7lich", ",", "von", "ge\u00b7mischter", "Ras\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$,", "ADV", "ADJD", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Kohlschwarz das Aug, ein bleiches Braun die Haut:", "tokens": ["Kohl\u00b7schwarz", "das", "Aug", ",", "ein", "blei\u00b7ches", "Braun", "die", "Haut", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "$,", "ART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "So gehn Neapels T\u00f6chter vom geringern", "tokens": ["So", "gehn", "Nea\u00b7pels", "T\u00f6ch\u00b7ter", "vom", "ge\u00b7rin\u00b7gern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "NN", "APPRART", "ADJA"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Stand dir vorbei und scheinen keck zu sagen:", "tokens": ["Stand", "dir", "vor\u00b7bei", "und", "schei\u00b7nen", "keck", "zu", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wir sind nicht R\u00f6merinnen, mu\u00dft du wissen.", "tokens": ["Wir", "sind", "nicht", "R\u00f6\u00b7me\u00b7rin\u00b7nen", ",", "mu\u00dft", "du", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "NN", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Den Austern gleichen wir, den kleinen Dingern,", "tokens": ["Den", "Aus\u00b7tern", "glei\u00b7chen", "wir", ",", "den", "klei\u00b7nen", "Din\u00b7gern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die auch, wie wir, das Altern nicht vertragen,", "tokens": ["Die", "auch", ",", "wie", "wir", ",", "das", "Al\u00b7tern", "nicht", "ver\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "$,", "PWAV", "PPER", "$,", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Doch frisch geschl\u00fcrft sind sie ein Leckerbissen.", "tokens": ["Doch", "frisch", "ge\u00b7schl\u00fcrft", "sind", "sie", "ein", "Le\u00b7cker\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}