{"textgrid.poem.52816": {"metadata": {"author": {"name": "Eichrodt, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Zur Enth\u00fcllungsfeier des Christian Maier'schen Denkmals", "genre": "verse", "period": "N.A.", "pub_year": 1859, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Heil! der Mitwelt ist gelungen", "tokens": ["Heil", "!", "der", "Mit\u00b7welt", "ist", "ge\u00b7lun\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "ART", "NN", "VAFIN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was die Vorwelt kaum gedacht,", "tokens": ["Was", "die", "Vor\u00b7welt", "kaum", "ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Aus der finstern Grabesnacht,", "tokens": ["Aus", "der", "fins\u00b7tern", "Gra\u00b7bes\u00b7nacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "An dem st\u00e4dt'schen Wassergraben", "tokens": ["An", "dem", "st\u00e4dt'\u00b7schen", "Was\u00b7ser\u00b7gra\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Zum Genu\u00df dem ganzen Land,", "tokens": ["Zum", "Ge\u00b7nu\u00df", "dem", "gan\u00b7zen", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Er, der Gr\u00f6\u00dfte, den wir haben", "tokens": ["Er", ",", "der", "Gr\u00f6\u00df\u00b7te", ",", "den", "wir", "ha\u00b7ben"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "$,", "ART", "NN", "$,", "PRELS", "PPER", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Aus dem Deputirtenstand!", "tokens": ["Aus", "dem", "De\u00b7pu\u00b7tir\u00b7ten\u00b7stand", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Ausgezeichnet war sein Wirken,", "tokens": ["Aus\u00b7ge\u00b7zeich\u00b7net", "war", "sein", "Wir\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie er selbst auch in Person,", "tokens": ["Wie", "er", "selbst", "auch", "in", "Per\u00b7son", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "ADV", "APPR", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieses kann gottlob verb\u00fcrgen,", "tokens": ["Die\u00b7ses", "kann", "gott\u00b7lob", "ver\u00b7b\u00fcr\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Von dem Staat das Lexikon,", "tokens": ["Von", "dem", "Staat", "das", "Le\u00b7xi\u00b7kon", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Welches wirklich \u00fcbersch\u00fcttet", "tokens": ["Wel\u00b7ches", "wirk\u00b7lich", "\u00fc\u00b7ber\u00b7sch\u00fct\u00b7tet"], "token_info": ["word", "word", "word"], "pos": ["PWS", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ihn mit beispiellosem Preis,", "tokens": ["Ihn", "mit", "bei\u00b7spiel\u00b7lo\u00b7sem", "Preis", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Und, obschon er sich's verbittet,", "tokens": ["Und", ",", "ob\u00b7schon", "er", "sich's", "ver\u00b7bit\u00b7tet", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "PRF", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Dennoch es zu sch\u00e4tzen wei\u00df.", "tokens": ["Den\u00b7noch", "es", "zu", "sch\u00e4t\u00b7zen", "wei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "So besonders in der Kammer", "tokens": ["So", "be\u00b7son\u00b7ders", "in", "der", "Kam\u00b7mer"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Seine Reden klein und gro\u00df,", "tokens": ["Sei\u00b7ne", "Re\u00b7den", "klein", "und", "gro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo er f\u00fcr des Volkes Jammer", "tokens": ["Wo", "er", "f\u00fcr", "des", "Vol\u00b7kes", "Jam\u00b7mer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zog auf die Regierung los;", "tokens": ["Zog", "auf", "die", "Re\u00b7gie\u00b7rung", "los", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Doch sie konnten ihn nicht leiden,", "tokens": ["Doch", "sie", "konn\u00b7ten", "ihn", "nicht", "lei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Weil er Alles sagte grad,", "tokens": ["Weil", "er", "Al\u00b7les", "sag\u00b7te", "grad", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "VVFIN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Mu\u00dften ihn jedoch beneiden", "tokens": ["Mu\u00df\u00b7ten", "ihn", "je\u00b7doch", "be\u00b7nei\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.8": {"text": "Wegen seiner gro\u00dfen Schwad.", "tokens": ["We\u00b7gen", "sei\u00b7ner", "gro\u00b7\u00dfen", "Schwad", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Niemand hat sich so erprobet,", "tokens": ["Nie\u00b7mand", "hat", "sich", "so", "er\u00b7pro\u00b7bet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PRF", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als des Fortschritts edler Sohn,", "tokens": ["Als", "des", "Fort\u00b7schritts", "ed\u00b7ler", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und mit solcher Wuth getobet", "tokens": ["Und", "mit", "sol\u00b7cher", "Wuth", "ge\u00b7to\u00b7bet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "In der Opposition,", "tokens": ["In", "der", "Op\u00b7po\u00b7si\u00b7ti\u00b7on", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Darum ward ihm auch genommen", "tokens": ["Da\u00b7rum", "ward", "ihm", "auch", "ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Seine sch\u00f6ne Professur,", "tokens": ["Sei\u00b7ne", "sch\u00f6\u00b7ne", "Pro\u00b7fes\u00b7sur", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Und um den Verdienst gekommen", "tokens": ["Und", "um", "den", "Ver\u00b7dienst", "ge\u00b7kom\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "VVPP"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Ist er fast durch die Censur.", "tokens": ["Ist", "er", "fast", "durch", "die", "Cen\u00b7sur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Gleichwohl wu\u00dft' er brav zu fristen", "tokens": ["Gleich\u00b7wohl", "wu\u00dft'", "er", "brav", "zu", "fris\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "PTKZU", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Weib und Kind die Existenz,", "tokens": ["Weib", "und", "Kind", "die", "E\u00b7xis\u00b7tenz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Niemals lie\u00df er sich gel\u00fcsten", "tokens": ["Nie\u00b7mals", "lie\u00df", "er", "sich", "ge\u00b7l\u00fcs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ordensstern und Excellenz,", "tokens": ["Or\u00b7denss\u00b7tern", "und", "Ex\u00b7cel\u00b7lenz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Lieber krank an Leib und Seele", "tokens": ["Lie\u00b7ber", "krank", "an", "Leib", "und", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "ADJD", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Und mit ", "tokens": ["Und", "mit"], "token_info": ["word", "word"], "pos": ["KON", "APPR"], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Fiel der Mann von seiner Stelle,", "tokens": ["Fiel", "der", "Mann", "von", "sei\u00b7ner", "Stel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Als von seinem Grundsatz ab.", "tokens": ["Als", "von", "sei\u00b7nem", "Grund\u00b7satz", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Kurz und gut, sein hohes Streben,", "tokens": ["Kurz", "und", "gut", ",", "sein", "ho\u00b7hes", "Stre\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Galt dem Volke ganz allein.", "tokens": ["Galt", "dem", "Vol\u00b7ke", "ganz", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bblieber\u00ab, sprach er, \u00bbgar nicht leben,", "tokens": ["\u00bb", "lie\u00b7ber", "\u00ab", ",", "sprach", "er", ",", "\u00bb", "gar", "nicht", "le\u00b7ben", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$(", "$,", "VVFIN", "PPER", "$,", "$(", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Als ein Bureaukrat zu sein,", "tokens": ["Als", "ein", "Bu\u00b7re\u00b7au\u00b7krat", "zu", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKZU", "VAINF", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.5": {"text": "Denn von den servilen Tr\u00f6pfen", "tokens": ["Denn", "von", "den", "ser\u00b7vi\u00b7len", "Tr\u00f6p\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Hab' ich stets mich abgewandt,", "tokens": ["Hab'", "ich", "stets", "mich", "ab\u00b7ge\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "VVFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Lieber h\u00e4ngen mich und k\u00f6pfen", "tokens": ["Lie\u00b7ber", "h\u00e4n\u00b7gen", "mich", "und", "k\u00f6p\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "F\u00fcr das deutsche Vaterland!\u00ab", "tokens": ["F\u00fcr", "das", "deut\u00b7sche", "Va\u00b7ter\u00b7land", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Und er wich von seinem Pfosten", "tokens": ["Und", "er", "wich", "von", "sei\u00b7nem", "Pfos\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nie zu seiner eig'nen Schand,", "tokens": ["Nie", "zu", "sei\u00b7ner", "eig'\u00b7nen", "Schand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Hat auf seines Herzens Kosten", "tokens": ["Hat", "auf", "sei\u00b7nes", "Her\u00b7zens", "Kos\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "PPOSAT", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nie gebildet den Verstand.", "tokens": ["Nie", "ge\u00b7bil\u00b7det", "den", "Ver\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Jede Zeitung, jede Zone", "tokens": ["Je\u00b7de", "Zei\u00b7tung", ",", "je\u00b7de", "Zo\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Richtet' sich nach seiner Uhr,", "tokens": ["Rich\u00b7tet'", "sich", "nach", "sei\u00b7ner", "Uhr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPOSAT", "NN", "$,"], "meter": "+---+-+", "measure": "dactylic.init"}, "line.7": {"text": "Destowen'ger nichts der Krone", "tokens": ["Des\u00b7to\u00b7wen'\u00b7ger", "nichts", "der", "Kro\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "PIS", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Rieth er stets zum Besten nur.", "tokens": ["Rieth", "er", "stets", "zum", "Bes\u00b7ten", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPRART", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Darum auch so viele Essen,", "tokens": ["Da\u00b7rum", "auch", "so", "vie\u00b7le", "Es\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "ADV", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hielt man diesem Mann zum Dank,", "tokens": ["Hielt", "man", "die\u00b7sem", "Mann", "zum", "Dank", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PDAT", "NN", "APPRART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Darum hat er auch besessen", "tokens": ["Da\u00b7rum", "hat", "er", "auch", "be\u00b7ses\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Einen gro\u00dfen Nu\u00dfbaumschrank,", "tokens": ["Ei\u00b7nen", "gro\u00b7\u00dfen", "Nu\u00df\u00b7baum\u00b7schrank", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Einen Schrank mit Glas, darinnen", "tokens": ["Ei\u00b7nen", "Schrank", "mit", "Glas", ",", "da\u00b7rin\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "APPR", "NN", "$,", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Er die Becher aufbewahrt,", "tokens": ["Er", "die", "Be\u00b7cher", "auf\u00b7be\u00b7wahrt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Die, zu ehren sein Beginnen,", "tokens": ["Die", ",", "zu", "eh\u00b7ren", "sein", "Be\u00b7gin\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PTKZU", "VVINF", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Sich das Volk am Mund erspart.", "tokens": ["Sich", "das", "Volk", "am", "Mund", "er\u00b7spart", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ART", "NN", "APPRART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Lang schon h\u00e4tt' es sich geh\u00f6ret,", "tokens": ["Lang", "schon", "h\u00e4tt'", "es", "sich", "ge\u00b7h\u00f6\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VAFIN", "PPER", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und wahrhaftig nicht erst jetzt,", "tokens": ["Und", "wahr\u00b7haf\u00b7tig", "nicht", "erst", "jetzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PTKNEG", "ADV", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df, was Jeder hei\u00df entbehret,", "tokens": ["Da\u00df", ",", "was", "Je\u00b7der", "hei\u00df", "ent\u00b7beh\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PWS", "PIS", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df ein Denkmal ward gesetzt,", "tokens": ["Da\u00df", "ein", "Denk\u00b7mal", "ward", "ge\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "L\u00e4nger ", "tokens": ["L\u00e4n\u00b7ger"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.6": {"text": "Und so kam man alsgemach,", "tokens": ["Und", "so", "kam", "man", "als\u00b7ge\u00b7mach", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Wie das Bildni\u00df kann bezeugen,", "tokens": ["Wie", "das", "Bild\u00b7ni\u00df", "kann", "be\u00b7zeu\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VMFIN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Dem Bed\u00fcrfni\u00df wirklich nach:", "tokens": ["Dem", "Be\u00b7d\u00fcrf\u00b7ni\u00df", "wirk\u00b7lich", "nach", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Nein! ein Mann, wie Der gewesen,", "tokens": ["Nein", "!", "ein", "Mann", ",", "wie", "Der", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$.", "ART", "NN", "$,", "PWAV", "ART", "VAPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Findet sich nicht alle Tag,", "tokens": ["Fin\u00b7det", "sich", "nicht", "al\u00b7le", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKNEG", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Was man auch von andern Gr\u00f6\u00dfen", "tokens": ["Was", "man", "auch", "von", "an\u00b7dern", "Gr\u00f6\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PIS", "ADV", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Unbegreiflich's sagen mag!", "tokens": ["Un\u00b7be\u00b7greif\u00b7lich's", "sa\u00b7gen", "mag", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "So ein Mann, ein vielgeplagter,", "tokens": ["So", "ein", "Mann", ",", "ein", "viel\u00b7ge\u00b7plag\u00b7ter", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "So gemein und grundgescheidt,", "tokens": ["So", "ge\u00b7mein", "und", "grund\u00b7ge\u00b7scheidt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "So ein Mann von ", "tokens": ["So", "ein", "Mann", "von"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR"], "meter": "+-+-", "measure": "trochaic.di"}, "line.8": {"text": "Und von ", "tokens": ["Und", "von"], "token_info": ["word", "word"], "pos": ["KON", "APPR"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.11": {"line.1": {"text": "Doch er ist nicht ausgestorben,", "tokens": ["Doch", "er", "ist", "nicht", "aus\u00b7ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn er lebt im Denkmal fort,", "tokens": ["Denn", "er", "lebt", "im", "Denk\u00b7mal", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Das er sich mit ", "tokens": ["Das", "er", "sich", "mit"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "PPER", "PRF", "APPR"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Auf mein heilig Ehrenwort!", "tokens": ["Auf", "mein", "hei\u00b7lig", "Eh\u00b7ren\u00b7wort", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJD", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn er lebt in seinen S\u00f6hnen", "tokens": ["Denn", "er", "lebt", "in", "sei\u00b7nen", "S\u00f6h\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Und in unsern Herzen noch,", "tokens": ["Und", "in", "un\u00b7sern", "Her\u00b7zen", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Darum la\u00dft zum Schlu\u00df ert\u00f6nen", "tokens": ["Da\u00b7rum", "la\u00dft", "zum", "Schlu\u00df", "er\u00b7t\u00f6\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "APPRART", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Ihm ein dreifach ", "tokens": ["Ihm", "ein", "drei\u00b7fach"], "token_info": ["word", "word", "word"], "pos": ["PPER", "ART", "NN"], "meter": "+-+-", "measure": "trochaic.di"}}}}}