{"textgrid.poem.52861": {"metadata": {"author": {"name": "Eichrodt, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "4. Gesang", "genre": "verse", "period": "N.A.", "pub_year": 1859, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Schier h\u00e4tte der Noah noch H\u00e4ndel gekriegt,", "tokens": ["Schier", "h\u00e4t\u00b7te", "der", "Noah", "noch", "H\u00e4n\u00b7del", "ge\u00b7kriegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NE", "ADV", "NN", "VVPP", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mit seinem Herrn Stammgast, der also verf\u00fcgt:", "tokens": ["Mit", "sei\u00b7nem", "Herrn", "Stamm\u00b7gast", ",", "der", "al\u00b7so", "ver\u00b7f\u00fcgt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "$,", "PRELS", "ADV", "VVPP", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Die Nilpferd, Kr\u00f6ten und Schlangen", "tokens": ["Die", "Nil\u00b7pferd", ",", "Kr\u00f6\u00b7ten", "und", "Schlan\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Zur Rettung rasch einzufangen.", "tokens": ["Zur", "Ret\u00b7tung", "rasch", "ein\u00b7zu\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "VVIZU", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Und der Noah sprach: Nimm den Regenschirm,", "tokens": ["Und", "der", "Noah", "sprach", ":", "Nimm", "den", "Re\u00b7gen\u00b7schirm", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NE", "VVFIN", "$.", "NE", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Hausknecht, und suche du das Gew\u00fcrm.", "tokens": ["Haus\u00b7knecht", ",", "und", "su\u00b7che", "du", "das", "Ge\u00b7w\u00fcrm", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Ich selber treib in die Scheuer", "tokens": ["Ich", "sel\u00b7ber", "treib", "in", "die", "Scheu\u00b7er"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Einstweilen die Wiederk\u00e4uer.", "tokens": ["Einst\u00b7wei\u00b7len", "die", "Wie\u00b7der\u00b7k\u00e4u\u00b7er", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Es war kein Spa\u00df. Was kreucht und fleucht", "tokens": ["Es", "war", "kein", "Spa\u00df", ".", "Was", "kreucht", "und", "fleucht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "$.", "PWS", "VVFIN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zusammen zu bringen, es war nicht leicht.", "tokens": ["Zu\u00b7sam\u00b7men", "zu", "brin\u00b7gen", ",", "es", "war", "nicht", "leicht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und der Noah wurde hitzig", "tokens": ["Und", "der", "Noah", "wur\u00b7de", "hit\u00b7zig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NE", "VAFIN", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und sprach zum Herrn f\u00fcrwitzig:", "tokens": ["Und", "sprach", "zum", "Herrn", "f\u00fcr\u00b7wit\u00b7zig", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Von was aber soll dieses viele Vieh", "tokens": ["Von", "was", "a\u00b7ber", "soll", "die\u00b7ses", "vie\u00b7le", "Vieh"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADV", "VMFIN", "PDAT", "PIAT", "NN"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Denn leben, o Herr, ich traute nie,", "tokens": ["Denn", "le\u00b7ben", ",", "o", "Herr", ",", "ich", "trau\u00b7te", "nie", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$,", "FM", "NN", "$,", "PPER", "VVFIN", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da\u00df sich's wird machen lassen,", "tokens": ["Da\u00df", "sich's", "wird", "ma\u00b7chen", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VAFIN", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ihr wolltet doch auch nicht ", "tokens": ["Ihr", "woll\u00b7tet", "doch", "auch", "nicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "PTKNEG"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Der Alte brummt, dann aber versetzt", "tokens": ["Der", "Al\u00b7te", "brummt", ",", "dann", "a\u00b7ber", "ver\u00b7setzt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "ADV", "ADV", "VVPP"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Er laut: \"Das ist ein dummes Geschw\u00e4tz.", "tokens": ["Er", "laut", ":", "\"", "Das", "ist", "ein", "dum\u00b7mes", "Ge\u00b7schw\u00e4tz", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "$.", "$(", "PDS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "So ist's, so steht es geschrieben!", "tokens": ["So", "ist's", ",", "so", "steht", "es", "ge\u00b7schrie\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$,", "ADV", "VVFIN", "PPER", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Willst Du mich ", "tokens": ["Willst", "Du", "mich"], "token_info": ["word", "word", "word"], "pos": ["VMFIN", "PPER", "PRF"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "Ei, dachte der kluge Schwanenwirth,", "tokens": ["Ei", ",", "dach\u00b7te", "der", "klu\u00b7ge", "Schwa\u00b7nen\u00b7wirth", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Doch hat er sein Maul nimmer dranriskirt,", "tokens": ["Doch", "hat", "er", "sein", "Maul", "nim\u00b7mer", "dran\u00b7ris\u00b7kirt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Seine wunderlichen Sachen", "tokens": ["Sei\u00b7ne", "wun\u00b7der\u00b7li\u00b7chen", "Sa\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "K\u00f6nnt Er ", "tokens": ["K\u00f6nnt", "Er"], "token_info": ["word", "word"], "pos": ["VMFIN", "PPER"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.7": {"line.1": {"text": "Er kann ja Alles, was braucht er denn mich?", "tokens": ["Er", "kann", "ja", "Al\u00b7les", ",", "was", "braucht", "er", "denn", "mich", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIS", "$,", "PWS", "VVFIN", "PPER", "ADV", "PPER", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Was mu\u00df er mich plagen mit alle dem Viech?", "tokens": ["Was", "mu\u00df", "er", "mich", "pla\u00b7gen", "mit", "al\u00b7le", "dem", "Viech", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PRF", "VVFIN", "APPR", "PIS", "ART", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Ich soll nicht mucksen, nicht klagen -", "tokens": ["Ich", "soll", "nicht", "muck\u00b7sen", ",", "nicht", "kla\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "PTKNEG", "VVINF", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Was wird meine Frau dazu sagen?", "tokens": ["Was", "wird", "mei\u00b7ne", "Frau", "da\u00b7zu", "sa\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPOSAT", "NN", "PAV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.8": {"line.1": {"text": "Doch baute der Noah sein viehm\u00e4\u00dfig Haus,", "tokens": ["Doch", "bau\u00b7te", "der", "Noah", "sein", "vieh\u00b7m\u00e4\u00b7\u00dfig", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NE", "PPOSAT", "CARD", "NN", "$,"], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und h\u00e4ngte den Schild auch des Schwanen heraus,", "tokens": ["Und", "h\u00e4ng\u00b7te", "den", "Schild", "auch", "des", "Schwa\u00b7nen", "he\u00b7raus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Thut Menschen und Vieh drein stecken,", "tokens": ["Thut", "Men\u00b7schen", "und", "Vieh", "drein", "ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "ADV", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und lustig den Hergott dann wecken.", "tokens": ["Und", "lus\u00b7tig", "den", "Her\u00b7gott", "dann", "we\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.9": {"line.1": {"text": "Der freut sich, da\u00df Alles schon ist arranschirt,", "tokens": ["Der", "freut", "sich", ",", "da\u00df", "Al\u00b7les", "schon", "ist", "ar\u00b7ran\u00b7schirt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "$,", "KOUS", "PIS", "ADV", "VAFIN", "VVPP", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Und lobet den Noah, den Schwanenwirth,", "tokens": ["Und", "lo\u00b7bet", "den", "Noah", ",", "den", "Schwa\u00b7nen\u00b7wirth", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NE", "$,", "ART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Begibt sich zum Himmel verwundert,", "tokens": ["Be\u00b7gibt", "sich", "zum", "Him\u00b7mel", "ver\u00b7wun\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und schickt die S\u00fcndfluth herunter.", "tokens": ["Und", "schickt", "die", "S\u00fcnd\u00b7fluth", "her\u00b7un\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}}}}