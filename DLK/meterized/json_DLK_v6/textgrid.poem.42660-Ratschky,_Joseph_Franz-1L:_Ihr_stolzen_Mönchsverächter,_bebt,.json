{"textgrid.poem.42660": {"metadata": {"author": {"name": "Ratschky, Joseph Franz", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ihr stolzen M\u00f6nchsver\u00e4chter, bebt,", "genre": "verse", "period": "N.A.", "pub_year": 1783, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr stolzen M\u00f6nchsver\u00e4chter, bebt,", "tokens": ["Ihr", "stol\u00b7zen", "M\u00f6nchs\u00b7ver\u00b7\u00e4ch\u00b7ter", ",", "bebt", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und nehmt euch wohl in Acht!", "tokens": ["Und", "nehmt", "euch", "wohl", "in", "Acht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "CARD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ich weiss ein heilsames Recept,", "tokens": ["Ich", "weiss", "ein", "heil\u00b7sa\u00b7mes", "Re\u00b7cept", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Das Orthodoxen macht.", "tokens": ["Das", "Or\u00b7tho\u00b7do\u00b7xen", "macht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Ihr wisst, Nabuchodonosor", "tokens": ["Ihr", "wisst", ",", "Na\u00b7buc\u00b7ho\u00b7do\u00b7no\u00b7sor"], "token_info": ["word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "War auch den M\u00f6nchen gram:", "tokens": ["War", "auch", "den", "M\u00f6n\u00b7chen", "gram", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Verriegelt waren Th\u00fcr' und Thor,", "tokens": ["Ver\u00b7rie\u00b7gelt", "wa\u00b7ren", "Th\u00fcr'", "und", "Thor", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sobald ein Sammler kam.", "tokens": ["So\u00b7bald", "ein", "Samm\u00b7ler", "kam", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Legenden schalt er ein Gedicht,", "tokens": ["Le\u00b7gen\u00b7den", "schalt", "er", "ein", "Ge\u00b7dicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Trug nie ein Skapulier,", "tokens": ["Trug", "nie", "ein", "Ska\u00b7pu\u00b7lier", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und sch\u00e4tzte Lukaszettel nicht", "tokens": ["Und", "sch\u00e4tz\u00b7te", "Lu\u00b7kas\u00b7zet\u00b7tel", "nicht"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Viel mehr, als L\u00f6schpapier.", "tokens": ["Viel", "mehr", ",", "als", "L\u00f6schpa\u00b7pier", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "NN", "$."], "meter": "-+---", "measure": "dactylic.init"}}, "stanza.4": {"line.1": {"text": "Der M\u00f6nche hochgeweihte Schaar,", "tokens": ["Der", "M\u00f6n\u00b7che", "hoch\u00b7ge\u00b7weih\u00b7te", "Schaar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die leider! nun nicht mehr", "tokens": ["Die", "lei\u00b7der", "!", "nun", "nicht", "mehr"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADV", "$.", "ADV", "PTKNEG", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bey Hofe Hahn im Korbe war,", "tokens": ["Bey", "Ho\u00b7fe", "Hahn", "im", "Kor\u00b7be", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NE", "APPRART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verdross der Unfug sehr.", "tokens": ["Ver\u00b7dross", "der", "Un\u00b7fug", "sehr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Was dermaleinst in jener Welt", "tokens": ["Was", "der\u00b7ma\u00b7leinst", "in", "je\u00b7ner", "Welt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Frevler wiederf\u00e4hrt,", "tokens": ["Dem", "Frev\u00b7ler", "wie\u00b7der\u00b7f\u00e4hrt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ward zwar oft ernstlich vorgestellt,", "tokens": ["Ward", "zwar", "oft", "ernst\u00b7lich", "vor\u00b7ge\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Doch l\u00e4chelnd angeh\u00f6rt.", "tokens": ["Doch", "l\u00e4\u00b7chelnd", "an\u00b7ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Man rief umsonst, der Antichrist", "tokens": ["Man", "rief", "um\u00b7sonst", ",", "der", "An\u00b7ti\u00b7ch\u00b7rist"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "$,", "ART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Mit Sack und Pack sey da:", "tokens": ["Mit", "Sack", "und", "Pack", "sey", "da", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VAFIN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er lachte nur, der Atheist!", "tokens": ["Er", "lach\u00b7te", "nur", ",", "der", "A\u00b7theist", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Doch h\u00f6rt nun, was geschah.", "tokens": ["Doch", "h\u00f6rt", "nun", ",", "was", "ge\u00b7schah", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "$,", "PWS", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Die Strafe kam in vollem Lauf:", "tokens": ["Die", "Stra\u00b7fe", "kam", "in", "vol\u00b7lem", "Lauf", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Frevler ward ein Ochs,", "tokens": ["Der", "Frev\u00b7ler", "ward", "ein", "Ochs", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Frass Heu und Gras, und wurde drauf", "tokens": ["Frass", "Heu", "und", "Gras", ",", "und", "wur\u00b7de", "drauf"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NN", "KON", "NN", "$,", "KON", "VAFIN", "PAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00c4chtm\u00f6nchisch orthodox.", "tokens": ["\u00c4cht\u00b7m\u00f6n\u00b7chisch", "or\u00b7tho\u00b7dox", "."], "token_info": ["word", "word", "punct"], "pos": ["FM.la", "FM.la", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}