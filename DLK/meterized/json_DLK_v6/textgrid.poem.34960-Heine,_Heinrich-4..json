{"textgrid.poem.34960": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "4.", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Meine Frau ist nicht zufrieden", "tokens": ["Mei\u00b7ne", "Frau", "ist", "nicht", "zu\u00b7frie\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mit dem vorigen Kapitel,", "tokens": ["Mit", "dem", "vo\u00b7ri\u00b7gen", "Ka\u00b7pi\u00b7tel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ganz besonders in bezug", "tokens": ["Ganz", "be\u00b7son\u00b7ders", "in", "be\u00b7zug"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Auf das K\u00e4stchen des Darius.", "tokens": ["Auf", "das", "K\u00e4st\u00b7chen", "des", "Da\u00b7ri\u00b7us", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NE", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.2": {"line.1": {"text": "Fast mit Bitterkeit bemerkt sie:", "tokens": ["Fast", "mit", "Bit\u00b7ter\u00b7keit", "be\u00b7merkt", "sie", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVFIN", "PPER", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df ein Ehemann, der wahrhaft", "tokens": ["Da\u00df", "ein", "E\u00b7he\u00b7mann", ",", "der", "wahr\u00b7haft"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ART", "NN", "$,", "PRELS", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Religi\u00f6se sei, das K\u00e4stchen", "tokens": ["Re\u00b7li\u00b7gi\u00b7\u00f6\u00b7se", "sei", ",", "das", "K\u00e4st\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "VAFIN", "$,", "ART", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Gleich zu Gelde machen w\u00fcrde,", "tokens": ["Gleich", "zu", "Gel\u00b7de", "ma\u00b7chen", "w\u00fcr\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVINF", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Um damit f\u00fcr seine arme", "tokens": ["Um", "da\u00b7mit", "f\u00fcr", "sei\u00b7ne", "ar\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUI", "PAV", "APPR", "PPOSAT", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Legitime Ehegattin", "tokens": ["Le\u00b7gi\u00b7ti\u00b7me", "E\u00b7he\u00b7gat\u00b7tin"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einen Kaschemir zu kaufen,", "tokens": ["Ei\u00b7nen", "Ka\u00b7sche\u00b7mir", "zu", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dessen sie so sehr bed\u00fcrfe.", "tokens": ["Des\u00b7sen", "sie", "so", "sehr", "be\u00b7d\u00fcr\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Jehuda ben Halevy,", "tokens": ["Der", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Meinte sie, der sei hinl\u00e4nglich", "tokens": ["Mein\u00b7te", "sie", ",", "der", "sei", "hin\u00b7l\u00e4ng\u00b7lich"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "PRELS", "VAFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ehrenvoll bewahrt in einem", "tokens": ["Eh\u00b7ren\u00b7voll", "be\u00b7wahrt", "in", "ei\u00b7nem"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "VVPP", "APPR", "ART"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Sch\u00f6nen Futteral von Pappe", "tokens": ["Sch\u00f6\u00b7nen", "Fut\u00b7te\u00b7ral", "von", "Pap\u00b7pe"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Mit chinesisch eleganten", "tokens": ["Mit", "chi\u00b7ne\u00b7sisch", "e\u00b7leg\u00b7an\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJD", "ADJA"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Arabesken, wie die h\u00fcbschen", "tokens": ["A\u00b7ra\u00b7bes\u00b7ken", ",", "wie", "die", "h\u00fcb\u00b7schen"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "PWAV", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Bonbonnieren von Marquis", "tokens": ["Bon\u00b7bon\u00b7nie\u00b7ren", "von", "Mar\u00b7quis"], "token_info": ["word", "word", "word"], "pos": ["NN", "APPR", "NE"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Im Passage-Panorama.", "tokens": ["Im", "Pas\u00b7sa\u00b7ge\u00b7Pan\u00b7ora\u00b7ma", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbsonderbar!\u00ab \u2013 setzt sie hinzu \u2013", "tokens": ["\u00bb", "son\u00b7der\u00b7bar", "!", "\u00ab", "\u2013", "setzt", "sie", "hin\u00b7zu", "\u2013"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "$.", "$(", "$(", "VVFIN", "PPER", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "\u00bbda\u00df ich niemals nennen h\u00f6rte", "tokens": ["\u00bb", "da\u00df", "ich", "nie\u00b7mals", "nen\u00b7nen", "h\u00f6r\u00b7te"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "KOUS", "PPER", "ADV", "VVINF", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Diesen gro\u00dfen Dichternamen,", "tokens": ["Die\u00b7sen", "gro\u00b7\u00dfen", "Dich\u00b7ter\u00b7na\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Jehuda ben Halevy.\u00ab", "tokens": ["Den", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NE", "NE", "NE", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Liebstes Kind, gab ich zur Antwort,", "tokens": ["Liebs\u00b7tes", "Kind", ",", "gab", "ich", "zur", "Ant\u00b7wort", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Solche holde Ignoranz,", "tokens": ["Sol\u00b7che", "hol\u00b7de", "Ig\u00b7no\u00b7ranz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie bekundet die Lakunen", "tokens": ["Sie", "be\u00b7kun\u00b7det", "die", "La\u00b7ku\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Der franz\u00f6sischen Erziehung,", "tokens": ["Der", "fran\u00b7z\u00f6\u00b7si\u00b7schen", "Er\u00b7zie\u00b7hung", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Der Pariser Pensionate,", "tokens": ["Der", "Pa\u00b7ri\u00b7ser", "Pen\u00b7si\u00b7o\u00b7na\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wo die M\u00e4dchen, diese k\u00fcnft'gen", "tokens": ["Wo", "die", "M\u00e4d\u00b7chen", ",", "die\u00b7se", "k\u00fcnft'\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PWAV", "ART", "NN", "$,", "PDAT", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fctter eines freien Volkes,", "tokens": ["M\u00fct\u00b7ter", "ei\u00b7nes", "frei\u00b7en", "Vol\u00b7kes", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihren Unterricht genie\u00dfen \u2013", "tokens": ["Ih\u00b7ren", "Un\u00b7ter\u00b7richt", "ge\u00b7nie\u00b7\u00dfen", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Alte Mumien, ausgestopfte", "tokens": ["Al\u00b7te", "Mu\u00b7mi\u00b7en", ",", "aus\u00b7ge\u00b7stopf\u00b7te"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADJA", "NN", "$,", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Pharaonen von \u00c4gypten,", "tokens": ["Pha\u00b7ra\u00b7o\u00b7nen", "von", "\u00c4\u00b7gyp\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,"], "meter": "+-+---+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Merowinger Schattenk\u00f6n'ge,", "tokens": ["Me\u00b7ro\u00b7win\u00b7ger", "Schat\u00b7ten\u00b7k\u00f6n'\u00b7ge", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ungepuderte Per\u00fccken,", "tokens": ["Un\u00b7ge\u00b7pu\u00b7der\u00b7te", "Pe\u00b7r\u00fc\u00b7cken", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Auch die Zopfmonarchen Chinas,", "tokens": ["Auch", "die", "Zopf\u00b7mo\u00b7nar\u00b7chen", "Chi\u00b7nas", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Porzellanpagodenkaiser \u2013", "tokens": ["Por\u00b7zel\u00b7lan\u00b7pa\u00b7go\u00b7den\u00b7kai\u00b7ser", "\u2013"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Alle lernen sie auswendig,", "tokens": ["Al\u00b7le", "ler\u00b7nen", "sie", "aus\u00b7wen\u00b7dig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Kluge M\u00e4dchen, aber Himmel \u2013", "tokens": ["Klu\u00b7ge", "M\u00e4d\u00b7chen", ",", "a\u00b7ber", "Him\u00b7mel", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "KON", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Fragt man sie nach gro\u00dfen Namen", "tokens": ["Fragt", "man", "sie", "nach", "gro\u00b7\u00dfen", "Na\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "PPER", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aus dem gro\u00dfen Goldzeitalter", "tokens": ["Aus", "dem", "gro\u00b7\u00dfen", "Gold\u00b7zei\u00b7tal\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der arabisch-althispanisch", "tokens": ["Der", "a\u00b7ra\u00b7bischal\u00b7thi\u00b7spa\u00b7nisch"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "J\u00fcdischen Poetenschule,", "tokens": ["J\u00fc\u00b7di\u00b7schen", "Poe\u00b7ten\u00b7schu\u00b7le", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.12": {"line.1": {"text": "Fragt man nach dem Dreigestirn,", "tokens": ["Fragt", "man", "nach", "dem", "Drei\u00b7ges\u00b7tirn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Nach Jehuda ben Halevy,", "tokens": ["Nach", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nach dem Salomon Gabirol", "tokens": ["Nach", "dem", "Sa\u00b7lo\u00b7mon", "Ga\u00b7bi\u00b7rol"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NE", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und dem Moses Iben Esra \u2013", "tokens": ["Und", "dem", "Mo\u00b7ses", "I\u00b7ben", "Es\u00b7ra", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NE", "NE", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Fragt man nach dergleichen Namen,", "tokens": ["Fragt", "man", "nach", "derg\u00b7lei\u00b7chen", "Na\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PIS", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dann mit gro\u00dfen Augen schaun", "tokens": ["Dann", "mit", "gro\u00b7\u00dfen", "Au\u00b7gen", "schaun"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADJA", "NN", "VVINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Uns die Kleinen an \u2013 alsdann", "tokens": ["Uns", "die", "Klei\u00b7nen", "an", "\u2013", "als\u00b7dann"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "ART", "NN", "PTKVZ", "$(", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Stehn am Berge die Ochsinnen.", "tokens": ["Stehn", "am", "Ber\u00b7ge", "die", "Och\u00b7sin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Raten m\u00f6cht ich dir, Geliebte,", "tokens": ["Ra\u00b7ten", "m\u00f6cht", "ich", "dir", ",", "Ge\u00b7lieb\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PPER", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nachzuholen das Vers\u00e4umte", "tokens": ["Nach\u00b7zu\u00b7ho\u00b7len", "das", "Ver\u00b7s\u00e4um\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Hebr\u00e4isch zu erlernen \u2013", "tokens": ["Und", "Heb\u00b7r\u00e4\u00b7isch", "zu", "er\u00b7ler\u00b7nen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "La\u00df Theater und Konzerte,", "tokens": ["La\u00df", "The\u00b7a\u00b7ter", "und", "Kon\u00b7zer\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "KON", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.15": {"line.1": {"text": "Widme ein'ge Jahre solchem", "tokens": ["Wid\u00b7me", "ein'\u00b7ge", "Jah\u00b7re", "sol\u00b7chem"], "token_info": ["word", "word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Studium, du kannst alsdann", "tokens": ["Stu\u00b7di\u00b7um", ",", "du", "kannst", "als\u00b7dann"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "PPER", "VMFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Im Originale lesen", "tokens": ["Im", "O\u00b7rig\u00b7i\u00b7na\u00b7le", "le\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "NN", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Iben Esra und Gabirol", "tokens": ["I\u00b7ben", "Es\u00b7ra", "und", "Ga\u00b7bi\u00b7rol"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "NE", "KON", "NE"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.16": {"line.1": {"text": "Und versteht sich den Halevy,", "tokens": ["Und", "ver\u00b7steht", "sich", "den", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Das Triumvirat der Dichtkunst,", "tokens": ["Das", "Tri\u00b7um\u00b7vi\u00b7rat", "der", "Dicht\u00b7kunst", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+--+---", "measure": "iambic.di.relaxed"}, "line.3": {"text": "Das dem Saitenspiel Davidis", "tokens": ["Das", "dem", "Sai\u00b7ten\u00b7spiel", "Da\u00b7vi\u00b7dis"], "token_info": ["word", "word", "word", "word"], "pos": ["PDS", "ART", "NN", "NE"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Einst entlockt die sch\u00f6nsten Laute.", "tokens": ["Einst", "ent\u00b7lockt", "die", "sch\u00f6ns\u00b7ten", "Lau\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Alcharisi \u2013 der, ich wette,", "tokens": ["Al\u00b7cha\u00b7ri\u00b7si", "\u2013", "der", ",", "ich", "wet\u00b7te", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$(", "ART", "$,", "PPER", "VVFIN", "$,"], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.2": {"text": "Dir nicht minder unbekannt ist,", "tokens": ["Dir", "nicht", "min\u00b7der", "un\u00b7be\u00b7kannt", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "ADV", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob er gleich, franz\u00f6s'scher Witzbold,", "tokens": ["Ob", "er", "gleich", ",", "fran\u00b7z\u00f6s'\u00b7scher", "Witz\u00b7bold", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Hariri \u00fcberwitzelt", "tokens": ["Den", "Ha\u00b7ri\u00b7ri", "\u00fc\u00b7ber\u00b7wit\u00b7zelt"], "token_info": ["word", "word", "word"], "pos": ["ART", "NE", "VVFIN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.18": {"line.1": {"text": "Im Gebiete der Makame,", "tokens": ["Im", "Ge\u00b7bie\u00b7te", "der", "Ma\u00b7ka\u00b7me", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und ein Voltairianer war", "tokens": ["Und", "ein", "Vol\u00b7tai\u00b7ri\u00b7a\u00b7ner", "war"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Schon sechshundert Jahr' vor Voltair' \u2013", "tokens": ["Schon", "sechs\u00b7hun\u00b7dert", "Jahr'", "vor", "Vol\u00b7tair'", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "APPR", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Jener Alcharisi sagte:", "tokens": ["Je\u00b7ner", "Al\u00b7cha\u00b7ri\u00b7si", "sag\u00b7te", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VVFIN", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}}, "stanza.19": {"line.1": {"text": "\u00bbdurch Gedanken gl\u00e4nzt Gabirol", "tokens": ["\u00bb", "durch", "Ge\u00b7dan\u00b7ken", "gl\u00e4nzt", "Ga\u00b7bi\u00b7rol"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "APPR", "NN", "VVFIN", "NE"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und gef\u00e4llt zumeist dem Denker,", "tokens": ["Und", "ge\u00b7f\u00e4llt", "zu\u00b7meist", "dem", "Den\u00b7ker", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Iben Esra gl\u00e4nzt durch Kunst", "tokens": ["I\u00b7ben", "Es\u00b7ra", "gl\u00e4nzt", "durch", "Kunst"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "VVFIN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und behagt weit mehr dem K\u00fcnstler \u2013", "tokens": ["Und", "be\u00b7hagt", "weit", "mehr", "dem", "K\u00fcnst\u00b7ler", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "ADV", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "Aber beider Eigenschaften", "tokens": ["A\u00b7ber", "bei\u00b7der", "Ei\u00b7gen\u00b7schaf\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat Jehuda ben Halevy,", "tokens": ["Hat", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und er ist ein gro\u00dfer Dichter", "tokens": ["Und", "er", "ist", "ein", "gro\u00b7\u00dfer", "Dich\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und ein Liebling aller Menschen.\u00ab", "tokens": ["Und", "ein", "Lieb\u00b7ling", "al\u00b7ler", "Men\u00b7schen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "PIAT", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.21": {"line.1": {"text": "Iben Esra war ein Freund", "tokens": ["I\u00b7ben", "Es\u00b7ra", "war", "ein", "Freund"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NE", "VAFIN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und, ich glaube, auch ein Vetter", "tokens": ["Und", ",", "ich", "glau\u00b7be", ",", "auch", "ein", "Vet\u00b7ter"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "PPER", "VVFIN", "$,", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Des Jehuda ben Halevy,", "tokens": ["Des", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der in seinem Wanderbuche", "tokens": ["Der", "in", "sei\u00b7nem", "Wan\u00b7der\u00b7bu\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "Schmerzlich klagt, wie er vergebens", "tokens": ["Schmerz\u00b7lich", "klagt", ",", "wie", "er", "ver\u00b7ge\u00b7bens"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "$,", "PWAV", "PPER", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In Granada aufgesucht hat", "tokens": ["In", "Gra\u00b7na\u00b7da", "auf\u00b7ge\u00b7sucht", "hat"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "VVPP", "VAFIN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Seinen Freund, und nur den Bruder", "tokens": ["Sei\u00b7nen", "Freund", ",", "und", "nur", "den", "Bru\u00b7der"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "KON", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dorten fand, den Medikus,", "tokens": ["Dor\u00b7ten", "fand", ",", "den", "Me\u00b7di\u00b7kus", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Rabbi Meyer, auch ein Dichter", "tokens": ["Rab\u00b7bi", "Me\u00b7yer", ",", "auch", "ein", "Dich\u00b7ter"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "NE", "$,", "ADV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und der Vater jener Sch\u00f6nen,", "tokens": ["Und", "der", "Va\u00b7ter", "je\u00b7ner", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die mit hoffnungsloser Flamme", "tokens": ["Die", "mit", "hoff\u00b7nungs\u00b7lo\u00b7ser", "Flam\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Iben Esras Herz entzunden \u2013", "tokens": ["I\u00b7ben", "Es\u00b7ras", "Herz", "ent\u00b7zun\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NN", "VVPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Um das M\u00fchmchen zu vergessen,", "tokens": ["Um", "das", "M\u00fchm\u00b7chen", "zu", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Griff er nach dem Wanderstabe,", "tokens": ["Griff", "er", "nach", "dem", "Wan\u00b7der\u00b7sta\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie so mancher der Kollegen;", "tokens": ["Wie", "so", "man\u00b7cher", "der", "Kol\u00b7le\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIS", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Lebte unstet, heimatlos.", "tokens": ["Leb\u00b7te", "un\u00b7stet", ",", "hei\u00b7mat\u00b7los", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADJD", "$,", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Pilgernd nach Jerusalem,", "tokens": ["Pil\u00b7gernd", "nach", "Je\u00b7ru\u00b7sa\u00b7lem", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "\u00dcberfielen ihn Tartaren,", "tokens": ["\u00dc\u00b7berf\u00b7ie\u00b7len", "ihn", "Tar\u00b7ta\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die an einen Gaul gebunden", "tokens": ["Die", "an", "ei\u00b7nen", "Gaul", "ge\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "ART", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ihn nach ihren Steppen schleppten.", "tokens": ["Ihn", "nach", "ih\u00b7ren", "Step\u00b7pen", "schlepp\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Mu\u00dfte Dienste dort verrichten,", "tokens": ["Mu\u00df\u00b7te", "Diens\u00b7te", "dort", "ver\u00b7rich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die nicht w\u00fcrdig eines Rabbi", "tokens": ["Die", "nicht", "w\u00fcr\u00b7dig", "ei\u00b7nes", "Rab\u00b7bi"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PTKNEG", "ADJD", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und noch wen'ger eines Dichters,", "tokens": ["Und", "noch", "wen'\u00b7ger", "ei\u00b7nes", "Dich\u00b7ters", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIS", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mu\u00dfte n\u00e4mlich K\u00fche melken.", "tokens": ["Mu\u00df\u00b7te", "n\u00e4m\u00b7lich", "K\u00fc\u00b7he", "mel\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "Einstens, als er unterm Bauche", "tokens": ["Eins\u00b7tens", ",", "als", "er", "un\u00b7term", "Bau\u00b7che"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Einer Kuh gekauert sa\u00df,", "tokens": ["Ei\u00b7ner", "Kuh", "ge\u00b7kau\u00b7ert", "sa\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihre Euter hastig fingernd,", "tokens": ["Ih\u00b7re", "Eu\u00b7ter", "has\u00b7tig", "fin\u00b7gernd", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df die Milch flo\u00df in den Zuber \u2013", "tokens": ["Da\u00df", "die", "Milch", "flo\u00df", "in", "den", "Zu\u00b7ber", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Eine Position, unw\u00fcrdig", "tokens": ["Ei\u00b7ne", "Po\u00b7si\u00b7ti\u00b7on", ",", "un\u00b7w\u00fcr\u00b7dig"], "token_info": ["word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "ADJD"], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Eines Rabbis, eines Dichters \u2013", "tokens": ["Ei\u00b7nes", "Rab\u00b7bis", ",", "ei\u00b7nes", "Dich\u00b7ters", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da befiel ihn tiefe Wehmut,", "tokens": ["Da", "be\u00b7fiel", "ihn", "tie\u00b7fe", "Weh\u00b7mut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er fing zu singen an,", "tokens": ["Und", "er", "fing", "zu", "sin\u00b7gen", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKZU", "VVINF", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.29": {"line.1": {"text": "Und er sang so sch\u00f6n und lieblich,", "tokens": ["Und", "er", "sang", "so", "sch\u00f6n", "und", "lieb\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df der Khan, der F\u00fcrst der Horde,", "tokens": ["Da\u00df", "der", "Khan", ",", "der", "F\u00fcrst", "der", "Hor\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der vorbeiging, ward ger\u00fchret", "tokens": ["Der", "vor\u00b7bei\u00b7ging", ",", "ward", "ge\u00b7r\u00fch\u00b7ret"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "$,", "VAFIN", "VVPP"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Und die Freiheit gab dem Sklaven.", "tokens": ["Und", "die", "Frei\u00b7heit", "gab", "dem", "Skla\u00b7ven", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.30": {"line.1": {"text": "Auch Geschenke gab er ihm,", "tokens": ["Auch", "Ge\u00b7schen\u00b7ke", "gab", "er", "ihm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "PPER", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Einen Fuchspelz, eine lange", "tokens": ["Ei\u00b7nen", "Fuchs\u00b7pelz", ",", "ei\u00b7ne", "lan\u00b7ge"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sarazenenmandoline", "tokens": ["Sa\u00b7ra\u00b7ze\u00b7nen\u00b7man\u00b7do\u00b7li\u00b7ne"], "token_info": ["word"], "pos": ["NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und das Zehrgeld f\u00fcr die Heimkehr.", "tokens": ["Und", "das", "Zehr\u00b7geld", "f\u00fcr", "die", "Heim\u00b7kehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.31": {"line.1": {"text": "Dichterschicksal! b\u00f6ser Unstern,", "tokens": ["Dich\u00b7ter\u00b7schick\u00b7sal", "!", "b\u00f6\u00b7ser", "Uns\u00b7tern", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$.", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Der die S\u00f6hne des Apollo", "tokens": ["Der", "die", "S\u00f6h\u00b7ne", "des", "A\u00b7pol\u00b7lo"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ART", "NE"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "T\u00f6dlich nergelt, und sogar", "tokens": ["T\u00f6d\u00b7lich", "ner\u00b7gelt", ",", "und", "so\u00b7gar"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADJD", "VVFIN", "$,", "KON", "ADV"], "meter": "+-----+", "measure": "dactylic.init"}, "line.4": {"text": "Ihren Vater nicht verschont hat,", "tokens": ["Ih\u00b7ren", "Va\u00b7ter", "nicht", "ver\u00b7schont", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "Als er, hinter Daphnen laufend,", "tokens": ["Als", "er", ",", "hin\u00b7ter", "Daph\u00b7nen", "lau\u00b7fend", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "NN", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Statt des wei\u00dfen Nymphenleibes", "tokens": ["Statt", "des", "wei\u00b7\u00dfen", "Nym\u00b7phen\u00b7lei\u00b7bes"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nur den Lorbeerbaum erfa\u00dfte,", "tokens": ["Nur", "den", "Lor\u00b7beer\u00b7baum", "er\u00b7fa\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Er, der g\u00f6ttliche Schlemihl!", "tokens": ["Er", ",", "der", "g\u00f6tt\u00b7li\u00b7che", "Schle\u00b7mihl", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Ja, der hohe Delphier ist", "tokens": ["Ja", ",", "der", "ho\u00b7he", "Del\u00b7phier", "ist"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "ART", "ADJA", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ein Schlemihl, und gar der Lorbeer,", "tokens": ["Ein", "Schle\u00b7mihl", ",", "und", "gar", "der", "Lor\u00b7beer", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KON", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der so stolz die Stirne kr\u00f6net,", "tokens": ["Der", "so", "stolz", "die", "Stir\u00b7ne", "kr\u00f6\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist ein Zeichen des Schlemihltums.", "tokens": ["Ist", "ein", "Zei\u00b7chen", "des", "Schle\u00b7mihl\u00b7tums", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.34": {"line.1": {"text": "Was das Wort Schlemihl bedeutet,", "tokens": ["Was", "das", "Wort", "Schle\u00b7mihl", "be\u00b7deu\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "NE", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wissen wir. Hat doch Chamisso", "tokens": ["Wis\u00b7sen", "wir", ".", "Hat", "doch", "Cha\u00b7mis\u00b7so"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$.", "VAFIN", "ADV", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ihm das B\u00fcrgerrecht in Deutschland", "tokens": ["Ihm", "das", "B\u00fcr\u00b7ger\u00b7recht", "in", "Deutschland"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ART", "NN", "APPR", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "L\u00e4ngst verschafft, dem Worte n\u00e4mlich.", "tokens": ["L\u00e4ngst", "ver\u00b7schafft", ",", "dem", "Wor\u00b7te", "n\u00e4m\u00b7lich", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "ART", "NN", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.35": {"line.1": {"text": "Aber unbekannt geblieben,", "tokens": ["A\u00b7ber", "un\u00b7be\u00b7kannt", "ge\u00b7blie\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie des heil'gen Niles Quellen,", "tokens": ["Wie", "des", "heil'\u00b7gen", "Ni\u00b7les", "Quel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist sein Ursprung; hab dar\u00fcber", "tokens": ["Ist", "sein", "Ur\u00b7sprung", ";", "hab", "da\u00b7r\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN", "$.", "VAFIN", "PAV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nachgegr\u00fcbelt manche Nacht.", "tokens": ["Nach\u00b7ge\u00b7gr\u00fc\u00b7belt", "man\u00b7che", "Nacht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "Zu Berlin vor vielen Jahren", "tokens": ["Zu", "Ber\u00b7lin", "vor", "vie\u00b7len", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wandt ich mich deshalb an unsern", "tokens": ["Wandt", "ich", "mich", "des\u00b7halb", "an", "un\u00b7sern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "PAV", "APPR", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Freund Chamisso, suchte Auskunft", "tokens": ["Freund", "Cha\u00b7mis\u00b7so", ",", "such\u00b7te", "Aus\u00b7kunft"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "NE", "$,", "VVFIN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Beim Dekane der Schlemihle.", "tokens": ["Beim", "De\u00b7ka\u00b7ne", "der", "Schle\u00b7mih\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.37": {"line.1": {"text": "Doch er konnt mich nicht befried'gen", "tokens": ["Doch", "er", "konnt", "mich", "nicht", "be\u00b7frie\u00b7d'\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Und verwies mich drob an Hitzig,", "tokens": ["Und", "ver\u00b7wies", "mich", "drob", "an", "Hit\u00b7zig", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der ihm den Familiennamen", "tokens": ["Der", "ihm", "den", "Fa\u00b7mi\u00b7li\u00b7en\u00b7na\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "PPER", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Seines schattenlosen Peters", "tokens": ["Sei\u00b7nes", "schat\u00b7ten\u00b7lo\u00b7sen", "Pe\u00b7ters"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.38": {"line.1": {"text": "Einst verraten. Alsbald nahm ich", "tokens": ["Einst", "ver\u00b7ra\u00b7ten", ".", "Als\u00b7bald", "nahm", "ich"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVPP", "$.", "ADV", "VVFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Eine Droschke, und ich rollte", "tokens": ["Ei\u00b7ne", "Droschke", ",", "und", "ich", "roll\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "KON", "PPER", "VVFIN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Zu dem Kriminalrat Hitzig,", "tokens": ["Zu", "dem", "Kri\u00b7mi\u00b7nal\u00b7rat", "Hit\u00b7zig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welcher eh'mals Itzig hie\u00df \u2013", "tokens": ["Wel\u00b7cher", "eh'\u00b7mals", "It\u00b7zig", "hie\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "KOUS", "NE", "VVFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.39": {"line.1": {"text": "Als er noch ein Itzig war,", "tokens": ["Als", "er", "noch", "ein", "It\u00b7zig", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NE", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Tr\u00e4umte ihm, er s\u00e4h geschrieben", "tokens": ["Tr\u00e4um\u00b7te", "ihm", ",", "er", "s\u00e4h", "ge\u00b7schrie\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "PPER", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "An dem Himmel seinen Namen", "tokens": ["An", "dem", "Him\u00b7mel", "sei\u00b7nen", "Na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und davor den Buchstab' H.", "tokens": ["Und", "da\u00b7vor", "den", "Buch\u00b7stab'", "H."], "token_info": ["word", "word", "word", "word", "abbreviation"], "pos": ["KON", "PAV", "ART", "NN", "NE"], "meter": "--+-+-", "measure": "anapaest.init"}}, "stanza.40": {"line.1": {"text": "\u00bbwas bedeutet dieses H?\u00ab", "tokens": ["\u00bb", "was", "be\u00b7deu\u00b7tet", "die\u00b7ses", "H", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PDAT", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Frug er sich \u2013 \u00bbetwa Herr Itzig", "tokens": ["Frug", "er", "sich", "\u2013", "\u00bb", "et\u00b7wa", "Herr", "It\u00b7zig"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "$(", "$(", "ADV", "NN", "NE"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Oder Heil'ger Itzig? Heil'ger", "tokens": ["O\u00b7der", "Heil'\u00b7ger", "It\u00b7zig", "?", "Heil'\u00b7ger"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["KON", "NN", "NE", "$.", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist ein sch\u00f6ner Titel \u2013 aber", "tokens": ["Ist", "ein", "sch\u00f6\u00b7ner", "Ti\u00b7tel", "\u2013", "a\u00b7ber"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$(", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.41": {"line.1": {"text": "In Berlin nicht passend\u00ab \u2013 Endlich", "tokens": ["In", "Ber\u00b7lin", "nicht", "pas\u00b7send", "\u00ab", "\u2013", "End\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word"], "pos": ["APPR", "NE", "PTKNEG", "VVPP", "$(", "$(", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gr\u00fcbelnsm\u00fcd', nannt er sich Hitzig,", "tokens": ["Gr\u00fc\u00b7belns\u00b7m\u00fcd'", ",", "nannt", "er", "sich", "Hit\u00b7zig", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "PPER", "PRF", "NE", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und nur die Getreuen wu\u00dften:", "tokens": ["Und", "nur", "die", "Ge\u00b7treu\u00b7en", "wu\u00df\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "In dem Hitzig steckt ein Heil'ger.", "tokens": ["In", "dem", "Hit\u00b7zig", "steckt", "ein", "Heil'", "ger."], "token_info": ["word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["APPR", "ART", "NN", "VVFIN", "ART", "NN", "NE"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.42": {"line.1": {"text": "\u00bbheil'ger Hitzig!\u00ab sprach ich also,", "tokens": ["\u00bb", "heil'\u00b7ger", "Hit\u00b7zig", "!", "\u00ab", "sprach", "ich", "al\u00b7so", ","], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$.", "$(", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Als ich zu ihm kam, \u00bbSie sollen", "tokens": ["Als", "ich", "zu", "ihm", "kam", ",", "\u00bb", "Sie", "sol\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PPER", "VVFIN", "$,", "$(", "PPER", "VMFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mir die Etymologie", "tokens": ["Mir", "die", "E\u00b7ty\u00b7mo\u00b7lo\u00b7gie"], "token_info": ["word", "word", "word"], "pos": ["PPER", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Von dem Wort Schlemihl erkl\u00e4ren.\u00ab", "tokens": ["Von", "dem", "Wort", "Schle\u00b7mihl", "er\u00b7kl\u00e4\u00b7ren", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.43": {"line.1": {"text": "Viel Umschweife nahm der Heil'ge,", "tokens": ["Viel", "Um\u00b7schwei\u00b7fe", "nahm", "der", "Heil'\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Konnte sich nicht recht erinnern,", "tokens": ["Konn\u00b7te", "sich", "nicht", "recht", "e\u00b7rin\u00b7nern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Eine Ausflucht nach der andern,", "tokens": ["Ei\u00b7ne", "Aus\u00b7flucht", "nach", "der", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Immer christlich \u2013 bis mir endlich,", "tokens": ["Im\u00b7mer", "christ\u00b7lich", "\u2013", "bis", "mir", "end\u00b7lich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$(", "APPR", "PPER", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.44": {"line.1": {"text": "Endlich alle Kn\u00f6pfe rissen", "tokens": ["End\u00b7lich", "al\u00b7le", "Kn\u00f6p\u00b7fe", "ris\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PIAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "An der Hose der Geduld,", "tokens": ["An", "der", "Ho\u00b7se", "der", "Ge\u00b7duld", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und ich anfing so zu fluchen,", "tokens": ["Und", "ich", "an\u00b7fing", "so", "zu", "flu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PTKZU", "VVINF", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "So gottl\u00e4sterlich zu fluchen,", "tokens": ["So", "gott\u00b7l\u00e4s\u00b7ter\u00b7lich", "zu", "flu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}}, "stanza.45": {"line.1": {"text": "Da\u00df der fromme Pietist,", "tokens": ["Da\u00df", "der", "from\u00b7me", "Pie\u00b7tist", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Leichenbla\u00df und beineschlotternd,", "tokens": ["Lei\u00b7chen\u00b7bla\u00df", "und", "bei\u00b7ne\u00b7schlot\u00b7ternd", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Unverz\u00fcglich mir willfahrte", "tokens": ["Un\u00b7ver\u00b7z\u00fcg\u00b7lich", "mir", "will\u00b7fahr\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["ADJD", "PPER", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und mir folgendes erz\u00e4hlte:", "tokens": ["Und", "mir", "fol\u00b7gen\u00b7des", "er\u00b7z\u00e4hl\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PIS", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.46": {"line.1": {"text": "\u00bbin der Bibel ist zu lesen,", "tokens": ["\u00bb", "in", "der", "Bi\u00b7bel", "ist", "zu", "le\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "ART", "NN", "VAFIN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als zur Zeit der W\u00fcstenwandrung", "tokens": ["Als", "zur", "Zeit", "der", "W\u00fcs\u00b7ten\u00b7wand\u00b7rung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPRART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Israel sich oft erlustigt", "tokens": ["Is\u00b7rael", "sich", "oft", "er\u00b7lus\u00b7tigt"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "PRF", "ADV", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Mit den T\u00f6chtern Kanaans,", "tokens": ["Mit", "den", "T\u00f6ch\u00b7tern", "Ka\u00b7na\u00b7ans", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NE", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.47": {"line.1": {"text": "Da geschah es, da\u00df der Pinhas", "tokens": ["Da", "ge\u00b7schah", "es", ",", "da\u00df", "der", "Pin\u00b7has"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "ART", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Sahe, wie der edle Simri", "tokens": ["Sa\u00b7he", ",", "wie", "der", "ed\u00b7le", "Sim\u00b7ri"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PWAV", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Buhlschaft trieb mit einem Weibsbild", "tokens": ["Buhl\u00b7schaft", "trieb", "mit", "ei\u00b7nem", "Weibs\u00b7bild"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Aus dem Stamm der Kananiter,", "tokens": ["Aus", "dem", "Stamm", "der", "Ka\u00b7na\u00b7ni\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.48": {"line.1": {"text": "Und alsbald ergriff er zornig", "tokens": ["Und", "als\u00b7bald", "er\u00b7griff", "er", "zor\u00b7nig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Seinen Speer und hat den Simri", "tokens": ["Sei\u00b7nen", "Speer", "und", "hat", "den", "Sim\u00b7ri"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "VAFIN", "ART", "NE"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auf der Stelle totgestochen \u2013", "tokens": ["Auf", "der", "Stel\u00b7le", "tot\u00b7ge\u00b7sto\u00b7chen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Also hei\u00dft es in der Bibel.", "tokens": ["Al\u00b7so", "hei\u00dft", "es", "in", "der", "Bi\u00b7bel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.49": {"line.1": {"text": "Aber m\u00fcndlich \u00fcberliefert", "tokens": ["A\u00b7ber", "m\u00fcnd\u00b7lich", "\u00fc\u00b7berl\u00b7ie\u00b7fert"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJD", "VVPP"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Hat im Volke sich die Sage,", "tokens": ["Hat", "im", "Vol\u00b7ke", "sich", "die", "Sa\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPRART", "NN", "PRF", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df es nicht der Simri war,", "tokens": ["Da\u00df", "es", "nicht", "der", "Sim\u00b7ri", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ART", "NE", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den des Pinhas Speer getroffen,", "tokens": ["Den", "des", "Pin\u00b7has", "Speer", "ge\u00b7trof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.50": {"line.1": {"text": "Sondern da\u00df der Blinderz\u00fcrnte,", "tokens": ["Son\u00b7dern", "da\u00df", "der", "Blin\u00b7der\u00b7z\u00fcrn\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Statt des S\u00fcnders, unversehens", "tokens": ["Statt", "des", "S\u00fcn\u00b7ders", ",", "un\u00b7ver\u00b7se\u00b7hens"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["NN", "ART", "NN", "$,", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Einen ganz Unschuld'gen traf,", "tokens": ["Ei\u00b7nen", "ganz", "Un\u00b7schuld'\u00b7gen", "traf", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Den Schlemihl ben Zuri Schadday.\u00ab \u2013", "tokens": ["Den", "Schle\u00b7mihl", "ben", "Zu\u00b7ri", "Schad\u00b7day", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "NE", "NE", "NE", "$.", "$(", "$("], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.51": {"line.1": {"text": "Dieser nun, Schlemihl I.,", "tokens": ["Die\u00b7ser", "nun", ",", "Schle\u00b7mihl", "I.", ","], "token_info": ["word", "word", "punct", "word", "abbreviation", "punct"], "pos": ["PDS", "ADV", "$,", "NN", "NE", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Ist der Ahnherr des Geschlechtes", "tokens": ["Ist", "der", "Ahn\u00b7herr", "des", "Ge\u00b7schlech\u00b7tes"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "ART", "NN"], "meter": "+-++--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Derer von Schlemihl. Wir stammen", "tokens": ["De\u00b7rer", "von", "Schle\u00b7mihl", ".", "Wir", "stam\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PDS", "APPR", "NN", "$.", "PPER", "VVFIN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Von Schlemihl ben Zuri Schadday.", "tokens": ["Von", "Schle\u00b7mihl", "ben", "Zu\u00b7ri", "Schad\u00b7day", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "NE", "NE", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.52": {"line.1": {"text": "Freilich keine Heldentaten", "tokens": ["Frei\u00b7lich", "kei\u00b7ne", "Hel\u00b7den\u00b7ta\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["ADV", "PIAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Meldet man von ihm, wir kennen", "tokens": ["Mel\u00b7det", "man", "von", "ihm", ",", "wir", "ken\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PIS", "APPR", "PPER", "$,", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nur den Namen und wir wissen,", "tokens": ["Nur", "den", "Na\u00b7men", "und", "wir", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "KON", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df er ein Schlemihl gewesen.", "tokens": ["Da\u00df", "er", "ein", "Schle\u00b7mihl", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VAPP", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.53": {"line.1": {"text": "Doch gesch\u00e4tzet wird ein Stammbaum", "tokens": ["Doch", "ge\u00b7sch\u00e4t\u00b7zet", "wird", "ein", "Stamm\u00b7baum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nicht ob seinen guten Fr\u00fcchten,", "tokens": ["Nicht", "ob", "sei\u00b7nen", "gu\u00b7ten", "Fr\u00fcch\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "KOUS", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sondern nur ob seinem Alter \u2013", "tokens": ["Son\u00b7dern", "nur", "ob", "sei\u00b7nem", "Al\u00b7ter", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KOUS", "PPOSAT", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Drei Jahrtausend' z\u00e4hlt der unsre!", "tokens": ["Drei", "Jahr\u00b7tau\u00b7send'", "z\u00e4hlt", "der", "uns\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.54": {"line.1": {"text": "Jahre kommen und vergehen \u2013", "tokens": ["Jah\u00b7re", "kom\u00b7men", "und", "ver\u00b7ge\u00b7hen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "KON", "VVINF", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Drei Jahrtausende verflossen,", "tokens": ["Drei", "Jahr\u00b7tau\u00b7sen\u00b7de", "ver\u00b7flos\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Seit gestorben unser Ahnherr,", "tokens": ["Seit", "ge\u00b7stor\u00b7ben", "un\u00b7ser", "Ahn\u00b7herr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Herr Schlemihl ben Zuri Schadday.", "tokens": ["Herr", "Schle\u00b7mihl", "ben", "Zu\u00b7ri", "Schad\u00b7day", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "NE", "NE", "NE", "$."], "meter": "+-+-+---", "measure": "unknown.measure.tri"}}, "stanza.55": {"line.1": {"text": "L\u00e4ngst ist auch der Pinhas tot \u2013", "tokens": ["L\u00e4ngst", "ist", "auch", "der", "Pin\u00b7has", "tot", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "NN", "ADJD", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Doch sein Speer hat sich erhalten,", "tokens": ["Doch", "sein", "Speer", "hat", "sich", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "PRF", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und wir h\u00f6ren ihn best\u00e4ndig", "tokens": ["Und", "wir", "h\u00f6\u00b7ren", "ihn", "be\u00b7st\u00e4n\u00b7dig"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "PPER", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00dcber unsre H\u00e4upter schwirren.", "tokens": ["\u00dc\u00b7ber", "uns\u00b7re", "H\u00e4up\u00b7ter", "schwir\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.56": {"line.1": {"text": "Und die besten Herzen trifft er \u2013", "tokens": ["Und", "die", "bes\u00b7ten", "Her\u00b7zen", "trifft", "er", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN", "PPER", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie Jehuda ben Halevy,", "tokens": ["Wie", "Je\u00b7hu\u00b7da", "ben", "Ha\u00b7le\u00b7vy", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Traf er Moses Iben Esra,", "tokens": ["Traf", "er", "Mo\u00b7ses", "I\u00b7ben", "Es\u00b7ra", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NE", "NE", "NE", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und er traf auch den Gabirol \u2013", "tokens": ["Und", "er", "traf", "auch", "den", "Ga\u00b7bi\u00b7rol", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ART", "NN", "$("], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.57": {"line.1": {"text": "Den Gabirol, diesen treuen", "tokens": ["Den", "Ga\u00b7bi\u00b7rol", ",", "die\u00b7sen", "treu\u00b7en"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PDAT", "ADJA"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Gottgeweihten Minnes\u00e4nger,", "tokens": ["Gott\u00b7ge\u00b7weih\u00b7ten", "Min\u00b7ne\u00b7s\u00e4n\u00b7ger", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Diese fromme Nachtigall,", "tokens": ["Die\u00b7se", "from\u00b7me", "Nach\u00b7ti\u00b7gall", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Deren Rose Gott gewesen \u2013", "tokens": ["De\u00b7ren", "Ro\u00b7se", "Gott", "ge\u00b7we\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "NE", "NN", "VAPP", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.58": {"line.1": {"text": "Diese Nachtigall, die z\u00e4rtlich", "tokens": ["Die\u00b7se", "Nach\u00b7ti\u00b7gall", ",", "die", "z\u00e4rt\u00b7lich"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PDAT", "NN", "$,", "PRELS", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ihre Liebeslieder sang", "tokens": ["Ih\u00b7re", "Lie\u00b7bes\u00b7lie\u00b7der", "sang"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "In der Dunkelheit der gotisch", "tokens": ["In", "der", "Dun\u00b7kel\u00b7heit", "der", "go\u00b7tisch"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mittelalterlichen Nacht!", "tokens": ["Mit\u00b7tel\u00b7al\u00b7ter\u00b7li\u00b7chen", "Nacht", "!"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.59": {"line.1": {"text": "Unerschrocken, unbek\u00fcmmert", "tokens": ["Un\u00b7er\u00b7schro\u00b7cken", ",", "un\u00b7be\u00b7k\u00fcm\u00b7mert"], "token_info": ["word", "punct", "word"], "pos": ["NN", "$,", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ob den Fratzen und Gespenstern,", "tokens": ["Ob", "den", "Frat\u00b7zen", "und", "Ge\u00b7spens\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ob dem Wust von Tod und Wahnsinn,", "tokens": ["Ob", "dem", "Wust", "von", "Tod", "und", "Wahn\u00b7sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die gespukt in jener Nacht \u2013", "tokens": ["Die", "ge\u00b7spukt", "in", "je\u00b7ner", "Nacht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "PDAT", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.60": {"line.1": {"text": "Sie, die Nachtigall, sie dachte", "tokens": ["Sie", ",", "die", "Nach\u00b7ti\u00b7gall", ",", "sie", "dach\u00b7te"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "$,", "ART", "NN", "$,", "PPER", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nur an ihren g\u00f6ttlich Liebsten", "tokens": ["Nur", "an", "ih\u00b7ren", "g\u00f6tt\u00b7lich", "Liebs\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "PPOSAT", "ADJD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dem sie ihre Liebe schluchzte,", "tokens": ["Dem", "sie", "ih\u00b7re", "Lie\u00b7be", "schluchz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Den ihr Lobgesang verherrlicht! \u2013", "tokens": ["Den", "ihr", "Lob\u00b7ge\u00b7sang", "ver\u00b7herr\u00b7licht", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVPP", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Drei\u00dfig Lenze sah Gabirol", "tokens": ["Drei\u00b7\u00dfig", "Len\u00b7ze", "sah", "Ga\u00b7bi\u00b7rol"], "token_info": ["word", "word", "word", "word"], "pos": ["CARD", "NN", "VVFIN", "NE"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Hier auf Erden, aber Fama", "tokens": ["Hier", "auf", "Er\u00b7den", ",", "a\u00b7ber", "Fa\u00b7ma"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPR", "NN", "$,", "KON", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ausposaunte seines Namens", "tokens": ["Aus\u00b7po\u00b7saun\u00b7te", "sei\u00b7nes", "Na\u00b7mens"], "token_info": ["word", "word", "word"], "pos": ["NN", "PPOSAT", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Herrlichkeit durch alle Lande.", "tokens": ["Herr\u00b7lich\u00b7keit", "durch", "al\u00b7le", "Lan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.62": {"line.1": {"text": "Zu Corduba, wo er wohnte,", "tokens": ["Zu", "Cor\u00b7du\u00b7ba", ",", "wo", "er", "wohn\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "War ein Mohr sein n\u00e4chster Nachbar,", "tokens": ["War", "ein", "Mohr", "sein", "n\u00e4chs\u00b7ter", "Nach\u00b7bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Welcher gleichfalls Verse machte", "tokens": ["Wel\u00b7cher", "gleich\u00b7falls", "Ver\u00b7se", "mach\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAT", "ADJA", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und des Dichters Ruhm beneidet'.", "tokens": ["Und", "des", "Dich\u00b7ters", "Ruhm", "be\u00b7nei\u00b7det'", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.63": {"line.1": {"text": "H\u00f6rte er den Dichter singen,", "tokens": ["H\u00f6r\u00b7te", "er", "den", "Dich\u00b7ter", "sin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schwoll dem Mohren gleich die Galle,", "tokens": ["Schwoll", "dem", "Moh\u00b7ren", "gleich", "die", "Gal\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ART", "NN", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und der Lieder S\u00fc\u00dfe wurde", "tokens": ["Und", "der", "Lie\u00b7der", "S\u00fc\u00b7\u00dfe", "wur\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NN", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Bittrer Wermut f\u00fcr den Neidhart.", "tokens": ["Bit\u00b7trer", "Wer\u00b7mut", "f\u00fcr", "den", "Neid\u00b7hart", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.64": {"line.1": {"text": "Er verlockte den Verha\u00dften", "tokens": ["Er", "ver\u00b7lock\u00b7te", "den", "Ver\u00b7ha\u00df\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "N\u00e4chtlich in sein Haus, erschlug ihn", "tokens": ["N\u00e4cht\u00b7lich", "in", "sein", "Haus", ",", "er\u00b7schlug", "ihn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "$,", "VVFIN", "PPER"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dorten und vergrub den Leichnam", "tokens": ["Dor\u00b7ten", "und", "ver\u00b7grub", "den", "Leich\u00b7nam"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Hinterm Hause in dem Garten.", "tokens": ["Hin\u00b7term", "Hau\u00b7se", "in", "dem", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.65": {"line.1": {"text": "Aber siehe! aus dem Boden,", "tokens": ["A\u00b7ber", "sie\u00b7he", "!", "aus", "dem", "Bo\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "$.", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wo die Leiche eingescharrt war,", "tokens": ["Wo", "die", "Lei\u00b7che", "ein\u00b7ge\u00b7scharrt", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wuchs hervor ein Feigenbaum", "tokens": ["Wuchs", "her\u00b7vor", "ein", "Fei\u00b7gen\u00b7baum"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PTKVZ", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Von der wunderbarsten Sch\u00f6nheit.", "tokens": ["Von", "der", "wun\u00b7der\u00b7bars\u00b7ten", "Sch\u00f6n\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.66": {"line.1": {"text": "Seine Frucht war seltsam l\u00e4nglich", "tokens": ["Sei\u00b7ne", "Frucht", "war", "selt\u00b7sam", "l\u00e4ng\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und von seltsam w\u00fcrz'ger S\u00fc\u00dfe;", "tokens": ["Und", "von", "selt\u00b7sam", "w\u00fcrz'\u00b7ger", "S\u00fc\u00b7\u00dfe", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJD", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wer davon geno\u00df, versank", "tokens": ["Wer", "da\u00b7von", "ge\u00b7no\u00df", ",", "ver\u00b7sank"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PWS", "PAV", "VVFIN", "$,", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "In ein tr\u00e4umerisch Entz\u00fccken.", "tokens": ["In", "ein", "tr\u00e4u\u00b7me\u00b7risch", "Ent\u00b7z\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJD", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.67": {"line.1": {"text": "In dem Volke ging dar\u00fcber", "tokens": ["In", "dem", "Vol\u00b7ke", "ging", "da\u00b7r\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "PAV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Viel Gerede und Gemunkel,", "tokens": ["Viel", "Ge\u00b7re\u00b7de", "und", "Ge\u00b7mun\u00b7kel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Das am End' zu den erlauchten", "tokens": ["Das", "am", "End'", "zu", "den", "er\u00b7lauch\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "APPRART", "NN", "APPR", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ohren des Kalifen kam.", "tokens": ["Oh\u00b7ren", "des", "Ka\u00b7li\u00b7fen", "kam", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.68": {"line.1": {"text": "Dieser pr\u00fcfte eigenz\u00fcngig", "tokens": ["Die\u00b7ser", "pr\u00fcf\u00b7te", "ei\u00b7gen\u00b7z\u00fcn\u00b7gig"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VVFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jenes Feigenph\u00e4nomen,", "tokens": ["Je\u00b7nes", "Fei\u00b7gen\u00b7ph\u00e4\u00b7no\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["PDAT", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Und ernannte eine strenge", "tokens": ["Und", "er\u00b7nann\u00b7te", "ei\u00b7ne", "stren\u00b7ge"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Untersuchungskommission.", "tokens": ["Un\u00b7ter\u00b7su\u00b7chungs\u00b7kom\u00b7mis\u00b7si\u00b7on", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.69": {"line.1": {"text": "Man verfuhr summarisch. Sechzig", "tokens": ["Man", "ver\u00b7fuhr", "sum\u00b7ma\u00b7risch", ".", "Sech\u00b7zig"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PIS", "VVFIN", "ADJD", "$.", "CARD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Bambushiebe auf die Sohlen", "tokens": ["Bam\u00b7bus\u00b7hie\u00b7be", "auf", "die", "Soh\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Gab man gleich dem Herrn des Baumes,", "tokens": ["Gab", "man", "gleich", "dem", "Herrn", "des", "Bau\u00b7mes", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welcher eingestand die Untat.", "tokens": ["Wel\u00b7cher", "ein\u00b7ge\u00b7stand", "die", "Un\u00b7tat", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAT", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.70": {"line.1": {"text": "Darauf ri\u00df man auch den Baum", "tokens": ["Da\u00b7rauf", "ri\u00df", "man", "auch", "den", "Baum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PIS", "ADV", "ART", "NN"], "meter": "-++-+-+", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Mit den Wurzeln aus dem Boden,", "tokens": ["Mit", "den", "Wur\u00b7zeln", "aus", "dem", "Bo\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und zum Vorschein kam die Leiche", "tokens": ["Und", "zum", "Vor\u00b7schein", "kam", "die", "Lei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPRART", "NN", "VVFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Des erschlagenen Gabirol.", "tokens": ["Des", "er\u00b7schla\u00b7ge\u00b7nen", "Ga\u00b7bi\u00b7rol", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.71": {"line.1": {"text": "Diese ward mit Pomp bestattet", "tokens": ["Die\u00b7se", "ward", "mit", "Pomp", "be\u00b7stat\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und betrauert von den Br\u00fcdern;", "tokens": ["Und", "be\u00b7trau\u00b7ert", "von", "den", "Br\u00fc\u00b7dern", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "An demselben Tage henkte", "tokens": ["An", "dem\u00b7sel\u00b7ben", "Ta\u00b7ge", "henk\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Man den Mohren zu Corduba.", "tokens": ["Man", "den", "Moh\u00b7ren", "zu", "Cor\u00b7du\u00b7ba", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ART", "NN", "APPR", "NE", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}}}}