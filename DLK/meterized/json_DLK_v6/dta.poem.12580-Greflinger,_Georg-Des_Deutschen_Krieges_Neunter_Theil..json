{"dta.poem.12580": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Neunter Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Bannier vom Gl\u00fcck ermahnt/ den Gegner zu ver-\nderben/", "tokens": ["Ban\u00b7nier", "vom", "Gl\u00fcck", "er\u00b7mahnt", "/", "den", "Geg\u00b7ner", "zu", "ver", "der\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "NN", "VVPP", "$(", "ART", "NN", "APPR", "TRUNC", "PDAT", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zog nach dem Danckfest\u2019 auf und legte sich vor", "tokens": ["Zog", "nach", "dem", "Dan\u00b7ck\u00b7fest'", "auf", "und", "leg\u00b7te", "sich", "vor"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "PTKVZ", "KON", "VVFIN", "PRF", "APPR"], "meter": "+--+--+-+--+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Werben/", "tokens": ["Wer\u00b7ben", "/"], "token_info": ["word", "punct"], "pos": ["VAFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Das ein par tausend Mann und anders nach der Schlacht", "tokens": ["Das", "ein", "par", "tau\u00b7send", "Mann", "und", "an\u00b7ders", "nach", "der", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "ART", "FM", "CARD", "NN", "KON", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vor Wittstock in der Flucht in dessen Wall gebracht/", "tokens": ["Vor", "Witt\u00b7stock", "in", "der", "Flucht", "in", "des\u00b7sen", "Wall", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "APPR", "PRELAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Behauste. Kaum daf\u00fcr/ da war er auch darinnen.", "tokens": ["Be\u00b7haus\u00b7te", ".", "Kaum", "da\u00b7f\u00fcr", "/", "da", "war", "er", "auch", "da\u00b7rin\u00b7nen", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ADV", "PAV", "$(", "ADV", "VAFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was wolte solches Volck vor solcher Macht beginnen?", "tokens": ["Was", "wol\u00b7te", "sol\u00b7ches", "Volck", "vor", "sol\u00b7cher", "Macht", "be\u00b7gin\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PIAT", "NN", "APPR", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Er nahm den festen Ort mit samt dem Volck an sich.", "tokens": ["Er", "nahm", "den", "fes\u00b7ten", "Ort", "mit", "samt", "dem", "Volck", "an", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "APPR", "APPR", "ART", "NN", "APPR", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Damit blieb seinem Feind ein gro\u00dfes in dem Stich.", "tokens": ["Da\u00b7mit", "blieb", "sei\u00b7nem", "Feind", "ein", "gro\u00b7\u00dfes", "in", "dem", "Stich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPOSAT", "NN", "ART", "ADJA", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Auf dieses gieng das Heer nach Egeln/ da sich Wrangel", "tokens": ["Auf", "die\u00b7ses", "gieng", "das", "Heer", "nach", "E\u00b7geln", "/", "da", "sich", "Wran\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PDAT", "VVFIN", "ART", "NN", "APPR", "NE", "$(", "KOUS", "PRF", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Nach Pommern/ Le\u00dfle sich zur Weser/ dessen Mangel", "tokens": ["Nach", "Pom\u00b7mern", "/", "Le\u00df\u00b7le", "sich", "zur", "We\u00b7ser", "/", "des\u00b7sen", "Man\u00b7gel"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NE", "$(", "NE", "PRF", "APPRART", "NN", "$(", "PRELAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Zu helffen/ neu erhob. Ein jeder that das sein\u2019.", "tokens": ["Zu", "helf\u00b7fen", "/", "neu", "er\u00b7hob", ".", "Ein", "je\u00b7der", "that", "das", "sein'", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "ADJD", "VVFIN", "$.", "ART", "PIS", "VVFIN", "PDS", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "In dessen legte sich Bannier um Erfurt ein/", "tokens": ["In", "des\u00b7sen", "leg\u00b7te", "sich", "Ban\u00b7nier", "um", "Er\u00b7furt", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "VVFIN", "PRF", "NN", "APPR", "NE", "ART", "$("], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.14": {"text": "Lie\u00df Feinde/ Land und Leuth in Th\u00fcringen und Mei\u00dfen", "tokens": ["Lie\u00df", "Fein\u00b7de", "/", "Land", "und", "Leuth", "in", "Th\u00fc\u00b7rin\u00b7gen", "und", "Mei\u00b7\u00dfen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "NN", "$(", "NN", "KON", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Durch seine Siegende vereufert niederrei\u00dfen.", "tokens": ["Durch", "sei\u00b7ne", "Sie\u00b7gen\u00b7de", "ver\u00b7eu\u00b7fert", "nie\u00b7der\u00b7rei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVPP", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Di\u00df alles ungeacht sprach Erfurt/ als sein Feind/", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "sprach", "Er\u00b7furt", "/", "als", "sein", "Feind", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "ADJD", "VVFIN", "NE", "$(", "KOUS", "PPOSAT", "NN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.17": {"text": "Dann es war solches nun des Prager-Friedens Freind.", "tokens": ["Dann", "es", "war", "sol\u00b7ches", "nun", "des", "Pra\u00b7ger\u00b7Frie\u00b7dens", "Freind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "PIS", "ADV", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Als aber sich Bannier mit Macht jhm wiederlegte/", "tokens": ["Als", "a\u00b7ber", "sich", "Ban\u00b7nier", "mit", "Macht", "jhm", "wie\u00b7der\u00b7leg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PRF", "NE", "APPR", "NN", "PPER", "VVFIN", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.19": {"text": "That solches nochmals so/ wie es vor diesem pflegte/", "tokens": ["That", "sol\u00b7ches", "noch\u00b7mals", "so", "/", "wie", "es", "vor", "die\u00b7sem", "pfleg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIAT", "ADV", "ADV", "$(", "PWAV", "PPER", "APPR", "PDAT", "VVFIN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "und war der Schweden Freind. Auf dieses schonte man", "tokens": ["und", "war", "der", "Schwe\u00b7den", "Freind", ".", "Auf", "die\u00b7ses", "schon\u00b7te", "man"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "ADJA", "NN", "$.", "APPR", "PDAT", "VVFIN", "PIS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Die Stadt und neben ihr des Weymars ", "tokens": ["Die", "Stadt", "und", "ne\u00b7ben", "ihr", "des", "Wey\u00b7mars"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "APPR", "PPOSAT", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Cur-Sachsen aber must hergegen m\u00e4chtig leyden/", "tokens": ["Cur\u00b7Sach\u00b7sen", "a\u00b7ber", "must", "her\u00b7ge\u00b7gen", "m\u00e4ch\u00b7tig", "ley\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VMFIN", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Bi\u00df man den Siegenden die Fl\u00fcgel zu beschneiden", "tokens": ["Bi\u00df", "man", "den", "Sie\u00b7gen\u00b7den", "die", "Fl\u00fc\u00b7gel", "zu", "be\u00b7schnei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ART", "NN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Noch gr\u00f6\u00dfre Macht aufbracht\u2019. Es eylte Hatzfelds Schar/", "tokens": ["Noch", "gr\u00f6\u00df\u00b7re", "Macht", "auf\u00b7bracht'", ".", "Es", "eyl\u00b7te", "Hatz\u00b7felds", "Schar", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVPP", "$.", "PPER", "VVFIN", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Die durch den letzten Schlag sehr schlecht zu sehen war/", "tokens": ["Die", "durch", "den", "letz\u00b7ten", "Schlag", "sehr", "schlecht", "zu", "se\u00b7hen", "war", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Nach G\u00f6tzens gro\u00dfem Volck in Hessen und Westfalen", "tokens": ["Nach", "G\u00f6t\u00b7zens", "gro\u00b7\u00dfem", "Volck", "in", "Hes\u00b7sen", "und", "West\u00b7fa\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "ADJA", "NN", "APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "Sich zu vereinigen/ und die Bannirsche Pfalen/", "tokens": ["Sich", "zu", "ver\u00b7ei\u00b7ni\u00b7gen", "/", "und", "die", "Ban\u00b7nir\u00b7sche", "Pfa\u00b7len", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKZU", "VVINF", "$(", "KON", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.28": {"text": "So ferne fort gesetzt/ zu br\u00e4chen. Diesen Sinn", "tokens": ["So", "fer\u00b7ne", "fort", "ge\u00b7setzt", "/", "zu", "br\u00e4\u00b7chen", ".", "Die\u00b7sen", "Sinn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "PTKVZ", "VVPP", "$(", "PTKZU", "VVINF", "$.", "PDAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Zu st\u00fctzen/ sandte man den tapfren Le\u00dfle hinn.", "tokens": ["Zu", "st\u00fct\u00b7zen", "/", "sand\u00b7te", "man", "den", "tapf\u00b7ren", "Le\u00df\u00b7le", "hinn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "PIS", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Er aber viel zu schwach/ auf drey\u00dfig tausend Seelen", "tokens": ["Er", "a\u00b7ber", "viel", "zu", "schwach", "/", "auf", "drey\u00b7\u00dfig", "tau\u00b7send", "See\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "ADV", "PTKA", "ADJD", "$(", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Zu schlagen/ lie\u00df es doch an diesem wenig f\u00e4hlen/", "tokens": ["Zu", "schla\u00b7gen", "/", "lie\u00df", "es", "doch", "an", "die\u00b7sem", "we\u00b7nig", "f\u00e4h\u00b7len", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVFIN", "PPER", "ADV", "APPR", "PDAT", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Da\u00df er der P\u00e4sse sich versicherte. Wie dann", "tokens": ["Da\u00df", "er", "der", "P\u00e4s\u00b7se", "sich", "ver\u00b7si\u00b7cher\u00b7te", ".", "Wie", "dann"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "PRF", "VVFIN", "$.", "PWAV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Der Oberst Stallhann\u00df viel zu solchem hat gethan.", "tokens": ["Der", "O\u00b7berst", "Stall\u00b7hann\u00df", "viel", "zu", "sol\u00b7chem", "hat", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "ADV", "PTKA", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Sie st\u00fctzten/ und Bannier gieng Leipzig zu gewinnen.", "tokens": ["Sie", "st\u00fctz\u00b7ten", "/", "und", "Ban\u00b7nier", "gieng", "Leip\u00b7zig", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "NN", "VVFIN", "NE", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.35": {"text": "Kaum da/ erhob er sich schon wiederum von hinnen/", "tokens": ["Kaum", "da", "/", "er\u00b7hob", "er", "sich", "schon", "wie\u00b7de\u00b7rum", "von", "hin\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$(", "VVFIN", "PPER", "PRF", "ADV", "ADV", "APPR", "ADV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "und schlug bey Eulenburg ein etlich hundert Mann", "tokens": ["und", "schlug", "bey", "Eu\u00b7len\u00b7burg", "ein", "et\u00b7lich", "hun\u00b7dert", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NE", "ART", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "Der S\u00e4chsischen Armee/ von welchen er fort an", "tokens": ["Der", "S\u00e4ch\u00b7si\u00b7schen", "Ar\u00b7mee", "/", "von", "wel\u00b7chen", "er", "fort", "an"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "$(", "APPR", "PWAT", "PPER", "PTKVZ", "APPR"], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.38": {"text": "Nach Torgau gieng/ den Feind/ der sich mit schnellen fliehen", "tokens": ["Nach", "Tor\u00b7gau", "gieng", "/", "den", "Feind", "/", "der", "sich", "mit", "schnel\u00b7len", "flie\u00b7hen"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VVFIN", "$(", "ART", "NN", "$(", "PRELS", "PRF", "APPR", "VVINF", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Dahin gereitet hatt\u2019/ allda zu \u00fcberziehen.", "tokens": ["Da\u00b7hin", "ge\u00b7rei\u00b7tet", "hatt'", "/", "all\u00b7da", "zu", "\u00fc\u00b7ber\u00b7zie\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "VAFIN", "$(", "PAV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Er nahm die gute Stadt mit samt der Sachsen Schar/", "tokens": ["Er", "nahm", "die", "gu\u00b7te", "Stadt", "mit", "samt", "der", "Sach\u00b7sen", "Schar", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "APPR", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Die in zweytausend starck bewehrt darinnen war.", "tokens": ["Die", "in", "zweyt\u00b7au\u00b7send", "starck", "be\u00b7wehrt", "da\u00b7rin\u00b7nen", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "CARD", "NN", "VVFIN", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "Acht Regimenter sind hiedurch zu scheitern gangen/", "tokens": ["Acht", "Re\u00b7gi\u00b7men\u00b7ter", "sind", "hie\u00b7durch", "zu", "schei\u00b7tern", "gan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PAV", "PTKZU", "VVINF", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Die meisten gaben sich den Schwedischen gefangen.", "tokens": ["Die", "meis\u00b7ten", "ga\u00b7ben", "sich", "den", "Schwe\u00b7di\u00b7schen", "ge\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PRF", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Di\u00df alles au\u00dfgericht/ versorgte man die Stadt", "tokens": ["Di\u00df", "al\u00b7les", "au\u00df\u00b7ge\u00b7richt", "/", "ver\u00b7sorg\u00b7te", "man", "die", "Stadt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "VVPP", "$(", "VVFIN", "PIS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Mit Mitteln/ die ein Heer vor solcher n\u00f6thig hat/", "tokens": ["Mit", "Mit\u00b7teln", "/", "die", "ein", "Heer", "vor", "sol\u00b7cher", "n\u00f6\u00b7thig", "hat", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "PRELS", "ART", "NN", "APPR", "PIAT", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Und gieng zum andernmal vor Leipzig sich zu setzen.", "tokens": ["Und", "gieng", "zum", "an\u00b7dern\u00b7mal", "vor", "Leip\u00b7zig", "sich", "zu", "set\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "ADV", "APPR", "NE", "PRF", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.47": {"text": "Di\u00df \u00e4ngstend/ h\u00f6rte man wie Hatzfeld mit dem G\u00f6tzen", "tokens": ["Di\u00df", "\u00e4ngs\u00b7tend", "/", "h\u00f6r\u00b7te", "man", "wie", "Hatz\u00b7feld", "mit", "dem", "G\u00f6t\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVPP", "$(", "VVFIN", "PIS", "KOKOM", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.48": {"text": "Sich durchgebrochen h\u00e4tt\u2019 und w\u00e4re schon herein", "tokens": ["Sich", "durch\u00b7ge\u00b7bro\u00b7chen", "h\u00e4tt'", "und", "w\u00e4\u00b7re", "schon", "her\u00b7ein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PRF", "VVINF", "VAFIN", "KON", "VAFIN", "ADV", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Mit einer gro\u00dfen Macht/ der Stadt Entsatz zu seyn.", "tokens": ["Mit", "ei\u00b7ner", "gro\u00b7\u00dfen", "Macht", "/", "der", "Stadt", "Ent\u00b7satz", "zu", "seyn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "ART", "NN", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "Damit versamlete Bannier sein Volck zusammen/", "tokens": ["Da\u00b7mit", "ver\u00b7sam\u00b7le\u00b7te", "Ban\u00b7nier", "sein", "Volck", "zu\u00b7sam\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "PPOSAT", "NN", "PTKVZ", "$("], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.51": {"text": "Schlng Mei\u00dfen rechts und lincks mit Schwerdt und Feuers", "tokens": ["Schlng", "Mei\u00b7\u00dfen", "rechts", "und", "lincks", "mit", "Schwerdt", "und", "Feu\u00b7ers"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ADV", "KON", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.52": {"text": "Flammen/", "tokens": ["Flam\u00b7men", "/"], "token_info": ["word", "punct"], "pos": ["NN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.53": {"text": "und gieng nach Torgau zu/ beschantzete sein Heer/", "tokens": ["und", "gieng", "nach", "Tor\u00b7gau", "zu", "/", "be\u00b7schant\u00b7ze\u00b7te", "sein", "Heer", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "PTKZU", "$(", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Pflantzt hundert donndernde Gesch\u00fctz und noch viel mehr", "tokens": ["Pflantzt", "hun\u00b7dert", "donn\u00b7dern\u00b7de", "Ge\u00b7sch\u00fctz", "und", "noch", "viel", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "CARD", "ADJA", "NN", "KON", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Auf seinen W\u00e4llen rum/ die Feinde zu begr\u00fc\u00dfen.", "tokens": ["Auf", "sei\u00b7nen", "W\u00e4l\u00b7len", "rum", "/", "die", "Fein\u00b7de", "zu", "be\u00b7gr\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NE", "$(", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Und sihe da die Macht die Schweden zu beschl\u00fc\u00dfen.", "tokens": ["Und", "si\u00b7he", "da", "die", "Macht", "die", "Schwe\u00b7den", "zu", "be\u00b7schl\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Bannier war nun ein L\u00f6u/ der mit Gewalt umringt/", "tokens": ["Ban\u00b7nier", "war", "nun", "ein", "L\u00f6u", "/", "der", "mit", "Ge\u00b7walt", "um\u00b7ringt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ART", "NN", "$(", "ART", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Bald ein Gebr\u00fcll erhebt/ bald auf die Feinde springt/", "tokens": ["Bald", "ein", "Ge\u00b7br\u00fcll", "er\u00b7hebt", "/", "bald", "auf", "die", "Fein\u00b7de", "springt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVFIN", "$(", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.59": {"text": "und keinen Nachklang wil von seinem fliehen haben.", "tokens": ["und", "kei\u00b7nen", "Nach\u00b7klang", "wil", "von", "sei\u00b7nem", "flie\u00b7hen", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VMFIN", "APPR", "PPOSAT", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.60": {"text": "Es lagen Freind und ", "tokens": ["Es", "la\u00b7gen", "Freind", "und"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "KON"], "meter": "-+-+-", "measure": "iambic.di"}, "line.61": {"text": "Und war des Gegners Heer von neuntzig tausend Mann/", "tokens": ["Und", "war", "des", "Geg\u00b7ners", "Heer", "von", "ne\u00b7unt\u00b7zig", "tau\u00b7send", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "NN", "APPR", "CARD", "CARD", "NN", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.62": {"text": "Dann es kam alle Macht des Deutschen Reiches an/", "tokens": ["Dann", "es", "kam", "al\u00b7le", "Macht", "des", "Deut\u00b7schen", "Rei\u00b7ches", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PIAT", "NN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.63": {"text": "Der Schwedischen Gewalt sich g\u00e4ntzlich lo\u00df zu machen.", "tokens": ["Der", "Schwe\u00b7di\u00b7schen", "Ge\u00b7walt", "sich", "g\u00e4ntz\u00b7lich", "lo\u00df", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.64": {"text": "Es stundt auch diesesmal mit der Bel\u00e4grer Sachen", "tokens": ["Es", "stundt", "auch", "die\u00b7ses\u00b7mal", "mit", "der", "Be\u00b7l\u00e4g\u00b7rer", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.65": {"text": "Sehr wol/ so da\u00df Bannier nach langer Gegenwehr", "tokens": ["Sehr", "wol", "/", "so", "da\u00df", "Ban\u00b7nier", "nach", "lan\u00b7ger", "Ge\u00b7gen\u00b7wehr"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "$(", "ADV", "KOUS", "NN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.66": {"text": "und d\u00fcnn-gemachter Macht/ (dann es war jhm sein Heer", "tokens": ["und", "d\u00fcnn\u00b7ge\u00b7mach\u00b7ter", "Macht", "/", "(", "dann", "es", "war", "jhm", "sein", "Heer"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "$(", "$(", "ADV", "PPER", "VAFIN", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.67": {"text": "Bi\u00df auf eilff tausend Mann geschmoltzen) muste weichen/", "tokens": ["Bi\u00df", "auf", "eilff", "tau\u00b7send", "Mann", "ge\u00b7schmolt\u00b7zen", ")", "mus\u00b7te", "wei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "APPR", "CARD", "CARD", "NN", "VVPP", "$(", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.68": {"text": "Da war noch Kraut noch Loth/ noch des Entsatzes Zeichen/", "tokens": ["Da", "war", "noch", "Kraut", "noch", "Loth", "/", "noch", "des", "Ent\u00b7sat\u00b7zes", "Zei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "NN", "ADV", "NN", "$(", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "Ich schweige was f\u00fcr Noth an Lebens-Mitteln war.", "tokens": ["Ich", "schwei\u00b7ge", "was", "f\u00fcr", "Noth", "an", "Le\u00b7bens\u00b7Mit\u00b7teln", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "NN", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Dann es war alles Land durch solche gro\u00dfe Schar", "tokens": ["Dann", "es", "war", "al\u00b7les", "Land", "durch", "sol\u00b7che", "gro\u00b7\u00dfe", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VAFIN", "PIAT", "NN", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.71": {"text": "Verdorben. Er brach auf/ nicht seines Feindes wegen/", "tokens": ["Ver\u00b7dor\u00b7ben", ".", "Er", "brach", "auf", "/", "nicht", "sei\u00b7nes", "Fein\u00b7des", "we\u00b7gen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$.", "PPER", "VVFIN", "APPR", "$(", "PTKNEG", "PPOSAT", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Vor dem er f\u00e4st genug an Torgau war gelegen/", "tokens": ["Vor", "dem", "er", "f\u00e4st", "ge\u00b7nug", "an", "Tor\u00b7gau", "war", "ge\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "VVFIN", "ADV", "APPR", "NE", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Vielmehr aus Noth von Brodt und anderm/ wie gedacht/", "tokens": ["Viel\u00b7mehr", "aus", "Noth", "von", "Brodt", "und", "an\u00b7derm", "/", "wie", "ge\u00b7dacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "APPR", "NN", "KON", "PIS", "$(", "PWAV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.74": {"text": "Und dieses that jhm mehr als eine gro\u00dfe Schlacht.", "tokens": ["Und", "die\u00b7ses", "that", "jhm", "mehr", "als", "ei\u00b7ne", "gro\u00b7\u00dfe", "Schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "PIAT", "KOKOM", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "Es hatt\u2019 jhm Le\u00dflens Heer mit Pfulens seinen Scharen/", "tokens": ["Es", "hatt'", "jhm", "Le\u00df\u00b7lens", "Heer", "mit", "Pfu\u00b7lens", "sei\u00b7nen", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "NE", "NN", "APPR", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.76": {"text": "Die durch die Lau\u00dfnitz was vorangegangen waren/", "tokens": ["Die", "durch", "die", "Lau\u00df\u00b7nitz", "was", "vor\u00b7an\u00b7ge\u00b7gan\u00b7gen", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "PWS", "VVPP", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "So guten Pa\u00df gemacht/ da\u00df er wol folgen kunt\u2019.", "tokens": ["So", "gu\u00b7ten", "Pa\u00df", "ge\u00b7macht", "/", "da\u00df", "er", "wol", "fol\u00b7gen", "kunt'", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVPP", "$(", "KOUS", "PPER", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.78": {"text": "Als aber dessen Feind allzeit sehr wachsam stundt\u2019", "tokens": ["Als", "a\u00b7ber", "des\u00b7sen", "Feind", "all\u00b7zeit", "sehr", "wach\u00b7sam", "stundt'"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PRELAT", "NN", "ADV", "ADV", "ADJD", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.79": {"text": "Auf seinen Zug zu sehn/ geschachs/ da\u00df die Croaten", "tokens": ["Auf", "sei\u00b7nen", "Zug", "zu", "sehn", "/", "ge\u00b7schachs", "/", "da\u00df", "die", "Croa\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$(", "NE", "$(", "KOUS", "ART", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.80": {"text": "Jhm folgten/ und f\u00fcrwar sehr gro\u00dfen Schaden thaten.", "tokens": ["Jhm", "folg\u00b7ten", "/", "und", "f\u00fcr\u00b7war", "sehr", "gro\u00b7\u00dfen", "Scha\u00b7den", "tha\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KON", "ADV", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Ein tausend blieb im Stich. Es kam auch \u00fcber di\u00df", "tokens": ["Ein", "tau\u00b7send", "blieb", "im", "Stich", ".", "Es", "kam", "auch", "\u00fc\u00b7ber", "di\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "VVFIN", "APPRART", "NN", "$.", "PPER", "VVFIN", "ADV", "APPR", "PDS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.82": {"text": "Das gantze Heer hernach/ das wenig ruhen lie\u00df.", "tokens": ["Das", "gant\u00b7ze", "Heer", "her\u00b7nach", "/", "das", "we\u00b7nig", "ru\u00b7hen", "lie\u00df", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "$(", "ART", "PIS", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "Und hatte jederman sich h\u00f6chlich zu verwundern", "tokens": ["Und", "hat\u00b7te", "je\u00b7der\u00b7man", "sich", "h\u00f6ch\u00b7lich", "zu", "ver\u00b7wun\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PIS", "PRF", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "Wie General Bannier durch so viel Feind\u2019 und dundern", "tokens": ["Wie", "Ge\u00b7ne\u00b7ral", "Ban\u00b7nier", "durch", "so", "viel", "Feind'", "und", "dun\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "NE", "APPR", "ADV", "PIAT", "NN", "KON", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Annoch entgehen kunt. Er kam bey Landsberg an.", "tokens": ["An\u00b7noch", "ent\u00b7ge\u00b7hen", "kunt", ".", "Er", "kam", "bey", "Lands\u00b7berg", "an", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKVZ", "$.", "PPER", "VVFIN", "APPR", "NE", "PTKVZ", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.86": {"text": "Kaum da/ war auch sein Feind bey jhm schon auf dem Plan/", "tokens": ["Kaum", "da", "/", "war", "auch", "sein", "Feind", "bey", "jhm", "schon", "auf", "dem", "Plan", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$(", "VAFIN", "ADV", "PPOSAT", "NN", "APPR", "PPER", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.87": {"text": "Nach welchem er so starck mit seinen Blitz-Gesch\u00fctzen", "tokens": ["Nach", "wel\u00b7chem", "er", "so", "starck", "mit", "sei\u00b7nen", "Blitz\u00b7Ge\u00b7sch\u00fct\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "ADV", "ADJD", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "Gespielt/ da\u00df tausend Mann hieselbsten blieben sitzen.", "tokens": ["Ge\u00b7spielt", "/", "da\u00df", "tau\u00b7send", "Mann", "hie\u00b7selbs\u00b7ten", "blie\u00b7ben", "sit\u00b7zen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "KOUS", "CARD", "NN", "ADV", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Hierauf begab er sich nicht ferne von C\u00fcstrin", "tokens": ["Hier\u00b7auf", "be\u00b7gab", "er", "sich", "nicht", "fer\u00b7ne", "von", "C\u00fcs\u00b7trin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "PTKNEG", "ADV", "APPR", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.90": {"text": "Mit seiner kleinen Macht/ die Oder \u00fcber hin/", "tokens": ["Mit", "sei\u00b7ner", "klei\u00b7nen", "Macht", "/", "die", "O\u00b7der", "\u00fc\u00b7ber", "hin", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$(", "ART", "NN", "APPR", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.91": {"text": "und gieng nach Neustadt zu/ woselbst Feld-Marschalck", "tokens": ["und", "gieng", "nach", "Neu\u00b7stadt", "zu", "/", "wo\u00b7selbst", "Feld\u00b7Mar\u00b7schalck"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NN", "PTKZU", "$(", "PWAV", "NN"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.92": {"text": "Wrangel", "tokens": ["Wran\u00b7gel"], "token_info": ["word"], "pos": ["NE"], "meter": "+-", "measure": "trochaic.single"}, "line.93": {"text": "Mit zehen tausend Mann/ des Volckes gro\u00dfen Mangel", "tokens": ["Mit", "ze\u00b7hen", "tau\u00b7send", "Mann", "/", "des", "Vol\u00b7ckes", "gro\u00b7\u00dfen", "Man\u00b7gel"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "CARD", "NN", "$(", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.94": {"text": "Erstzte. Di\u00df gethan/ begab man sich nach Schwed.", "tokens": ["Erstz\u00b7te", ".", "Di\u00df", "ge\u00b7than", "/", "be\u00b7gab", "man", "sich", "nach", "Schwe\u00b7d."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "abbreviation"], "pos": ["VVFIN", "$.", "PDS", "VVPP", "$(", "VVFIN", "PIS", "PRF", "APPR", "NN"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.95": {"text": "Viel hielten aber nun die Schwedischen vor bl\u00f6d/", "tokens": ["Viel", "hiel\u00b7ten", "a\u00b7ber", "nun", "die", "Schwe\u00b7di\u00b7schen", "vor", "bl\u00f6d", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ART", "NN", "APPR", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "Weil jhrer Feinde Macht der jhren \u00fcberlegen", "tokens": ["Weil", "jhrer", "Fein\u00b7de", "Macht", "der", "jhren", "\u00fc\u00b7berl\u00b7e\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "NN", "ART", "NN", "VVINF"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.97": {"text": "und nun sehr gl\u00fccklich war. Man zog von allen Wegen", "tokens": ["und", "nun", "sehr", "gl\u00fcck\u00b7lich", "war", ".", "Man", "zog", "von", "al\u00b7len", "We\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADV", "ADJD", "VAFIN", "$.", "PIS", "VVFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Zusammen/ jhre Flucht nach Schweden anzusehn.", "tokens": ["Zu\u00b7sam\u00b7men", "/", "jhre", "Flucht", "nach", "Schwe\u00b7den", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PPOSAT", "NN", "APPR", "NE", "VVIZU", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.99": {"text": "Da waren Gallas/ G\u00f6tz/ Jslani/ Hatzfeld/ Gleen/", "tokens": ["Da", "wa\u00b7ren", "Gal\u00b7las", "/", "G\u00f6tz", "/", "Js\u00b7la\u00b7ni", "/", "Hatz\u00b7feld", "/", "Gleen", "/"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Mit den Cur-S\u00e4chsischen und Brandenburgschen Scha-", "tokens": ["Mit", "den", "Cur\u00b7S\u00e4ch\u00b7si\u00b7schen", "und", "Bran\u00b7den\u00b7burg\u00b7schen", "Scha"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Bey denen Vitzthum und der Klitzing F\u00fchrer waren.", "tokens": ["Bey", "de\u00b7nen", "Vitzt\u00b7hum", "und", "der", "Klit\u00b7zing", "F\u00fch\u00b7rer", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "NN", "KON", "ART", "NN", "NN", "VAFIN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.102": {"text": "Um dieses gieng Bannier bi\u00df in das Pommerland", "tokens": ["Um", "die\u00b7ses", "gieng", "Ban\u00b7nier", "bi\u00df", "in", "das", "Pom\u00b7mer\u00b7land"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PDS", "VVFIN", "NE", "KON", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.103": {"text": "Da er zehn tausend Mann/ aus Schweden her gesand/", "tokens": ["Da", "er", "zehn", "tau\u00b7send", "Mann", "/", "aus", "Schwe\u00b7den", "her", "ge\u00b7sand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "CARD", "NN", "$(", "APPR", "NE", "APZR", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Zu seiner Hilff bekam. Noch wolt es nichts verschlagen.", "tokens": ["Zu", "sei\u00b7ner", "Hilff", "be\u00b7kam", ".", "Noch", "wolt", "es", "nichts", "ver\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$.", "ADV", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.105": {"text": "Dieweil die Gegen-Macht zn gro\u00df und sie zu jagen", "tokens": ["Die\u00b7weil", "die", "Ge\u00b7gen\u00b7Macht", "zn", "gro\u00df", "und", "sie", "zu", "ja\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "NE", "ADJD", "KON", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Von rechtem Eufer war. Sie nahm Gartz und Demmin/", "tokens": ["Von", "rech\u00b7tem", "Eu\u00b7fer", "war", ".", "Sie", "nahm", "Gartz", "und", "Dem\u00b7min", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$.", "PPER", "VVFIN", "NN", "KON", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Das Wolgast/ ", "tokens": ["Das", "Wol\u00b7gast", "/"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.108": {"text": "Bald aber wieder ab/ weil man da St\u00f6\u00df au\u00dftheilte.", "tokens": ["Bald", "a\u00b7ber", "wie\u00b7der", "ab", "/", "weil", "man", "da", "St\u00f6\u00df", "au\u00df\u00b7theil\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "PTKVZ", "$(", "KOUS", "PIS", "ADV", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Wie wenig Gallas sich bey diesem Zug verweilte/", "tokens": ["Wie", "we\u00b7nig", "Gal\u00b7las", "sich", "bey", "die\u00b7sem", "Zug", "ver\u00b7weil\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "PRF", "APPR", "PDAT", "NN", "VVFIN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.110": {"text": "War dennoch nachgesagt: Er hette wol was mehr", "tokens": ["War", "den\u00b7noch", "nach\u00b7ge\u00b7sagt", ":", "Er", "het\u00b7te", "wol", "was", "mehr"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "VVPP", "$.", "PPER", "VAFIN", "ADV", "PWS", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Zu thun vermocht/ dieweil das schwache Schweden-Heer", "tokens": ["Zu", "thun", "ver\u00b7mocht", "/", "die\u00b7weil", "das", "schwa\u00b7che", "Schwe\u00b7den\u00b7Heer"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "VVPP", "$(", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "Sich nach Wollin begab und da so lang bew\u00e4llte/", "tokens": ["Sich", "nach", "Wol\u00b7lin", "be\u00b7gab", "und", "da", "so", "lang", "be\u00b7w\u00e4ll\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NE", "VVFIN", "KON", "ADV", "ADV", "ADJD", "VVFIN", "$("], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.113": {"text": "Bi\u00df Schweden gr\u00f6\u00dfre Hilff an dessen Seite st\u00e4llte.", "tokens": ["Bi\u00df", "Schwe\u00b7den", "gr\u00f6\u00df\u00b7re", "Hilff", "an", "des\u00b7sen", "Sei\u00b7te", "st\u00e4ll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "NN", "APPR", "PRELAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Eh aber di\u00df geschah/ lieff ein halb Jahr dahin.", "tokens": ["Eh", "a\u00b7ber", "di\u00df", "ge\u00b7schah", "/", "lieff", "ein", "halb", "Jahr", "da\u00b7hin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PDS", "VVFIN", "$(", "VVFIN", "ART", "ADJD", "NN", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "Mit diesem zog Bannier neu-m\u00e4chtig durch Stetin.", "tokens": ["Mit", "die\u00b7sem", "zog", "Ban\u00b7nier", "neu\u00b7m\u00e4ch\u00b7tig", "durch", "Ste\u00b7tin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "NE", "ADJD", "APPR", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.116": {"text": "Und legte fich vor Gartz/ da\u00df er nach wenig st\u00fcrmen", "tokens": ["Und", "leg\u00b7te", "fich", "vor", "Gartz", "/", "da\u00df", "er", "nach", "we\u00b7nig", "st\u00fcr\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "APPR", "NN", "$(", "KOUS", "PPER", "APPR", "PIS", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.117": {"text": "Gewann/ und alles Volck/ was er in dem beschirmen", "tokens": ["Ge\u00b7wann", "/", "und", "al\u00b7les", "Volck", "/", "was", "er", "in", "dem", "be\u00b7schir\u00b7men"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "KON", "PIAT", "NN", "$(", "PWS", "PPER", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.118": {"text": "Befundte/ niederhieb/ die B\u00fcrger in das Land", "tokens": ["Be\u00b7fund\u00b7te", "/", "nie\u00b7der\u00b7hieb", "/", "die", "B\u00fcr\u00b7ger", "in", "das", "Land"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$(", "VVIMP", "$(", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.119": {"text": "Vertheilte/ Gartz hierauf mit einem gro\u00dfen Brand", "tokens": ["Ver\u00b7theil\u00b7te", "/", "Gartz", "hier\u00b7auf", "mit", "ei\u00b7nem", "gro\u00b7\u00dfen", "Brand"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$(", "NN", "PAV", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Vertilgte/ Mauern/ W\u00e4ll\u2019 und Th\u00fcrme niederf\u00e4llte. ", "tokens": ["Ver\u00b7tilg\u00b7te", "/", "Mau\u00b7ern", "/", "W\u00e4ll'", "und", "Th\u00fcr\u00b7me", "nie\u00b7der\u00b7f\u00e4ll\u00b7te", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "NN", "$(", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Damit der Ort hinfort sich keinem widerst\u00e4llte.", "tokens": ["Da\u00b7mit", "der", "Ort", "hin\u00b7fort", "sich", "kei\u00b7nem", "wi\u00b7der\u00b7st\u00e4ll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "ADV", "PRF", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Es war ein gro\u00dfer Pa\u00df/ der in dem auf und ab/", "tokens": ["Es", "war", "ein", "gro\u00b7\u00dfer", "Pa\u00df", "/", "der", "in", "dem", "auf", "und", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$(", "ART", "APPR", "ART", "APPR", "KON", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.123": {"text": "Wann er bewehret war/ sehr viel verhinderni\u00df gab.", "tokens": ["Wann", "er", "be\u00b7weh\u00b7ret", "war", "/", "sehr", "viel", "ver\u00b7hin\u00b7der\u00b7ni\u00df", "gab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "VAFIN", "$(", "ADV", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.124": {"text": "Di\u00df alles au\u00dfgericht/ setzt er dreyhundert Pferden/", "tokens": ["Di\u00df", "al\u00b7les", "au\u00df\u00b7ge\u00b7richt", "/", "setzt", "er", "drey\u00b7hun\u00b7dert", "Pfer\u00b7den", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVPP", "$(", "VVFIN", "PPER", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "Durch welche Wolgast solt in Noth erquicket werden/", "tokens": ["Durch", "wel\u00b7che", "Wol\u00b7gast", "solt", "in", "Noth", "er\u00b7quic\u00b7ket", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ADV", "VMFIN", "APPR", "NN", "VVFIN", "VAINF", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.126": {"text": "Bi\u00df Wolgast nach und schlug dieselbe meistens todt.", "tokens": ["Bi\u00df", "Wol\u00b7gast", "nach", "und", "schlug", "die\u00b7sel\u00b7be", "meis\u00b7tens", "todt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "KON", "VVFIN", "PDAT", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "Damit geriet die Stadt in eine solche Noth", "tokens": ["Da\u00b7mit", "ge\u00b7riet", "die", "Stadt", "in", "ei\u00b7ne", "sol\u00b7che", "Noth"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ART", "NN", "APPR", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Von Volck/ von Kraut und Loht/ von Mitteln um zu leben/", "tokens": ["Von", "Volck", "/", "von", "Kraut", "und", "Loht", "/", "von", "Mit\u00b7teln", "um", "zu", "le\u00b7ben", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "APPR", "NN", "KON", "NN", "$(", "APPR", "NN", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Geschwiegen/ was die Pest f\u00fcr Jammer hat gegeben/", "tokens": ["Ge\u00b7schwie\u00b7gen", "/", "was", "die", "Pest", "f\u00fcr", "Jam\u00b7mer", "hat", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWS", "ART", "NN", "APPR", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.130": {"text": "Da\u00df sie sich jhm hierauf nechst Brandenburg ergab.", "tokens": ["Da\u00df", "sie", "sich", "jhm", "hier\u00b7auf", "nechst", "Bran\u00b7den\u00b7burg", "er\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PPER", "PAV", "VVFIN", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "Auf dieses wartet\u2019 er die Macht von Gallas ab/", "tokens": ["Auf", "die\u00b7ses", "war\u00b7tet'", "er", "die", "Macht", "von", "Gal\u00b7las", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "ART", "NN", "APPR", "NE", "PTKVZ", "$("], "meter": "-+-++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.132": {"text": "Die zwantzig tausend starck nicht weit von Grabau stundte/", "tokens": ["Die", "zwant\u00b7zig", "tau\u00b7send", "starck", "nicht", "weit", "von", "Gra\u00b7bau", "stund\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "CARD", "NN", "PTKNEG", "ADJD", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "In Meynung/ da\u00df sie sich zur Schlacht gewillt befundte.", "tokens": ["In", "Mey\u00b7nung", "/", "da\u00df", "sie", "sich", "zur", "Schlacht", "ge\u00b7willt", "be\u00b7fund\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "KOUS", "PPER", "PRF", "APPRART", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.134": {"text": "Das warten war umsonst/ man dacht\u2019 an keine Schlacht/", "tokens": ["Das", "war\u00b7ten", "war", "um\u00b7sonst", "/", "man", "dacht'", "an", "kei\u00b7ne", "Schlacht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVINF", "VAFIN", "ADV", "$(", "PIS", "VVFIN", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.135": {"text": "Allein das scharmuziern das w\u00e4hrte Tag und Nacht/", "tokens": ["Al\u00b7lein", "das", "schar\u00b7mu\u00b7zi\u00b7ern", "das", "w\u00e4hr\u00b7te", "Tag", "und", "Nacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VVFIN", "ART", "ADJA", "NN", "KON", "NN", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.136": {"text": "und blieben bey Malchin der K\u00e4ysrischen und Polen", "tokens": ["und", "blie\u00b7ben", "bey", "Mal\u00b7chin", "der", "K\u00e4y\u00b7sri\u00b7schen", "und", "Po\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "NN", "ART", "NN", "KON", "NE"], "meter": "-+--++-++--+-", "measure": "iambic.hexa.relaxed"}, "line.137": {"text": "Fast bey ein tausend Mann durch Bande/ Schwerdt und", "tokens": ["Fast", "bey", "ein", "tau\u00b7send", "Mann", "durch", "Ban\u00b7de", "/", "Schwerdt", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "APPR", "ART", "CARD", "NN", "APPR", "NN", "$(", "NE", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.138": {"text": "Kolen.", "tokens": ["Ko\u00b7len", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.139": {"text": "Hergegen blieb durch Pest und gro\u00dfe Hungers Noht", "tokens": ["Her\u00b7ge\u00b7gen", "blieb", "durch", "Pest", "und", "gro\u00b7\u00dfe", "Hun\u00b7gers", "Noht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPR", "NN", "KON", "ADJA", "NN", "NN"], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}, "line.140": {"text": "Ein gro\u00dfes Schweden-Volck/ bevor vom neuen/ todt.", "tokens": ["Ein", "gro\u00b7\u00dfes", "Schwe\u00b7den\u00b7Volck", "/", "be\u00b7vor", "vom", "neu\u00b7en", "/", "todt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "KOUS", "APPRART", "ADJA", "$(", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.141": {"text": "Di\u00df gab dem Gallas Muht was l\u00e4nger da zu bleiben/", "tokens": ["Di\u00df", "gab", "dem", "Gal\u00b7las", "Muht", "was", "l\u00e4n\u00b7ger", "da", "zu", "blei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "NN", "PWS", "ADJD", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.142": {"text": "In Meynung seinen Feind durch solches zu zersteuben.", "tokens": ["In", "Mey\u00b7nung", "sei\u00b7nen", "Feind", "durch", "sol\u00b7ches", "zu", "zer\u00b7steu\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPOSAT", "NN", "APPR", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.143": {"text": "Und hatt' er diesesmal sein Lager bey Malchin/", "tokens": ["Und", "hatt'", "er", "die\u00b7ses\u00b7mal", "sein", "La\u00b7ger", "bey", "Mal\u00b7chin", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "PPOSAT", "NN", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "Sein Gegner aber lag anjetzo bey Demmin.", "tokens": ["Sein", "Geg\u00b7ner", "a\u00b7ber", "lag", "an\u00b7je\u00b7tzo", "bey", "Dem\u00b7min", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN", "ADV", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.145": {"text": "Ditz dauerte so lang bi\u00df sich der Frost erregte/", "tokens": ["Ditz", "dau\u00b7er\u00b7te", "so", "lang", "bi\u00df", "sich", "der", "Frost", "er\u00b7reg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "ADJD", "APPR", "PRF", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.146": {"text": "Der Schweden gro\u00dfe Noth/ die Pest/ gem\u00e4hlig legte/", "tokens": ["Der", "Schwe\u00b7den", "gro\u00b7\u00dfe", "Noth", "/", "die", "Pest", "/", "ge\u00b7m\u00e4h\u00b7lig", "leg\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NE", "ADJA", "NN", "$(", "ART", "NN", "$(", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "Den Gallas aber fort nach andern Pl\u00e4tzen trieb/", "tokens": ["Den", "Gal\u00b7las", "a\u00b7ber", "fort", "nach", "an\u00b7dern", "Pl\u00e4t\u00b7zen", "trieb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKVZ", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "Dem General Bannier mit schnellem Fu\u00df und Hieb", "tokens": ["Dem", "Ge\u00b7ne\u00b7ral", "Ban\u00b7nier", "mit", "schnel\u00b7lem", "Fu\u00df", "und", "Hieb"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NE", "APPR", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.149": {"text": "Allzeit im Nacken war/ und nach der Elbe r\u00fcckte/", "tokens": ["All\u00b7zeit", "im", "Na\u00b7cken", "war", "/", "und", "nach", "der", "El\u00b7be", "r\u00fcck\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "VAFIN", "$(", "KON", "APPR", "ART", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Wo er drey tausend Mann ins L\u00fcneburgsche schickte/", "tokens": ["Wo", "er", "drey", "tau\u00b7send", "Mann", "ins", "L\u00fc\u00b7ne\u00b7burg\u00b7sche", "schick\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "CARD", "CARD", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.151": {"text": "Des Kings geschlagen Volck (von Holl- und Engeland", "tokens": ["Des", "Kings", "ge\u00b7schla\u00b7gen", "Volck", "(", "von", "Holl", "und", "En\u00b7ge\u00b7land"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "NN", "$(", "APPR", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.152": {"text": "Dem Hause Heydelberg zum b\u00e4sten abgesand)", "tokens": ["Dem", "Hau\u00b7se", "Hey\u00b7del\u00b7berg", "zum", "b\u00e4s\u00b7ten", "ab\u00b7ge\u00b7sand", ")"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Durch Hatzfelds seine Macht bey Lemgau \u00fcbersieget/", "tokens": ["Durch", "Hatz\u00b7felds", "sei\u00b7ne", "Macht", "bey", "Lem\u00b7gau", "\u00fc\u00b7ber\u00b7sie\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PPOSAT", "NN", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.154": {"text": "Wo Pfaltzgraf Ruprecht selbst gefangen war gekrieget/", "tokens": ["Wo", "Pfaltz\u00b7graf", "Ru\u00b7precht", "selbst", "ge\u00b7fan\u00b7gen", "war", "ge\u00b7krie\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NE", "ADV", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Annoch an sich zu ziehn. Doch es fiel etwas schwehr/", "tokens": ["An\u00b7noch", "an", "sich", "zu", "ziehn", ".", "Doch", "es", "fiel", "et\u00b7was", "schwehr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRF", "PTKZU", "VVINF", "$.", "KON", "PPER", "VVFIN", "ADV", "ADJD", "$("], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.156": {"text": "Dann es kam Hertzog G\u00f6rg mit dem von Broy daher", "tokens": ["Dann", "es", "kam", "Hert\u00b7zog", "G\u00f6rg", "mit", "dem", "von", "Broy", "da\u00b7her"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VVFIN", "NE", "NE", "APPR", "ART", "APPR", "NE", "PAV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.157": {"text": "und schlug bey Boytzenburg vierhundert Mann darnieder/", "tokens": ["und", "schlug", "bey", "Boy\u00b7tzen\u00b7burg", "vier\u00b7hun\u00b7dert", "Mann", "dar\u00b7nie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "CARD", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.158": {"text": "Bey Goldberg gleich so viel. Daher Bannier sich wieder", "tokens": ["Bey", "Gold\u00b7berg", "gleich", "so", "viel", ".", "Da\u00b7her", "Ban\u00b7nier", "sich", "wie\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "ADV", "ADV", "ADV", "$.", "PAV", "NN", "PRF", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.159": {"text": "Nach Mecklenburg begab. Kaum da/ kam Zeitung an/", "tokens": ["Nach", "Meck\u00b7len\u00b7burg", "be\u00b7gab", ".", "Kaum", "da", "/", "kam", "Zei\u00b7tung", "an", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$.", "ADV", "ADV", "$(", "VVFIN", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.160": {"text": "Es w\u00e4re ", "tokens": ["Es", "w\u00e4\u00b7re"], "token_info": ["word", "word"], "pos": ["PPER", "VAFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.161": {"text": "Jm Anzug sich bey Wisch dem Gallas zu zu ziehen.", "tokens": ["Jm", "An\u00b7zug", "sich", "bey", "Wisch", "dem", "Gal\u00b7las", "zu", "zu", "zie\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PRF", "APPR", "NN", "ART", "NN", "PTKZU", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.162": {"text": "Man sah Cur-Sachsen sich vor di\u00dfmal gro\u00df bem\u00fchen/", "tokens": ["Man", "sah", "Cur\u00b7Sach\u00b7sen", "sich", "vor", "di\u00df\u00b7mal", "gro\u00df", "be\u00b7m\u00fc\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NE", "PRF", "APPR", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "Der Schweden lo\u00df zu seyn. Daher auch eine Macht", "tokens": ["Der", "Schwe\u00b7den", "lo\u00df", "zu", "seyn", ".", "Da\u00b7her", "auch", "ei\u00b7ne", "Macht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKVZ", "PTKZU", "VAINF", "$.", "PAV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "und Hilff der andern nach von jhr wurd aufgebracht.", "tokens": ["und", "Hilff", "der", "an\u00b7dern", "nach", "von", "jhr", "wurd", "auf\u00b7ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "ADJA", "APPR", "APPR", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.165": {"text": "Wie hier mit ", "tokens": ["Wie", "hier", "mit"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "ADV", "APPR"], "meter": "+-+", "measure": "trochaic.di"}, "line.166": {"text": "und muste dieser Held an Vitzthums Stelle stehen.", "tokens": ["und", "mus\u00b7te", "die\u00b7ser", "Held", "an", "Vitzt\u00b7hums", "Stel\u00b7le", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PDAT", "NN", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "Bannier di\u00df h\u00f6rende/ gieng auf denselben lo\u00df/", "tokens": ["Ban\u00b7nier", "di\u00df", "h\u00f6\u00b7ren\u00b7de", "/", "gieng", "auf", "den\u00b7sel\u00b7ben", "lo\u00df", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PDS", "VVFIN", "$(", "VVFIN", "APPR", "PDS", "PTKVZ", "$("], "meter": "+--+---+-+-+", "measure": "dactylic.di.plus"}, "line.168": {"text": "Schlug dritthalb tausend Man\u0303 ", "tokens": ["Schlug", "dritt\u00b7halb", "tau\u00b7send", "Ma\u00f1"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PAV", "CARD", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.169": {"text": "Die K\u00e4ysrischen aus Wisch zu dessen Hilff\u2019 erschienen.", "tokens": ["Die", "K\u00e4y\u00b7sri\u00b7schen", "aus", "Wisch", "zu", "des\u00b7sen", "Hilff'", "er\u00b7schie\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "APPR", "PRELAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Und nun sah man das Gl\u00fcck die Schweden neu bedienen.", "tokens": ["Und", "nun", "sah", "man", "das", "Gl\u00fcck", "die", "Schwe\u00b7den", "neu", "be\u00b7die\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "ART", "NN", "ART", "NE", "ADJD", "VVINF", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.171": {"text": "Kaum da\u00df die S\u00e4chsischen gedachte Niederlag", "tokens": ["Kaum", "da\u00df", "die", "S\u00e4ch\u00b7si\u00b7schen", "ge\u00b7dach\u00b7te", "Nie\u00b7der\u00b7lag"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Erlitten/ h\u00f6rte man von einem andern Schlag/", "tokens": ["Er\u00b7lit\u00b7ten", "/", "h\u00f6r\u00b7te", "man", "von", "ei\u00b7nem", "an\u00b7dern", "Schlag", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "VVFIN", "PIS", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.173": {"text": "Den die Gallasischen bey Perlenberg bekamen.", "tokens": ["Den", "die", "Gal\u00b7la\u00b7si\u00b7schen", "bey", "Per\u00b7len\u00b7berg", "be\u00b7ka\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "APPR", "NE", "VVFIN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.174": {"text": "Worauf sie allesamt den Zug was h\u00f6her nahmen.", "tokens": ["Wo\u00b7rauf", "sie", "al\u00b7le\u00b7samt", "den", "Zug", "was", "h\u00f6\u00b7her", "nah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "ART", "NN", "PWS", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.175": {"text": "Gallas nach Osterburg/ nach Stendel ", "tokens": ["Gal\u00b7las", "nach", "Os\u00b7ter\u00b7burg", "/", "nach", "Sten\u00b7del"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NE", "APPR", "NE", "$(", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.176": {"text": "Der Klitzing nach Berlin/ Bannier nach jhnen hin.", "tokens": ["Der", "Klit\u00b7zing", "nach", "Ber\u00b7lin", "/", "Ban\u00b7nier", "nach", "jh\u00b7nen", "hin", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "$(", "NE", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.177": {"text": "Damit so kriegten wir den Krieg mit allen Peitschen", "tokens": ["Da\u00b7mit", "so", "krieg\u00b7ten", "wir", "den", "Krieg", "mit", "al\u00b7len", "Peit\u00b7schen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.178": {"text": "Vom neuen von der See ins innerste der Deutschen.", "tokens": ["Vom", "neu\u00b7en", "von", "der", "See", "ins", "in\u00b7ners\u00b7te", "der", "Deut\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "APPR", "ART", "NN", "APPRART", "ADJA", "ART", "NN", "$."], "meter": "-+---+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.179": {"text": "Wie er mit starcker Flut sich an die See begab/", "tokens": ["Wie", "er", "mit", "star\u00b7cker", "Flut", "sich", "an", "die", "See", "be\u00b7gab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ADJA", "NN", "PRF", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "So gieng er ebende sehr schleunig wieder ab.", "tokens": ["So", "gieng", "er", "e\u00b7ben\u00b7de", "sehr", "schleu\u00b7nig", "wie\u00b7der", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "Der eine wolte fest beym Prager-Frieden stehen/", "tokens": ["Der", "ei\u00b7ne", "wol\u00b7te", "fest", "beym", "Pra\u00b7ger\u00b7Frie\u00b7den", "ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VMFIN", "ADJD", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Und dieser wolte noch nach einem b\u00e4ssern sehen.", "tokens": ["Und", "die\u00b7ser", "wol\u00b7te", "noch", "nach", "ei\u00b7nem", "b\u00e4s\u00b7sern", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VMFIN", "ADV", "APPR", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.183": {"text": "Beklagens-werte Zeit/ die Deutschland neu bekam/", "tokens": ["Be\u00b7kla\u00b7gen\u00b7swer\u00b7te", "Zeit", "/", "die", "Deutschland", "neu", "be\u00b7kam", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$(", "ART", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.184": {"text": "Eh GOtt dem Marspiter sein blntig Zepter nahm!", "tokens": ["Eh", "Gott", "dem", "Mar\u00b7spi\u00b7ter", "sein", "bln\u00b7tig", "Zep\u00b7ter", "nahm", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "PPOSAT", "ADJD", "NN", "VVFIN", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.185": {"text": "Es war der S\u00fcnden-Schuld/ aus Blut in Blut verfallen/", "tokens": ["Es", "war", "der", "S\u00fcn\u00b7den\u00b7Schuld", "/", "aus", "Blut", "in", "Blut", "ver\u00b7fal\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "APPR", "NN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "Es halff kein Gegen-stand/ wie m\u00e4chtig auch von allen", "tokens": ["Es", "halff", "kein", "Ge\u00b7gen\u00b7stand", "/", "wie", "m\u00e4ch\u00b7tig", "auch", "von", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$(", "PWAV", "ADJD", "ADV", "APPR", "PIAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Die Hand darwider war. Wann GOtt die L\u00e4nder strafft/", "tokens": ["Die", "Hand", "dar\u00b7wi\u00b7der", "war", ".", "Wann", "Gott", "die", "L\u00e4n\u00b7der", "strafft", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VAFIN", "$.", "PWAV", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "So hilfft es nicht/ was Macht man auch darwider schafft.", "tokens": ["So", "hilfft", "es", "nicht", "/", "was", "Macht", "man", "auch", "dar\u00b7wi\u00b7der", "schafft", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "$(", "PWS", "NN", "PIS", "ADV", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "Die Wider-R\u00f6msche Macht kam wiederum zu wachsen/", "tokens": ["Die", "Wi\u00b7der\u00b7R\u00f6m\u00b7sche", "Macht", "kam", "wie\u00b7de\u00b7rum", "zu", "wach\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "So wol bey dem Bannier/ als bey dem gro\u00dfen Sachsen", "tokens": ["So", "wol", "bey", "dem", "Ban\u00b7nier", "/", "als", "bey", "dem", "gro\u00b7\u00dfen", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "$(", "KOUS", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.191": {"text": "Von Weymar/ dessen Heer sich nach erhaltner Schlacht", "tokens": ["Von", "Wey\u00b7mar", "/", "des\u00b7sen", "Heer", "sich", "nach", "er\u00b7halt\u00b7ner", "Schlacht"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "$(", "PRELAT", "NN", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "Von Hohenthwiel begab/ ein Ort von gro\u00dfer Macht", "tokens": ["Von", "Ho\u00b7hent\u00b7hwiel", "be\u00b7gab", "/", "ein", "Ort", "von", "gro\u00b7\u00dfer", "Macht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VVFIN", "$(", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "und Wolcken-hoch gesetzt. Kaum da/ war man darinnen/", "tokens": ["und", "Wol\u00b7cken\u00b7hoch", "ge\u00b7setzt", ".", "Kaum", "da", "/", "war", "man", "da\u00b7rin\u00b7nen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVPP", "$.", "ADV", "ADV", "$(", "VAFIN", "PIS", "ADV", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.194": {"text": "Man sahe keinen Theil viel Streits darum beginnen.", "tokens": ["Man", "sa\u00b7he", "kei\u00b7nen", "Theil", "viel", "Streits", "da\u00b7rum", "be\u00b7gin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIAT", "NN", "PIAT", "NN", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Und muste Widerhold des Orts Beschirmer seyn/", "tokens": ["Und", "mus\u00b7te", "Wi\u00b7der\u00b7hold", "des", "Orts", "Be\u00b7schir\u00b7mer", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ART", "NN", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "Er lie\u00df auch keinen Feind bey seinen Zeiten ein/", "tokens": ["Er", "lie\u00df", "auch", "kei\u00b7nen", "Feind", "bey", "sei\u00b7nen", "Zei\u00b7ten", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN", "APPR", "PPOSAT", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.197": {"text": "Nicht achtende/ was Macht sich jhm gab zu erkennen/", "tokens": ["Nicht", "ach\u00b7ten\u00b7de", "/", "was", "Macht", "sich", "jhm", "gab", "zu", "er\u00b7ken\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "$(", "PWS", "NN", "PRF", "PPER", "VVFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.198": {"text": "Man mocht\u2019 jhn wolbefugt den Widerhalt benennen.", "tokens": ["Man", "mocht'", "jhn", "wol\u00b7be\u00b7fugt", "den", "Wi\u00b7der\u00b7halt", "be\u00b7nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Auf dieses setzte man die Macht vor Stutgart an/", "tokens": ["Auf", "die\u00b7ses", "setz\u00b7te", "man", "die", "Macht", "vor", "Stut\u00b7gart", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PIS", "ART", "NN", "APPR", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Das man mit leichter M\u00fch in wenig Zeit gewann.", "tokens": ["Das", "man", "mit", "leich\u00b7ter", "M\u00fch", "in", "we\u00b7nig", "Zeit", "ge\u00b7wann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "APPR", "ADJA", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.201": {"text": "Anjetzo wurd ein Danck- und Bet-Tag au\u00dfgeschrieben/", "tokens": ["An\u00b7je\u00b7tzo", "wurd", "ein", "Dan\u00b7ck", "und", "Bet\u00b7Tag", "au\u00df\u00b7ge\u00b7schrie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ART", "TRUNC", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.202": {"text": "Da\u00df es dem H\u00f6chsten m\u00f6cht hinfort noch mehr belieben/", "tokens": ["Da\u00df", "es", "dem", "H\u00f6chs\u00b7ten", "m\u00f6cht", "hin\u00b7fort", "noch", "mehr", "be\u00b7lie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VMFIN", "ADV", "ADV", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Den Seinen gut zu seyn. Di\u00df alles wol gethan/", "tokens": ["Den", "Sei\u00b7nen", "gut", "zu", "seyn", ".", "Di\u00df", "al\u00b7les", "wol", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKZU", "VAINF", "$.", "PDS", "PIS", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "Griff der ber\u00fchmte F\u00fcrst die Vestung Breysach an.", "tokens": ["Griff", "der", "be\u00b7r\u00fchm\u00b7te", "F\u00fcrst", "die", "Ves\u00b7tung", "Brey\u00b7sach", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "ART", "NN", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "Ein Ort von der Natur zum Wunder fest gemachet/", "tokens": ["Ein", "Ort", "von", "der", "Na\u00b7tur", "zum", "Wun\u00b7der", "fest", "ge\u00b7ma\u00b7chet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "APPRART", "NN", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Von starckem Krieges-Volck aufs tapferste bewachet/", "tokens": ["Von", "star\u00b7ckem", "Krie\u00b7ge\u00b7sVolck", "aufs", "tap\u00b7fers\u00b7te", "be\u00b7wa\u00b7chet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Mit allem wol versehn/ von Jederm\u00e4nniglich", "tokens": ["Mit", "al\u00b7lem", "wol", "ver\u00b7sehn", "/", "von", "Je\u00b7der\u00b7m\u00e4n\u00b7nig\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIS", "ADV", "VVINF", "$(", "APPR", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "Unwinnbar vorgestellt/ und hatte solche sich", "tokens": ["Un\u00b7winn\u00b7bar", "vor\u00b7ge\u00b7stellt", "/", "und", "hat\u00b7te", "sol\u00b7che", "sich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "VVPP", "$(", "KON", "VAFIN", "PIAT", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Drey hundert Jahre lang/ zu seines Preiss erheben/", "tokens": ["Drey", "hun\u00b7dert", "Jah\u00b7re", "lang", "/", "zu", "sei\u00b7nes", "Preiss", "er\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "NN", "ADJD", "$(", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "An keinen als allein an Oesterrich ergeben.", "tokens": ["An", "kei\u00b7nen", "als", "al\u00b7lein", "an", "O\u00b7es\u00b7ter\u00b7rich", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "KOKOM", "ADV", "APPR", "NE", "VVPP", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.211": {"text": "Di\u00df alles ungeacht/ umgab der Held den Platz.", "tokens": ["Di\u00df", "al\u00b7les", "un\u00b7ge\u00b7acht", "/", "um\u00b7gab", "der", "Held", "den", "Platz", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "ADJD", "$(", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.212": {"text": "Kaum da/ erschien ", "tokens": ["Kaum", "da", "/", "er\u00b7schien"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADV", "ADV", "$(", "VVFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.213": {"text": "Zu thun/ doch gantz umsonst. Dann Bernhard gieng entge-", "tokens": ["Zu", "thun", "/", "doch", "gantz", "um\u00b7sonst", ".", "Dann", "Bern\u00b7hard", "gieng", "ent\u00b7ge"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$(", "ADV", "ADV", "ADV", "$.", "ADV", "NE", "VVFIN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "und setzte so an sie/ ", "tokens": ["und", "setz\u00b7te", "so", "an", "sie", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PPER", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.215": {"text": "Voll blasser Leichen war. Zw\u00f6lff hundert blieben todt/", "tokens": ["Voll", "blas\u00b7ser", "Lei\u00b7chen", "war", ".", "Zw\u00f6lff", "hun\u00b7dert", "blie\u00b7ben", "todt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "VAFIN", "$.", "NN", "CARD", "VVFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "Drey tausend in verhafft/ der Rest entkam mit Noth.", "tokens": ["Drey", "tau\u00b7send", "in", "ver\u00b7hafft", "/", "der", "Rest", "ent\u00b7kam", "mit", "Noth", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "APPR", "NN", "$(", "ART", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Da blieb die Cantzeley/ da blieben alle St\u00fccke/", "tokens": ["Da", "blieb", "die", "Cant\u00b7ze\u00b7ley", "/", "da", "blie\u00b7ben", "al\u00b7le", "St\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$(", "ADV", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "und was dazu geh\u00f6rt mit vielem Meel zu r\u00fccke.", "tokens": ["und", "was", "da\u00b7zu", "ge\u00b7h\u00f6rt", "mit", "vie\u00b7lem", "Meel", "zu", "r\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PAV", "VVFIN", "APPR", "PIS", "NN", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.219": {"text": "Es hatte zwar der Feind sich m\u00e4chtig in der Schlacht", "tokens": ["Es", "hat\u00b7te", "zwar", "der", "Feind", "sich", "m\u00e4ch\u00b7tig", "in", "der", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "PRF", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.220": {"text": "Erwiesen/ und den Held Tupadel weggebracht/", "tokens": ["Er\u00b7wie\u00b7sen", "/", "und", "den", "Held", "Tu\u00b7pa\u00b7del", "weg\u00b7ge\u00b7bracht", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "ART", "NN", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.221": {"text": "Weil er sich allzu tieff hatt\u2019 in den Feind verhauen/", "tokens": ["Weil", "er", "sich", "all\u00b7zu", "tieff", "hatt'", "in", "den", "Feind", "ver\u00b7hau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PTKA", "ADJD", "VAFIN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Noch gleichwol lie\u00df der Sieg sich jhm von R\u00fccken schauen/", "tokens": ["Noch", "gleich\u00b7wol", "lie\u00df", "der", "Sieg", "sich", "jhm", "von", "R\u00fc\u00b7cken", "schau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "PRF", "PPER", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "und reichte seinen Krantz in Hertzog Bernhards Hand.", "tokens": ["und", "reich\u00b7te", "sei\u00b7nen", "Krantz", "in", "Hert\u00b7zog", "Bern\u00b7hards", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "APPR", "NE", "NE", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.224": {"text": "Damit kam Breysachs Sach\u2019 in einen tr\u00fcben Stand.", "tokens": ["Da\u00b7mit", "kam", "Brey\u00b7sachs", "Sach'", "in", "ei\u00b7nen", "tr\u00fc\u00b7ben", "Stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "NE", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "Und machete der Schlag/ am Dorffe Wittenweyer", "tokens": ["Und", "ma\u00b7che\u00b7te", "der", "Schlag", "/", "am", "Dorf\u00b7fe", "Wit\u00b7ten\u00b7we\u00b7yer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "$(", "APPRART", "NN", "NE"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.226": {"text": "Geschehen/ alle Ding\u2019 in Breysach trefflich teuer.", "tokens": ["Ge\u00b7sche\u00b7hen", "/", "al\u00b7le", "Ding'", "in", "Brey\u00b7sach", "treff\u00b7lich", "teu\u00b7er", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PIAT", "NN", "APPR", "NE", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.227": {"text": "Es schlug noch \u00fcber di\u00df ein ander ", "tokens": ["Es", "schlug", "noch", "\u00fc\u00b7ber", "di\u00df", "ein", "an\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDS", "ART", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.228": {"text": "In dem das Pulver-Hau\u00df daselbst in einem Nu", "tokens": ["In", "dem", "das", "Pul\u00b7ver\u00b7Hau\u00df", "da\u00b7selbst", "in", "ei\u00b7nem", "Nu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "NN", "PAV", "APPR", "ART", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "Durch einen Brand zersprung/ viel H\u00e4user niederst\u00fcrtzte/", "tokens": ["Durch", "ei\u00b7nen", "Brand", "zer\u00b7sprung", "/", "viel", "H\u00e4u\u00b7ser", "nie\u00b7ders\u00b7t\u00fcrtz\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$(", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.230": {"text": "und sonderlich an Wehr- und Lebens-Mitteln k\u00fcrtzte.", "tokens": ["und", "son\u00b7der\u00b7lich", "an", "Wehr", "und", "Le\u00b7bens\u00b7Mit\u00b7teln", "k\u00fcrtz\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "TRUNC", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Di\u00df nahm der Held in acht/ und satzte m\u00e4chtig an.", "tokens": ["Di\u00df", "nahm", "der", "Held", "in", "acht", "/", "und", "satz\u00b7te", "m\u00e4ch\u00b7tig", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "CARD", "$(", "KON", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "Er sah auch \u00fcber di\u00df ein etlich hundert Mann/", "tokens": ["Er", "sah", "auch", "\u00fc\u00b7ber", "di\u00df", "ein", "et\u00b7lich", "hun\u00b7dert", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDS", "ART", "ADJD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Die man aus Breysach trieb/ die Noth nicht zu vermehren/", "tokens": ["Die", "man", "aus", "Brey\u00b7sach", "trieb", "/", "die", "Noth", "nicht", "zu", "ver\u00b7meh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "APPR", "NE", "VVFIN", "$(", "ART", "NN", "PTKNEG", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.234": {"text": "Die alle gaben jhm genugsam anzuh\u00f6ren/", "tokens": ["Die", "al\u00b7le", "ga\u00b7ben", "jhm", "ge\u00b7nug\u00b7sam", "an\u00b7zu\u00b7h\u00f6\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PPER", "ADJD", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Was Mangel in der Stadt/ und da\u00df so ungefehr", "tokens": ["Was", "Man\u00b7gel", "in", "der", "Stadt", "/", "und", "da\u00df", "so", "un\u00b7ge\u00b7fehr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "NN", "APPR", "ART", "NN", "$(", "KON", "KOUS", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "Ein tausend Mann/ nicht mehr/ da zur Besatzung w\u00e4r.", "tokens": ["Ein", "tau\u00b7send", "Mann", "/", "nicht", "mehr", "/", "da", "zur", "Be\u00b7sat\u00b7zung", "w\u00e4r", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "$(", "PTKNEG", "ADV", "$(", "ADV", "APPRART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.237": {"text": "In dem er eufrig war die Vestung zu besiegen/", "tokens": ["In", "dem", "er", "euf\u00b7rig", "war", "die", "Ves\u00b7tung", "zu", "be\u00b7sie\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADJD", "VAFIN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "Kam der von Lothringen mit jhm darum zu kriegen.", "tokens": ["Kam", "der", "von", "Loth\u00b7rin\u00b7gen", "mit", "jhm", "da\u00b7rum", "zu", "krie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "APPR", "NE", "APPR", "PPER", "PAV", "PTKZU", "VVINF", "$."], "meter": "+--+---+-+-+-", "measure": "dactylic.di.plus"}, "line.239": {"text": "Wiewol er anders nicht mit diesem hat gesucht", "tokens": ["Wie\u00b7wol", "er", "an\u00b7ders", "nicht", "mit", "die\u00b7sem", "hat", "ge\u00b7sucht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "PTKNEG", "APPR", "PDAT", "VAFIN", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "Als nur der Vestung Noth mit Meel und andrer Frucht", "tokens": ["Als", "nur", "der", "Ves\u00b7tung", "Noth", "mit", "Meel", "und", "an\u00b7drer", "Frucht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "NN", "NN", "APPR", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Zu mindern. Aber h\u00f6rt es gieng jhm ", "tokens": ["Zu", "min\u00b7dern", ".", "A\u00b7ber", "h\u00f6rt", "es", "gieng", "jhm"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$.", "KON", "VVFIN", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.242": {"text": "Als es dem G\u00f6tzen gieng. Er war vom Gl\u00fcck verlassen/", "tokens": ["Als", "es", "dem", "G\u00f6t\u00b7zen", "gieng", ".", "Er", "war", "vom", "Gl\u00fcck", "ver\u00b7las\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$.", "PPER", "VAFIN", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Sein Volck blieb in dem Lauff/ sein lieber ", "tokens": ["Sein", "Volck", "blieb", "in", "dem", "Lauff", "/", "sein", "lie\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "ART", "NN", "$(", "PPOSAT", "ADV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.244": {"text": "Als F\u00fchrer dieses Heers/ wie auch der Held ", "tokens": ["Als", "F\u00fch\u00b7rer", "die\u00b7ses", "Heers", "/", "wie", "auch", "der", "Held"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "PDAT", "NN", "$(", "KOKOM", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.245": {"text": "Verfielen in Verhafft. Sehr so gieng di\u00df zu scheitern", "tokens": ["Ver\u00b7fie\u00b7len", "in", "Ver\u00b7hafft", ".", "Sehr", "so", "gieng", "di\u00df", "zu", "schei\u00b7tern"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "NN", "$.", "ADV", "ADV", "VVFIN", "PDS", "PTKZU", "VVINF"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.246": {"text": "und blo\u00df durch eine Schaar von Weymarischen Reitern.", "tokens": ["und", "blo\u00df", "durch", "ei\u00b7ne", "Schaar", "von", "Wey\u00b7ma\u00b7ri\u00b7schen", "Rei\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.247": {"text": "Di\u00df war das andre Heer das wegen Brysach lag.", "tokens": ["Di\u00df", "war", "das", "and\u00b7re", "Heer", "das", "we\u00b7gen", "Bry\u00b7sach", "lag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "ART", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "Und nun kam auch das dritt' und letzte vor den Tag.", "tokens": ["Und", "nun", "kam", "auch", "das", "dritt'", "und", "letz\u00b7te", "vor", "den", "Tag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "ART", "ADJA", "KON", "ADJA", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Ein Heer von gro\u00dfer Macht und Kriegs-gewohnten Scha-", "tokens": ["Ein", "Heer", "von", "gro\u00b7\u00dfer", "Macht", "und", "Kriegs\u00b7ge\u00b7wohn\u00b7ten", "Scha"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "KON", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.250": {"text": "Bey dem ", "tokens": ["Bey", "dem"], "token_info": ["word", "word"], "pos": ["APPR", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.251": {"text": "Sie hatten den Befehl: Geht hin/ und thut Entsatz/", "tokens": ["Sie", "hat\u00b7ten", "den", "Be\u00b7fehl", ":", "Geht", "hin", "/", "und", "thut", "Ent\u00b7satz", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$.", "VVFIN", "PTKVZ", "$(", "KON", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Wo nicht/ so sterbet nur geh\u00e4ufft vor solchen Platz.", "tokens": ["Wo", "nicht", "/", "so", "ster\u00b7bet", "nur", "ge\u00b7h\u00e4ufft", "vor", "sol\u00b7chen", "Platz", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$(", "ADV", "VVFIN", "ADV", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "Sie thaten jhre Pflicht/ und st\u00fcrmten nach der Br\u00fccken", "tokens": ["Sie", "tha\u00b7ten", "jhre", "Pflicht", "/", "und", "st\u00fcrm\u00b7ten", "nach", "der", "Br\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$(", "KON", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.254": {"text": "Mit solchem ", "tokens": ["Mit", "sol\u00b7chem"], "token_info": ["word", "word"], "pos": ["APPR", "PIAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.255": {"text": "Jm Felde fliegen sah/ also gieng das Gesch\u00fctz", "tokens": ["Jm", "Fel\u00b7de", "flie\u00b7gen", "sah", "/", "al\u00b7so", "gieng", "das", "Ge\u00b7sch\u00fctz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVINF", "VVFIN", "$(", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.256": {"text": "Auf derer Anlauff lo\u00df. Doch es wurd jhre Hitz", "tokens": ["Auf", "de\u00b7rer", "An\u00b7lauff", "lo\u00df", ".", "Doch", "es", "wurd", "jhre", "Hitz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDS", "NN", "PTKVZ", "$.", "KON", "PPER", "VAFIN", "PPOSAT", "NN"], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.257": {"text": "Hiedurch nur mehr vermehrt/ da\u00df sie sich der Verschantzten", "tokens": ["Hie\u00b7durch", "nur", "mehr", "ver\u00b7mehrt", "/", "da\u00df", "sie", "sich", "der", "Ver\u00b7schantz\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "ADV", "ADJD", "$(", "KOUS", "PPER", "PRF", "ART", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.258": {"text": "Bem\u00e4chtigten/ und da jhr\u2019 eigen Fahnen pftantzten.", "tokens": ["Be\u00b7m\u00e4ch\u00b7tig\u00b7ten", "/", "und", "da", "jhr'", "ei\u00b7gen", "Fah\u00b7nen", "pf\u00b7tantz\u00b7ten", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "KOUS", "PPER", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+--+--+--+-", "measure": "iambic.penta.relaxed"}, "line.259": {"text": "Als Hertzog Bernhards Heer des Feindes Macht vernam/", "tokens": ["Als", "Hert\u00b7zog", "Bern\u00b7hards", "Heer", "des", "Fein\u00b7des", "Macht", "ver\u00b7nam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "NN", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "und da\u00df sie als ein Sturm daher gewirbelt kam/", "tokens": ["und", "da\u00df", "sie", "als", "ein", "Sturm", "da\u00b7her", "ge\u00b7wir\u00b7belt", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "KOUS", "ART", "NN", "PAV", "VVPP", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.261": {"text": "Erwie\u00df es wiederum was Macht auf seiner Seiten.", "tokens": ["Er\u00b7wie\u00df", "es", "wie\u00b7de\u00b7rum", "was", "Macht", "auf", "sei\u00b7ner", "Sei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PWS", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Und sihe da zwey Heer nochmahls um Brysach streiten.", "tokens": ["Und", "si\u00b7he", "da", "zwey", "Heer", "noch\u00b7mahls", "um", "Bry\u00b7sach", "strei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "CARD", "NN", "ADV", "APPR", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "Lamboy hielt sich wol/ noch b\u00e4sser aber ficht", "tokens": ["Lam\u00b7boy", "hielt", "sich", "wol", "/", "noch", "b\u00e4s\u00b7ser", "a\u00b7ber", "ficht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PRF", "ADV", "$(", "ADV", "ADJD", "ADV", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.264": {"text": "Sein Feind/ der jhm sein Heer fast halb zu Grunde richt.", "tokens": ["Sein", "Feind", "/", "der", "jhm", "sein", "Heer", "fast", "halb", "zu", "Grun\u00b7de", "richt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PRELS", "PPER", "PPOSAT", "NN", "ADV", "ADJD", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "Theils in den Reyhn verjagt/ theils durch die Klinge f\u00e4llet/", "tokens": ["Theils", "in", "den", "Reyhn", "ver\u00b7jagt", "/", "theils", "durch", "die", "Klin\u00b7ge", "f\u00e4l\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVPP", "$(", "ADV", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.266": {"text": "Theils unter seine Macht mit Macht gefangen st\u00e4llet. ", "tokens": ["Theils", "un\u00b7ter", "sei\u00b7ne", "Macht", "mit", "Macht", "ge\u00b7fan\u00b7gen", "st\u00e4l\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "APPR", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.267": {"text": "G\u00f6tz hatt\u2019 es eben so/ wie m\u00e4chtig er auch focht.", "tokens": ["G\u00f6tz", "hatt'", "es", "e\u00b7ben", "so", "/", "wie", "m\u00e4ch\u00b7tig", "er", "auch", "focht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "ADV", "ADV", "$(", "PWAV", "ADJD", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Es war nun mehr an dem/ da\u00df niemand mehr vermocht", "tokens": ["Es", "war", "nun", "mehr", "an", "dem", "/", "da\u00df", "nie\u00b7mand", "mehr", "ver\u00b7mocht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "APPR", "ART", "$(", "KOUS", "PIS", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.269": {"text": "Ein m\u00e4chtiges zu thun/ des gro\u00dfen Weymars H\u00e4nden", "tokens": ["Ein", "m\u00e4ch\u00b7ti\u00b7ges", "zu", "thun", "/", "des", "gro\u00b7\u00dfen", "Wey\u00b7mars", "H\u00e4n\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "PTKZU", "VVINF", "$(", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Die lang-verlangte Braut- von Brysach abzuwenden.", "tokens": ["Die", "lang\u00b7ver\u00b7lang\u00b7te", "Braut", "von", "Bry\u00b7sach", "ab\u00b7zu\u00b7wen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "TRUNC", "APPR", "NE", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "Was auch der K\u00e4yser selbst dem Reynach ernstlich schrieb:", "tokens": ["Was", "auch", "der", "K\u00e4y\u00b7ser", "selbst", "dem", "Rey\u00b7nach", "ernst\u00b7lich", "schrieb", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "ADV", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Nim Brysach wol in acht/ der Ort ist uns sehr lieb/", "tokens": ["Nim", "Bry\u00b7sach", "wol", "in", "acht", "/", "der", "Ort", "ist", "uns", "sehr", "lieb", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "ADV", "APPR", "CARD", "$(", "ART", "NN", "VAFIN", "PPER", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Denck da\u00df in aller Welt ein Brysach sey zu finden/", "tokens": ["Denck", "da\u00df", "in", "al\u00b7ler", "Welt", "ein", "Bry\u00b7sach", "sey", "zu", "fin\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOUS", "APPR", "PIAT", "NN", "ART", "NN", "VAFIN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "So must\u2019 es endlich doch sich lassen \u00fcberwinden. ", "tokens": ["So", "must'", "es", "end\u00b7lich", "doch", "sich", "las\u00b7sen", "\u00fc\u00b7berw\u00b7in\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADV", "PRF", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "Dann sie die Hungers Noth zu solchem Jammer bracht\u2019/", "tokens": ["Dann", "sie", "die", "Hun\u00b7gers", "Noth", "zu", "sol\u00b7chem", "Jam\u00b7mer", "bracht'", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ART", "NN", "NN", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.276": {"text": "Als nie in aller Welt dergleichen wird gedacht.", "tokens": ["Als", "nie", "in", "al\u00b7ler", "Welt", "derg\u00b7lei\u00b7chen", "wird", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "PIAT", "NN", "PIS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Schweig nur Jerusalem/ Samaria de\u00dfgleichen.", "tokens": ["Schweig", "nur", "Je\u00b7ru\u00b7sa\u00b7lem", "/", "Sa\u00b7ma\u00b7ria", "de\u00df\u00b7glei\u00b7chen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "NE", "$(", "NE", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.278": {"text": "Des Brysachs Hungers Noth war keine zuvergleichen.", "tokens": ["Des", "Bry\u00b7sachs", "Hun\u00b7gers", "Noth", "war", "kei\u00b7ne", "zu\u00b7ver\u00b7glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "NN", "VAFIN", "PIAT", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "Der Mensch fra\u00df Menschen auf/ er \u00f6ffnete das Grab/", "tokens": ["Der", "Mensch", "fra\u00df", "Men\u00b7schen", "auf", "/", "er", "\u00f6ff\u00b7ne\u00b7te", "das", "Grab", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "APPR", "$(", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "und nagete das Fleisch von alten Todten ab.", "tokens": ["und", "na\u00b7ge\u00b7te", "das", "Fleisch", "von", "al\u00b7ten", "Tod\u00b7ten", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Die Hand bestarret mir ob solchen Greuel-Dingen/", "tokens": ["Die", "Hand", "be\u00b7star\u00b7ret", "mir", "ob", "sol\u00b7chen", "Greu\u00b7el\u00b7Din\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "KOUS", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Der Mund wird Eckels voll hiervon mehr vorzubringen.", "tokens": ["Der", "Mund", "wird", "E\u00b7ckels", "voll", "hier\u00b7von", "mehr", "vor\u00b7zu\u00b7brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "NE", "ADJD", "PAV", "ADV", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "So gieng dann Brysach nun in Hertzog Bernhards Macht/", "tokens": ["So", "gieng", "dann", "Bry\u00b7sach", "nun", "in", "Hert\u00b7zog", "Bern\u00b7hards", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NE", "ADV", "APPR", "NE", "NE", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.284": {"text": "Nach dem er ein halb Jahr/ und auch die dritte Schlacht", "tokens": ["Nach", "dem", "er", "ein", "halb", "Jahr", "/", "und", "auch", "die", "drit\u00b7te", "Schlacht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "ART", "ADJD", "NN", "$(", "KON", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "Davor gehalten hatt\u2019. ", "tokens": ["Da\u00b7vor", "ge\u00b7hal\u00b7ten", "hatt'", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PAV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.286": {"text": "Er und gantz Franckreich lie\u00df Trompet- und Paucken regen.", "tokens": ["Er", "und", "gantz", "Fran\u00b7ck\u00b7reich", "lie\u00df", "Trom\u00b7pet", "und", "Pau\u00b7cken", "re\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "ADV", "NE", "VVFIN", "TRUNC", "KON", "NN", "ADJA", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.287": {"text": "Vor allem aber wurd an GOtt ein Danck gethan/", "tokens": ["Vor", "al\u00b7lem", "a\u00b7ber", "wurd", "an", "Gott", "ein", "Danck", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "ADV", "VAFIN", "APPR", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.288": {"text": "Denn Er ist der allein/ der Siege geben kan.", "tokens": ["Denn", "Er", "ist", "der", "al\u00b7lein", "/", "der", "Sie\u00b7ge", "ge\u00b7ben", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "ADV", "$(", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.289": {"text": "Dem Brysach folgete die Landskrohn/ derer Spitzen", "tokens": ["Dem", "Bry\u00b7sach", "fol\u00b7ge\u00b7te", "die", "Lands\u00b7krohn", "/", "de\u00b7rer", "Spit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$(", "PDS", "NN"], "meter": "-+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.290": {"text": "Bi\u00df an die Wolcken gehn/ die immer frey zu sitzen", "tokens": ["Bi\u00df", "an", "die", "Wol\u00b7cken", "gehn", "/", "die", "im\u00b7mer", "frey", "zu", "sit\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "APPR", "ART", "NN", "VVINF", "$(", "ART", "ADV", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Bi\u00dfher gewohnet war. Wie m\u00e4chtig der Verlust", "tokens": ["Bi\u00df\u00b7her", "ge\u00b7woh\u00b7net", "war", ".", "Wie", "m\u00e4ch\u00b7tig", "der", "Ver\u00b7lust"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVPP", "VAFIN", "$.", "PWAV", "ADJD", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Von solchem festen Platz des gro\u00dfen Adlers Brust", "tokens": ["Von", "sol\u00b7chem", "fes\u00b7ten", "Platz", "des", "gro\u00b7\u00dfen", "Ad\u00b7lers", "Brust"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJA", "NN", "ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.293": {"text": "Betr\u00fcbet/ hab\u2019 ich mehr zu schweigen als zu sagen.", "tokens": ["Be\u00b7tr\u00fc\u00b7bet", "/", "hab'", "ich", "mehr", "zu", "schwei\u00b7gen", "als", "zu", "sa\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VAFIN", "PPER", "ADV", "PTKZU", "VVINF", "KOKOM", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Rom selbst mit Spannien betrieb hierob sein Klagen.", "tokens": ["Rom", "selbst", "mit", "Span\u00b7ni\u00b7en", "be\u00b7trieb", "hier\u00b7ob", "sein", "Kla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "NN", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "Es war nun um die Zeit da ", "tokens": ["Es", "war", "nun", "um", "die", "Zeit", "da"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "ART", "NN", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.296": {"text": "Jm kalten Steinbock hatt\u2019/ da sich die N\u00e4chte lang", "tokens": ["Jm", "kal\u00b7ten", "Stein\u00b7bock", "hatt'", "/", "da", "sich", "die", "N\u00e4ch\u00b7te", "lang"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VAFIN", "$(", "KOUS", "PRF", "ART", "NN", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "und voll vom harten Frost bezeigten/ derowegen", "tokens": ["und", "voll", "vom", "har\u00b7ten", "Frost", "be\u00b7zeig\u00b7ten", "/", "de\u00b7ro\u00b7we\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "ADJD", "APPRART", "ADJA", "NN", "VVFIN", "$(", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Gieng Bernhards seine Macht Burgundien zu belegen/", "tokens": ["Gieng", "Bern\u00b7hards", "sei\u00b7ne", "Macht", "Bur\u00b7gun\u00b7di\u00b7en", "zu", "be\u00b7le\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PPOSAT", "NN", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+--+-+-++", "measure": "iambic.septa.relaxed"}, "line.299": {"text": "und das/ was feindlich war/ zu zwingen/ wie sie dann", "tokens": ["und", "das", "/", "was", "feind\u00b7lich", "war", "/", "zu", "zwin\u00b7gen", "/", "wie", "sie", "dann"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ART", "$(", "PWS", "ADJD", "VAFIN", "$(", "PTKZU", "VVINF", "$(", "PWAV", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Joux, Pontalier und mehr/ die ich nicht nennen kan/", "tokens": ["Joux", ",", "Pon\u00b7ta\u00b7lier", "und", "mehr", "/", "die", "ich", "nicht", "nen\u00b7nen", "kan", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "KON", "ADV", "$(", "PRELS", "PPER", "PTKNEG", "VVINF", "VMFIN", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.301": {"text": "Eroborte. ", "tokens": ["E\u00b7ro\u00b7bor\u00b7te", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "--+-", "measure": "anapaest.init"}, "line.302": {"text": "Der Brysach/ einen Prei\u00df der Vesten/ unterbrachte/", "tokens": ["Der", "Bry\u00b7sach", "/", "ei\u00b7nen", "Prei\u00df", "der", "Ves\u00b7ten", "/", "un\u00b7ter\u00b7brach\u00b7te", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "NN", "ART", "NN", "$(", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.303": {"text": "Mocht andre freylich wol erschrecken. Es geschah/", "tokens": ["Mocht", "and\u00b7re", "frey\u00b7lich", "wol", "er\u00b7schre\u00b7cken", ".", "Es", "ge\u00b7schah", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "ADV", "VVINF", "$.", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Der strenge Frost war weg/ der Lentz war wieder da/", "tokens": ["Der", "stren\u00b7ge", "Frost", "war", "weg", "/", "der", "Lentz", "war", "wie\u00b7der", "da", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "$(", "ART", "NN", "VAFIN", "ADV", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.305": {"text": "und Bernhard naherte sich seinem Brysach wieder/", "tokens": ["und", "Bern\u00b7hard", "na\u00b7her\u00b7te", "sich", "sei\u00b7nem", "Bry\u00b7sach", "wie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PRF", "PPOSAT", "NN", "ADV", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "Fiel aber bald darauf in eine Kranckheit nieder.", "tokens": ["Fiel", "a\u00b7ber", "bald", "da\u00b7rauf", "in", "ei\u00b7ne", "Kran\u00b7ck\u00b7heit", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "PAV", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.307": {"text": "Die jhm/ der Sage nach/ des gelben Neydes Gifft/", "tokens": ["Die", "jhm", "/", "der", "Sa\u00b7ge", "nach", "/", "des", "gel\u00b7ben", "Ney\u00b7des", "Gifft", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "$(", "ART", "NN", "APPR", "$(", "ART", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Das sonst gemeiniglich die tapfren Helden trifft/", "tokens": ["Das", "sonst", "ge\u00b7mei\u00b7nig\u00b7lich", "die", "tapf\u00b7ren", "Hel\u00b7den", "trifft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Hatt\u2019 eingefl\u00f6st. Er starb ", "tokens": ["Hatt'", "ein\u00b7ge\u00b7fl\u00f6st", ".", "Er", "starb"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["VAFIN", "VVPP", "$.", "PPER", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.310": {"text": "Sehr sanfft und seeliglich. Sein Tod wurd\u2019 an der Seyne/", "tokens": ["Sehr", "sanfft", "und", "see\u00b7lig\u00b7lich", ".", "Sein", "Tod", "wurd'", "an", "der", "Sey\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$.", "PPOSAT", "NN", "VAFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.311": {"text": "und wo sein Lob erscholl/ h\u00f6chst-trauerig beklagt.", "tokens": ["und", "wo", "sein", "Lob", "er\u00b7scholl", "/", "h\u00f6chst\u00b7trau\u00b7e\u00b7rig", "be\u00b7klagt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPOSAT", "NN", "ADJD", "$(", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.312": {"text": "Der allen Prei\u00df und Ruhm von diesem F\u00fcrsten sagt", "tokens": ["Der", "al\u00b7len", "Prei\u00df", "und", "Ruhm", "von", "die\u00b7sem", "F\u00fcrs\u00b7ten", "sagt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "KON", "NN", "APPR", "PDAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "Hat ", "tokens": ["Hat"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "-", "measure": "single.down"}, "line.314": {"text": "Was man um seinen Sarck zu Ehren hat geschrieben.", "tokens": ["Was", "man", "um", "sei\u00b7nen", "Sarck", "zu", "Eh\u00b7ren", "hat", "ge\u00b7schrie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "PPOSAT", "NN", "APPR", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Dem vormals Sterblichen von Weymar/ aber itzt", "tokens": ["Dem", "vor\u00b7mals", "Sterb\u00b7li\u00b7chen", "von", "Wey\u00b7mar", "/", "a\u00b7ber", "itzt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "$(", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.316": {"text": "Unsterblichen/ sey di\u00df zum Denckmahl eingeritzt:", "tokens": ["U\u00b7nsterb\u00b7li\u00b7chen", "/", "sey", "di\u00df", "zum", "Denck\u00b7mahl", "ein\u00b7ge\u00b7ritzt", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VAFIN", "PDS", "APPRART", "NN", "VVPP", "$."], "meter": "++-+-+-+-+-+", "measure": "unknown.measure.septa"}, "line.317": {"text": "Steh lieber Wandersmann und lies/ so ohne weinen", "tokens": ["Steh", "lie\u00b7ber", "Wan\u00b7ders\u00b7mann", "und", "lies", "/", "so", "oh\u00b7ne", "wei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "NN", "KON", "VVFIN", "$(", "ADV", "APPR", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.318": {"text": "Dein Aug des Weymars Grab kan lesende bescheinen/", "tokens": ["Dein", "Aug", "des", "Wey\u00b7mars", "Grab", "kan", "le\u00b7sen\u00b7de", "be\u00b7schei\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ART", "NN", "NN", "VMFIN", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.319": {"text": "Er starb in Deutschlands Scho\u00df/ von allen starck bekriegt/", "tokens": ["Er", "starb", "in", "Deutschlands", "Scho\u00df", "/", "von", "al\u00b7len", "starck", "be\u00b7kriegt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "NN", "$(", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.320": {"text": "Da Deutschland selber doch in jhm seeltagend ligt.", "tokens": ["Da", "Deutschland", "sel\u00b7ber", "doch", "in", "jhm", "seel\u00b7ta\u00b7gend", "ligt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADV", "ADV", "APPR", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.321": {"text": "Er war in Franckreich lieb/ dem K\u00e4yserthum entgegen/", "tokens": ["Er", "war", "in", "Fran\u00b7ck\u00b7reich", "lieb", "/", "dem", "K\u00e4y\u00b7ser\u00b7thum", "ent\u00b7ge\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NE", "ADJD", "$(", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.322": {"text": "Er zeigte jenem Hilff und diesem seinen Degen.", "tokens": ["Er", "zeig\u00b7te", "je\u00b7nem", "Hilff", "und", "die\u00b7sem", "sei\u00b7nen", "De\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "KON", "PDAT", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.323": {"text": "Er starb auf seinen Sieg von Brysach/ Er bestritt\u2019", "tokens": ["Er", "starb", "auf", "sei\u00b7nen", "Sieg", "von", "Bry\u00b7sach", "/", "Er", "be\u00b7stritt'"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "APPR", "NE", "$(", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.324": {"text": "und nam allein mit dem die andern alle mit.", "tokens": ["und", "nam", "al\u00b7lein", "mit", "dem", "die", "an\u00b7dern", "al\u00b7le", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PRELS", "ART", "ADJA", "PIS", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.325": {"text": "Er wuste hier nicht mehr Triumffe zu erlangen/", "tokens": ["Er", "wus\u00b7te", "hier", "nicht", "mehr", "Tri\u00b7umf\u00b7fe", "zu", "er\u00b7lan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "PIAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.326": {"text": "Daher er auch von hier zum H\u00f6hern ist gegangen.", "tokens": ["Da\u00b7her", "er", "auch", "von", "hier", "zum", "H\u00f6\u00b7hern", "ist", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADV", "APPR", "ADV", "APPRART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.327": {"text": "Er starb auf seinem Bett\u2019/ als einer/ der mit Macht", "tokens": ["Er", "starb", "auf", "sei\u00b7nem", "Bett'", "/", "als", "ei\u00b7ner", "/", "der", "mit", "Macht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$(", "KOUS", "ART", "$(", "ART", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.328": {"text": "In Schlachten nimmermehr kunt werden unterbracht.", "tokens": ["In", "Schlach\u00b7ten", "nim\u00b7mer\u00b7mehr", "kunt", "wer\u00b7den", "un\u00b7ter\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "ADJD", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.329": {"text": "Er hat vor keinem Feind\u2019/ allein jhm selbst gewichen/", "tokens": ["Er", "hat", "vor", "kei\u00b7nem", "Feind'", "/", "al\u00b7lein", "jhm", "selbst", "ge\u00b7wi\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PIAT", "NN", "$(", "ADV", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "Sein Grab ist rund herum mit Siegen au\u00dfgestrichen.", "tokens": ["Sein", "Grab", "ist", "rund", "he\u00b7rum", "mit", "Sie\u00b7gen", "au\u00df\u00b7ge\u00b7stri\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "PTKVZ", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Setzt Lorbeer-Kr\u00e4ntz hierauf/ thut die Cypressen ab/", "tokens": ["Setzt", "Lor\u00b7beer\u00b7Kr\u00e4ntz", "hier\u00b7auf", "/", "thut", "die", "Cyp\u00b7res\u00b7sen", "ab", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "PAV", "$(", "VVFIN", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.332": {"text": "Hier ist des Siegers Ort/ und kein bekl\u00e4glich Grab.", "tokens": ["Hier", "ist", "des", "Sie\u00b7gers", "Ort", "/", "und", "kein", "be\u00b7kl\u00e4g\u00b7lich", "Grab", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "NN", "$(", "KON", "PIAT", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.333": {"text": "Wil jemand aber viel mit wenig Worten sagen/", "tokens": ["Wil", "je\u00b7mand", "a\u00b7ber", "viel", "mit", "we\u00b7nig", "Wor\u00b7ten", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ADV", "ADV", "APPR", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Der spr\u00e4che so: Hier liegt annoch von jungen Tagen", "tokens": ["Der", "spr\u00e4\u00b7che", "so", ":", "Hier", "liegt", "an\u00b7noch", "von", "jun\u00b7gen", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "$.", "ADV", "VVFIN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.335": {"text": "Der gro\u00dfe Weymar-F\u00fcrst in einer engen Grufft/", "tokens": ["Der", "gro\u00b7\u00dfe", "Wey\u00b7ma\u00b7rF\u00fcrst", "in", "ei\u00b7ner", "en\u00b7gen", "Grufft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.336": {"text": "Vom Fieber weggerafft/ Sein Lob hegt alle Lufft.", "tokens": ["Vom", "Fie\u00b7ber", "weg\u00b7ge\u00b7rafft", "/", "Sein", "Lob", "hegt", "al\u00b7le", "Lufft", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$(", "PPOSAT", "NN", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "Er ruhet von dem Streit und triumfiert im sterben/", "tokens": ["Er", "ru\u00b7het", "von", "dem", "Streit", "und", "tri\u00b7um\u00b7fiert", "im", "ster\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "KON", "VVFIN", "APPRART", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.338": {"text": "Es kan der Rauten Glantz durch keinen Frost verderben.", "tokens": ["Es", "kan", "der", "Rau\u00b7ten", "Glantz", "durch", "kei\u00b7nen", "Frost", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.339": {"text": "Seht dieses war die Schrifft um dieses Helden Grab.", "tokens": ["Seht", "die\u00b7ses", "war", "die", "Schrifft", "um", "die\u00b7ses", "Hel\u00b7den", "Grab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "VAFIN", "ART", "NN", "APPR", "PDAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "Sein Tod setzt meine Hand von jhrem schreiben ab.", "tokens": ["Sein", "Tod", "setzt", "mei\u00b7ne", "Hand", "von", "jhrem", "schrei\u00b7ben", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPOSAT", "NN", "APPR", "PPOSAT", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}}}}