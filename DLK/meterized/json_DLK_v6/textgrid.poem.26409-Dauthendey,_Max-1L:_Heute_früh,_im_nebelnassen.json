{"textgrid.poem.26409": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "1L: Heute fr\u00fch, im nebelnassen", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Heute fr\u00fch, im nebelnassen", "tokens": ["Heu\u00b7te", "fr\u00fch", ",", "im", "ne\u00b7bel\u00b7nas\u00b7sen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "$,", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Morgen r\u00fcckten durch die Gassen", "tokens": ["Mor\u00b7gen", "r\u00fcck\u00b7ten", "durch", "die", "Gas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Heulende Fleischklumpen an.", "tokens": ["Heu\u00b7len\u00b7de", "Fleischklum\u00b7pen", "an", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.4": {"text": "Menschen man nicht sagen kann.", "tokens": ["Men\u00b7schen", "man", "nicht", "sa\u00b7gen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Eine w\u00fcste Bettlerherde", "tokens": ["Ei\u00b7ne", "w\u00fcs\u00b7te", "Bett\u00b7ler\u00b7her\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schwoll an wie der Schlamm der Erde,", "tokens": ["Schwoll", "an", "wie", "der", "Schlamm", "der", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PTKVZ", "KOKOM", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Als ob reif, mit einem Schrei,", "tokens": ["Als", "ob", "reif", ",", "mit", "ei\u00b7nem", "Schrei", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ADJD", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ein Geschw\u00fcr geborsten sei.", "tokens": ["Ein", "Ge\u00b7schw\u00fcr", "ge\u00b7bors\u00b7ten", "sei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Und die eleganten Stra\u00dfen", "tokens": ["Und", "die", "e\u00b7leg\u00b7an\u00b7ten", "Stra\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schienen kaum den Kot zu fassen.", "tokens": ["Schie\u00b7nen", "kaum", "den", "Kot", "zu", "fas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Der Gesch\u00e4ftsgang stockte still.", "tokens": ["Der", "Ge\u00b7sch\u00e4fts\u00b7gang", "stock\u00b7te", "still", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Jeder fragte, was das will.", "tokens": ["Je\u00b7der", "frag\u00b7te", ",", "was", "das", "will", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PWS", "PDS", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Tausend M\u00e4uler voll Gegreine,", "tokens": ["Tau\u00b7send", "M\u00e4u\u00b7ler", "voll", "Ge\u00b7grei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Menschenstummel, ohne Beine,", "tokens": ["Men\u00b7schen\u00b7stum\u00b7mel", ",", "oh\u00b7ne", "Bei\u00b7ne", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "KOUI", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schoben, karrten sich heran.", "tokens": ["Scho\u00b7ben", ",", "karr\u00b7ten", "sich", "he\u00b7ran", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "PRF", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie ein Lumpenberg kam's an.", "tokens": ["Wie", "ein", "Lum\u00b7pen\u00b7berg", "kam's", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Vor dem Polizeigeb\u00e4ude", "tokens": ["Vor", "dem", "Po\u00b7li\u00b7zei\u00b7ge\u00b7b\u00e4u\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sammelte sich an die Meute,", "tokens": ["Sam\u00b7mel\u00b7te", "sich", "an", "die", "Meu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Unheimlich und trauervoll,", "tokens": ["Un\u00b7heim\u00b7lich", "und", "trau\u00b7er\u00b7voll", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Keiner wu\u00dfte, was das soll.", "tokens": ["Kei\u00b7ner", "wu\u00df\u00b7te", ",", "was", "das", "soll", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "PWS", "PDS", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Drunter die D\u00e4monenalten", "tokens": ["Drun\u00b7ter", "die", "D\u00e4\u00b7mo\u00b7nen\u00b7al\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["PAV", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schienen stutzig Rat zu halten,", "tokens": ["Schie\u00b7nen", "stut\u00b7zig", "Rat", "zu", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Fragten endlich, scheu versteckt,", "tokens": ["Frag\u00b7ten", "end\u00b7lich", ",", "scheu", "ver\u00b7steckt", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach dem Polizeipr\u00e4fekt.", "tokens": ["Nach", "dem", "Po\u00b7li\u00b7zei\u00b7pr\u00e4\u00b7fekt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Schwierig sie die Rede bauten,", "tokens": ["Schwie\u00b7rig", "sie", "die", "Re\u00b7de", "bau\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die sie erst wie Speichel kauten.", "tokens": ["Die", "sie", "erst", "wie", "Spei\u00b7chel", "kau\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "KOKOM", "NE", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nur das Eine wurde klar:", "tokens": ["Nur", "das", "Ei\u00b7ne", "wur\u00b7de", "klar", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "PIS", "VAFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Irgendwo 'ne Jungfrau war.", "tokens": ["Ir\u00b7gend\u00b7wo", "'ne", "Jung\u00b7frau", "war", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Von der Jungfrau war die Sprache", "tokens": ["Von", "der", "Jung\u00b7frau", "war", "die", "Spra\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und von einer M\u00f6rdersache.", "tokens": ["Und", "von", "ei\u00b7ner", "M\u00f6r\u00b7der\u00b7sa\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Endlich schrie sich Jemand rot:", "tokens": ["End\u00b7lich", "schrie", "sich", "Je\u00b7mand", "rot", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "PIS", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00bbunsere Jungfrau, die ist tot!", "tokens": ["\u00bb", "un\u00b7se\u00b7re", "Jung\u00b7frau", ",", "die", "ist", "tot", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PRELS", "VAFIN", "ADJD", "$."], "meter": "+--+---+", "measure": "dactylic.di.plus"}}, "stanza.9": {"line.1": {"text": "Tot ist sie, tot ist Fifine!\u00ab", "tokens": ["Tot", "ist", "sie", ",", "tot", "ist", "Fi\u00b7fi\u00b7ne", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VAFIN", "PPER", "$,", "ADJD", "VAFIN", "NE", "$.", "$("], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Tausend schrien's mit ", "tokens": ["Tau\u00b7send", "schri\u00b7en's", "mit"], "token_info": ["word", "word", "word"], "pos": ["CARD", "ADJA", "APPR"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Ein Geheule w\u00fcst entstand,", "tokens": ["Ein", "Ge\u00b7heu\u00b7le", "w\u00fcst", "ent\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Als k\u00e4m's Weltend' in das Land.", "tokens": ["Als", "k\u00e4\u00b7m's", "Wel\u00b7tend'", "in", "das", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Weiter tat sich's klar dann machen:", "tokens": ["Wei\u00b7ter", "tat", "sich's", "klar", "dann", "ma\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ADJD", "ADV", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gestern konnt' Fifin' noch lachen,", "tokens": ["Ge\u00b7stern", "konnt'", "Fi\u00b7fin'", "noch", "la\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "NE", "ADV", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Heute fr\u00fch liegt Fifin' tot,", "tokens": ["Heu\u00b7te", "fr\u00fch", "liegt", "Fi\u00b7fin'", "tot", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "NE", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Grauerw\u00fcrgt im Morgenrot.", "tokens": ["Grau\u00b7er\u00b7w\u00fcrgt", "im", "Mor\u00b7gen\u00b7rot", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Die Fifin', die eine, ihre", "tokens": ["Die", "Fi\u00b7fin'", ",", "die", "ei\u00b7ne", ",", "ih\u00b7re"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "$,", "PPOSAT"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Bettlerk\u00f6chin im Quartiere,", "tokens": ["Bett\u00b7ler\u00b7k\u00f6\u00b7chin", "im", "Quar\u00b7tie\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Sie, die in der Gark\u00fcch' stand", "tokens": ["Sie", ",", "die", "in", "der", "Gar\u00b7k\u00fc\u00b7ch'", "stand"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Von dem Betteleiverband.", "tokens": ["Von", "dem", "Bet\u00b7te\u00b7lei\u00b7ver\u00b7band", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Unsterblich ist sie gewesen,", "tokens": ["U\u00b7nsterb\u00b7lich", "ist", "sie", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "VAPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.2": {"text": "Nicht nur durch gekochtes Essen,", "tokens": ["Nicht", "nur", "durch", "ge\u00b7koch\u00b7tes", "Es\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Mehr noch durch die Jugendkraft", "tokens": ["Mehr", "noch", "durch", "die", "Ju\u00b7gend\u00b7kraft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und die holde Jungfraunschaft.", "tokens": ["Und", "die", "hol\u00b7de", "Jung\u00b7fraun\u00b7schaft", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Fifin' war am heutgen Tage", "tokens": ["Fi\u00b7fin'", "war", "am", "heut\u00b7gen", "Ta\u00b7ge"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unbedingte Lebensfrage.", "tokens": ["Un\u00b7be\u00b7ding\u00b7te", "Le\u00b7bens\u00b7fra\u00b7ge", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Soviel ward allm\u00e4hlich klar,", "tokens": ["So\u00b7viel", "ward", "all\u00b7m\u00e4h\u00b7lich", "klar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Da\u00df ihr Tod nicht richtig war.", "tokens": ["Da\u00df", "ihr", "Tod", "nicht", "rich\u00b7tig", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKNEG", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Noch mu\u00dft' man den M\u00f6rder missen.", "tokens": ["Noch", "mu\u00dft'", "man", "den", "M\u00f6r\u00b7der", "mis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wer hat sie auf dem Gewissen?", "tokens": ["Wer", "hat", "sie", "auf", "dem", "Ge\u00b7wis\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und es sprach sich scheu herum:", "tokens": ["Und", "es", "sprach", "sich", "scheu", "he\u00b7rum", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PRF", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Unger\u00e4cht geht sie jetzt um.", "tokens": ["Un\u00b7ge\u00b7r\u00e4cht", "geht", "sie", "jetzt", "um", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Keiner konnt' den M\u00f6rder raten,", "tokens": ["Kei\u00b7ner", "konnt'", "den", "M\u00f6r\u00b7der", "ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Selbst nicht Polizeisoldaten.", "tokens": ["Selbst", "nicht", "Po\u00b7li\u00b7zei\u00b7sol\u00b7da\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und man murrte schon darob,", "tokens": ["Und", "man", "murr\u00b7te", "schon", "da\u00b7rob", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "PAV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Als sich Jemand vorw\u00e4rtsschob.", "tokens": ["Als", "sich", "Je\u00b7mand", "vor\u00b7w\u00e4rts\u00b7schob", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "PIS", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "\u00dcbern\u00e4chtig in den Haaren,", "tokens": ["\u00dc\u00b7bern\u00b7\u00e4ch\u00b7tig", "in", "den", "Haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dr\u00e4ngte Jemand durch die Scharen.", "tokens": ["Dr\u00e4ng\u00b7te", "Je\u00b7mand", "durch", "die", "Scha\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch sein Inneres, verst\u00f6rt,", "tokens": ["Auch", "sein", "In\u00b7ne\u00b7res", ",", "ver\u00b7st\u00f6rt", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "$,", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "War, als ob's ihm nicht geh\u00f6rt.", "tokens": ["War", ",", "als", "ob's", "ihm", "nicht", "ge\u00b7h\u00f6rt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "NE", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Seine Arme, die erschreckten,", "tokens": ["Sei\u00b7ne", "Ar\u00b7me", ",", "die", "er\u00b7schreck\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "H\u00e4ngten sich um den Pr\u00e4fekten.", "tokens": ["H\u00e4ng\u00b7ten", "sich", "um", "den", "Pr\u00e4\u00b7fek\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Schreiend klappt er in die Knie:", "tokens": ["Schrei\u00b7end", "klappt", "er", "in", "die", "Knie", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Unschuldig w\u00e4r' er wie nie!", "tokens": ["Un\u00b7schul\u00b7dig", "w\u00e4r'", "er", "wie", "nie", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "KOKOM", "ADV", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.18": {"line.1": {"text": "Aufrichtig mit ganzer Miene", "tokens": ["Auf\u00b7rich\u00b7tig", "mit", "gan\u00b7zer", "Mie\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ADJA", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Schwur der M\u00f6rder von Fifine:", "tokens": ["Schwur", "der", "M\u00f6r\u00b7der", "von", "Fi\u00b7fi\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "NE", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbniemand hat den Mord gemacht.", "tokens": ["\u00bb", "nie\u00b7mand", "hat", "den", "Mord", "ge\u00b7macht", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Liebe hat sie umgebracht.\u00ab", "tokens": ["Lie\u00b7be", "hat", "sie", "um\u00b7ge\u00b7bracht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VAFIN", "PPER", "VVPP", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.19": {"line.1": {"text": "Alle sollten es nur wissen:", "tokens": ["Al\u00b7le", "soll\u00b7ten", "es", "nur", "wis\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Niemand hat sie am Gewissen,", "tokens": ["Nie\u00b7mand", "hat", "sie", "am", "Ge\u00b7wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Tot lag sie mit stummem Mund", "tokens": ["Tot", "lag", "sie", "mit", "stum\u00b7mem", "Mund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Schon in erster Morgenstund'.", "tokens": ["Schon", "in", "ers\u00b7ter", "Mor\u00b7gen\u00b7stund'", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.20": {"line.1": {"text": "N\u00e4mlich Jeden lie\u00df sie schwitzen,", "tokens": ["N\u00e4m\u00b7lich", "Je\u00b7den", "lie\u00df", "sie", "schwit\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "VVFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jeder wollte sie besitzen.", "tokens": ["Je\u00b7der", "woll\u00b7te", "sie", "be\u00b7sit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Alle diese Bettelleut'", "tokens": ["Al\u00b7le", "die\u00b7se", "Bet\u00b7tel\u00b7leut'"], "token_info": ["word", "word", "word"], "pos": ["PIS", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hatten sich um sie gebl\u00e4ut.", "tokens": ["Hat\u00b7ten", "sich", "um", "sie", "ge\u00b7bl\u00e4ut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "APPR", "PPER", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.21": {"line.1": {"text": "Endlich mu\u00dfte man sich einen:", "tokens": ["End\u00b7lich", "mu\u00df\u00b7te", "man", "sich", "ei\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PRF", "ART", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Lieben sollt' Fifine Keinen,", "tokens": ["Lie\u00b7ben", "sollt'", "Fi\u00b7fi\u00b7ne", "Kei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "PIAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jungfrau bleiben, vorderhand,", "tokens": ["Jung\u00b7frau", "blei\u00b7ben", ",", "vor\u00b7der\u00b7hand", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["NN", "VVINF", "$,", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr den Betteleiverband.", "tokens": ["F\u00fcr", "den", "Bet\u00b7te\u00b7lei\u00b7ver\u00b7band", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.22": {"line.1": {"text": "\u00bbdoch ich kann's nicht mehr verhehlen,", "tokens": ["\u00bb", "doch", "ich", "kann's", "nicht", "mehr", "ver\u00b7heh\u00b7len", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "PPER", "VMFIN", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Heimlich tat sie sich verm\u00e4hlen,", "tokens": ["Heim\u00b7lich", "tat", "sie", "sich", "ver\u00b7m\u00e4h\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und ich bin der Jungfrau Mann,", "tokens": ["Und", "ich", "bin", "der", "Jung\u00b7frau", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Der sie nur beweinen kann.", "tokens": ["Der", "sie", "nur", "be\u00b7wei\u00b7nen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Gestern traute uns ein Pater.", "tokens": ["Ge\u00b7stern", "trau\u00b7te", "uns", "ein", "Pa\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Abends war'n wir im Theater.", "tokens": ["A\u00b7bends", "wa\u00b7r'n", "wir", "im", "The\u00b7a\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Wundersch\u00f6n war es darin,", "tokens": ["Wun\u00b7der\u00b7sch\u00f6n", "war", "es", "da\u00b7rin", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "PAV", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": "Wollten heute wieder hin.", "tokens": ["Woll\u00b7ten", "heu\u00b7te", "wie\u00b7der", "hin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Wohl ist Liebe nicht zum Lachen,", "tokens": ["Wohl", "ist", "Lie\u00b7be", "nicht", "zum", "La\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "PTKNEG", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fifin' nahm zu ernst die Sachen.", "tokens": ["Fi\u00b7fin'", "nahm", "zu", "ernst", "die", "Sa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKA", "ADJD", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jeden Augenblick sie schwor:", "tokens": ["Je\u00b7den", "Au\u00b7gen\u00b7blick", "sie", "schwor", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "So was Sch\u00f6n's k\u00e4m' nie mehr vor.", "tokens": ["So", "was", "Sch\u00f6n's", "k\u00e4m'", "nie", "mehr", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "NN", "VVFIN", "ADV", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Sonst tat sie nur immer kochen", "tokens": ["Sonst", "tat", "sie", "nur", "im\u00b7mer", "ko\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "VVINF"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Alle Tage, alle Wochen.", "tokens": ["Al\u00b7le", "Ta\u00b7ge", ",", "al\u00b7le", "Wo\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nicht weil sie nach Essen roch,", "tokens": ["Nicht", "weil", "sie", "nach", "Es\u00b7sen", "roch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hielt sie jeder Bettler hoch.", "tokens": ["Hielt", "sie", "je\u00b7der", "Bett\u00b7ler", "hoch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.26": {"line.1": {"text": "Sie stand in der Bettlerk\u00fcchen", "tokens": ["Sie", "stand", "in", "der", "Bett\u00b7ler\u00b7k\u00fc\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "\u00dcber allen den Ger\u00fcchen,", "tokens": ["\u00dc\u00b7ber", "al\u00b7len", "den", "Ge\u00b7r\u00fc\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schon ihr Anblick hat gen\u00e4hrt,", "tokens": ["Schon", "ihr", "An\u00b7blick", "hat", "ge\u00b7n\u00e4hrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Heilig wurde sie erkl\u00e4rt.", "tokens": ["Hei\u00b7lig", "wur\u00b7de", "sie", "er\u00b7kl\u00e4rt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "Reichlich gab sie zum Erbauen", "tokens": ["Reich\u00b7lich", "gab", "sie", "zum", "Er\u00b7bau\u00b7en"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Br\u00fcste, Wangen, sch\u00f6n zu schauen.", "tokens": ["Br\u00fcs\u00b7te", ",", "Wan\u00b7gen", ",", "sch\u00f6n", "zu", "schau\u00b7en", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ADJD", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ich vertiefte mich mit Lust \u2013", "tokens": ["Ich", "ver\u00b7tief\u00b7te", "mich", "mit", "Lust", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Keiner hat das End' gewu\u00dft.", "tokens": ["Kei\u00b7ner", "hat", "das", "End'", "ge\u00b7wu\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "H\u00f6r' sie noch ins Ohr mir sagen:", "tokens": ["H\u00f6r'", "sie", "noch", "ins", "Ohr", "mir", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPRART", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Jede w\u00e4re zu beklagen,", "tokens": ["Je\u00b7de", "w\u00e4\u00b7re", "zu", "be\u00b7kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "VAFIN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die nicht eine Nacht bek\u00e4m' \u2013", "tokens": ["Die", "nicht", "ei\u00b7ne", "Nacht", "be\u00b7k\u00e4m'", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "ART", "NN", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Auch, wenn's b\u00f6s' ein Ende n\u00e4hm.", "tokens": ["Auch", ",", "wenn's", "b\u00f6s'", "ein", "En\u00b7de", "n\u00e4hm", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.29": {"line.1": {"text": "W\u00e4hrend alle Pulse rasen,", "tokens": ["W\u00e4h\u00b7rend", "al\u00b7le", "Pul\u00b7se", "ra\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Rief sie lachend in Extasen:", "tokens": ["Rief", "sie", "la\u00b7chend", "in", "Ex\u00b7ta\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00f6glich ist's, da\u00df ich am Tag", "tokens": ["M\u00f6g\u00b7lich", "ist's", ",", "da\u00df", "ich", "am", "Tag"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "VAFIN", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Jetzt nie wieder kochen mag.\u00ab", "tokens": ["Jetzt", "nie", "wie\u00b7der", "ko\u00b7chen", "mag.", "\u00ab"], "token_info": ["word", "word", "word", "word", "abbreviation", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "NE", "$("], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.30": {"line.1": {"text": "Liebe, die vergoldet Lumpen.", "tokens": ["Lie\u00b7be", ",", "die", "ver\u00b7gol\u00b7det", "Lum\u00b7pen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "VVFIN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fifin' rief's bei Talglichtstumpen:", "tokens": ["Fi\u00b7fin'", "rie\u00b7f's", "bei", "Talg\u00b7licht\u00b7stum\u00b7pen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbk\u00fc\u00dft Du mich, wird's allemal", "tokens": ["\u00bb", "k\u00fc\u00dft", "Du", "mich", ",", "wird's", "al\u00b7le\u00b7mal"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word"], "pos": ["$(", "VVFIN", "PPER", "PRF", "$,", "VAFIN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hell wie im Theatersaal.\u00ab", "tokens": ["Hell", "wie", "im", "The\u00b7a\u00b7ter\u00b7saal", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "KOKOM", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.31": {"line.1": {"text": "\u00bbund kein Ku\u00df ging ihr daneben.", "tokens": ["\u00bb", "und", "kein", "Ku\u00df", "ging", "ihr", "da\u00b7ne\u00b7ben", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "PIAT", "NN", "VVFIN", "PPER", "PAV", "$."], "meter": "--+--+--", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ihr Lieb' mu\u00dft man erleben.", "tokens": ["Ihr", "Lieb'", "mu\u00dft", "man", "er\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "In und um sie, ohne Ma\u00df,", "tokens": ["In", "und", "um", "sie", ",", "oh\u00b7ne", "Ma\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "KON", "APPR", "PPER", "$,", "KOUI", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00dcberall ein Herz ihr sa\u00df.", "tokens": ["\u00dc\u00b7be\u00b7rall", "ein", "Herz", "ihr", "sa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.32": {"line.1": {"text": "Erst im fr\u00fchsten Morgenschlummer", "tokens": ["Erst", "im", "fr\u00fchs\u00b7ten", "Mor\u00b7gen\u00b7schlum\u00b7mer"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "War's, als pre\u00dfte mich ein Kummer.", "tokens": ["Wa\u00b7r's", ",", "als", "pre\u00df\u00b7te", "mich", "ein", "Kum\u00b7mer", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "VVFIN", "PRF", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als ob etwas dr\u00fcckt aufs Dach,", "tokens": ["Als", "ob", "et\u00b7was", "dr\u00fcckt", "aufs", "Dach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PIS", "VVFIN", "APPRART", "NN", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Wurde ich beklommen wach.", "tokens": ["Wur\u00b7de", "ich", "be\u00b7klom\u00b7men", "wach", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.33": {"line.1": {"text": "Schrecken ri\u00df mich fast in Fetzen,", "tokens": ["Schre\u00b7cken", "ri\u00df", "mich", "fast", "in", "Fet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Eiskalt sa\u00df ich im Entsetzen.", "tokens": ["Eis\u00b7kalt", "sa\u00df", "ich", "im", "Ent\u00b7set\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Grau, wie's Licht vom fr\u00fchen Tag,", "tokens": ["Grau", ",", "wie's", "Licht", "vom", "fr\u00fc\u00b7hen", "Tag", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJA", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Fifins Leiche bei mir lag,", "tokens": ["Fi\u00b7fins", "Lei\u00b7che", "bei", "mir", "lag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.34": {"line.1": {"text": "Mit dem L\u00e4cheln ohne Gleichen,", "tokens": ["Mit", "dem", "L\u00e4\u00b7cheln", "oh\u00b7ne", "Glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unwahrscheinlich wie nur Leichen. \u2013", "tokens": ["Un\u00b7wahr\u00b7schein\u00b7lich", "wie", "nur", "Lei\u00b7chen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADJD", "KOKOM", "ADV", "NN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schnell ich einen Ruck mir gab", "tokens": ["Schnell", "ich", "ei\u00b7nen", "Ruck", "mir", "gab"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "PPER", "ART", "NN", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und sprang auf wie aus dem Grab.", "tokens": ["Und", "sprang", "auf", "wie", "aus", "dem", "Grab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "KOKOM", "APPR", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.35": {"line.1": {"text": "Nichts wollt' sich an Fifin' r\u00fchren.", "tokens": ["Nichts", "wollt'", "sich", "an", "Fi\u00b7fin'", "r\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PRF", "APPR", "NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Furchtbar war ihr Tod zu sp\u00fcren.", "tokens": ["Furcht\u00b7bar", "war", "ihr", "Tod", "zu", "sp\u00fc\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "All mein Schrein blieb ohne Zweck \u2013", "tokens": ["All", "mein", "Schrein", "blieb", "oh\u00b7ne", "Zweck", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "VVFIN", "APPR", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Meine F\u00fc\u00dfe rannten weg.", "tokens": ["Mei\u00b7ne", "F\u00fc\u00b7\u00dfe", "rann\u00b7ten", "weg", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.36": {"line.1": {"text": "Und ich lief und kann's nicht nennen,", "tokens": ["Und", "ich", "lief", "und", "kann's", "nicht", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "KON", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie und wohin ich tat rennen.", "tokens": ["Wie", "und", "wo\u00b7hin", "ich", "tat", "ren\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "KON", "PWAV", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Vorw\u00e4rts lief ich ohne Ziel,", "tokens": ["Vor\u00b7w\u00e4rts", "lief", "ich", "oh\u00b7ne", "Ziel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Hinter mir lag's leichenstill.", "tokens": ["Hin\u00b7ter", "mir", "lag's", "lei\u00b7chen\u00b7still", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.37": {"line.1": {"text": "Hab nochs Aug' voll Totenflecken,", "tokens": ["Hab", "nochs", "Aug'", "voll", "To\u00b7ten\u00b7fle\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Seh' die Stadt voll Leichen stecken.", "tokens": ["Seh'", "die", "Stadt", "voll", "Lei\u00b7chen", "ste\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADJD", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und ich bitt' Euch: nehmt mich auf,", "tokens": ["Und", "ich", "bitt'", "Euch", ":", "nehmt", "mich", "auf", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "$.", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df ich nicht ins Wasserlauf'!\u00ab", "tokens": ["Da\u00df", "ich", "nicht", "ins", "Was\u00b7ser\u00b7lauf'", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.38": {"line.1": {"text": "Jenem Mann, der so gesprochen,", "tokens": ["Je\u00b7nem", "Mann", ",", "der", "so", "ge\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "$,", "PRELS", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hing die Kinnlad' wie gebrochen.", "tokens": ["Hing", "die", "Kinn\u00b7lad'", "wie", "ge\u00b7bro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "KOKOM", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Alle sahen es ihm an:", "tokens": ["Al\u00b7le", "sa\u00b7hen", "es", "ihm", "an", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "L\u00fcgen sind da keine dran.", "tokens": ["L\u00fc\u00b7gen", "sind", "da", "kei\u00b7ne", "dran", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "PIAT", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.39": {"line.1": {"text": "Der Pr\u00e4fekt sprach: \u00bbMeine Herren,", "tokens": ["Der", "Pr\u00e4\u00b7fekt", "sprach", ":", "\u00bb", "Mei\u00b7ne", "Her\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Keiner darf sich hier beschweren.", "tokens": ["Kei\u00b7ner", "darf", "sich", "hier", "be\u00b7schwe\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PRF", "ADV", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist das Leben mal vorbei,", "tokens": ["Ist", "das", "Le\u00b7ben", "mal", "vor\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "N\u00fctzt Euch keine Polizei.", "tokens": ["N\u00fctzt", "Euch", "kei\u00b7ne", "Po\u00b7li\u00b7zei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.40": {"line.1": {"text": "Geht jetzt heim, Ihr guten Leute!", "tokens": ["Geht", "jetzt", "heim", ",", "Ihr", "gu\u00b7ten", "Leu\u00b7te", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fifin' ward der Liebe Beute.", "tokens": ["Fi\u00b7fin'", "ward", "der", "Lie\u00b7be", "Beu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ART", "NN", "NN", "$."], "meter": "----+-+-", "measure": "unknown.measure.di"}, "line.3": {"text": "Die Natur 's nicht Jedem gibt,", "tokens": ["Die", "Na\u00b7tur", "'s", "nicht", "Je\u00b7dem", "gibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PTKNEG", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df er so wie Fifin' liebt.\u00ab", "tokens": ["Da\u00df", "er", "so", "wie", "Fi\u00b7fin'", "liebt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "KOKOM", "NE", "VVFIN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.41": {"line.1": {"text": "Und gleich feuchten Brunnensteinen", "tokens": ["Und", "gleich", "feuch\u00b7ten", "Brun\u00b7nen\u00b7stei\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sah man Tausend lautlos weinen.", "tokens": ["Sah", "man", "Tau\u00b7send", "laut\u00b7los", "wei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "NN", "ADJD", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Jedes Herz ward Fifin's Grab,", "tokens": ["Je\u00b7des", "Herz", "ward", "Fi\u00b7fin's", "Grab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "NE", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Jammernd senkt man sie hinab.", "tokens": ["Jam\u00b7mernd", "senkt", "man", "sie", "hin\u00b7ab", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PIS", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.42": {"line.1": {"text": "Heimw\u00e4rts dann die Bettler krochen,", "tokens": ["Heim\u00b7w\u00e4rts", "dann", "die", "Bett\u00b7ler", "kro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Weiterschleppend ihre Knochen.", "tokens": ["Wei\u00b7ter\u00b7schlep\u00b7pend", "ih\u00b7re", "Kno\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber jeder Bettelblick", "tokens": ["A\u00b7ber", "je\u00b7der", "Bet\u00b7tel\u00b7blick"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Trug verkl\u00e4rter sein Geschick.", "tokens": ["Trug", "ver\u00b7kl\u00e4r\u00b7ter", "sein", "Ge\u00b7schick", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.43": {"line.1": {"text": "Stolz gab Jeder Dir zu lesen:", "tokens": ["Stolz", "gab", "Je\u00b7der", "Dir", "zu", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fifin' ist dran schuld gewesen,", "tokens": ["Fi\u00b7fin'", "ist", "dran", "schuld", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PAV", "ADJD", "VAPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Da\u00df man dort, wo's elegant,", "tokens": ["Da\u00df", "man", "dort", ",", "wo's", "e\u00b7le\u00b7gant", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "$,", "PWAV", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "'s Herz gezeigt vom Bettlerstand.", "tokens": ["'s", "Herz", "ge\u00b7zeigt", "vom", "Bett\u00b7ler\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVPP", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Trotz des Stankes und des Schimmel", "tokens": ["Trotz", "des", "Stan\u00b7kes", "und", "des", "Schim\u00b7mel"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat der \u00c4rmste einen Himmel.", "tokens": ["Hat", "der", "\u00c4rms\u00b7te", "ei\u00b7nen", "Him\u00b7mel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Lieb' macht selbst 'ne Bettlerin", "tokens": ["Lieb'", "macht", "selbst", "'ne", "Bett\u00b7le\u00b7rin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ADV", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Zu des Tages K\u00f6nigin.", "tokens": ["Zu", "des", "Ta\u00b7ges", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}