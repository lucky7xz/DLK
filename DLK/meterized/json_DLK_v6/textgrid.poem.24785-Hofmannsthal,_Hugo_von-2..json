{"textgrid.poem.24785": {"metadata": {"author": {"name": "Hofmannsthal, Hugo von", "birth": "N.A.", "death": "N.A."}, "title": "2.", "genre": "verse", "period": "N.A.", "pub_year": 1901, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "War der Himmel tr\u00fcb und schwer,", "tokens": ["War", "der", "Him\u00b7mel", "tr\u00fcb", "und", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Waren einsam wir so sehr,", "tokens": ["Wa\u00b7ren", "ein\u00b7sam", "wir", "so", "sehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "PPER", "ADV", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Voneinander abgeschnitten!", "tokens": ["Von\u00b7ein\u00b7an\u00b7der", "ab\u00b7ge\u00b7schnit\u00b7ten", "!"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Aber das ist nun nicht mehr:", "tokens": ["A\u00b7ber", "das", "ist", "nun", "nicht", "mehr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PTKNEG", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "L\u00fcfte flie\u00dfen hin und her;", "tokens": ["L\u00fcf\u00b7te", "flie\u00b7\u00dfen", "hin", "und", "her", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Und die ganze Welt inmitten", "tokens": ["Und", "die", "gan\u00b7ze", "Welt", "in\u00b7mit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "APPR"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Gl\u00e4nzt, als ob sie gl\u00e4sern w\u00e4r.", "tokens": ["Gl\u00e4nzt", ",", "als", "ob", "sie", "gl\u00e4\u00b7sern", "w\u00e4r", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOKOM", "KOUS", "PPER", "VVINF", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Sterne kamen aufgegangen,", "tokens": ["Ster\u00b7ne", "ka\u00b7men", "auf\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Flimmern mein- und deinen Wangen,", "tokens": ["Flim\u00b7mern", "mein", "und", "dei\u00b7nen", "Wan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "TRUNC", "KON", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und sie wissens auch:", "tokens": ["Und", "sie", "wis\u00b7sens", "auch", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Stark und st\u00e4rker wird ihr Prangen;", "tokens": ["Stark", "und", "st\u00e4r\u00b7ker", "wird", "ihr", "Pran\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Und wir atmen mit Verlangen,", "tokens": ["Und", "wir", "at\u00b7men", "mit", "Ver\u00b7lan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Liegen selig wie gefangen,", "tokens": ["Lie\u00b7gen", "se\u00b7lig", "wie", "ge\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "KOKOM", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Sp\u00fcren eins des andern Hauch.", "tokens": ["Sp\u00fc\u00b7ren", "eins", "des", "an\u00b7dern", "Hauch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}