{"textgrid.poem.53436": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "Betrachtung der unseligen Ewigkeit", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "O Eitle Welt, O kurtze Zeit,", "tokens": ["O", "Eit\u00b7le", "Welt", ",", "O", "kurt\u00b7ze", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "NE", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dort f\u00fcr der langen Ewigkeit,", "tokens": ["Dort", "f\u00fcr", "der", "lan\u00b7gen", "E\u00b7wig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die ich mit nichts weis zu vergleichen,", "tokens": ["Die", "ich", "mit", "nichts", "weis", "zu", "ver\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PIS", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Vnd keine Wei\u00dfheit kan erreichen.", "tokens": ["Vnd", "kei\u00b7ne", "Wei\u00df\u00b7heit", "kan", "er\u00b7rei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein Tr\u00f6pffchen bey der gro\u00dfen See,", "tokens": ["Ein", "Tr\u00f6pffc\u00b7hen", "bey", "der", "gro\u00b7\u00dfen", "See", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Fl\u00f6ckchen itzt bey allem Schnee,", "tokens": ["Ein", "Fl\u00f6ck\u00b7chen", "itzt", "bey", "al\u00b7lem", "Schnee", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "PIS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Sandkorn bey der Erden", "tokens": ["Ein", "Sand\u00b7korn", "bey", "der", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "M\u00f6cht' etwas angesehen werden.", "tokens": ["M\u00f6cht'", "et\u00b7was", "an\u00b7ge\u00b7se\u00b7hen", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Allein auch so viel tausend Jah,", "tokens": ["Al\u00b7lein", "auch", "so", "viel", "tau\u00b7send", "Jah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "CARD", "NN", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Als aller Welt Vieh tr\u00e4get Haarr", "tokens": ["Als", "al\u00b7ler", "Welt", "Vieh", "tr\u00e4\u00b7get", "Haarr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIAT", "NN", "NN", "VVFIN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Fr\u00fchling Gra\u00df, sind nicht zu, nennen,", "tokens": ["Der", "Fr\u00fch\u00b7ling", "Gra\u00df", ",", "sind", "nicht", "zu", ",", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "VAFIN", "PTKNEG", "PTKVZ", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Ziel der Ewigkeit zu kennen.", "tokens": ["Das", "Ziel", "der", "E\u00b7wig\u00b7keit", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Was sind die kurtzen Jahre dann,", "tokens": ["Was", "sind", "die", "kurt\u00b7zen", "Jah\u00b7re", "dann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die hie erreichen mag ein Mann,", "tokens": ["Die", "hie", "er\u00b7rei\u00b7chen", "mag", "ein", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVINF", "VMFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd w\u00fcst' er gleich mit langem Leben", "tokens": ["Vnd", "w\u00fcst'", "er", "gleich", "mit", "lan\u00b7gem", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Methusalem nichts nachzugeben?", "tokens": ["Me\u00b7thu\u00b7sa\u00b7lem", "nichts", "nach\u00b7zu\u00b7ge\u00b7ben", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Nun senckt man so viel tausend ein,", "tokens": ["Nun", "senckt", "man", "so", "viel", "tau\u00b7send", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die lang nicht achtzig-j\u00e4hrig seyn,", "tokens": ["Die", "lang", "nicht", "acht\u00b7zig\u00b7j\u00e4h\u00b7rig", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "PTKNEG", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Stirbt wer von zehnmal sieben Jahren,", "tokens": ["Stirbt", "wer", "von", "zehn\u00b7mal", "sie\u00b7ben", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PWS", "APPR", "ADV", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der ist sehr alt dahin gefahren.", "tokens": ["Der", "ist", "sehr", "alt", "da\u00b7hin", "ge\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Das leugnet keiner, und gleichwol", "tokens": ["Das", "leug\u00b7net", "kei\u00b7ner", ",", "und", "gleich\u00b7wol"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PDS", "VVFIN", "PIS", "$,", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind wir so blind und Thorheit voll,", "tokens": ["Sind", "wir", "so", "blind", "und", "Thor\u00b7heit", "voll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "KON", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df wir die Ewigkeit f\u00fcr allen", "tokens": ["Da\u00df", "wir", "die", "E\u00b7wig\u00b7keit", "f\u00fcr", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "PIAT"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vns lassen also leicht entfallen.", "tokens": ["Vns", "las\u00b7sen", "al\u00b7so", "leicht", "ent\u00b7fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Wir bawen tieff in diese Welt", "tokens": ["Wir", "ba\u00b7wen", "tieff", "in", "die\u00b7se", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd stehn nach Hoheit, Macht und Geld,", "tokens": ["Vnd", "stehn", "nach", "Ho\u00b7heit", ",", "Macht", "und", "Geld", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zucht, Recht und Liebe mu\u00df erkalten", "tokens": ["Zucht", ",", "Recht", "und", "Lie\u00b7be", "mu\u00df", "er\u00b7kal\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "NN", "KON", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd aller Frevel Platz behalten.", "tokens": ["Vnd", "al\u00b7ler", "Fre\u00b7vel", "Platz", "be\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Di\u00df w\u00e4re lang nicht so gemein,", "tokens": ["Di\u00df", "w\u00e4\u00b7re", "lang", "nicht", "so", "ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Fiel uns die Ewigkeit recht ein,", "tokens": ["Fiel", "uns", "die", "E\u00b7wig\u00b7keit", "recht", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie w\u00fcrd' uns bald das Fleisch bet\u00e4uben", "tokens": ["Sie", "w\u00fcrd'", "uns", "bald", "das", "Fleisch", "be\u00b7t\u00e4u\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd ihm den Kitzel wol vertreiben.", "tokens": ["Vnd", "ihm", "den", "Kit\u00b7zel", "wol", "ver\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Sie z\u00fcchtigt unsern geilen Sinn,", "tokens": ["Sie", "z\u00fcch\u00b7tigt", "un\u00b7sern", "gei\u00b7len", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie ist der Sitten Meisterinn,", "tokens": ["Sie", "ist", "der", "Sit\u00b7ten", "Meis\u00b7te\u00b7rinn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie ist der Brechzaum aller L\u00fcste", "tokens": ["Sie", "ist", "der", "Brech\u00b7zaum", "al\u00b7ler", "L\u00fcs\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd macht den Weg zur H\u00f6llen w\u00fcste.", "tokens": ["Vnd", "macht", "den", "Weg", "zur", "H\u00f6l\u00b7len", "w\u00fcs\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Kein W\u00fcterich, der sie zuletzt", "tokens": ["Kein", "W\u00fc\u00b7te\u00b7rich", ",", "der", "sie", "zu\u00b7letzt"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PIAT", "NN", "$,", "PRELS", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihm recht hat in das Hertz gesetzt,", "tokens": ["Ihm", "recht", "hat", "in", "das", "Hertz", "ge\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "War jemals von so harten Sinnen,", "tokens": ["War", "je\u00b7mals", "von", "so", "har\u00b7ten", "Sin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Ewigkeit kunt' ihn gewinnen.", "tokens": ["Die", "E\u00b7wig\u00b7keit", "kunt'", "ihn", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Sie hat f\u00fcr k\u00f6niglichen Pracht", "tokens": ["Sie", "hat", "f\u00fcr", "k\u00f6\u00b7nig\u00b7li\u00b7chen", "Pracht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihn in ein h\u00e4ren Kleid gebracht,", "tokens": ["Ihn", "in", "ein", "h\u00e4\u00b7ren", "Kleid", "ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durst, Hitz' und K\u00e4lt' und andre Plagen", "tokens": ["Durst", ",", "Hitz'", "und", "K\u00e4lt'", "und", "and\u00b7re", "Pla\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "NN", "KON", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der D\u00fcrfftigkeit gelehrt ertragen.", "tokens": ["Der", "D\u00fcr\u00b7ff\u00b7tig\u00b7keit", "ge\u00b7lehrt", "er\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Denn welches wilden Menschen Hertz", "tokens": ["Denn", "wel\u00b7ches", "wil\u00b7den", "Men\u00b7schen", "Hertz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAT", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist so aus hartem Stahl und Ertz,", "tokens": ["Ist", "so", "aus", "har\u00b7tem", "Stahl", "und", "Ertz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der, wann er an die Glut gedencket,", "tokens": ["Der", ",", "wann", "er", "an", "die", "Glut", "ge\u00b7den\u00b7cket", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die ewig brennt, den Sinn nicht lencket?", "tokens": ["Die", "e\u00b7wig", "brennt", ",", "den", "Sinn", "nicht", "len\u00b7cket", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "$,", "ART", "NN", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Der H\u00f6llen-Hencker dreut uns dort", "tokens": ["Der", "H\u00f6l\u00b7len\u00b7Hen\u00b7cker", "dreut", "uns", "dort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Ewigkeit nur Quaal und Mord,", "tokens": ["In", "E\u00b7wig\u00b7keit", "nur", "Qua\u00b7al", "und", "Mord", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Er speyt aus seinem Bauch zusammen", "tokens": ["Er", "speyt", "aus", "sei\u00b7nem", "Bauch", "zu\u00b7sam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Rauch, Nebel, Schwefel, Pech und Flammen.", "tokens": ["Rauch", ",", "Ne\u00b7bel", ",", "Schwe\u00b7fel", ",", "Pech", "und", "Flam\u00b7men", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Die Folterbanck und ihre Pein", "tokens": ["Die", "Fol\u00b7ter\u00b7banck", "und", "ih\u00b7re", "Pein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind dort zu schlecht und zu gemein,", "tokens": ["Sind", "dort", "zu", "schlecht", "und", "zu", "ge\u00b7mein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKA", "ADJD", "KON", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dort ist viel ander Ungeheuer,", "tokens": ["Dort", "ist", "viel", "an\u00b7der", "Un\u00b7ge\u00b7heu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Viel andre Noht, viel ander Feuer.", "tokens": ["Viel", "and\u00b7re", "Noht", ",", "viel", "an\u00b7der", "Feu\u00b7er", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "ADV", "ADJD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Die Finsterni\u00df, die vor der Zeit", "tokens": ["Die", "Fins\u00b7ter\u00b7ni\u00df", ",", "die", "vor", "der", "Zeit"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Egypten schuff so grosses Leid,", "tokens": ["E\u00b7gyp\u00b7ten", "schuff", "so", "gros\u00b7ses", "Leid", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Nachtgespenster und was Schrecken,", "tokens": ["Die", "Nacht\u00b7ge\u00b7spens\u00b7ter", "und", "was", "Schre\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PWS", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Furcht, Gram und Grauen kan erwecken.", "tokens": ["Furcht", ",", "Gram", "und", "Grau\u00b7en", "kan", "er\u00b7we\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Das Wetter das ohne abla\u00df schl\u00e4gt,", "tokens": ["Das", "Wet\u00b7ter", "das", "oh\u00b7ne", "ab\u00b7la\u00df", "schl\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "APPR", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Gifft das Todes-Angst erregt,", "tokens": ["Das", "Gifft", "das", "To\u00b7des\u00b7Angst", "er\u00b7regt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Antiochs Pein, Herodis L\u00e4use,", "tokens": ["An\u00b7tiochs", "Pein", ",", "He\u00b7ro\u00b7dis", "L\u00e4u\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "NE", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die Ratten Popiels, Hattons M\u00e4use,", "tokens": ["Die", "Rat\u00b7ten", "Po\u00b7piels", ",", "Hat\u00b7tons", "M\u00e4u\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "NE", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Was Marter je erdacht Busir,", "tokens": ["Was", "Mar\u00b7ter", "je", "er\u00b7dacht", "Bu\u00b7sir", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "ADV", "VVFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der R\u00f6mer Creutz, Perillen Stier,", "tokens": ["Der", "R\u00f6\u00b7mer", "Creutz", ",", "Pe\u00b7ril\u00b7len", "Stier", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was Hunde Jesabel zerrissen,", "tokens": ["Was", "Hun\u00b7de", "Je\u00b7sa\u00b7bel", "zer\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Was Schlangen Israel gebissen.", "tokens": ["Was", "Schlan\u00b7gen", "Is\u00b7rael", "ge\u00b7bis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "NE", "VVPP", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.18": {"line.1": {"text": "Das h\u00f6chste Leid, das alle Welt", "tokens": ["Das", "h\u00f6chs\u00b7te", "Leid", ",", "das", "al\u00b7le", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr gro\u00df und unertr\u00e4glich h\u00e4lt,", "tokens": ["F\u00fcr", "gro\u00df", "und", "un\u00b7er\u00b7tr\u00e4g\u00b7lich", "h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "KON", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wird beydes einzel und mit Hauffen", "tokens": ["Wird", "bey\u00b7des", "ein\u00b7zel", "und", "mit", "Hauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "ADJD", "KON", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dort \u00fcber uns zusammenlauffen.", "tokens": ["Dort", "\u00fc\u00b7ber", "uns", "zu\u00b7sam\u00b7men\u00b7lauf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Vnd w\u00e4ret dieses Trauer-Spiel", "tokens": ["Vnd", "w\u00e4\u00b7ret", "die\u00b7ses", "Trau\u00b7e\u00b7rSpiel"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ach Ewig und ohn alles Ziel!", "tokens": ["Ach", "E\u00b7wig", "und", "ohn", "al\u00b7les", "Ziel", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "KON", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Tod der sehnlich wird gebeten,", "tokens": ["Der", "Tod", "der", "sehn\u00b7lich", "wird", "ge\u00b7be\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJD", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wird ewig, ewig von uns treten.", "tokens": ["Wird", "e\u00b7wig", ",", "e\u00b7wig", "von", "uns", "tre\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "ADJD", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Es wird dort eines jeden Pein", "tokens": ["Es", "wird", "dort", "ei\u00b7nes", "je\u00b7den", "Pein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des andern und die unsre seyn,", "tokens": ["Des", "an\u00b7dern", "und", "die", "uns\u00b7re", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "KON", "ART", "PPOSAT", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr welcher Angst und blossen Zeichen", "tokens": ["F\u00fcr", "wel\u00b7cher", "Angst", "und", "blos\u00b7sen", "Zei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Man tausendmal wol m\u00f6cht' erbleichen.", "tokens": ["Man", "tau\u00b7send\u00b7mal", "wol", "m\u00f6cht'", "er\u00b7blei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.21": {"line.1": {"text": "Die hochbetr\u00fcbte Melodey,", "tokens": ["Die", "hoch\u00b7be\u00b7tr\u00fcb\u00b7te", "Me\u00b7lo\u00b7dey", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Zetter-, Noht- und Quaal-Geschrey", "tokens": ["Das", "Zet\u00b7ter", ",", "Noht", "und", "Quaa\u00b7lGe\u00b7schrey"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "TRUNC", "$,", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Leidenden wird ewig w\u00e4ren,", "tokens": ["Der", "Lei\u00b7den\u00b7den", "wird", "e\u00b7wig", "w\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd keiner wird daran sich kehren.", "tokens": ["Vnd", "kei\u00b7ner", "wird", "da\u00b7ran", "sich", "keh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PAV", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Bedencket dieses in der Zeit,", "tokens": ["Be\u00b7den\u00b7cket", "die\u00b7ses", "in", "der", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDAT", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd flieht die rohe Sicherheit", "tokens": ["Vnd", "flieht", "die", "ro\u00b7he", "Si\u00b7cher\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die ihr allhie der S\u00fcnden Leben,", "tokens": ["Die", "ihr", "all\u00b7hie", "der", "S\u00fcn\u00b7den", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das ewig t\u00f6dtet, seyd ergeben.", "tokens": ["Das", "e\u00b7wig", "t\u00f6d\u00b7tet", ",", "seyd", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "ADJD", "VVFIN", "$,", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Seht da\u00df ihr in Bereitschaft steht,", "tokens": ["Seht", "da\u00df", "ihr", "in", "Be\u00b7reit\u00b7schaft", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der eiteln Dinge m\u00fcssig geht,", "tokens": ["Der", "ei\u00b7teln", "Din\u00b7ge", "m\u00fcs\u00b7sig", "geht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch wahre Reu euch Gott bequemet", "tokens": ["Durch", "wah\u00b7re", "Reu", "euch", "Gott", "be\u00b7que\u00b7met"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NE", "PPER", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd eures FleischesReitzung z\u00e4hmet.", "tokens": ["Vnd", "eu\u00b7res", "Flei\u00b7sches", "Reit\u00b7zung", "z\u00e4h\u00b7met", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Wir wissen umb die Stunde nicht,", "tokens": ["Wir", "wis\u00b7sen", "umb", "die", "Stun\u00b7de", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn uns der Tod stellt vor Gericht,", "tokens": ["Wenn", "uns", "der", "Tod", "stellt", "vor", "Ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Drumb sollen wir zu allen Zeiten", "tokens": ["Drumb", "sol\u00b7len", "wir", "zu", "al\u00b7len", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VMFIN", "PPER", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vns zu der letzten Fahrt bereiten.", "tokens": ["Vns", "zu", "der", "letz\u00b7ten", "Fahrt", "be\u00b7rei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Ist dann geendet unser Lauff,", "tokens": ["Ist", "dann", "ge\u00b7en\u00b7det", "un\u00b7ser", "Lauff", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Thun sich nur zweene Weg uns auff,", "tokens": ["Thun", "sich", "nur", "zwee\u00b7ne", "Weg", "uns", "auff", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Der breite f\u00fchrt hinab zur Hellen,", "tokens": ["Der", "brei\u00b7te", "f\u00fchrt", "hin\u00b7ab", "zur", "Hel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der schmale zeigt die Himmels-Stellen.", "tokens": ["Der", "schma\u00b7le", "zeigt", "die", "Him\u00b7mels\u00b7Stel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Die ihr allhie in Tr\u00fcbsal schwebt,", "tokens": ["Die", "ihr", "all\u00b7hie", "in", "Tr\u00fcb\u00b7sal", "schwebt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Verachtet, kranck und d\u00fcrfftig lebt,", "tokens": ["Ver\u00b7ach\u00b7tet", ",", "kranck", "und", "d\u00fcr\u00b7ff\u00b7tig", "lebt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "ADJD", "KON", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Seyd froh und hofft nach diesem Leiden", "tokens": ["Seyd", "froh", "und", "hofft", "nach", "die\u00b7sem", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAIMP", "ADJD", "KON", "VVFIN", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die ewig-selig Himmels-Freuden.", "tokens": ["Die", "e\u00b7wig\u00b7se\u00b7lig", "Him\u00b7mels\u00b7Freu\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Was ist es gro\u00df ein zehen Jahr", "tokens": ["Was", "ist", "es", "gro\u00df", "ein", "ze\u00b7hen", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "PPER", "ADJD", "ART", "CARD", "NN"], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Vnd zwantzig leben in Gefahr,", "tokens": ["Vnd", "zwant\u00b7zig", "le\u00b7ben", "in", "Ge\u00b7fahr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd tragen Noht und schmach auff Erden,", "tokens": ["Vnd", "tra\u00b7gen", "Noht", "und", "schmach", "auff", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "KON", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd ewig dort erfreuet werden?", "tokens": ["Vnd", "e\u00b7wig", "dort", "er\u00b7freu\u00b7et", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADV", "VVFIN", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Hie herrschen eine kurtze Zeit", "tokens": ["Hie", "herr\u00b7schen", "ei\u00b7ne", "kurt\u00b7ze", "Zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Trotz und Vngerechtigkeit", "tokens": ["In", "Trotz", "und", "Vn\u00b7ge\u00b7rech\u00b7tig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd wegen seiner b\u00f6sen Thaten", "tokens": ["Vnd", "we\u00b7gen", "sei\u00b7ner", "b\u00f6\u00b7sen", "Tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dort ewig in der H\u00f6llen braten?", "tokens": ["Dort", "e\u00b7wig", "in", "der", "H\u00f6l\u00b7len", "bra\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "O Gott schick deines Creutzes Glut", "tokens": ["O", "Gott", "schick", "dei\u00b7nes", "Creut\u00b7zes", "Glut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "ADJD", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd l\u00e4uter unser Fleisch und Blut,", "tokens": ["Vnd", "l\u00e4u\u00b7ter", "un\u00b7ser", "Fleisch", "und", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Such unsrer Schuld allhie zu lohnen", "tokens": ["Such", "uns\u00b7rer", "Schuld", "all\u00b7hie", "zu", "loh\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "PPOSAT", "NN", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd ewig unser dort zu schonen.", "tokens": ["Vnd", "e\u00b7wig", "un\u00b7ser", "dort", "zu", "scho\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PPOSAT", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}