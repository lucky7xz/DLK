{"dta.poem.2364": {"metadata": {"author": {"name": "Neumark, Georg", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1652", "urn": "urn:nbn:de:kobv:b4-20428-0", "language": ["de:0.99"], "booktitle": "Neumark, Georg: Poetisch- und Musikalisches Lustw\u00e4ldchen. Hamburg, 1652."}, "poem": {"stanza.1": {"line.1": {"text": "Ach m\u00f6cht' ein Jegl icher doch in sich selber gehen/", "tokens": ["Ach", "m\u00f6cht'", "ein", "Jegl", "ic\u00b7her", "doch", "in", "sich", "sel\u00b7ber", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ART", "NE", "PPER", "ADV", "APPR", "PRF", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und in dem Spiegel des Gewissens sich besehen;", "tokens": ["Und", "in", "dem", "Spie\u00b7gel", "des", "Ge\u00b7wis\u00b7sens", "sich", "be\u00b7se\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vielleichte w\u00fcrd\u2019 Er bald", "tokens": ["Viel\u00b7leich\u00b7te", "w\u00fcrd'", "Er", "bald"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Au\u00df seinen eignen Werken/", "tokens": ["Au\u00df", "sei\u00b7nen", "eig\u00b7nen", "Wer\u00b7ken", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Wie sein Gem\u00fchte sey gestalt/", "tokens": ["Wie", "sein", "Ge\u00b7m\u00fch\u00b7te", "sey", "ge\u00b7stalt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit eigner Schande merken.", "tokens": ["Mit", "eig\u00b7ner", "Schan\u00b7de", "mer\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Solt\u2019 Er nur bey sich selbst sein Leben wol erwegen/", "tokens": ["Solt'", "Er", "nur", "bey", "sich", "selbst", "sein", "Le\u00b7ben", "wol", "er\u00b7we\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "APPR", "PRF", "ADV", "PPOSAT", "NN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und setne Schand\u2019 und Ehr\u2019 ein wenig \u00fcberlegen/", "tokens": ["Und", "set\u00b7ne", "Schand'", "und", "Ehr'", "ein", "we\u00b7nig", "\u00fc\u00b7berl\u00b7e\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "KON", "NN", "ART", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So w\u00fcrde mancher Mann/", "tokens": ["So", "w\u00fcr\u00b7de", "man\u00b7cher", "Mann", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Des Be\u00dfren sich bequemen/", "tokens": ["Des", "Be\u00df\u00b7ren", "sich", "be\u00b7que\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADJA", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Und dessen/ was Er vor gethan/", "tokens": ["Und", "des\u00b7sen", "/", "was", "Er", "vor", "ge\u00b7than", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "$(", "PWS", "PPER", "APPR", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich innerlichen sch\u00e4men.", "tokens": ["Sich", "in\u00b7ner\u00b7li\u00b7chen", "sch\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Wer seinen Nebensreund mit Schimpf gern\u2019 schamrohe", "tokens": ["Wer", "sei\u00b7nen", "Ne\u00b7bens\u00b7reund", "mit", "Schimpf", "gern'", "scham\u00b7ro\u00b7he"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPOSAT", "NN", "APPR", "NN", "ADJD", "ADJA"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "machet", "tokens": ["ma\u00b7chet"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Der wird gar offtermals mit Schanden au\u00dfgelachet.", "tokens": ["Der", "wird", "gar", "off\u00b7ter\u00b7mals", "mit", "Schan\u00b7den", "au\u00df\u00b7ge\u00b7la\u00b7chet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wer andre Leute schmeht/", "tokens": ["Wer", "and\u00b7re", "Leu\u00b7te", "schmeht", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Hat offtmals \u00fcber hoffen/", "tokens": ["Hat", "offt\u00b7mals", "\u00fc\u00b7ber", "hof\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wies in gemein den Sp\u00f6ttern geht/", "tokens": ["Wies", "in", "ge\u00b7mein", "den", "Sp\u00f6t\u00b7tern", "geht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJD", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Sich selber recht getroffen.", "tokens": ["Sich", "sel\u00b7ber", "recht", "ge\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Wer/ andre Leute nur zu schimpfen/ sich ergetzet/", "tokens": ["Wer", "/", "and\u00b7re", "Leu\u00b7te", "nur", "zu", "schimp\u00b7fen", "/", "sich", "er\u00b7get\u00b7zet", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "$(", "ADJA", "NN", "ADV", "PTKZU", "VVINF", "$(", "PRF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wird von der klugen Welt den Affen gleich gesch\u00e4tzet/", "tokens": ["Wird", "von", "der", "klu\u00b7gen", "Welt", "den", "Af\u00b7fen", "gleich", "ge\u00b7sch\u00e4t\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "ADJA", "NN", "ART", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der wil gern allezeit/", "tokens": ["Der", "wil", "gern", "al\u00b7le\u00b7zeit", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ADV", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Was Er nur siehet/ gekken/", "tokens": ["Was", "Er", "nur", "sie\u00b7het", "/", "gek\u00b7ken", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "$(", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Da Er doch sein\u2019 Unh\u00f6fligkeit/", "tokens": ["Da", "Er", "doch", "sein'", "Un\u00b7h\u00f6f\u00b7lig\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kan selber nicht bedekken.", "tokens": ["Kan", "sel\u00b7ber", "nicht", "be\u00b7dek\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Ein solches Sch\u00e4ndemaul hat endlich diesen Frommen/", "tokens": ["Ein", "sol\u00b7ches", "Sch\u00e4n\u00b7de\u00b7maul", "hat", "end\u00b7lich", "die\u00b7sen", "From\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VAFIN", "ADV", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df/ wenn ein Ungel\u00fck ist \u00fcber ihn gekommen/", "tokens": ["Da\u00df", "/", "wenn", "ein", "Un\u00b7ge\u00b7l\u00fck", "ist", "\u00fc\u00b7ber", "ihn", "ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$(", "KOUS", "ART", "NN", "VAFIN", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man ihn darzu verlacht/", "tokens": ["Man", "ihn", "dar\u00b7zu", "ver\u00b7lacht", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "PPER", "PAV", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Und g\u00f6nnets ihm von Hertzen/", "tokens": ["Und", "g\u00f6n\u00b7nets", "ihm", "von", "Hert\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Da\u00df man sein Klagen wenig acht/", "tokens": ["Da\u00df", "man", "sein", "Kla\u00b7gen", "we\u00b7nig", "acht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPOSAT", "NN", "ADV", "CARD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und seine bittre Schmertzen.", "tokens": ["Und", "sei\u00b7ne", "bitt\u00b7re", "Schmert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Wenn solch ein b\u00f6ser Mensch wird auch einmal geplaget/", "tokens": ["Wenn", "solch", "ein", "b\u00f6\u00b7ser", "Mensch", "wird", "auch", "ein\u00b7mal", "ge\u00b7pla\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ART", "ADJA", "NN", "VAFIN", "ADV", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mit Unfall hie und da/ dann wird Er schtecht beklaget/", "tokens": ["Mit", "Un\u00b7fall", "hie", "und", "da", "/", "dann", "wird", "Er", "schtecht", "be\u00b7kla\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "KON", "ADV", "$(", "ADV", "VAFIN", "PPER", "VVFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Es d\u00e4nket jederman:", "tokens": ["Es", "d\u00e4n\u00b7ket", "je\u00b7der\u00b7man", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Wie? ist es jene Schlangen", "tokens": ["Wie", "?", "ist", "es", "je\u00b7ne", "Schlan\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "$.", "VAFIN", "PPER", "PDAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Die Manchen so verh\u00f6hnen kan?", "tokens": ["Die", "Man\u00b7chen", "so", "ver\u00b7h\u00f6h\u00b7nen", "kan", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Es ist ihr recht geschehen.", "tokens": ["Es", "ist", "ihr", "recht", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Dr\u00fcm wenn du dir ged\u00e4nkst des Menschen Gunst zu ma-", "tokens": ["Dr\u00fcm", "wenn", "du", "dir", "ge\u00b7d\u00e4nkst", "des", "Men\u00b7schen", "Gunst", "zu", "ma"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "VVFIN", "ART", "NN", "NN", "APPR", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "chen/", "tokens": ["chen", "/"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Und da\u00df man deiner nicht im Ungl\u00fckk m\u00fcsse lachen/", "tokens": ["Und", "da\u00df", "man", "dei\u00b7ner", "nicht", "im", "Un\u00b7gl\u00fckk", "m\u00fcs\u00b7se", "la\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "PPOSAT", "PTKNEG", "APPRART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "So stell dein Schimpfen ein/", "tokens": ["So", "stell", "dein", "Schimp\u00b7fen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPOSAT", "NN", "ART", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "La\u00df andre Leute gehen/", "tokens": ["La\u00df", "and\u00b7re", "Leu\u00b7te", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Las dir dein Hertz zur Tadlung sein", "tokens": ["Las", "dir", "dein", "Hertz", "zur", "Tad\u00b7lung", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "PPER", "PPOSAT", "NN", "APPRART", "NN", "PPOSAT"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Da wirstu gnugsam sehen.", "tokens": ["Da", "wirs\u00b7tu", "gnug\u00b7sam", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}