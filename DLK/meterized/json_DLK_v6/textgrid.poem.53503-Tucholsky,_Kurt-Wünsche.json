{"textgrid.poem.53503": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "W\u00fcnsche", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die gn\u00e4dige Frau ist hell und blond,", "tokens": ["Die", "gn\u00e4\u00b7di\u00b7ge", "Frau", "ist", "hell", "und", "blond", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "von sommerlichem Licht durchsonnt \u2013", "tokens": ["von", "som\u00b7mer\u00b7li\u00b7chem", "Licht", "durch\u00b7sonnt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "sie scheint sich schlechtgeraten.", "tokens": ["sie", "scheint", "sich", "schlecht\u00b7ge\u00b7ra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Braun will sie sein, das dumme Kind,", "tokens": ["Braun", "will", "sie", "sein", ",", "das", "dum\u00b7me", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "VAINF", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "braun, wie Zigeunerweiber sind \u2013", "tokens": ["braun", ",", "wie", "Zi\u00b7geu\u00b7ner\u00b7wei\u00b7ber", "sind", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWAV", "NN", "VAFIN", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "und l\u00e4\u00dft am Strand sich braten.", "tokens": ["und", "l\u00e4\u00dft", "am", "Strand", "sich", "bra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Jung-Deutschlands Dichter gehn zur Zeit", "tokens": ["Jung\u00b7Deut\u00b7schlands", "Dich\u00b7ter", "gehn", "zur", "Zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "NN", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "in Fritz von Schillers Sch\u00fclerkleid \u2013", "tokens": ["in", "Fritz", "von", "Schil\u00b7lers", "Sch\u00fc\u00b7ler\u00b7kleid", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPR", "NE", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "(der war nicht so behende).", "tokens": ["(", "der", "war", "nicht", "so", "be\u00b7hen\u00b7de", ")", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "VAFIN", "PTKNEG", "ADV", "ADJA", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Vom Recken wird man noch nicht gro\u00df;", "tokens": ["Vom", "Re\u00b7cken", "wird", "man", "noch", "nicht", "gro\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PIS", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "bleibt ruhig noch auf Mutterns Scho\u00df:", "tokens": ["bleibt", "ru\u00b7hig", "noch", "auf", "Mut\u00b7terns", "Scho\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADV", "APPR", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "sie hat die kl\u00fcgern H\u00e4nde.", "tokens": ["sie", "hat", "die", "kl\u00fc\u00b7gern", "H\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Alt-Deutschland macht in Politik", "tokens": ["Al\u00b7tDeut\u00b7schland", "macht", "in", "Po\u00b7li\u00b7tik"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und zieht Bilanz aus diesem Krieg:", "tokens": ["und", "zieht", "Bi\u00b7lanz", "aus", "die\u00b7sem", "Krieg", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Indien mu\u00df badisch werden!", "tokens": ["In\u00b7di\u00b7en", "mu\u00df", "ba\u00b7disch", "wer\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "ADJD", "VAINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "\u00c4gypten her! die Ostsee auch!", "tokens": ["\u00c4\u00b7gyp\u00b7ten", "her", "!", "die", "Ost\u00b7see", "auch", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$.", "ART", "NN", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Wir treten alle vor den Bauch", "tokens": ["Wir", "tre\u00b7ten", "al\u00b7le", "vor", "den", "Bauch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "mit sieghaften Geb\u00e4rden!", "tokens": ["mit", "sieg\u00b7haf\u00b7ten", "Ge\u00b7b\u00e4r\u00b7den", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.4": {"line.1": {"text": "Und so hat jeder was zu schrein.", "tokens": ["Und", "so", "hat", "je\u00b7der", "was", "zu", "schrein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PIAT", "PIS", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Neger will ein Wei\u00dfer sein,", "tokens": ["Der", "Ne\u00b7ger", "will", "ein", "Wei\u00b7\u00dfer", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "der Fu\u00dffantrist ein Reiter . . .", "tokens": ["der", "Fu\u00df\u00b7fant\u00b7rist", "ein", "Rei\u00b7ter", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "ART", "NN", "$.", "$.", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wir wollen aufrecht stehn, mein Kind,", "tokens": ["Wir", "wol\u00b7len", "auf\u00b7recht", "stehn", ",", "mein", "Kind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "VVINF", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "und bleiben, was wir selber sind!", "tokens": ["und", "blei\u00b7ben", ",", "was", "wir", "sel\u00b7ber", "sind", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$,", "PRELS", "PPER", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ich glaub, das ist gescheiter.", "tokens": ["Ich", "glaub", ",", "das", "ist", "ge\u00b7schei\u00b7ter", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PDS", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}