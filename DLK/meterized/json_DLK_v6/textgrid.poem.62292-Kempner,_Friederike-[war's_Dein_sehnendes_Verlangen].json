{"textgrid.poem.62292": {"metadata": {"author": {"name": "Kempner, Friederike", "birth": "N.A.", "death": "N.A."}, "title": "[war's Dein sehnendes Verlangen]", "genre": "verse", "period": "N.A.", "pub_year": 1868, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "War's Dein sehnendes Verlangen,", "tokens": ["Wa\u00b7r's", "Dein", "seh\u00b7nen\u00b7des", "Ver\u00b7lan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Deiner Liebe \u00e4ngstlich Bangen,", "tokens": ["Dei\u00b7ner", "Lie\u00b7be", "\u00e4ngst\u00b7lich", "Ban\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Treue Seele, ach,", "tokens": ["Treu\u00b7e", "See\u00b7le", ",", "ach", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["NN", "NN", "$,", "ITJ", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Was das Herz Dir brach?", "tokens": ["Was", "das", "Herz", "Dir", "brach", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "PPER", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Oder ist's ein Gift gewesen,", "tokens": ["O\u00b7der", "ist's", "ein", "Gift", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "VAPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "O so richte Gott die B\u00f6sen,", "tokens": ["O", "so", "rich\u00b7te", "Gott", "die", "B\u00f6\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VVFIN", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Kann unschuldiges Tier daf\u00fcr,", "tokens": ["Kann", "un\u00b7schul\u00b7di\u00b7ges", "Tier", "da\u00b7f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADJA", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcr den dummen Ha\u00df zu mir?", "tokens": ["F\u00fcr", "den", "dum\u00b7men", "Ha\u00df", "zu", "mir", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "APPR", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Schlangengift, es st\u00f6rte Eden \u2013", "tokens": ["Schlan\u00b7gen\u00b7gift", ",", "es", "st\u00f6r\u00b7te", "E\u00b7den", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "VVFIN", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "St\u00f6ret auch das kleinste Gl\u00fcck \u2013", "tokens": ["St\u00f6\u00b7ret", "auch", "das", "kleins\u00b7te", "Gl\u00fcck", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Warnen m\u00f6cht' ich endlich jeden,", "tokens": ["War\u00b7nen", "m\u00f6cht'", "ich", "end\u00b7lich", "je\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "ADV", "PIAT", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Lasset nie ein Tier zur\u00fcck \u2013", "tokens": ["Las\u00b7set", "nie", "ein", "Tier", "zu\u00b7r\u00fcck", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "PTKVZ", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn Strichnin braucht nicht zu reden,", "tokens": ["Denn", "Strich\u00b7nin", "braucht", "nicht", "zu", "re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "T\u00f6tet wie ein Schlangenblick! \u2013", "tokens": ["T\u00f6\u00b7tet", "wie", "ein", "Schlan\u00b7gen\u00b7blick", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}