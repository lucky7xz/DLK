{"textgrid.poem.40232": {"metadata": {"author": {"name": "Dehmel, Richard Fedor Leopold", "birth": "N.A.", "death": "N.A."}, "title": "Zu eng", "genre": "verse", "period": "N.A.", "pub_year": 1891, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Vier Treppen hoch, nach Hinten hinaus:", "tokens": ["Vier", "Trep\u00b7pen", "hoch", ",", "nach", "Hin\u00b7ten", "hin\u00b7aus", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "$,", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "ein hundertfenstriges Vorstadthaus.", "tokens": ["ein", "hun\u00b7dert\u00b7fen\u00b7stri\u00b7ges", "Vor\u00b7stadt\u00b7haus", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Kammer schmal", "tokens": ["Die", "Kam\u00b7mer", "schmal"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "und niedrig und kahl;", "tokens": ["und", "nied\u00b7rig", "und", "kahl", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "ein rissiger Spiegel, zerschlissen ein Bette,", "tokens": ["ein", "ris\u00b7si\u00b7ger", "Spie\u00b7gel", ",", "zer\u00b7schlis\u00b7sen", "ein", "Bet\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.6": {"text": "ein Wassernapf; kein Stuhl, kein Tisch;", "tokens": ["ein", "Was\u00b7ser\u00b7napf", ";", "kein", "Stuhl", ",", "kein", "Tisch", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PIAT", "NN", "$,", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "und von den W\u00e4nden gl\u00e4nzte frisch", "tokens": ["und", "von", "den", "W\u00e4n\u00b7den", "gl\u00e4nz\u00b7te", "frisch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "der Armut schimmlichte Tapete.", "tokens": ["der", "Ar\u00b7mut", "schimm\u00b7lich\u00b7te", "Ta\u00b7pe\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$."], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "Kaum konnt' ich durch die Th\u00fcr und kaum", "tokens": ["Kaum", "konnt'", "ich", "durch", "die", "Th\u00fcr", "und", "kaum"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "ART", "NN", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "mich drinnen bewegen: so f\u00fcllte den Raum", "tokens": ["mich", "drin\u00b7nen", "be\u00b7we\u00b7gen", ":", "so", "f\u00fcll\u00b7te", "den", "Raum"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVINF", "$.", "ADV", "VVFIN", "ART", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.11": {"text": "ein plumper Sarg, schmucklos und roh,", "tokens": ["ein", "plum\u00b7per", "Sarg", ",", "schmuck\u00b7los", "und", "roh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "ein Armensarg. Und auf dem Stroh", "tokens": ["ein", "Ar\u00b7men\u00b7sarg", ".", "Und", "auf", "dem", "Stroh"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$.", "KON", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "des Bettes sa\u00df ein magrer Mann,", "tokens": ["des", "Bet\u00b7tes", "sa\u00df", "ein", "mag\u00b7rer", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "noch jung, allein mit jenen alten Z\u00fcgen,", "tokens": ["noch", "jung", ",", "al\u00b7lein", "mit", "je\u00b7nen", "al\u00b7ten", "Z\u00fc\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "mit denen Gram und Not die Zeit ", "tokens": ["mit", "de\u00b7nen", "Gram", "und", "Not", "die", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "NN", "KON", "NN", "ART", "NN"], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.2": {"line.1": {"text": "Ich gr\u00fc\u00dfte scheu. Er sah mich an", "tokens": ["Ich", "gr\u00fc\u00df\u00b7te", "scheu", ".", "Er", "sah", "mich", "an"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "$.", "PPER", "VVFIN", "PPER", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "und nickte stumpf", "tokens": ["und", "nick\u00b7te", "stumpf"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "und seufzte dumpf", "tokens": ["und", "seufz\u00b7te", "dumpf"], "token_info": ["word", "word", "word"], "pos": ["KON", "VVFIN", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "und stierte br\u00fctend wieder hin", "tokens": ["und", "stier\u00b7te", "br\u00fc\u00b7tend", "wie\u00b7der", "hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADJD", "ADV", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "aus hohlem Aug' zum offnen Sarg,", "tokens": ["aus", "hoh\u00b7lem", "Aug'", "zum", "off\u00b7nen", "Sarg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "in den ich gestern mit ihm barg", "tokens": ["in", "den", "ich", "ge\u00b7stern", "mit", "ihm", "barg"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "ADV", "APPR", "PPER", "VVFIN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.7": {"text": "die tote Curbelstepperin,", "tokens": ["die", "to\u00b7te", "Cur\u00b7bels\u00b7tep\u00b7pe\u00b7rin", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "ihr totes Kind im welken Arm.", "tokens": ["ihr", "to\u00b7tes", "Kind", "im", "wel\u00b7ken", "Arm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Mich peinigte sein starrer Harm;", "tokens": ["Mich", "pei\u00b7nig\u00b7te", "sein", "star\u00b7rer", "Harm", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "drum nahm ich ihn fast grob am Kragen", "tokens": ["drum", "nahm", "ich", "ihn", "fast", "grob", "am", "Kra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "ADV", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "und habe derb ihn angesprochen,", "tokens": ["und", "ha\u00b7be", "derb", "ihn", "an\u00b7ge\u00b7spro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "er solle erz\u00e4hlen, sein Leid mir ", "tokens": ["er", "sol\u00b7le", "er\u00b7z\u00e4h\u00b7len", ",", "sein", "Leid", "mir"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "VVINF", "$,", "PPOSAT", "NN", "PPER"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.13": {"text": "nicht sitzen, als h\u00e4tt' er's selbst verbrochen!", "tokens": ["nicht", "sit\u00b7zen", ",", "als", "h\u00e4tt'", "er's", "selbst", "ver\u00b7bro\u00b7chen", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVINF", "$,", "KOKOM", "VAFIN", "PIS", "ADV", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "bis er sich endlich zusammenger\u00fcttet", "tokens": ["bis", "er", "sich", "end\u00b7lich", "zu\u00b7sam\u00b7men\u00b7ge\u00b7r\u00fct\u00b7tet"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "ADV", "VVPP"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.15": {"text": "und seine Seele mir ausgesch\u00fcttet.", "tokens": ["und", "sei\u00b7ne", "See\u00b7le", "mir", "aus\u00b7ge\u00b7sch\u00fct\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "PPER", "VVPP", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbherr Doktor, da ist nicht viel zu erz\u00e4hlen;", "tokens": ["\u00bb", "herr", "Dok\u00b7tor", ",", "da", "ist", "nicht", "viel", "zu", "er\u00b7z\u00e4h\u00b7len", ";"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NN", "NN", "$,", "ADV", "VAFIN", "PTKNEG", "ADV", "PTKZU", "VVINF", "$."], "meter": "++--+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "es war ein einziges, langes Qu\u00e4len.", "tokens": ["es", "war", "ein", "ein\u00b7zi\u00b7ges", ",", "lan\u00b7ges", "Qu\u00e4\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es mag wol bald zwei Jahr her sein,", "tokens": ["Es", "mag", "wol", "bald", "zwei", "Jahr", "her", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "CARD", "NN", "APZR", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "da zogen wir hier Beide ein, \u2013", "tokens": ["da", "zo\u00b7gen", "wir", "hier", "Bei\u00b7de", "ein", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PIS", "PTKVZ", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "das hei\u00dft, noch eh' wir Bekanntschaft gemacht, \u2013", "tokens": ["das", "hei\u00dft", ",", "noch", "eh'", "wir", "Be\u00b7kannt\u00b7schaft", "ge\u00b7macht", ",", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "$,", "ADV", "KOUS", "PPER", "NN", "VVPP", "$,", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Schlafstelle blos, in Aftermiete:", "tokens": ["Schlaf\u00b7stel\u00b7le", "blos", ",", "in", "Af\u00b7ter\u00b7mie\u00b7te", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Sie steppte damals Trauerh\u00fcte", "tokens": ["Sie", "stepp\u00b7te", "da\u00b7mals", "Trau\u00b7er\u00b7h\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "in der Fabrik bis abends Acht", "tokens": ["in", "der", "Fab\u00b7rik", "bis", "a\u00b7bends", "Acht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "ADV", "CARD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "und kam erst gegen Neun nachhaus;", "tokens": ["und", "kam", "erst", "ge\u00b7gen", "Neun", "nach\u00b7haus", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "CARD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "f\u00fcr meinen Fuhrherrn nachts hinaus.", "tokens": ["f\u00fcr", "mei\u00b7nen", "Fuhr\u00b7herrn", "nachts", "hin\u00b7aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "So ging es wol zwei Monat lang;", "tokens": ["So", "ging", "es", "wol", "zwei", "Mo\u00b7nat", "lang", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "CARD", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "wir ", "tokens": ["wir"], "token_info": ["word"], "pos": ["PPER"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Herbst war's; in ihrem d\u00fcnnen Rock", "tokens": ["Herbst", "wa\u00b7r's", ";", "in", "ih\u00b7rem", "d\u00fcn\u00b7nen", "Rock"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "$.", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "und bei dem weiten, nassen Gang \u2013", "tokens": ["und", "bei", "dem", "wei\u00b7ten", ",", "nas\u00b7sen", "Gang", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "$,", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "sie war schon immer zart gewesen \u2013", "tokens": ["sie", "war", "schon", "im\u00b7mer", "zart", "ge\u00b7we\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJD", "VAPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "da hat sie wol was weggekriegt.", "tokens": ["da", "hat", "sie", "wol", "was", "weg\u00b7ge\u00b7kriegt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ja, Herr! da gab's kein Federlesen:", "tokens": ["Ja", ",", "Herr", "!", "da", "gab's", "kein", "Fe\u00b7der\u00b7le\u00b7sen", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$.", "ADV", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Geld hatten wir alle Beide nicht,", "tokens": ["Geld", "hat\u00b7ten", "wir", "al\u00b7le", "Bei\u00b7de", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "PIAT", "PIS", "PTKNEG", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "ihr Bischen blos im Kassenbuch,", "tokens": ["ihr", "Bi\u00b7schen", "blos", "im", "Kas\u00b7sen\u00b7buch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "f\u00fcrs Krankenhaus war sie nicht krank ", "tokens": ["f\u00fcrs", "Kran\u00b7ken\u00b7haus", "war", "sie", "nicht", "krank"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "PTKNEG", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "wir konnten kein ander Gela\u00df uns nehmen,", "tokens": ["wir", "konn\u00b7ten", "kein", "an\u00b7der", "Ge\u00b7la\u00df", "uns", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIAT", "ADJD", "NN", "PPER", "VVINF", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.12": {"text": "wir mu\u00dften uns hier ", "tokens": ["wir", "mu\u00df\u00b7ten", "uns", "hier"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "ADV"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.13": {"text": "bis wieder sie konnte auf Arbeit gehn.", "tokens": ["bis", "wie\u00b7der", "sie", "konn\u00b7te", "auf", "Ar\u00b7beit", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.5": {"line.1": {"text": "Ja, Herr! und da, da ist es ", "tokens": ["Ja", ",", "Herr", "!", "und", "da", ",", "da", "ist", "es"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "NN", "$.", "KON", "ADV", "$,", "ADV", "VAFIN", "PPER"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Wir hielten's ", "tokens": ["Wir", "hiel\u00b7ten's"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "wir beide! man ist ein Mensch doch blos!", "tokens": ["wir", "bei\u00b7de", "!", "man", "ist", "ein", "Mensch", "doch", "blos", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "$.", "PIS", "VAFIN", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "und unsre Sehnsucht war so gro\u00df, \u2013", "tokens": ["und", "uns\u00b7re", "Sehn\u00b7sucht", "war", "so", "gro\u00df", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "wir wohnten ", "tokens": ["wir", "wohn\u00b7ten"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.6": {"line.1": {"text": "Seitdem ist sie mit mir gegangen;", "tokens": ["Seit\u00b7dem", "ist", "sie", "mit", "mir", "ge\u00b7gan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$."], "meter": "-++-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "hat's auch zur Heirat nicht gelangt,", "tokens": ["hat's", "auch", "zur", "Hei\u00b7rat", "nicht", "ge\u00b7langt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "wir haben unserm Sch\u00f6pfer gedankt,", "tokens": ["wir", "ha\u00b7ben", "un\u00b7serm", "Sch\u00f6p\u00b7fer", "ge\u00b7dankt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "da\u00df wir so eben ", "tokens": ["da\u00df", "wir", "so", "e\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Wir thaten unser Lohn in Eines", "tokens": ["Wir", "tha\u00b7ten", "un\u00b7ser", "Lohn", "in", "Ei\u00b7nes"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "und legten noch ", "tokens": ["und", "leg\u00b7ten", "noch"], "token_info": ["word", "word", "word"], "pos": ["KON", "VVFIN", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "So haben wir unsern Weg genommen,", "tokens": ["So", "ha\u00b7ben", "wir", "un\u00b7sern", "Weg", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "ganz gut, \u2013 bis ihre Zeit gekommen.", "tokens": ["ganz", "gut", ",", "\u2013", "bis", "ih\u00b7re", "Zeit", "ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "$(", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Da kam auch die Not. Da half uns kein Beten.", "tokens": ["Da", "kam", "auch", "die", "Not", ".", "Da", "half", "uns", "kein", "Be\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "$.", "ADV", "VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Sie konnte nicht mehr die Maschine treten;", "tokens": ["Sie", "konn\u00b7te", "nicht", "mehr", "die", "Ma\u00b7schi\u00b7ne", "tre\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "was Andres hatte sie nicht gelernt,", "tokens": ["was", "And\u00b7res", "hat\u00b7te", "sie", "nicht", "ge\u00b7lernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VAFIN", "PPER", "PTKNEG", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "die Eltern hatten sie fr\u00fch entfernt.", "tokens": ["die", "El\u00b7tern", "hat\u00b7ten", "sie", "fr\u00fch", "ent\u00b7fernt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.13": {"text": "Nun lebten wir Beide von meinem Lohn,", "tokens": ["Nun", "leb\u00b7ten", "wir", "Bei\u00b7de", "von", "mei\u00b7nem", "Lohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.14": {"text": "und's war f\u00fcr Mich zu knapp fast schon.", "tokens": ["un\u00b7d's", "war", "f\u00fcr", "Mich", "zu", "knapp", "fast", "schon", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPER", "PTKA", "ADJD", "ADV", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.15": {"text": "Was half uns da nun unser Plagen,", "tokens": ["Was", "half", "uns", "da", "nun", "un\u00b7ser", "Pla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "was half uns da nun unser Sparen:", "tokens": ["was", "half", "uns", "da", "nun", "un\u00b7ser", "Spa\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "wir mu\u00dften die Sachen zum Juden tragen!", "tokens": ["wir", "mu\u00df\u00b7ten", "die", "Sa\u00b7chen", "zum", "Ju\u00b7den", "tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.7": {"line.1": {"text": "Ich habe bei Tag und bei Nacht gefahren,", "tokens": ["Ich", "ha\u00b7be", "bei", "Tag", "und", "bei", "Nacht", "ge\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "KON", "APPR", "NN", "VVPP", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "ich hab' mich vor keiner M\u00fche gesch\u00e4mt,", "tokens": ["ich", "hab'", "mich", "vor", "kei\u00b7ner", "M\u00fc\u00b7he", "ge\u00b7sch\u00e4mt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "ich habe mir keinen Schluck mehr bez\u00e4hmt, \u2013", "tokens": ["ich", "ha\u00b7be", "mir", "kei\u00b7nen", "Schluck", "mehr", "be\u00b7z\u00e4hmt", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PIAT", "NN", "ADV", "VVFIN", "$,", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "sie wurde ", "tokens": ["sie", "wur\u00b7de"], "token_info": ["word", "word"], "pos": ["PPER", "VAFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.5": {"text": "sie hat sich zuschanden gedarbt und gegr\u00e4mt, \u2013", "tokens": ["sie", "hat", "sich", "zu\u00b7schan\u00b7den", "ge\u00b7darbt", "und", "ge\u00b7gr\u00e4mt", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PRF", "ADV", "VVPP", "KON", "VVPP", "$,", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.6": {"text": "und dann, dann kam das ", "tokens": ["und", "dann", ",", "dann", "kam", "das"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "ADV", "VVFIN", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "ich sah sie weinen, ich h\u00f6rte es wimmern,", "tokens": ["ich", "sah", "sie", "wei\u00b7nen", ",", "ich", "h\u00f6r\u00b7te", "es", "wim\u00b7mern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VVINF", "$,", "PPER", "VVFIN", "PPER", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "ich sah sie Beide verschmachten, verk\u00fcmmern, \u2013", "tokens": ["ich", "sah", "sie", "Bei\u00b7de", "ver\u00b7schmach\u00b7ten", ",", "ver\u00b7k\u00fcm\u00b7mern", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIS", "VVFIN", "$,", "VVFIN", "$,", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "da hab' ich verloren meine Ruh',", "tokens": ["da", "hab'", "ich", "ver\u00b7lo\u00b7ren", "mei\u00b7ne", "Ruh'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.10": {"text": "da hab' ich zum ersten Mal betrogen,", "tokens": ["da", "hab'", "ich", "zum", "ers\u00b7ten", "Mal", "be\u00b7tro\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "den ersten Fahrgast beim Fahrgeld belogen,", "tokens": ["den", "ers\u00b7ten", "Fahr\u00b7gast", "beim", "Fahr\u00b7geld", "be\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-++-++-+-", "measure": "unknown.measure.hexa"}, "line.12": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.13": {"text": "mir schnitt zusehr ins Herz die Qual, \u2013", "tokens": ["mir", "schnitt", "zu\u00b7sehr", "ins", "Herz", "die", "Qual", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "und Mancher thut's jahrein jahraus,", "tokens": ["und", "Man\u00b7cher", "thut's", "ja\u00b7hrein", "ja\u00b7hraus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "um's beim Budiker zu versaufen,", "tokens": ["um's", "beim", "Bu\u00b7di\u00b7ker", "zu", "ver\u00b7sau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.16": {"text": "und ", "tokens": ["und"], "token_info": ["word"], "pos": ["KON"], "meter": "-", "measure": "single.down"}, "line.17": {"text": "und, Herr \u2013 bei ", "tokens": ["und", ",", "Herr", "\u2013", "bei"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["KON", "$,", "NN", "$(", "APPR"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.8": {"line.1": {"text": "Mir wurde noch von Gl\u00fcck gesagt,", "tokens": ["Mir", "wur\u00b7de", "noch", "von", "Gl\u00fcck", "ge\u00b7sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "da\u00df mich mein Herr blos weggejagt.", "tokens": ["da\u00df", "mich", "mein", "Herr", "blos", "weg\u00b7ge\u00b7jagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr und dem Wurm da gab's den Rest:", "tokens": ["Ihr", "und", "dem", "Wurm", "da", "gab's", "den", "Rest", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "ART", "NN", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "nach Arbeit bin ich in Ost und West", "tokens": ["nach", "Ar\u00b7beit", "bin", "ich", "in", "Ost", "und", "West"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VAFIN", "PPER", "APPR", "NN", "KON", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "seit vierzehn Tagen herumgelungert,", "tokens": ["seit", "vier\u00b7zehn", "Ta\u00b7gen", "her\u00b7um\u00b7ge\u00b7lun\u00b7gert", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "und dabei, scheint's, sind sie verhungert.\u00ab", "tokens": ["und", "da\u00b7bei", ",", "scheint's", ",", "sind", "sie", "ver\u00b7hun\u00b7gert", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PAV", "$,", "VVFIN", "$,", "VAFIN", "PPER", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Er nickte stumpf", "tokens": ["Er", "nick\u00b7te", "stumpf"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "und keuchte dumpf", "tokens": ["und", "keuch\u00b7te", "dumpf"], "token_info": ["word", "word", "word"], "pos": ["KON", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "und glotzte hohlen Augs mich an", "tokens": ["und", "glotz\u00b7te", "hoh\u00b7len", "Augs", "mich", "an"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADJA", "NN", "PPER", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "mit einem Blick so m\u00fcdgehetzt,", "tokens": ["mit", "ei\u00b7nem", "Blick", "so", "m\u00fcd\u00b7ge\u00b7hetzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "so jeder andern Regung ", "tokens": ["so", "je\u00b7der", "an\u00b7dern", "Re\u00b7gung"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "da\u00df mir's den R\u00fccken niederrann.", "tokens": ["da\u00df", "mir's", "den", "R\u00fc\u00b7cken", "nie\u00b7der\u00b7rann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ich hatte, zu tr\u00f6sten, mich hingesetzt", "tokens": ["Ich", "hat\u00b7te", ",", "zu", "tr\u00f6s\u00b7ten", ",", "mich", "hin\u00b7ge\u00b7setzt"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "$,", "PTKZU", "VVINF", "$,", "PPER", "VVPP"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.8": {"text": "und sah, da\u00df Tr\u00f6sten Hohn hier war,", "tokens": ["und", "sah", ",", "da\u00df", "Tr\u00f6s\u00b7ten", "Hohn", "hier", "war", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "ADJA", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "wo also stumm das Elend schrie.", "tokens": ["wo", "al\u00b7so", "stumm", "das", "E\u00b7lend", "schrie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADJD", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Ich dr\u00fcckt' ihm blos das spitze Knie,", "tokens": ["Ich", "dr\u00fcckt'", "ihm", "blos", "das", "spit\u00b7ze", "Knie", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "den d\u00fcnnen Arm und nahm den Hut", "tokens": ["den", "d\u00fcn\u00b7nen", "Arm", "und", "nahm", "den", "Hut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "KON", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "und sagte: \u00bbKommen Sie ", "tokens": ["und", "sag\u00b7te", ":", "\u00bb", "Kom\u00b7men", "Sie"], "token_info": ["word", "word", "punct", "punct", "word", "word"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "ich werde Arbeit f\u00fcr Sie besorgen.\u00ab", "tokens": ["ich", "wer\u00b7de", "Ar\u00b7beit", "f\u00fcr", "Sie", "be\u00b7sor\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "NN", "APPR", "PPER", "VVFIN", "$.", "$("], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.14": {"text": "Er dankte: \u00bbHerr Doktor, Sie meinen's gut;", "tokens": ["Er", "dank\u00b7te", ":", "\u00bb", "Herr", "Dok\u00b7tor", ",", "Sie", "mei\u00b7nen's", "gut", ";"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "NN", "$,", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "ich will auch kommen, und ehrlich mich schinden,", "tokens": ["ich", "will", "auch", "kom\u00b7men", ",", "und", "ehr\u00b7lich", "mich", "schin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VVINF", "$,", "KON", "ADJD", "PPER", "VVFIN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.16": {"text": "und werde auch wol weiterfinden, \u2013", "tokens": ["und", "wer\u00b7de", "auch", "wol", "wei\u00b7ter\u00b7fin\u00b7den", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADV", "VVINF", "$,", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "blos ", "tokens": ["blos"], "token_info": ["word"], "pos": ["ADV"], "meter": "+", "measure": "single.up"}, "line.18": {"text": "Ja, Herr! blos einen kleinen Verschlag,", "tokens": ["Ja", ",", "Herr", "!", "blos", "ei\u00b7nen", "klei\u00b7nen", "Ver\u00b7schlag", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "NN", "$.", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.19": {"text": "ja h\u00e4tten wir blos gehabt ein ", "tokens": ["ja", "h\u00e4t\u00b7ten", "wir", "blos", "ge\u00b7habt", "ein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VAPP", "ART"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.20": {"text": "da\u00df wir nicht immer uns mu\u00dften sehen:", "tokens": ["da\u00df", "wir", "nicht", "im\u00b7mer", "uns", "mu\u00df\u00b7ten", "se\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "PPER", "VMFIN", "VVINF", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.21": {"text": "es w\u00e4re Alles nicht geschehen,", "tokens": ["es", "w\u00e4\u00b7re", "Al\u00b7les", "nicht", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "sie lebten alle Beide noch!", "tokens": ["sie", "leb\u00b7ten", "al\u00b7le", "Bei\u00b7de", "noch", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "PIS", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Wir h\u00e4tten gewartet, wir h\u00e4tten gespart,", "tokens": ["Wir", "h\u00e4t\u00b7ten", "ge\u00b7war\u00b7tet", ",", "wir", "h\u00e4t\u00b7ten", "ge\u00b7spart", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$,", "PPER", "VAFIN", "VVPP", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.24": {"text": "wir waren ja beide von frommer Art, \u2013", "tokens": ["wir", "wa\u00b7ren", "ja", "bei\u00b7de", "von", "from\u00b7mer", "Art", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "APPR", "ADJA", "NN", "$,", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.25": {"text": "wir h\u00e4tten uns selber' ne Droschke geschafft,", "tokens": ["wir", "h\u00e4t\u00b7ten", "uns", "sel\u00b7ber'", "ne", "Droschke", "ge\u00b7schafft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.26": {"text": "dann hatt' ich ja Verdienst die Menge, \u2013", "tokens": ["dann", "hatt'", "ich", "ja", "Ver\u00b7dienst", "die", "Men\u00b7ge", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "NN", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.27": {"text": "so aber ging's uns \u00fcber die Kraft:", "tokens": ["so", "a\u00b7ber", "ging's", "uns", "\u00fc\u00b7ber", "die", "Kraft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.28": {"text": "wir wohnten zu eng zusammen, zu enge!\u00ab", "tokens": ["wir", "wohn\u00b7ten", "zu", "eng", "zu\u00b7sam\u00b7men", ",", "zu", "en\u00b7ge", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKA", "ADJD", "PTKVZ", "$,", "APPR", "ADJA", "$.", "$("], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "Und nach dem Sarge stierte er wieder,", "tokens": ["Und", "nach", "dem", "Sar\u00b7ge", "stier\u00b7te", "er", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "da fuhr ein Zucken ihm durch die Lider:", "tokens": ["da", "fuhr", "ein", "Zu\u00b7cken", "ihm", "durch", "die", "Li\u00b7der", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "\u00bbo wenn ich doch wenigstens ", "tokens": ["\u00bb", "o", "wenn", "ich", "doch", "we\u00b7nigs\u00b7tens"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "FM", "KOUS", "PPER", "ADV", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "dadrinnen in dem engen Kasten!", "tokens": ["dad\u00b7rin\u00b7nen", "in", "dem", "en\u00b7gen", "Kas\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "jetzt ", "tokens": ["jetzt"], "token_info": ["word"], "pos": ["ADV"], "meter": "-", "measure": "single.down"}, "line.6": {"text": "jetzt ist's ihr auch zu ", "tokens": ["jetzt", "ist's", "ihr", "auch", "zu"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPR"], "meter": "--+-+", "measure": "anapaest.init"}, "line.7": {"text": "Er stie\u00df ihn heiser heraus den Witz,", "tokens": ["Er", "stie\u00df", "ihn", "hei\u00b7ser", "he\u00b7raus", "den", "Witz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ADV", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "er wollte lachen vor w\u00fchlendem Weh;", "tokens": ["er", "woll\u00b7te", "la\u00b7chen", "vor", "w\u00fch\u00b7len\u00b7dem", "Weh", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "APPR", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "da ri\u00df es ihn um \u2013 so brach's in die H\u00f6h',", "tokens": ["da", "ri\u00df", "es", "ihn", "um", "\u2013", "so", "brach's", "in", "die", "H\u00f6h'", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "APPR", "$(", "ADV", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "da schmi\u00df es ihn nieder von seinem Sitz,", "tokens": ["da", "schmi\u00df", "es", "ihn", "nie\u00b7der", "von", "sei\u00b7nem", "Sitz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PTKVZ", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.11": {"text": "und weinend warf er sich \u00fcber die Leiche", "tokens": ["und", "wei\u00b7nend", "warf", "er", "sich", "\u00fc\u00b7ber", "die", "Lei\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "PRF", "APPR", "ART", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "und k\u00fc\u00dfte das Antlitz, das abgezehrt bleiche.", "tokens": ["und", "k\u00fc\u00df\u00b7te", "das", "Ant\u00b7litz", ",", "das", "ab\u00b7ge\u00b7zehrt", "blei\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$,", "PRELS", "VVPP", "VVFIN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}}, "stanza.11": {"line.1": {"text": "Da bin ich stille weggegangen,", "tokens": ["Da", "bin", "ich", "stil\u00b7le", "weg\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "mir graute vor der schmalen Kammer,", "tokens": ["mir", "grau\u00b7te", "vor", "der", "schma\u00b7len", "Kam\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "und durch die Brust schlich mir ein Bangen,", "tokens": ["und", "durch", "die", "Brust", "schlich", "mir", "ein", "Ban\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADJD", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "als sei ", "tokens": ["als", "sei"], "token_info": ["word", "word"], "pos": ["KOKOM", "VAFIN"], "meter": "-+", "measure": "iambic.single"}}}}}