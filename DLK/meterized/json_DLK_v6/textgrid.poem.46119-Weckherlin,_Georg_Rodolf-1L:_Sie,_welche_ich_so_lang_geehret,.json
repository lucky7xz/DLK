{"textgrid.poem.46119": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "1L: Sie, welche ich so lang geehret,", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sie, welche ich so lang geehret,", "tokens": ["Sie", ",", "wel\u00b7che", "ich", "so", "lang", "ge\u00b7eh\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "weil ich ihr lieb standhaft gedacht,", "tokens": ["weil", "ich", "ihr", "lieb", "stand\u00b7haft", "ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADJD", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "hat durch ihr untreu ihren pracht", "tokens": ["hat", "durch", "ihr", "un\u00b7treu", "ih\u00b7ren", "pracht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "PPOSAT", "ADJD", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und meiner hofnung freud zerst\u00f6ret.", "tokens": ["und", "mei\u00b7ner", "hof\u00b7nung", "freud", "zer\u00b7st\u00f6\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "\u00bbjedoch gl\u00fcckselig ist die pein,", "tokens": ["\u00bb", "je\u00b7doch", "gl\u00fcck\u00b7se\u00b7lig", "ist", "die", "pein", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ADJD", "VAFIN", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "dadurch ein buhler weis mag sein.\u00ab", "tokens": ["da\u00b7durch", "ein", "buh\u00b7ler", "weis", "mag", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PAV", "ART", "NN", "PTKVZ", "VMFIN", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sie, die ihr angesicht zu feuchten", "tokens": ["Sie", ",", "die", "ihr", "an\u00b7ge\u00b7sicht", "zu", "feuch\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "$,", "PRELS", "PPER", "VVPP", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "nur meinetwegen allzeit schwur,", "tokens": ["nur", "mei\u00b7net\u00b7we\u00b7gen", "all\u00b7zeit", "schwur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "die lasset, als ein andre hur,", "tokens": ["die", "las\u00b7set", ",", "als", "ein", "and\u00b7re", "hur", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "$,", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "f\u00fcr andre ihre augen leuchten.", "tokens": ["f\u00fcr", "and\u00b7re", "ih\u00b7re", "au\u00b7gen", "leuch\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "darum ha\u00df ich auch ihren schein,", "tokens": ["da\u00b7rum", "ha\u00df", "ich", "auch", "ih\u00b7ren", "schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "\u00bbdan sch\u00f6n ist nichts, was zu gemein.\u00ab", "tokens": ["\u00bb", "dan", "sch\u00f6n", "ist", "nichts", ",", "was", "zu", "ge\u00b7mein", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADJD", "VAFIN", "PIS", "$,", "PRELS", "PTKA", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wie oft hat f\u00e4lschlich sie geschworen,", "tokens": ["Wie", "oft", "hat", "f\u00e4lschlich", "sie", "ge\u00b7schwo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "ADJD", "PPER", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "da\u00df ihr herz von betrug ganz frei;", "tokens": ["da\u00df", "ihr", "herz", "von", "be\u00b7trug", "ganz", "frei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPR", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "da doch ihr wort, eid, lieb und treu", "tokens": ["da", "doch", "ihr", "wort", ",", "eid", ",", "lieb", "und", "treu"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "PPOSAT", "NN", "$,", "VVFIN", "$,", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "nu zumal in dem wind verloren.", "tokens": ["nu", "zu\u00b7mal", "in", "dem", "wind", "ver\u00b7lo\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "und jetz bezeuget mein verdru\u00df", "tokens": ["und", "jetz", "be\u00b7zeu\u00b7get", "mein", "ver\u00b7dru\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "ihr gro\u00dfe schand und meine bu\u00df.", "tokens": ["ihr", "gro\u00b7\u00dfe", "schand", "und", "mei\u00b7ne", "bu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVFIN", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Doch mein verdru\u00df kan nicht lang wehren,", "tokens": ["Doch", "mein", "ver\u00b7dru\u00df", "kan", "nicht", "lang", "weh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "weil ihr torechter wankelmut", "tokens": ["weil", "ihr", "to\u00b7rech\u00b7ter", "wan\u00b7kel\u00b7mut"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "erst kam nach \u00fcbergebnem gut", "tokens": ["erst", "kam", "nach", "\u00fc\u00b7ber\u00b7geb\u00b7nem", "gut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "ADJA", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "und nach dem hinflug ihrer ehren.", "tokens": ["und", "nach", "dem", "hin\u00b7flug", "ih\u00b7rer", "eh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "VVFIN", "PPOSAT", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "\u00bbzu spat und umsunst ist die flucht,", "tokens": ["\u00bb", "zu", "spat", "und", "um\u00b7sunst", "ist", "die", "flucht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKZU", "VVFIN", "KON", "ADV", "VAFIN", "ART", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "wan man behaftet mit der sucht.\u00ab", "tokens": ["wan", "man", "be\u00b7haf\u00b7tet", "mit", "der", "sucht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PIS", "VVFIN", "APPR", "ART", "VVFIN", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.5": {"line.1": {"text": "Ich kan zwar und will nicht verneinen,", "tokens": ["Ich", "kan", "zwar", "und", "will", "nicht", "ver\u00b7nei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "KON", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "da\u00df ihr f\u00fcrtrefliche sch\u00f6nheit", "tokens": ["da\u00df", "ihr", "f\u00fcr\u00b7tre\u00b7fli\u00b7che", "sch\u00f6n\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "beraubte mein herz der freiheit", "tokens": ["be\u00b7raub\u00b7te", "mein", "herz", "der", "frei\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "ART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "mit kosen, k\u00fcssen, klagen, weinen:", "tokens": ["mit", "ko\u00b7sen", ",", "k\u00fcs\u00b7sen", ",", "kla\u00b7gen", ",", "wei\u00b7nen", ":"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "nu aber scheidet meine reu,", "tokens": ["nu", "a\u00b7ber", "schei\u00b7det", "mei\u00b7ne", "reu", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPOSAT", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "wie billich, die lieb und untreu.", "tokens": ["wie", "bil\u00b7lich", ",", "die", "lieb", "und", "un\u00b7treu", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "$,", "PRELS", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Ich war ihr herz, ihr trost, ihr leben,", "tokens": ["Ich", "war", "ihr", "herz", ",", "ihr", "trost", ",", "ihr", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "sie war die g\u00f6ttin meiner brust;", "tokens": ["sie", "war", "die", "g\u00f6t\u00b7tin", "mei\u00b7ner", "brust", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "jetz hab ich bei ihr keinen lust,", "tokens": ["jetz", "hab", "ich", "bei", "ihr", "kei\u00b7nen", "lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "PPOSAT", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "will auch ihr keine freud mehr geben.", "tokens": ["will", "auch", "ihr", "kei\u00b7ne", "freud", "mehr", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "PPOSAT", "PIAT", "VVFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "ihr unbestand und mein verstand", "tokens": ["ihr", "un\u00b7be\u00b7stand", "und", "mein", "ver\u00b7stand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "verl\u00f6schen Amors s\u00fc\u00dfen brand.", "tokens": ["ver\u00b7l\u00f6\u00b7schen", "A\u00b7mors", "s\u00fc\u00b7\u00dfen", "brand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Und ob sie schon wolt wieder schw\u00f6ren,", "tokens": ["Und", "ob", "sie", "schon", "wolt", "wie\u00b7der", "schw\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "als ob ihr mein verdru\u00df sehr leid,", "tokens": ["als", "ob", "ihr", "mein", "ver\u00b7dru\u00df", "sehr", "leid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOKOM", "KOUS", "PPER", "PPOSAT", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "so soll mich doch kein neuer eid,", "tokens": ["so", "soll", "mich", "doch", "kein", "neu\u00b7er", "eid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wie hoch und s\u00fc\u00df er auch, beth\u00f6ren.", "tokens": ["wie", "hoch", "und", "s\u00fc\u00df", "er", "auch", ",", "be\u00b7th\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "ADJD", "KON", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "\u00bbein doppelt leichtfertiger fehl", "tokens": ["\u00bb", "ein", "dop\u00b7pelt", "leicht\u00b7fer\u00b7ti\u00b7ger", "fehl"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJD", "ADJA", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "ist allzeit b\u00f6s f\u00fcr leib und seel.\u00ab", "tokens": ["ist", "all\u00b7zeit", "b\u00f6s", "f\u00fcr", "leib", "und", "seel", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "APPR", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}