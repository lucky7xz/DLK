{"textgrid.poem.44515": {"metadata": {"author": {"name": "Grillparzer, Franz", "birth": "N.A.", "death": "N.A."}, "title": "1L: Potz Hegel und Schlegel!", "genre": "verse", "period": "N.A.", "pub_year": 1841, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Potz Hegel und Schlegel!", "tokens": ["Potz", "He\u00b7gel", "und", "Schle\u00b7gel", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Was gibts in Berlin?", "tokens": ["Was", "gibts", "in", "Ber\u00b7lin", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "NE", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Man sieht ja die G\u00e4ste,", "tokens": ["Man", "sieht", "ja", "die", "G\u00e4s\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wie Spielleut zum Feste,", "tokens": ["Wie", "Spiel\u00b7leut", "zum", "Fes\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "APPRART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Dort haufenweis ziehn.", "tokens": ["Dort", "hau\u00b7fen\u00b7weis", "ziehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Gehts wohl zum Kongresse?", "tokens": ["Gehts", "wohl", "zum", "Kon\u00b7gres\u00b7se", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Wie, oder h\u00e4lt Messe", "tokens": ["Wie", ",", "o\u00b7der", "h\u00e4lt", "Mes\u00b7se"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PWAV", "$,", "KON", "VVFIN", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Der Deutsche Verein?", "tokens": ["Der", "Deut\u00b7sche", "Ver\u00b7ein", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Sie bringen die Waren,", "tokens": ["Sie", "brin\u00b7gen", "die", "Wa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Die kurzen, gefahren,", "tokens": ["Die", "kur\u00b7zen", ",", "ge\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "$,", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Von Elbe und Rhein.", "tokens": ["Von", "El\u00b7be", "und", "Rhein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.3": {"line.1": {"text": "Und alles fein billig,", "tokens": ["Und", "al\u00b7les", "fein", "bil\u00b7lig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADJD", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Gilt Zindel wie Zwillich,", "tokens": ["Gilt", "Zin\u00b7del", "wie", "Zwil\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KOKOM", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Seit einig die Kraft,", "tokens": ["Seit", "ei\u00b7nig", "die", "Kraft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ART", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Der Zoll innerlandes", "tokens": ["Der", "Zoll", "in\u00b7ner\u00b7lan\u00b7des"], "token_info": ["word", "word", "word"], "pos": ["ART", "NN", "ADJA"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Der Kunst, des Verstandes", "tokens": ["Der", "Kunst", ",", "des", "Ver\u00b7stan\u00b7des"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "ART", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Ward ab ja geschafft.", "tokens": ["Ward", "ab", "ja", "ge\u00b7schafft", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ADV", "VVPP", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.4": {"line.1": {"text": "Papier hier ohn Ende,", "tokens": ["Pa\u00b7pier", "hier", "ohn", "En\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Durch flei\u00dfige H\u00e4nde", "tokens": ["Durch", "flei\u00b7\u00dfi\u00b7ge", "H\u00e4n\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Mit Versen besprengt,", "tokens": ["Mit", "Ver\u00b7sen", "be\u00b7sprengt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Belehrend und nutzend,", "tokens": ["Be\u00b7leh\u00b7rend", "und", "nut\u00b7zend", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ADJD", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Man macht sie im Dutzend,", "tokens": ["Man", "macht", "sie", "im", "Dut\u00b7zend", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Die Form geht geschenkt.", "tokens": ["Die", "Form", "geht", "ge\u00b7schenkt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVPP", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.5": {"line.1": {"text": "Hier k\u00f6nnt ihr Novellen", "tokens": ["Hier", "k\u00f6nnt", "ihr", "No\u00b7vel\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Nach Ellen bestellen,", "tokens": ["Nach", "El\u00b7len", "be\u00b7stel\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Der Stuhl feiert nie.", "tokens": ["Der", "Stuhl", "fei\u00b7ert", "nie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "$."], "meter": "-++-+", "measure": "unknown.measure.tri"}, "line.4": {"text": "Ein Dichter in Prosa,", "tokens": ["Ein", "Dich\u00b7ter", "in", "Pro\u00b7sa", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Beredt wie ein Posa,", "tokens": ["Be\u00b7redt", "wie", "ein", "Po\u00b7sa", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Statt Glut Ironie.", "tokens": ["Statt", "Glut", "I\u00b7ro\u00b7nie", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "NN", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.6": {"line.1": {"text": "Dort deutsche Grammatik", "tokens": ["Dort", "deut\u00b7sche", "Gram\u00b7ma\u00b7tik"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Verkauft mit Fanatik", "tokens": ["Ver\u00b7kauft", "mit", "Fa\u00b7na\u00b7tik"], "token_info": ["word", "word", "word"], "pos": ["NN", "APPR", "NN"], "meter": "-+----", "measure": "dactylic.init"}, "line.3": {"text": "Ein Mann, sonst wohl gut.", "tokens": ["Ein", "Mann", ",", "sonst", "wohl", "gut", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "ADV", "ADJD", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Wo Goten, Vandalen", "tokens": ["Wo", "Go\u00b7ten", ",", "Van\u00b7da\u00b7len"], "token_info": ["word", "word", "punct", "word"], "pos": ["PWAV", "NN", "$,", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Als Vorbilder strahlen,", "tokens": ["Als", "Vor\u00b7bil\u00b7der", "strah\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Da, Kunst, fasse Mut.", "tokens": ["Da", ",", "Kunst", ",", "fas\u00b7se", "Mut", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "$,", "NN", "$,", "ADJA", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.7": {"line.1": {"text": "Bei so viel des Neuen", "tokens": ["Bei", "so", "viel", "des", "Neu\u00b7en"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ADV", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "La\u00dft euch nicht gereuen", "tokens": ["La\u00dft", "euch", "nicht", "ge\u00b7reu\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "PTKNEG", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Ein St\u00fcck Rokoko.", "tokens": ["Ein", "St\u00fcck", "Ro\u00b7ko\u00b7ko", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Frisiert \u00e0 la France", "tokens": ["Fri\u00b7siert", "\u00e0", "la", "Fran\u00b7ce"], "token_info": ["word", "word", "word", "word"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "H\u00e4lt hier Renaissance", "tokens": ["H\u00e4lt", "hier", "Re\u00b7nais\u00b7san\u00b7ce"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ADV", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.6": {"text": "Ein Mann comme il faut.", "tokens": ["Ein", "Mann", "com\u00b7me", "il", "faut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "FM", "FM", "FM", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.8": {"line.1": {"text": "Nun fehlt, ob man b\u00f6te,", "tokens": ["Nun", "fehlt", ",", "ob", "man", "b\u00f6\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Nur Wolfgang \u2013 ei, Goethe? \u2013", "tokens": ["Nur", "Wolf\u00b7gang", "\u2013", "ei", ",", "Goe\u00b7the", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["ADV", "NE", "$(", "ITJ", "$,", "NE", "$.", "$("], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Wer denkt noch an das.", "tokens": ["Wer", "denkt", "noch", "an", "das", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "APPR", "PDS", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "Der schn\u00fcrte sein R\u00e4nzel.", "tokens": ["Der", "schn\u00fcr\u00b7te", "sein", "R\u00e4n\u00b7zel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Fehlt, meint ich, nur Menzel", "tokens": ["Fehlt", ",", "meint", "ich", ",", "nur", "Men\u00b7zel"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "$,", "VVFIN", "PPER", "$,", "ADV", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.6": {"text": "Zum deutschen Parna\u00df.", "tokens": ["Zum", "deut\u00b7schen", "Par\u00b7na\u00df", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}}}}