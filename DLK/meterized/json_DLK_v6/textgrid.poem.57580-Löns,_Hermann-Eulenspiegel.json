{"textgrid.poem.57580": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Eulenspiegel", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn wir uns auf der Stra\u00dfe sehen,", "tokens": ["Wenn", "wir", "uns", "auf", "der", "Stra\u00b7\u00dfe", "se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Rei\u00dft's mich an allen Nerven fort,", "tokens": ["Rei\u00dft's", "mich", "an", "al\u00b7len", "Ner\u00b7ven", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PRF", "APPR", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dir meine Neigung zu gestehen", "tokens": ["Dir", "mei\u00b7ne", "Nei\u00b7gung", "zu", "ge\u00b7ste\u00b7hen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "PPOSAT", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mit ungest\u00fcmem Liebeswort.", "tokens": ["Mit", "un\u00b7ge\u00b7st\u00fc\u00b7mem", "Lie\u00b7bes\u00b7wort", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich seh' im Brande deiner Wangen,", "tokens": ["Ich", "seh'", "im", "Bran\u00b7de", "dei\u00b7ner", "Wan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In deiner Augen Demutsglut", "tokens": ["In", "dei\u00b7ner", "Au\u00b7gen", "De\u00b7muts\u00b7glut"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bejahung auf mein hei\u00df Verlangen,", "tokens": ["Be\u00b7ja\u00b7hung", "auf", "mein", "hei\u00df", "Ver\u00b7lan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und doch fehlt mir der Werbemut.", "tokens": ["Und", "doch", "fehlt", "mir", "der", "Wer\u00b7be\u00b7mut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.3": {"line.1": {"text": "Vom Baume der Erkenntnis habe", "tokens": ["Vom", "Bau\u00b7me", "der", "Er\u00b7kennt\u00b7nis", "ha\u00b7be"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich manche s\u00fc\u00dfe Frucht gepfl\u00fcckt,", "tokens": ["Ich", "man\u00b7che", "s\u00fc\u00b7\u00dfe", "Frucht", "ge\u00b7pfl\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch trug ich bald darauf zu Grabe", "tokens": ["Doch", "trug", "ich", "bald", "da\u00b7rauf", "zu", "Gra\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PAV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Hoffnungen, die mich entz\u00fcckt.", "tokens": ["Die", "Hoff\u00b7nun\u00b7gen", ",", "die", "mich", "ent\u00b7z\u00fcckt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Wenn wir uns k\u00fcssend nie umschlossen,", "tokens": ["Wenn", "wir", "uns", "k\u00fcs\u00b7send", "nie", "um\u00b7schlos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann bleiben wir uns ideal,", "tokens": ["Dann", "blei\u00b7ben", "wir", "uns", "i\u00b7deal", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADJD", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch was errungen und genossen,", "tokens": ["Doch", "was", "er\u00b7run\u00b7gen", "und", "ge\u00b7nos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das ist schon morgen welk und schal.", "tokens": ["Das", "ist", "schon", "mor\u00b7gen", "welk", "und", "schal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Drum will ich schnell vor\u00fcberschreiten", "tokens": ["Drum", "will", "ich", "schnell", "vor\u00b7\u00fc\u00b7bersc\u00b7hrei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VMFIN", "PPER", "ADJD", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit stummem Munde, kaltem Blick,", "tokens": ["Mit", "stum\u00b7mem", "Mun\u00b7de", ",", "kal\u00b7tem", "Blick", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Damit uns sp\u00e4ter nicht begleiten", "tokens": ["Da\u00b7mit", "uns", "sp\u00e4\u00b7ter", "nicht", "be\u00b7glei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "PTKNEG", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Erinn'rungen an totes Gl\u00fcck.", "tokens": ["Er\u00b7inn'\u00b7run\u00b7gen", "an", "to\u00b7tes", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}}}}}